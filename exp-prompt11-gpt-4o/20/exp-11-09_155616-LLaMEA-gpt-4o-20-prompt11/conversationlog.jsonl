{"role": "LLaMEA", "time": "2024-11-09 15:56:16.753054", "content": "You are a highly skilled computer scientist in the field of natural computing. Your task is to design novel metaheuristic algorithms to solve black box optimization problems.\n\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the BBOB test suite of 24 noiseless functions. Your task is to write the optimization algorithm in Python code. The code should contain an `__init__(self, budget, dim)` function and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. Each of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nGive an excellent and novel heuristic algorithm to solve this task and also give it a one-line description with the main idea.\n\nProvide the Python code and a one-line description with the main idea (without enters). Give the response in the format:\n# Description: <short-description>\n# Code: \n# ```python\n# <code>\n# ```\n# "}
{"role": "gpt-4o", "time": "2024-11-09 15:56:26.560121", "content": "# Description: A dynamic swarm-based optimization algorithm that adaptively refines search regions using momentum-driven particle movements to efficiently explore and exploit the search space.\n# Code:\n```python\nimport numpy as np\n\nclass DynamicSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(20, self.budget // 2)\n        self.inertia_weight = 0.7\n        self.cognitive_coeff = 1.5\n        self.social_coeff = 1.5\n\n    def __call__(self, func):\n        # Initialize particles\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.zeros((self.population_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        # Evaluate initial particles\n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        # Initialize global best\n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        # Update personal bests\n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        while eval_count < self.budget:\n            # Update velocities and positions\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            positions += velocities\n            \n            # Clamp positions within bounds\n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            # Evaluate new positions\n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            # Update personal bests\n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            # Update global best\n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n\n        return global_best_score, global_best_position\n```\n"}
{"role": "LLaMEA", "time": "2024-11-09 15:57:28.191198", "content": "You are a highly skilled computer scientist in the field of natural computing. Your task is to design novel metaheuristic algorithms to solve black box optimization problems.\n\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the BBOB test suite of 24 noiseless functions. Your task is to write the optimization algorithm in Python code. The code should contain an `__init__(self, budget, dim)` function and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. Each of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nGive an excellent and novel heuristic algorithm to solve this task and also give it a one-line description with the main idea.\n\nThe current population of algorithms already evaluated (name, description, score) is:\nDynamicSwarmOptimizer: A dynamic swarm-based optimization algorithm that adaptively refines search regions using momentum-driven particle movements to efficiently explore and exploit the search space. (Score: 0.2843497343912691)\n\nThe selected solution to update is:\nA dynamic swarm-based optimization algorithm that adaptively refines search regions using momentum-driven particle movements to efficiently explore and exploit the search space.\n\nWith code:\nimport numpy as np\n\nclass DynamicSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(20, self.budget // 2)\n        self.inertia_weight = 0.7\n        self.cognitive_coeff = 1.5\n        self.social_coeff = 1.5\n\n    def __call__(self, func):\n        # Initialize particles\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.zeros((self.population_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        # Evaluate initial particles\n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        # Initialize global best\n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        # Update personal bests\n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        while eval_count < self.budget:\n            # Update velocities and positions\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            positions += velocities\n            \n            # Clamp positions within bounds\n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            # Evaluate new positions\n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            # Update personal bests\n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            # Update global best\n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n\n        return global_best_score, global_best_position\n\nThe algorithm DynamicSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28 with standard deviation 0.27.\n\n\nPlease transform this optimization algorithm code by exactly 20.0% in a \nway that enhances convergence speed. Keep the code difference precisely at \n20.0%, and focus solely on achieving performance improvements through \nalgorithmic adjustments across various elements of the code. Avoid focusing on \ncode efficiency; instead, prioritize exploration of diverse approaches within \nthe allowed modification percentage.\n\n\nProvide the Python code and a one-line description with the main idea (without enters). Give the response in the format:\n# Description: <short-description>\n# Code: \n# ```python\n# <code>\n# ```\n# \n"}
{"role": "gpt-4o", "time": "2024-11-09 15:57:39.791395", "content": "# Description: A dynamic swarm-based optimization algorithm leveraging adaptive inertia and mutation operator to enhance exploration and convergence speed.\n# Code:\n# ```python\nimport numpy as np\n\nclass EnhancedDynamicSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(30, self.budget // 3)  # increased population size\n        self.inertia_weight_max = 0.9  # adaptive inertia\n        self.inertia_weight_min = 0.4\n        self.cognitive_coeff = 1.3  # adjusted coefficients\n        self.social_coeff = 1.7\n\n    def __call__(self, func):\n        # Initialize particles\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))  # random initial velocities\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        # Evaluate initial particles\n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        # Initialize global best\n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        # Update personal bests\n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        iteration = 0\n        max_iterations = self.budget // self.population_size\n        while eval_count < self.budget:\n            # Adaptive inertia weight\n            w = self.inertia_weight_max - (self.inertia_weight_max - self.inertia_weight_min) * (iteration / max_iterations)\n            \n            # Update velocities and positions\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = w * velocities + cognitive_component + social_component\n            positions += velocities\n            \n            # Apply mutation operator for exploration\n            mutation_probability = 0.1\n            mutation_strength = 0.05 * (self.upper_bound - self.lower_bound)\n            mutation_mask = np.random.rand(self.population_size, self.dim) < mutation_probability\n            positions += mutation_mask * np.random.uniform(-mutation_strength, mutation_strength, positions.shape)\n            \n            # Clamp positions within bounds\n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            # Evaluate new positions\n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            # Update personal bests\n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            # Update global best\n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n\n            iteration += 1\n\n        return global_best_score, global_best_position\n# ```"}
{"role": "LLaMEA", "time": "2024-11-09 15:57:39.795769", "content": "You are a highly skilled computer scientist in the field of natural computing. Your task is to design novel metaheuristic algorithms to solve black box optimization problems.\n\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the BBOB test suite of 24 noiseless functions. Your task is to write the optimization algorithm in Python code. The code should contain an `__init__(self, budget, dim)` function and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. Each of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nGive an excellent and novel heuristic algorithm to solve this task and also give it a one-line description with the main idea.\n\nThe current population of algorithms already evaluated (name, description, score) is:\nDynamicSwarmOptimizer: A dynamic swarm-based optimization algorithm that adaptively refines search regions using momentum-driven particle movements to efficiently explore and exploit the search space. (Score: 0.2843497343912691)\n\nThe selected solution to update is:\nA dynamic swarm-based optimization algorithm that adaptively refines search regions using momentum-driven particle movements to efficiently explore and exploit the search space.\n\nWith code:\nimport numpy as np\n\nclass DynamicSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(20, self.budget // 2)\n        self.inertia_weight = 0.7\n        self.cognitive_coeff = 1.5\n        self.social_coeff = 1.5\n\n    def __call__(self, func):\n        # Initialize particles\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.zeros((self.population_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        # Evaluate initial particles\n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        # Initialize global best\n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        # Update personal bests\n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        while eval_count < self.budget:\n            # Update velocities and positions\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            positions += velocities\n            \n            # Clamp positions within bounds\n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            # Evaluate new positions\n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            # Update personal bests\n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            # Update global best\n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n\n        return global_best_score, global_best_position\n\nThe algorithm DynamicSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28 with standard deviation 0.27.\n\n\nPlease transform this optimization algorithm code by exactly 20.0% in a \nway that enhances convergence speed. Keep the code difference precisely at \n20.0%, and focus solely on achieving performance improvements through \nalgorithmic adjustments across various elements of the code. Avoid focusing on \ncode efficiency; instead, prioritize exploration of diverse approaches within \nthe allowed modification percentage.\n\n\nProvide the Python code and a one-line description with the main idea (without enters). Give the response in the format:\n# Description: <short-description>\n# Code: \n# ```python\n# <code>\n# ```\n# \n"}
{"role": "gpt-4o", "time": "2024-11-09 15:57:52.221765", "content": "# Description: Enhanced Swarm Optimizer with Adaptive Parameters and Random Restarts for Improved Convergence Speed.\n# Code:\n# ```python\nimport numpy as np\n\nclass EnhancedSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(30, self.budget // 2)  # Increased population size\n        self.inertia_weight = 0.9  # Adjusted inertia weight\n        self.cognitive_coeff = 2.0  # Adjusted cognitive coefficient\n        self.social_coeff = 1.0  # Adjusted social coefficient\n        self.random_restart_rate = 0.1  # Introduced random restarts\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))  # Random initial velocities\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        while eval_count < self.budget:\n            if np.random.rand() < self.random_restart_rate:\n                random_indices = np.random.choice(self.population_size, self.population_size // 3, replace=False)\n                positions[random_indices] = np.random.uniform(self.lower_bound, self.upper_bound, (len(random_indices), self.dim))\n                velocities[random_indices] = np.random.uniform(-1, 1, (len(random_indices), self.dim))\n            \n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            positions += velocities\n            \n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n\n        return global_best_score, global_best_position\n# ```"}
{"role": "LLaMEA", "time": "2024-11-09 15:57:52.225145", "content": "You are a highly skilled computer scientist in the field of natural computing. Your task is to design novel metaheuristic algorithms to solve black box optimization problems.\n\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the BBOB test suite of 24 noiseless functions. Your task is to write the optimization algorithm in Python code. The code should contain an `__init__(self, budget, dim)` function and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. Each of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nGive an excellent and novel heuristic algorithm to solve this task and also give it a one-line description with the main idea.\n\nThe current population of algorithms already evaluated (name, description, score) is:\nDynamicSwarmOptimizer: A dynamic swarm-based optimization algorithm that adaptively refines search regions using momentum-driven particle movements to efficiently explore and exploit the search space. (Score: 0.2843497343912691)\n\nThe selected solution to update is:\nA dynamic swarm-based optimization algorithm that adaptively refines search regions using momentum-driven particle movements to efficiently explore and exploit the search space.\n\nWith code:\nimport numpy as np\n\nclass DynamicSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(20, self.budget // 2)\n        self.inertia_weight = 0.7\n        self.cognitive_coeff = 1.5\n        self.social_coeff = 1.5\n\n    def __call__(self, func):\n        # Initialize particles\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.zeros((self.population_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        # Evaluate initial particles\n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        # Initialize global best\n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        # Update personal bests\n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        while eval_count < self.budget:\n            # Update velocities and positions\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            positions += velocities\n            \n            # Clamp positions within bounds\n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            # Evaluate new positions\n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            # Update personal bests\n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            # Update global best\n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n\n        return global_best_score, global_best_position\n\nThe algorithm DynamicSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28 with standard deviation 0.27.\n\n\nPlease transform this optimization algorithm code by exactly 20.0% in a \nway that enhances convergence speed. Keep the code difference precisely at \n20.0%, and focus solely on achieving performance improvements through \nalgorithmic adjustments across various elements of the code. Avoid focusing on \ncode efficiency; instead, prioritize exploration of diverse approaches within \nthe allowed modification percentage.\n\n\nProvide the Python code and a one-line description with the main idea (without enters). Give the response in the format:\n# Description: <short-description>\n# Code: \n# ```python\n# <code>\n# ```\n# \n"}
{"role": "gpt-4o", "time": "2024-11-09 15:57:59.747023", "content": "\n# Description: A dual-swarm optimization algorithm introducing an elite subgroup to accelerate convergence by leveraging refined social dynamics.\n# Code: \n# ```python\nimport numpy as np\n\nclass DualSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(20, self.budget // 2)\n        self.inertia_weight = 0.6\n        self.cognitive_coeff = 1.8\n        self.social_coeff = 1.8\n        self.elite_fraction = 0.2  # New: Fraction of elite particles\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.zeros((self.population_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        elite_size = int(self.population_size * self.elite_fraction)\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            positions += velocities\n            \n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n            \n            elite_indices = np.argsort(personal_best_scores)[:elite_size]\n            elite_positions = personal_best_positions[elite_indices]\n            global_best_position = np.mean(elite_positions, axis=0)  # New: Update global best with elite mean\n\n        return global_best_score, global_best_position\n# ```"}
{"role": "LLaMEA", "time": "2024-11-09 15:57:59.751144", "content": "You are a highly skilled computer scientist in the field of natural computing. Your task is to design novel metaheuristic algorithms to solve black box optimization problems.\n\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the BBOB test suite of 24 noiseless functions. Your task is to write the optimization algorithm in Python code. The code should contain an `__init__(self, budget, dim)` function and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. Each of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nGive an excellent and novel heuristic algorithm to solve this task and also give it a one-line description with the main idea.\n\nThe current population of algorithms already evaluated (name, description, score) is:\nDynamicSwarmOptimizer: A dynamic swarm-based optimization algorithm that adaptively refines search regions using momentum-driven particle movements to efficiently explore and exploit the search space. (Score: 0.2843497343912691)\n\nThe selected solution to update is:\nA dynamic swarm-based optimization algorithm that adaptively refines search regions using momentum-driven particle movements to efficiently explore and exploit the search space.\n\nWith code:\nimport numpy as np\n\nclass DynamicSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(20, self.budget // 2)\n        self.inertia_weight = 0.7\n        self.cognitive_coeff = 1.5\n        self.social_coeff = 1.5\n\n    def __call__(self, func):\n        # Initialize particles\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.zeros((self.population_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        # Evaluate initial particles\n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        # Initialize global best\n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        # Update personal bests\n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        while eval_count < self.budget:\n            # Update velocities and positions\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            positions += velocities\n            \n            # Clamp positions within bounds\n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            # Evaluate new positions\n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            # Update personal bests\n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            # Update global best\n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n\n        return global_best_score, global_best_position\n\nThe algorithm DynamicSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28 with standard deviation 0.27.\n\n\nPlease transform this optimization algorithm code by exactly 20.0% in a \nway that enhances convergence speed. Keep the code difference precisely at \n20.0%, and focus solely on achieving performance improvements through \nalgorithmic adjustments across various elements of the code. Avoid focusing on \ncode efficiency; instead, prioritize exploration of diverse approaches within \nthe allowed modification percentage.\n\n\nProvide the Python code and a one-line description with the main idea (without enters). Give the response in the format:\n# Description: <short-description>\n# Code: \n# ```python\n# <code>\n# ```\n# \n"}
{"role": "gpt-4o", "time": "2024-11-09 15:58:09.101080", "content": "# Description: DynamicSwarmOptimizer with adaptive inertia and hybrid local search to enhance convergence speed.\n# Code:\n# ```python\nimport numpy as np\n\nclass DynamicSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(30, self.budget // 2)  # Increased population size\n        self.inertia_weight = 0.9  # Start with higher inertia\n        self.cognitive_coeff = 1.5\n        self.social_coeff = 1.5\n        self.local_search_prob = 0.1  # Probability of local search\n\n    def __call__(self, func):\n        # Initialize particles\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))  # Random initial velocities\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        # Evaluate initial particles\n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        # Initialize global best\n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        # Update personal bests\n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        while eval_count < self.budget:\n            # Update velocities and positions\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            positions += velocities\n            \n            # Adaptive inertia weight\n            self.inertia_weight *= 0.99\n            \n            # Clamp positions within bounds\n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            # Evaluate new positions\n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            # Hybrid local search\n            if np.random.rand() < self.local_search_prob:\n                perturbation = np.random.normal(0, 0.1, size=positions.shape)\n                local_positions = np.clip(positions + perturbation, self.lower_bound, self.upper_bound)\n                local_scores = np.array([func(pos) for pos in local_positions])\n                eval_count += self.population_size\n                improvement_mask = local_scores < scores\n                scores[improvement_mask] = local_scores[improvement_mask]\n                positions[improvement_mask] = local_positions[improvement_mask]\n\n            # Update personal bests\n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            # Update global best\n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n\n        return global_best_score, global_best_position\n# ```"}
{"role": "LLaMEA", "time": "2024-11-09 15:58:09.104680", "content": "You are a highly skilled computer scientist in the field of natural computing. Your task is to design novel metaheuristic algorithms to solve black box optimization problems.\n\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the BBOB test suite of 24 noiseless functions. Your task is to write the optimization algorithm in Python code. The code should contain an `__init__(self, budget, dim)` function and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. Each of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nGive an excellent and novel heuristic algorithm to solve this task and also give it a one-line description with the main idea.\n\nThe current population of algorithms already evaluated (name, description, score) is:\nDynamicSwarmOptimizer: A dynamic swarm-based optimization algorithm that adaptively refines search regions using momentum-driven particle movements to efficiently explore and exploit the search space. (Score: 0.2843497343912691)\n\nThe selected solution to update is:\nA dynamic swarm-based optimization algorithm that adaptively refines search regions using momentum-driven particle movements to efficiently explore and exploit the search space.\n\nWith code:\nimport numpy as np\n\nclass DynamicSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(20, self.budget // 2)\n        self.inertia_weight = 0.7\n        self.cognitive_coeff = 1.5\n        self.social_coeff = 1.5\n\n    def __call__(self, func):\n        # Initialize particles\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.zeros((self.population_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        # Evaluate initial particles\n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        # Initialize global best\n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        # Update personal bests\n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        while eval_count < self.budget:\n            # Update velocities and positions\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            positions += velocities\n            \n            # Clamp positions within bounds\n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            # Evaluate new positions\n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            # Update personal bests\n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            # Update global best\n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n\n        return global_best_score, global_best_position\n\nThe algorithm DynamicSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28 with standard deviation 0.27.\n\n\nPlease transform this optimization algorithm code by exactly 20.0% in a \nway that enhances convergence speed. Keep the code difference precisely at \n20.0%, and focus solely on achieving performance improvements through \nalgorithmic adjustments across various elements of the code. Avoid focusing on \ncode efficiency; instead, prioritize exploration of diverse approaches within \nthe allowed modification percentage.\n\n\nProvide the Python code and a one-line description with the main idea (without enters). Give the response in the format:\n# Description: <short-description>\n# Code: \n# ```python\n# <code>\n# ```\n# \n"}
{"role": "gpt-4o", "time": "2024-11-09 15:58:19.845271", "content": "# Description: An enhanced swarm-based optimizer introducing adaptive inertia and velocity clamping for improved convergence speed.\n# Code:\n# ```python\nimport numpy as np\n\nclass EnhancedDynamicSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(20, self.budget // 2)\n        self.inertia_weight = 0.9  # Increased initial inertia\n        self.cognitive_coeff = 2.0  # Adjusted cognitive coefficient\n        self.social_coeff = 1.5\n\n    def __call__(self, func):\n        # Initialize particles\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))  # Initialize with random velocities\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        # Evaluate initial particles\n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        # Initialize global best\n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        # Update personal bests\n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        while eval_count < self.budget:\n            # Adapt inertia weight\n            self.inertia_weight = 0.4 + 0.5 * (self.budget - eval_count) / self.budget\n            \n            # Update velocities and positions\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            \n            # Clamp velocities to prevent excessive movement\n            velocities = np.clip(velocities, -2.0, 2.0)\n            positions += velocities\n            \n            # Clamp positions within bounds\n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            # Evaluate new positions\n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            # Update personal bests\n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            # Update global best\n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n\n        return global_best_score, global_best_position\n# ```"}
{"role": "LLaMEA", "time": "2024-11-09 15:58:19.849202", "content": "You are a highly skilled computer scientist in the field of natural computing. Your task is to design novel metaheuristic algorithms to solve black box optimization problems.\n\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the BBOB test suite of 24 noiseless functions. Your task is to write the optimization algorithm in Python code. The code should contain an `__init__(self, budget, dim)` function and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. Each of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nGive an excellent and novel heuristic algorithm to solve this task and also give it a one-line description with the main idea.\n\nThe current population of algorithms already evaluated (name, description, score) is:\nDynamicSwarmOptimizer: A dynamic swarm-based optimization algorithm that adaptively refines search regions using momentum-driven particle movements to efficiently explore and exploit the search space. (Score: 0.2843497343912691)\n\nThe selected solution to update is:\nA dynamic swarm-based optimization algorithm that adaptively refines search regions using momentum-driven particle movements to efficiently explore and exploit the search space.\n\nWith code:\nimport numpy as np\n\nclass DynamicSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(20, self.budget // 2)\n        self.inertia_weight = 0.7\n        self.cognitive_coeff = 1.5\n        self.social_coeff = 1.5\n\n    def __call__(self, func):\n        # Initialize particles\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.zeros((self.population_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        # Evaluate initial particles\n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        # Initialize global best\n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        # Update personal bests\n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        while eval_count < self.budget:\n            # Update velocities and positions\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            positions += velocities\n            \n            # Clamp positions within bounds\n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            # Evaluate new positions\n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            # Update personal bests\n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            # Update global best\n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n\n        return global_best_score, global_best_position\n\nThe algorithm DynamicSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28 with standard deviation 0.27.\n\n\nPlease transform this optimization algorithm code by exactly 20.0% in a \nway that enhances convergence speed. Keep the code difference precisely at \n20.0%, and focus solely on achieving performance improvements through \nalgorithmic adjustments across various elements of the code. Avoid focusing on \ncode efficiency; instead, prioritize exploration of diverse approaches within \nthe allowed modification percentage.\n\n\nProvide the Python code and a one-line description with the main idea (without enters). Give the response in the format:\n# Description: <short-description>\n# Code: \n# ```python\n# <code>\n# ```\n# \n"}
{"role": "gpt-4o", "time": "2024-11-09 15:58:28.121907", "content": "# Description: A swarm optimizer with adaptive inertia and differential evolution inspired perturbations for improved convergence speed.\n# Code:\n# ```python\nimport numpy as np\n\nclass AdaptiveDifferentialSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(20, self.budget // 2)\n        self.inertia_weight = 0.9\n        self.inertia_damping = 0.99\n        self.cognitive_coeff = 1.4\n        self.social_coeff = 1.6\n        self.differential_weight = 0.5\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            r3 = np.random.rand(self.population_size, self.dim)\n            \n            differential_component = self.differential_weight * (r3 * (np.random.permutation(positions) - np.random.permutation(positions)))\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component + differential_component\n            positions += velocities\n            \n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n            \n            self.inertia_weight *= self.inertia_damping\n\n        return global_best_score, global_best_position\n# ```"}
{"role": "LLaMEA", "time": "2024-11-09 15:58:28.125133", "content": "You are a highly skilled computer scientist in the field of natural computing. Your task is to design novel metaheuristic algorithms to solve black box optimization problems.\n\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the BBOB test suite of 24 noiseless functions. Your task is to write the optimization algorithm in Python code. The code should contain an `__init__(self, budget, dim)` function and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. Each of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nGive an excellent and novel heuristic algorithm to solve this task and also give it a one-line description with the main idea.\n\nThe current population of algorithms already evaluated (name, description, score) is:\nDynamicSwarmOptimizer: A dynamic swarm-based optimization algorithm that adaptively refines search regions using momentum-driven particle movements to efficiently explore and exploit the search space. (Score: 0.2843497343912691)\n\nThe selected solution to update is:\nA dynamic swarm-based optimization algorithm that adaptively refines search regions using momentum-driven particle movements to efficiently explore and exploit the search space.\n\nWith code:\nimport numpy as np\n\nclass DynamicSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(20, self.budget // 2)\n        self.inertia_weight = 0.7\n        self.cognitive_coeff = 1.5\n        self.social_coeff = 1.5\n\n    def __call__(self, func):\n        # Initialize particles\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.zeros((self.population_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        # Evaluate initial particles\n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        # Initialize global best\n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        # Update personal bests\n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        while eval_count < self.budget:\n            # Update velocities and positions\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            positions += velocities\n            \n            # Clamp positions within bounds\n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            # Evaluate new positions\n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            # Update personal bests\n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            # Update global best\n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n\n        return global_best_score, global_best_position\n\nThe algorithm DynamicSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28 with standard deviation 0.27.\n\n\nPlease transform this optimization algorithm code by exactly 20.0% in a \nway that enhances convergence speed. Keep the code difference precisely at \n20.0%, and focus solely on achieving performance improvements through \nalgorithmic adjustments across various elements of the code. Avoid focusing on \ncode efficiency; instead, prioritize exploration of diverse approaches within \nthe allowed modification percentage.\n\n\nProvide the Python code and a one-line description with the main idea (without enters). Give the response in the format:\n# Description: <short-description>\n# Code: \n# ```python\n# <code>\n# ```\n# \n"}
{"role": "gpt-4o", "time": "2024-11-09 15:58:40.756717", "content": "# Description: An enhanced particle swarm optimization incorporating adaptive inertia and random restarts to improve convergence speed and robustness.\n# Code:\n# ```python\nimport numpy as np\n\nclass EnhancedSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(25, self.budget // 2)\n        self.inertia_weight = 0.9\n        self.cognitive_coeff = 1.6\n        self.social_coeff = 1.4\n        self.restart_threshold = 0.1\n\n    def __call__(self, func):\n        # Initialize particles\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.zeros((self.population_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        # Evaluate initial particles\n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        # Initialize global best\n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        # Update personal bests\n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        while eval_count < self.budget:\n            # Adaptive inertia weight adjustment\n            self.inertia_weight = 0.4 + 0.5 * (eval_count / self.budget)\n            \n            # Update velocities and positions\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            positions += velocities\n            \n            # Clamp positions within bounds\n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            # Evaluate new positions\n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            # Update personal bests\n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            # Update global best\n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n            else:\n                # Restart mechanism if no improvement\n                if np.random.rand() < self.restart_threshold:\n                    positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n                    personal_best_positions = np.copy(positions)\n                    personal_best_scores = np.full(self.population_size, np.inf)\n\n        return global_best_score, global_best_position\n# ```"}
{"role": "LLaMEA", "time": "2024-11-09 15:58:40.760877", "content": "You are a highly skilled computer scientist in the field of natural computing. Your task is to design novel metaheuristic algorithms to solve black box optimization problems.\n\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the BBOB test suite of 24 noiseless functions. Your task is to write the optimization algorithm in Python code. The code should contain an `__init__(self, budget, dim)` function and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. Each of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nGive an excellent and novel heuristic algorithm to solve this task and also give it a one-line description with the main idea.\n\nThe current population of algorithms already evaluated (name, description, score) is:\nDynamicSwarmOptimizer: A dynamic swarm-based optimization algorithm that adaptively refines search regions using momentum-driven particle movements to efficiently explore and exploit the search space. (Score: 0.2843497343912691)\n\nThe selected solution to update is:\nA dynamic swarm-based optimization algorithm that adaptively refines search regions using momentum-driven particle movements to efficiently explore and exploit the search space.\n\nWith code:\nimport numpy as np\n\nclass DynamicSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(20, self.budget // 2)\n        self.inertia_weight = 0.7\n        self.cognitive_coeff = 1.5\n        self.social_coeff = 1.5\n\n    def __call__(self, func):\n        # Initialize particles\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.zeros((self.population_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        # Evaluate initial particles\n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        # Initialize global best\n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        # Update personal bests\n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        while eval_count < self.budget:\n            # Update velocities and positions\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            positions += velocities\n            \n            # Clamp positions within bounds\n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            # Evaluate new positions\n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            # Update personal bests\n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            # Update global best\n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n\n        return global_best_score, global_best_position\n\nThe algorithm DynamicSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28 with standard deviation 0.27.\n\n\nPlease transform this optimization algorithm code by exactly 20.0% in a \nway that enhances convergence speed. Keep the code difference precisely at \n20.0%, and focus solely on achieving performance improvements through \nalgorithmic adjustments across various elements of the code. Avoid focusing on \ncode efficiency; instead, prioritize exploration of diverse approaches within \nthe allowed modification percentage.\n\n\nProvide the Python code and a one-line description with the main idea (without enters). Give the response in the format:\n# Description: <short-description>\n# Code: \n# ```python\n# <code>\n# ```\n# \n"}
{"role": "gpt-4o", "time": "2024-11-09 15:58:50.722923", "content": "# Description: Optimized particle dynamics by incorporating adaptive acceleration coefficients and non-linear inertia reduction to improve convergence speed.\n# Code: \n# ```python\nimport numpy as np\n\nclass DynamicSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(20, self.budget // 2)\n        self.initial_inertia_weight = 0.9\n        self.final_inertia_weight = 0.4\n        self.cognitive_coeff = 2.0\n        self.social_coeff = 2.0\n        self.acceleration_coeff_decrease = 1.0\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.zeros((self.population_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        while eval_count < self.budget:\n            inertia_weight = self.final_inertia_weight + (self.initial_inertia_weight - self.final_inertia_weight) * \\\n                             (1 - eval_count / self.budget) ** self.acceleration_coeff_decrease\n                             \n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = inertia_weight * velocities + cognitive_component + social_component\n            positions += velocities\n            \n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n\n        return global_best_score, global_best_position\n# ```"}
{"role": "LLaMEA", "time": "2024-11-09 15:58:50.726598", "content": "You are a highly skilled computer scientist in the field of natural computing. Your task is to design novel metaheuristic algorithms to solve black box optimization problems.\n\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the BBOB test suite of 24 noiseless functions. Your task is to write the optimization algorithm in Python code. The code should contain an `__init__(self, budget, dim)` function and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. Each of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nGive an excellent and novel heuristic algorithm to solve this task and also give it a one-line description with the main idea.\n\nThe current population of algorithms already evaluated (name, description, score) is:\nDynamicSwarmOptimizer: A dynamic swarm-based optimization algorithm that adaptively refines search regions using momentum-driven particle movements to efficiently explore and exploit the search space. (Score: 0.2843497343912691)\n\nThe selected solution to update is:\nA dynamic swarm-based optimization algorithm that adaptively refines search regions using momentum-driven particle movements to efficiently explore and exploit the search space.\n\nWith code:\nimport numpy as np\n\nclass DynamicSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(20, self.budget // 2)\n        self.inertia_weight = 0.7\n        self.cognitive_coeff = 1.5\n        self.social_coeff = 1.5\n\n    def __call__(self, func):\n        # Initialize particles\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.zeros((self.population_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        # Evaluate initial particles\n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        # Initialize global best\n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        # Update personal bests\n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        while eval_count < self.budget:\n            # Update velocities and positions\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            positions += velocities\n            \n            # Clamp positions within bounds\n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            # Evaluate new positions\n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            # Update personal bests\n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            # Update global best\n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n\n        return global_best_score, global_best_position\n\nThe algorithm DynamicSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28 with standard deviation 0.27.\n\n\nPlease transform this optimization algorithm code by exactly 20.0% in a \nway that enhances convergence speed. Keep the code difference precisely at \n20.0%, and focus solely on achieving performance improvements through \nalgorithmic adjustments across various elements of the code. Avoid focusing on \ncode efficiency; instead, prioritize exploration of diverse approaches within \nthe allowed modification percentage.\n\n\nProvide the Python code and a one-line description with the main idea (without enters). Give the response in the format:\n# Description: <short-description>\n# Code: \n# ```python\n# <code>\n# ```\n# \n"}
{"role": "gpt-4o", "time": "2024-11-09 15:59:00.974650", "content": "# Description: Adaptive Quantum-Inspired Dynamic Swarm Optimizer integrates quantum-inspired dynamics in particle movements to enhance convergence speed.\n# Code: \n# ```python\nimport numpy as np\n\nclass QuantumSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(25, self.budget // 2)  # Increased population\n        self.inertia_weight = 0.5  # Adjusted inertia weight for faster convergence\n        self.cognitive_coeff = 1.3  # Slightly reduced cognitive coefficient\n        self.social_coeff = 1.7  # Increased social coefficient for stronger global influence\n        self.quantum_factor = 0.1  # Quantum influence factor\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))  # Initialize with random velocities\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            quantum_component = self.quantum_factor * (global_best_position - positions) * np.sin(2 * np.pi * r1)  # Quantum influence\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            \n            velocities = self.inertia_weight * velocities + cognitive_component + social_component + quantum_component\n            positions += velocities\n            \n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n\n        return global_best_score, global_best_position\n# ```"}
{"role": "LLaMEA", "time": "2024-11-09 15:59:00.978250", "content": "You are a highly skilled computer scientist in the field of natural computing. Your task is to design novel metaheuristic algorithms to solve black box optimization problems.\n\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the BBOB test suite of 24 noiseless functions. Your task is to write the optimization algorithm in Python code. The code should contain an `__init__(self, budget, dim)` function and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. Each of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nGive an excellent and novel heuristic algorithm to solve this task and also give it a one-line description with the main idea.\n\nThe current population of algorithms already evaluated (name, description, score) is:\nDynamicSwarmOptimizer: A dynamic swarm-based optimization algorithm that adaptively refines search regions using momentum-driven particle movements to efficiently explore and exploit the search space. (Score: 0.2843497343912691)\n\nThe selected solution to update is:\nA dynamic swarm-based optimization algorithm that adaptively refines search regions using momentum-driven particle movements to efficiently explore and exploit the search space.\n\nWith code:\nimport numpy as np\n\nclass DynamicSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(20, self.budget // 2)\n        self.inertia_weight = 0.7\n        self.cognitive_coeff = 1.5\n        self.social_coeff = 1.5\n\n    def __call__(self, func):\n        # Initialize particles\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.zeros((self.population_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        # Evaluate initial particles\n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        # Initialize global best\n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        # Update personal bests\n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        while eval_count < self.budget:\n            # Update velocities and positions\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            positions += velocities\n            \n            # Clamp positions within bounds\n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            # Evaluate new positions\n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            # Update personal bests\n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            # Update global best\n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n\n        return global_best_score, global_best_position\n\nThe algorithm DynamicSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28 with standard deviation 0.27.\n\n\nPlease transform this optimization algorithm code by exactly 20.0% in a \nway that enhances convergence speed. Keep the code difference precisely at \n20.0%, and focus solely on achieving performance improvements through \nalgorithmic adjustments across various elements of the code. Avoid focusing on \ncode efficiency; instead, prioritize exploration of diverse approaches within \nthe allowed modification percentage.\n\n\nProvide the Python code and a one-line description with the main idea (without enters). Give the response in the format:\n# Description: <short-description>\n# Code: \n# ```python\n# <code>\n# ```\n# \n"}
{"role": "gpt-4o", "time": "2024-11-09 15:59:13.338576", "content": "# Description: Incorporating adaptive learning coefficients and neighborhood influence to enhance convergence speed by dynamically balancing exploration and exploitation.\n# Code:\n# ```python\nimport numpy as np\n\nclass DynamicSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(20, self.budget // 2)\n        self.inertia_weight = 0.7\n        self.cognitive_coeff = 1.5\n        self.social_coeff = 1.5\n        self.lbest_coeff = 1.2  # Local best influence coefficient added\n    \n    def __call__(self, func):\n        # Initialize particles\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.zeros((self.population_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        # Evaluate initial particles\n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        # Initialize global best\n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        # Update personal bests\n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        while eval_count < self.budget:\n            # Update velocities and positions with added local best influence\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            r3 = np.random.rand(self.population_size, self.dim)\n            \n            local_best_positions = np.roll(personal_best_positions, shift=1, axis=0)  # Simple local neighborhood\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            local_best_component = self.lbest_coeff * r3 * (local_best_positions - positions)\n            \n            velocities = self.inertia_weight * velocities + cognitive_component + social_component + local_best_component\n            positions += velocities\n            \n            # Clamp positions within bounds\n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            # Evaluate new positions\n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            # Update personal bests\n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            # Update global best\n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n\n        return global_best_score, global_best_position\n# ```"}
{"role": "LLaMEA", "time": "2024-11-09 15:59:13.342693", "content": "You are a highly skilled computer scientist in the field of natural computing. Your task is to design novel metaheuristic algorithms to solve black box optimization problems.\n\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the BBOB test suite of 24 noiseless functions. Your task is to write the optimization algorithm in Python code. The code should contain an `__init__(self, budget, dim)` function and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. Each of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nGive an excellent and novel heuristic algorithm to solve this task and also give it a one-line description with the main idea.\n\nThe current population of algorithms already evaluated (name, description, score) is:\nDynamicSwarmOptimizer: A dynamic swarm-based optimization algorithm that adaptively refines search regions using momentum-driven particle movements to efficiently explore and exploit the search space. (Score: 0.2843497343912691)\n\nThe selected solution to update is:\nA dynamic swarm-based optimization algorithm that adaptively refines search regions using momentum-driven particle movements to efficiently explore and exploit the search space.\n\nWith code:\nimport numpy as np\n\nclass DynamicSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(20, self.budget // 2)\n        self.inertia_weight = 0.7\n        self.cognitive_coeff = 1.5\n        self.social_coeff = 1.5\n\n    def __call__(self, func):\n        # Initialize particles\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.zeros((self.population_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        # Evaluate initial particles\n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        # Initialize global best\n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        # Update personal bests\n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        while eval_count < self.budget:\n            # Update velocities and positions\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            positions += velocities\n            \n            # Clamp positions within bounds\n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            # Evaluate new positions\n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            # Update personal bests\n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            # Update global best\n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n\n        return global_best_score, global_best_position\n\nThe algorithm DynamicSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28 with standard deviation 0.27.\n\n\nPlease transform this optimization algorithm code by exactly 20.0% in a \nway that enhances convergence speed. Keep the code difference precisely at \n20.0%, and focus solely on achieving performance improvements through \nalgorithmic adjustments across various elements of the code. Avoid focusing on \ncode efficiency; instead, prioritize exploration of diverse approaches within \nthe allowed modification percentage.\n\n\nProvide the Python code and a one-line description with the main idea (without enters). Give the response in the format:\n# Description: <short-description>\n# Code: \n# ```python\n# <code>\n# ```\n# \n"}
{"role": "gpt-4o", "time": "2024-11-09 15:59:22.969868", "content": "# Description: An adaptive swarm approach enhancing convergence with dynamic coefficients and diversity promotion through chaotic initialization and velocity reset.\n# Code:\n# ```python\nimport numpy as np\n\nclass DynamicSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(25, self.budget // 2)  # Increased population size\n        self.inertia_weight = 0.5  # Reduced inertia weight for faster convergence\n        self.cognitive_coeff = 1.2 + np.random.rand()  # Dynamic cognitive component\n        self.social_coeff = 1.8 + np.random.rand()  # Dynamic social component\n\n    def __call__(self, func):\n        # Chaotic Initialization of particles\n        positions = self.lower_bound + (self.upper_bound - self.lower_bound) * np.random.rand(self.population_size, self.dim)\n        velocities = np.zeros((self.population_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n\n        # Evaluate initial particles\n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n\n        # Initialize global best\n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n\n        # Update personal bests\n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        while eval_count < self.budget:\n            # Update velocities and positions\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            positions += velocities\n            \n            # Clamp positions within bounds\n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            # Evaluate new positions\n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            # Update personal bests\n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            # Update global best\n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n\n            # Introduce velocity reset for diversity\n            if eval_count % (self.population_size * 2) == 0:\n                velocities = np.zeros((self.population_size, self.dim))\n\n        return global_best_score, global_best_position\n# ```"}
{"role": "LLaMEA", "time": "2024-11-09 15:59:22.973479", "content": "You are a highly skilled computer scientist in the field of natural computing. Your task is to design novel metaheuristic algorithms to solve black box optimization problems.\n\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the BBOB test suite of 24 noiseless functions. Your task is to write the optimization algorithm in Python code. The code should contain an `__init__(self, budget, dim)` function and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. Each of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nGive an excellent and novel heuristic algorithm to solve this task and also give it a one-line description with the main idea.\n\nThe current population of algorithms already evaluated (name, description, score) is:\nDynamicSwarmOptimizer: A dynamic swarm-based optimization algorithm that adaptively refines search regions using momentum-driven particle movements to efficiently explore and exploit the search space. (Score: 0.2843497343912691)\n\nThe selected solution to update is:\nA dynamic swarm-based optimization algorithm that adaptively refines search regions using momentum-driven particle movements to efficiently explore and exploit the search space.\n\nWith code:\nimport numpy as np\n\nclass DynamicSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(20, self.budget // 2)\n        self.inertia_weight = 0.7\n        self.cognitive_coeff = 1.5\n        self.social_coeff = 1.5\n\n    def __call__(self, func):\n        # Initialize particles\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.zeros((self.population_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        # Evaluate initial particles\n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        # Initialize global best\n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        # Update personal bests\n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        while eval_count < self.budget:\n            # Update velocities and positions\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            positions += velocities\n            \n            # Clamp positions within bounds\n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            # Evaluate new positions\n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            # Update personal bests\n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            # Update global best\n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n\n        return global_best_score, global_best_position\n\nThe algorithm DynamicSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28 with standard deviation 0.27.\n\n\nPlease transform this optimization algorithm code by exactly 20.0% in a \nway that enhances convergence speed. Keep the code difference precisely at \n20.0%, and focus solely on achieving performance improvements through \nalgorithmic adjustments across various elements of the code. Avoid focusing on \ncode efficiency; instead, prioritize exploration of diverse approaches within \nthe allowed modification percentage.\n\n\nProvide the Python code and a one-line description with the main idea (without enters). Give the response in the format:\n# Description: <short-description>\n# Code: \n# ```python\n# <code>\n# ```\n# \n"}
{"role": "gpt-4o", "time": "2024-11-09 15:59:32.060699", "content": "# Description: Optimized Swarm with Adaptive Neighborhood and Dynamic Learning for faster convergence and improved search.\n# Code:\n# ```python\nimport numpy as np\n\nclass OptimizedSwarm:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(30, self.budget // 2)  # Increased population for diversity\n        self.inertia_weight = 0.6  # Slightly reduced inertia for quicker changes\n        self.cognitive_coeff = 1.7  # Increased cognitive coefficient for more self-exploration\n        self.social_coeff = 1.3  # Reduced social influence for less premature convergence\n        self.learning_rate_adaptive = 0.9  # Introduce adaptive learning rate for dynamic adjustment\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1.0, 1.0, (self.population_size, self.dim))  # Added initial velocity variance\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n\n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n\n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n\n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        while eval_count < self.budget:\n            adaptive_inertia = self.inertia_weight * (1 - eval_count / self.budget)\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = adaptive_inertia * velocities + cognitive_component + social_component\n            positions += self.learning_rate_adaptive * velocities  # Apply adaptive learning rate\n\n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n\n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n\n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n\n        return global_best_score, global_best_position\n# ```"}
{"role": "LLaMEA", "time": "2024-11-09 15:59:32.065392", "content": "You are a highly skilled computer scientist in the field of natural computing. Your task is to design novel metaheuristic algorithms to solve black box optimization problems.\n\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the BBOB test suite of 24 noiseless functions. Your task is to write the optimization algorithm in Python code. The code should contain an `__init__(self, budget, dim)` function and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. Each of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nGive an excellent and novel heuristic algorithm to solve this task and also give it a one-line description with the main idea.\n\nThe current population of algorithms already evaluated (name, description, score) is:\nDynamicSwarmOptimizer: A dynamic swarm-based optimization algorithm that adaptively refines search regions using momentum-driven particle movements to efficiently explore and exploit the search space. (Score: 0.2843497343912691)\n\nThe selected solution to update is:\nA dynamic swarm-based optimization algorithm that adaptively refines search regions using momentum-driven particle movements to efficiently explore and exploit the search space.\n\nWith code:\nimport numpy as np\n\nclass DynamicSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(20, self.budget // 2)\n        self.inertia_weight = 0.7\n        self.cognitive_coeff = 1.5\n        self.social_coeff = 1.5\n\n    def __call__(self, func):\n        # Initialize particles\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.zeros((self.population_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        # Evaluate initial particles\n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        # Initialize global best\n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        # Update personal bests\n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        while eval_count < self.budget:\n            # Update velocities and positions\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            positions += velocities\n            \n            # Clamp positions within bounds\n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            # Evaluate new positions\n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            # Update personal bests\n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            # Update global best\n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n\n        return global_best_score, global_best_position\n\nThe algorithm DynamicSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28 with standard deviation 0.27.\n\n\nPlease transform this optimization algorithm code by exactly 20.0% in a \nway that enhances convergence speed. Keep the code difference precisely at \n20.0%, and focus solely on achieving performance improvements through \nalgorithmic adjustments across various elements of the code. Avoid focusing on \ncode efficiency; instead, prioritize exploration of diverse approaches within \nthe allowed modification percentage.\n\n\nProvide the Python code and a one-line description with the main idea (without enters). Give the response in the format:\n# Description: <short-description>\n# Code: \n# ```python\n# <code>\n# ```\n# \n"}
{"role": "gpt-4o", "time": "2024-11-09 15:59:43.373603", "content": "# Description: An enhanced swarm optimizer that improves convergence speed by introducing adaptive learning rates and variable swarm size.\n# Code:\n# ```python\nimport numpy as np\n\nclass EnhancedSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.initial_population_size = max(10, self.budget // 5)\n        self.inertia_weight = 0.6\n        self.cognitive_coeff = 2.0\n        self.social_coeff = 1.3\n        self.adaptive_lr = 0.9\n\n    def __call__(self, func):\n        # Initialize particles\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, \n                                      (self.initial_population_size, self.dim))\n        velocities = np.zeros((self.initial_population_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.initial_population_size, np.inf)\n        \n        # Evaluate initial particles\n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.initial_population_size\n        \n        # Initialize global best\n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        # Update personal bests\n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        while eval_count < self.budget:\n            # Update velocities and positions\n            r1 = np.random.rand(self.initial_population_size, self.dim)\n            r2 = np.random.rand(self.initial_population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            self.inertia_weight *= self.adaptive_lr\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            positions += velocities\n            \n            # Clamp positions within bounds\n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            # Evaluate new positions\n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.initial_population_size\n            \n            # Update personal bests\n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            # Update global best\n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n\n            # Adjust population size dynamically based on convergence\n            if eval_count % 15 == 0:\n                positions = np.vstack((positions, np.random.uniform(\n                                      self.lower_bound, self.upper_bound, \n                                      (self.dim // 2, self.dim))))\n                velocities = np.vstack((velocities, np.zeros((self.dim // 2, self.dim))))\n                personal_best_positions = np.vstack((personal_best_positions, \n                                                     np.copy(positions[-(self.dim // 2):])))\n                personal_best_scores = np.append(personal_best_scores, \n                                                 np.full(self.dim // 2, np.inf))\n        return global_best_score, global_best_position\n# ```"}
{"role": "LLaMEA", "time": "2024-11-09 15:59:43.377075", "content": "You are a highly skilled computer scientist in the field of natural computing. Your task is to design novel metaheuristic algorithms to solve black box optimization problems.\n\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the BBOB test suite of 24 noiseless functions. Your task is to write the optimization algorithm in Python code. The code should contain an `__init__(self, budget, dim)` function and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. Each of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nGive an excellent and novel heuristic algorithm to solve this task and also give it a one-line description with the main idea.\n\nThe current population of algorithms already evaluated (name, description, score) is:\nDynamicSwarmOptimizer: A dynamic swarm-based optimization algorithm that adaptively refines search regions using momentum-driven particle movements to efficiently explore and exploit the search space. (Score: 0.2843497343912691)\n\nThe selected solution to update is:\nA dynamic swarm-based optimization algorithm that adaptively refines search regions using momentum-driven particle movements to efficiently explore and exploit the search space.\n\nWith code:\nimport numpy as np\n\nclass DynamicSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(20, self.budget // 2)\n        self.inertia_weight = 0.7\n        self.cognitive_coeff = 1.5\n        self.social_coeff = 1.5\n\n    def __call__(self, func):\n        # Initialize particles\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.zeros((self.population_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        # Evaluate initial particles\n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        # Initialize global best\n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        # Update personal bests\n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        while eval_count < self.budget:\n            # Update velocities and positions\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            positions += velocities\n            \n            # Clamp positions within bounds\n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            # Evaluate new positions\n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            # Update personal bests\n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            # Update global best\n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n\n        return global_best_score, global_best_position\n\nThe algorithm DynamicSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28 with standard deviation 0.27.\n\n\nPlease transform this optimization algorithm code by exactly 20.0% in a \nway that enhances convergence speed. Keep the code difference precisely at \n20.0%, and focus solely on achieving performance improvements through \nalgorithmic adjustments across various elements of the code. Avoid focusing on \ncode efficiency; instead, prioritize exploration of diverse approaches within \nthe allowed modification percentage.\n\n\nProvide the Python code and a one-line description with the main idea (without enters). Give the response in the format:\n# Description: <short-description>\n# Code: \n# ```python\n# <code>\n# ```\n# \n"}
{"role": "gpt-4o", "time": "2024-11-09 15:59:56.391610", "content": "# Description: A stochastic adaptive particle optimization method that dynamically adjusts learning parameters and uses an elite-guided search strategy to enhance exploration and exploitation.\n# Code: \n```python\nimport numpy as np\n\nclass StochasticAdaptiveParticleOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(25, self.budget // 2)\n        self.inertia_weight = 0.6  # Reduced for faster convergence\n        self.cognitive_coeff = 2.0  # Increased to emphasize personal best\n        self.social_coeff = 1.5\n        self.adaptive_scaling = 0.9  # New parameter for adaptive scaling\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            positions += velocities\n            \n            if eval_count % (self.budget // 5) == 0:  # Adaptive adjustment\n                median_score = np.median(scores)\n                self.inertia_weight *= self.adaptive_scaling if median_score > global_best_score else 1.0 / self.adaptive_scaling\n                top_elite_index = np.argmin(scores)\n                elite_position = positions[top_elite_index]\n                positions = (positions + elite_position) / 2\n            \n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n\n        return global_best_score, global_best_position\n```"}
{"role": "LLaMEA", "time": "2024-11-09 16:00:55.149500", "content": "You are a highly skilled computer scientist in the field of natural computing. Your task is to design novel metaheuristic algorithms to solve black box optimization problems.\n\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the BBOB test suite of 24 noiseless functions. Your task is to write the optimization algorithm in Python code. The code should contain an `__init__(self, budget, dim)` function and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. Each of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nGive an excellent and novel heuristic algorithm to solve this task and also give it a one-line description with the main idea.\n\nThe current population of algorithms already evaluated (name, description, score) is:\nStochasticAdaptiveParticleOptimizer: A stochastic adaptive particle optimization method that dynamically adjusts learning parameters and uses an elite-guided search strategy to enhance exploration and exploitation. (Score: 0.3090060317450278)\n\nThe selected solution to update is:\nA stochastic adaptive particle optimization method that dynamically adjusts learning parameters and uses an elite-guided search strategy to enhance exploration and exploitation.\n\nWith code:\nimport numpy as np\n\nclass StochasticAdaptiveParticleOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(25, self.budget // 2)\n        self.inertia_weight = 0.6  # Reduced for faster convergence\n        self.cognitive_coeff = 2.0  # Increased to emphasize personal best\n        self.social_coeff = 1.5\n        self.adaptive_scaling = 0.9  # New parameter for adaptive scaling\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            positions += velocities\n            \n            if eval_count % (self.budget // 5) == 0:  # Adaptive adjustment\n                median_score = np.median(scores)\n                self.inertia_weight *= self.adaptive_scaling if median_score > global_best_score else 1.0 / self.adaptive_scaling\n                top_elite_index = np.argmin(scores)\n                elite_position = positions[top_elite_index]\n                positions = (positions + elite_position) / 2\n            \n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n\n        return global_best_score, global_best_position\n\nThe algorithm StochasticAdaptiveParticleOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.31 with standard deviation 0.29.\n\n\nPlease transform this optimization algorithm code by exactly 20.0% in a \nway that enhances convergence speed. Keep the code difference precisely at \n20.0%, and focus solely on achieving performance improvements through \nalgorithmic adjustments across various elements of the code. Avoid focusing on \ncode efficiency; instead, prioritize exploration of diverse approaches within \nthe allowed modification percentage.\n\n\nProvide the Python code and a one-line description with the main idea (without enters). Give the response in the format:\n# Description: <short-description>\n# Code: \n# ```python\n# <code>\n# ```\n# \n"}
{"role": "gpt-4o", "time": "2024-11-09 16:01:05.853811", "content": "# Description: An enhanced adaptive particle optimization method with stochastic velocity modulation and elite-centric adaptive learning.\n# Code:\n# ```python\nimport numpy as np\n\nclass EnhancedAdaptiveParticleOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(30, self.budget // 3)  # Increased population for better exploration\n        self.inertia_weight = 0.5  # Tweaked for balance between exploration and exploitation\n        self.cognitive_coeff = 2.2  # Slight increase to boost personal best attraction\n        self.social_coeff = 1.3  # Reduced to lessen global influence\n        self.adaptive_scaling = 0.85  # Adjusted for better convergence adaptability\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-0.5, 0.5, (self.population_size, self.dim))  # Lower initial velocities\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            stochastic_modulation = np.random.uniform(0.9, 1.1, (self.population_size, self.dim))  # Introduced stochastic modulation\n            velocities *= stochastic_modulation\n            positions += velocities\n            \n            if eval_count % (self.budget // 4) == 0:  # More frequent adaptive adjustment\n                median_score = np.median(scores)\n                self.inertia_weight *= self.adaptive_scaling if median_score > global_best_score else 1.0 / self.adaptive_scaling\n                top_elite_index = np.argmin(scores)\n                elite_position = positions[top_elite_index]\n                positions = elite_position + (positions - elite_position) * 0.5  # More elite-centric adaptation\n            \n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n\n        return global_best_score, global_best_position\n# ```\n"}
{"role": "LLaMEA", "time": "2024-11-09 16:01:05.858211", "content": "You are a highly skilled computer scientist in the field of natural computing. Your task is to design novel metaheuristic algorithms to solve black box optimization problems.\n\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the BBOB test suite of 24 noiseless functions. Your task is to write the optimization algorithm in Python code. The code should contain an `__init__(self, budget, dim)` function and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. Each of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nGive an excellent and novel heuristic algorithm to solve this task and also give it a one-line description with the main idea.\n\nThe current population of algorithms already evaluated (name, description, score) is:\nStochasticAdaptiveParticleOptimizer: A stochastic adaptive particle optimization method that dynamically adjusts learning parameters and uses an elite-guided search strategy to enhance exploration and exploitation. (Score: 0.3090060317450278)\n\nThe selected solution to update is:\nA stochastic adaptive particle optimization method that dynamically adjusts learning parameters and uses an elite-guided search strategy to enhance exploration and exploitation.\n\nWith code:\nimport numpy as np\n\nclass StochasticAdaptiveParticleOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(25, self.budget // 2)\n        self.inertia_weight = 0.6  # Reduced for faster convergence\n        self.cognitive_coeff = 2.0  # Increased to emphasize personal best\n        self.social_coeff = 1.5\n        self.adaptive_scaling = 0.9  # New parameter for adaptive scaling\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            positions += velocities\n            \n            if eval_count % (self.budget // 5) == 0:  # Adaptive adjustment\n                median_score = np.median(scores)\n                self.inertia_weight *= self.adaptive_scaling if median_score > global_best_score else 1.0 / self.adaptive_scaling\n                top_elite_index = np.argmin(scores)\n                elite_position = positions[top_elite_index]\n                positions = (positions + elite_position) / 2\n            \n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n\n        return global_best_score, global_best_position\n\nThe algorithm StochasticAdaptiveParticleOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.31 with standard deviation 0.29.\n\n\nPlease transform this optimization algorithm code by exactly 20.0% in a \nway that enhances convergence speed. Keep the code difference precisely at \n20.0%, and focus solely on achieving performance improvements through \nalgorithmic adjustments across various elements of the code. Avoid focusing on \ncode efficiency; instead, prioritize exploration of diverse approaches within \nthe allowed modification percentage.\n\n\nProvide the Python code and a one-line description with the main idea (without enters). Give the response in the format:\n# Description: <short-description>\n# Code: \n# ```python\n# <code>\n# ```\n# \n"}
{"role": "gpt-4o", "time": "2024-11-09 16:01:16.636706", "content": "# Description: A modified adaptive particle optimizer with dynamic velocity reset and swarm-based perturbation to enhance convergence speed.\n# Code:\n# ```python\nimport numpy as np\n\nclass EnhancedAdaptiveParticleOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(30, self.budget // 2)  # Increased population size\n        self.inertia_weight = 0.7  # Adjusted for balance\n        self.cognitive_coeff = 1.7  # Slightly reduced\n        self.social_coeff = 1.8  # Slightly increased\n        self.adaptive_scaling = 0.85  # Adjusted adaptive scaling\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n\n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n\n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n\n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n\n            # Dynamic velocity reset for exploration\n            if eval_count % (self.budget // 4) == 0:\n                velocities += np.random.uniform(-0.5, 0.5, (self.population_size, self.dim))\n            \n            positions += velocities\n\n            # Swarm-based perturbation\n            if eval_count % (self.budget // 5) == 0:\n                positions += np.random.uniform(-0.3, 0.3, (self.population_size, self.dim))\n                self.inertia_weight *= self.adaptive_scaling if np.mean(scores) > global_best_score else 1.0 / self.adaptive_scaling\n\n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n\n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n\n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n\n        return global_best_score, global_best_position\n# ```"}
{"role": "LLaMEA", "time": "2024-11-09 16:01:16.640631", "content": "You are a highly skilled computer scientist in the field of natural computing. Your task is to design novel metaheuristic algorithms to solve black box optimization problems.\n\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the BBOB test suite of 24 noiseless functions. Your task is to write the optimization algorithm in Python code. The code should contain an `__init__(self, budget, dim)` function and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. Each of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nGive an excellent and novel heuristic algorithm to solve this task and also give it a one-line description with the main idea.\n\nThe current population of algorithms already evaluated (name, description, score) is:\nStochasticAdaptiveParticleOptimizer: A stochastic adaptive particle optimization method that dynamically adjusts learning parameters and uses an elite-guided search strategy to enhance exploration and exploitation. (Score: 0.3090060317450278)\n\nThe selected solution to update is:\nA stochastic adaptive particle optimization method that dynamically adjusts learning parameters and uses an elite-guided search strategy to enhance exploration and exploitation.\n\nWith code:\nimport numpy as np\n\nclass StochasticAdaptiveParticleOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(25, self.budget // 2)\n        self.inertia_weight = 0.6  # Reduced for faster convergence\n        self.cognitive_coeff = 2.0  # Increased to emphasize personal best\n        self.social_coeff = 1.5\n        self.adaptive_scaling = 0.9  # New parameter for adaptive scaling\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            positions += velocities\n            \n            if eval_count % (self.budget // 5) == 0:  # Adaptive adjustment\n                median_score = np.median(scores)\n                self.inertia_weight *= self.adaptive_scaling if median_score > global_best_score else 1.0 / self.adaptive_scaling\n                top_elite_index = np.argmin(scores)\n                elite_position = positions[top_elite_index]\n                positions = (positions + elite_position) / 2\n            \n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n\n        return global_best_score, global_best_position\n\nThe algorithm StochasticAdaptiveParticleOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.31 with standard deviation 0.29.\n\n\nPlease transform this optimization algorithm code by exactly 20.0% in a \nway that enhances convergence speed. Keep the code difference precisely at \n20.0%, and focus solely on achieving performance improvements through \nalgorithmic adjustments across various elements of the code. Avoid focusing on \ncode efficiency; instead, prioritize exploration of diverse approaches within \nthe allowed modification percentage.\n\n\nProvide the Python code and a one-line description with the main idea (without enters). Give the response in the format:\n# Description: <short-description>\n# Code: \n# ```python\n# <code>\n# ```\n# \n"}
{"role": "gpt-4o", "time": "2024-11-09 16:01:26.789707", "content": "# Description: A modified adaptive particle optimizer with tuned inertia dynamics and a localized search enhancement for improved convergence speed.\n# Code:\n# ```python\nimport numpy as np\n\nclass ModifiedAdaptiveParticleOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0  # Unchanged\n        self.upper_bound = 5.0  # Unchanged\n        self.population_size = min(30, self.budget // 3)  # Adjusted for exploration\n        self.inertia_weight = 0.5  # Tuned for balance between exploration and exploitation\n        self.cognitive_coeff = 1.8  # Slightly reduced for diverse exploration\n        self.social_coeff = 1.7  # Increased to intensify global guidance\n        self.adaptive_scaling = 0.93  # Slightly adjusted scaling\n        self.local_search_prob = 0.1  # New local search mechanism\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n\n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            positions += velocities\n            \n            if np.random.rand() < self.local_search_prob:  # Introduce local search\n                local_index = np.random.randint(self.population_size)\n                perturbation = np.random.uniform(-0.1, 0.1, self.dim)\n                positions[local_index] += perturbation\n            \n            if eval_count % (self.budget // 5) == 0:  # Adaptive adjustment with improved inertia dynamics\n                median_score = np.median(scores)\n                self.inertia_weight *= self.adaptive_scaling if median_score > global_best_score else 1.0 / self.adaptive_scaling\n            \n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n\n        return global_best_score, global_best_position\n# ```"}
{"role": "LLaMEA", "time": "2024-11-09 16:01:26.793581", "content": "You are a highly skilled computer scientist in the field of natural computing. Your task is to design novel metaheuristic algorithms to solve black box optimization problems.\n\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the BBOB test suite of 24 noiseless functions. Your task is to write the optimization algorithm in Python code. The code should contain an `__init__(self, budget, dim)` function and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. Each of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nGive an excellent and novel heuristic algorithm to solve this task and also give it a one-line description with the main idea.\n\nThe current population of algorithms already evaluated (name, description, score) is:\nStochasticAdaptiveParticleOptimizer: A stochastic adaptive particle optimization method that dynamically adjusts learning parameters and uses an elite-guided search strategy to enhance exploration and exploitation. (Score: 0.3090060317450278)\n\nThe selected solution to update is:\nA stochastic adaptive particle optimization method that dynamically adjusts learning parameters and uses an elite-guided search strategy to enhance exploration and exploitation.\n\nWith code:\nimport numpy as np\n\nclass StochasticAdaptiveParticleOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(25, self.budget // 2)\n        self.inertia_weight = 0.6  # Reduced for faster convergence\n        self.cognitive_coeff = 2.0  # Increased to emphasize personal best\n        self.social_coeff = 1.5\n        self.adaptive_scaling = 0.9  # New parameter for adaptive scaling\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            positions += velocities\n            \n            if eval_count % (self.budget // 5) == 0:  # Adaptive adjustment\n                median_score = np.median(scores)\n                self.inertia_weight *= self.adaptive_scaling if median_score > global_best_score else 1.0 / self.adaptive_scaling\n                top_elite_index = np.argmin(scores)\n                elite_position = positions[top_elite_index]\n                positions = (positions + elite_position) / 2\n            \n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n\n        return global_best_score, global_best_position\n\nThe algorithm StochasticAdaptiveParticleOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.31 with standard deviation 0.29.\n\n\nPlease transform this optimization algorithm code by exactly 20.0% in a \nway that enhances convergence speed. Keep the code difference precisely at \n20.0%, and focus solely on achieving performance improvements through \nalgorithmic adjustments across various elements of the code. Avoid focusing on \ncode efficiency; instead, prioritize exploration of diverse approaches within \nthe allowed modification percentage.\n\n\nProvide the Python code and a one-line description with the main idea (without enters). Give the response in the format:\n# Description: <short-description>\n# Code: \n# ```python\n# <code>\n# ```\n# \n"}
{"role": "gpt-4o", "time": "2024-11-09 16:01:37.410026", "content": "\n# Description: An enhanced particle optimizer with adaptive inertia and adaptive velocity clamping for rapid convergence and improved solution quality.\n# Code:\n# ```python\nimport numpy as np\n\nclass StochasticAdaptiveParticleOptimizerEnhanced:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(30, self.budget // 3)\n        self.inertia_weight = 0.7\n        self.cognitive_coeff = 1.8\n        self.social_coeff = 1.7\n        self.adaptive_scaling = 0.85\n        self.velocity_clamp = 2.0\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n\n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n\n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n\n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            velocities = np.clip(velocities, -self.velocity_clamp, self.velocity_clamp)  # Adaptive velocity clamping\n            positions += velocities\n\n            if eval_count % (self.budget // 4) == 0:\n                mean_score = np.mean(scores)\n                self.inertia_weight *= self.adaptive_scaling if mean_score > global_best_score else 1.0 / self.adaptive_scaling\n                top_elite_index = np.argmin(scores)\n                elite_position = positions[top_elite_index]\n                positions = (positions * 0.75 + elite_position * 0.25)  # Enhanced elite influence\n\n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n\n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n\n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n\n        return global_best_score, global_best_position\n# ```"}
{"role": "LLaMEA", "time": "2024-11-09 16:01:37.413888", "content": "You are a highly skilled computer scientist in the field of natural computing. Your task is to design novel metaheuristic algorithms to solve black box optimization problems.\n\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the BBOB test suite of 24 noiseless functions. Your task is to write the optimization algorithm in Python code. The code should contain an `__init__(self, budget, dim)` function and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. Each of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nGive an excellent and novel heuristic algorithm to solve this task and also give it a one-line description with the main idea.\n\nThe current population of algorithms already evaluated (name, description, score) is:\nStochasticAdaptiveParticleOptimizer: A stochastic adaptive particle optimization method that dynamically adjusts learning parameters and uses an elite-guided search strategy to enhance exploration and exploitation. (Score: 0.3090060317450278)\n\nThe selected solution to update is:\nA stochastic adaptive particle optimization method that dynamically adjusts learning parameters and uses an elite-guided search strategy to enhance exploration and exploitation.\n\nWith code:\nimport numpy as np\n\nclass StochasticAdaptiveParticleOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(25, self.budget // 2)\n        self.inertia_weight = 0.6  # Reduced for faster convergence\n        self.cognitive_coeff = 2.0  # Increased to emphasize personal best\n        self.social_coeff = 1.5\n        self.adaptive_scaling = 0.9  # New parameter for adaptive scaling\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            positions += velocities\n            \n            if eval_count % (self.budget // 5) == 0:  # Adaptive adjustment\n                median_score = np.median(scores)\n                self.inertia_weight *= self.adaptive_scaling if median_score > global_best_score else 1.0 / self.adaptive_scaling\n                top_elite_index = np.argmin(scores)\n                elite_position = positions[top_elite_index]\n                positions = (positions + elite_position) / 2\n            \n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n\n        return global_best_score, global_best_position\n\nThe algorithm StochasticAdaptiveParticleOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.31 with standard deviation 0.29.\n\n\nPlease transform this optimization algorithm code by exactly 20.0% in a \nway that enhances convergence speed. Keep the code difference precisely at \n20.0%, and focus solely on achieving performance improvements through \nalgorithmic adjustments across various elements of the code. Avoid focusing on \ncode efficiency; instead, prioritize exploration of diverse approaches within \nthe allowed modification percentage.\n\n\nProvide the Python code and a one-line description with the main idea (without enters). Give the response in the format:\n# Description: <short-description>\n# Code: \n# ```python\n# <code>\n# ```\n# \n"}
{"role": "gpt-4o", "time": "2024-11-09 16:01:49.781257", "content": "# Description: An enhanced stochastic adaptive particle optimization method with improved dynamic learning coefficients and strategic reinitialization for faster convergence.\n# Code:\n# ```python\nimport numpy as np\n\nclass EnhancedStochasticAdaptiveParticleOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(20, self.budget // 3)  # Reduced population size for faster updates\n        self.inertia_weight = 0.7  # Adjusted for dynamic behavior\n        self.cognitive_coeff = 1.8  # Moderated to balance personal and social influence\n        self.social_coeff = 1.8\n        self.adaptive_scaling = 0.85  # Fine-tuned adaptive scaling\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-0.5, 0.5, (self.population_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            positions += velocities\n            \n            if eval_count % (self.budget // 4) == 0:  # More frequent adaptive adjustment\n                if np.random.rand() < 0.5:  # Add randomness to escape local optima\n                    velocities += np.random.uniform(-0.1, 0.1, velocities.shape)\n                self.inertia_weight *= self.adaptive_scaling if np.mean(scores) > global_best_score else 1.0 / self.adaptive_scaling\n                positions = np.where(np.random.rand(self.population_size, self.dim) < 0.1,  # Random reinitialization strategy\n                                     np.random.uniform(self.lower_bound, self.upper_bound, positions.shape), positions)\n            \n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n\n        return global_best_score, global_best_position\n# ```"}
{"role": "LLaMEA", "time": "2024-11-09 16:01:49.785994", "content": "You are a highly skilled computer scientist in the field of natural computing. Your task is to design novel metaheuristic algorithms to solve black box optimization problems.\n\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the BBOB test suite of 24 noiseless functions. Your task is to write the optimization algorithm in Python code. The code should contain an `__init__(self, budget, dim)` function and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. Each of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nGive an excellent and novel heuristic algorithm to solve this task and also give it a one-line description with the main idea.\n\nThe current population of algorithms already evaluated (name, description, score) is:\nStochasticAdaptiveParticleOptimizer: A stochastic adaptive particle optimization method that dynamically adjusts learning parameters and uses an elite-guided search strategy to enhance exploration and exploitation. (Score: 0.3090060317450278)\n\nThe selected solution to update is:\nA stochastic adaptive particle optimization method that dynamically adjusts learning parameters and uses an elite-guided search strategy to enhance exploration and exploitation.\n\nWith code:\nimport numpy as np\n\nclass StochasticAdaptiveParticleOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(25, self.budget // 2)\n        self.inertia_weight = 0.6  # Reduced for faster convergence\n        self.cognitive_coeff = 2.0  # Increased to emphasize personal best\n        self.social_coeff = 1.5\n        self.adaptive_scaling = 0.9  # New parameter for adaptive scaling\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            positions += velocities\n            \n            if eval_count % (self.budget // 5) == 0:  # Adaptive adjustment\n                median_score = np.median(scores)\n                self.inertia_weight *= self.adaptive_scaling if median_score > global_best_score else 1.0 / self.adaptive_scaling\n                top_elite_index = np.argmin(scores)\n                elite_position = positions[top_elite_index]\n                positions = (positions + elite_position) / 2\n            \n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n\n        return global_best_score, global_best_position\n\nThe algorithm StochasticAdaptiveParticleOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.31 with standard deviation 0.29.\n\n\nPlease transform this optimization algorithm code by exactly 20.0% in a \nway that enhances convergence speed. Keep the code difference precisely at \n20.0%, and focus solely on achieving performance improvements through \nalgorithmic adjustments across various elements of the code. Avoid focusing on \ncode efficiency; instead, prioritize exploration of diverse approaches within \nthe allowed modification percentage.\n\n\nProvide the Python code and a one-line description with the main idea (without enters). Give the response in the format:\n# Description: <short-description>\n# Code: \n# ```python\n# <code>\n# ```\n# \n"}
{"role": "gpt-4o", "time": "2024-11-09 16:01:59.974913", "content": "# Description: An enhanced adaptive particle swarm optimization that introduces a dynamic neighborhood influence and stochastic learning rates for improved convergence.\n# Code:\n# ```python\nimport numpy as np\n\nclass EnhancedAdaptiveParticleOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(30, self.budget // 3)\n        self.inertia_weight = 0.7  # Slightly increased inertia for balanced exploration\n        self.cognitive_coeff = 1.8  # Reduced to moderate personal influence\n        self.social_coeff = 1.6\n        self.adaptive_scaling = 0.85  # More frequent scaling\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            \n            # Introduce neighborhood influence\n            neighbors_influence = np.mean(positions, axis=0) - positions\n            velocities = (self.inertia_weight * velocities + cognitive_component +\n                          social_component + 0.5 * neighbors_influence * np.random.rand(self.population_size, self.dim))\n            positions += velocities\n            \n            if eval_count % (self.budget // 6) == 0:  # More frequent adaptive adjustment\n                median_score = np.median(scores)\n                self.inertia_weight *= self.adaptive_scaling if median_score > global_best_score else 1.0 / self.adaptive_scaling\n                top_elite_index = np.argmin(scores)\n                elite_position = positions[top_elite_index]\n                positions += 0.5 * (elite_position - positions)  # Direct elite influence\n            \n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n\n        return global_best_score, global_best_position\n# ```"}
{"role": "LLaMEA", "time": "2024-11-09 16:01:59.978473", "content": "You are a highly skilled computer scientist in the field of natural computing. Your task is to design novel metaheuristic algorithms to solve black box optimization problems.\n\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the BBOB test suite of 24 noiseless functions. Your task is to write the optimization algorithm in Python code. The code should contain an `__init__(self, budget, dim)` function and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. Each of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nGive an excellent and novel heuristic algorithm to solve this task and also give it a one-line description with the main idea.\n\nThe current population of algorithms already evaluated (name, description, score) is:\nStochasticAdaptiveParticleOptimizer: A stochastic adaptive particle optimization method that dynamically adjusts learning parameters and uses an elite-guided search strategy to enhance exploration and exploitation. (Score: 0.3090060317450278)\n\nThe selected solution to update is:\nA stochastic adaptive particle optimization method that dynamically adjusts learning parameters and uses an elite-guided search strategy to enhance exploration and exploitation.\n\nWith code:\nimport numpy as np\n\nclass StochasticAdaptiveParticleOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(25, self.budget // 2)\n        self.inertia_weight = 0.6  # Reduced for faster convergence\n        self.cognitive_coeff = 2.0  # Increased to emphasize personal best\n        self.social_coeff = 1.5\n        self.adaptive_scaling = 0.9  # New parameter for adaptive scaling\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            positions += velocities\n            \n            if eval_count % (self.budget // 5) == 0:  # Adaptive adjustment\n                median_score = np.median(scores)\n                self.inertia_weight *= self.adaptive_scaling if median_score > global_best_score else 1.0 / self.adaptive_scaling\n                top_elite_index = np.argmin(scores)\n                elite_position = positions[top_elite_index]\n                positions = (positions + elite_position) / 2\n            \n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n\n        return global_best_score, global_best_position\n\nThe algorithm StochasticAdaptiveParticleOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.31 with standard deviation 0.29.\n\n\nPlease transform this optimization algorithm code by exactly 20.0% in a \nway that enhances convergence speed. Keep the code difference precisely at \n20.0%, and focus solely on achieving performance improvements through \nalgorithmic adjustments across various elements of the code. Avoid focusing on \ncode efficiency; instead, prioritize exploration of diverse approaches within \nthe allowed modification percentage.\n\n\nProvide the Python code and a one-line description with the main idea (without enters). Give the response in the format:\n# Description: <short-description>\n# Code: \n# ```python\n# <code>\n# ```\n# \n"}
{"role": "gpt-4o", "time": "2024-11-09 16:02:14.814273", "content": "# Description: A hybrid particle optimizer incorporating adaptive learning rates and differential evolution strategies to accelerate convergence across diverse landscapes.\n# Code:\n# ```python\nimport numpy as np\n\nclass HybridAdaptiveParticleOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(30, self.budget // 3)  # Adjusted population size\n        self.inertia_weight = 0.5  # Further reduced for faster convergence\n        self.cognitive_coeff_begin = 2.5  # Variable cognitive coefficient \n        self.cognitive_coeff_end = 1.5\n        self.social_coeff = 1.3  # Reduced to focus more on cognitive\n        self.adaptive_scaling = 0.95  # Slightly adjusted adaptive scaling\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n\n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n\n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n\n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            t = eval_count / self.budget\n            cognitive_coeff = self.cognitive_coeff_begin - (self.cognitive_coeff_begin - self.cognitive_coeff_end) * t\n            cognitive_component = cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            positions += velocities\n\n            if eval_count % (self.budget // 4) == 0:  # More frequent adaptive adjustment\n                median_score = np.median(scores)\n                self.inertia_weight *= self.adaptive_scaling if median_score > global_best_score else 1.0 / self.adaptive_scaling\n                for i in range(self.population_size):\n                    a, b, c = np.random.choice(self.population_size, 3, replace=False)\n                    trial_position = personal_best_positions[a] + 0.8 * (personal_best_positions[b] - personal_best_positions[c])\n                    if func(trial_position) < personal_best_scores[i]:\n                        personal_best_positions[i] = trial_position\n                        personal_best_scores[i] = func(trial_position)\n\n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n\n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n\n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n\n        return global_best_score, global_best_position\n# ```"}
{"role": "LLaMEA", "time": "2024-11-09 16:02:14.817868", "content": "You are a highly skilled computer scientist in the field of natural computing. Your task is to design novel metaheuristic algorithms to solve black box optimization problems.\n\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the BBOB test suite of 24 noiseless functions. Your task is to write the optimization algorithm in Python code. The code should contain an `__init__(self, budget, dim)` function and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. Each of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nGive an excellent and novel heuristic algorithm to solve this task and also give it a one-line description with the main idea.\n\nThe current population of algorithms already evaluated (name, description, score) is:\nStochasticAdaptiveParticleOptimizer: A stochastic adaptive particle optimization method that dynamically adjusts learning parameters and uses an elite-guided search strategy to enhance exploration and exploitation. (Score: 0.3090060317450278)\n\nThe selected solution to update is:\nA stochastic adaptive particle optimization method that dynamically adjusts learning parameters and uses an elite-guided search strategy to enhance exploration and exploitation.\n\nWith code:\nimport numpy as np\n\nclass StochasticAdaptiveParticleOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(25, self.budget // 2)\n        self.inertia_weight = 0.6  # Reduced for faster convergence\n        self.cognitive_coeff = 2.0  # Increased to emphasize personal best\n        self.social_coeff = 1.5\n        self.adaptive_scaling = 0.9  # New parameter for adaptive scaling\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            positions += velocities\n            \n            if eval_count % (self.budget // 5) == 0:  # Adaptive adjustment\n                median_score = np.median(scores)\n                self.inertia_weight *= self.adaptive_scaling if median_score > global_best_score else 1.0 / self.adaptive_scaling\n                top_elite_index = np.argmin(scores)\n                elite_position = positions[top_elite_index]\n                positions = (positions + elite_position) / 2\n            \n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n\n        return global_best_score, global_best_position\n\nThe algorithm StochasticAdaptiveParticleOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.31 with standard deviation 0.29.\n\n\nPlease transform this optimization algorithm code by exactly 20.0% in a \nway that enhances convergence speed. Keep the code difference precisely at \n20.0%, and focus solely on achieving performance improvements through \nalgorithmic adjustments across various elements of the code. Avoid focusing on \ncode efficiency; instead, prioritize exploration of diverse approaches within \nthe allowed modification percentage.\n\n\nProvide the Python code and a one-line description with the main idea (without enters). Give the response in the format:\n# Description: <short-description>\n# Code: \n# ```python\n# <code>\n# ```\n# \n"}
{"role": "gpt-4o", "time": "2024-11-09 16:02:26.136009", "content": "# Description: A refined particle optimization algorithm with enhanced inertia adaptation and elite-guided convergence to improve convergence speed.\n# Code: \n```python\nimport numpy as np\n\nclass EnhancedParticleOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(30, self.budget // 3)  # Adjusted population size\n        self.inertia_weight = 0.7  # Adjusted inertia weight for balancing exploration\n        self.cognitive_coeff = 1.5  # Lowered to reduce excessive local search focus\n        self.social_coeff = 2.0  # Increased to leverage global influence\n        self.adaptive_scaling = 0.95  # Enhanced adaptive scaling for refined control\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            positions += velocities\n            \n            if eval_count % (self.budget // 7) == 0:  # More frequent adaptive adjustment\n                median_score = np.median(scores)\n                self.inertia_weight *= self.adaptive_scaling if median_score > global_best_score else 1.0 / self.adaptive_scaling\n                top_elite_index = np.argmin(scores)\n                elite_position = positions[top_elite_index]\n                positions = (positions + elite_position) / 2.5  # Adjusted elite influence factor\n            \n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n\n        return global_best_score, global_best_position\n```"}
{"role": "LLaMEA", "time": "2024-11-09 16:03:22.906588", "content": "You are a highly skilled computer scientist in the field of natural computing. Your task is to design novel metaheuristic algorithms to solve black box optimization problems.\n\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the BBOB test suite of 24 noiseless functions. Your task is to write the optimization algorithm in Python code. The code should contain an `__init__(self, budget, dim)` function and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. Each of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nGive an excellent and novel heuristic algorithm to solve this task and also give it a one-line description with the main idea.\n\nThe current population of algorithms already evaluated (name, description, score) is:\nStochasticAdaptiveParticleOptimizer: A stochastic adaptive particle optimization method that dynamically adjusts learning parameters and uses an elite-guided search strategy to enhance exploration and exploitation. (Score: 0.3090060317450278)\n\nThe selected solution to update is:\nA stochastic adaptive particle optimization method that dynamically adjusts learning parameters and uses an elite-guided search strategy to enhance exploration and exploitation.\n\nWith code:\nimport numpy as np\n\nclass StochasticAdaptiveParticleOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(25, self.budget // 2)\n        self.inertia_weight = 0.6  # Reduced for faster convergence\n        self.cognitive_coeff = 2.0  # Increased to emphasize personal best\n        self.social_coeff = 1.5\n        self.adaptive_scaling = 0.9  # New parameter for adaptive scaling\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            positions += velocities\n            \n            if eval_count % (self.budget // 5) == 0:  # Adaptive adjustment\n                median_score = np.median(scores)\n                self.inertia_weight *= self.adaptive_scaling if median_score > global_best_score else 1.0 / self.adaptive_scaling\n                top_elite_index = np.argmin(scores)\n                elite_position = positions[top_elite_index]\n                positions = (positions + elite_position) / 2\n            \n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n\n        return global_best_score, global_best_position\n\nThe algorithm StochasticAdaptiveParticleOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.31 with standard deviation 0.29.\n\n\nPlease transform this optimization algorithm code by exactly 20.0% in a \nway that enhances convergence speed. Keep the code difference precisely at \n20.0%, and focus solely on achieving performance improvements through \nalgorithmic adjustments across various elements of the code. Avoid focusing on \ncode efficiency; instead, prioritize exploration of diverse approaches within \nthe allowed modification percentage.\n\n\nProvide the Python code and a one-line description with the main idea (without enters). Give the response in the format:\n# Description: <short-description>\n# Code: \n# ```python\n# <code>\n# ```\n# \n"}
{"role": "gpt-4o", "time": "2024-11-09 16:03:37.633341", "content": "# Description: A hybridized adaptive particle optimization method incorporating differential evolution crossover for enhanced solution diversity and convergence speed.\n# Code: \n# ```python\nimport numpy as np\n\nclass HybridAdaptiveParticleOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(25, self.budget // 2)\n        self.inertia_weight = 0.5  # Further reduced for faster convergence\n        self.cognitive_coeff = 1.5  # Adjusted for balance\n        self.social_coeff = 1.7\n        self.adaptive_scaling = 0.85  # Adjusted scaling for adaptation\n        self.de_weight = 0.8  # Differential evolution weight\n        self.de_cross_prob = 0.7  # Crossover probability\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            positions += velocities\n            \n            # Differential Evolution Crossover\n            for i in range(self.population_size):\n                indices = np.random.choice(self.population_size, 3, replace=False)\n                x1, x2, x3 = positions[indices]\n                mutant = x1 + self.de_weight * (x2 - x3)\n                crossover = np.random.rand(self.dim) < self.de_cross_prob\n                trial = np.where(crossover, mutant, positions[i])\n                positions[i] = np.clip(trial, self.lower_bound, self.upper_bound)\n            \n            if eval_count % (self.budget // 5) == 0:\n                median_score = np.median(scores)\n                self.inertia_weight *= self.adaptive_scaling if median_score > global_best_score else 1.0 / self.adaptive_scaling\n                top_elite_index = np.argmin(scores)\n                elite_position = positions[top_elite_index]\n                positions = (positions + elite_position) / 2\n            \n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n\n        return global_best_score, global_best_position\n```"}
{"role": "LLaMEA", "time": "2024-11-09 16:06:07.105128", "content": "You are a highly skilled computer scientist in the field of natural computing. Your task is to design novel metaheuristic algorithms to solve black box optimization problems.\n\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the BBOB test suite of 24 noiseless functions. Your task is to write the optimization algorithm in Python code. The code should contain an `__init__(self, budget, dim)` function and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. Each of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nGive an excellent and novel heuristic algorithm to solve this task and also give it a one-line description with the main idea.\n\nThe current population of algorithms already evaluated (name, description, score) is:\nStochasticAdaptiveParticleOptimizer: A stochastic adaptive particle optimization method that dynamically adjusts learning parameters and uses an elite-guided search strategy to enhance exploration and exploitation. (Score: 0.3090060317450278)\n\nThe selected solution to update is:\nA stochastic adaptive particle optimization method that dynamically adjusts learning parameters and uses an elite-guided search strategy to enhance exploration and exploitation.\n\nWith code:\nimport numpy as np\n\nclass StochasticAdaptiveParticleOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(25, self.budget // 2)\n        self.inertia_weight = 0.6  # Reduced for faster convergence\n        self.cognitive_coeff = 2.0  # Increased to emphasize personal best\n        self.social_coeff = 1.5\n        self.adaptive_scaling = 0.9  # New parameter for adaptive scaling\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            positions += velocities\n            \n            if eval_count % (self.budget // 5) == 0:  # Adaptive adjustment\n                median_score = np.median(scores)\n                self.inertia_weight *= self.adaptive_scaling if median_score > global_best_score else 1.0 / self.adaptive_scaling\n                top_elite_index = np.argmin(scores)\n                elite_position = positions[top_elite_index]\n                positions = (positions + elite_position) / 2\n            \n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n\n        return global_best_score, global_best_position\n\nThe algorithm StochasticAdaptiveParticleOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.31 with standard deviation 0.29.\n\n\nPlease transform this optimization algorithm code by exactly 20.0% in a \nway that enhances convergence speed. Keep the code difference precisely at \n20.0%, and focus solely on achieving performance improvements through \nalgorithmic adjustments across various elements of the code. Avoid focusing on \ncode efficiency; instead, prioritize exploration of diverse approaches within \nthe allowed modification percentage.\n\n\nProvide the Python code and a one-line description with the main idea (without enters). Give the response in the format:\n# Description: <short-description>\n# Code: \n# ```python\n# <code>\n# ```\n# \n"}
{"role": "gpt-4o", "time": "2024-11-09 16:06:16.926791", "content": "# Description: An enhanced adaptive particle optimizer that strengthens exploration with chaotic maps and employs adaptive learning coefficients for improved convergence.\n# Code:\n# ```python\nimport numpy as np\n\nclass EnhancedStochasticAdaptiveParticleOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = max(30, self.budget // 3)\n        self.inertia_weight = 0.7\n        self.cognitive_coeff = 1.8  # Slightly reduced\n        self.social_coeff = 2.1  # Slightly increased\n        self.adaptive_scaling = 0.85  # More aggressive scaling\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-0.5, 0.5, (self.population_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            positions += velocities\n            \n            # Introduce chaotic map-based perturbation for exploration\n            if eval_count % (self.budget // 4) == 0:\n                chaos_seq = 4 * r1 * (1 - r1)\n                perturbation = (chaos_seq - 0.5) * (self.upper_bound - self.lower_bound) * 0.01\n                positions += perturbation\n            \n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n\n        return global_best_score, global_best_position\n# ```"}
{"role": "LLaMEA", "time": "2024-11-09 16:06:16.930535", "content": "You are a highly skilled computer scientist in the field of natural computing. Your task is to design novel metaheuristic algorithms to solve black box optimization problems.\n\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the BBOB test suite of 24 noiseless functions. Your task is to write the optimization algorithm in Python code. The code should contain an `__init__(self, budget, dim)` function and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. Each of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nGive an excellent and novel heuristic algorithm to solve this task and also give it a one-line description with the main idea.\n\nThe current population of algorithms already evaluated (name, description, score) is:\nStochasticAdaptiveParticleOptimizer: A stochastic adaptive particle optimization method that dynamically adjusts learning parameters and uses an elite-guided search strategy to enhance exploration and exploitation. (Score: 0.3090060317450278)\n\nThe selected solution to update is:\nA stochastic adaptive particle optimization method that dynamically adjusts learning parameters and uses an elite-guided search strategy to enhance exploration and exploitation.\n\nWith code:\nimport numpy as np\n\nclass StochasticAdaptiveParticleOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(25, self.budget // 2)\n        self.inertia_weight = 0.6  # Reduced for faster convergence\n        self.cognitive_coeff = 2.0  # Increased to emphasize personal best\n        self.social_coeff = 1.5\n        self.adaptive_scaling = 0.9  # New parameter for adaptive scaling\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            positions += velocities\n            \n            if eval_count % (self.budget // 5) == 0:  # Adaptive adjustment\n                median_score = np.median(scores)\n                self.inertia_weight *= self.adaptive_scaling if median_score > global_best_score else 1.0 / self.adaptive_scaling\n                top_elite_index = np.argmin(scores)\n                elite_position = positions[top_elite_index]\n                positions = (positions + elite_position) / 2\n            \n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n\n        return global_best_score, global_best_position\n\nThe algorithm StochasticAdaptiveParticleOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.31 with standard deviation 0.29.\n\n\nPlease transform this optimization algorithm code by exactly 20.0% in a \nway that enhances convergence speed. Keep the code difference precisely at \n20.0%, and focus solely on achieving performance improvements through \nalgorithmic adjustments across various elements of the code. Avoid focusing on \ncode efficiency; instead, prioritize exploration of diverse approaches within \nthe allowed modification percentage.\n\n\nProvide the Python code and a one-line description with the main idea (without enters). Give the response in the format:\n# Description: <short-description>\n# Code: \n# ```python\n# <code>\n# ```\n# \n"}
{"role": "gpt-4o", "time": "2024-11-09 16:06:32.963013", "content": "# Description: A diversity-enhanced stochastic adaptive particle optimization algorithm with dynamic population resizing and velocity mutation for improved convergence speed.\n# Code:\n# ```python\nimport numpy as np\n\nclass EnhancedStochasticAdaptiveParticleOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(30, self.budget // 3)  # Increased initial population\n        self.inertia_weight = 0.5  # Reduced inertia for quicker response\n        self.cognitive_coeff = 2.5  # Enhanced cognitive influence\n        self.social_coeff = 1.2  # Reduced social influence\n        self.adaptive_scaling = 0.85  # Adjusted for more adaptive scaling\n        self.velocity_mutation_prob = 0.1  # Introduce mutation on velocity\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            \n            # Apply velocity mutation\n            mutation_mask = np.random.rand(self.population_size, self.dim) < self.velocity_mutation_prob\n            velocities[mutation_mask] = np.random.uniform(-1, 1, np.sum(mutation_mask))\n            \n            positions += velocities\n            \n            if eval_count % (self.budget // 5) == 0:\n                median_score = np.median(scores)\n                self.inertia_weight *= self.adaptive_scaling if median_score > global_best_score else 1.0 / self.adaptive_scaling\n                new_population_size = max(10, self.population_size // 2)  # Dynamic resizing\n                positions = positions[:new_population_size]\n                velocities = velocities[:new_population_size]\n            \n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += len(positions)\n            \n            personal_better_mask = scores < personal_best_scores[:len(scores)]\n            personal_best_scores[:len(scores)][personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[:len(scores)][personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n\n        return global_best_score, global_best_position\n# ```"}
{"role": "LLaMEA", "time": "2024-11-09 16:06:32.966819", "content": "You are a highly skilled computer scientist in the field of natural computing. Your task is to design novel metaheuristic algorithms to solve black box optimization problems.\n\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the BBOB test suite of 24 noiseless functions. Your task is to write the optimization algorithm in Python code. The code should contain an `__init__(self, budget, dim)` function and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. Each of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nGive an excellent and novel heuristic algorithm to solve this task and also give it a one-line description with the main idea.\n\nThe current population of algorithms already evaluated (name, description, score) is:\nStochasticAdaptiveParticleOptimizer: A stochastic adaptive particle optimization method that dynamically adjusts learning parameters and uses an elite-guided search strategy to enhance exploration and exploitation. (Score: 0.3090060317450278)\n\nThe selected solution to update is:\nA stochastic adaptive particle optimization method that dynamically adjusts learning parameters and uses an elite-guided search strategy to enhance exploration and exploitation.\n\nWith code:\nimport numpy as np\n\nclass StochasticAdaptiveParticleOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(25, self.budget // 2)\n        self.inertia_weight = 0.6  # Reduced for faster convergence\n        self.cognitive_coeff = 2.0  # Increased to emphasize personal best\n        self.social_coeff = 1.5\n        self.adaptive_scaling = 0.9  # New parameter for adaptive scaling\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            positions += velocities\n            \n            if eval_count % (self.budget // 5) == 0:  # Adaptive adjustment\n                median_score = np.median(scores)\n                self.inertia_weight *= self.adaptive_scaling if median_score > global_best_score else 1.0 / self.adaptive_scaling\n                top_elite_index = np.argmin(scores)\n                elite_position = positions[top_elite_index]\n                positions = (positions + elite_position) / 2\n            \n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n\n        return global_best_score, global_best_position\n\nThe algorithm StochasticAdaptiveParticleOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.31 with standard deviation 0.29.\n\n\nPlease transform this optimization algorithm code by exactly 20.0% in a \nway that enhances convergence speed. Keep the code difference precisely at \n20.0%, and focus solely on achieving performance improvements through \nalgorithmic adjustments across various elements of the code. Avoid focusing on \ncode efficiency; instead, prioritize exploration of diverse approaches within \nthe allowed modification percentage.\n\n\nProvide the Python code and a one-line description with the main idea (without enters). Give the response in the format:\n# Description: <short-description>\n# Code: \n# ```python\n# <code>\n# ```\n# \n"}
{"role": "gpt-4o", "time": "2024-11-09 16:06:42.457049", "content": "# Description: Enhanced stochastic adaptive particle optimizer with dynamic velocity adjustment and mutation for improved convergence speed.\n# Code:\n# ```python\nimport numpy as np\n\nclass EnhancedStochasticAdaptiveParticleOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(25, self.budget // 3)  # Adjusted for more focus\n        self.inertia_weight = 0.7  # Slightly increased for better exploration\n        self.cognitive_coeff = 1.5  # Adjusted to balance personal influence\n        self.social_coeff = 2.0  # Increased social influence for convergence\n        self.mutation_prob = 0.1  # Introduced mutation probability\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            \n            mutation_mask = np.random.rand(self.population_size, self.dim) < self.mutation_prob\n            velocities += mutation_mask * np.random.uniform(-0.5, 0.5, (self.population_size, self.dim))\n            \n            positions += velocities\n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n\n        return global_best_score, global_best_position\n# ```"}
{"role": "LLaMEA", "time": "2024-11-09 16:06:42.461012", "content": "You are a highly skilled computer scientist in the field of natural computing. Your task is to design novel metaheuristic algorithms to solve black box optimization problems.\n\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the BBOB test suite of 24 noiseless functions. Your task is to write the optimization algorithm in Python code. The code should contain an `__init__(self, budget, dim)` function and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. Each of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nGive an excellent and novel heuristic algorithm to solve this task and also give it a one-line description with the main idea.\n\nThe current population of algorithms already evaluated (name, description, score) is:\nStochasticAdaptiveParticleOptimizer: A stochastic adaptive particle optimization method that dynamically adjusts learning parameters and uses an elite-guided search strategy to enhance exploration and exploitation. (Score: 0.3090060317450278)\n\nThe selected solution to update is:\nA stochastic adaptive particle optimization method that dynamically adjusts learning parameters and uses an elite-guided search strategy to enhance exploration and exploitation.\n\nWith code:\nimport numpy as np\n\nclass StochasticAdaptiveParticleOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(25, self.budget // 2)\n        self.inertia_weight = 0.6  # Reduced for faster convergence\n        self.cognitive_coeff = 2.0  # Increased to emphasize personal best\n        self.social_coeff = 1.5\n        self.adaptive_scaling = 0.9  # New parameter for adaptive scaling\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            positions += velocities\n            \n            if eval_count % (self.budget // 5) == 0:  # Adaptive adjustment\n                median_score = np.median(scores)\n                self.inertia_weight *= self.adaptive_scaling if median_score > global_best_score else 1.0 / self.adaptive_scaling\n                top_elite_index = np.argmin(scores)\n                elite_position = positions[top_elite_index]\n                positions = (positions + elite_position) / 2\n            \n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n\n        return global_best_score, global_best_position\n\nThe algorithm StochasticAdaptiveParticleOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.31 with standard deviation 0.29.\n\n\nPlease transform this optimization algorithm code by exactly 20.0% in a \nway that enhances convergence speed. Keep the code difference precisely at \n20.0%, and focus solely on achieving performance improvements through \nalgorithmic adjustments across various elements of the code. Avoid focusing on \ncode efficiency; instead, prioritize exploration of diverse approaches within \nthe allowed modification percentage.\n\n\nProvide the Python code and a one-line description with the main idea (without enters). Give the response in the format:\n# Description: <short-description>\n# Code: \n# ```python\n# <code>\n# ```\n# \n"}
{"role": "gpt-4o", "time": "2024-11-09 16:06:52.542654", "content": "# Description: An enhanced dynamic particle swarm optimizer with adaptive coefficients and elite local search for faster convergence.\n# Code: \n# ```python\nimport numpy as np\n\nclass EnhancedDynamicParticleOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(30, self.budget // 3)  # Increased population size\n        self.inertia_weight = 0.5  # Further reduced for faster convergence\n        self.cognitive_coeff = 1.8  # Slightly adjusted for balance\n        self.social_coeff = 2.0  # Increased to enhance social learning\n        self.adaptive_scaling = 0.85  # More aggressive scaling\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-0.5, 0.5, (self.population_size, self.dim))  # Reduced initial velocity range\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            positions += velocities\n            \n            if eval_count % (self.budget // 4) == 0:  # More frequent adaptive adjustments\n                self.inertia_weight *= self.adaptive_scaling if np.mean(scores) > global_best_score else 1.0 / self.adaptive_scaling\n                elite_index = np.argsort(scores)[:3]  # Use top 3 instead of top 1 for elite influence\n                elite_positions = positions[elite_index].mean(axis=0)\n                positions = (positions + elite_positions) / 2\n            \n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n\n        return global_best_score, global_best_position\n```"}
{"role": "LLaMEA", "time": "2024-11-09 16:07:52.187312", "content": "You are a highly skilled computer scientist in the field of natural computing. Your task is to design novel metaheuristic algorithms to solve black box optimization problems.\n\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the BBOB test suite of 24 noiseless functions. Your task is to write the optimization algorithm in Python code. The code should contain an `__init__(self, budget, dim)` function and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. Each of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nGive an excellent and novel heuristic algorithm to solve this task and also give it a one-line description with the main idea.\n\nThe current population of algorithms already evaluated (name, description, score) is:\nStochasticAdaptiveParticleOptimizer: A stochastic adaptive particle optimization method that dynamically adjusts learning parameters and uses an elite-guided search strategy to enhance exploration and exploitation. (Score: 0.3090060317450278)\n\nThe selected solution to update is:\nA stochastic adaptive particle optimization method that dynamically adjusts learning parameters and uses an elite-guided search strategy to enhance exploration and exploitation.\n\nWith code:\nimport numpy as np\n\nclass StochasticAdaptiveParticleOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(25, self.budget // 2)\n        self.inertia_weight = 0.6  # Reduced for faster convergence\n        self.cognitive_coeff = 2.0  # Increased to emphasize personal best\n        self.social_coeff = 1.5\n        self.adaptive_scaling = 0.9  # New parameter for adaptive scaling\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            positions += velocities\n            \n            if eval_count % (self.budget // 5) == 0:  # Adaptive adjustment\n                median_score = np.median(scores)\n                self.inertia_weight *= self.adaptive_scaling if median_score > global_best_score else 1.0 / self.adaptive_scaling\n                top_elite_index = np.argmin(scores)\n                elite_position = positions[top_elite_index]\n                positions = (positions + elite_position) / 2\n            \n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n\n        return global_best_score, global_best_position\n\nThe algorithm StochasticAdaptiveParticleOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.31 with standard deviation 0.29.\n\n\nPlease transform this optimization algorithm code by exactly 20.0% in a \nway that enhances convergence speed. Keep the code difference precisely at \n20.0%, and focus solely on achieving performance improvements through \nalgorithmic adjustments across various elements of the code. Avoid focusing on \ncode efficiency; instead, prioritize exploration of diverse approaches within \nthe allowed modification percentage.\n\n\nProvide the Python code and a one-line description with the main idea (without enters). Give the response in the format:\n# Description: <short-description>\n# Code: \n# ```python\n# <code>\n# ```\n# \n"}
{"role": "gpt-4o", "time": "2024-11-09 16:08:04.338273", "content": "# Description: Enhanced Particle Optimization with Momentum and Dynamic Population Size for Improved Convergence.\n# Code:\n# ```python\nimport numpy as np\n\nclass EnhancedParticleOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(30, self.budget // 3)  # Increased initial population\n        self.inertia_weight = 0.7  # Increased for better exploration\n        self.cognitive_coeff = 1.5  # Slightly reduced to balance exploration\n        self.social_coeff = 1.8\n        self.momentum = 0.9  # New parameter for momentum effect\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.momentum * velocities + self.inertia_weight * velocities + cognitive_component + social_component\n            positions += velocities\n            \n            if eval_count % (self.budget // 10) == 0:  # More frequent adaptive adjustment\n                self.population_size = max(10, self.population_size // 2)  # Dynamic population size\n                positions = positions[:self.population_size]\n                velocities = velocities[:self.population_size]\n                scores = scores[:self.population_size]\n                personal_best_positions = personal_best_positions[:self.population_size]\n                personal_best_scores = personal_best_scores[:self.population_size]\n            \n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n\n        return global_best_score, global_best_position\n# ```"}
{"role": "LLaMEA", "time": "2024-11-09 16:08:04.341813", "content": "You are a highly skilled computer scientist in the field of natural computing. Your task is to design novel metaheuristic algorithms to solve black box optimization problems.\n\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the BBOB test suite of 24 noiseless functions. Your task is to write the optimization algorithm in Python code. The code should contain an `__init__(self, budget, dim)` function and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. Each of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nGive an excellent and novel heuristic algorithm to solve this task and also give it a one-line description with the main idea.\n\nThe current population of algorithms already evaluated (name, description, score) is:\nStochasticAdaptiveParticleOptimizer: A stochastic adaptive particle optimization method that dynamically adjusts learning parameters and uses an elite-guided search strategy to enhance exploration and exploitation. (Score: 0.3090060317450278)\n\nThe selected solution to update is:\nA stochastic adaptive particle optimization method that dynamically adjusts learning parameters and uses an elite-guided search strategy to enhance exploration and exploitation.\n\nWith code:\nimport numpy as np\n\nclass StochasticAdaptiveParticleOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(25, self.budget // 2)\n        self.inertia_weight = 0.6  # Reduced for faster convergence\n        self.cognitive_coeff = 2.0  # Increased to emphasize personal best\n        self.social_coeff = 1.5\n        self.adaptive_scaling = 0.9  # New parameter for adaptive scaling\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            positions += velocities\n            \n            if eval_count % (self.budget // 5) == 0:  # Adaptive adjustment\n                median_score = np.median(scores)\n                self.inertia_weight *= self.adaptive_scaling if median_score > global_best_score else 1.0 / self.adaptive_scaling\n                top_elite_index = np.argmin(scores)\n                elite_position = positions[top_elite_index]\n                positions = (positions + elite_position) / 2\n            \n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n\n        return global_best_score, global_best_position\n\nThe algorithm StochasticAdaptiveParticleOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.31 with standard deviation 0.29.\n\n\nPlease transform this optimization algorithm code by exactly 20.0% in a \nway that enhances convergence speed. Keep the code difference precisely at \n20.0%, and focus solely on achieving performance improvements through \nalgorithmic adjustments across various elements of the code. Avoid focusing on \ncode efficiency; instead, prioritize exploration of diverse approaches within \nthe allowed modification percentage.\n\n\nProvide the Python code and a one-line description with the main idea (without enters). Give the response in the format:\n# Description: <short-description>\n# Code: \n# ```python\n# <code>\n# ```\n# \n"}
{"role": "gpt-4o", "time": "2024-11-09 16:08:16.350812", "content": "# Description: A modified particle optimization algorithm with enhanced convergence using dynamic inertia, mutation strategies, and elite utilization.\n# Code: \n```python\nimport numpy as np\n\nclass EnhancedParticleOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(30, self.budget // 2)  # Increase population size\n        self.inertia_weight = 0.9  # Start with higher inertia for exploration\n        self.cognitive_coeff = 1.5  # Adjusted to reduce personal bias\n        self.social_coeff = 1.7  # Slightly increased for global influence\n        self.adaptive_scaling = 0.8  # More aggressive scaling\n        self.mutation_prob = 0.1  # Introduce mutation probability\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            positions += velocities\n            \n            mutation_mask = np.random.rand(self.population_size, self.dim) < self.mutation_prob\n            random_mutation = np.random.uniform(-1, 1, (self.population_size, self.dim))\n            positions = np.where(mutation_mask, positions + random_mutation, positions)\n            \n            if eval_count % (self.budget // 5) == 0:  # Adjust inertia dynamically\n                median_score = np.median(scores)\n                self.inertia_weight *= self.adaptive_scaling if median_score > global_best_score else 1.1\n                elite_position = positions[np.argmin(scores)]\n                positions = (positions + 2 * elite_position) / 3  # Refined elite influence\n            \n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n\n        return global_best_score, global_best_position\n```"}
{"role": "LLaMEA", "time": "2024-11-09 16:09:17.543780", "content": "You are a highly skilled computer scientist in the field of natural computing. Your task is to design novel metaheuristic algorithms to solve black box optimization problems.\n\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the BBOB test suite of 24 noiseless functions. Your task is to write the optimization algorithm in Python code. The code should contain an `__init__(self, budget, dim)` function and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. Each of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nGive an excellent and novel heuristic algorithm to solve this task and also give it a one-line description with the main idea.\n\nThe current population of algorithms already evaluated (name, description, score) is:\nStochasticAdaptiveParticleOptimizer: A stochastic adaptive particle optimization method that dynamically adjusts learning parameters and uses an elite-guided search strategy to enhance exploration and exploitation. (Score: 0.3090060317450278)\n\nThe selected solution to update is:\nA stochastic adaptive particle optimization method that dynamically adjusts learning parameters and uses an elite-guided search strategy to enhance exploration and exploitation.\n\nWith code:\nimport numpy as np\n\nclass StochasticAdaptiveParticleOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(25, self.budget // 2)\n        self.inertia_weight = 0.6  # Reduced for faster convergence\n        self.cognitive_coeff = 2.0  # Increased to emphasize personal best\n        self.social_coeff = 1.5\n        self.adaptive_scaling = 0.9  # New parameter for adaptive scaling\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            positions += velocities\n            \n            if eval_count % (self.budget // 5) == 0:  # Adaptive adjustment\n                median_score = np.median(scores)\n                self.inertia_weight *= self.adaptive_scaling if median_score > global_best_score else 1.0 / self.adaptive_scaling\n                top_elite_index = np.argmin(scores)\n                elite_position = positions[top_elite_index]\n                positions = (positions + elite_position) / 2\n            \n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n\n        return global_best_score, global_best_position\n\nThe algorithm StochasticAdaptiveParticleOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.31 with standard deviation 0.29.\n\n\nPlease transform this optimization algorithm code by exactly 20.0% in a \nway that enhances convergence speed. Keep the code difference precisely at \n20.0%, and focus solely on achieving performance improvements through \nalgorithmic adjustments across various elements of the code. Avoid focusing on \ncode efficiency; instead, prioritize exploration of diverse approaches within \nthe allowed modification percentage.\n\n\nProvide the Python code and a one-line description with the main idea (without enters). Give the response in the format:\n# Description: <short-description>\n# Code: \n# ```python\n# <code>\n# ```\n# \n"}
{"role": "gpt-4o", "time": "2024-11-09 16:09:29.791387", "content": "# Description: A hybrid particle optimization approach integrating mutation-based diversification and adaptive velocity scaling to enhance convergence speed.\n# Code:\n# ```python\nimport numpy as np\n\nclass HybridParticleOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(20, self.budget // 3)  # Slightly adjusted population\n        self.inertia_weight = 0.5  # Adjusted for balance\n        self.cognitive_coeff = 1.8  # Slightly reduced for stability\n        self.social_coeff = 1.7\n        self.adaptive_scaling = 0.85  # More conservative scaling\n        self.mutation_prob = 0.1  # Introduced mutation probability\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-0.5, 0.5, (self.population_size, self.dim))  # Reduced velocity range\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            \n            # Mutation for diversification\n            mutation_mask = np.random.rand(self.population_size, self.dim) < self.mutation_prob\n            mutation_values = np.random.normal(0, 0.1, (self.population_size, self.dim))\n            velocities = np.where(mutation_mask, velocities + mutation_values, velocities)\n            \n            positions += velocities\n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            # Adaptive velocity scaling\n            if eval_count % (self.budget // 4) == 0:\n                median_score = np.median(scores)\n                self.inertia_weight *= self.adaptive_scaling if median_score > global_best_score else 1.0 / self.adaptive_scaling\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n\n        return global_best_score, global_best_position\n# ```"}
{"role": "LLaMEA", "time": "2024-11-09 16:09:29.794927", "content": "You are a highly skilled computer scientist in the field of natural computing. Your task is to design novel metaheuristic algorithms to solve black box optimization problems.\n\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the BBOB test suite of 24 noiseless functions. Your task is to write the optimization algorithm in Python code. The code should contain an `__init__(self, budget, dim)` function and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. Each of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nGive an excellent and novel heuristic algorithm to solve this task and also give it a one-line description with the main idea.\n\nThe current population of algorithms already evaluated (name, description, score) is:\nStochasticAdaptiveParticleOptimizer: A stochastic adaptive particle optimization method that dynamically adjusts learning parameters and uses an elite-guided search strategy to enhance exploration and exploitation. (Score: 0.3090060317450278)\n\nThe selected solution to update is:\nA stochastic adaptive particle optimization method that dynamically adjusts learning parameters and uses an elite-guided search strategy to enhance exploration and exploitation.\n\nWith code:\nimport numpy as np\n\nclass StochasticAdaptiveParticleOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(25, self.budget // 2)\n        self.inertia_weight = 0.6  # Reduced for faster convergence\n        self.cognitive_coeff = 2.0  # Increased to emphasize personal best\n        self.social_coeff = 1.5\n        self.adaptive_scaling = 0.9  # New parameter for adaptive scaling\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            positions += velocities\n            \n            if eval_count % (self.budget // 5) == 0:  # Adaptive adjustment\n                median_score = np.median(scores)\n                self.inertia_weight *= self.adaptive_scaling if median_score > global_best_score else 1.0 / self.adaptive_scaling\n                top_elite_index = np.argmin(scores)\n                elite_position = positions[top_elite_index]\n                positions = (positions + elite_position) / 2\n            \n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n\n        return global_best_score, global_best_position\n\nThe algorithm StochasticAdaptiveParticleOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.31 with standard deviation 0.29.\n\n\nPlease transform this optimization algorithm code by exactly 20.0% in a \nway that enhances convergence speed. Keep the code difference precisely at \n20.0%, and focus solely on achieving performance improvements through \nalgorithmic adjustments across various elements of the code. Avoid focusing on \ncode efficiency; instead, prioritize exploration of diverse approaches within \nthe allowed modification percentage.\n\n\nProvide the Python code and a one-line description with the main idea (without enters). Give the response in the format:\n# Description: <short-description>\n# Code: \n# ```python\n# <code>\n# ```\n# \n"}
{"role": "gpt-4o", "time": "2024-11-09 16:09:42.343154", "content": "\n# Description: An enhanced stochastic adaptive particle optimizer with diversified velocity update and strategic perturbations to improve convergence speed.\n# Code: \n```python\nimport numpy as np\n\nclass EnhancedStochasticAdaptiveParticleOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(30, self.budget // 2)  # Slight increase in population size\n        self.inertia_weight = 0.5  # Further reduced for faster convergence\n        self.cognitive_coeff = 1.8  # Slightly reduced for balance\n        self.social_coeff = 1.7  # Slightly increased for enhanced social influence\n        self.adaptive_scaling = 0.85  # Adjusted for better scaling dynamics\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-0.5, 0.5, (self.population_size, self.dim))  # Narrower velocity range\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            r3 = np.random.rand(self.population_size, self.dim)  # Additional random component for perturbation\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            perturbation = 0.1 * r3 * (np.random.randn(*positions.shape))  # Strategic noise\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component + perturbation\n            positions += velocities\n            \n            if eval_count % (self.budget // 4) == 0:  # More frequent adaptive adjustment\n                median_score = np.median(scores)\n                self.inertia_weight *= self.adaptive_scaling if median_score > global_best_score else 1.0 / self.adaptive_scaling\n                top_elite_index = np.argmin(scores)\n                elite_position = positions[top_elite_index]\n                positions = (positions + elite_position) / 2\n            \n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n\n        return global_best_score, global_best_position\n```"}
{"role": "LLaMEA", "time": "2024-11-09 16:10:41.322121", "content": "You are a highly skilled computer scientist in the field of natural computing. Your task is to design novel metaheuristic algorithms to solve black box optimization problems.\n\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the BBOB test suite of 24 noiseless functions. Your task is to write the optimization algorithm in Python code. The code should contain an `__init__(self, budget, dim)` function and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. Each of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nGive an excellent and novel heuristic algorithm to solve this task and also give it a one-line description with the main idea.\n\nThe current population of algorithms already evaluated (name, description, score) is:\nStochasticAdaptiveParticleOptimizer: A stochastic adaptive particle optimization method that dynamically adjusts learning parameters and uses an elite-guided search strategy to enhance exploration and exploitation. (Score: 0.3090060317450278)\n\nThe selected solution to update is:\nA stochastic adaptive particle optimization method that dynamically adjusts learning parameters and uses an elite-guided search strategy to enhance exploration and exploitation.\n\nWith code:\nimport numpy as np\n\nclass StochasticAdaptiveParticleOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(25, self.budget // 2)\n        self.inertia_weight = 0.6  # Reduced for faster convergence\n        self.cognitive_coeff = 2.0  # Increased to emphasize personal best\n        self.social_coeff = 1.5\n        self.adaptive_scaling = 0.9  # New parameter for adaptive scaling\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            positions += velocities\n            \n            if eval_count % (self.budget // 5) == 0:  # Adaptive adjustment\n                median_score = np.median(scores)\n                self.inertia_weight *= self.adaptive_scaling if median_score > global_best_score else 1.0 / self.adaptive_scaling\n                top_elite_index = np.argmin(scores)\n                elite_position = positions[top_elite_index]\n                positions = (positions + elite_position) / 2\n            \n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n\n        return global_best_score, global_best_position\n\nThe algorithm StochasticAdaptiveParticleOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.31 with standard deviation 0.29.\n\n\nPlease transform this optimization algorithm code by exactly 20.0% in a \nway that enhances convergence speed. Keep the code difference precisely at \n20.0%, and focus solely on achieving performance improvements through \nalgorithmic adjustments across various elements of the code. Avoid focusing on \ncode efficiency; instead, prioritize exploration of diverse approaches within \nthe allowed modification percentage.\n\n\nProvide the Python code and a one-line description with the main idea (without enters). Give the response in the format:\n# Description: <short-description>\n# Code: \n# ```python\n# <code>\n# ```\n# \n"}
{"role": "gpt-4o", "time": "2024-11-09 16:10:50.573698", "content": "# Description: A modified adaptive particle optimizer that enhances convergence by introducing a variable inertia weight, enhanced swarm diversity, and dynamic velocity scaling.\n# Code: \n```python\nimport numpy as np\n\nclass EnhancedAdaptiveParticleOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(30, self.budget // 3)  # Slightly increased for diversity\n        self.initial_inertia_weight = 0.7  # Start with a higher inertia weight\n        self.final_inertia_weight = 0.4\n        self.cognitive_coeff = 1.8  # Adjusted for better personal exploration\n        self.social_coeff = 1.6\n        self.dynamic_scaling = 0.95  # Introduced dynamic scaling for velocities\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        while eval_count < self.budget:\n            inertia_weight = (self.final_inertia_weight + \n                              (self.initial_inertia_weight - self.final_inertia_weight) * \n                              (self.budget - eval_count) / self.budget)\n            \n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = inertia_weight * velocities + cognitive_component + social_component\n            velocities *= self.dynamic_scaling  # Dynamic velocity scaling\n            positions += velocities\n            \n            if eval_count % (self.budget // 4) == 0:  # More frequent adaptive adjustment\n                median_score = np.median(scores)\n                self.dynamic_scaling *= 0.98 if median_score > global_best_score else 1.02\n                top_elite_index = np.argmin(scores)\n                elite_position = positions[top_elite_index]\n                positions = positions + (elite_position - positions) * 0.5\n            \n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n\n        return global_best_score, global_best_position\n```"}
{"role": "LLaMEA", "time": "2024-11-09 16:11:43.516143", "content": "You are a highly skilled computer scientist in the field of natural computing. Your task is to design novel metaheuristic algorithms to solve black box optimization problems.\n\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the BBOB test suite of 24 noiseless functions. Your task is to write the optimization algorithm in Python code. The code should contain an `__init__(self, budget, dim)` function and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. Each of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nGive an excellent and novel heuristic algorithm to solve this task and also give it a one-line description with the main idea.\n\nThe current population of algorithms already evaluated (name, description, score) is:\nStochasticAdaptiveParticleOptimizer: A stochastic adaptive particle optimization method that dynamically adjusts learning parameters and uses an elite-guided search strategy to enhance exploration and exploitation. (Score: 0.3090060317450278)\n\nThe selected solution to update is:\nA stochastic adaptive particle optimization method that dynamically adjusts learning parameters and uses an elite-guided search strategy to enhance exploration and exploitation.\n\nWith code:\nimport numpy as np\n\nclass StochasticAdaptiveParticleOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(25, self.budget // 2)\n        self.inertia_weight = 0.6  # Reduced for faster convergence\n        self.cognitive_coeff = 2.0  # Increased to emphasize personal best\n        self.social_coeff = 1.5\n        self.adaptive_scaling = 0.9  # New parameter for adaptive scaling\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            positions += velocities\n            \n            if eval_count % (self.budget // 5) == 0:  # Adaptive adjustment\n                median_score = np.median(scores)\n                self.inertia_weight *= self.adaptive_scaling if median_score > global_best_score else 1.0 / self.adaptive_scaling\n                top_elite_index = np.argmin(scores)\n                elite_position = positions[top_elite_index]\n                positions = (positions + elite_position) / 2\n            \n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n\n        return global_best_score, global_best_position\n\nThe algorithm StochasticAdaptiveParticleOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.31 with standard deviation 0.29.\n\n\nPlease transform this optimization algorithm code by exactly 20.0% in a \nway that enhances convergence speed. Keep the code difference precisely at \n20.0%, and focus solely on achieving performance improvements through \nalgorithmic adjustments across various elements of the code. Avoid focusing on \ncode efficiency; instead, prioritize exploration of diverse approaches within \nthe allowed modification percentage.\n\n\nProvide the Python code and a one-line description with the main idea (without enters). Give the response in the format:\n# Description: <short-description>\n# Code: \n# ```python\n# <code>\n# ```\n# \n"}
{"role": "gpt-4o", "time": "2024-11-09 16:11:54.998271", "content": "# Description: An enhanced particle optimization method utilizing dynamic inertia adaptation, velocity clamping, and a mutation-like perturbation strategy to improve convergence speed.\n# Code: \n# ```python\nimport numpy as np\n\nclass EnhancedParticleOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(30, self.budget // 2)  # Increased population size\n        self.inertia_weight = 0.7  # Initially larger inertia weight\n        self.cognitive_coeff = 1.8  # Slightly reduced cognitive component\n        self.social_coeff = 1.6  # Increased social component\n        self.min_inertia_weight = 0.4  # New parameter for minimum inertia\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-0.5, 0.5, (self.population_size, self.dim))  # Reduced initial velocity\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            velocities = np.clip(velocities, -2, 2)  # Velocity clamping\n            \n            positions += velocities\n            mutation_chance = 0.1  # Mutation-like perturbation\n            mutation_mask = np.random.rand(self.population_size, self.dim) < mutation_chance\n            mutation_values = np.random.uniform(-0.1, 0.1, (self.population_size, self.dim))\n            positions += mutation_mask * mutation_values\n\n            if eval_count % (self.budget // 5) == 0:\n                self.inertia_weight = max(self.min_inertia_weight, self.inertia_weight * 0.9)  # Dynamic inertia adaptation\n            \n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n\n        return global_best_score, global_best_position\n# ```"}
{"role": "LLaMEA", "time": "2024-11-09 16:11:55.001861", "content": "You are a highly skilled computer scientist in the field of natural computing. Your task is to design novel metaheuristic algorithms to solve black box optimization problems.\n\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the BBOB test suite of 24 noiseless functions. Your task is to write the optimization algorithm in Python code. The code should contain an `__init__(self, budget, dim)` function and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. Each of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nGive an excellent and novel heuristic algorithm to solve this task and also give it a one-line description with the main idea.\n\nThe current population of algorithms already evaluated (name, description, score) is:\nStochasticAdaptiveParticleOptimizer: A stochastic adaptive particle optimization method that dynamically adjusts learning parameters and uses an elite-guided search strategy to enhance exploration and exploitation. (Score: 0.3090060317450278)\n\nThe selected solution to update is:\nA stochastic adaptive particle optimization method that dynamically adjusts learning parameters and uses an elite-guided search strategy to enhance exploration and exploitation.\n\nWith code:\nimport numpy as np\n\nclass StochasticAdaptiveParticleOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(25, self.budget // 2)\n        self.inertia_weight = 0.6  # Reduced for faster convergence\n        self.cognitive_coeff = 2.0  # Increased to emphasize personal best\n        self.social_coeff = 1.5\n        self.adaptive_scaling = 0.9  # New parameter for adaptive scaling\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            positions += velocities\n            \n            if eval_count % (self.budget // 5) == 0:  # Adaptive adjustment\n                median_score = np.median(scores)\n                self.inertia_weight *= self.adaptive_scaling if median_score > global_best_score else 1.0 / self.adaptive_scaling\n                top_elite_index = np.argmin(scores)\n                elite_position = positions[top_elite_index]\n                positions = (positions + elite_position) / 2\n            \n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n\n        return global_best_score, global_best_position\n\nThe algorithm StochasticAdaptiveParticleOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.31 with standard deviation 0.29.\n\n\nPlease transform this optimization algorithm code by exactly 20.0% in a \nway that enhances convergence speed. Keep the code difference precisely at \n20.0%, and focus solely on achieving performance improvements through \nalgorithmic adjustments across various elements of the code. Avoid focusing on \ncode efficiency; instead, prioritize exploration of diverse approaches within \nthe allowed modification percentage.\n\n\nProvide the Python code and a one-line description with the main idea (without enters). Give the response in the format:\n# Description: <short-description>\n# Code: \n# ```python\n# <code>\n# ```\n# \n"}
{"role": "gpt-4o", "time": "2024-11-09 16:12:04.945990", "content": "# Description: An enhanced stochastic particle optimizer incorporating nonlinear inertia decay and dynamic learning rate scaling to improve convergence speed.\n# Code:\n# ```python\nimport numpy as np\n\nclass EnhancedParticleOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(30, self.budget // 3)  # Adjusted population size\n        self.inertia_weight = 0.9  # Initial higher inertia for exploration\n        self.cognitive_coeff = 1.8  # Slightly reduced cognitive component\n        self.social_coeff = 1.7  # Slightly increased social component\n        self.inertia_decay = 0.95  # Nonlinear inertia decay factor\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-0.5, 0.5, (self.population_size, self.dim))  # Reduced initial velocity range\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            positions += velocities\n\n            if eval_count % (self.budget // 10) == 0:  # More frequent adaptive adjustment\n                self.inertia_weight *= self.inertia_decay  # Apply inertia decay\n                elite_position = positions[np.argmin(scores)]\n                positions = (positions * 0.8 + elite_position * 0.2)  # Reduced influence of elite\n\n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n\n        return global_best_score, global_best_position\n# ```"}
{"role": "LLaMEA", "time": "2024-11-09 16:12:04.949208", "content": "You are a highly skilled computer scientist in the field of natural computing. Your task is to design novel metaheuristic algorithms to solve black box optimization problems.\n\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the BBOB test suite of 24 noiseless functions. Your task is to write the optimization algorithm in Python code. The code should contain an `__init__(self, budget, dim)` function and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. Each of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nGive an excellent and novel heuristic algorithm to solve this task and also give it a one-line description with the main idea.\n\nThe current population of algorithms already evaluated (name, description, score) is:\nStochasticAdaptiveParticleOptimizer: A stochastic adaptive particle optimization method that dynamically adjusts learning parameters and uses an elite-guided search strategy to enhance exploration and exploitation. (Score: 0.3090060317450278)\n\nThe selected solution to update is:\nA stochastic adaptive particle optimization method that dynamically adjusts learning parameters and uses an elite-guided search strategy to enhance exploration and exploitation.\n\nWith code:\nimport numpy as np\n\nclass StochasticAdaptiveParticleOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(25, self.budget // 2)\n        self.inertia_weight = 0.6  # Reduced for faster convergence\n        self.cognitive_coeff = 2.0  # Increased to emphasize personal best\n        self.social_coeff = 1.5\n        self.adaptive_scaling = 0.9  # New parameter for adaptive scaling\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            positions += velocities\n            \n            if eval_count % (self.budget // 5) == 0:  # Adaptive adjustment\n                median_score = np.median(scores)\n                self.inertia_weight *= self.adaptive_scaling if median_score > global_best_score else 1.0 / self.adaptive_scaling\n                top_elite_index = np.argmin(scores)\n                elite_position = positions[top_elite_index]\n                positions = (positions + elite_position) / 2\n            \n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n\n        return global_best_score, global_best_position\n\nThe algorithm StochasticAdaptiveParticleOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.31 with standard deviation 0.29.\n\n\nPlease transform this optimization algorithm code by exactly 20.0% in a \nway that enhances convergence speed. Keep the code difference precisely at \n20.0%, and focus solely on achieving performance improvements through \nalgorithmic adjustments across various elements of the code. Avoid focusing on \ncode efficiency; instead, prioritize exploration of diverse approaches within \nthe allowed modification percentage.\n\n\nProvide the Python code and a one-line description with the main idea (without enters). Give the response in the format:\n# Description: <short-description>\n# Code: \n# ```python\n# <code>\n# ```\n# \n"}
{"role": "gpt-4o", "time": "2024-11-09 16:12:18.663999", "content": "# Description: A refined adaptive particle optimization leveraging dynamic velocity scaling and enhanced convergence triggers for improved performance.\n# Code: \n# ```python\nimport numpy as np\n\nclass EnhancedAdaptiveParticleOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(30, self.budget // 3)  # Adjusted population size\n        self.inertia_weight = 0.5  # Further reduced to accelerate convergence\n        self.cognitive_coeff = 1.8  # Slightly reduced to balance exploration and exploitation\n        self.social_coeff = 1.7  # Increased to strengthen social influence\n        self.adaptive_scaling = 0.85  # Adjusted adaptive scaling\n        self.dynamic_velocity_scale = 0.9  # New parameter for dynamic velocity scaling\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-0.5, 0.5, (self.population_size, self.dim))  # Reduced initial velocity range\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities * self.dynamic_velocity_scale + cognitive_component + social_component\n            positions += velocities\n            \n            if eval_count % (self.budget // 4) == 0:  # More frequent adaptive adjustment\n                median_score = np.median(scores)\n                self.inertia_weight *= self.adaptive_scaling if median_score > global_best_score else 1.0 / self.adaptive_scaling\n                top_elite_index = np.argmin(scores)\n                elite_position = positions[top_elite_index]\n                positions = (positions + elite_position) / 2\n            \n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n\n        return global_best_score, global_best_position\n# ```"}
{"role": "LLaMEA", "time": "2024-11-09 16:12:18.667232", "content": "You are a highly skilled computer scientist in the field of natural computing. Your task is to design novel metaheuristic algorithms to solve black box optimization problems.\n\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the BBOB test suite of 24 noiseless functions. Your task is to write the optimization algorithm in Python code. The code should contain an `__init__(self, budget, dim)` function and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. Each of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nGive an excellent and novel heuristic algorithm to solve this task and also give it a one-line description with the main idea.\n\nThe current population of algorithms already evaluated (name, description, score) is:\nStochasticAdaptiveParticleOptimizer: A stochastic adaptive particle optimization method that dynamically adjusts learning parameters and uses an elite-guided search strategy to enhance exploration and exploitation. (Score: 0.3090060317450278)\n\nThe selected solution to update is:\nA stochastic adaptive particle optimization method that dynamically adjusts learning parameters and uses an elite-guided search strategy to enhance exploration and exploitation.\n\nWith code:\nimport numpy as np\n\nclass StochasticAdaptiveParticleOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(25, self.budget // 2)\n        self.inertia_weight = 0.6  # Reduced for faster convergence\n        self.cognitive_coeff = 2.0  # Increased to emphasize personal best\n        self.social_coeff = 1.5\n        self.adaptive_scaling = 0.9  # New parameter for adaptive scaling\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            positions += velocities\n            \n            if eval_count % (self.budget // 5) == 0:  # Adaptive adjustment\n                median_score = np.median(scores)\n                self.inertia_weight *= self.adaptive_scaling if median_score > global_best_score else 1.0 / self.adaptive_scaling\n                top_elite_index = np.argmin(scores)\n                elite_position = positions[top_elite_index]\n                positions = (positions + elite_position) / 2\n            \n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n\n        return global_best_score, global_best_position\n\nThe algorithm StochasticAdaptiveParticleOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.31 with standard deviation 0.29.\n\n\nPlease transform this optimization algorithm code by exactly 20.0% in a \nway that enhances convergence speed. Keep the code difference precisely at \n20.0%, and focus solely on achieving performance improvements through \nalgorithmic adjustments across various elements of the code. Avoid focusing on \ncode efficiency; instead, prioritize exploration of diverse approaches within \nthe allowed modification percentage.\n\n\nProvide the Python code and a one-line description with the main idea (without enters). Give the response in the format:\n# Description: <short-description>\n# Code: \n# ```python\n# <code>\n# ```\n# \n"}
{"role": "gpt-4o", "time": "2024-11-09 16:12:27.639309", "content": "# Description: An improved stochastic adaptive particle optimizer with dynamic velocity clamping and perturbation for enhanced convergence speed.\n# Code: \n# ```python\nimport numpy as np\n\nclass StochasticAdaptiveParticleOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(25, self.budget // 2)\n        self.inertia_weight = 0.7  # Adjust inertia to balance exploration/exploitation\n        self.cognitive_coeff = 1.8  # Tweaked coefficients for balanced personal/global search\n        self.social_coeff = 1.7\n        self.adaptive_scaling = 0.85  # Slightly more aggressive adaptive scaling\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-0.5, 0.5, (self.population_size, self.dim))  # Reduced initial velocity range\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        velocity_clamp = (self.upper_bound - self.lower_bound) * 0.1  # Dynamic velocity clamp\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            \n            # Velocity clamping to prevent explosions\n            velocities = np.clip(velocities, -velocity_clamp, velocity_clamp)\n            positions += velocities\n            \n            if eval_count % (self.budget // 5) == 0:  # Adaptive adjustment\n                self.inertia_weight *= self.adaptive_scaling\n                perturbation = np.random.normal(0, 0.1, positions.shape)  # Introduce perturbation\n                positions += perturbation\n            \n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n\n        return global_best_score, global_best_position\n```"}
{"role": "LLaMEA", "time": "2024-11-09 16:13:26.837866", "content": "You are a highly skilled computer scientist in the field of natural computing. Your task is to design novel metaheuristic algorithms to solve black box optimization problems.\n\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the BBOB test suite of 24 noiseless functions. Your task is to write the optimization algorithm in Python code. The code should contain an `__init__(self, budget, dim)` function and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. Each of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nGive an excellent and novel heuristic algorithm to solve this task and also give it a one-line description with the main idea.\n\nThe current population of algorithms already evaluated (name, description, score) is:\nStochasticAdaptiveParticleOptimizer: An improved stochastic adaptive particle optimizer with dynamic velocity clamping and perturbation for enhanced convergence speed. (Score: 0.3304703293497984)\n\nThe selected solution to update is:\nAn improved stochastic adaptive particle optimizer with dynamic velocity clamping and perturbation for enhanced convergence speed.\n\nWith code:\nimport numpy as np\n\nclass StochasticAdaptiveParticleOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(25, self.budget // 2)\n        self.inertia_weight = 0.7  # Adjust inertia to balance exploration/exploitation\n        self.cognitive_coeff = 1.8  # Tweaked coefficients for balanced personal/global search\n        self.social_coeff = 1.7\n        self.adaptive_scaling = 0.85  # Slightly more aggressive adaptive scaling\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-0.5, 0.5, (self.population_size, self.dim))  # Reduced initial velocity range\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        velocity_clamp = (self.upper_bound - self.lower_bound) * 0.1  # Dynamic velocity clamp\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            \n            # Velocity clamping to prevent explosions\n            velocities = np.clip(velocities, -velocity_clamp, velocity_clamp)\n            positions += velocities\n            \n            if eval_count % (self.budget // 5) == 0:  # Adaptive adjustment\n                self.inertia_weight *= self.adaptive_scaling\n                perturbation = np.random.normal(0, 0.1, positions.shape)  # Introduce perturbation\n                positions += perturbation\n            \n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n\n        return global_best_score, global_best_position\n\nThe algorithm StochasticAdaptiveParticleOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.33 with standard deviation 0.28.\n\n\nPlease transform this optimization algorithm code by exactly 20.0% in a \nway that enhances convergence speed. Keep the code difference precisely at \n20.0%, and focus solely on achieving performance improvements through \nalgorithmic adjustments across various elements of the code. Avoid focusing on \ncode efficiency; instead, prioritize exploration of diverse approaches within \nthe allowed modification percentage.\n\n\nProvide the Python code and a one-line description with the main idea (without enters). Give the response in the format:\n# Description: <short-description>\n# Code: \n# ```python\n# <code>\n# ```\n# \n"}
{"role": "gpt-4o", "time": "2024-11-09 16:13:36.891993", "content": "# Description: A refined particle swarm optimizer with adaptive neighborhood topologies and oscillatory inertia for improved convergence.\n# Code: \n# ```python\nimport numpy as np\n\nclass RefinedParticleSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(30, self.budget // 2)  # Increased population size for better exploration\n        self.inertia_weight = 0.9  # Initial high inertia for wide exploration\n        self.cognitive_coeff = 1.5  # Adjusted cognitive coefficient for quicker convergence\n        self.social_coeff = 1.8\n        self.oscillation_factor = 0.5  # Introduce oscillation in inertia for adaptive behavior\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-0.3, 0.3, (self.population_size, self.dim))  # Tweaked initial velocity range\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        velocity_clamp = (self.upper_bound - self.lower_bound) * 0.15  # Increase velocity clamp\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            \n            velocities = np.clip(velocities, -velocity_clamp, velocity_clamp)\n            positions += velocities\n            \n            # Oscillatory inertia implementation\n            if eval_count % (self.budget // 4) == 0:\n                self.inertia_weight *= -self.oscillation_factor if eval_count % (self.budget // 2) else 1/self.oscillation_factor\n            \n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n\n        return global_best_score, global_best_position\n# ```\n"}
{"role": "LLaMEA", "time": "2024-11-09 16:13:36.895907", "content": "You are a highly skilled computer scientist in the field of natural computing. Your task is to design novel metaheuristic algorithms to solve black box optimization problems.\n\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the BBOB test suite of 24 noiseless functions. Your task is to write the optimization algorithm in Python code. The code should contain an `__init__(self, budget, dim)` function and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. Each of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nGive an excellent and novel heuristic algorithm to solve this task and also give it a one-line description with the main idea.\n\nThe current population of algorithms already evaluated (name, description, score) is:\nStochasticAdaptiveParticleOptimizer: An improved stochastic adaptive particle optimizer with dynamic velocity clamping and perturbation for enhanced convergence speed. (Score: 0.3304703293497984)\n\nThe selected solution to update is:\nAn improved stochastic adaptive particle optimizer with dynamic velocity clamping and perturbation for enhanced convergence speed.\n\nWith code:\nimport numpy as np\n\nclass StochasticAdaptiveParticleOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(25, self.budget // 2)\n        self.inertia_weight = 0.7  # Adjust inertia to balance exploration/exploitation\n        self.cognitive_coeff = 1.8  # Tweaked coefficients for balanced personal/global search\n        self.social_coeff = 1.7\n        self.adaptive_scaling = 0.85  # Slightly more aggressive adaptive scaling\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-0.5, 0.5, (self.population_size, self.dim))  # Reduced initial velocity range\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        velocity_clamp = (self.upper_bound - self.lower_bound) * 0.1  # Dynamic velocity clamp\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            \n            # Velocity clamping to prevent explosions\n            velocities = np.clip(velocities, -velocity_clamp, velocity_clamp)\n            positions += velocities\n            \n            if eval_count % (self.budget // 5) == 0:  # Adaptive adjustment\n                self.inertia_weight *= self.adaptive_scaling\n                perturbation = np.random.normal(0, 0.1, positions.shape)  # Introduce perturbation\n                positions += perturbation\n            \n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n\n        return global_best_score, global_best_position\n\nThe algorithm StochasticAdaptiveParticleOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.33 with standard deviation 0.28.\n\n\nPlease transform this optimization algorithm code by exactly 20.0% in a \nway that enhances convergence speed. Keep the code difference precisely at \n20.0%, and focus solely on achieving performance improvements through \nalgorithmic adjustments across various elements of the code. Avoid focusing on \ncode efficiency; instead, prioritize exploration of diverse approaches within \nthe allowed modification percentage.\n\n\nProvide the Python code and a one-line description with the main idea (without enters). Give the response in the format:\n# Description: <short-description>\n# Code: \n# ```python\n# <code>\n# ```\n# \n"}
{"role": "gpt-4o", "time": "2024-11-09 16:13:46.823053", "content": "# Description: Enhanced stochastic adaptive particle optimizer with adaptive inertia, non-linear convergence pressure, and differential perturbation strategies.\n# Code:\n# ```python\nimport numpy as np\n\nclass EnhancedStochasticAdaptiveParticleOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(30, self.budget // 2)  # Slightly increased population size\n        self.inertia_weight = 0.8  # Increased inertia to promote exploration\n        self.cognitive_coeff = 1.5  # Adjusted to balance search dimensionally\n        self.social_coeff = 1.9  # Increased for stronger social influence\n        self.adaptive_scaling = 0.9  # Modified scaling for gradual adaptation\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-0.3, 0.3, (self.population_size, self.dim))  # Adjusted initial velocity range\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        velocity_clamp = (self.upper_bound - self.lower_bound) * 0.15  # Increased dynamic velocity clamp\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            \n            # Non-linear convergence pressure via sigmoid function\n            velocities = velocities / (1 + np.exp(-np.linalg.norm(velocities, axis=1)).reshape(-1, 1))\n            velocities = np.clip(velocities, -velocity_clamp, velocity_clamp)\n            positions += velocities\n            \n            if eval_count % (self.budget // 4) == 0:  # More frequent adaptive adjustment\n                self.inertia_weight *= self.adaptive_scaling\n                perturbation = np.random.standard_cauchy(positions.shape) * 0.05  # Differential perturbation\n                positions += perturbation\n            \n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n\n        return global_best_score, global_best_position\n# ```"}
{"role": "LLaMEA", "time": "2024-11-09 16:13:46.826664", "content": "You are a highly skilled computer scientist in the field of natural computing. Your task is to design novel metaheuristic algorithms to solve black box optimization problems.\n\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the BBOB test suite of 24 noiseless functions. Your task is to write the optimization algorithm in Python code. The code should contain an `__init__(self, budget, dim)` function and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. Each of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nGive an excellent and novel heuristic algorithm to solve this task and also give it a one-line description with the main idea.\n\nThe current population of algorithms already evaluated (name, description, score) is:\nStochasticAdaptiveParticleOptimizer: An improved stochastic adaptive particle optimizer with dynamic velocity clamping and perturbation for enhanced convergence speed. (Score: 0.3304703293497984)\n\nThe selected solution to update is:\nAn improved stochastic adaptive particle optimizer with dynamic velocity clamping and perturbation for enhanced convergence speed.\n\nWith code:\nimport numpy as np\n\nclass StochasticAdaptiveParticleOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(25, self.budget // 2)\n        self.inertia_weight = 0.7  # Adjust inertia to balance exploration/exploitation\n        self.cognitive_coeff = 1.8  # Tweaked coefficients for balanced personal/global search\n        self.social_coeff = 1.7\n        self.adaptive_scaling = 0.85  # Slightly more aggressive adaptive scaling\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-0.5, 0.5, (self.population_size, self.dim))  # Reduced initial velocity range\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        velocity_clamp = (self.upper_bound - self.lower_bound) * 0.1  # Dynamic velocity clamp\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            \n            # Velocity clamping to prevent explosions\n            velocities = np.clip(velocities, -velocity_clamp, velocity_clamp)\n            positions += velocities\n            \n            if eval_count % (self.budget // 5) == 0:  # Adaptive adjustment\n                self.inertia_weight *= self.adaptive_scaling\n                perturbation = np.random.normal(0, 0.1, positions.shape)  # Introduce perturbation\n                positions += perturbation\n            \n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n\n        return global_best_score, global_best_position\n\nThe algorithm StochasticAdaptiveParticleOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.33 with standard deviation 0.28.\n\n\nPlease transform this optimization algorithm code by exactly 20.0% in a \nway that enhances convergence speed. Keep the code difference precisely at \n20.0%, and focus solely on achieving performance improvements through \nalgorithmic adjustments across various elements of the code. Avoid focusing on \ncode efficiency; instead, prioritize exploration of diverse approaches within \nthe allowed modification percentage.\n\n\nProvide the Python code and a one-line description with the main idea (without enters). Give the response in the format:\n# Description: <short-description>\n# Code: \n# ```python\n# <code>\n# ```\n# \n"}
{"role": "gpt-4o", "time": "2024-11-09 16:13:57.017956", "content": "# Description: An enhanced hybrid particle swarm optimizer with modified inertia and dynamic swarm re-initialization to improve convergence.\n# Code:\n# ```python\nimport numpy as np\n\nclass HybridEnhancedParticleOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(30, self.budget // 2)  # Slight increase in population\n        self.inertia_weight = 0.9  # Start with higher inertia for better exploration\n        self.cognitive_coeff = 1.9  # Slightly increased cognitive coefficient\n        self.social_coeff = 2.0  # Increased social coefficient for stronger convergence\n        self.adaptive_scaling = 0.9\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-0.5, 0.5, (self.population_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        velocity_clamp = (self.upper_bound - self.lower_bound) * 0.2  # Increased velocity clamp\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n\n            velocities = np.clip(velocities, -velocity_clamp, velocity_clamp)\n            positions += velocities\n            \n            if eval_count % (self.budget // 4) == 0:  # More frequent adaptive adjustment\n                self.inertia_weight *= self.adaptive_scaling\n                perturbation = np.random.normal(0, 0.2, positions.shape)  # Larger perturbation\n                positions += perturbation\n                # Dynamic re-initialization of worst half of the swarm\n                worst_indices = np.argsort(scores)[-self.population_size//2:]\n                positions[worst_indices] = np.random.uniform(self.lower_bound, self.upper_bound, (len(worst_indices), self.dim))\n            \n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n\n        return global_best_score, global_best_position\n# ```"}
{"role": "LLaMEA", "time": "2024-11-09 16:13:57.022105", "content": "You are a highly skilled computer scientist in the field of natural computing. Your task is to design novel metaheuristic algorithms to solve black box optimization problems.\n\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the BBOB test suite of 24 noiseless functions. Your task is to write the optimization algorithm in Python code. The code should contain an `__init__(self, budget, dim)` function and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. Each of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nGive an excellent and novel heuristic algorithm to solve this task and also give it a one-line description with the main idea.\n\nThe current population of algorithms already evaluated (name, description, score) is:\nStochasticAdaptiveParticleOptimizer: An improved stochastic adaptive particle optimizer with dynamic velocity clamping and perturbation for enhanced convergence speed. (Score: 0.3304703293497984)\n\nThe selected solution to update is:\nAn improved stochastic adaptive particle optimizer with dynamic velocity clamping and perturbation for enhanced convergence speed.\n\nWith code:\nimport numpy as np\n\nclass StochasticAdaptiveParticleOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(25, self.budget // 2)\n        self.inertia_weight = 0.7  # Adjust inertia to balance exploration/exploitation\n        self.cognitive_coeff = 1.8  # Tweaked coefficients for balanced personal/global search\n        self.social_coeff = 1.7\n        self.adaptive_scaling = 0.85  # Slightly more aggressive adaptive scaling\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-0.5, 0.5, (self.population_size, self.dim))  # Reduced initial velocity range\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        velocity_clamp = (self.upper_bound - self.lower_bound) * 0.1  # Dynamic velocity clamp\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            \n            # Velocity clamping to prevent explosions\n            velocities = np.clip(velocities, -velocity_clamp, velocity_clamp)\n            positions += velocities\n            \n            if eval_count % (self.budget // 5) == 0:  # Adaptive adjustment\n                self.inertia_weight *= self.adaptive_scaling\n                perturbation = np.random.normal(0, 0.1, positions.shape)  # Introduce perturbation\n                positions += perturbation\n            \n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n\n        return global_best_score, global_best_position\n\nThe algorithm StochasticAdaptiveParticleOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.33 with standard deviation 0.28.\n\n\nPlease transform this optimization algorithm code by exactly 20.0% in a \nway that enhances convergence speed. Keep the code difference precisely at \n20.0%, and focus solely on achieving performance improvements through \nalgorithmic adjustments across various elements of the code. Avoid focusing on \ncode efficiency; instead, prioritize exploration of diverse approaches within \nthe allowed modification percentage.\n\n\nProvide the Python code and a one-line description with the main idea (without enters). Give the response in the format:\n# Description: <short-description>\n# Code: \n# ```python\n# <code>\n# ```\n# \n"}
{"role": "gpt-4o", "time": "2024-11-09 16:14:06.700969", "content": "\n# Description: A faster-converging particle optimizer using dynamic inertia adaptation and elite perturbation for accelerated exploration and exploitation balance.\n# Code: \n# ```python\nimport numpy as np\n\nclass EnhancedAdaptiveParticleOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(30, self.budget // 3)  # Increased population size for diversity\n        self.inertia_weight = 0.9  # Start with higher inertia for global exploration\n        self.cognitive_coeff = 1.5  # Adjusted for more balanced search\n        self.social_coeff = 1.8\n        self.adaptive_scaling = 0.9  # Adaptive scaling is now less aggressive\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1.0, 1.0, (self.population_size, self.dim))  # Increased initial velocity range\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        velocity_clamp = (self.upper_bound - self.lower_bound) * 0.15  # Adjusted dynamic velocity clamp\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            \n            # Velocity clamping to prevent explosions\n            velocities = np.clip(velocities, -velocity_clamp, velocity_clamp)\n            positions += velocities\n            \n            if eval_count % (self.budget // 4) == 0:  # Adaptive adjustment\n                self.inertia_weight *= self.adaptive_scaling\n                elite_perturbation = np.random.normal(0, 0.05, (1, self.dim))  # Elite perturbation\n                elite_index = np.argmin(personal_best_scores)\n                positions[elite_index] += elite_perturbation\n            \n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n\n        return global_best_score, global_best_position\n# ```"}
{"role": "LLaMEA", "time": "2024-11-09 16:14:06.705119", "content": "You are a highly skilled computer scientist in the field of natural computing. Your task is to design novel metaheuristic algorithms to solve black box optimization problems.\n\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the BBOB test suite of 24 noiseless functions. Your task is to write the optimization algorithm in Python code. The code should contain an `__init__(self, budget, dim)` function and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. Each of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nGive an excellent and novel heuristic algorithm to solve this task and also give it a one-line description with the main idea.\n\nThe current population of algorithms already evaluated (name, description, score) is:\nStochasticAdaptiveParticleOptimizer: An improved stochastic adaptive particle optimizer with dynamic velocity clamping and perturbation for enhanced convergence speed. (Score: 0.3304703293497984)\n\nThe selected solution to update is:\nAn improved stochastic adaptive particle optimizer with dynamic velocity clamping and perturbation for enhanced convergence speed.\n\nWith code:\nimport numpy as np\n\nclass StochasticAdaptiveParticleOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(25, self.budget // 2)\n        self.inertia_weight = 0.7  # Adjust inertia to balance exploration/exploitation\n        self.cognitive_coeff = 1.8  # Tweaked coefficients for balanced personal/global search\n        self.social_coeff = 1.7\n        self.adaptive_scaling = 0.85  # Slightly more aggressive adaptive scaling\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-0.5, 0.5, (self.population_size, self.dim))  # Reduced initial velocity range\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        velocity_clamp = (self.upper_bound - self.lower_bound) * 0.1  # Dynamic velocity clamp\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            \n            # Velocity clamping to prevent explosions\n            velocities = np.clip(velocities, -velocity_clamp, velocity_clamp)\n            positions += velocities\n            \n            if eval_count % (self.budget // 5) == 0:  # Adaptive adjustment\n                self.inertia_weight *= self.adaptive_scaling\n                perturbation = np.random.normal(0, 0.1, positions.shape)  # Introduce perturbation\n                positions += perturbation\n            \n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n\n        return global_best_score, global_best_position\n\nThe algorithm StochasticAdaptiveParticleOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.33 with standard deviation 0.28.\n\n\nPlease transform this optimization algorithm code by exactly 20.0% in a \nway that enhances convergence speed. Keep the code difference precisely at \n20.0%, and focus solely on achieving performance improvements through \nalgorithmic adjustments across various elements of the code. Avoid focusing on \ncode efficiency; instead, prioritize exploration of diverse approaches within \nthe allowed modification percentage.\n\n\nProvide the Python code and a one-line description with the main idea (without enters). Give the response in the format:\n# Description: <short-description>\n# Code: \n# ```python\n# <code>\n# ```\n# \n"}
{"role": "gpt-4o", "time": "2024-11-09 16:14:17.524723", "content": "# Description: Adaptive Particle Swarm Optimization with Dynamic Inertia and Enhanced Perturbation for Accelerated Convergence.\n# Code:\n# ```python\nimport numpy as np\n\nclass AdaptiveParticleOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(30, self.budget // 2)  # Increased population size for better exploration\n        self.inertia_weight = 0.9  # Dynamic inertia weight starting higher for better initial exploration\n        self.cognitive_coeff = 1.5  # Adjusted to balance exploration and exploitation\n        self.social_coeff = 1.5\n        self.adaptive_scaling = 0.9  # Slightly less aggressive scaling to retain exploration\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-0.3, 0.3, (self.population_size, self.dim))  # Reduced velocity for finer movement\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        velocity_clamp = (self.upper_bound - self.lower_bound) * 0.2  # Increased clamp to allow more movement\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            \n            velocities = np.clip(velocities, -velocity_clamp, velocity_clamp)\n            positions += velocities\n            \n            if eval_count % (self.budget // 4) == 0:  # More frequent adaptive adjustments\n                self.inertia_weight *= self.adaptive_scaling\n                perturbation = np.random.normal(0, 0.15, positions.shape)  # Enhanced perturbation\n                positions += perturbation\n            \n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n\n        return global_best_score, global_best_position\n# ```"}
{"role": "LLaMEA", "time": "2024-11-09 16:14:17.528789", "content": "You are a highly skilled computer scientist in the field of natural computing. Your task is to design novel metaheuristic algorithms to solve black box optimization problems.\n\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the BBOB test suite of 24 noiseless functions. Your task is to write the optimization algorithm in Python code. The code should contain an `__init__(self, budget, dim)` function and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. Each of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nGive an excellent and novel heuristic algorithm to solve this task and also give it a one-line description with the main idea.\n\nThe current population of algorithms already evaluated (name, description, score) is:\nStochasticAdaptiveParticleOptimizer: An improved stochastic adaptive particle optimizer with dynamic velocity clamping and perturbation for enhanced convergence speed. (Score: 0.3304703293497984)\n\nThe selected solution to update is:\nAn improved stochastic adaptive particle optimizer with dynamic velocity clamping and perturbation for enhanced convergence speed.\n\nWith code:\nimport numpy as np\n\nclass StochasticAdaptiveParticleOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(25, self.budget // 2)\n        self.inertia_weight = 0.7  # Adjust inertia to balance exploration/exploitation\n        self.cognitive_coeff = 1.8  # Tweaked coefficients for balanced personal/global search\n        self.social_coeff = 1.7\n        self.adaptive_scaling = 0.85  # Slightly more aggressive adaptive scaling\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-0.5, 0.5, (self.population_size, self.dim))  # Reduced initial velocity range\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        velocity_clamp = (self.upper_bound - self.lower_bound) * 0.1  # Dynamic velocity clamp\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            \n            # Velocity clamping to prevent explosions\n            velocities = np.clip(velocities, -velocity_clamp, velocity_clamp)\n            positions += velocities\n            \n            if eval_count % (self.budget // 5) == 0:  # Adaptive adjustment\n                self.inertia_weight *= self.adaptive_scaling\n                perturbation = np.random.normal(0, 0.1, positions.shape)  # Introduce perturbation\n                positions += perturbation\n            \n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n\n        return global_best_score, global_best_position\n\nThe algorithm StochasticAdaptiveParticleOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.33 with standard deviation 0.28.\n\n\nPlease transform this optimization algorithm code by exactly 20.0% in a \nway that enhances convergence speed. Keep the code difference precisely at \n20.0%, and focus solely on achieving performance improvements through \nalgorithmic adjustments across various elements of the code. Avoid focusing on \ncode efficiency; instead, prioritize exploration of diverse approaches within \nthe allowed modification percentage.\n\n\nProvide the Python code and a one-line description with the main idea (without enters). Give the response in the format:\n# Description: <short-description>\n# Code: \n# ```python\n# <code>\n# ```\n# \n"}
{"role": "gpt-4o", "time": "2024-11-09 16:14:27.939268", "content": "# Description: Enhanced StochasticAdaptiveParticleOptimizer with scale-invariant self-adaptive control and dynamic swarm intelligence.\n# Code:\n# ```python\nimport numpy as np\n\nclass EnhancedStochasticAdaptiveParticleOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(30, self.budget // 3)\n        self.initial_inertia = 0.9\n        self.final_inertia = 0.4\n        self.cognitive_coeff = 1.5\n        self.social_coeff = 1.9\n        self.adaptive_scaling = 0.9\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-0.3, 0.3, (self.population_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        velocity_clamp = (self.upper_bound - self.lower_bound) * 0.2\n\n        while eval_count < self.budget:\n            inertia = self.final_inertia + (self.initial_inertia - self.final_inertia) * ((self.budget - eval_count) / self.budget)\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = inertia * velocities + cognitive_component + social_component\n            \n            velocities = np.clip(velocities, -velocity_clamp, velocity_clamp)\n            positions += velocities\n            \n            if eval_count % (self.budget // 4) == 0:\n                perturbation = np.random.normal(0, 0.05, positions.shape)\n                positions += perturbation\n            \n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n\n        return global_best_score, global_best_position\n# ```"}
{"role": "LLaMEA", "time": "2024-11-09 16:14:27.943364", "content": "You are a highly skilled computer scientist in the field of natural computing. Your task is to design novel metaheuristic algorithms to solve black box optimization problems.\n\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the BBOB test suite of 24 noiseless functions. Your task is to write the optimization algorithm in Python code. The code should contain an `__init__(self, budget, dim)` function and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. Each of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nGive an excellent and novel heuristic algorithm to solve this task and also give it a one-line description with the main idea.\n\nThe current population of algorithms already evaluated (name, description, score) is:\nStochasticAdaptiveParticleOptimizer: An improved stochastic adaptive particle optimizer with dynamic velocity clamping and perturbation for enhanced convergence speed. (Score: 0.3304703293497984)\n\nThe selected solution to update is:\nAn improved stochastic adaptive particle optimizer with dynamic velocity clamping and perturbation for enhanced convergence speed.\n\nWith code:\nimport numpy as np\n\nclass StochasticAdaptiveParticleOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(25, self.budget // 2)\n        self.inertia_weight = 0.7  # Adjust inertia to balance exploration/exploitation\n        self.cognitive_coeff = 1.8  # Tweaked coefficients for balanced personal/global search\n        self.social_coeff = 1.7\n        self.adaptive_scaling = 0.85  # Slightly more aggressive adaptive scaling\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-0.5, 0.5, (self.population_size, self.dim))  # Reduced initial velocity range\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        velocity_clamp = (self.upper_bound - self.lower_bound) * 0.1  # Dynamic velocity clamp\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            \n            # Velocity clamping to prevent explosions\n            velocities = np.clip(velocities, -velocity_clamp, velocity_clamp)\n            positions += velocities\n            \n            if eval_count % (self.budget // 5) == 0:  # Adaptive adjustment\n                self.inertia_weight *= self.adaptive_scaling\n                perturbation = np.random.normal(0, 0.1, positions.shape)  # Introduce perturbation\n                positions += perturbation\n            \n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n\n        return global_best_score, global_best_position\n\nThe algorithm StochasticAdaptiveParticleOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.33 with standard deviation 0.28.\n\n\nPlease transform this optimization algorithm code by exactly 20.0% in a \nway that enhances convergence speed. Keep the code difference precisely at \n20.0%, and focus solely on achieving performance improvements through \nalgorithmic adjustments across various elements of the code. Avoid focusing on \ncode efficiency; instead, prioritize exploration of diverse approaches within \nthe allowed modification percentage.\n\n\nProvide the Python code and a one-line description with the main idea (without enters). Give the response in the format:\n# Description: <short-description>\n# Code: \n# ```python\n# <code>\n# ```\n# \n"}
{"role": "gpt-4o", "time": "2024-11-09 16:14:37.156940", "content": "\n# Description: An enhanced stochastic adaptive particle optimizer leveraging a convergence-focused dynamic memory mechanism for improved solution quality.\n# Code: \n# ```python\nimport numpy as np\n\nclass StochasticAdaptiveParticleOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(30, self.budget // 2)  # Slight increase in population size for diversity\n        self.inertia_weight = 0.8  # Slightly higher inertia for initial exploration\n        self.cognitive_coeff = 2.0  # Increased cognitive coefficient for enhanced personal learning\n        self.social_coeff = 1.5  # Reduced social coefficient to moderate global attraction\n        self.adaptive_scaling = 0.9  # Adjusted adaptive scaling for stability\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-0.3, 0.3, (self.population_size, self.dim))  # Altered initial velocity range\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n\n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n\n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n\n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        velocity_clamp = (self.upper_bound - self.lower_bound) * 0.15  # Adjusted velocity clamp\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            \n            velocities = np.clip(velocities, -velocity_clamp, velocity_clamp)\n            positions += velocities\n            \n            if eval_count % (self.budget // 4) == 0:  # More frequent adaptive adjustment\n                self.inertia_weight *= self.adaptive_scaling\n                perturbation = np.random.normal(0, 0.05, positions.shape)  # Subtle perturbation for fine-tuning\n                positions += perturbation\n            \n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n\n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n\n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n\n        return global_best_score, global_best_position\n# ```"}
{"role": "LLaMEA", "time": "2024-11-09 16:14:37.161161", "content": "You are a highly skilled computer scientist in the field of natural computing. Your task is to design novel metaheuristic algorithms to solve black box optimization problems.\n\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the BBOB test suite of 24 noiseless functions. Your task is to write the optimization algorithm in Python code. The code should contain an `__init__(self, budget, dim)` function and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. Each of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nGive an excellent and novel heuristic algorithm to solve this task and also give it a one-line description with the main idea.\n\nThe current population of algorithms already evaluated (name, description, score) is:\nStochasticAdaptiveParticleOptimizer: An improved stochastic adaptive particle optimizer with dynamic velocity clamping and perturbation for enhanced convergence speed. (Score: 0.3304703293497984)\n\nThe selected solution to update is:\nAn improved stochastic adaptive particle optimizer with dynamic velocity clamping and perturbation for enhanced convergence speed.\n\nWith code:\nimport numpy as np\n\nclass StochasticAdaptiveParticleOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(25, self.budget // 2)\n        self.inertia_weight = 0.7  # Adjust inertia to balance exploration/exploitation\n        self.cognitive_coeff = 1.8  # Tweaked coefficients for balanced personal/global search\n        self.social_coeff = 1.7\n        self.adaptive_scaling = 0.85  # Slightly more aggressive adaptive scaling\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-0.5, 0.5, (self.population_size, self.dim))  # Reduced initial velocity range\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        velocity_clamp = (self.upper_bound - self.lower_bound) * 0.1  # Dynamic velocity clamp\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            \n            # Velocity clamping to prevent explosions\n            velocities = np.clip(velocities, -velocity_clamp, velocity_clamp)\n            positions += velocities\n            \n            if eval_count % (self.budget // 5) == 0:  # Adaptive adjustment\n                self.inertia_weight *= self.adaptive_scaling\n                perturbation = np.random.normal(0, 0.1, positions.shape)  # Introduce perturbation\n                positions += perturbation\n            \n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n\n        return global_best_score, global_best_position\n\nThe algorithm StochasticAdaptiveParticleOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.33 with standard deviation 0.28.\n\n\nPlease transform this optimization algorithm code by exactly 20.0% in a \nway that enhances convergence speed. Keep the code difference precisely at \n20.0%, and focus solely on achieving performance improvements through \nalgorithmic adjustments across various elements of the code. Avoid focusing on \ncode efficiency; instead, prioritize exploration of diverse approaches within \nthe allowed modification percentage.\n\n\nProvide the Python code and a one-line description with the main idea (without enters). Give the response in the format:\n# Description: <short-description>\n# Code: \n# ```python\n# <code>\n# ```\n# \n"}
{"role": "gpt-4o", "time": "2024-11-09 16:14:46.578919", "content": "# Description: An enhanced stochastic adaptive particle optimizer integrating dynamic momentum and individual perturbation for improved convergence speed.\n# Code:\n# ```python\nimport numpy as np\n\nclass EnhancedStochasticAdaptiveParticleOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(30, self.budget // 2)  # Slightly increased population size\n        self.inertia_weight = 0.6  # Reduced inertia for quicker adaptation\n        self.cognitive_coeff = 2.0  # Enhanced cognitive influence for personal best\n        self.social_coeff = 1.5  # Reduced social influence to balance exploration\n        self.adaptive_scaling = 0.9  # Adjusted to maintain exploration longer\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-0.3, 0.3, (self.population_size, self.dim))  # Altered initial velocity range\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        velocity_clamp = (self.upper_bound - self.lower_bound) * 0.15  # Modified velocity clamp\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            \n            velocities = np.clip(velocities, -velocity_clamp, velocity_clamp)\n            positions += velocities\n            \n            if eval_count % (self.budget // 4) == 0:  # Dynamic momentum adjustment\n                self.inertia_weight *= self.adaptive_scaling\n                perturbation = np.random.uniform(-0.1, 0.1, positions.shape)  # Individual perturbation\n                positions += perturbation\n            \n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n\n        return global_best_score, global_best_position\n# ```"}
{"role": "LLaMEA", "time": "2024-11-09 16:14:46.583031", "content": "You are a highly skilled computer scientist in the field of natural computing. Your task is to design novel metaheuristic algorithms to solve black box optimization problems.\n\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the BBOB test suite of 24 noiseless functions. Your task is to write the optimization algorithm in Python code. The code should contain an `__init__(self, budget, dim)` function and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. Each of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nGive an excellent and novel heuristic algorithm to solve this task and also give it a one-line description with the main idea.\n\nThe current population of algorithms already evaluated (name, description, score) is:\nStochasticAdaptiveParticleOptimizer: An improved stochastic adaptive particle optimizer with dynamic velocity clamping and perturbation for enhanced convergence speed. (Score: 0.3304703293497984)\n\nThe selected solution to update is:\nAn improved stochastic adaptive particle optimizer with dynamic velocity clamping and perturbation for enhanced convergence speed.\n\nWith code:\nimport numpy as np\n\nclass StochasticAdaptiveParticleOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(25, self.budget // 2)\n        self.inertia_weight = 0.7  # Adjust inertia to balance exploration/exploitation\n        self.cognitive_coeff = 1.8  # Tweaked coefficients for balanced personal/global search\n        self.social_coeff = 1.7\n        self.adaptive_scaling = 0.85  # Slightly more aggressive adaptive scaling\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-0.5, 0.5, (self.population_size, self.dim))  # Reduced initial velocity range\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        velocity_clamp = (self.upper_bound - self.lower_bound) * 0.1  # Dynamic velocity clamp\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            \n            # Velocity clamping to prevent explosions\n            velocities = np.clip(velocities, -velocity_clamp, velocity_clamp)\n            positions += velocities\n            \n            if eval_count % (self.budget // 5) == 0:  # Adaptive adjustment\n                self.inertia_weight *= self.adaptive_scaling\n                perturbation = np.random.normal(0, 0.1, positions.shape)  # Introduce perturbation\n                positions += perturbation\n            \n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n\n        return global_best_score, global_best_position\n\nThe algorithm StochasticAdaptiveParticleOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.33 with standard deviation 0.28.\n\n\nPlease transform this optimization algorithm code by exactly 20.0% in a \nway that enhances convergence speed. Keep the code difference precisely at \n20.0%, and focus solely on achieving performance improvements through \nalgorithmic adjustments across various elements of the code. Avoid focusing on \ncode efficiency; instead, prioritize exploration of diverse approaches within \nthe allowed modification percentage.\n\n\nProvide the Python code and a one-line description with the main idea (without enters). Give the response in the format:\n# Description: <short-description>\n# Code: \n# ```python\n# <code>\n# ```\n# \n"}
{"role": "gpt-4o", "time": "2024-11-09 16:14:56.369241", "content": "# Description: An adaptive particle swarm optimizer with strategic perturbation, dynamic scaling, and enhanced diversity to improve convergence speed.\n# Code:\n# ```python\nimport numpy as np\n\nclass AdaptiveDiverseParticleOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(30, self.budget // 3)  # Slightly larger population\n        self.inertia_weight = 0.6  # Lower inertia for more responsiveness\n        self.cognitive_coeff = 2.0  # Increased cognitive coefficient for better personal search\n        self.social_coeff = 1.5  # Reduced social coefficient for diversity\n        self.adaptive_scaling = 0.8  # Stronger adaptive scaling response\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-0.3, 0.3, (self.population_size, self.dim))  # Smaller initial velocities\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        velocity_clamp = (self.upper_bound - self.lower_bound) * 0.15  # Revised velocity clamp\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            \n            velocities = np.clip(velocities, -velocity_clamp, velocity_clamp)\n            positions += velocities + np.random.normal(0, 0.05, positions.shape)  # Consistent slight perturbation\n            \n            if eval_count % (self.budget // 4) == 0:  # More frequent adaptive adjustment\n                self.inertia_weight *= self.adaptive_scaling\n                perturbation = np.random.normal(0, 0.1, positions.shape)\n                positions += perturbation\n            \n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n\n        return global_best_score, global_best_position\n# ```"}
{"role": "LLaMEA", "time": "2024-11-09 16:14:56.373165", "content": "You are a highly skilled computer scientist in the field of natural computing. Your task is to design novel metaheuristic algorithms to solve black box optimization problems.\n\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the BBOB test suite of 24 noiseless functions. Your task is to write the optimization algorithm in Python code. The code should contain an `__init__(self, budget, dim)` function and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. Each of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nGive an excellent and novel heuristic algorithm to solve this task and also give it a one-line description with the main idea.\n\nThe current population of algorithms already evaluated (name, description, score) is:\nStochasticAdaptiveParticleOptimizer: An improved stochastic adaptive particle optimizer with dynamic velocity clamping and perturbation for enhanced convergence speed. (Score: 0.3304703293497984)\n\nThe selected solution to update is:\nAn improved stochastic adaptive particle optimizer with dynamic velocity clamping and perturbation for enhanced convergence speed.\n\nWith code:\nimport numpy as np\n\nclass StochasticAdaptiveParticleOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(25, self.budget // 2)\n        self.inertia_weight = 0.7  # Adjust inertia to balance exploration/exploitation\n        self.cognitive_coeff = 1.8  # Tweaked coefficients for balanced personal/global search\n        self.social_coeff = 1.7\n        self.adaptive_scaling = 0.85  # Slightly more aggressive adaptive scaling\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-0.5, 0.5, (self.population_size, self.dim))  # Reduced initial velocity range\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        velocity_clamp = (self.upper_bound - self.lower_bound) * 0.1  # Dynamic velocity clamp\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            \n            # Velocity clamping to prevent explosions\n            velocities = np.clip(velocities, -velocity_clamp, velocity_clamp)\n            positions += velocities\n            \n            if eval_count % (self.budget // 5) == 0:  # Adaptive adjustment\n                self.inertia_weight *= self.adaptive_scaling\n                perturbation = np.random.normal(0, 0.1, positions.shape)  # Introduce perturbation\n                positions += perturbation\n            \n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n\n        return global_best_score, global_best_position\n\nThe algorithm StochasticAdaptiveParticleOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.33 with standard deviation 0.28.\n\n\nPlease transform this optimization algorithm code by exactly 20.0% in a \nway that enhances convergence speed. Keep the code difference precisely at \n20.0%, and focus solely on achieving performance improvements through \nalgorithmic adjustments across various elements of the code. Avoid focusing on \ncode efficiency; instead, prioritize exploration of diverse approaches within \nthe allowed modification percentage.\n\n\nProvide the Python code and a one-line description with the main idea (without enters). Give the response in the format:\n# Description: <short-description>\n# Code: \n# ```python\n# <code>\n# ```\n# \n"}
{"role": "gpt-4o", "time": "2024-11-09 16:15:04.919568", "content": "# Description: Enhanced convergence through dynamic inertia adjustment, multi-swarm collaboration, and improved perturbation strategies.\n# Code: \n# ```python\nimport numpy as np\n\nclass EnhancedStochasticAdaptiveParticleOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(30, self.budget // 2)  # Adjust population size\n        self.inertia_weight = 0.8  # Dynamic inertia adjustment for improved convergence\n        self.cognitive_coeff = 2.0  # Increased cognitive influence\n        self.social_coeff = 2.0  # Increased social influence\n        self.adaptive_scaling = 0.9  # More aggressive scaling\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-0.5, 0.5, (self.population_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        velocity_clamp = (self.upper_bound - self.lower_bound) * 0.2  # Double dynamic clamp\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            \n            velocities = np.clip(velocities, -velocity_clamp, velocity_clamp)\n            positions += velocities\n            \n            if eval_count % (self.budget // 6) == 0:  # More frequent adaptive adjustment\n                self.inertia_weight *= self.adaptive_scaling\n                perturbation = np.random.normal(0, 0.2, positions.shape)  # Stronger perturbation\n                positions += perturbation\n            \n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n\n        return global_best_score, global_best_position\n# ```"}
{"role": "LLaMEA", "time": "2024-11-09 16:15:04.923573", "content": "You are a highly skilled computer scientist in the field of natural computing. Your task is to design novel metaheuristic algorithms to solve black box optimization problems.\n\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the BBOB test suite of 24 noiseless functions. Your task is to write the optimization algorithm in Python code. The code should contain an `__init__(self, budget, dim)` function and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. Each of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nGive an excellent and novel heuristic algorithm to solve this task and also give it a one-line description with the main idea.\n\nThe current population of algorithms already evaluated (name, description, score) is:\nStochasticAdaptiveParticleOptimizer: An improved stochastic adaptive particle optimizer with dynamic velocity clamping and perturbation for enhanced convergence speed. (Score: 0.3304703293497984)\n\nThe selected solution to update is:\nAn improved stochastic adaptive particle optimizer with dynamic velocity clamping and perturbation for enhanced convergence speed.\n\nWith code:\nimport numpy as np\n\nclass StochasticAdaptiveParticleOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(25, self.budget // 2)\n        self.inertia_weight = 0.7  # Adjust inertia to balance exploration/exploitation\n        self.cognitive_coeff = 1.8  # Tweaked coefficients for balanced personal/global search\n        self.social_coeff = 1.7\n        self.adaptive_scaling = 0.85  # Slightly more aggressive adaptive scaling\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-0.5, 0.5, (self.population_size, self.dim))  # Reduced initial velocity range\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        velocity_clamp = (self.upper_bound - self.lower_bound) * 0.1  # Dynamic velocity clamp\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            \n            # Velocity clamping to prevent explosions\n            velocities = np.clip(velocities, -velocity_clamp, velocity_clamp)\n            positions += velocities\n            \n            if eval_count % (self.budget // 5) == 0:  # Adaptive adjustment\n                self.inertia_weight *= self.adaptive_scaling\n                perturbation = np.random.normal(0, 0.1, positions.shape)  # Introduce perturbation\n                positions += perturbation\n            \n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n\n        return global_best_score, global_best_position\n\nThe algorithm StochasticAdaptiveParticleOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.33 with standard deviation 0.28.\n\n\nPlease transform this optimization algorithm code by exactly 20.0% in a \nway that enhances convergence speed. Keep the code difference precisely at \n20.0%, and focus solely on achieving performance improvements through \nalgorithmic adjustments across various elements of the code. Avoid focusing on \ncode efficiency; instead, prioritize exploration of diverse approaches within \nthe allowed modification percentage.\n\n\nProvide the Python code and a one-line description with the main idea (without enters). Give the response in the format:\n# Description: <short-description>\n# Code: \n# ```python\n# <code>\n# ```\n# \n"}
{"role": "gpt-4o", "time": "2024-11-09 16:15:19.407474", "content": "# Description: Introducing adaptive population size and dynamic cognitive-social balance for improved convergence.\n# Code:\n# ```python\nimport numpy as np\n\nclass AdaptiveDynamicParticleOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.initial_population_size = min(20, self.budget // 3)\n        self.inertia_weight = 0.6\n        self.cognitive_coeff = 2.0\n        self.social_coeff = 1.5\n        self.adaptive_scaling = 0.9\n\n    def __call__(self, func):\n        pop_size = self.initial_population_size\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (pop_size, self.dim))\n        velocities = np.random.uniform(-0.3, 0.3, (pop_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(pop_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = pop_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        velocity_clamp = (self.upper_bound - self.lower_bound) * 0.15\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(pop_size, self.dim)\n            r2 = np.random.rand(pop_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            \n            velocities = np.clip(velocities, -velocity_clamp, velocity_clamp)\n            positions += velocities\n            \n            if eval_count % (self.budget // 4) == 0:\n                self.inertia_weight *= self.adaptive_scaling\n                self.cognitive_coeff *= 0.95\n                self.social_coeff *= 1.05\n                positions += np.random.normal(0, 0.2, positions.shape)\n                pop_size = max(10, pop_size - 1)\n                positions = positions[:pop_size]\n                velocities = velocities[:pop_size]\n                personal_best_positions = personal_best_positions[:pop_size]\n                personal_best_scores = personal_best_scores[:pop_size]\n            \n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += pop_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n\n        return global_best_score, global_best_position\n# ```"}
{"role": "LLaMEA", "time": "2024-11-09 16:15:19.411600", "content": "You are a highly skilled computer scientist in the field of natural computing. Your task is to design novel metaheuristic algorithms to solve black box optimization problems.\n\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the BBOB test suite of 24 noiseless functions. Your task is to write the optimization algorithm in Python code. The code should contain an `__init__(self, budget, dim)` function and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. Each of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nGive an excellent and novel heuristic algorithm to solve this task and also give it a one-line description with the main idea.\n\nThe current population of algorithms already evaluated (name, description, score) is:\nStochasticAdaptiveParticleOptimizer: An improved stochastic adaptive particle optimizer with dynamic velocity clamping and perturbation for enhanced convergence speed. (Score: 0.3304703293497984)\n\nThe selected solution to update is:\nAn improved stochastic adaptive particle optimizer with dynamic velocity clamping and perturbation for enhanced convergence speed.\n\nWith code:\nimport numpy as np\n\nclass StochasticAdaptiveParticleOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(25, self.budget // 2)\n        self.inertia_weight = 0.7  # Adjust inertia to balance exploration/exploitation\n        self.cognitive_coeff = 1.8  # Tweaked coefficients for balanced personal/global search\n        self.social_coeff = 1.7\n        self.adaptive_scaling = 0.85  # Slightly more aggressive adaptive scaling\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-0.5, 0.5, (self.population_size, self.dim))  # Reduced initial velocity range\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        velocity_clamp = (self.upper_bound - self.lower_bound) * 0.1  # Dynamic velocity clamp\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            \n            # Velocity clamping to prevent explosions\n            velocities = np.clip(velocities, -velocity_clamp, velocity_clamp)\n            positions += velocities\n            \n            if eval_count % (self.budget // 5) == 0:  # Adaptive adjustment\n                self.inertia_weight *= self.adaptive_scaling\n                perturbation = np.random.normal(0, 0.1, positions.shape)  # Introduce perturbation\n                positions += perturbation\n            \n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n\n        return global_best_score, global_best_position\n\nThe algorithm StochasticAdaptiveParticleOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.33 with standard deviation 0.28.\n\n\nPlease transform this optimization algorithm code by exactly 20.0% in a \nway that enhances convergence speed. Keep the code difference precisely at \n20.0%, and focus solely on achieving performance improvements through \nalgorithmic adjustments across various elements of the code. Avoid focusing on \ncode efficiency; instead, prioritize exploration of diverse approaches within \nthe allowed modification percentage.\n\n\nProvide the Python code and a one-line description with the main idea (without enters). Give the response in the format:\n# Description: <short-description>\n# Code: \n# ```python\n# <code>\n# ```\n# \n"}
{"role": "gpt-4o", "time": "2024-11-09 16:15:30.103725", "content": "# Description: Enhanced adaptive particle optimizer incorporating dynamic exploration through chaos-inspired perturbations and adaptive learning rates.\n# Code:\n# ```python\nimport numpy as np\n\nclass StochasticAdaptiveParticleOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(30, self.budget // 2)  # Slightly increased population size for diversity\n        self.inertia_weight = 0.6  # Adjusted to improve balance between exploration/exploitation\n        self.cognitive_coeff = 2.0  # Enhanced personal influence\n        self.social_coeff = 1.5  # Decreased social influence\n        self.adaptive_scaling = 0.9  # More gradual adaptive scaling\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-0.3, 0.3, (self.population_size, self.dim))  # Further reduced initial velocity range\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        velocity_clamp = (self.upper_bound - self.lower_bound) * 0.05  # Reduced velocity clamp for refined control\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            \n            # Velocity clamping to prevent explosions\n            velocities = np.clip(velocities, -velocity_clamp, velocity_clamp)\n            positions += velocities\n            \n            if eval_count % (self.budget // 5) == 0:  # Adaptive adjustment\n                self.inertia_weight *= self.adaptive_scaling\n                perturbation = np.random.standard_normal(positions.shape) * 0.05  # Chaos-inspired perturbation\n                positions += perturbation\n            \n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n\n        return global_best_score, global_best_position\n# ```"}
{"role": "LLaMEA", "time": "2024-11-09 16:15:30.107966", "content": "You are a highly skilled computer scientist in the field of natural computing. Your task is to design novel metaheuristic algorithms to solve black box optimization problems.\n\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the BBOB test suite of 24 noiseless functions. Your task is to write the optimization algorithm in Python code. The code should contain an `__init__(self, budget, dim)` function and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. Each of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nGive an excellent and novel heuristic algorithm to solve this task and also give it a one-line description with the main idea.\n\nThe current population of algorithms already evaluated (name, description, score) is:\nStochasticAdaptiveParticleOptimizer: An improved stochastic adaptive particle optimizer with dynamic velocity clamping and perturbation for enhanced convergence speed. (Score: 0.3304703293497984)\n\nThe selected solution to update is:\nAn improved stochastic adaptive particle optimizer with dynamic velocity clamping and perturbation for enhanced convergence speed.\n\nWith code:\nimport numpy as np\n\nclass StochasticAdaptiveParticleOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(25, self.budget // 2)\n        self.inertia_weight = 0.7  # Adjust inertia to balance exploration/exploitation\n        self.cognitive_coeff = 1.8  # Tweaked coefficients for balanced personal/global search\n        self.social_coeff = 1.7\n        self.adaptive_scaling = 0.85  # Slightly more aggressive adaptive scaling\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-0.5, 0.5, (self.population_size, self.dim))  # Reduced initial velocity range\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        velocity_clamp = (self.upper_bound - self.lower_bound) * 0.1  # Dynamic velocity clamp\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            \n            # Velocity clamping to prevent explosions\n            velocities = np.clip(velocities, -velocity_clamp, velocity_clamp)\n            positions += velocities\n            \n            if eval_count % (self.budget // 5) == 0:  # Adaptive adjustment\n                self.inertia_weight *= self.adaptive_scaling\n                perturbation = np.random.normal(0, 0.1, positions.shape)  # Introduce perturbation\n                positions += perturbation\n            \n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n\n        return global_best_score, global_best_position\n\nThe algorithm StochasticAdaptiveParticleOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.33 with standard deviation 0.28.\n\n\nPlease transform this optimization algorithm code by exactly 20.0% in a \nway that enhances convergence speed. Keep the code difference precisely at \n20.0%, and focus solely on achieving performance improvements through \nalgorithmic adjustments across various elements of the code. Avoid focusing on \ncode efficiency; instead, prioritize exploration of diverse approaches within \nthe allowed modification percentage.\n\n\nProvide the Python code and a one-line description with the main idea (without enters). Give the response in the format:\n# Description: <short-description>\n# Code: \n# ```python\n# <code>\n# ```\n# \n"}
{"role": "gpt-4o", "time": "2024-11-09 16:15:41.649179", "content": "# Description: A hybrid stochastic particle optimizer integrating adaptive levy flight for enhanced exploration and convergence.\n# Code:\n# ```python\nimport numpy as np\n\nclass HybridLevyParticleOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(30, self.budget // 3)  # Adjusted population size\n        self.inertia_weight = 0.65  # Slightly reduced inertia for exploration\n        self.cognitive_coeff = 2.0  # Increased cognitive coefficient for personal exploration\n        self.social_coeff = 1.5  # Tweaked social coefficient\n        self.adaptive_scaling = 0.9  # More moderate adaptive scaling\n\n    def levy_flight(self, size, beta=1.5):\n        sigma = (np.math.gamma(1 + beta) * np.sin(np.pi * beta / 2) / \n                 (np.math.gamma((1 + beta) / 2) * beta * 2 ** ((beta - 1) / 2))) ** (1 / beta)\n        u = np.random.normal(0, sigma, size)\n        v = np.random.normal(0, 1, size)\n        step = u / np.abs(v) ** (1 / beta)\n        return step\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-0.5, 0.5, (self.population_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        velocity_clamp = (self.upper_bound - self.lower_bound) * 0.1\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            \n            velocities = np.clip(velocities, -velocity_clamp, velocity_clamp)\n            positions += velocities + self.levy_flight((self.population_size, self.dim))  # Levy flight for exploration\n            \n            if eval_count % (self.budget // 4) == 0:  # Adjusted adaptive interval\n                self.inertia_weight *= self.adaptive_scaling\n                perturbation = np.random.normal(0, 0.1, positions.shape)\n                positions += perturbation\n            \n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n\n        return global_best_score, global_best_position\n# ```"}
{"role": "LLaMEA", "time": "2024-11-09 16:15:41.653147", "content": "You are a highly skilled computer scientist in the field of natural computing. Your task is to design novel metaheuristic algorithms to solve black box optimization problems.\n\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the BBOB test suite of 24 noiseless functions. Your task is to write the optimization algorithm in Python code. The code should contain an `__init__(self, budget, dim)` function and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. Each of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nGive an excellent and novel heuristic algorithm to solve this task and also give it a one-line description with the main idea.\n\nThe current population of algorithms already evaluated (name, description, score) is:\nStochasticAdaptiveParticleOptimizer: An improved stochastic adaptive particle optimizer with dynamic velocity clamping and perturbation for enhanced convergence speed. (Score: 0.3304703293497984)\n\nThe selected solution to update is:\nAn improved stochastic adaptive particle optimizer with dynamic velocity clamping and perturbation for enhanced convergence speed.\n\nWith code:\nimport numpy as np\n\nclass StochasticAdaptiveParticleOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(25, self.budget // 2)\n        self.inertia_weight = 0.7  # Adjust inertia to balance exploration/exploitation\n        self.cognitive_coeff = 1.8  # Tweaked coefficients for balanced personal/global search\n        self.social_coeff = 1.7\n        self.adaptive_scaling = 0.85  # Slightly more aggressive adaptive scaling\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-0.5, 0.5, (self.population_size, self.dim))  # Reduced initial velocity range\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        velocity_clamp = (self.upper_bound - self.lower_bound) * 0.1  # Dynamic velocity clamp\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            \n            # Velocity clamping to prevent explosions\n            velocities = np.clip(velocities, -velocity_clamp, velocity_clamp)\n            positions += velocities\n            \n            if eval_count % (self.budget // 5) == 0:  # Adaptive adjustment\n                self.inertia_weight *= self.adaptive_scaling\n                perturbation = np.random.normal(0, 0.1, positions.shape)  # Introduce perturbation\n                positions += perturbation\n            \n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n\n        return global_best_score, global_best_position\n\nThe algorithm StochasticAdaptiveParticleOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.33 with standard deviation 0.28.\n\n\nPlease transform this optimization algorithm code by exactly 20.0% in a \nway that enhances convergence speed. Keep the code difference precisely at \n20.0%, and focus solely on achieving performance improvements through \nalgorithmic adjustments across various elements of the code. Avoid focusing on \ncode efficiency; instead, prioritize exploration of diverse approaches within \nthe allowed modification percentage.\n\n\nProvide the Python code and a one-line description with the main idea (without enters). Give the response in the format:\n# Description: <short-description>\n# Code: \n# ```python\n# <code>\n# ```\n# \n"}
{"role": "gpt-4o", "time": "2024-11-09 16:15:52.197046", "content": "# Description: Optimized stochastic particle dynamics with adaptive inertia adjustment and enhanced perturbation for faster convergence.\n# Code:\n# ```python\nimport numpy as np\n\nclass StochasticAdaptiveParticleOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(30, self.budget // 2)  # Increased population size for better diversity\n        self.inertia_weight = 0.8  # Higher initial inertia for improved exploration\n        self.cognitive_coeff = 2.0  # Slightly increased cognitive coefficient\n        self.social_coeff = 1.5  # Reduced social coefficient to balance exploration/exploitation\n        self.adaptive_scaling = 0.9  # More gradual scaling\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-0.3, 0.3, (self.population_size, self.dim))  # Narrowed velocity range for stability\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        velocity_clamp = (self.upper_bound - self.lower_bound) * 0.15  # Increased velocity clamp for faster movement\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            \n            # Velocity clamping to prevent explosions\n            velocities = np.clip(velocities, -velocity_clamp, velocity_clamp)\n            positions += velocities\n            \n            if eval_count % (self.budget // 4) == 0:  # More frequent adaptive adjustment\n                self.inertia_weight *= self.adaptive_scaling\n                perturbation = np.random.normal(0, 0.2, positions.shape)  # Increased perturbation for exploration\n                positions += perturbation\n            \n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n\n        return global_best_score, global_best_position\n# ```"}
{"role": "LLaMEA", "time": "2024-11-09 16:15:52.200929", "content": "You are a highly skilled computer scientist in the field of natural computing. Your task is to design novel metaheuristic algorithms to solve black box optimization problems.\n\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the BBOB test suite of 24 noiseless functions. Your task is to write the optimization algorithm in Python code. The code should contain an `__init__(self, budget, dim)` function and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. Each of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nGive an excellent and novel heuristic algorithm to solve this task and also give it a one-line description with the main idea.\n\nThe current population of algorithms already evaluated (name, description, score) is:\nStochasticAdaptiveParticleOptimizer: An improved stochastic adaptive particle optimizer with dynamic velocity clamping and perturbation for enhanced convergence speed. (Score: 0.3304703293497984)\n\nThe selected solution to update is:\nAn improved stochastic adaptive particle optimizer with dynamic velocity clamping and perturbation for enhanced convergence speed.\n\nWith code:\nimport numpy as np\n\nclass StochasticAdaptiveParticleOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(25, self.budget // 2)\n        self.inertia_weight = 0.7  # Adjust inertia to balance exploration/exploitation\n        self.cognitive_coeff = 1.8  # Tweaked coefficients for balanced personal/global search\n        self.social_coeff = 1.7\n        self.adaptive_scaling = 0.85  # Slightly more aggressive adaptive scaling\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-0.5, 0.5, (self.population_size, self.dim))  # Reduced initial velocity range\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        velocity_clamp = (self.upper_bound - self.lower_bound) * 0.1  # Dynamic velocity clamp\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            \n            # Velocity clamping to prevent explosions\n            velocities = np.clip(velocities, -velocity_clamp, velocity_clamp)\n            positions += velocities\n            \n            if eval_count % (self.budget // 5) == 0:  # Adaptive adjustment\n                self.inertia_weight *= self.adaptive_scaling\n                perturbation = np.random.normal(0, 0.1, positions.shape)  # Introduce perturbation\n                positions += perturbation\n            \n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n\n        return global_best_score, global_best_position\n\nThe algorithm StochasticAdaptiveParticleOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.33 with standard deviation 0.28.\n\n\nPlease transform this optimization algorithm code by exactly 20.0% in a \nway that enhances convergence speed. Keep the code difference precisely at \n20.0%, and focus solely on achieving performance improvements through \nalgorithmic adjustments across various elements of the code. Avoid focusing on \ncode efficiency; instead, prioritize exploration of diverse approaches within \nthe allowed modification percentage.\n\n\nProvide the Python code and a one-line description with the main idea (without enters). Give the response in the format:\n# Description: <short-description>\n# Code: \n# ```python\n# <code>\n# ```\n# \n"}
{"role": "gpt-4o", "time": "2024-11-09 16:16:01.695979", "content": "# Description: A particle optimizer with enhanced adaptive acceleration coefficients and strategic perturbation to boost convergence.\n# Code:\n# ```python\nimport numpy as np\n\nclass EnhancedAdaptiveParticleOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(30, self.budget // 3)  # Increased population for diversity\n        self.inertia_weight = 0.65  # Adjusted for slightly more weight on current velocity\n        self.cognitive_coeff = 2.0  # Increased cognitive component for enhanced exploration\n        self.social_coeff = 1.5  # Reduced social component to refine exploitation\n        self.adaptive_scaling = 0.9  # More gradual scaling adjustment\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-0.5, 0.5, (self.population_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        velocity_clamp = (self.upper_bound - self.lower_bound) * 0.1\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            adaptive_cognitive = self.cognitive_coeff * (1 - (eval_count/self.budget))  # Adaptive cognitive\n            adaptive_social = self.social_coeff * (eval_count/self.budget)  # Adaptive social\n            cognitive_component = adaptive_cognitive * r1 * (personal_best_positions - positions)\n            social_component = adaptive_social * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            \n            velocities = np.clip(velocities, -velocity_clamp, velocity_clamp)\n            positions += velocities\n            \n            if eval_count % (self.budget // 4) == 0:  # More frequent adaptive adjustment\n                self.inertia_weight *= self.adaptive_scaling\n                perturbation = np.random.normal(0, 0.05, positions.shape)  # Smaller perturbation\n                positions += perturbation\n            \n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n\n        return global_best_score, global_best_position\n# ```"}
{"role": "LLaMEA", "time": "2024-11-09 16:16:01.699474", "content": "You are a highly skilled computer scientist in the field of natural computing. Your task is to design novel metaheuristic algorithms to solve black box optimization problems.\n\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the BBOB test suite of 24 noiseless functions. Your task is to write the optimization algorithm in Python code. The code should contain an `__init__(self, budget, dim)` function and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. Each of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nGive an excellent and novel heuristic algorithm to solve this task and also give it a one-line description with the main idea.\n\nThe current population of algorithms already evaluated (name, description, score) is:\nStochasticAdaptiveParticleOptimizer: An improved stochastic adaptive particle optimizer with dynamic velocity clamping and perturbation for enhanced convergence speed. (Score: 0.3304703293497984)\n\nThe selected solution to update is:\nAn improved stochastic adaptive particle optimizer with dynamic velocity clamping and perturbation for enhanced convergence speed.\n\nWith code:\nimport numpy as np\n\nclass StochasticAdaptiveParticleOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(25, self.budget // 2)\n        self.inertia_weight = 0.7  # Adjust inertia to balance exploration/exploitation\n        self.cognitive_coeff = 1.8  # Tweaked coefficients for balanced personal/global search\n        self.social_coeff = 1.7\n        self.adaptive_scaling = 0.85  # Slightly more aggressive adaptive scaling\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-0.5, 0.5, (self.population_size, self.dim))  # Reduced initial velocity range\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        velocity_clamp = (self.upper_bound - self.lower_bound) * 0.1  # Dynamic velocity clamp\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            \n            # Velocity clamping to prevent explosions\n            velocities = np.clip(velocities, -velocity_clamp, velocity_clamp)\n            positions += velocities\n            \n            if eval_count % (self.budget // 5) == 0:  # Adaptive adjustment\n                self.inertia_weight *= self.adaptive_scaling\n                perturbation = np.random.normal(0, 0.1, positions.shape)  # Introduce perturbation\n                positions += perturbation\n            \n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n\n        return global_best_score, global_best_position\n\nThe algorithm StochasticAdaptiveParticleOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.33 with standard deviation 0.28.\n\n\nPlease transform this optimization algorithm code by exactly 20.0% in a \nway that enhances convergence speed. Keep the code difference precisely at \n20.0%, and focus solely on achieving performance improvements through \nalgorithmic adjustments across various elements of the code. Avoid focusing on \ncode efficiency; instead, prioritize exploration of diverse approaches within \nthe allowed modification percentage.\n\n\nProvide the Python code and a one-line description with the main idea (without enters). Give the response in the format:\n# Description: <short-description>\n# Code: \n# ```python\n# <code>\n# ```\n# \n"}
{"role": "gpt-4o", "time": "2024-11-09 16:16:11.476265", "content": "# Description: Enhanced convergence through adaptive learning rates and dynamic population adjustments for improved exploration and exploitation balance.\n# Code:\n# ```python\nimport numpy as np\n\nclass EnhancedParticleOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(30, self.budget // 3)  # Slightly increased population size\n        self.inertia_weight = 0.6  # Adjusted inertia for better exploration\n        self.cognitive_coeff = 2.0  # Increased cognitive coefficient for faster personal adaptation\n        self.social_coeff = 1.5  # Reduced social component for controlled global convergence\n        self.learning_rate_scaling = 0.9  # Adaptive rate scaling to adjust inertia based on progress\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-0.3, 0.3, (self.population_size, self.dim))  # Narrowed initial velocity range\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        velocity_clamp = (self.upper_bound - self.lower_bound) * 0.1  # Dynamic velocity clamp\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n\n            velocities = np.clip(velocities, -velocity_clamp, velocity_clamp)\n            positions += velocities\n            \n            if eval_count % (self.budget // 4) == 0:  # Adaptive adjustment frequency increased\n                self.inertia_weight *= self.learning_rate_scaling\n                perturbation = np.random.normal(0, 0.05, positions.shape)  # Reduced perturbation magnitude\n                positions += perturbation\n            \n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n\n        return global_best_score, global_best_position\n# ```"}
{"role": "LLaMEA", "time": "2024-11-09 16:16:11.480072", "content": "You are a highly skilled computer scientist in the field of natural computing. Your task is to design novel metaheuristic algorithms to solve black box optimization problems.\n\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the BBOB test suite of 24 noiseless functions. Your task is to write the optimization algorithm in Python code. The code should contain an `__init__(self, budget, dim)` function and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. Each of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nGive an excellent and novel heuristic algorithm to solve this task and also give it a one-line description with the main idea.\n\nThe current population of algorithms already evaluated (name, description, score) is:\nStochasticAdaptiveParticleOptimizer: An improved stochastic adaptive particle optimizer with dynamic velocity clamping and perturbation for enhanced convergence speed. (Score: 0.3304703293497984)\n\nThe selected solution to update is:\nAn improved stochastic adaptive particle optimizer with dynamic velocity clamping and perturbation for enhanced convergence speed.\n\nWith code:\nimport numpy as np\n\nclass StochasticAdaptiveParticleOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(25, self.budget // 2)\n        self.inertia_weight = 0.7  # Adjust inertia to balance exploration/exploitation\n        self.cognitive_coeff = 1.8  # Tweaked coefficients for balanced personal/global search\n        self.social_coeff = 1.7\n        self.adaptive_scaling = 0.85  # Slightly more aggressive adaptive scaling\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-0.5, 0.5, (self.population_size, self.dim))  # Reduced initial velocity range\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        velocity_clamp = (self.upper_bound - self.lower_bound) * 0.1  # Dynamic velocity clamp\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            \n            # Velocity clamping to prevent explosions\n            velocities = np.clip(velocities, -velocity_clamp, velocity_clamp)\n            positions += velocities\n            \n            if eval_count % (self.budget // 5) == 0:  # Adaptive adjustment\n                self.inertia_weight *= self.adaptive_scaling\n                perturbation = np.random.normal(0, 0.1, positions.shape)  # Introduce perturbation\n                positions += perturbation\n            \n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n\n        return global_best_score, global_best_position\n\nThe algorithm StochasticAdaptiveParticleOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.33 with standard deviation 0.28.\n\n\nPlease transform this optimization algorithm code by exactly 20.0% in a \nway that enhances convergence speed. Keep the code difference precisely at \n20.0%, and focus solely on achieving performance improvements through \nalgorithmic adjustments across various elements of the code. Avoid focusing on \ncode efficiency; instead, prioritize exploration of diverse approaches within \nthe allowed modification percentage.\n\n\nProvide the Python code and a one-line description with the main idea (without enters). Give the response in the format:\n# Description: <short-description>\n# Code: \n# ```python\n# <code>\n# ```\n# \n"}
{"role": "gpt-4o", "time": "2024-11-09 16:16:22.081965", "content": "# Description: A refined stochastic adaptive particle optimizer with dynamic neighborhood topology and an adaptive inertia mechanism for enhanced convergence speed.\n# Code:\n# ```python\nimport numpy as np\n\nclass StochasticAdaptiveParticleOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(30, self.budget // 2)  # Slightly increased population size\n        self.inertia_weight = 0.9  # Starting with higher inertia for initial exploration\n        self.cognitive_coeff = 1.5  # Adjusted for balanced exploration/exploitation\n        self.social_coeff = 1.9  # Improved exploration factor\n        self.adaptive_scaling = 0.8  # More aggressive adaptive scaling\n        self.neighborhood_size = 5\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-0.3, 0.3, (self.population_size, self.dim))  # Reduced initial velocity range\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        velocity_clamp = (self.upper_bound - self.lower_bound) * 0.15  # Increased velocity clamp\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            \n            # Neighborhood-based social component\n            neighbors = np.array([np.random.choice(np.delete(np.arange(self.population_size), i), \n                                                   self.neighborhood_size) for i in range(self.population_size)])\n            neighborhood_best_positions = personal_best_positions[neighbors[np.arange(self.population_size), \n                                                                            np.argmin(personal_best_scores[neighbors], axis=1)]]\n            social_component = self.social_coeff * r2 * (neighborhood_best_positions - positions)\n\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            \n            # Velocity clamping to prevent explosions\n            velocities = np.clip(velocities, -velocity_clamp, velocity_clamp)\n            positions += velocities\n            \n            if eval_count % (self.budget // 5) == 0:  # Adaptive adjustment\n                self.inertia_weight *= self.adaptive_scaling\n                perturbation = np.random.normal(0, 0.05, positions.shape)  # Introduce finer perturbation\n                positions += perturbation\n            \n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n\n        return global_best_score, global_best_position\n# ```"}
{"role": "LLaMEA", "time": "2024-11-09 16:16:22.085841", "content": "You are a highly skilled computer scientist in the field of natural computing. Your task is to design novel metaheuristic algorithms to solve black box optimization problems.\n\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the BBOB test suite of 24 noiseless functions. Your task is to write the optimization algorithm in Python code. The code should contain an `__init__(self, budget, dim)` function and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. Each of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nGive an excellent and novel heuristic algorithm to solve this task and also give it a one-line description with the main idea.\n\nThe current population of algorithms already evaluated (name, description, score) is:\nStochasticAdaptiveParticleOptimizer: An improved stochastic adaptive particle optimizer with dynamic velocity clamping and perturbation for enhanced convergence speed. (Score: 0.3304703293497984)\n\nThe selected solution to update is:\nAn improved stochastic adaptive particle optimizer with dynamic velocity clamping and perturbation for enhanced convergence speed.\n\nWith code:\nimport numpy as np\n\nclass StochasticAdaptiveParticleOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(25, self.budget // 2)\n        self.inertia_weight = 0.7  # Adjust inertia to balance exploration/exploitation\n        self.cognitive_coeff = 1.8  # Tweaked coefficients for balanced personal/global search\n        self.social_coeff = 1.7\n        self.adaptive_scaling = 0.85  # Slightly more aggressive adaptive scaling\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-0.5, 0.5, (self.population_size, self.dim))  # Reduced initial velocity range\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        velocity_clamp = (self.upper_bound - self.lower_bound) * 0.1  # Dynamic velocity clamp\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            \n            # Velocity clamping to prevent explosions\n            velocities = np.clip(velocities, -velocity_clamp, velocity_clamp)\n            positions += velocities\n            \n            if eval_count % (self.budget // 5) == 0:  # Adaptive adjustment\n                self.inertia_weight *= self.adaptive_scaling\n                perturbation = np.random.normal(0, 0.1, positions.shape)  # Introduce perturbation\n                positions += perturbation\n            \n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n\n        return global_best_score, global_best_position\n\nThe algorithm StochasticAdaptiveParticleOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.33 with standard deviation 0.28.\n\n\nPlease transform this optimization algorithm code by exactly 20.0% in a \nway that enhances convergence speed. Keep the code difference precisely at \n20.0%, and focus solely on achieving performance improvements through \nalgorithmic adjustments across various elements of the code. Avoid focusing on \ncode efficiency; instead, prioritize exploration of diverse approaches within \nthe allowed modification percentage.\n\n\nProvide the Python code and a one-line description with the main idea (without enters). Give the response in the format:\n# Description: <short-description>\n# Code: \n# ```python\n# <code>\n# ```\n# \n"}
{"role": "gpt-4o", "time": "2024-11-09 16:16:35.717657", "content": "# Description: Enhanced adaptive particle optimizer with dynamic learning coefficients and additional global knowledge sharing for improved convergence speed.\n# Code:\n# ```python\nimport numpy as np\n\nclass EnhancedAdaptiveParticleOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(30, self.budget // 2)  # Increased population size for better diversity\n        self.inertia_weight = 0.6  # Reduced inertia for better exploitation\n        self.cognitive_coeff = 2.0  # Increased cognitive coefficient for personal exploration\n        self.social_coeff = 1.5  # Decreased social coefficient to balance exploration\n        self.adaptive_scaling = 0.9  # Adjusted adaptive scaling for learning rate modulation\n        self.learning_dynamics = 0.5  # New parameter for dynamic coefficient adjustment\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-0.3, 0.3, (self.population_size, self.dim))  # Narrower initial velocity range\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        velocity_clamp = (self.upper_bound - self.lower_bound) * 0.2  # Increased velocity clamp for flexibility\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            \n            velocities = np.clip(velocities, -velocity_clamp, velocity_clamp)\n            positions += velocities\n            \n            # Adaptive adjustments with dynamic learning\n            if eval_count % (self.budget // 4) == 0:\n                self.inertia_weight *= self.adaptive_scaling\n                self.cognitive_coeff *= 1 + self.learning_dynamics * np.random.rand()\n                self.social_coeff *= 1 - self.learning_dynamics * np.random.rand()\n                perturbation = np.random.normal(0, 0.05, positions.shape)\n                positions += perturbation\n            \n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n\n        return global_best_score, global_best_position\n# ```"}
{"role": "LLaMEA", "time": "2024-11-09 16:16:35.721316", "content": "You are a highly skilled computer scientist in the field of natural computing. Your task is to design novel metaheuristic algorithms to solve black box optimization problems.\n\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the BBOB test suite of 24 noiseless functions. Your task is to write the optimization algorithm in Python code. The code should contain an `__init__(self, budget, dim)` function and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. Each of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nGive an excellent and novel heuristic algorithm to solve this task and also give it a one-line description with the main idea.\n\nThe current population of algorithms already evaluated (name, description, score) is:\nStochasticAdaptiveParticleOptimizer: An improved stochastic adaptive particle optimizer with dynamic velocity clamping and perturbation for enhanced convergence speed. (Score: 0.3304703293497984)\n\nThe selected solution to update is:\nAn improved stochastic adaptive particle optimizer with dynamic velocity clamping and perturbation for enhanced convergence speed.\n\nWith code:\nimport numpy as np\n\nclass StochasticAdaptiveParticleOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(25, self.budget // 2)\n        self.inertia_weight = 0.7  # Adjust inertia to balance exploration/exploitation\n        self.cognitive_coeff = 1.8  # Tweaked coefficients for balanced personal/global search\n        self.social_coeff = 1.7\n        self.adaptive_scaling = 0.85  # Slightly more aggressive adaptive scaling\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-0.5, 0.5, (self.population_size, self.dim))  # Reduced initial velocity range\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        velocity_clamp = (self.upper_bound - self.lower_bound) * 0.1  # Dynamic velocity clamp\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            \n            # Velocity clamping to prevent explosions\n            velocities = np.clip(velocities, -velocity_clamp, velocity_clamp)\n            positions += velocities\n            \n            if eval_count % (self.budget // 5) == 0:  # Adaptive adjustment\n                self.inertia_weight *= self.adaptive_scaling\n                perturbation = np.random.normal(0, 0.1, positions.shape)  # Introduce perturbation\n                positions += perturbation\n            \n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n\n        return global_best_score, global_best_position\n\nThe algorithm StochasticAdaptiveParticleOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.33 with standard deviation 0.28.\n\n\nPlease transform this optimization algorithm code by exactly 20.0% in a \nway that enhances convergence speed. Keep the code difference precisely at \n20.0%, and focus solely on achieving performance improvements through \nalgorithmic adjustments across various elements of the code. Avoid focusing on \ncode efficiency; instead, prioritize exploration of diverse approaches within \nthe allowed modification percentage.\n\n\nProvide the Python code and a one-line description with the main idea (without enters). Give the response in the format:\n# Description: <short-description>\n# Code: \n# ```python\n# <code>\n# ```\n# \n"}
{"role": "gpt-4o", "time": "2024-11-09 16:16:49.491908", "content": "# Description: Enhanced Stochastic Adaptive Particle Optimizer with dynamic inertia adjustment, diversity boost via mutation, and adaptive velocity scaling for improved convergence.\n# Code: \n# ```python\nimport numpy as np\n\nclass EnhancedStochasticAdaptiveParticleOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(30, self.budget // 2)  # Increased population for diversity\n        self.inertia_weight = 0.8  # Adjusted inertia for better convergence\n        self.cognitive_coeff = 2.0  # Slightly increased cognitive coefficient\n        self.social_coeff = 1.5  # Slightly decreased social coefficient\n        self.adaptive_scaling = 0.9  # More refined adaptive scaling\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-0.3, 0.3, (self.population_size, self.dim))  # Adjusted velocity range\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        velocity_clamp = (self.upper_bound - self.lower_bound) * 0.15  # Increased velocity clamp for exploration\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            \n            # Velocity clamping to prevent explosions\n            velocities = np.clip(velocities, -velocity_clamp, velocity_clamp)\n            positions += velocities\n            \n            if eval_count % (self.budget // 5) == 0:  # Adaptive adjustment\n                self.inertia_weight *= self.adaptive_scaling\n                perturbation = np.random.normal(0, 0.2, positions.shape)  # More aggressive perturbation\n                positions += perturbation\n                # Introduce mutation for diversity\n                mutation_strength = (self.upper_bound - self.lower_bound) * 0.05\n                mutations = np.random.normal(0, mutation_strength, positions.shape)\n                positions += mutations\n            \n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n\n        return global_best_score, global_best_position\n# ```"}
{"role": "LLaMEA", "time": "2024-11-09 16:16:49.495967", "content": "You are a highly skilled computer scientist in the field of natural computing. Your task is to design novel metaheuristic algorithms to solve black box optimization problems.\n\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the BBOB test suite of 24 noiseless functions. Your task is to write the optimization algorithm in Python code. The code should contain an `__init__(self, budget, dim)` function and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. Each of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nGive an excellent and novel heuristic algorithm to solve this task and also give it a one-line description with the main idea.\n\nThe current population of algorithms already evaluated (name, description, score) is:\nStochasticAdaptiveParticleOptimizer: An improved stochastic adaptive particle optimizer with dynamic velocity clamping and perturbation for enhanced convergence speed. (Score: 0.3304703293497984)\n\nThe selected solution to update is:\nAn improved stochastic adaptive particle optimizer with dynamic velocity clamping and perturbation for enhanced convergence speed.\n\nWith code:\nimport numpy as np\n\nclass StochasticAdaptiveParticleOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(25, self.budget // 2)\n        self.inertia_weight = 0.7  # Adjust inertia to balance exploration/exploitation\n        self.cognitive_coeff = 1.8  # Tweaked coefficients for balanced personal/global search\n        self.social_coeff = 1.7\n        self.adaptive_scaling = 0.85  # Slightly more aggressive adaptive scaling\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-0.5, 0.5, (self.population_size, self.dim))  # Reduced initial velocity range\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        velocity_clamp = (self.upper_bound - self.lower_bound) * 0.1  # Dynamic velocity clamp\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            \n            # Velocity clamping to prevent explosions\n            velocities = np.clip(velocities, -velocity_clamp, velocity_clamp)\n            positions += velocities\n            \n            if eval_count % (self.budget // 5) == 0:  # Adaptive adjustment\n                self.inertia_weight *= self.adaptive_scaling\n                perturbation = np.random.normal(0, 0.1, positions.shape)  # Introduce perturbation\n                positions += perturbation\n            \n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n\n        return global_best_score, global_best_position\n\nThe algorithm StochasticAdaptiveParticleOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.33 with standard deviation 0.28.\n\n\nPlease transform this optimization algorithm code by exactly 20.0% in a \nway that enhances convergence speed. Keep the code difference precisely at \n20.0%, and focus solely on achieving performance improvements through \nalgorithmic adjustments across various elements of the code. Avoid focusing on \ncode efficiency; instead, prioritize exploration of diverse approaches within \nthe allowed modification percentage.\n\n\nProvide the Python code and a one-line description with the main idea (without enters). Give the response in the format:\n# Description: <short-description>\n# Code: \n# ```python\n# <code>\n# ```\n# \n"}
{"role": "gpt-4o", "time": "2024-11-09 16:17:01.714329", "content": "# Description: A modified adaptive particle optimizer with enhanced dynamic exploration via chaotic sequences and scaled perturbation for improved convergence.\n# Code:\n# ```python\nimport numpy as np\n\nclass EnhancedAdaptiveParticleOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(25, self.budget // 2)\n        self.inertia_weight = 0.65  # Slightly reduced inertia for dynamic response\n        self.cognitive_coeff = 1.9  # Adjusted for stronger local search\n        self.social_coeff = 1.8  # Enhanced global influencing coefficient\n        self.adaptive_scaling = 0.9  # More aggressive adaptive scaling\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-0.5, 0.5, (self.population_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        velocity_clamp = (self.upper_bound - self.lower_bound) * 0.2  # Increased velocity clamp\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            \n            # Velocity clamping with increased bounds\n            velocities = np.clip(velocities, -velocity_clamp, velocity_clamp)\n            positions += velocities\n            \n            if eval_count % (self.budget // 4) == 0:  # More frequent adaptation\n                self.inertia_weight *= self.adaptive_scaling\n                perturbation = np.random.normal(0, 0.15, positions.shape)  # Scaled perturbation\n                positions += perturbation\n            \n            # Implementing chaotic map for enhanced exploration\n            chaos_sequence = np.random.uniform(size=positions.shape)\n            positions = np.clip(positions * (1 + 0.1 * np.sin(2 * np.pi * chaos_sequence)), self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n\n        return global_best_score, global_best_position\n# ```"}
{"role": "LLaMEA", "time": "2024-11-09 16:17:01.717905", "content": "You are a highly skilled computer scientist in the field of natural computing. Your task is to design novel metaheuristic algorithms to solve black box optimization problems.\n\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the BBOB test suite of 24 noiseless functions. Your task is to write the optimization algorithm in Python code. The code should contain an `__init__(self, budget, dim)` function and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. Each of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nGive an excellent and novel heuristic algorithm to solve this task and also give it a one-line description with the main idea.\n\nThe current population of algorithms already evaluated (name, description, score) is:\nStochasticAdaptiveParticleOptimizer: An improved stochastic adaptive particle optimizer with dynamic velocity clamping and perturbation for enhanced convergence speed. (Score: 0.3304703293497984)\n\nThe selected solution to update is:\nAn improved stochastic adaptive particle optimizer with dynamic velocity clamping and perturbation for enhanced convergence speed.\n\nWith code:\nimport numpy as np\n\nclass StochasticAdaptiveParticleOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(25, self.budget // 2)\n        self.inertia_weight = 0.7  # Adjust inertia to balance exploration/exploitation\n        self.cognitive_coeff = 1.8  # Tweaked coefficients for balanced personal/global search\n        self.social_coeff = 1.7\n        self.adaptive_scaling = 0.85  # Slightly more aggressive adaptive scaling\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-0.5, 0.5, (self.population_size, self.dim))  # Reduced initial velocity range\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        velocity_clamp = (self.upper_bound - self.lower_bound) * 0.1  # Dynamic velocity clamp\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            \n            # Velocity clamping to prevent explosions\n            velocities = np.clip(velocities, -velocity_clamp, velocity_clamp)\n            positions += velocities\n            \n            if eval_count % (self.budget // 5) == 0:  # Adaptive adjustment\n                self.inertia_weight *= self.adaptive_scaling\n                perturbation = np.random.normal(0, 0.1, positions.shape)  # Introduce perturbation\n                positions += perturbation\n            \n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n\n        return global_best_score, global_best_position\n\nThe algorithm StochasticAdaptiveParticleOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.33 with standard deviation 0.28.\n\n\nPlease transform this optimization algorithm code by exactly 20.0% in a \nway that enhances convergence speed. Keep the code difference precisely at \n20.0%, and focus solely on achieving performance improvements through \nalgorithmic adjustments across various elements of the code. Avoid focusing on \ncode efficiency; instead, prioritize exploration of diverse approaches within \nthe allowed modification percentage.\n\n\nProvide the Python code and a one-line description with the main idea (without enters). Give the response in the format:\n# Description: <short-description>\n# Code: \n# ```python\n# <code>\n# ```\n# \n"}
{"role": "gpt-4o", "time": "2024-11-09 16:17:10.811089", "content": "# Description: A stochastic adaptive particle optimizer with enhanced social influence and dynamic inertia for faster convergence.\n# Code: \n# ```python\nimport numpy as np\n\nclass StochasticAdaptiveParticleOptimizerEnhanced:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(30, self.budget // 2)  # Slightly increased population size\n        self.inertia_weight = 0.9  # Increased initial inertia for more exploration\n        self.cognitive_coeff = 1.5  # Adjusted for better convergence balance\n        self.social_coeff = 2.0  # Increased social component for more influence\n        self.adaptive_scaling = 0.90  # Less aggressive decay on inertia\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-0.3, 0.3, (self.population_size, self.dim))  # Narrowed initial velocity range\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        velocity_clamp = (self.upper_bound - self.lower_bound) * 0.2  # Increased velocity clamp\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            \n            velocities = np.clip(velocities, -velocity_clamp, velocity_clamp)\n            positions += velocities\n            \n            if eval_count % (self.budget // 4) == 0:  # More frequent adaptive adjustment\n                self.inertia_weight *= self.adaptive_scaling\n                perturbation = np.random.normal(0, 0.2, positions.shape)  # Increased perturbation\n                positions += perturbation\n            \n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n\n        return global_best_score, global_best_position\n# ```"}
{"role": "LLaMEA", "time": "2024-11-09 16:17:10.814733", "content": "You are a highly skilled computer scientist in the field of natural computing. Your task is to design novel metaheuristic algorithms to solve black box optimization problems.\n\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the BBOB test suite of 24 noiseless functions. Your task is to write the optimization algorithm in Python code. The code should contain an `__init__(self, budget, dim)` function and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. Each of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nGive an excellent and novel heuristic algorithm to solve this task and also give it a one-line description with the main idea.\n\nThe current population of algorithms already evaluated (name, description, score) is:\nStochasticAdaptiveParticleOptimizer: An improved stochastic adaptive particle optimizer with dynamic velocity clamping and perturbation for enhanced convergence speed. (Score: 0.3304703293497984)\n\nThe selected solution to update is:\nAn improved stochastic adaptive particle optimizer with dynamic velocity clamping and perturbation for enhanced convergence speed.\n\nWith code:\nimport numpy as np\n\nclass StochasticAdaptiveParticleOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(25, self.budget // 2)\n        self.inertia_weight = 0.7  # Adjust inertia to balance exploration/exploitation\n        self.cognitive_coeff = 1.8  # Tweaked coefficients for balanced personal/global search\n        self.social_coeff = 1.7\n        self.adaptive_scaling = 0.85  # Slightly more aggressive adaptive scaling\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-0.5, 0.5, (self.population_size, self.dim))  # Reduced initial velocity range\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        velocity_clamp = (self.upper_bound - self.lower_bound) * 0.1  # Dynamic velocity clamp\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            \n            # Velocity clamping to prevent explosions\n            velocities = np.clip(velocities, -velocity_clamp, velocity_clamp)\n            positions += velocities\n            \n            if eval_count % (self.budget // 5) == 0:  # Adaptive adjustment\n                self.inertia_weight *= self.adaptive_scaling\n                perturbation = np.random.normal(0, 0.1, positions.shape)  # Introduce perturbation\n                positions += perturbation\n            \n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n\n        return global_best_score, global_best_position\n\nThe algorithm StochasticAdaptiveParticleOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.33 with standard deviation 0.28.\n\n\nPlease transform this optimization algorithm code by exactly 20.0% in a \nway that enhances convergence speed. Keep the code difference precisely at \n20.0%, and focus solely on achieving performance improvements through \nalgorithmic adjustments across various elements of the code. Avoid focusing on \ncode efficiency; instead, prioritize exploration of diverse approaches within \nthe allowed modification percentage.\n\n\nProvide the Python code and a one-line description with the main idea (without enters). Give the response in the format:\n# Description: <short-description>\n# Code: \n# ```python\n# <code>\n# ```\n# \n"}
{"role": "gpt-4o", "time": "2024-11-09 16:17:23.166882", "content": "# Description: Enhanced particle optimizer with mutation-based exploration and adaptive learning rates for improved convergence.\n# Code:\n# ```python\nimport numpy as np\n\nclass MutationBasedParticleOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(30, self.budget // 3)  # Adjusted population size\n        self.inertia_weight = 0.6  # Adjusted inertia for adaptive behavior\n        self.cognitive_coeff = 2.0  # Enhanced personal component\n        self.social_coeff = 1.9  # Enhanced global component\n        self.adaptive_scaling = 0.9  # New adaptive scaling\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-0.6, 0.6, (self.population_size, self.dim))  # Adjusted initial velocity range\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        velocity_clamp = (self.upper_bound - self.lower_bound) * 0.2  # Enhanced velocity clamp\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            \n            # Velocity clamping to prevent explosions\n            velocities = np.clip(velocities, -velocity_clamp, velocity_clamp)\n            positions += velocities\n            \n            if eval_count % (self.budget // 4) == 0:  # Adaptive adjustment\n                self.inertia_weight *= self.adaptive_scaling\n                mutation = np.random.uniform(-0.1, 0.1, positions.shape)  # Mutation for exploration\n                positions += mutation\n            \n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n\n        return global_best_score, global_best_position\n# ```"}
{"role": "LLaMEA", "time": "2024-11-09 16:17:23.170577", "content": "You are a highly skilled computer scientist in the field of natural computing. Your task is to design novel metaheuristic algorithms to solve black box optimization problems.\n\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the BBOB test suite of 24 noiseless functions. Your task is to write the optimization algorithm in Python code. The code should contain an `__init__(self, budget, dim)` function and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. Each of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nGive an excellent and novel heuristic algorithm to solve this task and also give it a one-line description with the main idea.\n\nThe current population of algorithms already evaluated (name, description, score) is:\nStochasticAdaptiveParticleOptimizer: An improved stochastic adaptive particle optimizer with dynamic velocity clamping and perturbation for enhanced convergence speed. (Score: 0.3304703293497984)\n\nThe selected solution to update is:\nAn improved stochastic adaptive particle optimizer with dynamic velocity clamping and perturbation for enhanced convergence speed.\n\nWith code:\nimport numpy as np\n\nclass StochasticAdaptiveParticleOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(25, self.budget // 2)\n        self.inertia_weight = 0.7  # Adjust inertia to balance exploration/exploitation\n        self.cognitive_coeff = 1.8  # Tweaked coefficients for balanced personal/global search\n        self.social_coeff = 1.7\n        self.adaptive_scaling = 0.85  # Slightly more aggressive adaptive scaling\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-0.5, 0.5, (self.population_size, self.dim))  # Reduced initial velocity range\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        velocity_clamp = (self.upper_bound - self.lower_bound) * 0.1  # Dynamic velocity clamp\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            \n            # Velocity clamping to prevent explosions\n            velocities = np.clip(velocities, -velocity_clamp, velocity_clamp)\n            positions += velocities\n            \n            if eval_count % (self.budget // 5) == 0:  # Adaptive adjustment\n                self.inertia_weight *= self.adaptive_scaling\n                perturbation = np.random.normal(0, 0.1, positions.shape)  # Introduce perturbation\n                positions += perturbation\n            \n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n\n        return global_best_score, global_best_position\n\nThe algorithm StochasticAdaptiveParticleOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.33 with standard deviation 0.28.\n\n\nPlease transform this optimization algorithm code by exactly 20.0% in a \nway that enhances convergence speed. Keep the code difference precisely at \n20.0%, and focus solely on achieving performance improvements through \nalgorithmic adjustments across various elements of the code. Avoid focusing on \ncode efficiency; instead, prioritize exploration of diverse approaches within \nthe allowed modification percentage.\n\n\nProvide the Python code and a one-line description with the main idea (without enters). Give the response in the format:\n# Description: <short-description>\n# Code: \n# ```python\n# <code>\n# ```\n# \n"}
{"role": "gpt-4o", "time": "2024-11-09 16:17:35.039998", "content": "Certainly! Below is an updated version of the optimization algorithm that incorporates a 20% transformation by enhancing the convergence speed through strategic algorithmic adjustments.\n\n# Description: A refined particle swarm optimizer with dynamic acceleration coefficients and adaptive neighborhood-based velocity adjustment for improved convergence.\n# Code: \n```python\nimport numpy as np\n\nclass RefinedParticleOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(30, self.budget // 2)  # Increased population size for better diversity\n        self.inertia_weight = 0.6  # Decreased inertia weight for faster convergence\n        self.cognitive_coeff = 1.5  \n        self.social_coeff = 1.5  # Adjusted coefficients for more balanced search\n        self.adaptive_scaling = 0.9  # More aggressive adaptive scaling\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-0.3, 0.3, (self.population_size, self.dim))  # Reduced velocity range\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        velocity_clamp = (self.upper_bound - self.lower_bound) * 0.15  # Adjusted dynamic velocity clamp\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            \n            velocities = np.clip(velocities, -velocity_clamp, velocity_clamp)\n            positions += velocities\n            \n            if eval_count % (self.budget // 4) == 0:  # More frequent adaptive adjustment\n                self.inertia_weight *= self.adaptive_scaling\n                perturbation = np.random.normal(0, 0.05, positions.shape)  # Lower perturbation for fine-tuning\n                positions += perturbation\n            \n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n                \n        return global_best_score, global_best_position\n```\n\nThis version introduces changes such as increasing the population size, adjusting inertia weight and coefficients, and more frequent adaptive adjustments, while ensuring that the modifications remain within 20% of the original algorithm."}
{"role": "LLaMEA", "time": "2024-11-09 16:18:32.444987", "content": "You are a highly skilled computer scientist in the field of natural computing. Your task is to design novel metaheuristic algorithms to solve black box optimization problems.\n\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the BBOB test suite of 24 noiseless functions. Your task is to write the optimization algorithm in Python code. The code should contain an `__init__(self, budget, dim)` function and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. Each of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nGive an excellent and novel heuristic algorithm to solve this task and also give it a one-line description with the main idea.\n\nThe current population of algorithms already evaluated (name, description, score) is:\nRefinedParticleOptimizer: A refined particle swarm optimizer with dynamic acceleration coefficients and adaptive neighborhood-based velocity adjustment for improved convergence. (Score: 0.3490816234226132)\n\nThe selected solution to update is:\nA refined particle swarm optimizer with dynamic acceleration coefficients and adaptive neighborhood-based velocity adjustment for improved convergence.\n\nWith code:\nimport numpy as np\n\nclass RefinedParticleOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(30, self.budget // 2)  # Increased population size for better diversity\n        self.inertia_weight = 0.6  # Decreased inertia weight for faster convergence\n        self.cognitive_coeff = 1.5  \n        self.social_coeff = 1.5  # Adjusted coefficients for more balanced search\n        self.adaptive_scaling = 0.9  # More aggressive adaptive scaling\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-0.3, 0.3, (self.population_size, self.dim))  # Reduced velocity range\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        velocity_clamp = (self.upper_bound - self.lower_bound) * 0.15  # Adjusted dynamic velocity clamp\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            \n            velocities = np.clip(velocities, -velocity_clamp, velocity_clamp)\n            positions += velocities\n            \n            if eval_count % (self.budget // 4) == 0:  # More frequent adaptive adjustment\n                self.inertia_weight *= self.adaptive_scaling\n                perturbation = np.random.normal(0, 0.05, positions.shape)  # Lower perturbation for fine-tuning\n                positions += perturbation\n            \n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n                \n        return global_best_score, global_best_position\n\nThe algorithm RefinedParticleOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.35 with standard deviation 0.30.\n\n\nPlease transform this optimization algorithm code by exactly 20.0% in a \nway that enhances convergence speed. Keep the code difference precisely at \n20.0%, and focus solely on achieving performance improvements through \nalgorithmic adjustments across various elements of the code. Avoid focusing on \ncode efficiency; instead, prioritize exploration of diverse approaches within \nthe allowed modification percentage.\n\n\nProvide the Python code and a one-line description with the main idea (without enters). Give the response in the format:\n# Description: <short-description>\n# Code: \n# ```python\n# <code>\n# ```\n# \n"}
{"role": "gpt-4o", "time": "2024-11-09 16:18:41.650368", "content": "# Description: An enhanced particle swarm optimizer with improved neighborhood exploration and dynamic inertia adjustment for faster convergence.\n# Code: \n# ```python\nimport numpy as np\n\nclass EnhancedParticleOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(40, self.budget // 2)  # Increased population size for better diversity\n        self.inertia_weight = 0.8  # Increased inertia weight for better exploration\n        self.cognitive_coeff = 1.4  # Slightly reduced cognitive coefficient to enhance group behavior\n        self.social_coeff = 1.6  # Slightly increased social coefficient for better social learning\n        self.adaptive_scaling = 0.95  # Less aggressive adaptive scaling for stability\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-0.4, 0.4, (self.population_size, self.dim))  # Expanded velocity range\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        velocity_clamp = (self.upper_bound - self.lower_bound) * 0.2  # Adjusted dynamic velocity clamp\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            \n            velocities = np.clip(velocities, -velocity_clamp, velocity_clamp)\n            positions += velocities\n            \n            if eval_count % (self.budget // 5) == 0:  # More frequent adaptive adjustment\n                self.inertia_weight *= self.adaptive_scaling\n                perturbation = np.random.normal(0, 0.1, positions.shape)  # Increased perturbation for exploration\n                positions += perturbation\n            \n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n                \n        return global_best_score, global_best_position\n# ```"}
{"role": "LLaMEA", "time": "2024-11-09 16:18:41.654295", "content": "You are a highly skilled computer scientist in the field of natural computing. Your task is to design novel metaheuristic algorithms to solve black box optimization problems.\n\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the BBOB test suite of 24 noiseless functions. Your task is to write the optimization algorithm in Python code. The code should contain an `__init__(self, budget, dim)` function and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. Each of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nGive an excellent and novel heuristic algorithm to solve this task and also give it a one-line description with the main idea.\n\nThe current population of algorithms already evaluated (name, description, score) is:\nRefinedParticleOptimizer: A refined particle swarm optimizer with dynamic acceleration coefficients and adaptive neighborhood-based velocity adjustment for improved convergence. (Score: 0.3490816234226132)\n\nThe selected solution to update is:\nA refined particle swarm optimizer with dynamic acceleration coefficients and adaptive neighborhood-based velocity adjustment for improved convergence.\n\nWith code:\nimport numpy as np\n\nclass RefinedParticleOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(30, self.budget // 2)  # Increased population size for better diversity\n        self.inertia_weight = 0.6  # Decreased inertia weight for faster convergence\n        self.cognitive_coeff = 1.5  \n        self.social_coeff = 1.5  # Adjusted coefficients for more balanced search\n        self.adaptive_scaling = 0.9  # More aggressive adaptive scaling\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-0.3, 0.3, (self.population_size, self.dim))  # Reduced velocity range\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        velocity_clamp = (self.upper_bound - self.lower_bound) * 0.15  # Adjusted dynamic velocity clamp\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            \n            velocities = np.clip(velocities, -velocity_clamp, velocity_clamp)\n            positions += velocities\n            \n            if eval_count % (self.budget // 4) == 0:  # More frequent adaptive adjustment\n                self.inertia_weight *= self.adaptive_scaling\n                perturbation = np.random.normal(0, 0.05, positions.shape)  # Lower perturbation for fine-tuning\n                positions += perturbation\n            \n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n                \n        return global_best_score, global_best_position\n\nThe algorithm RefinedParticleOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.35 with standard deviation 0.30.\n\n\nPlease transform this optimization algorithm code by exactly 20.0% in a \nway that enhances convergence speed. Keep the code difference precisely at \n20.0%, and focus solely on achieving performance improvements through \nalgorithmic adjustments across various elements of the code. Avoid focusing on \ncode efficiency; instead, prioritize exploration of diverse approaches within \nthe allowed modification percentage.\n\n\nProvide the Python code and a one-line description with the main idea (without enters). Give the response in the format:\n# Description: <short-description>\n# Code: \n# ```python\n# <code>\n# ```\n# \n"}
{"role": "gpt-4o", "time": "2024-11-09 16:18:51.551259", "content": "# Description: Enhanced particle swarm optimizer with adaptive learning rates and dynamic neighborhood structures for improved convergence speed.\n# Code:\n# ```python\nimport numpy as np\n\nclass EnhancedParticleOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(40, self.budget // 3)  # Increased population size for better exploration\n        self.inertia_weight = 0.7  # Slightly higher inertia weight for improved exploration\n        self.cognitive_coeff = 1.2  # Reduced cognitive coefficient to focus on exploration\n        self.social_coeff = 1.8  # Increased social coefficient for stronger convergence\n        self.adaptive_scaling = 0.95  # More gradual adaptive scaling\n        self.learning_rate = 0.1  # Introduced adaptive learning rate\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-0.3, 0.3, (self.population_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        velocity_clamp = (self.upper_bound - self.lower_bound) * 0.2  # Increased dynamic velocity clamp\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            \n            velocities = np.clip(velocities, -velocity_clamp, velocity_clamp)\n            positions += self.learning_rate * velocities  # Apply adaptive learning rate\n            \n            if eval_count % (self.budget // 5) == 0:  # More frequent adaptive adjustment\n                self.inertia_weight *= self.adaptive_scaling\n                self.learning_rate *= self.adaptive_scaling  # Adjust learning rate dynamically\n                perturbation = np.random.normal(0, 0.1, positions.shape)  # Adjusted perturbation for exploration\n                positions += perturbation\n            \n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n                \n        return global_best_score, global_best_position\n# ```"}
{"role": "LLaMEA", "time": "2024-11-09 16:18:51.554885", "content": "You are a highly skilled computer scientist in the field of natural computing. Your task is to design novel metaheuristic algorithms to solve black box optimization problems.\n\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the BBOB test suite of 24 noiseless functions. Your task is to write the optimization algorithm in Python code. The code should contain an `__init__(self, budget, dim)` function and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. Each of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nGive an excellent and novel heuristic algorithm to solve this task and also give it a one-line description with the main idea.\n\nThe current population of algorithms already evaluated (name, description, score) is:\nRefinedParticleOptimizer: A refined particle swarm optimizer with dynamic acceleration coefficients and adaptive neighborhood-based velocity adjustment for improved convergence. (Score: 0.3490816234226132)\n\nThe selected solution to update is:\nA refined particle swarm optimizer with dynamic acceleration coefficients and adaptive neighborhood-based velocity adjustment for improved convergence.\n\nWith code:\nimport numpy as np\n\nclass RefinedParticleOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(30, self.budget // 2)  # Increased population size for better diversity\n        self.inertia_weight = 0.6  # Decreased inertia weight for faster convergence\n        self.cognitive_coeff = 1.5  \n        self.social_coeff = 1.5  # Adjusted coefficients for more balanced search\n        self.adaptive_scaling = 0.9  # More aggressive adaptive scaling\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-0.3, 0.3, (self.population_size, self.dim))  # Reduced velocity range\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        velocity_clamp = (self.upper_bound - self.lower_bound) * 0.15  # Adjusted dynamic velocity clamp\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            \n            velocities = np.clip(velocities, -velocity_clamp, velocity_clamp)\n            positions += velocities\n            \n            if eval_count % (self.budget // 4) == 0:  # More frequent adaptive adjustment\n                self.inertia_weight *= self.adaptive_scaling\n                perturbation = np.random.normal(0, 0.05, positions.shape)  # Lower perturbation for fine-tuning\n                positions += perturbation\n            \n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n                \n        return global_best_score, global_best_position\n\nThe algorithm RefinedParticleOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.35 with standard deviation 0.30.\n\n\nPlease transform this optimization algorithm code by exactly 20.0% in a \nway that enhances convergence speed. Keep the code difference precisely at \n20.0%, and focus solely on achieving performance improvements through \nalgorithmic adjustments across various elements of the code. Avoid focusing on \ncode efficiency; instead, prioritize exploration of diverse approaches within \nthe allowed modification percentage.\n\n\nProvide the Python code and a one-line description with the main idea (without enters). Give the response in the format:\n# Description: <short-description>\n# Code: \n# ```python\n# <code>\n# ```\n# \n"}
{"role": "gpt-4o", "time": "2024-11-09 16:19:00.336727", "content": "# Description: An adaptive particle swarm optimizer with dynamic neighborhood exploration and enhanced diversity for faster convergence.\n# Code:\n# ```python\nimport numpy as np\n\nclass AdaptiveParticleOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(40, self.budget // 2)  # Slightly increased population size\n        self.inertia_weight = 0.5  # Further decreased inertia weight for quicker convergence\n        self.cognitive_coeff = 1.6  # Slightly increased cognitive coefficient\n        self.social_coeff = 1.4  # Slightly decreased social coefficient\n        self.adaptive_scaling = 0.85  # More aggressive adaptive scaling\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-0.3, 0.3, (self.population_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        velocity_clamp = (self.upper_bound - self.lower_bound) * 0.2  # Increased dynamic velocity clamp\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            \n            velocities = np.clip(velocities, -velocity_clamp, velocity_clamp)\n            positions += velocities\n            \n            if eval_count % (self.budget // 5) == 0:  # More frequent adaptive adjustment\n                self.inertia_weight *= self.adaptive_scaling\n                perturbation = np.random.normal(0, 0.1, positions.shape)  # Increased perturbation for exploration\n                positions += perturbation\n            \n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n                \n        return global_best_score, global_best_position\n# ```"}
{"role": "LLaMEA", "time": "2024-11-09 16:19:00.340680", "content": "You are a highly skilled computer scientist in the field of natural computing. Your task is to design novel metaheuristic algorithms to solve black box optimization problems.\n\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the BBOB test suite of 24 noiseless functions. Your task is to write the optimization algorithm in Python code. The code should contain an `__init__(self, budget, dim)` function and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. Each of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nGive an excellent and novel heuristic algorithm to solve this task and also give it a one-line description with the main idea.\n\nThe current population of algorithms already evaluated (name, description, score) is:\nRefinedParticleOptimizer: A refined particle swarm optimizer with dynamic acceleration coefficients and adaptive neighborhood-based velocity adjustment for improved convergence. (Score: 0.3490816234226132)\n\nThe selected solution to update is:\nA refined particle swarm optimizer with dynamic acceleration coefficients and adaptive neighborhood-based velocity adjustment for improved convergence.\n\nWith code:\nimport numpy as np\n\nclass RefinedParticleOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(30, self.budget // 2)  # Increased population size for better diversity\n        self.inertia_weight = 0.6  # Decreased inertia weight for faster convergence\n        self.cognitive_coeff = 1.5  \n        self.social_coeff = 1.5  # Adjusted coefficients for more balanced search\n        self.adaptive_scaling = 0.9  # More aggressive adaptive scaling\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-0.3, 0.3, (self.population_size, self.dim))  # Reduced velocity range\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        velocity_clamp = (self.upper_bound - self.lower_bound) * 0.15  # Adjusted dynamic velocity clamp\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            \n            velocities = np.clip(velocities, -velocity_clamp, velocity_clamp)\n            positions += velocities\n            \n            if eval_count % (self.budget // 4) == 0:  # More frequent adaptive adjustment\n                self.inertia_weight *= self.adaptive_scaling\n                perturbation = np.random.normal(0, 0.05, positions.shape)  # Lower perturbation for fine-tuning\n                positions += perturbation\n            \n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n                \n        return global_best_score, global_best_position\n\nThe algorithm RefinedParticleOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.35 with standard deviation 0.30.\n\n\nPlease transform this optimization algorithm code by exactly 20.0% in a \nway that enhances convergence speed. Keep the code difference precisely at \n20.0%, and focus solely on achieving performance improvements through \nalgorithmic adjustments across various elements of the code. Avoid focusing on \ncode efficiency; instead, prioritize exploration of diverse approaches within \nthe allowed modification percentage.\n\n\nProvide the Python code and a one-line description with the main idea (without enters). Give the response in the format:\n# Description: <short-description>\n# Code: \n# ```python\n# <code>\n# ```\n# \n"}
{"role": "gpt-4o", "time": "2024-11-09 16:19:09.955726", "content": "# Description: Enhanced Particle Swarm Optimizer with chaotic maps for initialization and non-linear inertia weight adaptation to improve convergence.\n# Code: \n```python\nimport numpy as np\nimport chaotic_maps  # Assuming a library for generating chaotic sequences\n\nclass EnhancedParticleOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(30, self.budget // 2)\n        self.inertia_weight = 0.9  # Start with higher inertia and adapt non-linearly\n        self.inertia_weight_min = 0.4\n        self.cognitive_coeff = 2.0  # Slightly increased for stronger focus on individual learning\n        self.social_coeff = 1.3 \n        self.adaptive_scaling = 0.95\n\n    def __call__(self, func):\n        chaotic_sequence = chaotic_maps.logistic_map(self.population_size * self.dim, 3.9)  # Logistic map for diversity\n        positions = self.lower_bound + (self.upper_bound - self.lower_bound) * chaotic_sequence.reshape(self.population_size, self.dim)\n        velocities = np.random.uniform(-0.5, 0.5, (self.population_size, self.dim))  # Expanded initial velocity range\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        velocity_clamp = (self.upper_bound - self.lower_bound) * 0.2\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            \n            velocities = np.clip(velocities, -velocity_clamp, velocity_clamp)\n            positions += velocities\n\n            self.inertia_weight = max(self.inertia_weight_min, self.inertia_weight * self.adaptive_scaling)\n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n                \n        return global_best_score, global_best_position\n```"}
{"role": "LLaMEA", "time": "2024-11-09 16:19:09.961092", "content": "You are a highly skilled computer scientist in the field of natural computing. Your task is to design novel metaheuristic algorithms to solve black box optimization problems.\n\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the BBOB test suite of 24 noiseless functions. Your task is to write the optimization algorithm in Python code. The code should contain an `__init__(self, budget, dim)` function and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. Each of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nGive an excellent and novel heuristic algorithm to solve this task and also give it a one-line description with the main idea.\n\nThe current population of algorithms already evaluated (name, description, score) is:\nRefinedParticleOptimizer: A refined particle swarm optimizer with dynamic acceleration coefficients and adaptive neighborhood-based velocity adjustment for improved convergence. (Score: 0.3490816234226132)\n\nThe selected solution to update is:\nA refined particle swarm optimizer with dynamic acceleration coefficients and adaptive neighborhood-based velocity adjustment for improved convergence.\n\nWith code:\nimport numpy as np\n\nclass RefinedParticleOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(30, self.budget // 2)  # Increased population size for better diversity\n        self.inertia_weight = 0.6  # Decreased inertia weight for faster convergence\n        self.cognitive_coeff = 1.5  \n        self.social_coeff = 1.5  # Adjusted coefficients for more balanced search\n        self.adaptive_scaling = 0.9  # More aggressive adaptive scaling\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-0.3, 0.3, (self.population_size, self.dim))  # Reduced velocity range\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        velocity_clamp = (self.upper_bound - self.lower_bound) * 0.15  # Adjusted dynamic velocity clamp\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            \n            velocities = np.clip(velocities, -velocity_clamp, velocity_clamp)\n            positions += velocities\n            \n            if eval_count % (self.budget // 4) == 0:  # More frequent adaptive adjustment\n                self.inertia_weight *= self.adaptive_scaling\n                perturbation = np.random.normal(0, 0.05, positions.shape)  # Lower perturbation for fine-tuning\n                positions += perturbation\n            \n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n                \n        return global_best_score, global_best_position\n\nThe algorithm RefinedParticleOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.35 with standard deviation 0.30.\n\n\nPlease transform this optimization algorithm code by exactly 20.0% in a \nway that enhances convergence speed. Keep the code difference precisely at \n20.0%, and focus solely on achieving performance improvements through \nalgorithmic adjustments across various elements of the code. Avoid focusing on \ncode efficiency; instead, prioritize exploration of diverse approaches within \nthe allowed modification percentage.\n\n\nProvide the Python code and a one-line description with the main idea (without enters). Give the response in the format:\n# Description: <short-description>\n# Code: \n# ```python\n# <code>\n# ```\n# \n"}
{"role": "gpt-4o", "time": "2024-11-09 16:19:19.611553", "content": "\n# Description: An enhanced particle swarm optimizer using adaptive inertia and mutation strategies for faster convergence.\n# Code: \n```python\nimport numpy as np\n\nclass EnhancedParticleOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(30, self.budget // 3)  # Increased initial diversity\n        self.inertia_weight = 0.8  # Adaptive inertia for better exploration-exploitation balance\n        self.cognitive_coeff = 2.0  # Enhanced cognitive component for individual learning\n        self.social_coeff = 1.3  # Adjusted social factor for collective learning\n        self.adaptive_scaling = 0.95  # Smoother adaptive scaling\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-0.5, 0.5, (self.population_size, self.dim))  # Broader initial velocity range\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        velocity_clamp = (self.upper_bound - self.lower_bound) * 0.20  # Dynamic velocity clamp heightened\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            \n            velocities = np.clip(velocities, -velocity_clamp, velocity_clamp)\n            positions += velocities\n            \n            if eval_count % (self.budget // 8) == 0:  # More frequent inertia adjustment\n                self.inertia_weight *= self.adaptive_scaling\n                mutation = np.random.normal(0, 0.1, positions.shape)  # Slightly increased mutation for exploration\n                positions += mutation\n            \n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n                \n        return global_best_score, global_best_position\n```"}
{"role": "LLaMEA", "time": "2024-11-09 16:20:17.114273", "content": "You are a highly skilled computer scientist in the field of natural computing. Your task is to design novel metaheuristic algorithms to solve black box optimization problems.\n\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the BBOB test suite of 24 noiseless functions. Your task is to write the optimization algorithm in Python code. The code should contain an `__init__(self, budget, dim)` function and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. Each of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nGive an excellent and novel heuristic algorithm to solve this task and also give it a one-line description with the main idea.\n\nThe current population of algorithms already evaluated (name, description, score) is:\nRefinedParticleOptimizer: A refined particle swarm optimizer with dynamic acceleration coefficients and adaptive neighborhood-based velocity adjustment for improved convergence. (Score: 0.3490816234226132)\n\nThe selected solution to update is:\nA refined particle swarm optimizer with dynamic acceleration coefficients and adaptive neighborhood-based velocity adjustment for improved convergence.\n\nWith code:\nimport numpy as np\n\nclass RefinedParticleOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(30, self.budget // 2)  # Increased population size for better diversity\n        self.inertia_weight = 0.6  # Decreased inertia weight for faster convergence\n        self.cognitive_coeff = 1.5  \n        self.social_coeff = 1.5  # Adjusted coefficients for more balanced search\n        self.adaptive_scaling = 0.9  # More aggressive adaptive scaling\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-0.3, 0.3, (self.population_size, self.dim))  # Reduced velocity range\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        velocity_clamp = (self.upper_bound - self.lower_bound) * 0.15  # Adjusted dynamic velocity clamp\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            \n            velocities = np.clip(velocities, -velocity_clamp, velocity_clamp)\n            positions += velocities\n            \n            if eval_count % (self.budget // 4) == 0:  # More frequent adaptive adjustment\n                self.inertia_weight *= self.adaptive_scaling\n                perturbation = np.random.normal(0, 0.05, positions.shape)  # Lower perturbation for fine-tuning\n                positions += perturbation\n            \n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n                \n        return global_best_score, global_best_position\n\nThe algorithm RefinedParticleOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.35 with standard deviation 0.30.\n\n\nPlease transform this optimization algorithm code by exactly 20.0% in a \nway that enhances convergence speed. Keep the code difference precisely at \n20.0%, and focus solely on achieving performance improvements through \nalgorithmic adjustments across various elements of the code. Avoid focusing on \ncode efficiency; instead, prioritize exploration of diverse approaches within \nthe allowed modification percentage.\n\n\nProvide the Python code and a one-line description with the main idea (without enters). Give the response in the format:\n# Description: <short-description>\n# Code: \n# ```python\n# <code>\n# ```\n# \n"}
{"role": "gpt-4o", "time": "2024-11-09 16:20:27.131978", "content": "# Description: An enhanced particle swarm optimizer that incorporates chaotic initialization and dynamic inertia adaptation for expedited convergence.\n# Code: \n# ```python\nimport numpy as np\n\nclass EnhancedParticleOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(35, self.budget // 2)\n        self.inertia_weight = 0.9\n        self.cognitive_coeff = 2.0\n        self.social_coeff = 2.0\n        self.adaptive_scaling = 0.85\n\n    def chaotic_initialization(self, size, dim):\n        return self.lower_bound + (self.upper_bound - self.lower_bound) * np.random.random((size, dim))\n    \n    def __call__(self, func):\n        positions = self.chaotic_initialization(self.population_size, self.dim)\n        velocities = np.random.uniform(-0.3, 0.3, (self.population_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        velocity_clamp = (self.upper_bound - self.lower_bound) * 0.1\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            \n            velocities = np.clip(velocities, -velocity_clamp, velocity_clamp)\n            positions += velocities\n            \n            if eval_count % (self.budget // 3) == 0:\n                self.inertia_weight *= self.adaptive_scaling\n                perturbation = np.random.normal(0, 0.1, positions.shape)\n                positions += perturbation\n            \n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n                \n        return global_best_score, global_best_position\n# ```"}
{"role": "LLaMEA", "time": "2024-11-09 16:20:27.135559", "content": "You are a highly skilled computer scientist in the field of natural computing. Your task is to design novel metaheuristic algorithms to solve black box optimization problems.\n\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the BBOB test suite of 24 noiseless functions. Your task is to write the optimization algorithm in Python code. The code should contain an `__init__(self, budget, dim)` function and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. Each of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nGive an excellent and novel heuristic algorithm to solve this task and also give it a one-line description with the main idea.\n\nThe current population of algorithms already evaluated (name, description, score) is:\nRefinedParticleOptimizer: A refined particle swarm optimizer with dynamic acceleration coefficients and adaptive neighborhood-based velocity adjustment for improved convergence. (Score: 0.3490816234226132)\n\nThe selected solution to update is:\nA refined particle swarm optimizer with dynamic acceleration coefficients and adaptive neighborhood-based velocity adjustment for improved convergence.\n\nWith code:\nimport numpy as np\n\nclass RefinedParticleOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(30, self.budget // 2)  # Increased population size for better diversity\n        self.inertia_weight = 0.6  # Decreased inertia weight for faster convergence\n        self.cognitive_coeff = 1.5  \n        self.social_coeff = 1.5  # Adjusted coefficients for more balanced search\n        self.adaptive_scaling = 0.9  # More aggressive adaptive scaling\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-0.3, 0.3, (self.population_size, self.dim))  # Reduced velocity range\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        velocity_clamp = (self.upper_bound - self.lower_bound) * 0.15  # Adjusted dynamic velocity clamp\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            \n            velocities = np.clip(velocities, -velocity_clamp, velocity_clamp)\n            positions += velocities\n            \n            if eval_count % (self.budget // 4) == 0:  # More frequent adaptive adjustment\n                self.inertia_weight *= self.adaptive_scaling\n                perturbation = np.random.normal(0, 0.05, positions.shape)  # Lower perturbation for fine-tuning\n                positions += perturbation\n            \n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n                \n        return global_best_score, global_best_position\n\nThe algorithm RefinedParticleOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.35 with standard deviation 0.30.\n\n\nPlease transform this optimization algorithm code by exactly 20.0% in a \nway that enhances convergence speed. Keep the code difference precisely at \n20.0%, and focus solely on achieving performance improvements through \nalgorithmic adjustments across various elements of the code. Avoid focusing on \ncode efficiency; instead, prioritize exploration of diverse approaches within \nthe allowed modification percentage.\n\n\nProvide the Python code and a one-line description with the main idea (without enters). Give the response in the format:\n# Description: <short-description>\n# Code: \n# ```python\n# <code>\n# ```\n# \n"}
{"role": "gpt-4o", "time": "2024-11-09 16:20:38.807515", "content": "# Description: A refined particle swarm optimizer with adaptive inertia and social interaction enhancement for accelerated convergence.\n# Code:\n# ```python\nimport numpy as np\n\nclass AdaptedParticleOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(40, self.budget // 2)  # Slightly increased population size for diversity\n        self.inertia_weight = 0.7  # Adaptive inertia weight for dynamic exploration-exploitation balance\n        self.cognitive_coeff = 1.5  \n        self.social_coeff = 1.8  # Enhanced social component to accelerate convergence\n        self.adaptive_scaling = 0.95  # Less aggressive scaling for gradual adjustment\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-0.2, 0.2, (self.population_size, self.dim))  # Further reduced velocity range\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        velocity_clamp = (self.upper_bound - self.lower_bound) * 0.1  # Further adjusted velocity clamp\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            \n            velocities = np.clip(velocities, -velocity_clamp, velocity_clamp)\n            positions += velocities\n            \n            if eval_count % (self.budget // 2) == 0:  # Adjusted adaptive adjustment frequency\n                self.inertia_weight *= self.adaptive_scaling\n                perturbation = np.random.normal(0, 0.05, positions.shape)  # Consistent perturbation for stability\n                positions += perturbation\n            \n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n                \n        return global_best_score, global_best_position\n# ```"}
{"role": "LLaMEA", "time": "2024-11-09 16:20:38.810973", "content": "You are a highly skilled computer scientist in the field of natural computing. Your task is to design novel metaheuristic algorithms to solve black box optimization problems.\n\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the BBOB test suite of 24 noiseless functions. Your task is to write the optimization algorithm in Python code. The code should contain an `__init__(self, budget, dim)` function and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. Each of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nGive an excellent and novel heuristic algorithm to solve this task and also give it a one-line description with the main idea.\n\nThe current population of algorithms already evaluated (name, description, score) is:\nRefinedParticleOptimizer: A refined particle swarm optimizer with dynamic acceleration coefficients and adaptive neighborhood-based velocity adjustment for improved convergence. (Score: 0.3490816234226132)\n\nThe selected solution to update is:\nA refined particle swarm optimizer with dynamic acceleration coefficients and adaptive neighborhood-based velocity adjustment for improved convergence.\n\nWith code:\nimport numpy as np\n\nclass RefinedParticleOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(30, self.budget // 2)  # Increased population size for better diversity\n        self.inertia_weight = 0.6  # Decreased inertia weight for faster convergence\n        self.cognitive_coeff = 1.5  \n        self.social_coeff = 1.5  # Adjusted coefficients for more balanced search\n        self.adaptive_scaling = 0.9  # More aggressive adaptive scaling\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-0.3, 0.3, (self.population_size, self.dim))  # Reduced velocity range\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        velocity_clamp = (self.upper_bound - self.lower_bound) * 0.15  # Adjusted dynamic velocity clamp\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            \n            velocities = np.clip(velocities, -velocity_clamp, velocity_clamp)\n            positions += velocities\n            \n            if eval_count % (self.budget // 4) == 0:  # More frequent adaptive adjustment\n                self.inertia_weight *= self.adaptive_scaling\n                perturbation = np.random.normal(0, 0.05, positions.shape)  # Lower perturbation for fine-tuning\n                positions += perturbation\n            \n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n                \n        return global_best_score, global_best_position\n\nThe algorithm RefinedParticleOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.35 with standard deviation 0.30.\n\n\nPlease transform this optimization algorithm code by exactly 20.0% in a \nway that enhances convergence speed. Keep the code difference precisely at \n20.0%, and focus solely on achieving performance improvements through \nalgorithmic adjustments across various elements of the code. Avoid focusing on \ncode efficiency; instead, prioritize exploration of diverse approaches within \nthe allowed modification percentage.\n\n\nProvide the Python code and a one-line description with the main idea (without enters). Give the response in the format:\n# Description: <short-description>\n# Code: \n# ```python\n# <code>\n# ```\n# \n"}
{"role": "gpt-4o", "time": "2024-11-09 16:20:52.721290", "content": "# Description: A hybrid optimizer combining particle swarm with differential evolution strategies for improved exploration and convergence.\n# Code:\n# ```python\nimport numpy as np\n\nclass HybridParticleDEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(40, self.budget // 2)  # Increased population size for richer diversity\n        self.inertia_weight = 0.7  # Slightly increased inertia weight for better exploration\n        self.cognitive_coeff = 1.4  \n        self.social_coeff = 1.6  # Slightly enhancing social influence\n        self.adaptive_scaling = 0.85  # Adjusted scaling for better fine-tuning\n        self.mutation_factor = 0.8  # New mutation factor for differential evolution\n        self.crossover_rate = 0.7  # New crossover rate for differential evolution\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-0.2, 0.2, (self.population_size, self.dim))  # Reduced velocity range for stability\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        velocity_clamp = (self.upper_bound - self.lower_bound) * 0.2  # Adjusted dynamic velocity clamp\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            \n            velocities = np.clip(velocities, -velocity_clamp, velocity_clamp)\n            positions += velocities\n            \n            # Differential Evolution\n            for i in range(self.population_size):\n                indices = list(range(self.population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant_vector = positions[a] + self.mutation_factor * (positions[b] - positions[c])\n                crossover = np.random.rand(self.dim) < self.crossover_rate\n                trial_vector = np.where(crossover, mutant_vector, positions[i])\n                trial_vector = np.clip(trial_vector, self.lower_bound, self.upper_bound)\n                trial_score = func(trial_vector)\n                eval_count += 1\n                if trial_score < scores[i]:\n                    positions[i] = trial_vector\n                    scores[i] = trial_score\n\n            if eval_count % (self.budget // 5) == 0:  # More frequent adaptive adjustment\n                self.inertia_weight *= self.adaptive_scaling\n                perturbation = np.random.normal(0, 0.03, positions.shape)  # Lower perturbation for fine-tuning\n                positions += perturbation\n            \n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n                \n        return global_best_score, global_best_position\n# ```"}
{"role": "LLaMEA", "time": "2024-11-09 16:20:52.724901", "content": "You are a highly skilled computer scientist in the field of natural computing. Your task is to design novel metaheuristic algorithms to solve black box optimization problems.\n\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the BBOB test suite of 24 noiseless functions. Your task is to write the optimization algorithm in Python code. The code should contain an `__init__(self, budget, dim)` function and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. Each of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nGive an excellent and novel heuristic algorithm to solve this task and also give it a one-line description with the main idea.\n\nThe current population of algorithms already evaluated (name, description, score) is:\nRefinedParticleOptimizer: A refined particle swarm optimizer with dynamic acceleration coefficients and adaptive neighborhood-based velocity adjustment for improved convergence. (Score: 0.3490816234226132)\n\nThe selected solution to update is:\nA refined particle swarm optimizer with dynamic acceleration coefficients and adaptive neighborhood-based velocity adjustment for improved convergence.\n\nWith code:\nimport numpy as np\n\nclass RefinedParticleOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(30, self.budget // 2)  # Increased population size for better diversity\n        self.inertia_weight = 0.6  # Decreased inertia weight for faster convergence\n        self.cognitive_coeff = 1.5  \n        self.social_coeff = 1.5  # Adjusted coefficients for more balanced search\n        self.adaptive_scaling = 0.9  # More aggressive adaptive scaling\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-0.3, 0.3, (self.population_size, self.dim))  # Reduced velocity range\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        velocity_clamp = (self.upper_bound - self.lower_bound) * 0.15  # Adjusted dynamic velocity clamp\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            \n            velocities = np.clip(velocities, -velocity_clamp, velocity_clamp)\n            positions += velocities\n            \n            if eval_count % (self.budget // 4) == 0:  # More frequent adaptive adjustment\n                self.inertia_weight *= self.adaptive_scaling\n                perturbation = np.random.normal(0, 0.05, positions.shape)  # Lower perturbation for fine-tuning\n                positions += perturbation\n            \n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n                \n        return global_best_score, global_best_position\n\nThe algorithm RefinedParticleOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.35 with standard deviation 0.30.\n\n\nPlease transform this optimization algorithm code by exactly 20.0% in a \nway that enhances convergence speed. Keep the code difference precisely at \n20.0%, and focus solely on achieving performance improvements through \nalgorithmic adjustments across various elements of the code. Avoid focusing on \ncode efficiency; instead, prioritize exploration of diverse approaches within \nthe allowed modification percentage.\n\n\nProvide the Python code and a one-line description with the main idea (without enters). Give the response in the format:\n# Description: <short-description>\n# Code: \n# ```python\n# <code>\n# ```\n# \n"}
{"role": "gpt-4o", "time": "2024-11-09 16:21:04.552561", "content": "# Description: A modified particle swarm optimizer with nonlinear inertia weight decay and an elite-guided search strategy for faster convergence.\n# Code:\n# ```python\nimport numpy as np\n\nclass EnhancedParticleOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(35, self.budget // 2)  # Slightly increased population size for diversity\n        self.inertia_weight_start = 0.9  # Higher initial inertia\n        self.inertia_weight_end = 0.4  # Lower final inertia\n        self.cognitive_coeff = 1.7  # Increased cognitive coefficient for exploration\n        self.social_coeff = 1.3  # Slightly decreased social coefficient\n        self.elite_fraction = 0.1  # Introduce elite selection\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-0.3, 0.3, (self.population_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        velocity_clamp = (self.upper_bound - self.lower_bound) * 0.2  # Adjusted velocity clamp\n\n        while eval_count < self.budget:\n            inertia_weight = self.inertia_weight_start - ((self.inertia_weight_start - self.inertia_weight_end) * (eval_count / self.budget))\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = inertia_weight * velocities + cognitive_component + social_component\n            \n            velocities = np.clip(velocities, -velocity_clamp, velocity_clamp)\n            positions += velocities\n            \n            if eval_count % (self.budget // 5) == 0:  # More frequent elite enhancement\n                num_elites = int(self.population_size * self.elite_fraction)\n                elite_indices = np.argsort(personal_best_scores)[:num_elites]\n                elites = personal_best_positions[elite_indices]\n                perturbation = np.random.normal(0, 0.02, elites.shape)\n                positions[:num_elites] = elites + perturbation\n            \n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n                \n        return global_best_score, global_best_position\n# ```"}
{"role": "LLaMEA", "time": "2024-11-09 16:21:04.556502", "content": "You are a highly skilled computer scientist in the field of natural computing. Your task is to design novel metaheuristic algorithms to solve black box optimization problems.\n\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the BBOB test suite of 24 noiseless functions. Your task is to write the optimization algorithm in Python code. The code should contain an `__init__(self, budget, dim)` function and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. Each of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nGive an excellent and novel heuristic algorithm to solve this task and also give it a one-line description with the main idea.\n\nThe current population of algorithms already evaluated (name, description, score) is:\nRefinedParticleOptimizer: A refined particle swarm optimizer with dynamic acceleration coefficients and adaptive neighborhood-based velocity adjustment for improved convergence. (Score: 0.3490816234226132)\n\nThe selected solution to update is:\nA refined particle swarm optimizer with dynamic acceleration coefficients and adaptive neighborhood-based velocity adjustment for improved convergence.\n\nWith code:\nimport numpy as np\n\nclass RefinedParticleOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(30, self.budget // 2)  # Increased population size for better diversity\n        self.inertia_weight = 0.6  # Decreased inertia weight for faster convergence\n        self.cognitive_coeff = 1.5  \n        self.social_coeff = 1.5  # Adjusted coefficients for more balanced search\n        self.adaptive_scaling = 0.9  # More aggressive adaptive scaling\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-0.3, 0.3, (self.population_size, self.dim))  # Reduced velocity range\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        velocity_clamp = (self.upper_bound - self.lower_bound) * 0.15  # Adjusted dynamic velocity clamp\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            \n            velocities = np.clip(velocities, -velocity_clamp, velocity_clamp)\n            positions += velocities\n            \n            if eval_count % (self.budget // 4) == 0:  # More frequent adaptive adjustment\n                self.inertia_weight *= self.adaptive_scaling\n                perturbation = np.random.normal(0, 0.05, positions.shape)  # Lower perturbation for fine-tuning\n                positions += perturbation\n            \n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n                \n        return global_best_score, global_best_position\n\nThe algorithm RefinedParticleOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.35 with standard deviation 0.30.\n\n\nPlease transform this optimization algorithm code by exactly 20.0% in a \nway that enhances convergence speed. Keep the code difference precisely at \n20.0%, and focus solely on achieving performance improvements through \nalgorithmic adjustments across various elements of the code. Avoid focusing on \ncode efficiency; instead, prioritize exploration of diverse approaches within \nthe allowed modification percentage.\n\n\nProvide the Python code and a one-line description with the main idea (without enters). Give the response in the format:\n# Description: <short-description>\n# Code: \n# ```python\n# <code>\n# ```\n# \n"}
{"role": "gpt-4o", "time": "2024-11-09 16:21:15.039831", "content": "# Description: A hybrid velocity update strategy combining differential evolution principles with particle swarm optimization for enhanced exploration and convergence.\n# Code:\n# ```python\nimport numpy as np\n\nclass HybridParticleOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(30, self.budget // 2)\n        self.inertia_weight = 0.5  # Further decreased inertia weight for faster convergence\n        self.cognitive_coeff = 1.4  \n        self.social_coeff = 1.6  # Slightly increased social influence for exploration\n        self.adaptive_scaling = 0.85  # More aggressive adaptive scaling\n        self.de_scale = 0.5  # Introduced differential evolution scale factor\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-0.3, 0.3, (self.population_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        velocity_clamp = (self.upper_bound - self.lower_bound) * 0.2  # Adjusted dynamic velocity clamp\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            \n            # Differential Evolution inspired mutation\n            rand_idx = np.random.choice(self.population_size, (self.population_size, 3), replace=True)\n            mutant_vectors = personal_best_positions[rand_idx[:, 0]] + self.de_scale * (personal_best_positions[rand_idx[:, 1]] - personal_best_positions[rand_idx[:, 2]])\n            mutant_vectors = np.clip(mutant_vectors, self.lower_bound, self.upper_bound)\n            \n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            \n            # Hybrid velocity update\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component + (mutant_vectors - positions)\n            velocities = np.clip(velocities, -velocity_clamp, velocity_clamp)\n            positions += velocities\n            \n            if eval_count % (self.budget // 3) == 0:\n                self.inertia_weight *= self.adaptive_scaling\n                perturbation = np.random.normal(0, 0.04, positions.shape)  # Slightly lower perturbation for fine-tuning\n                positions += perturbation\n            \n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n                \n        return global_best_score, global_best_position\n# ```"}
{"role": "LLaMEA", "time": "2024-11-09 16:21:15.043348", "content": "You are a highly skilled computer scientist in the field of natural computing. Your task is to design novel metaheuristic algorithms to solve black box optimization problems.\n\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the BBOB test suite of 24 noiseless functions. Your task is to write the optimization algorithm in Python code. The code should contain an `__init__(self, budget, dim)` function and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. Each of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nGive an excellent and novel heuristic algorithm to solve this task and also give it a one-line description with the main idea.\n\nThe current population of algorithms already evaluated (name, description, score) is:\nRefinedParticleOptimizer: A refined particle swarm optimizer with dynamic acceleration coefficients and adaptive neighborhood-based velocity adjustment for improved convergence. (Score: 0.3490816234226132)\n\nThe selected solution to update is:\nA refined particle swarm optimizer with dynamic acceleration coefficients and adaptive neighborhood-based velocity adjustment for improved convergence.\n\nWith code:\nimport numpy as np\n\nclass RefinedParticleOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(30, self.budget // 2)  # Increased population size for better diversity\n        self.inertia_weight = 0.6  # Decreased inertia weight for faster convergence\n        self.cognitive_coeff = 1.5  \n        self.social_coeff = 1.5  # Adjusted coefficients for more balanced search\n        self.adaptive_scaling = 0.9  # More aggressive adaptive scaling\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-0.3, 0.3, (self.population_size, self.dim))  # Reduced velocity range\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        velocity_clamp = (self.upper_bound - self.lower_bound) * 0.15  # Adjusted dynamic velocity clamp\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            \n            velocities = np.clip(velocities, -velocity_clamp, velocity_clamp)\n            positions += velocities\n            \n            if eval_count % (self.budget // 4) == 0:  # More frequent adaptive adjustment\n                self.inertia_weight *= self.adaptive_scaling\n                perturbation = np.random.normal(0, 0.05, positions.shape)  # Lower perturbation for fine-tuning\n                positions += perturbation\n            \n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n                \n        return global_best_score, global_best_position\n\nThe algorithm RefinedParticleOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.35 with standard deviation 0.30.\n\n\nPlease transform this optimization algorithm code by exactly 20.0% in a \nway that enhances convergence speed. Keep the code difference precisely at \n20.0%, and focus solely on achieving performance improvements through \nalgorithmic adjustments across various elements of the code. Avoid focusing on \ncode efficiency; instead, prioritize exploration of diverse approaches within \nthe allowed modification percentage.\n\n\nProvide the Python code and a one-line description with the main idea (without enters). Give the response in the format:\n# Description: <short-description>\n# Code: \n# ```python\n# <code>\n# ```\n# \n"}
{"role": "gpt-4o", "time": "2024-11-09 16:21:24.503781", "content": "# Description: Enhanced Particle Swarm Optimizer with non-linear inertia adaptation and stochastic velocity perturbations for improved convergence speed.\n# Code: \n# ```python\nimport numpy as np\n\nclass EnhancedParticleOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(40, self.budget // 2)  # Increased population size for diversity\n        self.inertia_weight = 0.7  # Adjusted inertia weight for balance\n        self.cognitive_coeff = 2.0  # More emphasis on cognitive component\n        self.social_coeff = 1.8  # Slight increase for social component\n        self.adaptive_scaling = 0.85  # More frequent scaling\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-0.4, 0.4, (self.population_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        velocity_clamp = (self.upper_bound - self.lower_bound) * 0.2  # Adjusted velocity clamp\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            \n            velocities = np.clip(velocities, -velocity_clamp, velocity_clamp)\n            positions += velocities\n            \n            if eval_count % (self.budget // 6) == 0:  # Increased frequency of adaptation\n                self.inertia_weight *= self.adaptive_scaling\n                perturbation = np.random.normal(0, 0.1, positions.shape)  # Increased perturbation for exploration\n                positions += perturbation\n            \n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n                \n        return global_best_score, global_best_position\n```"}
{"role": "LLaMEA", "time": "2024-11-09 16:22:21.693955", "content": "You are a highly skilled computer scientist in the field of natural computing. Your task is to design novel metaheuristic algorithms to solve black box optimization problems.\n\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the BBOB test suite of 24 noiseless functions. Your task is to write the optimization algorithm in Python code. The code should contain an `__init__(self, budget, dim)` function and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. Each of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nGive an excellent and novel heuristic algorithm to solve this task and also give it a one-line description with the main idea.\n\nThe current population of algorithms already evaluated (name, description, score) is:\nRefinedParticleOptimizer: A refined particle swarm optimizer with dynamic acceleration coefficients and adaptive neighborhood-based velocity adjustment for improved convergence. (Score: 0.3490816234226132)\n\nThe selected solution to update is:\nA refined particle swarm optimizer with dynamic acceleration coefficients and adaptive neighborhood-based velocity adjustment for improved convergence.\n\nWith code:\nimport numpy as np\n\nclass RefinedParticleOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(30, self.budget // 2)  # Increased population size for better diversity\n        self.inertia_weight = 0.6  # Decreased inertia weight for faster convergence\n        self.cognitive_coeff = 1.5  \n        self.social_coeff = 1.5  # Adjusted coefficients for more balanced search\n        self.adaptive_scaling = 0.9  # More aggressive adaptive scaling\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-0.3, 0.3, (self.population_size, self.dim))  # Reduced velocity range\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        velocity_clamp = (self.upper_bound - self.lower_bound) * 0.15  # Adjusted dynamic velocity clamp\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            \n            velocities = np.clip(velocities, -velocity_clamp, velocity_clamp)\n            positions += velocities\n            \n            if eval_count % (self.budget // 4) == 0:  # More frequent adaptive adjustment\n                self.inertia_weight *= self.adaptive_scaling\n                perturbation = np.random.normal(0, 0.05, positions.shape)  # Lower perturbation for fine-tuning\n                positions += perturbation\n            \n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n                \n        return global_best_score, global_best_position\n\nThe algorithm RefinedParticleOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.35 with standard deviation 0.30.\n\n\nPlease transform this optimization algorithm code by exactly 20.0% in a \nway that enhances convergence speed. Keep the code difference precisely at \n20.0%, and focus solely on achieving performance improvements through \nalgorithmic adjustments across various elements of the code. Avoid focusing on \ncode efficiency; instead, prioritize exploration of diverse approaches within \nthe allowed modification percentage.\n\n\nProvide the Python code and a one-line description with the main idea (without enters). Give the response in the format:\n# Description: <short-description>\n# Code: \n# ```python\n# <code>\n# ```\n# \n"}
{"role": "gpt-4o", "time": "2024-11-09 16:22:34.888139", "content": "# Description: A refined PSO with enhanced convergence by adaptive learning rates and self-organizing velocities.\n# Code: \n```python\nimport numpy as np\n\nclass EnhancedPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(40, self.budget // 2)  # Increased population size\n        self.inertia_weight = 0.7  # Adjusted inertia weight for exploration balance\n        self.cognitive_coeff = 1.4  \n        self.social_coeff = 1.6  # Slightly increased social coefficient\n        self.adaptive_scaling = 0.95  # Adjusted adaptive scaling\n        self.velocity_decay = 0.98  # New velocity decay factor\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-0.3, 0.3, (self.population_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        velocity_clamp = (self.upper_bound - self.lower_bound) * 0.1  # Updated dynamic velocity clamp\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.velocity_decay * (self.inertia_weight * velocities + cognitive_component + social_component)\n            \n            velocities = np.clip(velocities, -velocity_clamp, velocity_clamp)\n            positions += velocities\n            \n            if eval_count % (self.budget // 3) == 0:  # More frequent adaptive adjustment\n                self.inertia_weight *= self.adaptive_scaling\n                perturbation = np.random.normal(0, 0.03, positions.shape)  # Lower perturbation\n                positions += perturbation\n            \n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n\n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n                \n        return global_best_score, global_best_position\n```"}
{"role": "LLaMEA", "time": "2024-11-09 16:23:27.180101", "content": "You are a highly skilled computer scientist in the field of natural computing. Your task is to design novel metaheuristic algorithms to solve black box optimization problems.\n\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the BBOB test suite of 24 noiseless functions. Your task is to write the optimization algorithm in Python code. The code should contain an `__init__(self, budget, dim)` function and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. Each of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nGive an excellent and novel heuristic algorithm to solve this task and also give it a one-line description with the main idea.\n\nThe current population of algorithms already evaluated (name, description, score) is:\nRefinedParticleOptimizer: A refined particle swarm optimizer with dynamic acceleration coefficients and adaptive neighborhood-based velocity adjustment for improved convergence. (Score: 0.3490816234226132)\n\nThe selected solution to update is:\nA refined particle swarm optimizer with dynamic acceleration coefficients and adaptive neighborhood-based velocity adjustment for improved convergence.\n\nWith code:\nimport numpy as np\n\nclass RefinedParticleOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(30, self.budget // 2)  # Increased population size for better diversity\n        self.inertia_weight = 0.6  # Decreased inertia weight for faster convergence\n        self.cognitive_coeff = 1.5  \n        self.social_coeff = 1.5  # Adjusted coefficients for more balanced search\n        self.adaptive_scaling = 0.9  # More aggressive adaptive scaling\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-0.3, 0.3, (self.population_size, self.dim))  # Reduced velocity range\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        velocity_clamp = (self.upper_bound - self.lower_bound) * 0.15  # Adjusted dynamic velocity clamp\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            \n            velocities = np.clip(velocities, -velocity_clamp, velocity_clamp)\n            positions += velocities\n            \n            if eval_count % (self.budget // 4) == 0:  # More frequent adaptive adjustment\n                self.inertia_weight *= self.adaptive_scaling\n                perturbation = np.random.normal(0, 0.05, positions.shape)  # Lower perturbation for fine-tuning\n                positions += perturbation\n            \n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n                \n        return global_best_score, global_best_position\n\nThe algorithm RefinedParticleOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.35 with standard deviation 0.30.\n\n\nPlease transform this optimization algorithm code by exactly 20.0% in a \nway that enhances convergence speed. Keep the code difference precisely at \n20.0%, and focus solely on achieving performance improvements through \nalgorithmic adjustments across various elements of the code. Avoid focusing on \ncode efficiency; instead, prioritize exploration of diverse approaches within \nthe allowed modification percentage.\n\n\nProvide the Python code and a one-line description with the main idea (without enters). Give the response in the format:\n# Description: <short-description>\n# Code: \n# ```python\n# <code>\n# ```\n# \n"}
{"role": "gpt-4o", "time": "2024-11-09 16:23:42.640602", "content": "# Description: Enhanced Particle Optimizer with Chaotic Initialization and Diversity Injection for Faster Convergence.\n# Code:\n# ```python\nimport numpy as np\nimport random\n\nclass EnhancedParticleOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(40, self.budget // 2)  # Increased population size again for exploration\n        self.inertia_weight = 0.7  # Slightly increased inertia for better initial exploration\n        self.cognitive_coeff = 1.7  # Enhanced cognitive coefficient for individual learning emphasis\n        self.social_coeff = 1.3  # Slightly reduced social coefficient\n        self.adaptive_scaling = 0.8  # Stronger adaptive scaling for rapid convergence\n        \n    def chaotic_initialization(self):\n        return self.lower_bound + (self.upper_bound - self.lower_bound) * np.sin(np.linspace(0, np.pi, self.population_size * self.dim)).reshape(self.population_size, self.dim)\n\n    def __call__(self, func):\n        positions = self.chaotic_initialization()  # Use chaotic initialization for diversity\n        velocities = np.random.uniform(-0.4, 0.4, (self.population_size, self.dim))  # Increased velocity range\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        velocity_clamp = (self.upper_bound - self.lower_bound) * 0.2  # Increased velocity clamp\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            \n            velocities = np.clip(velocities, -velocity_clamp, velocity_clamp)\n            positions += velocities\n            \n            if eval_count % (self.budget // 3) == 0:  # More frequent adaptation\n                self.inertia_weight *= self.adaptive_scaling\n                random_indices = np.random.choice(self.population_size, size=self.population_size // 5, replace=False)\n                positions[random_indices] = np.random.uniform(self.lower_bound, self.upper_bound, (len(random_indices), self.dim))  # Diversity injection\n            \n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n                \n        return global_best_score, global_best_position\n# ```"}
{"role": "LLaMEA", "time": "2024-11-09 16:23:42.644067", "content": "You are a highly skilled computer scientist in the field of natural computing. Your task is to design novel metaheuristic algorithms to solve black box optimization problems.\n\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the BBOB test suite of 24 noiseless functions. Your task is to write the optimization algorithm in Python code. The code should contain an `__init__(self, budget, dim)` function and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. Each of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nGive an excellent and novel heuristic algorithm to solve this task and also give it a one-line description with the main idea.\n\nThe current population of algorithms already evaluated (name, description, score) is:\nRefinedParticleOptimizer: A refined particle swarm optimizer with dynamic acceleration coefficients and adaptive neighborhood-based velocity adjustment for improved convergence. (Score: 0.3490816234226132)\n\nThe selected solution to update is:\nA refined particle swarm optimizer with dynamic acceleration coefficients and adaptive neighborhood-based velocity adjustment for improved convergence.\n\nWith code:\nimport numpy as np\n\nclass RefinedParticleOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(30, self.budget // 2)  # Increased population size for better diversity\n        self.inertia_weight = 0.6  # Decreased inertia weight for faster convergence\n        self.cognitive_coeff = 1.5  \n        self.social_coeff = 1.5  # Adjusted coefficients for more balanced search\n        self.adaptive_scaling = 0.9  # More aggressive adaptive scaling\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-0.3, 0.3, (self.population_size, self.dim))  # Reduced velocity range\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        velocity_clamp = (self.upper_bound - self.lower_bound) * 0.15  # Adjusted dynamic velocity clamp\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            \n            velocities = np.clip(velocities, -velocity_clamp, velocity_clamp)\n            positions += velocities\n            \n            if eval_count % (self.budget // 4) == 0:  # More frequent adaptive adjustment\n                self.inertia_weight *= self.adaptive_scaling\n                perturbation = np.random.normal(0, 0.05, positions.shape)  # Lower perturbation for fine-tuning\n                positions += perturbation\n            \n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n                \n        return global_best_score, global_best_position\n\nThe algorithm RefinedParticleOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.35 with standard deviation 0.30.\n\n\nPlease transform this optimization algorithm code by exactly 20.0% in a \nway that enhances convergence speed. Keep the code difference precisely at \n20.0%, and focus solely on achieving performance improvements through \nalgorithmic adjustments across various elements of the code. Avoid focusing on \ncode efficiency; instead, prioritize exploration of diverse approaches within \nthe allowed modification percentage.\n\n\nProvide the Python code and a one-line description with the main idea (without enters). Give the response in the format:\n# Description: <short-description>\n# Code: \n# ```python\n# <code>\n# ```\n# \n"}
{"role": "gpt-4o", "time": "2024-11-09 16:23:52.615296", "content": "# Description: A refined particle swarm optimizer with enhanced convergence through dynamic inertia reduction, tournament selection, and diversified velocity updates.\n# Code:\n# ```python\nimport numpy as np\n\nclass EnhancedParticleOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(40, self.budget // 2)  # Increased population size for enhanced exploration\n        self.inertia_weight = 0.7  # Slightly higher initial inertia weight for better exploration\n        self.cognitive_coeff = 2.0  # Increased cognitive coefficient for personal best attraction\n        self.social_coeff = 1.3  # Slightly reduced social coefficient for less aggressive convergence\n        self.adaptive_scaling = 0.85  # More aggressive adaptive scaling for faster convergence\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-0.5, 0.5, (self.population_size, self.dim))  # Wider initial velocity range\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        velocity_clamp = (self.upper_bound - self.lower_bound) * 0.2  # Increased dynamic velocity clamp\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            \n            velocities = np.clip(velocities, -velocity_clamp, velocity_clamp)\n            positions += velocities\n            \n            if eval_count % (self.budget // 3) == 0:  # More frequent adaptive adjustment\n                self.inertia_weight *= self.adaptive_scaling\n                perturbation = np.random.normal(0, 0.1, positions.shape)  # Increased perturbation for diversification\n                positions += perturbation\n            \n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n                \n        return global_best_score, global_best_position\n# ```"}
{"role": "LLaMEA", "time": "2024-11-09 16:23:52.618609", "content": "You are a highly skilled computer scientist in the field of natural computing. Your task is to design novel metaheuristic algorithms to solve black box optimization problems.\n\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the BBOB test suite of 24 noiseless functions. Your task is to write the optimization algorithm in Python code. The code should contain an `__init__(self, budget, dim)` function and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. Each of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nGive an excellent and novel heuristic algorithm to solve this task and also give it a one-line description with the main idea.\n\nThe current population of algorithms already evaluated (name, description, score) is:\nRefinedParticleOptimizer: A refined particle swarm optimizer with dynamic acceleration coefficients and adaptive neighborhood-based velocity adjustment for improved convergence. (Score: 0.3490816234226132)\n\nThe selected solution to update is:\nA refined particle swarm optimizer with dynamic acceleration coefficients and adaptive neighborhood-based velocity adjustment for improved convergence.\n\nWith code:\nimport numpy as np\n\nclass RefinedParticleOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(30, self.budget // 2)  # Increased population size for better diversity\n        self.inertia_weight = 0.6  # Decreased inertia weight for faster convergence\n        self.cognitive_coeff = 1.5  \n        self.social_coeff = 1.5  # Adjusted coefficients for more balanced search\n        self.adaptive_scaling = 0.9  # More aggressive adaptive scaling\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-0.3, 0.3, (self.population_size, self.dim))  # Reduced velocity range\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        velocity_clamp = (self.upper_bound - self.lower_bound) * 0.15  # Adjusted dynamic velocity clamp\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            \n            velocities = np.clip(velocities, -velocity_clamp, velocity_clamp)\n            positions += velocities\n            \n            if eval_count % (self.budget // 4) == 0:  # More frequent adaptive adjustment\n                self.inertia_weight *= self.adaptive_scaling\n                perturbation = np.random.normal(0, 0.05, positions.shape)  # Lower perturbation for fine-tuning\n                positions += perturbation\n            \n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n                \n        return global_best_score, global_best_position\n\nThe algorithm RefinedParticleOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.35 with standard deviation 0.30.\n\n\nPlease transform this optimization algorithm code by exactly 20.0% in a \nway that enhances convergence speed. Keep the code difference precisely at \n20.0%, and focus solely on achieving performance improvements through \nalgorithmic adjustments across various elements of the code. Avoid focusing on \ncode efficiency; instead, prioritize exploration of diverse approaches within \nthe allowed modification percentage.\n\n\nProvide the Python code and a one-line description with the main idea (without enters). Give the response in the format:\n# Description: <short-description>\n# Code: \n# ```python\n# <code>\n# ```\n# \n"}
{"role": "gpt-4o", "time": "2024-11-09 16:24:04.640149", "content": "# Description: Hybrid Particle Swarm with Differential Evolution Elements for Enhanced Convergence Speed\n# Code:\n# ```python\nimport numpy as np\n\nclass HybridParticleDEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(40, self.budget // 2)  # Increased population size for diverse exploration\n        self.inertia_weight = 0.7  # Increased inertia weight to maintain exploration longer\n        self.cognitive_coeff = 1.4  # Slightly adjusted coefficients\n        self.social_coeff = 1.6  \n        self.adaptive_scaling = 0.95  # Less aggressive adaptive scaling for stability\n        self.mutation_factor = 0.8  # Added mutation factor for Differential Evolution\n        self.crossover_rate = 0.9  # Added crossover rate for Differential Evolution\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-0.2, 0.2, (self.population_size, self.dim))  # Even more reduced velocity range\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        velocity_clamp = (self.upper_bound - self.lower_bound) * 0.1  # Further reduced dynamic velocity clamp\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            \n            velocities = np.clip(velocities, -velocity_clamp, velocity_clamp)\n            positions += velocities\n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            # Differential Evolution mutation and crossover\n            for i in range(self.population_size):\n                idxs = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = positions[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + self.mutation_factor * (b - c), self.lower_bound, self.upper_bound)\n                cross_points = np.random.rand(self.dim) < self.crossover_rate\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, positions[i])\n                if func(trial) < scores[i]:\n                    positions[i] = trial\n\n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n                \n        return global_best_score, global_best_position\n# ```"}
{"role": "LLaMEA", "time": "2024-11-09 16:24:04.643443", "content": "You are a highly skilled computer scientist in the field of natural computing. Your task is to design novel metaheuristic algorithms to solve black box optimization problems.\n\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the BBOB test suite of 24 noiseless functions. Your task is to write the optimization algorithm in Python code. The code should contain an `__init__(self, budget, dim)` function and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. Each of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nGive an excellent and novel heuristic algorithm to solve this task and also give it a one-line description with the main idea.\n\nThe current population of algorithms already evaluated (name, description, score) is:\nRefinedParticleOptimizer: A refined particle swarm optimizer with dynamic acceleration coefficients and adaptive neighborhood-based velocity adjustment for improved convergence. (Score: 0.3490816234226132)\n\nThe selected solution to update is:\nA refined particle swarm optimizer with dynamic acceleration coefficients and adaptive neighborhood-based velocity adjustment for improved convergence.\n\nWith code:\nimport numpy as np\n\nclass RefinedParticleOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(30, self.budget // 2)  # Increased population size for better diversity\n        self.inertia_weight = 0.6  # Decreased inertia weight for faster convergence\n        self.cognitive_coeff = 1.5  \n        self.social_coeff = 1.5  # Adjusted coefficients for more balanced search\n        self.adaptive_scaling = 0.9  # More aggressive adaptive scaling\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-0.3, 0.3, (self.population_size, self.dim))  # Reduced velocity range\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        velocity_clamp = (self.upper_bound - self.lower_bound) * 0.15  # Adjusted dynamic velocity clamp\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            \n            velocities = np.clip(velocities, -velocity_clamp, velocity_clamp)\n            positions += velocities\n            \n            if eval_count % (self.budget // 4) == 0:  # More frequent adaptive adjustment\n                self.inertia_weight *= self.adaptive_scaling\n                perturbation = np.random.normal(0, 0.05, positions.shape)  # Lower perturbation for fine-tuning\n                positions += perturbation\n            \n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n                \n        return global_best_score, global_best_position\n\nThe algorithm RefinedParticleOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.35 with standard deviation 0.30.\n\n\nPlease transform this optimization algorithm code by exactly 20.0% in a \nway that enhances convergence speed. Keep the code difference precisely at \n20.0%, and focus solely on achieving performance improvements through \nalgorithmic adjustments across various elements of the code. Avoid focusing on \ncode efficiency; instead, prioritize exploration of diverse approaches within \nthe allowed modification percentage.\n\n\nProvide the Python code and a one-line description with the main idea (without enters). Give the response in the format:\n# Description: <short-description>\n# Code: \n# ```python\n# <code>\n# ```\n# \n"}
{"role": "gpt-4o", "time": "2024-11-09 16:24:14.659070", "content": "# Description: An enhanced particle swarm optimizer utilizing adaptive mutation and dynamic inertia adjustments for improved convergence.\n# Code:\n# ```python\nimport numpy as np\n\nclass EnhancedParticleOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(40, self.budget // 2)  # Slightly increased population size\n        self.inertia_weight = 0.7  # Adjusted inertia for better balance\n        self.cognitive_coeff = 1.7  # Increased cognitive coefficient\n        self.social_coeff = 1.3  # Slightly decreased social coefficient\n        self.adaptive_scaling = 0.85  # Enhanced adaptive scaling\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-0.2, 0.2, (self.population_size, self.dim))  # Narrowed velocity range\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        velocity_clamp = (self.upper_bound - self.lower_bound) * 0.1  # Increased dynamic velocity clamp\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            \n            velocities = np.clip(velocities, -velocity_clamp, velocity_clamp)\n            positions += velocities\n            \n            if eval_count % (self.budget // 5) == 0:  # More frequent adaptive adjustment\n                self.inertia_weight *= self.adaptive_scaling\n                mutation = np.random.uniform(-0.1, 0.1, positions.shape)  # Added mutation for variability\n                positions += mutation\n            \n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n                \n        return global_best_score, global_best_position\n# ```"}
{"role": "LLaMEA", "time": "2024-11-09 16:24:14.662791", "content": "You are a highly skilled computer scientist in the field of natural computing. Your task is to design novel metaheuristic algorithms to solve black box optimization problems.\n\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the BBOB test suite of 24 noiseless functions. Your task is to write the optimization algorithm in Python code. The code should contain an `__init__(self, budget, dim)` function and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. Each of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nGive an excellent and novel heuristic algorithm to solve this task and also give it a one-line description with the main idea.\n\nThe current population of algorithms already evaluated (name, description, score) is:\nRefinedParticleOptimizer: A refined particle swarm optimizer with dynamic acceleration coefficients and adaptive neighborhood-based velocity adjustment for improved convergence. (Score: 0.3490816234226132)\n\nThe selected solution to update is:\nA refined particle swarm optimizer with dynamic acceleration coefficients and adaptive neighborhood-based velocity adjustment for improved convergence.\n\nWith code:\nimport numpy as np\n\nclass RefinedParticleOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(30, self.budget // 2)  # Increased population size for better diversity\n        self.inertia_weight = 0.6  # Decreased inertia weight for faster convergence\n        self.cognitive_coeff = 1.5  \n        self.social_coeff = 1.5  # Adjusted coefficients for more balanced search\n        self.adaptive_scaling = 0.9  # More aggressive adaptive scaling\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-0.3, 0.3, (self.population_size, self.dim))  # Reduced velocity range\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        velocity_clamp = (self.upper_bound - self.lower_bound) * 0.15  # Adjusted dynamic velocity clamp\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            \n            velocities = np.clip(velocities, -velocity_clamp, velocity_clamp)\n            positions += velocities\n            \n            if eval_count % (self.budget // 4) == 0:  # More frequent adaptive adjustment\n                self.inertia_weight *= self.adaptive_scaling\n                perturbation = np.random.normal(0, 0.05, positions.shape)  # Lower perturbation for fine-tuning\n                positions += perturbation\n            \n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n                \n        return global_best_score, global_best_position\n\nThe algorithm RefinedParticleOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.35 with standard deviation 0.30.\n\n\nPlease transform this optimization algorithm code by exactly 20.0% in a \nway that enhances convergence speed. Keep the code difference precisely at \n20.0%, and focus solely on achieving performance improvements through \nalgorithmic adjustments across various elements of the code. Avoid focusing on \ncode efficiency; instead, prioritize exploration of diverse approaches within \nthe allowed modification percentage.\n\n\nProvide the Python code and a one-line description with the main idea (without enters). Give the response in the format:\n# Description: <short-description>\n# Code: \n# ```python\n# <code>\n# ```\n# \n"}
{"role": "gpt-4o", "time": "2024-11-09 16:24:24.860233", "content": "# Description: Adaptive Self-Tuning Particle Swarm Optimizer with enhanced diversity through elite perturbation and variable neighborhood search.\n# Code:\n# ```python\nimport numpy as np\n\nclass AdaptiveSelfTuningPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(40, self.budget // 2)  # Increased population size for diversity\n        self.inertia_weight = 0.7  # Slightly increased inertia weight for exploratory moves\n        self.cognitive_coeff = 1.4  \n        self.social_coeff = 1.6  # Adjusted coefficients for more exploration\n        self.adaptive_scaling = 0.85  # More aggressive adaptive scaling\n        self.elite_fraction = 0.1  # Fraction of population considered as elite\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-0.3, 0.3, (self.population_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        velocity_clamp = (self.upper_bound - self.lower_bound) * 0.2  # Adjusted dynamic velocity clamp\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            \n            velocities = np.clip(velocities, -velocity_clamp, velocity_clamp)\n            positions += velocities\n            \n            if eval_count % (self.budget // 3) == 0:  # Frequent adaptive adjustment\n                self.inertia_weight *= self.adaptive_scaling\n                perturbation = np.random.normal(0, 0.1, positions.shape)  # Increased perturbation for exploration\n                elite_count = int(self.population_size * self.elite_fraction)\n                elite_indices = np.argsort(personal_best_scores)[:elite_count]\n                elite_positions = positions[elite_indices]\n                positions[:elite_count] += perturbation[:elite_count]  # Perturb elite for diversity\n            \n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n                \n        return global_best_score, global_best_position\n# ```"}
{"role": "LLaMEA", "time": "2024-11-09 16:24:24.863787", "content": "You are a highly skilled computer scientist in the field of natural computing. Your task is to design novel metaheuristic algorithms to solve black box optimization problems.\n\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the BBOB test suite of 24 noiseless functions. Your task is to write the optimization algorithm in Python code. The code should contain an `__init__(self, budget, dim)` function and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. Each of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nGive an excellent and novel heuristic algorithm to solve this task and also give it a one-line description with the main idea.\n\nThe current population of algorithms already evaluated (name, description, score) is:\nRefinedParticleOptimizer: A refined particle swarm optimizer with dynamic acceleration coefficients and adaptive neighborhood-based velocity adjustment for improved convergence. (Score: 0.3490816234226132)\n\nThe selected solution to update is:\nA refined particle swarm optimizer with dynamic acceleration coefficients and adaptive neighborhood-based velocity adjustment for improved convergence.\n\nWith code:\nimport numpy as np\n\nclass RefinedParticleOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(30, self.budget // 2)  # Increased population size for better diversity\n        self.inertia_weight = 0.6  # Decreased inertia weight for faster convergence\n        self.cognitive_coeff = 1.5  \n        self.social_coeff = 1.5  # Adjusted coefficients for more balanced search\n        self.adaptive_scaling = 0.9  # More aggressive adaptive scaling\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-0.3, 0.3, (self.population_size, self.dim))  # Reduced velocity range\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        velocity_clamp = (self.upper_bound - self.lower_bound) * 0.15  # Adjusted dynamic velocity clamp\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            \n            velocities = np.clip(velocities, -velocity_clamp, velocity_clamp)\n            positions += velocities\n            \n            if eval_count % (self.budget // 4) == 0:  # More frequent adaptive adjustment\n                self.inertia_weight *= self.adaptive_scaling\n                perturbation = np.random.normal(0, 0.05, positions.shape)  # Lower perturbation for fine-tuning\n                positions += perturbation\n            \n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n                \n        return global_best_score, global_best_position\n\nThe algorithm RefinedParticleOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.35 with standard deviation 0.30.\n\n\nPlease transform this optimization algorithm code by exactly 20.0% in a \nway that enhances convergence speed. Keep the code difference precisely at \n20.0%, and focus solely on achieving performance improvements through \nalgorithmic adjustments across various elements of the code. Avoid focusing on \ncode efficiency; instead, prioritize exploration of diverse approaches within \nthe allowed modification percentage.\n\n\nProvide the Python code and a one-line description with the main idea (without enters). Give the response in the format:\n# Description: <short-description>\n# Code: \n# ```python\n# <code>\n# ```\n# \n"}
{"role": "gpt-4o", "time": "2024-11-09 16:24:33.672052", "content": "# Description: Enhanced PSO with directional learning and convergence tuning for accelerated optimization.\n# Code:\n# ```python\nimport numpy as np\n\nclass EnhancedParticleOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(40, self.budget // 3)  # Increased population size for better exploration\n        self.inertia_weight = 0.7  # Slightly higher inertia weight for initial exploration\n        self.cognitive_coeff = 1.7  # Increased for more personal exploration\n        self.social_coeff = 1.3  # Lowered for reduced reliance on global best  \n        self.adaptive_scaling = 0.85  # Slightly aggressive scaling to enhance tuning\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-0.5, 0.5, (self.population_size, self.dim))  # Broader velocity range\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        velocity_clamp = (self.upper_bound - self.lower_bound) * 0.2  # Dynamic velocity clamp\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            directional_learning = 0.1 * np.sign(positions - global_best_position)  # New directional learning\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component + directional_learning\n            \n            velocities = np.clip(velocities, -velocity_clamp, velocity_clamp)\n            positions += velocities\n            \n            if eval_count % (self.budget // 3) == 0:  # Adjusted frequency of adaptive adjustment\n                self.inertia_weight *= self.adaptive_scaling\n                perturbation = np.random.normal(0, 0.1, positions.shape)  # Slightly increased perturbation\n                positions += perturbation\n            \n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n                \n        return global_best_score, global_best_position\n# ```"}
{"role": "LLaMEA", "time": "2024-11-09 16:24:33.675802", "content": "You are a highly skilled computer scientist in the field of natural computing. Your task is to design novel metaheuristic algorithms to solve black box optimization problems.\n\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the BBOB test suite of 24 noiseless functions. Your task is to write the optimization algorithm in Python code. The code should contain an `__init__(self, budget, dim)` function and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. Each of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nGive an excellent and novel heuristic algorithm to solve this task and also give it a one-line description with the main idea.\n\nThe current population of algorithms already evaluated (name, description, score) is:\nRefinedParticleOptimizer: A refined particle swarm optimizer with dynamic acceleration coefficients and adaptive neighborhood-based velocity adjustment for improved convergence. (Score: 0.3490816234226132)\n\nThe selected solution to update is:\nA refined particle swarm optimizer with dynamic acceleration coefficients and adaptive neighborhood-based velocity adjustment for improved convergence.\n\nWith code:\nimport numpy as np\n\nclass RefinedParticleOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(30, self.budget // 2)  # Increased population size for better diversity\n        self.inertia_weight = 0.6  # Decreased inertia weight for faster convergence\n        self.cognitive_coeff = 1.5  \n        self.social_coeff = 1.5  # Adjusted coefficients for more balanced search\n        self.adaptive_scaling = 0.9  # More aggressive adaptive scaling\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-0.3, 0.3, (self.population_size, self.dim))  # Reduced velocity range\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        velocity_clamp = (self.upper_bound - self.lower_bound) * 0.15  # Adjusted dynamic velocity clamp\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            \n            velocities = np.clip(velocities, -velocity_clamp, velocity_clamp)\n            positions += velocities\n            \n            if eval_count % (self.budget // 4) == 0:  # More frequent adaptive adjustment\n                self.inertia_weight *= self.adaptive_scaling\n                perturbation = np.random.normal(0, 0.05, positions.shape)  # Lower perturbation for fine-tuning\n                positions += perturbation\n            \n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n                \n        return global_best_score, global_best_position\n\nThe algorithm RefinedParticleOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.35 with standard deviation 0.30.\n\n\nPlease transform this optimization algorithm code by exactly 20.0% in a \nway that enhances convergence speed. Keep the code difference precisely at \n20.0%, and focus solely on achieving performance improvements through \nalgorithmic adjustments across various elements of the code. Avoid focusing on \ncode efficiency; instead, prioritize exploration of diverse approaches within \nthe allowed modification percentage.\n\n\nProvide the Python code and a one-line description with the main idea (without enters). Give the response in the format:\n# Description: <short-description>\n# Code: \n# ```python\n# <code>\n# ```\n# \n"}
{"role": "gpt-4o", "time": "2024-11-09 16:24:43.848828", "content": "# Description: Introducing local search and dynamic adaptive parameters in a refined PSO to enhance convergence speed.\n# Code: \n# ```python\nimport numpy as np\n\nclass EnhancedParticleSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(30, self.budget // 2)\n        self.inertia_weight = 0.5  # Further decreased inertia for more exploration\n        self.cognitive_coeff = 1.7  # Slightly increased for improved personal exploration\n        self.social_coeff = 1.3  # Slightly decreased to reduce premature convergence\n        self.adaptive_scaling = 0.92  # Adjusted scaling for robust adaptation\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-0.3, 0.3, (self.population_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        velocity_clamp = (self.upper_bound - self.lower_bound) * 0.1  # Reduced dynamic velocity clamp\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            \n            velocities = np.clip(velocities, -velocity_clamp, velocity_clamp)\n            positions += velocities\n            \n            if eval_count % (self.budget // 3) == 0:  # Adjusted frequency for adaptive adjustment\n                self.inertia_weight *= self.adaptive_scaling\n                perturbation = np.random.uniform(-0.05, 0.05, positions.shape)  # Uniform perturbation for exploration\n                positions += perturbation\n            \n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n\n            # Local search enhancement step\n            if eval_count % (self.budget // 5) == 0:\n                local_search_idx = np.random.randint(self.population_size)\n                candidate = positions[local_search_idx] + np.random.normal(0, 0.1, self.dim)\n                candidate = np.clip(candidate, self.lower_bound, self.upper_bound)\n                candidate_score = func(candidate)\n                eval_count += 1\n                if candidate_score < personal_best_scores[local_search_idx]:\n                    personal_best_scores[local_search_idx] = candidate_score\n                    personal_best_positions[local_search_idx] = candidate\n                    if candidate_score < global_best_score:\n                        global_best_score = candidate_score\n                        global_best_position = candidate\n                \n        return global_best_score, global_best_position\n```"}
{"role": "LLaMEA", "time": "2024-11-09 16:25:41.531831", "content": "You are a highly skilled computer scientist in the field of natural computing. Your task is to design novel metaheuristic algorithms to solve black box optimization problems.\n\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the BBOB test suite of 24 noiseless functions. Your task is to write the optimization algorithm in Python code. The code should contain an `__init__(self, budget, dim)` function and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. Each of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nGive an excellent and novel heuristic algorithm to solve this task and also give it a one-line description with the main idea.\n\nThe current population of algorithms already evaluated (name, description, score) is:\nRefinedParticleOptimizer: A refined particle swarm optimizer with dynamic acceleration coefficients and adaptive neighborhood-based velocity adjustment for improved convergence. (Score: 0.3490816234226132)\n\nThe selected solution to update is:\nA refined particle swarm optimizer with dynamic acceleration coefficients and adaptive neighborhood-based velocity adjustment for improved convergence.\n\nWith code:\nimport numpy as np\n\nclass RefinedParticleOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(30, self.budget // 2)  # Increased population size for better diversity\n        self.inertia_weight = 0.6  # Decreased inertia weight for faster convergence\n        self.cognitive_coeff = 1.5  \n        self.social_coeff = 1.5  # Adjusted coefficients for more balanced search\n        self.adaptive_scaling = 0.9  # More aggressive adaptive scaling\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-0.3, 0.3, (self.population_size, self.dim))  # Reduced velocity range\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        velocity_clamp = (self.upper_bound - self.lower_bound) * 0.15  # Adjusted dynamic velocity clamp\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            \n            velocities = np.clip(velocities, -velocity_clamp, velocity_clamp)\n            positions += velocities\n            \n            if eval_count % (self.budget // 4) == 0:  # More frequent adaptive adjustment\n                self.inertia_weight *= self.adaptive_scaling\n                perturbation = np.random.normal(0, 0.05, positions.shape)  # Lower perturbation for fine-tuning\n                positions += perturbation\n            \n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n                \n        return global_best_score, global_best_position\n\nThe algorithm RefinedParticleOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.35 with standard deviation 0.30.\n\n\nPlease transform this optimization algorithm code by exactly 20.0% in a \nway that enhances convergence speed. Keep the code difference precisely at \n20.0%, and focus solely on achieving performance improvements through \nalgorithmic adjustments across various elements of the code. Avoid focusing on \ncode efficiency; instead, prioritize exploration of diverse approaches within \nthe allowed modification percentage.\n\n\nProvide the Python code and a one-line description with the main idea (without enters). Give the response in the format:\n# Description: <short-description>\n# Code: \n# ```python\n# <code>\n# ```\n# \n"}
{"role": "gpt-4o", "time": "2024-11-09 16:25:53.810316", "content": "# Description: Introducing a hybrid strategy with differential evolution concepts for enhanced convergence within particle swarm optimization.\n# Code:\n# ```python\nimport numpy as np\n\nclass HybridPSO_DE_Optimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(40, self.budget // 2)  # Slightly increased population size\n        self.inertia_weight = 0.5  # Further reduced inertia weight\n        self.cognitive_coeff = 1.7  # Increased cognitive coefficient\n        self.social_coeff = 1.3  # Reduced social coefficient\n        self.adaptive_scaling = 0.85  # Adjusted adaptive scaling\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-0.2, 0.2, (self.population_size, self.dim))  # Adjusted velocity range\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        velocity_clamp = (self.upper_bound - self.lower_bound) * 0.1  # Further adjusted velocity clamp\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n\n            velocities = np.clip(velocities, -velocity_clamp, velocity_clamp)\n            positions += velocities\n\n            # Differential Evolution-like crossover and mutation\n            for i in range(self.population_size):\n                idxs = np.random.choice(self.population_size, 3, replace=False)\n                mutant = personal_best_positions[idxs[0]] + 0.8 * (personal_best_positions[idxs[1]] - personal_best_positions[idxs[2]])\n                cross_points = np.random.rand(self.dim) < 0.9\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, positions[i])\n                positions[i] = np.clip(trial, self.lower_bound, self.upper_bound)\n\n            if eval_count % (self.budget // 3) == 0:  # Modified adaptive adjustment frequency\n                self.inertia_weight *= self.adaptive_scaling\n                perturbation = np.random.normal(0, 0.03, positions.shape)  # Even lower perturbation for fine-tuning\n                positions += perturbation\n            \n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n                \n        return global_best_score, global_best_position\n# ```"}
{"role": "LLaMEA", "time": "2024-11-09 16:25:53.813918", "content": "You are a highly skilled computer scientist in the field of natural computing. Your task is to design novel metaheuristic algorithms to solve black box optimization problems.\n\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the BBOB test suite of 24 noiseless functions. Your task is to write the optimization algorithm in Python code. The code should contain an `__init__(self, budget, dim)` function and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. Each of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nGive an excellent and novel heuristic algorithm to solve this task and also give it a one-line description with the main idea.\n\nThe current population of algorithms already evaluated (name, description, score) is:\nRefinedParticleOptimizer: A refined particle swarm optimizer with dynamic acceleration coefficients and adaptive neighborhood-based velocity adjustment for improved convergence. (Score: 0.3490816234226132)\n\nThe selected solution to update is:\nA refined particle swarm optimizer with dynamic acceleration coefficients and adaptive neighborhood-based velocity adjustment for improved convergence.\n\nWith code:\nimport numpy as np\n\nclass RefinedParticleOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(30, self.budget // 2)  # Increased population size for better diversity\n        self.inertia_weight = 0.6  # Decreased inertia weight for faster convergence\n        self.cognitive_coeff = 1.5  \n        self.social_coeff = 1.5  # Adjusted coefficients for more balanced search\n        self.adaptive_scaling = 0.9  # More aggressive adaptive scaling\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-0.3, 0.3, (self.population_size, self.dim))  # Reduced velocity range\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        velocity_clamp = (self.upper_bound - self.lower_bound) * 0.15  # Adjusted dynamic velocity clamp\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            \n            velocities = np.clip(velocities, -velocity_clamp, velocity_clamp)\n            positions += velocities\n            \n            if eval_count % (self.budget // 4) == 0:  # More frequent adaptive adjustment\n                self.inertia_weight *= self.adaptive_scaling\n                perturbation = np.random.normal(0, 0.05, positions.shape)  # Lower perturbation for fine-tuning\n                positions += perturbation\n            \n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n                \n        return global_best_score, global_best_position\n\nThe algorithm RefinedParticleOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.35 with standard deviation 0.30.\n\n\nPlease transform this optimization algorithm code by exactly 20.0% in a \nway that enhances convergence speed. Keep the code difference precisely at \n20.0%, and focus solely on achieving performance improvements through \nalgorithmic adjustments across various elements of the code. Avoid focusing on \ncode efficiency; instead, prioritize exploration of diverse approaches within \nthe allowed modification percentage.\n\n\nProvide the Python code and a one-line description with the main idea (without enters). Give the response in the format:\n# Description: <short-description>\n# Code: \n# ```python\n# <code>\n# ```\n# \n"}
{"role": "gpt-4o", "time": "2024-11-09 16:26:03.316005", "content": "# Description: A dynamically tuned particle swarm optimizer with adaptive inertia and velocity strategies for accelerated convergence.\n# Code: \n# ```python\nimport numpy as np\n\nclass DynamicTunedPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = max(10, min(50, self.budget // 3))  # Adjust population size for better diversity and exploration\n        self.inertia_weight = 0.7  # Increased inertia weight for initial exploration\n        self.cognitive_coeff = 2.0  # Enhanced cognitive coefficient for better personal exploration\n        self.social_coeff = 1.2  # Slightly reduced social coefficient for focused exploitation\n        self.adaptive_scaling_factor = 0.95  # Gradual adaptive scaling for steady convergence\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-0.2, 0.2, (self.population_size, self.dim))  # Further reduced velocity range\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        velocity_clamp = (self.upper_bound - self.lower_bound) * 0.1  # Tighter velocity clamp for controlled movements\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            \n            velocities = np.clip(velocities, -velocity_clamp, velocity_clamp)\n            positions += velocities\n            \n            if eval_count % (self.budget // 3) == 0:  # Adaptive adjustment frequency slightly increased\n                self.inertia_weight *= self.adaptive_scaling_factor\n                perturbation = np.random.normal(0, 0.03, positions.shape)  # Further reduced perturbation for finer tuning\n                positions += perturbation\n            \n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n                \n        return global_best_score, global_best_position\n# ```"}
{"role": "LLaMEA", "time": "2024-11-09 16:26:03.319304", "content": "You are a highly skilled computer scientist in the field of natural computing. Your task is to design novel metaheuristic algorithms to solve black box optimization problems.\n\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the BBOB test suite of 24 noiseless functions. Your task is to write the optimization algorithm in Python code. The code should contain an `__init__(self, budget, dim)` function and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. Each of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nGive an excellent and novel heuristic algorithm to solve this task and also give it a one-line description with the main idea.\n\nThe current population of algorithms already evaluated (name, description, score) is:\nRefinedParticleOptimizer: A refined particle swarm optimizer with dynamic acceleration coefficients and adaptive neighborhood-based velocity adjustment for improved convergence. (Score: 0.3490816234226132)\n\nThe selected solution to update is:\nA refined particle swarm optimizer with dynamic acceleration coefficients and adaptive neighborhood-based velocity adjustment for improved convergence.\n\nWith code:\nimport numpy as np\n\nclass RefinedParticleOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(30, self.budget // 2)  # Increased population size for better diversity\n        self.inertia_weight = 0.6  # Decreased inertia weight for faster convergence\n        self.cognitive_coeff = 1.5  \n        self.social_coeff = 1.5  # Adjusted coefficients for more balanced search\n        self.adaptive_scaling = 0.9  # More aggressive adaptive scaling\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-0.3, 0.3, (self.population_size, self.dim))  # Reduced velocity range\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        velocity_clamp = (self.upper_bound - self.lower_bound) * 0.15  # Adjusted dynamic velocity clamp\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            \n            velocities = np.clip(velocities, -velocity_clamp, velocity_clamp)\n            positions += velocities\n            \n            if eval_count % (self.budget // 4) == 0:  # More frequent adaptive adjustment\n                self.inertia_weight *= self.adaptive_scaling\n                perturbation = np.random.normal(0, 0.05, positions.shape)  # Lower perturbation for fine-tuning\n                positions += perturbation\n            \n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n                \n        return global_best_score, global_best_position\n\nThe algorithm RefinedParticleOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.35 with standard deviation 0.30.\n\n\nPlease transform this optimization algorithm code by exactly 20.0% in a \nway that enhances convergence speed. Keep the code difference precisely at \n20.0%, and focus solely on achieving performance improvements through \nalgorithmic adjustments across various elements of the code. Avoid focusing on \ncode efficiency; instead, prioritize exploration of diverse approaches within \nthe allowed modification percentage.\n\n\nProvide the Python code and a one-line description with the main idea (without enters). Give the response in the format:\n# Description: <short-description>\n# Code: \n# ```python\n# <code>\n# ```\n# \n"}
{"role": "gpt-4o", "time": "2024-11-09 16:26:16.395578", "content": "# Description: A semi-randomized particle swarm optimizer with adaptive inertia and personal adjustments for enhanced convergence speed.\n# Code:\n# ```python\nimport numpy as np\n\nclass EnhancedParticleOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(35, self.budget // 2)  # Slightly larger population size for increased exploration\n        self.inertia_weight = 0.7  # Increased initial inertia weight for keeping diversity longer\n        self.cognitive_coeff = 1.3  # Reduced cognitive coefficient for smoother personal adjustments\n        self.social_coeff = 1.7  # Increased social coefficient for stronger global convergence\n        self.adaptive_scaling = 0.85  # Slightly more conservative adaptive scaling\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-0.4, 0.4, (self.population_size, self.dim))  # Expanded velocity range\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        velocity_clamp = (self.upper_bound - self.lower_bound) * 0.2  # Increased dynamic velocity clamp\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            \n            velocities = np.clip(velocities, -velocity_clamp, velocity_clamp)\n            positions += velocities\n            \n            if eval_count % (self.budget // 3) == 0:  # Adjusted frequency of adaptive adjustment\n                self.inertia_weight *= self.adaptive_scaling\n                perturbation = np.random.normal(0, 0.1, positions.shape)  # Increased perturbation for broader search\n                positions += perturbation\n            \n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n                \n        return global_best_score, global_best_position\n# ```"}
{"role": "LLaMEA", "time": "2024-11-09 16:26:16.399000", "content": "You are a highly skilled computer scientist in the field of natural computing. Your task is to design novel metaheuristic algorithms to solve black box optimization problems.\n\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the BBOB test suite of 24 noiseless functions. Your task is to write the optimization algorithm in Python code. The code should contain an `__init__(self, budget, dim)` function and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. Each of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nGive an excellent and novel heuristic algorithm to solve this task and also give it a one-line description with the main idea.\n\nThe current population of algorithms already evaluated (name, description, score) is:\nRefinedParticleOptimizer: A refined particle swarm optimizer with dynamic acceleration coefficients and adaptive neighborhood-based velocity adjustment for improved convergence. (Score: 0.3490816234226132)\n\nThe selected solution to update is:\nA refined particle swarm optimizer with dynamic acceleration coefficients and adaptive neighborhood-based velocity adjustment for improved convergence.\n\nWith code:\nimport numpy as np\n\nclass RefinedParticleOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(30, self.budget // 2)  # Increased population size for better diversity\n        self.inertia_weight = 0.6  # Decreased inertia weight for faster convergence\n        self.cognitive_coeff = 1.5  \n        self.social_coeff = 1.5  # Adjusted coefficients for more balanced search\n        self.adaptive_scaling = 0.9  # More aggressive adaptive scaling\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-0.3, 0.3, (self.population_size, self.dim))  # Reduced velocity range\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        velocity_clamp = (self.upper_bound - self.lower_bound) * 0.15  # Adjusted dynamic velocity clamp\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            \n            velocities = np.clip(velocities, -velocity_clamp, velocity_clamp)\n            positions += velocities\n            \n            if eval_count % (self.budget // 4) == 0:  # More frequent adaptive adjustment\n                self.inertia_weight *= self.adaptive_scaling\n                perturbation = np.random.normal(0, 0.05, positions.shape)  # Lower perturbation for fine-tuning\n                positions += perturbation\n            \n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n                \n        return global_best_score, global_best_position\n\nThe algorithm RefinedParticleOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.35 with standard deviation 0.30.\n\n\nPlease transform this optimization algorithm code by exactly 20.0% in a \nway that enhances convergence speed. Keep the code difference precisely at \n20.0%, and focus solely on achieving performance improvements through \nalgorithmic adjustments across various elements of the code. Avoid focusing on \ncode efficiency; instead, prioritize exploration of diverse approaches within \nthe allowed modification percentage.\n\n\nProvide the Python code and a one-line description with the main idea (without enters). Give the response in the format:\n# Description: <short-description>\n# Code: \n# ```python\n# <code>\n# ```\n# \n"}
{"role": "gpt-4o", "time": "2024-11-09 16:26:27.803258", "content": "# Description: A hybrid particle swarm and differential evolution optimizer with adaptive mutation for improved convergence speed.\n# Code:\n# ```python\nimport numpy as np\n\nclass HybridParticleDEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(30, self.budget // 2)\n        self.inertia_weight = 0.5\n        self.cognitive_coeff = 1.4\n        self.social_coeff = 1.6\n        self.adaptive_scaling = 0.85\n        self.mutation_factor = 0.8\n        self.crossover_rate = 0.9\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-0.2, 0.2, (self.population_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        velocity_clamp = (self.upper_bound - self.lower_bound) * 0.15\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            \n            velocities = np.clip(velocities, -velocity_clamp, velocity_clamp)\n            positions += velocities\n            \n            if eval_count % (self.budget // 4) == 0:\n                self.inertia_weight *= self.adaptive_scaling\n                for i in range(self.population_size):\n                    if np.random.rand() < self.crossover_rate:\n                        idxs = [idx for idx in range(self.population_size) if idx != i]\n                        a, b, c = positions[np.random.choice(idxs, 3, replace=False)]\n                        mutant = a + self.mutation_factor * (b - c)\n                        trial = np.where(np.random.rand(self.dim) < self.crossover_rate, mutant, positions[i])\n                        trial = np.clip(trial, self.lower_bound, self.upper_bound)\n                        trial_score = func(trial)\n                        eval_count += 1\n                        if trial_score < personal_best_scores[i]:\n                            personal_best_positions[i] = trial\n                            personal_best_scores[i] = trial_score\n                            if trial_score < global_best_score:\n                                global_best_score = trial_score\n                                global_best_position = trial\n            \n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n                \n        return global_best_score, global_best_position\n# ```"}
{"role": "LLaMEA", "time": "2024-11-09 16:26:27.806862", "content": "You are a highly skilled computer scientist in the field of natural computing. Your task is to design novel metaheuristic algorithms to solve black box optimization problems.\n\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the BBOB test suite of 24 noiseless functions. Your task is to write the optimization algorithm in Python code. The code should contain an `__init__(self, budget, dim)` function and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. Each of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nGive an excellent and novel heuristic algorithm to solve this task and also give it a one-line description with the main idea.\n\nThe current population of algorithms already evaluated (name, description, score) is:\nRefinedParticleOptimizer: A refined particle swarm optimizer with dynamic acceleration coefficients and adaptive neighborhood-based velocity adjustment for improved convergence. (Score: 0.3490816234226132)\n\nThe selected solution to update is:\nA refined particle swarm optimizer with dynamic acceleration coefficients and adaptive neighborhood-based velocity adjustment for improved convergence.\n\nWith code:\nimport numpy as np\n\nclass RefinedParticleOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(30, self.budget // 2)  # Increased population size for better diversity\n        self.inertia_weight = 0.6  # Decreased inertia weight for faster convergence\n        self.cognitive_coeff = 1.5  \n        self.social_coeff = 1.5  # Adjusted coefficients for more balanced search\n        self.adaptive_scaling = 0.9  # More aggressive adaptive scaling\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-0.3, 0.3, (self.population_size, self.dim))  # Reduced velocity range\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        velocity_clamp = (self.upper_bound - self.lower_bound) * 0.15  # Adjusted dynamic velocity clamp\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            \n            velocities = np.clip(velocities, -velocity_clamp, velocity_clamp)\n            positions += velocities\n            \n            if eval_count % (self.budget // 4) == 0:  # More frequent adaptive adjustment\n                self.inertia_weight *= self.adaptive_scaling\n                perturbation = np.random.normal(0, 0.05, positions.shape)  # Lower perturbation for fine-tuning\n                positions += perturbation\n            \n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n                \n        return global_best_score, global_best_position\n\nThe algorithm RefinedParticleOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.35 with standard deviation 0.30.\n\n\nPlease transform this optimization algorithm code by exactly 20.0% in a \nway that enhances convergence speed. Keep the code difference precisely at \n20.0%, and focus solely on achieving performance improvements through \nalgorithmic adjustments across various elements of the code. Avoid focusing on \ncode efficiency; instead, prioritize exploration of diverse approaches within \nthe allowed modification percentage.\n\n\nProvide the Python code and a one-line description with the main idea (without enters). Give the response in the format:\n# Description: <short-description>\n# Code: \n# ```python\n# <code>\n# ```\n# \n"}
{"role": "gpt-4o", "time": "2024-11-09 16:26:38.162861", "content": "# Description: A hybrid particle swarm algorithm with neighborhood best consideration and chaotic perturbation for improved diversity and convergence speed.\n# Code:\n# ```python\nimport numpy as np\n\nclass HybridParticleOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(40, self.budget // 3)  # Slightly increased population size\n        self.inertia_weight = 0.5  # Further decreased inertia weight\n        self.cognitive_coeff = 1.7  # Increased cognitive coefficient\n        self.social_coeff = 1.4  # Reduced social coefficient\n        self.neighborhood_coeff = 1.5  # Added new parameter for neighborhood best\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-0.4, 0.4, (self.population_size, self.dim))  # Slightly increased velocity range\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n\n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n\n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n\n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        velocity_clamp = (self.upper_bound - self.lower_bound) * 0.1  # Adjusted dynamic velocity clamp\n\n        def neighborhood_best(positions, scores):\n            neighborhood_indices = np.argsort(scores)[:5]  # Consider a neighborhood of top 5\n            return np.mean(positions[neighborhood_indices], axis=0)\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            r3 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            neighborhood_best_position = neighborhood_best(positions, scores)\n            neighborhood_component = self.neighborhood_coeff * r3 * (neighborhood_best_position - positions)\n\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component + neighborhood_component\n            velocities = np.clip(velocities, -velocity_clamp, velocity_clamp)\n            positions += velocities\n\n            if eval_count % (self.budget // 3) == 0:  # Chaotic perturbation for exploration\n                perturbation = np.random.standard_normal(positions.shape) * 0.1\n                positions += perturbation\n\n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n\n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n\n        return global_best_score, global_best_position\n# ```"}
{"role": "LLaMEA", "time": "2024-11-09 16:26:38.166458", "content": "You are a highly skilled computer scientist in the field of natural computing. Your task is to design novel metaheuristic algorithms to solve black box optimization problems.\n\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the BBOB test suite of 24 noiseless functions. Your task is to write the optimization algorithm in Python code. The code should contain an `__init__(self, budget, dim)` function and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. Each of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nGive an excellent and novel heuristic algorithm to solve this task and also give it a one-line description with the main idea.\n\nThe current population of algorithms already evaluated (name, description, score) is:\nRefinedParticleOptimizer: A refined particle swarm optimizer with dynamic acceleration coefficients and adaptive neighborhood-based velocity adjustment for improved convergence. (Score: 0.3490816234226132)\n\nThe selected solution to update is:\nA refined particle swarm optimizer with dynamic acceleration coefficients and adaptive neighborhood-based velocity adjustment for improved convergence.\n\nWith code:\nimport numpy as np\n\nclass RefinedParticleOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(30, self.budget // 2)  # Increased population size for better diversity\n        self.inertia_weight = 0.6  # Decreased inertia weight for faster convergence\n        self.cognitive_coeff = 1.5  \n        self.social_coeff = 1.5  # Adjusted coefficients for more balanced search\n        self.adaptive_scaling = 0.9  # More aggressive adaptive scaling\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-0.3, 0.3, (self.population_size, self.dim))  # Reduced velocity range\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        velocity_clamp = (self.upper_bound - self.lower_bound) * 0.15  # Adjusted dynamic velocity clamp\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            \n            velocities = np.clip(velocities, -velocity_clamp, velocity_clamp)\n            positions += velocities\n            \n            if eval_count % (self.budget // 4) == 0:  # More frequent adaptive adjustment\n                self.inertia_weight *= self.adaptive_scaling\n                perturbation = np.random.normal(0, 0.05, positions.shape)  # Lower perturbation for fine-tuning\n                positions += perturbation\n            \n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n                \n        return global_best_score, global_best_position\n\nThe algorithm RefinedParticleOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.35 with standard deviation 0.30.\n\n\nPlease transform this optimization algorithm code by exactly 20.0% in a \nway that enhances convergence speed. Keep the code difference precisely at \n20.0%, and focus solely on achieving performance improvements through \nalgorithmic adjustments across various elements of the code. Avoid focusing on \ncode efficiency; instead, prioritize exploration of diverse approaches within \nthe allowed modification percentage.\n\n\nProvide the Python code and a one-line description with the main idea (without enters). Give the response in the format:\n# Description: <short-description>\n# Code: \n# ```python\n# <code>\n# ```\n# \n"}
{"role": "gpt-4o", "time": "2024-11-09 16:26:48.532597", "content": "# Description: An enhanced particle swarm optimizer with non-linear inertia weight decay and adaptive velocity control for accelerated convergence.\n# Code:\n# ```python\nimport numpy as np\n\nclass EnhancedParticleOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(40, self.budget // 2)  # Slightly increased population for better exploration\n        self.inertia_weight = 0.7  # Slightly increased inertia for initial exploration\n        self.cognitive_coeff = 1.4\n        self.social_coeff = 1.6  # More aggressive social component\n        self.adaptive_scaling = 0.95  # Less aggressive scaling, promotes stability\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-0.2, 0.2, (self.population_size, self.dim))  # More refined velocity range\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        velocity_clamp = (self.upper_bound - self.lower_bound) * 0.2  # Slightly increased clamp for flexibility\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            \n            velocities = np.clip(velocities, -velocity_clamp, velocity_clamp)\n            positions += velocities\n            \n            if eval_count % (self.budget // 3) == 0:  # More frequent, less aggressive adaptive adjustment\n                self.inertia_weight *= self.adaptive_scaling\n                perturbation = np.random.normal(0, 0.03, positions.shape)  # Finer perturbation for precision\n                positions += perturbation\n            \n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n                \n        return global_best_score, global_best_position\n# ```"}
{"role": "LLaMEA", "time": "2024-11-09 16:26:48.535974", "content": "You are a highly skilled computer scientist in the field of natural computing. Your task is to design novel metaheuristic algorithms to solve black box optimization problems.\n\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the BBOB test suite of 24 noiseless functions. Your task is to write the optimization algorithm in Python code. The code should contain an `__init__(self, budget, dim)` function and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. Each of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nGive an excellent and novel heuristic algorithm to solve this task and also give it a one-line description with the main idea.\n\nThe current population of algorithms already evaluated (name, description, score) is:\nRefinedParticleOptimizer: A refined particle swarm optimizer with dynamic acceleration coefficients and adaptive neighborhood-based velocity adjustment for improved convergence. (Score: 0.3490816234226132)\n\nThe selected solution to update is:\nA refined particle swarm optimizer with dynamic acceleration coefficients and adaptive neighborhood-based velocity adjustment for improved convergence.\n\nWith code:\nimport numpy as np\n\nclass RefinedParticleOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(30, self.budget // 2)  # Increased population size for better diversity\n        self.inertia_weight = 0.6  # Decreased inertia weight for faster convergence\n        self.cognitive_coeff = 1.5  \n        self.social_coeff = 1.5  # Adjusted coefficients for more balanced search\n        self.adaptive_scaling = 0.9  # More aggressive adaptive scaling\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-0.3, 0.3, (self.population_size, self.dim))  # Reduced velocity range\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        velocity_clamp = (self.upper_bound - self.lower_bound) * 0.15  # Adjusted dynamic velocity clamp\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            \n            velocities = np.clip(velocities, -velocity_clamp, velocity_clamp)\n            positions += velocities\n            \n            if eval_count % (self.budget // 4) == 0:  # More frequent adaptive adjustment\n                self.inertia_weight *= self.adaptive_scaling\n                perturbation = np.random.normal(0, 0.05, positions.shape)  # Lower perturbation for fine-tuning\n                positions += perturbation\n            \n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n                \n        return global_best_score, global_best_position\n\nThe algorithm RefinedParticleOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.35 with standard deviation 0.30.\n\n\nPlease transform this optimization algorithm code by exactly 20.0% in a \nway that enhances convergence speed. Keep the code difference precisely at \n20.0%, and focus solely on achieving performance improvements through \nalgorithmic adjustments across various elements of the code. Avoid focusing on \ncode efficiency; instead, prioritize exploration of diverse approaches within \nthe allowed modification percentage.\n\n\nProvide the Python code and a one-line description with the main idea (without enters). Give the response in the format:\n# Description: <short-description>\n# Code: \n# ```python\n# <code>\n# ```\n# \n"}
{"role": "gpt-4o", "time": "2024-11-09 16:26:56.838871", "content": "# Description: Enhanced Particle Swarm Optimizer with elitist strategy and adaptive inertia for improved exploration and convergence speed.\n# Code:\n# ```python\nimport numpy as np\n\nclass EnhancedParticleOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(35, self.budget // 2)  # Slightly increased population size\n        self.inertia_weight = 0.7  # Adjusted inertia weight for better balance\n        self.cognitive_coeff = 1.4  \n        self.social_coeff = 1.8  # Increased social coefficient for stronger global attraction\n        self.adaptive_scaling = 0.85  # Slightly more aggressive adaptive scaling\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-0.2, 0.2, (self.population_size, self.dim))  # Further reduced velocity range\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        velocity_clamp = (self.upper_bound - self.lower_bound) * 0.1  # Tighter dynamic velocity clamp\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            \n            velocities = np.clip(velocities, -velocity_clamp, velocity_clamp)\n            positions += velocities\n            \n            if eval_count % (self.budget // 5) == 0:  # More frequent adaptive adjustment\n                self.inertia_weight *= self.adaptive_scaling\n                perturbation = np.random.normal(0, 0.03, positions.shape)  # Decreased perturbation for precision\n                positions += perturbation\n            \n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n                \n            # Elitist strategy: replace worst particles with best known position\n            worst_indices = np.argsort(scores)[-2:]  # Replace two worst performing particles\n            positions[worst_indices] = global_best_position + np.random.normal(0, 0.02, (2, self.dim))  # Slight perturbation\n\n        return global_best_score, global_best_position\n# ```"}
{"role": "LLaMEA", "time": "2024-11-09 16:26:56.842427", "content": "You are a highly skilled computer scientist in the field of natural computing. Your task is to design novel metaheuristic algorithms to solve black box optimization problems.\n\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the BBOB test suite of 24 noiseless functions. Your task is to write the optimization algorithm in Python code. The code should contain an `__init__(self, budget, dim)` function and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. Each of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nGive an excellent and novel heuristic algorithm to solve this task and also give it a one-line description with the main idea.\n\nThe current population of algorithms already evaluated (name, description, score) is:\nRefinedParticleOptimizer: A refined particle swarm optimizer with dynamic acceleration coefficients and adaptive neighborhood-based velocity adjustment for improved convergence. (Score: 0.3490816234226132)\n\nThe selected solution to update is:\nA refined particle swarm optimizer with dynamic acceleration coefficients and adaptive neighborhood-based velocity adjustment for improved convergence.\n\nWith code:\nimport numpy as np\n\nclass RefinedParticleOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(30, self.budget // 2)  # Increased population size for better diversity\n        self.inertia_weight = 0.6  # Decreased inertia weight for faster convergence\n        self.cognitive_coeff = 1.5  \n        self.social_coeff = 1.5  # Adjusted coefficients for more balanced search\n        self.adaptive_scaling = 0.9  # More aggressive adaptive scaling\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-0.3, 0.3, (self.population_size, self.dim))  # Reduced velocity range\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        velocity_clamp = (self.upper_bound - self.lower_bound) * 0.15  # Adjusted dynamic velocity clamp\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            \n            velocities = np.clip(velocities, -velocity_clamp, velocity_clamp)\n            positions += velocities\n            \n            if eval_count % (self.budget // 4) == 0:  # More frequent adaptive adjustment\n                self.inertia_weight *= self.adaptive_scaling\n                perturbation = np.random.normal(0, 0.05, positions.shape)  # Lower perturbation for fine-tuning\n                positions += perturbation\n            \n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n                \n        return global_best_score, global_best_position\n\nThe algorithm RefinedParticleOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.35 with standard deviation 0.30.\n\n\nPlease transform this optimization algorithm code by exactly 20.0% in a \nway that enhances convergence speed. Keep the code difference precisely at \n20.0%, and focus solely on achieving performance improvements through \nalgorithmic adjustments across various elements of the code. Avoid focusing on \ncode efficiency; instead, prioritize exploration of diverse approaches within \nthe allowed modification percentage.\n\n\nProvide the Python code and a one-line description with the main idea (without enters). Give the response in the format:\n# Description: <short-description>\n# Code: \n# ```python\n# <code>\n# ```\n# \n"}
{"role": "gpt-4o", "time": "2024-11-09 16:27:05.816799", "content": "# Description: Enhanced Particle Swarm Optimizer with dynamic inertia, adaptive learning rates, and neighborhood interaction to boost convergence speed.\n# Code:\n# ```python\nimport numpy as np\n\nclass EnhancedParticleOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(40, self.budget // 2)  # Increased population size for higher diversity\n        self.inertia_weight_initial = 0.7  # Dynamically adjusted inertia weight\n        self.inertia_weight_final = 0.3\n        self.cognitive_coeff = 2.0  # Increased cognitive component for exploration\n        self.social_coeff = 1.3  # Slightly reduced social component for adaptive balance\n        self.adaptive_scaling = 0.95  # Less aggressive scaling for stability\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-0.5, 0.5, (self.population_size, self.dim))  # Expanded velocity range\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        velocity_clamp = (self.upper_bound - self.lower_bound) * 0.2  # Adjusted velocity clamp for wider exploration\n\n        while eval_count < self.budget:\n            iter_fraction = eval_count / self.budget\n            inertia_weight = (self.inertia_weight_initial - self.inertia_weight_final) * (1 - iter_fraction) + self.inertia_weight_final\n            \n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = inertia_weight * velocities + cognitive_component + social_component\n            \n            velocities = np.clip(velocities, -velocity_clamp, velocity_clamp)\n            positions += velocities\n            \n            perturbation = np.random.normal(0, 0.03, positions.shape)  # Lower perturbation for stability\n            positions += perturbation\n            \n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n                \n        return global_best_score, global_best_position\n# ```"}
{"role": "LLaMEA", "time": "2024-11-09 16:27:05.820125", "content": "You are a highly skilled computer scientist in the field of natural computing. Your task is to design novel metaheuristic algorithms to solve black box optimization problems.\n\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the BBOB test suite of 24 noiseless functions. Your task is to write the optimization algorithm in Python code. The code should contain an `__init__(self, budget, dim)` function and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. Each of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nGive an excellent and novel heuristic algorithm to solve this task and also give it a one-line description with the main idea.\n\nThe current population of algorithms already evaluated (name, description, score) is:\nRefinedParticleOptimizer: A refined particle swarm optimizer with dynamic acceleration coefficients and adaptive neighborhood-based velocity adjustment for improved convergence. (Score: 0.3490816234226132)\n\nThe selected solution to update is:\nA refined particle swarm optimizer with dynamic acceleration coefficients and adaptive neighborhood-based velocity adjustment for improved convergence.\n\nWith code:\nimport numpy as np\n\nclass RefinedParticleOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(30, self.budget // 2)  # Increased population size for better diversity\n        self.inertia_weight = 0.6  # Decreased inertia weight for faster convergence\n        self.cognitive_coeff = 1.5  \n        self.social_coeff = 1.5  # Adjusted coefficients for more balanced search\n        self.adaptive_scaling = 0.9  # More aggressive adaptive scaling\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-0.3, 0.3, (self.population_size, self.dim))  # Reduced velocity range\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        velocity_clamp = (self.upper_bound - self.lower_bound) * 0.15  # Adjusted dynamic velocity clamp\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            \n            velocities = np.clip(velocities, -velocity_clamp, velocity_clamp)\n            positions += velocities\n            \n            if eval_count % (self.budget // 4) == 0:  # More frequent adaptive adjustment\n                self.inertia_weight *= self.adaptive_scaling\n                perturbation = np.random.normal(0, 0.05, positions.shape)  # Lower perturbation for fine-tuning\n                positions += perturbation\n            \n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n                \n        return global_best_score, global_best_position\n\nThe algorithm RefinedParticleOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.35 with standard deviation 0.30.\n\n\nPlease transform this optimization algorithm code by exactly 20.0% in a \nway that enhances convergence speed. Keep the code difference precisely at \n20.0%, and focus solely on achieving performance improvements through \nalgorithmic adjustments across various elements of the code. Avoid focusing on \ncode efficiency; instead, prioritize exploration of diverse approaches within \nthe allowed modification percentage.\n\n\nProvide the Python code and a one-line description with the main idea (without enters). Give the response in the format:\n# Description: <short-description>\n# Code: \n# ```python\n# <code>\n# ```\n# \n"}
{"role": "gpt-4o", "time": "2024-11-09 16:27:19.119191", "content": "# Description: An enhanced particle swarm optimizer with stochastic velocity mutation and dynamic population resizing for improved exploration and convergence.\n# Code:\n# ```python\nimport numpy as np\n\nclass EnhancedParticleOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.initial_population_size = min(40, self.budget // 2)  # Larger initial population for better exploration\n        self.inertia_weight = 0.7  # Increased inertia weight to maintain diversity\n        self.cognitive_coeff = 2.0  # Higher cognitive coefficient to enhance personal best exploration\n        self.social_coeff = 1.3  # Reduced social coefficient to prevent premature convergence\n        self.dynamic_scaling = 0.95  # Slightly less aggressive scaling for gradual convergence\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.initial_population_size, self.dim))\n        velocities = np.random.uniform(-0.4, 0.4, (self.initial_population_size, self.dim))  # Increased initial velocity range\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.initial_population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.initial_population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        velocity_clamp = (self.upper_bound - self.lower_bound) * 0.2  # Broader dynamic velocity clamp\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.initial_population_size, self.dim)\n            r2 = np.random.rand(self.initial_population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            \n            # Introduce stochastic velocity mutation\n            mutation_strength = np.random.normal(0, 0.1, velocities.shape)\n            velocities += mutation_strength\n            velocities = np.clip(velocities, -velocity_clamp, velocity_clamp)\n            positions += velocities\n\n            if eval_count % (self.budget // 3) == 0:  # Adjusted for dynamic scaling\n                self.inertia_weight *= self.dynamic_scaling\n                perturbation = np.random.normal(0, 0.07, positions.shape)  # Adjusted perturbation for local search enhancement\n                positions += perturbation\n            \n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.initial_population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n\n            # Dynamic population resizing\n            if eval_count < self.budget // 2:\n                positions = np.concatenate((positions, np.random.uniform(self.lower_bound, self.upper_bound, (5, self.dim))), axis=0)\n                velocities = np.concatenate((velocities, np.random.uniform(-0.4, 0.4, (5, self.dim))), axis=0)\n                personal_best_positions = np.concatenate((personal_best_positions, np.copy(positions[-5:])), axis=0)\n                personal_best_scores = np.concatenate((personal_best_scores, np.full(5, np.inf)), axis=0)\n\n        return global_best_score, global_best_position\n# ```"}
{"role": "LLaMEA", "time": "2024-11-09 16:27:19.122775", "content": "You are a highly skilled computer scientist in the field of natural computing. Your task is to design novel metaheuristic algorithms to solve black box optimization problems.\n\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the BBOB test suite of 24 noiseless functions. Your task is to write the optimization algorithm in Python code. The code should contain an `__init__(self, budget, dim)` function and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. Each of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nGive an excellent and novel heuristic algorithm to solve this task and also give it a one-line description with the main idea.\n\nThe current population of algorithms already evaluated (name, description, score) is:\nRefinedParticleOptimizer: A refined particle swarm optimizer with dynamic acceleration coefficients and adaptive neighborhood-based velocity adjustment for improved convergence. (Score: 0.3490816234226132)\n\nThe selected solution to update is:\nA refined particle swarm optimizer with dynamic acceleration coefficients and adaptive neighborhood-based velocity adjustment for improved convergence.\n\nWith code:\nimport numpy as np\n\nclass RefinedParticleOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(30, self.budget // 2)  # Increased population size for better diversity\n        self.inertia_weight = 0.6  # Decreased inertia weight for faster convergence\n        self.cognitive_coeff = 1.5  \n        self.social_coeff = 1.5  # Adjusted coefficients for more balanced search\n        self.adaptive_scaling = 0.9  # More aggressive adaptive scaling\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-0.3, 0.3, (self.population_size, self.dim))  # Reduced velocity range\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        velocity_clamp = (self.upper_bound - self.lower_bound) * 0.15  # Adjusted dynamic velocity clamp\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            \n            velocities = np.clip(velocities, -velocity_clamp, velocity_clamp)\n            positions += velocities\n            \n            if eval_count % (self.budget // 4) == 0:  # More frequent adaptive adjustment\n                self.inertia_weight *= self.adaptive_scaling\n                perturbation = np.random.normal(0, 0.05, positions.shape)  # Lower perturbation for fine-tuning\n                positions += perturbation\n            \n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n                \n        return global_best_score, global_best_position\n\nThe algorithm RefinedParticleOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.35 with standard deviation 0.30.\n\n\nPlease transform this optimization algorithm code by exactly 20.0% in a \nway that enhances convergence speed. Keep the code difference precisely at \n20.0%, and focus solely on achieving performance improvements through \nalgorithmic adjustments across various elements of the code. Avoid focusing on \ncode efficiency; instead, prioritize exploration of diverse approaches within \nthe allowed modification percentage.\n\n\nProvide the Python code and a one-line description with the main idea (without enters). Give the response in the format:\n# Description: <short-description>\n# Code: \n# ```python\n# <code>\n# ```\n# \n"}
{"role": "gpt-4o", "time": "2024-11-09 16:27:30.534848", "content": "# Description: An enhanced particle swarm optimizer using dynamic subpopulation grouping and momentum control for faster convergence.\n# Code:\n# ```python\nimport numpy as np\n\nclass EnhancedParticleOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(40, self.budget // 2)  # Larger population for better coverage\n        self.inertia_weight = 0.7  # Initial inertia to maintain exploration\n        self.cognitive_coeff = 1.4  \n        self.social_coeff = 1.6  # More influence from social component\n        self.adaptive_scaling = 0.85  # Slightly less aggressive scaling\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-0.4, 0.4, (self.population_size, self.dim))  # Increased velocity range for diverse exploration\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        velocity_clamp = (self.upper_bound - self.lower_bound) * 0.2  # Increased velocity clamp for broader search\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            \n            velocities = np.clip(velocities, -velocity_clamp, velocity_clamp)\n            positions += velocities\n            \n            if eval_count % (self.budget // 5) == 0:  # More frequent tuning with momentum control\n                self.inertia_weight *= self.adaptive_scaling\n                subpop_size = self.population_size // 4\n                for i in range(0, self.population_size, subpop_size):\n                    perturbation = np.random.normal(0, 0.03, (subpop_size, self.dim))  # Slightly smaller perturbation\n                    positions[i:i+subpop_size] += perturbation\n            \n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n                \n        return global_best_score, global_best_position\n# ```"}
{"role": "LLaMEA", "time": "2024-11-09 16:27:30.538304", "content": "You are a highly skilled computer scientist in the field of natural computing. Your task is to design novel metaheuristic algorithms to solve black box optimization problems.\n\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the BBOB test suite of 24 noiseless functions. Your task is to write the optimization algorithm in Python code. The code should contain an `__init__(self, budget, dim)` function and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. Each of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nGive an excellent and novel heuristic algorithm to solve this task and also give it a one-line description with the main idea.\n\nThe current population of algorithms already evaluated (name, description, score) is:\nRefinedParticleOptimizer: A refined particle swarm optimizer with dynamic acceleration coefficients and adaptive neighborhood-based velocity adjustment for improved convergence. (Score: 0.3490816234226132)\n\nThe selected solution to update is:\nA refined particle swarm optimizer with dynamic acceleration coefficients and adaptive neighborhood-based velocity adjustment for improved convergence.\n\nWith code:\nimport numpy as np\n\nclass RefinedParticleOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(30, self.budget // 2)  # Increased population size for better diversity\n        self.inertia_weight = 0.6  # Decreased inertia weight for faster convergence\n        self.cognitive_coeff = 1.5  \n        self.social_coeff = 1.5  # Adjusted coefficients for more balanced search\n        self.adaptive_scaling = 0.9  # More aggressive adaptive scaling\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-0.3, 0.3, (self.population_size, self.dim))  # Reduced velocity range\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        velocity_clamp = (self.upper_bound - self.lower_bound) * 0.15  # Adjusted dynamic velocity clamp\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            \n            velocities = np.clip(velocities, -velocity_clamp, velocity_clamp)\n            positions += velocities\n            \n            if eval_count % (self.budget // 4) == 0:  # More frequent adaptive adjustment\n                self.inertia_weight *= self.adaptive_scaling\n                perturbation = np.random.normal(0, 0.05, positions.shape)  # Lower perturbation for fine-tuning\n                positions += perturbation\n            \n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n                \n        return global_best_score, global_best_position\n\nThe algorithm RefinedParticleOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.35 with standard deviation 0.30.\n\n\nPlease transform this optimization algorithm code by exactly 20.0% in a \nway that enhances convergence speed. Keep the code difference precisely at \n20.0%, and focus solely on achieving performance improvements through \nalgorithmic adjustments across various elements of the code. Avoid focusing on \ncode efficiency; instead, prioritize exploration of diverse approaches within \nthe allowed modification percentage.\n\n\nProvide the Python code and a one-line description with the main idea (without enters). Give the response in the format:\n# Description: <short-description>\n# Code: \n# ```python\n# <code>\n# ```\n# \n"}
{"role": "gpt-4o", "time": "2024-11-09 16:27:40.717881", "content": "# Description: A hybrid particle swarm optimizer with adaptive inertia and mutation strategy for enhanced convergence speed.\n# Code:\n# ```python\nimport numpy as np\n\nclass HybridParticleOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(40, self.budget // 3)  # Increased population size for more exploration\n        self.inertia_weight = 0.7  # Adjusted inertia weight for better balance\n        self.cognitive_coeff = 1.7  \n        self.social_coeff = 1.7  # Slightly increased coefficients for stronger guidance\n        self.adaptive_scaling = 0.95  # Adaptive scaling with a softer decay\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-0.2, 0.2, (self.population_size, self.dim))  # Adjusted velocity range\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        velocity_clamp = (self.upper_bound - self.lower_bound) * 0.1  # Adjusted dynamic velocity clamp\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            \n            velocities = np.clip(velocities, -velocity_clamp, velocity_clamp)\n            positions += velocities\n            \n            if eval_count % (self.budget // 3) == 0:  # Adaptive adjustment with mutation\n                self.inertia_weight *= self.adaptive_scaling\n                mutation = np.random.normal(0, 0.1, positions.shape)  # Introduced mutation for diversity\n                positions += mutation\n            \n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n                \n        return global_best_score, global_best_position\n# ```"}
{"role": "LLaMEA", "time": "2024-11-09 16:27:40.721382", "content": "You are a highly skilled computer scientist in the field of natural computing. Your task is to design novel metaheuristic algorithms to solve black box optimization problems.\n\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the BBOB test suite of 24 noiseless functions. Your task is to write the optimization algorithm in Python code. The code should contain an `__init__(self, budget, dim)` function and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. Each of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nGive an excellent and novel heuristic algorithm to solve this task and also give it a one-line description with the main idea.\n\nThe current population of algorithms already evaluated (name, description, score) is:\nRefinedParticleOptimizer: A refined particle swarm optimizer with dynamic acceleration coefficients and adaptive neighborhood-based velocity adjustment for improved convergence. (Score: 0.3490816234226132)\n\nThe selected solution to update is:\nA refined particle swarm optimizer with dynamic acceleration coefficients and adaptive neighborhood-based velocity adjustment for improved convergence.\n\nWith code:\nimport numpy as np\n\nclass RefinedParticleOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(30, self.budget // 2)  # Increased population size for better diversity\n        self.inertia_weight = 0.6  # Decreased inertia weight for faster convergence\n        self.cognitive_coeff = 1.5  \n        self.social_coeff = 1.5  # Adjusted coefficients for more balanced search\n        self.adaptive_scaling = 0.9  # More aggressive adaptive scaling\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-0.3, 0.3, (self.population_size, self.dim))  # Reduced velocity range\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        velocity_clamp = (self.upper_bound - self.lower_bound) * 0.15  # Adjusted dynamic velocity clamp\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            \n            velocities = np.clip(velocities, -velocity_clamp, velocity_clamp)\n            positions += velocities\n            \n            if eval_count % (self.budget // 4) == 0:  # More frequent adaptive adjustment\n                self.inertia_weight *= self.adaptive_scaling\n                perturbation = np.random.normal(0, 0.05, positions.shape)  # Lower perturbation for fine-tuning\n                positions += perturbation\n            \n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n                \n        return global_best_score, global_best_position\n\nThe algorithm RefinedParticleOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.35 with standard deviation 0.30.\n\n\nPlease transform this optimization algorithm code by exactly 20.0% in a \nway that enhances convergence speed. Keep the code difference precisely at \n20.0%, and focus solely on achieving performance improvements through \nalgorithmic adjustments across various elements of the code. Avoid focusing on \ncode efficiency; instead, prioritize exploration of diverse approaches within \nthe allowed modification percentage.\n\n\nProvide the Python code and a one-line description with the main idea (without enters). Give the response in the format:\n# Description: <short-description>\n# Code: \n# ```python\n# <code>\n# ```\n# \n"}
{"role": "gpt-4o", "time": "2024-11-09 16:27:54.322317", "content": "# Description: A diversified particle swarm optimizer with dynamic neighborhood topology and adaptive mutation for improved convergence speed.\n# Code:\n# ```python\nimport numpy as np\n\nclass DiversifiedParticleOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(40, self.budget // 3)  # Increased population size for more diversity\n        self.inertia_weight = 0.7  # Slightly increased inertia for stability\n        self.cognitive_coeff = 1.3  \n        self.social_coeff = 1.7  # Adjusted coefficients for exploratory search\n        self.adaptive_scaling = 0.8  # Different scaling for adaptation\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-0.4, 0.4, (self.population_size, self.dim))  # Adjusted velocity range\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        velocity_clamp = (self.upper_bound - self.lower_bound) * 0.2  # More lenient dynamic velocity clamp\n\n        neighborhood_size = 5  # Dynamic neighborhood size for local interactions\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            for i in range(self.population_size):\n                neighbors = np.random.choice(self.population_size, neighborhood_size, replace=False)\n                local_best_index = neighbors[np.argmin(personal_best_scores[neighbors])]\n                local_best_position = personal_best_positions[local_best_index]\n                \n                cognitive_component = self.cognitive_coeff * r1[i] * (personal_best_positions[i] - positions[i])\n                social_component = self.social_coeff * r2[i] * (local_best_position - positions[i])\n                velocities[i] = self.inertia_weight * velocities[i] + cognitive_component + social_component\n            \n            velocities = np.clip(velocities, -velocity_clamp, velocity_clamp)\n            positions += velocities\n            \n            if eval_count % (self.budget // 3) == 0:  # Adaptive adjustment using mutation\n                self.inertia_weight *= self.adaptive_scaling\n                mutation = np.random.normal(0, 0.1, positions.shape)  # Adaptive mutation for diversity\n                positions += mutation\n            \n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n                \n        return global_best_score, global_best_position\n# ```"}
{"role": "LLaMEA", "time": "2024-11-09 16:27:54.325966", "content": "You are a highly skilled computer scientist in the field of natural computing. Your task is to design novel metaheuristic algorithms to solve black box optimization problems.\n\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the BBOB test suite of 24 noiseless functions. Your task is to write the optimization algorithm in Python code. The code should contain an `__init__(self, budget, dim)` function and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. Each of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nGive an excellent and novel heuristic algorithm to solve this task and also give it a one-line description with the main idea.\n\nThe current population of algorithms already evaluated (name, description, score) is:\nRefinedParticleOptimizer: A refined particle swarm optimizer with dynamic acceleration coefficients and adaptive neighborhood-based velocity adjustment for improved convergence. (Score: 0.3490816234226132)\n\nThe selected solution to update is:\nA refined particle swarm optimizer with dynamic acceleration coefficients and adaptive neighborhood-based velocity adjustment for improved convergence.\n\nWith code:\nimport numpy as np\n\nclass RefinedParticleOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(30, self.budget // 2)  # Increased population size for better diversity\n        self.inertia_weight = 0.6  # Decreased inertia weight for faster convergence\n        self.cognitive_coeff = 1.5  \n        self.social_coeff = 1.5  # Adjusted coefficients for more balanced search\n        self.adaptive_scaling = 0.9  # More aggressive adaptive scaling\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-0.3, 0.3, (self.population_size, self.dim))  # Reduced velocity range\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        velocity_clamp = (self.upper_bound - self.lower_bound) * 0.15  # Adjusted dynamic velocity clamp\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            \n            velocities = np.clip(velocities, -velocity_clamp, velocity_clamp)\n            positions += velocities\n            \n            if eval_count % (self.budget // 4) == 0:  # More frequent adaptive adjustment\n                self.inertia_weight *= self.adaptive_scaling\n                perturbation = np.random.normal(0, 0.05, positions.shape)  # Lower perturbation for fine-tuning\n                positions += perturbation\n            \n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n                \n        return global_best_score, global_best_position\n\nThe algorithm RefinedParticleOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.35 with standard deviation 0.30.\n\n\nPlease transform this optimization algorithm code by exactly 20.0% in a \nway that enhances convergence speed. Keep the code difference precisely at \n20.0%, and focus solely on achieving performance improvements through \nalgorithmic adjustments across various elements of the code. Avoid focusing on \ncode efficiency; instead, prioritize exploration of diverse approaches within \nthe allowed modification percentage.\n\n\nProvide the Python code and a one-line description with the main idea (without enters). Give the response in the format:\n# Description: <short-description>\n# Code: \n# ```python\n# <code>\n# ```\n# \n"}
{"role": "gpt-4o", "time": "2024-11-09 16:28:05.960599", "content": "# Description: A particle swarm optimizer with enhanced adaptive inertia reduction, nonlinear social-cognitive balance, and variance-guided velocity adjustment for faster convergence.\n# Code:\n# ```python\nimport numpy as np\n\nclass EnhancedAdaptivePSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(35, self.budget // 3)  # Slightly increased population for diversity\n        self.inertia_weight = 0.7  # Higher initial inertia\n        self.cognitive_coeff = 1.3  \n        self.social_coeff = 1.7  # Nonlinear increase in social influence\n        self.adaptive_scaling = 0.85  # Enhanced adaptive scaling for inertia\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-0.5, 0.5, (self.population_size, self.dim))  # Broader initial velocity range\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        velocity_clamp = (self.upper_bound - self.lower_bound) * 0.1  # Tighter velocity clamp for precision\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * (0.5 + r2) * (global_best_position - positions)  # Adjusted social component\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            \n            velocities = np.clip(velocities, -velocity_clamp, velocity_clamp)\n            positions += velocities\n            \n            if eval_count % (self.budget // 5) == 0:  # More frequent adaptive adjustment\n                self.inertia_weight *= self.adaptive_scaling\n                perturbation = np.random.normal(0, 0.02, positions.shape)  # Finer perturbation\n                positions += perturbation\n            \n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n                \n        return global_best_score, global_best_position\n# ```"}
{"role": "LLaMEA", "time": "2024-11-09 16:28:05.963838", "content": "You are a highly skilled computer scientist in the field of natural computing. Your task is to design novel metaheuristic algorithms to solve black box optimization problems.\n\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the BBOB test suite of 24 noiseless functions. Your task is to write the optimization algorithm in Python code. The code should contain an `__init__(self, budget, dim)` function and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. Each of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nGive an excellent and novel heuristic algorithm to solve this task and also give it a one-line description with the main idea.\n\nThe current population of algorithms already evaluated (name, description, score) is:\nRefinedParticleOptimizer: A refined particle swarm optimizer with dynamic acceleration coefficients and adaptive neighborhood-based velocity adjustment for improved convergence. (Score: 0.3490816234226132)\n\nThe selected solution to update is:\nA refined particle swarm optimizer with dynamic acceleration coefficients and adaptive neighborhood-based velocity adjustment for improved convergence.\n\nWith code:\nimport numpy as np\n\nclass RefinedParticleOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(30, self.budget // 2)  # Increased population size for better diversity\n        self.inertia_weight = 0.6  # Decreased inertia weight for faster convergence\n        self.cognitive_coeff = 1.5  \n        self.social_coeff = 1.5  # Adjusted coefficients for more balanced search\n        self.adaptive_scaling = 0.9  # More aggressive adaptive scaling\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-0.3, 0.3, (self.population_size, self.dim))  # Reduced velocity range\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        velocity_clamp = (self.upper_bound - self.lower_bound) * 0.15  # Adjusted dynamic velocity clamp\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            \n            velocities = np.clip(velocities, -velocity_clamp, velocity_clamp)\n            positions += velocities\n            \n            if eval_count % (self.budget // 4) == 0:  # More frequent adaptive adjustment\n                self.inertia_weight *= self.adaptive_scaling\n                perturbation = np.random.normal(0, 0.05, positions.shape)  # Lower perturbation for fine-tuning\n                positions += perturbation\n            \n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n                \n        return global_best_score, global_best_position\n\nThe algorithm RefinedParticleOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.35 with standard deviation 0.30.\n\n\nPlease transform this optimization algorithm code by exactly 20.0% in a \nway that enhances convergence speed. Keep the code difference precisely at \n20.0%, and focus solely on achieving performance improvements through \nalgorithmic adjustments across various elements of the code. Avoid focusing on \ncode efficiency; instead, prioritize exploration of diverse approaches within \nthe allowed modification percentage.\n\n\nProvide the Python code and a one-line description with the main idea (without enters). Give the response in the format:\n# Description: <short-description>\n# Code: \n# ```python\n# <code>\n# ```\n# \n"}
{"role": "gpt-4o", "time": "2024-11-09 16:28:16.817763", "content": "# Description: A hybrid particle swarm optimizer integrating differential evolution for enhanced exploration and convergence speed.\n# Code:\n# ```python\nimport numpy as np\n\nclass HybridParticleDEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(40, self.budget // 2)  # Increased population size for diversity\n        self.inertia_weight = 0.5  # Further decreased inertia to enhance convergence\n        self.cognitive_coeff = 1.8  # Increased cognitive component for local exploration\n        self.social_coeff = 1.6  # Slightly increased social component for global exploration\n        self.mutation_factor = 0.8  # Mutation factor for DE\n        self.crossover_prob = 0.9  # Crossover probability for DE\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-0.2, 0.2, (self.population_size, self.dim))  # Further reduced velocity range\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        velocity_clamp = (self.upper_bound - self.lower_bound) * 0.1  # Reduced dynamic velocity clamp\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            \n            velocities = np.clip(velocities, -velocity_clamp, velocity_clamp)\n            positions += velocities\n            \n            for i in range(self.population_size):  # Apply differential evolution\n                idxs = np.random.choice(self.population_size, 3, replace=False)\n                a, b, c = positions[idxs]\n                mutant = a + self.mutation_factor * (b - c)\n                trial = np.copy(positions[i])\n                for j in range(self.dim):\n                    if np.random.rand() < self.crossover_prob:\n                        trial[j] = mutant[j]\n                trial = np.clip(trial, self.lower_bound, self.upper_bound)\n                trial_score = func(trial)\n                eval_count += 1\n                if trial_score < scores[i]:\n                    positions[i] = trial\n                    scores[i] = trial_score\n                    if trial_score < personal_best_scores[i]:\n                        personal_best_scores[i] = trial_score\n                        personal_best_positions[i] = trial\n                    if trial_score < global_best_score:\n                        global_best_score = trial_score\n                        global_best_position = trial\n\n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n\n        return global_best_score, global_best_position\n# ```"}
{"role": "LLaMEA", "time": "2024-11-09 16:28:16.821239", "content": "You are a highly skilled computer scientist in the field of natural computing. Your task is to design novel metaheuristic algorithms to solve black box optimization problems.\n\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the BBOB test suite of 24 noiseless functions. Your task is to write the optimization algorithm in Python code. The code should contain an `__init__(self, budget, dim)` function and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. Each of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nGive an excellent and novel heuristic algorithm to solve this task and also give it a one-line description with the main idea.\n\nThe current population of algorithms already evaluated (name, description, score) is:\nRefinedParticleOptimizer: A refined particle swarm optimizer with dynamic acceleration coefficients and adaptive neighborhood-based velocity adjustment for improved convergence. (Score: 0.3490816234226132)\n\nThe selected solution to update is:\nA refined particle swarm optimizer with dynamic acceleration coefficients and adaptive neighborhood-based velocity adjustment for improved convergence.\n\nWith code:\nimport numpy as np\n\nclass RefinedParticleOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(30, self.budget // 2)  # Increased population size for better diversity\n        self.inertia_weight = 0.6  # Decreased inertia weight for faster convergence\n        self.cognitive_coeff = 1.5  \n        self.social_coeff = 1.5  # Adjusted coefficients for more balanced search\n        self.adaptive_scaling = 0.9  # More aggressive adaptive scaling\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-0.3, 0.3, (self.population_size, self.dim))  # Reduced velocity range\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        velocity_clamp = (self.upper_bound - self.lower_bound) * 0.15  # Adjusted dynamic velocity clamp\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            \n            velocities = np.clip(velocities, -velocity_clamp, velocity_clamp)\n            positions += velocities\n            \n            if eval_count % (self.budget // 4) == 0:  # More frequent adaptive adjustment\n                self.inertia_weight *= self.adaptive_scaling\n                perturbation = np.random.normal(0, 0.05, positions.shape)  # Lower perturbation for fine-tuning\n                positions += perturbation\n            \n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n                \n        return global_best_score, global_best_position\n\nThe algorithm RefinedParticleOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.35 with standard deviation 0.30.\n\n\nPlease transform this optimization algorithm code by exactly 20.0% in a \nway that enhances convergence speed. Keep the code difference precisely at \n20.0%, and focus solely on achieving performance improvements through \nalgorithmic adjustments across various elements of the code. Avoid focusing on \ncode efficiency; instead, prioritize exploration of diverse approaches within \nthe allowed modification percentage.\n\n\nProvide the Python code and a one-line description with the main idea (without enters). Give the response in the format:\n# Description: <short-description>\n# Code: \n# ```python\n# <code>\n# ```\n# \n"}
{"role": "gpt-4o", "time": "2024-11-09 16:28:27.169237", "content": "# Description: An Enhanced Adaptive Particle Optimizer with Turbulence and Exponential Decay for Faster Convergence.\n# Code:\n# ```python\nimport numpy as np\n\nclass EnhancedAdaptiveParticleOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(40, self.budget // 3)\n        self.inertia_weight = 0.7\n        self.cognitive_coeff = 1.4\n        self.social_coeff = 1.6\n        self.adaptive_scaling = 0.85\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-0.4, 0.4, (self.population_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        velocity_clamp = (self.upper_bound - self.lower_bound) * 0.2\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            \n            turbulence = np.random.normal(0, 0.1, positions.shape) * (1 - eval_count / self.budget)\n            velocities += turbulence\n\n            velocities = np.clip(velocities, -velocity_clamp, velocity_clamp)\n            positions += velocities\n            \n            if eval_count % (self.budget // 3) == 0:\n                self.inertia_weight *= self.adaptive_scaling\n                exponential_decay = np.exp(-0.1 * eval_count / self.budget)\n                positions += np.random.normal(0, 0.05 * exponential_decay, positions.shape)\n            \n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n                \n        return global_best_score, global_best_position\n# ```"}
{"role": "LLaMEA", "time": "2024-11-09 16:28:27.172753", "content": "You are a highly skilled computer scientist in the field of natural computing. Your task is to design novel metaheuristic algorithms to solve black box optimization problems.\n\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the BBOB test suite of 24 noiseless functions. Your task is to write the optimization algorithm in Python code. The code should contain an `__init__(self, budget, dim)` function and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. Each of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nGive an excellent and novel heuristic algorithm to solve this task and also give it a one-line description with the main idea.\n\nThe current population of algorithms already evaluated (name, description, score) is:\nRefinedParticleOptimizer: A refined particle swarm optimizer with dynamic acceleration coefficients and adaptive neighborhood-based velocity adjustment for improved convergence. (Score: 0.3490816234226132)\n\nThe selected solution to update is:\nA refined particle swarm optimizer with dynamic acceleration coefficients and adaptive neighborhood-based velocity adjustment for improved convergence.\n\nWith code:\nimport numpy as np\n\nclass RefinedParticleOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(30, self.budget // 2)  # Increased population size for better diversity\n        self.inertia_weight = 0.6  # Decreased inertia weight for faster convergence\n        self.cognitive_coeff = 1.5  \n        self.social_coeff = 1.5  # Adjusted coefficients for more balanced search\n        self.adaptive_scaling = 0.9  # More aggressive adaptive scaling\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-0.3, 0.3, (self.population_size, self.dim))  # Reduced velocity range\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        velocity_clamp = (self.upper_bound - self.lower_bound) * 0.15  # Adjusted dynamic velocity clamp\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            \n            velocities = np.clip(velocities, -velocity_clamp, velocity_clamp)\n            positions += velocities\n            \n            if eval_count % (self.budget // 4) == 0:  # More frequent adaptive adjustment\n                self.inertia_weight *= self.adaptive_scaling\n                perturbation = np.random.normal(0, 0.05, positions.shape)  # Lower perturbation for fine-tuning\n                positions += perturbation\n            \n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n                \n        return global_best_score, global_best_position\n\nThe algorithm RefinedParticleOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.35 with standard deviation 0.30.\n\n\nPlease transform this optimization algorithm code by exactly 20.0% in a \nway that enhances convergence speed. Keep the code difference precisely at \n20.0%, and focus solely on achieving performance improvements through \nalgorithmic adjustments across various elements of the code. Avoid focusing on \ncode efficiency; instead, prioritize exploration of diverse approaches within \nthe allowed modification percentage.\n\n\nProvide the Python code and a one-line description with the main idea (without enters). Give the response in the format:\n# Description: <short-description>\n# Code: \n# ```python\n# <code>\n# ```\n# \n"}
{"role": "gpt-4o", "time": "2024-11-09 16:28:36.377211", "content": "# Description: Enhanced Particle Swarm Optimizer with adaptive learning rates and gradient-based fine-tuning for improved convergence speed.\n# Code:\n# ```python\nimport numpy as np\n\nclass EnhancedParticleOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(30, self.budget // 2)\n        self.inertia_weight = 0.5  # Slightly reduced inertia for faster convergence\n        self.cognitive_coeff = 1.6  # Adjusted cognitive learning rate\n        self.social_coeff = 1.8  # Increased social learning rate\n        self.adaptive_scaling = 0.95  # Enhanced scaling for better fine-tuning\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-0.2, 0.2, (self.population_size, self.dim))  # Reduced velocity range\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        velocity_clamp = (self.upper_bound - self.lower_bound) * 0.1  # Tighter velocity clamp\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            \n            velocities = np.clip(velocities, -velocity_clamp, velocity_clamp)\n            positions += velocities\n            \n            if eval_count % (self.budget // 3) == 0:  # More adaptive adjustment intervals\n                self.inertia_weight *= self.adaptive_scaling\n                perturbation = np.random.normal(0, 0.03, positions.shape)  # Lower perturbation for precision\n                positions += perturbation\n            \n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n                \n        return global_best_score, global_best_position\n# ```"}
{"role": "LLaMEA", "time": "2024-11-09 16:28:36.380655", "content": "You are a highly skilled computer scientist in the field of natural computing. Your task is to design novel metaheuristic algorithms to solve black box optimization problems.\n\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the BBOB test suite of 24 noiseless functions. Your task is to write the optimization algorithm in Python code. The code should contain an `__init__(self, budget, dim)` function and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. Each of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nGive an excellent and novel heuristic algorithm to solve this task and also give it a one-line description with the main idea.\n\nThe current population of algorithms already evaluated (name, description, score) is:\nRefinedParticleOptimizer: A refined particle swarm optimizer with dynamic acceleration coefficients and adaptive neighborhood-based velocity adjustment for improved convergence. (Score: 0.3490816234226132)\n\nThe selected solution to update is:\nA refined particle swarm optimizer with dynamic acceleration coefficients and adaptive neighborhood-based velocity adjustment for improved convergence.\n\nWith code:\nimport numpy as np\n\nclass RefinedParticleOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(30, self.budget // 2)  # Increased population size for better diversity\n        self.inertia_weight = 0.6  # Decreased inertia weight for faster convergence\n        self.cognitive_coeff = 1.5  \n        self.social_coeff = 1.5  # Adjusted coefficients for more balanced search\n        self.adaptive_scaling = 0.9  # More aggressive adaptive scaling\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-0.3, 0.3, (self.population_size, self.dim))  # Reduced velocity range\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        velocity_clamp = (self.upper_bound - self.lower_bound) * 0.15  # Adjusted dynamic velocity clamp\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            \n            velocities = np.clip(velocities, -velocity_clamp, velocity_clamp)\n            positions += velocities\n            \n            if eval_count % (self.budget // 4) == 0:  # More frequent adaptive adjustment\n                self.inertia_weight *= self.adaptive_scaling\n                perturbation = np.random.normal(0, 0.05, positions.shape)  # Lower perturbation for fine-tuning\n                positions += perturbation\n            \n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n                \n        return global_best_score, global_best_position\n\nThe algorithm RefinedParticleOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.35 with standard deviation 0.30.\n\n\nPlease transform this optimization algorithm code by exactly 20.0% in a \nway that enhances convergence speed. Keep the code difference precisely at \n20.0%, and focus solely on achieving performance improvements through \nalgorithmic adjustments across various elements of the code. Avoid focusing on \ncode efficiency; instead, prioritize exploration of diverse approaches within \nthe allowed modification percentage.\n\n\nProvide the Python code and a one-line description with the main idea (without enters). Give the response in the format:\n# Description: <short-description>\n# Code: \n# ```python\n# <code>\n# ```\n# \n"}
{"role": "gpt-4o", "time": "2024-11-09 16:28:44.762552", "content": "# Description: A refined particle swarm optimizer with adaptive inertia weight and enhanced diversity through dynamic subpopulation division.\n# Code:\n# ```python\nimport numpy as np\n\nclass EnhancedParticleOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(35, self.budget // 2)  # Increased population size for diversity\n        self.inertia_weight = 0.9  # Adaptive inertia weight for exploration-exploitation balance\n        self.cognitive_coeff = 1.4\n        self.social_coeff = 1.6  # Adjusted for improved convergence\n        self.adaptive_scaling = 0.95  # Slightly aggressive scaling\n        self.subpopulation_division = 5  # Introduced subpopulation concept\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-0.3, 0.3, (self.population_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        velocity_clamp = (self.upper_bound - self.lower_bound) * 0.1  # Further clamped velocities\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = (self.inertia_weight * velocities + cognitive_component + social_component) / self.subpopulation_division\n            \n            velocities = np.clip(velocities, -velocity_clamp, velocity_clamp)\n            positions += velocities\n            \n            if eval_count % (self.budget // 3) == 0:  # More frequent adaptive adjustment\n                self.inertia_weight *= self.adaptive_scaling\n                perturbation = np.random.normal(0, 0.1, positions.shape)  # Increased perturbation for exploration\n                positions += perturbation\n            \n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n                \n        return global_best_score, global_best_position\n# ```"}
{"role": "LLaMEA", "time": "2024-11-09 16:28:44.765991", "content": "You are a highly skilled computer scientist in the field of natural computing. Your task is to design novel metaheuristic algorithms to solve black box optimization problems.\n\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the BBOB test suite of 24 noiseless functions. Your task is to write the optimization algorithm in Python code. The code should contain an `__init__(self, budget, dim)` function and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. Each of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nGive an excellent and novel heuristic algorithm to solve this task and also give it a one-line description with the main idea.\n\nThe current population of algorithms already evaluated (name, description, score) is:\nRefinedParticleOptimizer: A refined particle swarm optimizer with dynamic acceleration coefficients and adaptive neighborhood-based velocity adjustment for improved convergence. (Score: 0.3490816234226132)\n\nThe selected solution to update is:\nA refined particle swarm optimizer with dynamic acceleration coefficients and adaptive neighborhood-based velocity adjustment for improved convergence.\n\nWith code:\nimport numpy as np\n\nclass RefinedParticleOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(30, self.budget // 2)  # Increased population size for better diversity\n        self.inertia_weight = 0.6  # Decreased inertia weight for faster convergence\n        self.cognitive_coeff = 1.5  \n        self.social_coeff = 1.5  # Adjusted coefficients for more balanced search\n        self.adaptive_scaling = 0.9  # More aggressive adaptive scaling\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-0.3, 0.3, (self.population_size, self.dim))  # Reduced velocity range\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        velocity_clamp = (self.upper_bound - self.lower_bound) * 0.15  # Adjusted dynamic velocity clamp\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            \n            velocities = np.clip(velocities, -velocity_clamp, velocity_clamp)\n            positions += velocities\n            \n            if eval_count % (self.budget // 4) == 0:  # More frequent adaptive adjustment\n                self.inertia_weight *= self.adaptive_scaling\n                perturbation = np.random.normal(0, 0.05, positions.shape)  # Lower perturbation for fine-tuning\n                positions += perturbation\n            \n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n                \n        return global_best_score, global_best_position\n\nThe algorithm RefinedParticleOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.35 with standard deviation 0.30.\n\n\nPlease transform this optimization algorithm code by exactly 20.0% in a \nway that enhances convergence speed. Keep the code difference precisely at \n20.0%, and focus solely on achieving performance improvements through \nalgorithmic adjustments across various elements of the code. Avoid focusing on \ncode efficiency; instead, prioritize exploration of diverse approaches within \nthe allowed modification percentage.\n\n\nProvide the Python code and a one-line description with the main idea (without enters). Give the response in the format:\n# Description: <short-description>\n# Code: \n# ```python\n# <code>\n# ```\n# \n"}
{"role": "gpt-4o", "time": "2024-11-09 16:28:53.360391", "content": "# Description: An enhanced particle swarm optimizer with adaptive inertia and dynamic neighborhood convergence adjustments for optimized search efficiency.\n# Code:\n# ```python\nimport numpy as np\n\nclass EnhancedParticleOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(35, self.budget // 2)  # Adjusted population size for improved exploration\n        self.inertia_weight = 0.7  # Modified inertia weight for better balance between exploration and exploitation\n        self.cognitive_coeff = 1.4  \n        self.social_coeff = 1.6  # Slightly increased social coefficient to enhance convergence\n        self.adaptive_scaling = 0.85  # More moderate adaptive scaling\n        self.local_search_weight = 0.1  # Added local search component\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-0.2, 0.2, (self.population_size, self.dim))  # Further reduced velocity range\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        velocity_clamp = (self.upper_bound - self.lower_bound) * 0.1  # Further adjusted velocity clamp\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n\n            # Local search enhancement\n            local_search = self.local_search_weight * np.random.uniform(-1, 1, positions.shape)\n            \n            velocities = np.clip(velocities + local_search, -velocity_clamp, velocity_clamp)\n            positions += velocities\n            \n            if eval_count % (self.budget // 5) == 0:  # More frequent adaptive adjustment\n                self.inertia_weight *= self.adaptive_scaling\n                perturbation = np.random.normal(0, 0.03, positions.shape)  # Adjusted perturbation magnitude\n                positions += perturbation\n            \n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n                \n        return global_best_score, global_best_position\n# ```"}
{"role": "LLaMEA", "time": "2024-11-09 16:28:53.363765", "content": "You are a highly skilled computer scientist in the field of natural computing. Your task is to design novel metaheuristic algorithms to solve black box optimization problems.\n\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the BBOB test suite of 24 noiseless functions. Your task is to write the optimization algorithm in Python code. The code should contain an `__init__(self, budget, dim)` function and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. Each of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nGive an excellent and novel heuristic algorithm to solve this task and also give it a one-line description with the main idea.\n\nThe current population of algorithms already evaluated (name, description, score) is:\nRefinedParticleOptimizer: A refined particle swarm optimizer with dynamic acceleration coefficients and adaptive neighborhood-based velocity adjustment for improved convergence. (Score: 0.3490816234226132)\n\nThe selected solution to update is:\nA refined particle swarm optimizer with dynamic acceleration coefficients and adaptive neighborhood-based velocity adjustment for improved convergence.\n\nWith code:\nimport numpy as np\n\nclass RefinedParticleOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(30, self.budget // 2)  # Increased population size for better diversity\n        self.inertia_weight = 0.6  # Decreased inertia weight for faster convergence\n        self.cognitive_coeff = 1.5  \n        self.social_coeff = 1.5  # Adjusted coefficients for more balanced search\n        self.adaptive_scaling = 0.9  # More aggressive adaptive scaling\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-0.3, 0.3, (self.population_size, self.dim))  # Reduced velocity range\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        velocity_clamp = (self.upper_bound - self.lower_bound) * 0.15  # Adjusted dynamic velocity clamp\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            \n            velocities = np.clip(velocities, -velocity_clamp, velocity_clamp)\n            positions += velocities\n            \n            if eval_count % (self.budget // 4) == 0:  # More frequent adaptive adjustment\n                self.inertia_weight *= self.adaptive_scaling\n                perturbation = np.random.normal(0, 0.05, positions.shape)  # Lower perturbation for fine-tuning\n                positions += perturbation\n            \n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n                \n        return global_best_score, global_best_position\n\nThe algorithm RefinedParticleOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.35 with standard deviation 0.30.\n\n\nPlease transform this optimization algorithm code by exactly 20.0% in a \nway that enhances convergence speed. Keep the code difference precisely at \n20.0%, and focus solely on achieving performance improvements through \nalgorithmic adjustments across various elements of the code. Avoid focusing on \ncode efficiency; instead, prioritize exploration of diverse approaches within \nthe allowed modification percentage.\n\n\nProvide the Python code and a one-line description with the main idea (without enters). Give the response in the format:\n# Description: <short-description>\n# Code: \n# ```python\n# <code>\n# ```\n# \n"}
{"role": "gpt-4o", "time": "2024-11-09 16:29:02.794858", "content": "# Description: A refined particle swarm optimizer with enhanced convergence through adaptive inertia adjustment and elite particle perturbation.\n# Code:\n# ```python\nimport numpy as np\n\nclass EnhancedParticleOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(40, self.budget // 2)  # Increased population size for better diversity\n        self.inertia_weight = 0.7  # Increased inertia weight initially for exploration\n        self.cognitive_coeff = 1.4  # Slightly reduced for steadier individual searches\n        self.social_coeff = 1.6  # Slightly increased for stronger convergence\n        self.adaptive_scaling = 0.85  # More aggressive decay\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-0.5, 0.5, (self.population_size, self.dim))  # Expanded velocity range\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        velocity_clamp = (self.upper_bound - self.lower_bound) * 0.2  # Adjusted dynamic velocity clamp\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            \n            velocities = np.clip(velocities, -velocity_clamp, velocity_clamp)\n            positions += velocities\n            \n            if eval_count % (self.budget // 3) == 0:  # More frequent adaptive adjustment\n                self.inertia_weight *= self.adaptive_scaling\n                elite_index = np.argmin(scores)\n                elite_perturbation = np.random.normal(0, 0.01, positions[elite_index].shape)  # Fine-tuning elite particle\n                positions[elite_index] += elite_perturbation\n            \n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n                \n        return global_best_score, global_best_position\n# ```"}
{"role": "LLaMEA", "time": "2024-11-09 16:29:02.798271", "content": "You are a highly skilled computer scientist in the field of natural computing. Your task is to design novel metaheuristic algorithms to solve black box optimization problems.\n\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the BBOB test suite of 24 noiseless functions. Your task is to write the optimization algorithm in Python code. The code should contain an `__init__(self, budget, dim)` function and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. Each of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nGive an excellent and novel heuristic algorithm to solve this task and also give it a one-line description with the main idea.\n\nThe current population of algorithms already evaluated (name, description, score) is:\nRefinedParticleOptimizer: A refined particle swarm optimizer with dynamic acceleration coefficients and adaptive neighborhood-based velocity adjustment for improved convergence. (Score: 0.3490816234226132)\n\nThe selected solution to update is:\nA refined particle swarm optimizer with dynamic acceleration coefficients and adaptive neighborhood-based velocity adjustment for improved convergence.\n\nWith code:\nimport numpy as np\n\nclass RefinedParticleOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(30, self.budget // 2)  # Increased population size for better diversity\n        self.inertia_weight = 0.6  # Decreased inertia weight for faster convergence\n        self.cognitive_coeff = 1.5  \n        self.social_coeff = 1.5  # Adjusted coefficients for more balanced search\n        self.adaptive_scaling = 0.9  # More aggressive adaptive scaling\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-0.3, 0.3, (self.population_size, self.dim))  # Reduced velocity range\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        velocity_clamp = (self.upper_bound - self.lower_bound) * 0.15  # Adjusted dynamic velocity clamp\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            \n            velocities = np.clip(velocities, -velocity_clamp, velocity_clamp)\n            positions += velocities\n            \n            if eval_count % (self.budget // 4) == 0:  # More frequent adaptive adjustment\n                self.inertia_weight *= self.adaptive_scaling\n                perturbation = np.random.normal(0, 0.05, positions.shape)  # Lower perturbation for fine-tuning\n                positions += perturbation\n            \n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n                \n        return global_best_score, global_best_position\n\nThe algorithm RefinedParticleOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.35 with standard deviation 0.30.\n\n\nPlease transform this optimization algorithm code by exactly 20.0% in a \nway that enhances convergence speed. Keep the code difference precisely at \n20.0%, and focus solely on achieving performance improvements through \nalgorithmic adjustments across various elements of the code. Avoid focusing on \ncode efficiency; instead, prioritize exploration of diverse approaches within \nthe allowed modification percentage.\n\n\nProvide the Python code and a one-line description with the main idea (without enters). Give the response in the format:\n# Description: <short-description>\n# Code: \n# ```python\n# <code>\n# ```\n# \n"}
{"role": "gpt-4o", "time": "2024-11-09 16:29:12.318946", "content": "# Description: Enhanced Particle Optimizer with adaptive learning rates and dynamic local search for accelerated convergence.\n# Code:\n# ```python\nimport numpy as np\n\nclass EnhancedParticleOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(40, self.budget // 3)  # Increased population size slightly for better sampling\n        self.inertia_weight = 0.7  # Revised inertia weight for balance\n        self.cognitive_coeff = 2.0  # Increased to enhance individual exploration\n        self.social_coeff = 1.8  # Adjusted for stronger social effect\n        self.adaptive_scaling = 0.92  # Slightly reduced scaling factor for gradual adaptiveness\n        self.local_search_prob = 0.1  # Introduced dynamic local search probability\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-0.2, 0.2, (self.population_size, self.dim))  # Tighter initial velocity range\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        velocity_clamp = (self.upper_bound - self.lower_bound) * 0.1  # Narrower dynamic velocity clamp\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            \n            velocities = np.clip(velocities, -velocity_clamp, velocity_clamp)\n            positions += velocities\n            \n            if np.random.rand() < self.local_search_prob:  # Probabilistic local search\n                perturbation = np.random.normal(0, 0.1, positions.shape)  # Slightly stronger perturbation\n                positions += perturbation\n            \n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n                \n        return global_best_score, global_best_position\n# ```"}
{"role": "LLaMEA", "time": "2024-11-09 16:29:12.322631", "content": "You are a highly skilled computer scientist in the field of natural computing. Your task is to design novel metaheuristic algorithms to solve black box optimization problems.\n\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the BBOB test suite of 24 noiseless functions. Your task is to write the optimization algorithm in Python code. The code should contain an `__init__(self, budget, dim)` function and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. Each of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nGive an excellent and novel heuristic algorithm to solve this task and also give it a one-line description with the main idea.\n\nThe current population of algorithms already evaluated (name, description, score) is:\nRefinedParticleOptimizer: A refined particle swarm optimizer with dynamic acceleration coefficients and adaptive neighborhood-based velocity adjustment for improved convergence. (Score: 0.3490816234226132)\n\nThe selected solution to update is:\nA refined particle swarm optimizer with dynamic acceleration coefficients and adaptive neighborhood-based velocity adjustment for improved convergence.\n\nWith code:\nimport numpy as np\n\nclass RefinedParticleOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(30, self.budget // 2)  # Increased population size for better diversity\n        self.inertia_weight = 0.6  # Decreased inertia weight for faster convergence\n        self.cognitive_coeff = 1.5  \n        self.social_coeff = 1.5  # Adjusted coefficients for more balanced search\n        self.adaptive_scaling = 0.9  # More aggressive adaptive scaling\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-0.3, 0.3, (self.population_size, self.dim))  # Reduced velocity range\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        velocity_clamp = (self.upper_bound - self.lower_bound) * 0.15  # Adjusted dynamic velocity clamp\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            \n            velocities = np.clip(velocities, -velocity_clamp, velocity_clamp)\n            positions += velocities\n            \n            if eval_count % (self.budget // 4) == 0:  # More frequent adaptive adjustment\n                self.inertia_weight *= self.adaptive_scaling\n                perturbation = np.random.normal(0, 0.05, positions.shape)  # Lower perturbation for fine-tuning\n                positions += perturbation\n            \n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n                \n        return global_best_score, global_best_position\n\nThe algorithm RefinedParticleOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.35 with standard deviation 0.30.\n\n\nPlease transform this optimization algorithm code by exactly 20.0% in a \nway that enhances convergence speed. Keep the code difference precisely at \n20.0%, and focus solely on achieving performance improvements through \nalgorithmic adjustments across various elements of the code. Avoid focusing on \ncode efficiency; instead, prioritize exploration of diverse approaches within \nthe allowed modification percentage.\n\n\nProvide the Python code and a one-line description with the main idea (without enters). Give the response in the format:\n# Description: <short-description>\n# Code: \n# ```python\n# <code>\n# ```\n# \n"}
{"role": "gpt-4o", "time": "2024-11-09 16:29:21.946748", "content": "# Description: Enhanced Particle Swarm Optimizer with adaptive inertia, diversity boosting, and elite learning strategy.\n# Code:\n# ```python\nimport numpy as np\n\nclass EnhancedParticleOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = min(50, self.budget // 2)  # Increased population size for diversity\n        self.inertia_weight = 0.7  # Initial increased inertia weight for exploration\n        self.cognitive_coeff = 1.4  \n        self.social_coeff = 1.6  # Slightly increased social component\n        self.inertia_damping = 0.975  # Adaptive inertia damping factor\n\n    def __call__(self, func):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        velocities = np.random.uniform(-0.4, 0.4, (self.population_size, self.dim))  # Increased initial velocity range\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.population_size, np.inf)\n        \n        scores = np.array([func(pos) for pos in positions])\n        eval_count = self.population_size\n        \n        global_best_index = np.argmin(scores)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_score = scores[global_best_index]\n        \n        personal_better_mask = scores < personal_best_scores\n        personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n        personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n\n        velocity_clamp = (self.upper_bound - self.lower_bound) * 0.2  # Adjusted velocity clamp\n\n        while eval_count < self.budget:\n            r1 = np.random.rand(self.population_size, self.dim)\n            r2 = np.random.rand(self.population_size, self.dim)\n            cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions - positions)\n            social_component = self.social_coeff * r2 * (global_best_position - positions)\n            velocities = self.inertia_weight * velocities + cognitive_component + social_component\n            \n            velocities = np.clip(velocities, -velocity_clamp, velocity_clamp)\n            positions += velocities\n            \n            if eval_count % (self.budget // 5) == 0:  # More frequent inertia adaptation\n                self.inertia_weight *= self.inertia_damping\n                elite_position = positions[np.argmin(scores)]  # Elite learning strategy\n                perturbation = np.random.normal(0, 0.1, positions.shape)  # Moderate perturbation\n                positions += perturbation * (elite_position - positions)\n            \n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n            \n            scores = np.array([func(pos) for pos in positions])\n            eval_count += self.population_size\n            \n            personal_better_mask = scores < personal_best_scores\n            personal_best_scores[personal_better_mask] = scores[personal_better_mask]\n            personal_best_positions[personal_better_mask] = positions[personal_better_mask]\n            \n            current_global_best_index = np.argmin(personal_best_scores)\n            current_global_best_score = personal_best_scores[current_global_best_index]\n            if current_global_best_score < global_best_score:\n                global_best_score = current_global_best_score\n                global_best_position = np.copy(personal_best_positions[current_global_best_index])\n                \n        return global_best_score, global_best_position\n# ```"}
