{"id": "de7234b3-9411-4ce1-be40-4d8b13477c6e", "solution": "import numpy as np\n\nclass PSO_SA_Optimizer:\n    def __init__(self, budget, dim, num_particles=30, max_iterations=1000, inertia_weight=0.7, cognitive_weight=1.4, social_weight=1.4, initial_temperature=100, cooling_rate=0.95):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = num_particles\n        self.max_iterations = max_iterations\n        self.inertia_weight = inertia_weight\n        self.cognitive_weight = cognitive_weight\n        self.social_weight = social_weight\n        self.initial_temperature = initial_temperature\n        self.cooling_rate = cooling_rate\n\n    def __call__(self, func):\n        def pso_sa_helper():\n            # PSO initialization\n            # PSO optimization loop\n            # SA initialization\n            # SA optimization loop\n\n        return pso_sa_helper()", "name": "PSO_SA_Optimizer", "description": "A novel algorithm combining Particle Swarm Optimization (PSO) with Simulated Annealing (SA) to explore the search space effectively and efficiently.", "configspace": "", "generation": 0, "fitness": -Infinity, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/scratch/hyin/LLaMEA/llamea/llamea.py\", line 187, in initialize_single\n    new_individual = self.evaluate_fitness(new_individual)\n  File \"/scratch/hyin/LLaMEA/llamea/llamea.py\", line 264, in evaluate_fitness\n    updated_individual = self.f(individual, self.logger)\n  File \"mutation_exp.py\", line 32, in evaluateBBOB\n    exec(code, globals())\n  File \"<string>\", line 22\n    return pso_sa_helper()\n    ^\nIndentationError: expected an indented block\n.", "error": "IndentationError('expected an indented block', ('<string>', 22, 9, '        return pso_sa_helper()\\n'))Traceback (most recent call last):\n  File \"/scratch/hyin/LLaMEA/llamea/llamea.py\", line 187, in initialize_single\n    new_individual = self.evaluate_fitness(new_individual)\n  File \"/scratch/hyin/LLaMEA/llamea/llamea.py\", line 264, in evaluate_fitness\n    updated_individual = self.f(individual, self.logger)\n  File \"mutation_exp.py\", line 32, in evaluateBBOB\n    exec(code, globals())\n  File \"<string>\", line 22\n    return pso_sa_helper()\n    ^\nIndentationError: expected an indented block\n", "parent_id": null, "metadata": {}, "mutation_prompt": null}
{"id": "a0f02fc8-29f8-4a58-b68e-8ade601d9b56", "solution": "import numpy as np\n\nclass Dynamic_PSO_SA_Optimizer(PSO_SA_Optimizer):\n    def __init__(self, budget, dim, num_particles=30, max_iterations=1000, inertia_weight=0.7, cognitive_weight=1.4, social_weight=1.4, initial_temperature=100, cooling_rate=0.95):\n        super().__init__(budget, dim, num_particles, max_iterations, inertia_weight, cognitive_weight, social_weight, initial_temperature, cooling_rate)\n\n    def __call__(self, func):\n        def pso_sa_helper():\n            # Dynamic parameter adaptation mechanism\n            # PSO initialization with dynamic parameters\n            # PSO optimization loop with dynamic parameters\n            # SA initialization with dynamic parameters\n            # SA optimization loop with dynamic parameters\n\n        return pso_sa_helper()", "name": "Dynamic_PSO_SA_Optimizer", "description": "Enhancing the PSO_SA_Optimizer by incorporating a dynamic parameter adaptation mechanism based on self-adaptation to improve convergence speed and solution quality.", "configspace": "", "generation": 1, "fitness": -Infinity, "feedback": "An exception occurred: IndentationError('expected an indented block', ('<string>', 15, 9, '        return pso_sa_helper()\\n')).", "error": "IndentationError('expected an indented block', ('<string>', 15, 9, '        return pso_sa_helper()\\n'))", "parent_id": "de7234b3-9411-4ce1-be40-4d8b13477c6e", "metadata": {}, "mutation_prompt": null}
{"id": "fb012c01-f4c0-4e28-8ff0-f0a5b47f339f", "solution": "import numpy as np\n\nclass PSO_SA_Optimizer:\n    def __init__(self, budget, dim, num_particles=30, max_iterations=1000, inertia_weight=0.7, cognitive_weight=1.4, social_weight=1.4, initial_temperature=100, cooling_rate=0.95):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = num_particles\n        self.max_iterations = max_iterations\n        self.inertia_weight = inertia_weight\n        self.cognitive_weight = cognitive_weight\n        self.social_weight = social_weight\n        self.initial_temperature = initial_temperature\n        self.cooling_rate = cooling_rate\n\n    def __call__(self, func):\n        def pso_sa_helper():\n            # PSO initialization\n            # PSO optimization loop\n            # SA initialization\n            # SA optimization loop\n\n        return pso_sa_helper()", "name": "PSO_SA_Optimizer", "description": "A novel algorithm combining Particle Swarm Optimization (PSO) with Simulated Annealing (SA) to explore the search space effectively and efficiently.", "configspace": "", "generation": 1, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "de7234b3-9411-4ce1-be40-4d8b13477c6e", "metadata": {}, "mutation_prompt": null}
{"id": "74c38760-19c3-410c-8b01-ce7a251db778", "solution": "import numpy as np\n\nclass Enhanced_PSO_SA_Optimizer:\n    def __init__(self, budget, dim, num_particles=30, max_iterations=1000, inertia_weight=0.7, cognitive_weight=1.4, social_weight=1.4, initial_temperature=100, cooling_rate=0.95, sa_iters=50):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = num_particles\n        self.max_iterations = max_iterations\n        self.inertia_weight = inertia_weight\n        self.cognitive_weight = cognitive_weight\n        self.social_weight = social_weight\n        self.initial_temperature = initial_temperature\n        self.cooling_rate = cooling_rate\n        self.sa_iters = sa_iters\n\n    def __call__(self, func):\n        def enhanced_pso_sa_helper():\n            # Enhanced PSO initialization\n            # Enhanced PSO optimization loop with SA updates\n\n        return enhanced_pso_sa_helper()", "name": "Enhanced_PSO_SA_Optimizer", "description": "Improve the PSO_SA_Optimizer by enhancing the coordination between PSO and SA for better exploration-exploitation balance.", "configspace": "", "generation": 3, "fitness": -Infinity, "feedback": "An exception occurred: IndentationError('expected an indented block', ('<string>', 21, 9, '        return enhanced_pso_sa_helper()\\n')).", "error": "IndentationError('expected an indented block', ('<string>', 21, 9, '        return enhanced_pso_sa_helper()\\n'))", "parent_id": "de7234b3-9411-4ce1-be40-4d8b13477c6e", "metadata": {}, "mutation_prompt": null}
{"id": "b960698d-801b-435e-a02d-7cbd44b7ba6e", "solution": "import numpy as np\n\nclass DynamicInertiaPSO_SA_Optimizer:\n    def __init__(self, budget, dim, num_particles=30, max_iterations=1000, initial_inertia_weight=0.9, cognitive_weight=1.4, social_weight=1.4, initial_temperature=100, cooling_rate=0.95):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = num_particles\n        self.max_iterations = max_iterations\n        self.initial_inertia_weight = initial_inertia_weight\n        self.cognitive_weight = cognitive_weight\n        self.social_weight = social_weight\n        self.initial_temperature = initial_temperature\n        self.cooling_rate = cooling_rate\n\n    def __call__(self, func):\n        def dynamic_inertia_pso_sa_helper():\n            # PSO initialization with dynamic inertia adaptation\n            # PSO optimization loop with dynamic inertia update\n            # SA initialization\n            # SA optimization loop\n\n        return dynamic_inertia_pso_sa_helper()", "name": "DynamicInertiaPSO_SA_Optimizer", "description": "Enhancing the PSO_SA_Optimizer algorithm by incorporating a dynamic adaptation mechanism for the inertia weight to improve convergence speed and accuracy.", "configspace": "", "generation": 4, "fitness": -Infinity, "feedback": "An exception occurred: IndentationError('expected an indented block', ('<string>', 22, 9, '        return dynamic_inertia_pso_sa_helper()\\n')).", "error": "IndentationError('expected an indented block', ('<string>', 22, 9, '        return dynamic_inertia_pso_sa_helper()\\n'))", "parent_id": "de7234b3-9411-4ce1-be40-4d8b13477c6e", "metadata": {}, "mutation_prompt": null}
{"id": "fd73f664-4651-4387-ae92-9155c9b41b0a", "solution": "import numpy as np\n\nclass PSO_SA_Optimizer:\n    def __init__(self, budget, dim, num_particles=30, max_iterations=1000, inertia_weight=0.7, cognitive_weight=1.4, social_weight=1.4, initial_temperature=100, cooling_rate=0.95):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = num_particles\n        self.max_iterations = max_iterations\n        self.inertia_weight = inertia_weight\n        self.cognitive_weight = cognitive_weight\n        self.social_weight = social_weight\n        self.initial_temperature = initial_temperature\n        self.cooling_rate = cooling_rate\n\n    def __call__(self, func):\n        def pso_sa_helper():\n            # PSO initialization\n            # PSO optimization loop\n            # SA initialization\n            # SA optimization loop\n\n        return pso_sa_helper()", "name": "PSO_SA_Optimizer", "description": "A novel algorithm combining Particle Swarm Optimization (PSO) with Simulated Annealing (SA) to explore the search space effectively and efficiently.", "configspace": "", "generation": 1, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "de7234b3-9411-4ce1-be40-4d8b13477c6e", "metadata": {}, "mutation_prompt": null}
{"id": "ddc5ce52-3460-450b-9398-0c91531fb33a", "solution": "import numpy as np\n\nclass Refined_PSO_SA_Optimizer:\n    def __init__(self, budget, dim, num_particles=30, max_iterations=1000, inertia_weight=0.7, cognitive_weight=1.4, social_weight=1.4, initial_temperature=100, cooling_rate=0.95):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = num_particles\n        self.max_iterations = max_iterations\n        self.inertia_weight = inertia_weight\n        self.cognitive_weight = cognitive_weight\n        self.social_weight = social_weight\n        self.initial_temperature = initial_temperature\n        self.cooling_rate = cooling_rate\n\n    def __call__(self, func):\n        def refined_pso_sa_helper():\n            # PSO initialization with refined strategy\n            # PSO optimization loop with refined strategy\n            # SA initialization with refined strategy\n            # SA optimization loop with refined strategy\n\n        return refined_pso_sa_helper()", "name": "Refined_PSO_SA_Optimizer", "description": "A novel hybrid metaheuristic algorithm combining Particle Swarm Optimization (PSO) and Simulated Annealing (SA) with refined strategy.", "configspace": "", "generation": 6, "fitness": -Infinity, "feedback": "An exception occurred: IndentationError('expected an indented block', ('<string>', 22, 9, '        return refined_pso_sa_helper()\\n')).", "error": "IndentationError('expected an indented block', ('<string>', 22, 9, '        return refined_pso_sa_helper()\\n'))", "parent_id": "de7234b3-9411-4ce1-be40-4d8b13477c6e", "metadata": {}, "mutation_prompt": null}
{"id": "062b7512-124d-4e18-8e00-9e6062de6c19", "solution": "import numpy as np\n\nclass Adaptive_PSO_SA_Optimizer:\n    def __init__(self, budget, dim, num_particles=30, max_iterations=1000, initial_temperature=100, cooling_rate=0.95):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = num_particles\n        self.max_iterations = max_iterations\n        self.initial_temperature = initial_temperature\n        self.cooling_rate = cooling_rate\n\n    def __call__(self, func):\n        def adaptive_pso_sa_helper():\n            # Adaptive PSO initialization\n            # Adaptive PSO optimization loop\n            # Adaptive SA initialization\n            # Adaptive SA optimization loop\n\n        return adaptive_pso_sa_helper()", "name": "Adaptive_PSO_SA_Optimizer", "description": "Combining Particle Swarm Optimization (PSO) with Simulated Annealing (SA) using adaptive parameters to dynamically balance exploration and exploitation for efficient search.", "configspace": "", "generation": 7, "fitness": -Infinity, "feedback": "An exception occurred: IndentationError('expected an indented block', ('<string>', 19, 9, '        return adaptive_pso_sa_helper()\\n')).", "error": "IndentationError('expected an indented block', ('<string>', 19, 9, '        return adaptive_pso_sa_helper()\\n'))", "parent_id": "de7234b3-9411-4ce1-be40-4d8b13477c6e", "metadata": {}, "mutation_prompt": null}
{"id": "504a628e-00bf-4a67-a840-04f5aa7aae3e", "solution": "import numpy as np\n\nclass Adaptive_PSO_SA_Optimizer:\n    def __init__(self, budget, dim, num_particles=30, max_iterations=1000, inertia_weight=0.7, cognitive_weight=1.4, social_weight=1.4, initial_temperature=100, cooling_rate=0.95):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = num_particles\n        self.max_iterations = max_iterations\n        self.inertia_weight = inertia_weight\n        self.cognitive_weight = cognitive_weight\n        self.social_weight = social_weight\n        self.initial_temperature = initial_temperature\n        self.cooling_rate = cooling_rate\n\n    def __call__(self, func):\n        def adaptive_pso_sa_helper():\n            # Adaptive mechanisms for parameter adjustment during optimization\n            \n            # PSO initialization\n            # PSO optimization loop with adaptive parameters\n            # SA initialization\n            # SA optimization loop with adaptive parameters\n\n        return adaptive_pso_sa_helper()", "name": "Adaptive_PSO_SA_Optimizer", "description": "Improve PSO_SA_Optimizer by incorporating adaptive mechanisms to dynamically adjust algorithm parameters during optimization.", "configspace": "", "generation": 8, "fitness": -Infinity, "feedback": "An exception occurred: IndentationError('expected an indented block', ('<string>', 24, 9, '        return adaptive_pso_sa_helper()\\n')).", "error": "IndentationError('expected an indented block', ('<string>', 24, 9, '        return adaptive_pso_sa_helper()\\n'))", "parent_id": "de7234b3-9411-4ce1-be40-4d8b13477c6e", "metadata": {}, "mutation_prompt": null}
{"id": "5d1aebab-aba0-4a8c-80d0-de6652888227", "solution": "import numpy as np\n\nclass Dynamic_PSO_SA_Optimizer:\n    def __init__(self, budget, dim, num_particles=30, max_iterations=1000, inertia_weight=0.7, cognitive_weight=1.4, social_weight=1.4, initial_temperature=100, cooling_rate=0.95):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = num_particles\n        self.max_iterations = max_iterations\n        self.inertia_weight = inertia_weight\n        self.cognitive_weight = cognitive_weight\n        self.social_weight = social_weight\n        self.initial_temperature = initial_temperature\n        self.cooling_rate = cooling_rate\n\n    def __call__(self, func):\n        def pso_sa_helper():\n            # PSO initialization\n            # PSO optimization loop with dynamic cognitive and social weights\n            # SA initialization\n            # SA optimization loop\n\n        return pso_sa_helper()", "name": "Dynamic_PSO_SA_Optimizer", "description": "Enhancing the PSO_SA_Optimizer algorithm by dynamically adjusting the cognitive and social weights during the optimization process.", "configspace": "", "generation": 9, "fitness": -Infinity, "feedback": "An exception occurred: IndentationError('expected an indented block', ('<string>', 22, 9, '        return pso_sa_helper()\\n')).", "error": "IndentationError('expected an indented block', ('<string>', 22, 9, '        return pso_sa_helper()\\n'))", "parent_id": "de7234b3-9411-4ce1-be40-4d8b13477c6e", "metadata": {}, "mutation_prompt": null}
{"id": "89de97f2-24cb-4ff2-8f7e-9106f839064b", "solution": "import numpy as np\n\nclass Enhanced_PSO_SA_Optimizer:\n    def __init__(self, budget, dim, num_particles=30, max_iterations=1000, inertia_weight=0.7, cognitive_weight=1.4, social_weight=1.4, initial_temperature=100, cooling_rate=0.95):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = num_particles\n        self.max_iterations = max_iterations\n        self.inertia_weight = inertia_weight\n        self.cognitive_weight = cognitive_weight\n        self.social_weight = social_weight\n        self.initial_temperature = initial_temperature\n        self.cooling_rate = cooling_rate\n\n    def __call__(self, func):\n        def enhanced_pso_sa_helper():\n            # Enhanced PSO initialization with dynamic weights\n            # Enhanced PSO optimization loop with dynamic cognitive and social weights adjustment\n            # SA initialization\n            # SA optimization loop\n\n        return enhanced_pso_sa_helper()", "name": "Enhanced_PSO_SA_Optimizer", "description": "An enhanced PSO_SA_Optimizer algorithm with dynamic adjustment of cognitive and social weights during optimization.", "configspace": "", "generation": 10, "fitness": -Infinity, "feedback": "An exception occurred: IndentationError('expected an indented block', ('<string>', 22, 9, '        return enhanced_pso_sa_helper()\\n')).", "error": "IndentationError('expected an indented block', ('<string>', 22, 9, '        return enhanced_pso_sa_helper()\\n'))", "parent_id": "de7234b3-9411-4ce1-be40-4d8b13477c6e", "metadata": {}, "mutation_prompt": null}
{"id": "317b2d19-f5c6-491b-93cc-d10a598b4962", "solution": "import numpy as np\n\nclass PSO_SA_Optimizer:\n    def __init__(self, budget, dim, num_particles=30, max_iterations=1000, inertia_weight=0.7, cognitive_weight=1.4, social_weight=1.4, initial_temperature=100, cooling_rate=0.95):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = num_particles\n        self.max_iterations = max_iterations\n        self.inertia_weight = inertia_weight\n        self.cognitive_weight = cognitive_weight\n        self.social_weight = social_weight\n        self.initial_temperature = initial_temperature\n        self.cooling_rate = cooling_rate\n\n    def __call__(self, func):\n        def pso_sa_helper():\n            # PSO initialization\n            # PSO optimization loop\n            # SA initialization\n            # SA optimization loop\n\n        return pso_sa_helper()", "name": "PSO_SA_Optimizer", "description": "A novel algorithm combining Particle Swarm Optimization (PSO) with Simulated Annealing (SA) to explore the search space effectively and efficiently.", "configspace": "", "generation": 1, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "de7234b3-9411-4ce1-be40-4d8b13477c6e", "metadata": {}, "mutation_prompt": null}
{"id": "50cd065b-271e-4179-9986-c60aae93c0b3", "solution": "import numpy as np\n\nclass Adaptive_PSO_SA_Optimizer:\n    def __init__(self, budget, dim, num_particles=30, max_iterations=1000, initial_inertia_weight=0.9, initial_cognitive_weight=1.5, initial_social_weight=1.5, initial_temperature=100, cooling_rate=0.95):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = num_particles\n        self.max_iterations = max_iterations\n        self.initial_inertia_weight = initial_inertia_weight\n        self.initial_cognitive_weight = initial_cognitive_weight\n        self.initial_social_weight = initial_social_weight\n        self.initial_temperature = initial_temperature\n        self.cooling_rate = cooling_rate\n\n    def __call__(self, func):\n        def adaptive_pso_sa_helper():\n            # Adaptive PSO initialization\n            # Adaptive PSO optimization loop with dynamic parameter adjustments\n            # Adaptive SA initialization with parameter adaptation\n            # Adaptive SA optimization loop with dynamic strategy changes\n\n        return adaptive_pso_sa_helper()", "name": "Adaptive_PSO_SA_Optimizer", "description": "An enhanced PSO_SA_Optimizer algorithm with adaptive parameters and dynamic strategy adjustments.", "configspace": "", "generation": 12, "fitness": -Infinity, "feedback": "An exception occurred: IndentationError('expected an indented block', ('<string>', 22, 9, '        return adaptive_pso_sa_helper()\\n')).", "error": "IndentationError('expected an indented block', ('<string>', 22, 9, '        return adaptive_pso_sa_helper()\\n'))", "parent_id": "de7234b3-9411-4ce1-be40-4d8b13477c6e", "metadata": {}, "mutation_prompt": null}
{"id": "a194bce4-1786-49c2-b8f2-67013e430ed0", "solution": "import numpy as np\n\nclass PSO_SA_Mutation_Optimizer:\n    def __init__(self, budget, dim, num_particles=30, max_iterations=1000, inertia_weight=0.7, cognitive_weight=1.4, social_weight=1.4, initial_temperature=100, cooling_rate=0.95, mutation_probability=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = num_particles\n        self.max_iterations = max_iterations\n        self.inertia_weight = inertia_weight\n        self.cognitive_weight = cognitive_weight\n        self.social_weight = social_weight\n        self.initial_temperature = initial_temperature\n        self.cooling_rate = cooling_rate\n        self.mutation_probability = mutation_probability\n\n    def __call__(self, func):\n        def pso_sa_mutation_helper():\n            # PSO initialization\n            # PSO optimization loop with dynamic mutation\n            # SA initialization\n            # SA optimization loop\n\n        return pso_sa_mutation_helper()", "name": "PSO_SA_Mutation_Optimizer", "description": "Enhancing the PSO_SA_Optimizer by incorporating a dynamic mutation operator to explore the search space more effectively.", "configspace": "", "generation": 13, "fitness": -Infinity, "feedback": "An exception occurred: IndentationError('expected an indented block', ('<string>', 23, 9, '        return pso_sa_mutation_helper()\\n')).", "error": "IndentationError('expected an indented block', ('<string>', 23, 9, '        return pso_sa_mutation_helper()\\n'))", "parent_id": "de7234b3-9411-4ce1-be40-4d8b13477c6e", "metadata": {}, "mutation_prompt": null}
{"id": "dc30e1be-e39a-4a27-9117-4337e123b1c2", "solution": "import numpy as np\n\nclass PSO_SA_Optimizer:\n    def __init__(self, budget, dim, num_particles=30, max_iterations=1000, inertia_weight=0.7, cognitive_weight=1.4, social_weight=1.4, initial_temperature=100, cooling_rate=0.95):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = num_particles\n        self.max_iterations = max_iterations\n        self.inertia_weight = inertia_weight\n        self.cognitive_weight = cognitive_weight\n        self.social_weight = social_weight\n        self.initial_temperature = initial_temperature\n        self.cooling_rate = cooling_rate\n\n    def __call__(self, func):\n        def pso_sa_helper():\n            # PSO initialization\n            # PSO optimization loop\n            # SA initialization\n            # SA optimization loop\n\n        return pso_sa_helper()", "name": "PSO_SA_Optimizer", "description": "A novel algorithm combining Particle Swarm Optimization (PSO) with Simulated Annealing (SA) to explore the search space effectively and efficiently.", "configspace": "", "generation": 1, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "de7234b3-9411-4ce1-be40-4d8b13477c6e", "metadata": {}, "mutation_prompt": null}
{"id": "3cc471d1-4f32-44e4-99dc-9103c1b5160d", "solution": "import numpy as np\n\nclass Enhanced_PSO_SA_Optimizer:\n    def __init__(self, budget, dim, num_particles=30, max_iterations=1000, inertia_weight=0.7, cognitive_weight=1.4, social_weight=1.4, initial_temperature=100, cooling_rate=0.95):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = num_particles\n        self.max_iterations = max_iterations\n        self.inertia_weight = inertia_weight\n        self.cognitive_weight = cognitive_weight\n        self.social_weight = social_weight\n        self.initial_temperature = initial_temperature\n        self.cooling_rate = cooling_rate\n\n    def __call__(self, func):\n        def enhanced_pso_sa_helper():\n            # Enhanced PSO with adaptive parameters\n            # Dynamic population size adjustment\n            # Incorporating self-adaptive mechanisms\n            \n        return enhanced_pso_sa_helper()", "name": "Enhanced_PSO_SA_Optimizer", "description": "Enhanced PSO_SA_Optimizer with adaptive parameter tuning and dynamic population size for improved exploration-exploitation balance.", "configspace": "", "generation": 15, "fitness": -Infinity, "feedback": "An exception occurred: IndentationError('expected an indented block', ('<string>', 21, 9, '        return enhanced_pso_sa_helper()\\n')).", "error": "IndentationError('expected an indented block', ('<string>', 21, 9, '        return enhanced_pso_sa_helper()\\n'))", "parent_id": "de7234b3-9411-4ce1-be40-4d8b13477c6e", "metadata": {}, "mutation_prompt": null}
{"id": "efcdb78b-a90e-4513-aa6c-6ace189bd211", "solution": "import numpy as np\n\nclass DE_SA_Optimizer:\n    def __init__(self, budget, dim, population_size=50, f=0.5, cr=0.9, initial_temperature=100, cooling_rate=0.95):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = population_size\n        self.f = f\n        self.cr = cr\n        self.initial_temperature = initial_temperature\n        self.cooling_rate = cooling_rate\n\n    def __call__(self, func):\n        def de_sa_helper():\n            # DE initialization\n            # DE optimization loop\n            # SA initialization\n            # SA optimization loop\n\n        return de_sa_helper()", "name": "DE_SA_Optimizer", "description": "Enhancing the exploration and exploitation balance by incorporating a hybrid Differential Evolution (DE) with Simulated Annealing (SA) algorithm.", "configspace": "", "generation": 16, "fitness": -Infinity, "feedback": "An exception occurred: IndentationError('expected an indented block', ('<string>', 20, 9, '        return de_sa_helper()\\n')).", "error": "IndentationError('expected an indented block', ('<string>', 20, 9, '        return de_sa_helper()\\n'))", "parent_id": "de7234b3-9411-4ce1-be40-4d8b13477c6e", "metadata": {}, "mutation_prompt": null}
{"id": "d4015450-65ca-47a9-ac4f-0b35ceb98db4", "solution": "import numpy as np\n\nclass PSO_SA_LocalSearch_Optimizer(PSO_SA_Optimizer):\n    def __init__(self, budget, dim, num_particles=30, max_iterations=1000, inertia_weight=0.7, cognitive_weight=1.4, social_weight=1.4, initial_temperature=100, cooling_rate=0.95, local_search_probability=0.1):\n        super().__init__(budget, dim, num_particles, max_iterations, inertia_weight, cognitive_weight, social_weight, initial_temperature, cooling_rate)\n        self.local_search_probability = local_search_probability\n\n    def local_search(self, particle):\n        # Implement a local search mechanism to exploit local regions\n\n    def __call__(self, func):\n        def pso_sa_helper():\n            # PSO initialization\n            # PSO optimization loop\n            # SA initialization\n            # SA optimization loop\n            # Integrate Local Search based on probability\n\n        return pso_sa_helper()", "name": "PSO_SA_LocalSearch_Optimizer", "description": "Enhancing the PSO_SA_Optimizer algorithm by integrating a Local Search mechanism to exploit promising regions and improve solution quality.", "configspace": "", "generation": 17, "fitness": -Infinity, "feedback": "An exception occurred: IndentationError('expected an indented block', ('<string>', 11, 5, '    def __call__(self, func):\\n')).", "error": "IndentationError('expected an indented block', ('<string>', 11, 5, '    def __call__(self, func):\\n'))", "parent_id": "de7234b3-9411-4ce1-be40-4d8b13477c6e", "metadata": {}, "mutation_prompt": null}
{"id": "476c3886-7eba-4069-b329-e1a3cbb2252a", "solution": "import numpy as np\n\nclass Enhanced_PSO_SA_Optimizer:\n    def __init__(self, budget, dim, num_particles=30, max_iterations=1000, inertia_weight=0.7, cognitive_weight=1.4, social_weight=1.4, initial_temperature=100, cooling_rate=0.95):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = num_particles\n        self.max_iterations = max_iterations\n        self.inertia_weight = inertia_weight\n        self.cognitive_weight = cognitive_weight\n        self.social_weight = social_weight\n        self.initial_temperature = initial_temperature\n        self.cooling_rate = cooling_rate\n\n    def __call__(self, func):\n        def pso_sa_helper():\n            # PSO initialization\n            # PSO optimization loop with dynamically adjusting cognitive and social weights\n            # SA initialization\n            # SA optimization loop\n\n        return pso_sa_helper()", "name": "Enhanced_PSO_SA_Optimizer", "description": "Enhancing the exploration-exploitation balance of the PSO_SA_Optimizer algorithm by dynamically adjusting the cognitive and social weights during optimization.", "configspace": "", "generation": 18, "fitness": -Infinity, "feedback": "An exception occurred: IndentationError('expected an indented block', ('<string>', 22, 9, '        return pso_sa_helper()\\n')).", "error": "IndentationError('expected an indented block', ('<string>', 22, 9, '        return pso_sa_helper()\\n'))", "parent_id": "de7234b3-9411-4ce1-be40-4d8b13477c6e", "metadata": {}, "mutation_prompt": null}
{"id": "d73272f5-38eb-40f3-9112-7e98a49581e8", "solution": "import numpy as np\n\nclass PSO_SA_Optimizer:\n    def __init__(self, budget, dim, num_particles=30, max_iterations=1000, inertia_weight=0.7, cognitive_weight=1.4, social_weight=1.4, initial_temperature=100, cooling_rate=0.95):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = num_particles\n        self.max_iterations = max_iterations\n        self.inertia_weight = inertia_weight\n        self.cognitive_weight = cognitive_weight\n        self.social_weight = social_weight\n        self.initial_temperature = initial_temperature\n        self.cooling_rate = cooling_rate\n\n    def __call__(self, func):\n        def pso_sa_helper():\n            # PSO initialization with multi-start strategy\n            # PSO optimization loop with enhanced global search\n            # SA initialization\n            # SA optimization loop\n\n        return pso_sa_helper()", "name": "PSO_SA_Optimizer", "description": "Enhancing PSO_SA_Optimizer algorithm by incorporating a multi-start strategy to improve the global exploration capability.", "configspace": "", "generation": 19, "fitness": -Infinity, "feedback": "An exception occurred: IndentationError('expected an indented block', ('<string>', 22, 9, '        return pso_sa_helper()\\n')).", "error": "IndentationError('expected an indented block', ('<string>', 22, 9, '        return pso_sa_helper()\\n'))", "parent_id": "de7234b3-9411-4ce1-be40-4d8b13477c6e", "metadata": {}, "mutation_prompt": null}
{"id": "c8b0d7ab-42cf-4534-8354-fb5486f131b0", "solution": "import numpy as np\n\nclass PSO_SA_Optimizer:\n    def __init__(self, budget, dim, num_particles=30, max_iterations=1000, inertia_weight=0.7, cognitive_weight=1.4, social_weight=1.4, initial_temperature=100, cooling_rate=0.95):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = num_particles\n        self.max_iterations = max_iterations\n        self.inertia_weight = inertia_weight\n        self.cognitive_weight = cognitive_weight\n        self.social_weight = social_weight\n        self.initial_temperature = initial_temperature\n        self.cooling_rate = cooling_rate\n\n    def __call__(self, func):\n        def pso_sa_helper():\n            # PSO initialization\n            best_particle_position = np.random.uniform(-5.0, 5.0, size=(self.num_particles, self.dim))\n            best_particle_fitness = np.full(self.num_particles, np.inf)\n            global_best_position = np.zeros(self.dim)\n            global_best_fitness = np.inf\n\n            # PSO optimization loop\n            for _ in range(self.max_iterations):\n                # PSO update rules\n\n            # SA initialization\n            current_solution = global_best_position\n            current_fitness = func(global_best_position)\n\n            # SA optimization loop\n            for _ in range(self.max_iterations):\n                proposed_solution = current_solution + np.random.normal(0, 1, self.dim)\n                proposed_solution = np.clip(proposed_solution, -5.0, 5.0)\n                proposed_fitness = func(proposed_solution)\n                if acceptance_probability(current_fitness, proposed_fitness, self.initial_temperature) > np.random.rand():\n                    current_solution = proposed_solution\n                    current_fitness = proposed_fitness\n                    if current_fitness < global_best_fitness:\n                        global_best_position = current_solution\n                        global_best_fitness = current_fitness\n\n        return pso_sa_helper()\n\ndef acceptance_probability(current_fitness, proposed_fitness, temperature):\n    if proposed_fitness < current_fitness:\n        return 1.0\n    return np.exp((current_fitness - proposed_fitness) / temperature)", "name": "PSO_SA_Optimizer", "description": "Improving the initialization and optimization loop handling within the PSO_SA_Optimizer class to ensure proper execution flow.", "configspace": "", "generation": 20, "fitness": -Infinity, "feedback": "An exception occurred: IndentationError('expected an indented block', ('<string>', 28, 13, '            current_solution = global_best_position\\n')).", "error": "IndentationError('expected an indented block', ('<string>', 28, 13, '            current_solution = global_best_position\\n'))", "parent_id": "de7234b3-9411-4ce1-be40-4d8b13477c6e", "metadata": {}, "mutation_prompt": null}
{"id": "600dcd3c-6565-4d21-a0e7-e5ccbcf49bcf", "solution": "import numpy as np\n\nclass PSO_SA_Optimizer:\n    def __init__(self, budget, dim, num_particles=30, max_iterations=1000, inertia_weight=0.7, cognitive_weight=1.4, social_weight=1.4, initial_temperature=100, cooling_rate=0.95):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = num_particles\n        self.max_iterations = max_iterations\n        self.inertia_weight = inertia_weight\n        self.cognitive_weight = cognitive_weight\n        self.social_weight = social_weight\n        self.initial_temperature = initial_temperature\n        self.cooling_rate = cooling_rate\n\n    def __call__(self, func):\n        def pso_sa_helper():\n            # PSO initialization\n            # PSO optimization loop\n            # SA initialization\n            # SA optimization loop\n\n        return pso_sa_helper()", "name": "PSO_SA_Optimizer", "description": "A novel algorithm combining Particle Swarm Optimization (PSO) with Simulated Annealing (SA) to explore the search space effectively and efficiently.", "configspace": "", "generation": 1, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "de7234b3-9411-4ce1-be40-4d8b13477c6e", "metadata": {}, "mutation_prompt": null}
{"id": "68fd0190-5ed8-4928-b39f-a04d46f103fb", "solution": "import numpy as np\n\nclass Adaptive_PSO_SA_Optimizer(PSO_SA_Optimizer):\n    def __init__(self, budget, dim, num_particles=30, max_iterations=1000, inertia_weight=0.7, cognitive_weight=1.4, social_weight=1.4, initial_temperature=100, cooling_rate=0.95):\n        super().__init__(budget, dim, num_particles, max_iterations, inertia_weight, cognitive_weight, social_weight, initial_temperature, cooling_rate)\n\n    def __call__(self, func):\n        def pso_sa_helper():\n            # Adaptive parameter update based on exploration and exploitation\n            # Enhanced PSO optimization loop with dynamic parameter adaptations\n            # Enhanced SA optimization loop with dynamic parameter adaptations\n\n        return pso_sa_helper()", "name": "Adaptive_PSO_SA_Optimizer", "description": "An enhanced PSO_SA_Optimizer algorithm with adaptive parameters for improved convergence and exploration.", "configspace": "", "generation": 22, "fitness": -Infinity, "feedback": "An exception occurred: IndentationError('expected an indented block', ('<string>', 13, 9, '        return pso_sa_helper()\\n')).", "error": "IndentationError('expected an indented block', ('<string>', 13, 9, '        return pso_sa_helper()\\n'))", "parent_id": "de7234b3-9411-4ce1-be40-4d8b13477c6e", "metadata": {}, "mutation_prompt": null}
{"id": "2e81293f-37d0-42af-8a4a-5df8f1c14a5b", "solution": "import numpy as np\n\nclass PSO_SA_Optimizer:\n    def __init__(self, budget, dim, num_particles=30, max_iterations=1000, inertia_weight=0.7, cognitive_weight=1.4, social_weight=1.4, initial_temperature=100, cooling_rate=0.95):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = num_particles\n        self.max_iterations = max_iterations\n        self.inertia_weight = inertia_weight\n        self.cognitive_weight = cognitive_weight\n        self.social_weight = social_weight\n        self.initial_temperature = initial_temperature\n        self.cooling_rate = cooling_rate\n\n    def __call__(self, func):\n        def pso_sa_helper():\n            # PSO initialization\n            # PSO optimization loop\n            # SA initialization\n            # SA optimization loop\n\n        return pso_sa_helper()", "name": "PSO_SA_Optimizer", "description": "A novel algorithm combining Particle Swarm Optimization (PSO) with Simulated Annealing (SA) to explore the search space effectively and efficiently.", "configspace": "", "generation": 1, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "de7234b3-9411-4ce1-be40-4d8b13477c6e", "metadata": {}, "mutation_prompt": null}
{"id": "dad94fcd-e874-424f-9adc-6e9ab83e2432", "solution": "import numpy as np\n\nclass Enhanced_PSO_SA_Optimizer(PSO_SA_Optimizer):\n    def __init__(self, budget, dim, num_particles=30, max_iterations=1000, inertia_weight=0.7, cognitive_weight=1.4, social_weight=1.4, initial_temperature=100, cooling_rate=0.95):\n        super().__init__(budget, dim, num_particles, max_iterations, inertia_weight, cognitive_weight, social_weight, initial_temperature, cooling_rate)\n\n    def __call__(self, func):\n        def enhanced_pso_sa_helper():\n            # Enhanced PSO initialization with dynamic cognitive and social weights\n            # Enhanced PSO optimization loop with dynamically adjusted weights\n            # SA initialization\n            # SA optimization loop\n\n        return enhanced_pso_sa_helper()", "name": "Enhanced_PSO_SA_Optimizer", "description": "Enhancing the PSO_SA_Optimizer algorithm by dynamically adjusting the cognitive and social weights based on the distance between particles for improved exploration-exploitation balance.", "configspace": "", "generation": 24, "fitness": -Infinity, "feedback": "An exception occurred: IndentationError('expected an indented block', ('<string>', 14, 9, '        return enhanced_pso_sa_helper()\\n')).", "error": "IndentationError('expected an indented block', ('<string>', 14, 9, '        return enhanced_pso_sa_helper()\\n'))", "parent_id": "de7234b3-9411-4ce1-be40-4d8b13477c6e", "metadata": {}, "mutation_prompt": null}
{"id": "5649d49b-21aa-4562-827d-e37593c77528", "solution": "import numpy as np\n\nclass Adaptive_PSO_SA_Optimizer(PSO_SA_Optimizer):\n    def __init__(self, budget, dim, num_particles=30, max_iterations=1000, inertia_weight=0.7, cognitive_weight=1.4, social_weight=1.4, initial_temperature=100, cooling_rate=0.95):\n        super().__init__(budget, dim, num_particles, max_iterations, inertia_weight, cognitive_weight, social_weight, initial_temperature, cooling_rate)\n\n    def __call__(self, func):\n        def pso_sa_helper():\n            # Adaptive mechanism for adjusting cognitive and social weights in PSO\n            # PSO initialization with adaptive weights\n            # PSO optimization loop with dynamically adjusted weights\n            # SA initialization\n            # SA optimization loop\n\n        return pso_sa_helper()", "name": "Adaptive_PSO_SA_Optimizer", "description": "Enhancing the PSO_SA_Optimizer by introducing a novel adaptive mechanism to adjust the cognitive and social weights dynamically during optimization.", "configspace": "", "generation": 25, "fitness": -Infinity, "feedback": "An exception occurred: IndentationError('expected an indented block', ('<string>', 15, 9, '        return pso_sa_helper()\\n')).", "error": "IndentationError('expected an indented block', ('<string>', 15, 9, '        return pso_sa_helper()\\n'))", "parent_id": "de7234b3-9411-4ce1-be40-4d8b13477c6e", "metadata": {}, "mutation_prompt": null}
{"id": "32e533b9-eb3d-44ec-9ae2-6fab1fe888e2", "solution": "import numpy as np\n\nclass DynamicWeight_PSO_SA_Optimizer:\n    def __init__(self, budget, dim, num_particles=30, max_iterations=1000, inertia_weight=0.7, cognitive_weight=1.4, social_weight=1.4, initial_temperature=100, cooling_rate=0.95):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = num_particles\n        self.max_iterations = max_iterations\n        self.inertia_weight = inertia_weight\n        self.cognitive_weight = cognitive_weight\n        self.social_weight = social_weight\n        self.initial_temperature = initial_temperature\n        self.cooling_rate = cooling_rate\n\n    def __call__(self, func):\n        def dynamic_pso_sa_helper():\n            # PSO initialization with dynamic weights\n            # Dynamic weights adaptation during PSO optimization loop\n            # SA initialization with dynamic weights\n            # Dynamic weights adaptation during SA optimization loop\n\n        return dynamic_pso_sa_helper()", "name": "DynamicWeight_PSO_SA_Optimizer", "description": "Enhancing PSO_SA_Optimizer by dynamically adjusting the weights during optimization to improve exploration and exploitation balance.", "configspace": "", "generation": 26, "fitness": -Infinity, "feedback": "An exception occurred: IndentationError('expected an indented block', ('<string>', 22, 9, '        return dynamic_pso_sa_helper()\\n')).", "error": "IndentationError('expected an indented block', ('<string>', 22, 9, '        return dynamic_pso_sa_helper()\\n'))", "parent_id": "de7234b3-9411-4ce1-be40-4d8b13477c6e", "metadata": {}, "mutation_prompt": null}
{"id": "c2ab5c82-27e6-4fac-ab5a-120eee61edb0", "solution": "import numpy as np\n\nclass PSO_SA_Optimizer:\n    def __init__(self, budget, dim, num_particles=30, max_iterations=1000, inertia_weight=0.7, cognitive_weight=1.4, social_weight=1.4, initial_temperature=100, cooling_rate=0.95):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = num_particles\n        self.max_iterations = max_iterations\n        self.inertia_weight = inertia_weight\n        self.cognitive_weight = cognitive_weight\n        self.social_weight = social_weight\n        self.initial_temperature = initial_temperature\n        self.cooling_rate = cooling_rate\n        self.dynamic_rate = 0.15  # Dynamic adaptation rate\n\n    def __call__(self, func):\n        def pso_sa_helper():\n            # PSO initialization with dynamic parameter adaptation\n            # PSO optimization loop with dynamic parameter adaptation\n            # SA initialization with dynamic parameter adaptation\n            # SA optimization loop with dynamic parameter adaptation\n\n        return pso_sa_helper()", "name": "PSO_SA_Optimizer", "description": "Enhancing PSO_SA_Optimizer algorithm by incorporating a dynamic parameter adaptation mechanism to improve convergence.", "configspace": "", "generation": 27, "fitness": -Infinity, "feedback": "An exception occurred: IndentationError('expected an indented block', ('<string>', 23, 9, '        return pso_sa_helper()\\n')).", "error": "IndentationError('expected an indented block', ('<string>', 23, 9, '        return pso_sa_helper()\\n'))", "parent_id": "de7234b3-9411-4ce1-be40-4d8b13477c6e", "metadata": {}, "mutation_prompt": null}
{"id": "aa1e84cc-0e18-4b11-aa3f-10c0a6eff3fa", "solution": "import numpy as np\n\nclass PSO_SA_Optimizer:\n    def __init__(self, budget, dim, num_particles=30, max_iterations=1000, inertia_weight=0.7, cognitive_weight=1.4, social_weight=1.4, initial_temperature=100, cooling_rate=0.95):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = num_particles\n        self.max_iterations = max_iterations\n        self.inertia_weight = inertia_weight\n        self.cognitive_weight = cognitive_weight\n        self.social_weight = social_weight\n        self.initial_temperature = initial_temperature\n        self.cooling_rate = cooling_rate\n\n    def __call__(self, func):\n        def pso_sa_helper():\n            # PSO initialization\n            # Dynamic adjustment of inertia weight based on exploration-exploitation balance\n            self.inertia_weight = np.random.uniform(0.4, 0.9)  # Update the inertia weight dynamically\n            # PSO optimization loop\n            # SA initialization\n            # SA optimization loop\n\n        return pso_sa_helper()", "name": "PSO_SA_Optimizer", "description": "Introducing dynamic adjustment of inertia weight in PSO to enhance exploration-exploitation balance and convergence speed.", "configspace": "", "generation": 28, "fitness": 0.0, "feedback": "The algorithm PSO_SA_Optimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.00 with standard deviation 0.00.", "error": "", "parent_id": "de7234b3-9411-4ce1-be40-4d8b13477c6e", "metadata": {"aucs": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, "mutation_prompt": null}
{"id": "245b5828-c55b-4714-89ca-d90c8586b73b", "solution": "import numpy as np\n\nclass PSO_SA_Optimizer:\n    def __init__(self, budget, dim, num_particles=30, max_iterations=1000, inertia_weight=0.7, cognitive_weight=1.4, social_weight=1.4, initial_temperature=100, cooling_rate=0.95):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = num_particles\n        self.max_iterations = max_iterations\n        self.inertia_weight = inertia_weight\n        self.cognitive_weight = cognitive_weight\n        self.social_weight = social_weight\n        self.initial_temperature = initial_temperature\n        self.cooling_rate = cooling_rate\n\n    def __call__(self, func):\n        def pso_sa_helper():\n            # PSO initialization\n            # Dynamic adjustment of inertia weight based on exploration-exploitation balance\n            self.inertia_weight = np.random.uniform(0.4, 0.9)  # Update the inertia weight dynamically\n            # PSO optimization loop\n            # SA initialization\n            # SA optimization loop\n\n        return pso_sa_helper()", "name": "PSO_SA_Optimizer", "description": "Introducing dynamic adjustment of inertia weight in PSO to enhance exploration-exploitation balance and convergence speed.", "configspace": "", "generation": 29, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "aa1e84cc-0e18-4b11-aa3f-10c0a6eff3fa", "metadata": {"aucs": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, "mutation_prompt": null}
{"id": "66520ac7-8043-46bc-8b83-a0935af4eff5", "solution": "import numpy as np\n\nclass PSO_SA_Optimizer:\n    def __init__(self, budget, dim, num_particles=30, max_iterations=1000, inertia_weight=0.7, cognitive_weight=1.4, social_weight=1.4, initial_temperature=100, cooling_rate=0.95):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = num_particles\n        self.max_iterations = max_iterations\n        self.inertia_weight = inertia_weight\n        self.cognitive_weight = cognitive_weight\n        self.social_weight = social_weight\n        self.initial_temperature = initial_temperature\n        self.cooling_rate = cooling_rate\n\n    def __call__(self, func):\n        def pso_sa_helper():\n            # PSO initialization\n            # Dynamic adjustment of inertia weight based on exploration-exploitation balance\n            self.inertia_weight = np.random.uniform(0.4, 0.9)  # Update the inertia weight dynamically\n            # PSO optimization loop\n            # SA initialization\n            # SA optimization loop\n\n        return pso_sa_helper()", "name": "PSO_SA_Optimizer", "description": "Introducing dynamic adjustment of inertia weight in PSO to enhance exploration-exploitation balance and convergence speed.", "configspace": "", "generation": 29, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "aa1e84cc-0e18-4b11-aa3f-10c0a6eff3fa", "metadata": {"aucs": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, "mutation_prompt": null}
{"id": "128b1d2d-a7ce-46d9-8c39-3b4cb20c672b", "solution": "import numpy as np\n\nclass PSO_SA_Optimizer:\n    def __init__(self, budget, dim, num_particles=30, max_iterations=1000, inertia_weight=0.7, cognitive_weight=1.4, social_weight=1.4, initial_temperature=100, cooling_rate=0.95):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = num_particles\n        self.max_iterations = max_iterations\n        self.inertia_weight = inertia_weight\n        self.cognitive_weight = cognitive_weight\n        self.social_weight = social_weight\n        self.initial_temperature = initial_temperature\n        self.cooling_rate = cooling_rate\n\n    def __call__(self, func):\n        def pso_sa_helper():\n            # PSO initialization\n            # Dynamic adjustment of inertia weight based on exploration-exploitation balance\n            self.inertia_weight = np.random.uniform(0.4, 0.9)  # Update the inertia weight dynamically\n            # PSO optimization loop\n            # SA initialization\n            # SA optimization loop\n\n        return pso_sa_helper()", "name": "PSO_SA_Optimizer", "description": "Introducing dynamic adjustment of inertia weight in PSO to enhance exploration-exploitation balance and convergence speed.", "configspace": "", "generation": 29, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "aa1e84cc-0e18-4b11-aa3f-10c0a6eff3fa", "metadata": {"aucs": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, "mutation_prompt": null}
{"id": "a32efc40-d98c-486e-96a3-32ba35669d11", "solution": "import numpy as np\n\nclass PSO_SA_Optimizer:\n    def __init__(self, budget, dim, num_particles=30, max_iterations=1000, inertia_weight=0.7, cognitive_weight=1.4, social_weight=1.4, initial_temperature=100, cooling_rate=0.95):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = num_particles\n        self.max_iterations = max_iterations\n        self.inertia_weight = inertia_weight\n        self.cognitive_weight = cognitive_weight\n        self.social_weight = social_weight\n        self.initial_temperature = initial_temperature\n        self.cooling_rate = cooling_rate\n\n    def __call__(self, func):\n        def pso_sa_helper():\n            # PSO initialization\n            # Dynamic adjustment of inertia weight based on exploration-exploitation balance\n            self.inertia_weight = np.random.uniform(0.4, 0.9)  # Update the inertia weight dynamically\n            # PSO optimization loop\n            # SA initialization\n            # SA optimization loop\n\n        return pso_sa_helper()", "name": "PSO_SA_Optimizer", "description": "Introducing dynamic adjustment of inertia weight in PSO to enhance exploration-exploitation balance and convergence speed.", "configspace": "", "generation": 29, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "aa1e84cc-0e18-4b11-aa3f-10c0a6eff3fa", "metadata": {"aucs": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, "mutation_prompt": null}
{"id": "5a44b73b-1a75-4e65-8b7e-26ec6d85c062", "solution": "import numpy as np\n\nclass PSO_SA_Optimizer:\n    def __init__(self, budget, dim, num_particles=30, max_iterations=1000, inertia_weight=0.7, cognitive_weight=1.4, social_weight=1.4, initial_temperature=100, cooling_rate=0.95):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = num_particles\n        self.max_iterations = max_iterations\n        self.inertia_weight = inertia_weight\n        self.cognitive_weight = cognitive_weight\n        self.social_weight = social_weight\n        self.initial_temperature = initial_temperature\n        self.cooling_rate = cooling_rate\n\n    def __call__(self, func):\n        def pso_sa_helper():\n            # PSO initialization\n            # Dynamic adjustment of inertia weight based on exploration-exploitation balance\n            self.inertia_weight = np.random.uniform(0.4, 0.9)  # Update the inertia weight dynamically\n            # PSO optimization loop\n            # SA initialization\n            # SA optimization loop\n\n        return pso_sa_helper()", "name": "PSO_SA_Optimizer", "description": "Introducing dynamic adjustment of inertia weight in PSO to enhance exploration-exploitation balance and convergence speed.", "configspace": "", "generation": 29, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "aa1e84cc-0e18-4b11-aa3f-10c0a6eff3fa", "metadata": {"aucs": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, "mutation_prompt": null}
{"id": "f13fcb6d-8120-4cc4-8c33-20214f2aaf20", "solution": "import numpy as np\n\nclass PSO_SA_Optimizer:\n    def __init__(self, budget, dim, num_particles=30, max_iterations=1000, inertia_weight=0.7, cognitive_weight=1.4, social_weight=1.4, initial_temperature=100, cooling_rate=0.95):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = num_particles\n        self.max_iterations = max_iterations\n        self.inertia_weight = inertia_weight\n        self.cognitive_weight = cognitive_weight\n        self.social_weight = social_weight\n        self.initial_temperature = initial_temperature\n        self.cooling_rate = cooling_rate\n\n    def __call__(self, func):\n        def pso_sa_helper():\n            # PSO initialization\n            # Dynamic adjustment of inertia weight based on exploration-exploitation balance\n            self.inertia_weight = np.random.uniform(0.4, 0.9)  # Update the inertia weight dynamically\n            # PSO optimization loop\n            # SA initialization\n            # SA optimization loop\n\n        return pso_sa_helper()", "name": "PSO_SA_Optimizer", "description": "Introducing dynamic adjustment of inertia weight in PSO to enhance exploration-exploitation balance and convergence speed.", "configspace": "", "generation": 29, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "aa1e84cc-0e18-4b11-aa3f-10c0a6eff3fa", "metadata": {"aucs": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, "mutation_prompt": null}
{"id": "23190d3f-39e8-4498-aa9c-1d04319173a3", "solution": "import numpy as np\n\nclass PSO_SA_Optimizer:\n    def __init__(self, budget, dim, num_particles=30, max_iterations=1000, inertia_weight=0.7, cognitive_weight=1.4, social_weight=1.4, initial_temperature=100, cooling_rate=0.95):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = num_particles\n        self.max_iterations = max_iterations\n        self.inertia_weight = inertia_weight\n        self.cognitive_weight = cognitive_weight\n        self.social_weight = social_weight\n        self.initial_temperature = initial_temperature\n        self.cooling_rate = cooling_rate\n\n    def __call__(self, func):\n        def pso_sa_helper():\n            # PSO initialization\n            # Dynamic adjustment of inertia weight based on exploration-exploitation balance\n            self.inertia_weight = np.random.uniform(0.4, 0.9)  # Update the inertia weight dynamically\n            # PSO optimization loop\n            # SA initialization\n            # SA optimization loop\n\n        return pso_sa_helper()", "name": "PSO_SA_Optimizer", "description": "Introducing dynamic adjustment of inertia weight in PSO to enhance exploration-exploitation balance and convergence speed.", "configspace": "", "generation": 29, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "aa1e84cc-0e18-4b11-aa3f-10c0a6eff3fa", "metadata": {"aucs": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, "mutation_prompt": null}
{"id": "eac9865a-fa38-47da-9bb0-bc9b8c3b49cd", "solution": "import numpy as np\n\nclass Hybrid_PSO_SA_Optimizer:\n    def __init__(self, budget, dim, num_particles=30, max_iterations=1000, inertia_weight=0.7, cognitive_weight=1.4, social_weight=1.4, initial_temperature=100, cooling_rate=0.95):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = num_particles\n        self.max_iterations = max_iterations\n        self.inertia_weight = inertia_weight\n        self.cognitive_weight = cognitive_weight\n        self.social_weight = social_weight\n        self.initial_temperature = initial_temperature\n        self.cooling_rate = cooling_rate\n\n    def __call__(self, func):\n        def hybrid_pso_sa_helper():\n            # PSO initialization\n            # Dynamic adjustment of inertia weight based on exploration-exploitation balance\n            self.inertia_weight = np.random.uniform(0.4, 0.9)  # Update the inertia weight dynamically\n            # PSO optimization loop\n            # SA initialization\n            # SA optimization loop\n\n        return hybrid_pso_sa_helper()", "name": "Hybrid_PSO_SA_Optimizer", "description": "Introducing a Novel Hybrid Metaheuristic Algorithm combining Particle Swarm Optimization (PSO) with Simulated Annealing (SA) for enhanced global exploration and exploitation in black box optimization.", "configspace": "", "generation": 36, "fitness": 0.0, "feedback": "The algorithm Hybrid_PSO_SA_Optimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.00 with standard deviation 0.00.", "error": "", "parent_id": "aa1e84cc-0e18-4b11-aa3f-10c0a6eff3fa", "metadata": {"aucs": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, "mutation_prompt": null}
{"id": "894a8509-8731-4be1-bae0-a7121706d84a", "solution": "import numpy as np\n\nclass PSO_SA_Optimizer:\n    def __init__(self, budget, dim, num_particles=30, max_iterations=1000, inertia_weight=0.7, cognitive_weight=1.4, social_weight=1.4, initial_temperature=100, cooling_rate=0.95):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = num_particles\n        self.max_iterations = max_iterations\n        self.inertia_weight = inertia_weight\n        self.cognitive_weight = cognitive_weight\n        self.social_weight = social_weight\n        self.initial_temperature = initial_temperature\n        self.cooling_rate = cooling_rate\n\n    def __call__(self, func):\n        def pso_sa_helper():\n            # PSO initialization\n            # Dynamic adjustment of inertia weight based on exploration-exploitation balance\n            self.inertia_weight = np.random.uniform(0.4, 0.9)  # Update the inertia weight dynamically\n            # PSO optimization loop\n            # SA initialization\n            # SA optimization loop\n\n        return pso_sa_helper()", "name": "PSO_SA_Optimizer", "description": "Introducing dynamic adjustment of inertia weight in PSO to enhance exploration-exploitation balance and convergence speed.", "configspace": "", "generation": 29, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "aa1e84cc-0e18-4b11-aa3f-10c0a6eff3fa", "metadata": {"aucs": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, "mutation_prompt": null}
{"id": "7ae952de-6cb2-4f3a-bd0b-5c702b2f58ba", "solution": "import numpy as np\n\nclass PSO_SA_Optimizer:\n    def __init__(self, budget, dim, num_particles=30, max_iterations=1000, inertia_weight=0.7, cognitive_weight=1.4, social_weight=1.4, initial_temperature=100, cooling_rate=0.95):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = num_particles\n        self.max_iterations = max_iterations\n        self.inertia_weight = inertia_weight\n        self.cognitive_weight = cognitive_weight\n        self.social_weight = social_weight\n        self.initial_temperature = initial_temperature\n        self.cooling_rate = cooling_rate\n\n    def __call__(self, func):\n        def pso_sa_helper():\n            # PSO initialization\n            # Dynamic adjustment of inertia weight based on exploration-exploitation balance\n            self.inertia_weight = np.random.uniform(0.4, 0.9)  # Update the inertia weight dynamically\n            # PSO optimization loop\n            # SA initialization\n            # SA optimization loop\n\n        return pso_sa_helper()", "name": "PSO_SA_Optimizer", "description": "Introducing dynamic adjustment of inertia weight in PSO to enhance exploration-exploitation balance and convergence speed.", "configspace": "", "generation": 29, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "aa1e84cc-0e18-4b11-aa3f-10c0a6eff3fa", "metadata": {"aucs": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, "mutation_prompt": null}
{"id": "f525f83c-6d48-4de3-9242-d640c10febf5", "solution": "import numpy as np\n\nclass PSO_SA_Optimizer:\n    def __init__(self, budget, dim, num_particles=30, max_iterations=1000, inertia_weight=0.7, cognitive_weight=1.4, social_weight=1.4, initial_temperature=100, cooling_rate=0.95):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = num_particles\n        self.max_iterations = max_iterations\n        self.inertia_weight = inertia_weight\n        self.cognitive_weight = cognitive_weight\n        self.social_weight = social_weight\n        self.initial_temperature = initial_temperature\n        self.cooling_rate = cooling_rate\n\n    def __call__(self, func):\n        def pso_sa_helper():\n            # PSO initialization\n            # Dynamic adjustment of inertia weight based on exploration-exploitation balance\n            self.inertia_weight = np.random.uniform(0.4, 0.9)  # Update the inertia weight dynamically\n            # PSO optimization loop\n            # SA initialization\n            # SA optimization loop\n\n        return pso_sa_helper()", "name": "PSO_SA_Optimizer", "description": "Introducing dynamic adjustment of inertia weight in PSO to enhance exploration-exploitation balance and convergence speed.", "configspace": "", "generation": 29, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "aa1e84cc-0e18-4b11-aa3f-10c0a6eff3fa", "metadata": {"aucs": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, "mutation_prompt": null}
{"id": "69a8fa35-434f-4608-9761-216b0f88e8ed", "solution": "import numpy as np\n\nclass PSO_SA_Optimizer:\n    def __init__(self, budget, dim, num_particles=30, max_iterations=1000, inertia_weight=0.7, cognitive_weight=1.4, social_weight=1.4, initial_temperature=100, cooling_rate=0.95):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = num_particles\n        self.max_iterations = max_iterations\n        self.inertia_weight = inertia_weight\n        self.cognitive_weight = cognitive_weight\n        self.social_weight = social_weight\n        self.initial_temperature = initial_temperature\n        self.cooling_rate = cooling_rate\n\n    def __call__(self, func):\n        def pso_sa_helper():\n            # PSO initialization\n            # Dynamic adjustment of inertia weight based on exploration-exploitation balance\n            self.inertia_weight = np.clip(self.inertia_weight + np.random.normal(0, 0.1), 0.4, 0.9)  # Update the inertia weight dynamically\n            # PSO optimization loop\n            # SA initialization\n            # SA optimization loop\n\n        return pso_sa_helper()", "name": "PSO_SA_Optimizer", "description": "Enhancing PSO with Simulated Annealing for improved convergence and exploration-exploitation balance.", "configspace": "", "generation": 40, "fitness": 0.0, "feedback": "The algorithm PSO_SA_Optimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.00 with standard deviation 0.00.", "error": "", "parent_id": "aa1e84cc-0e18-4b11-aa3f-10c0a6eff3fa", "metadata": {"aucs": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, "mutation_prompt": null}
{"id": "4279ec60-3060-479f-afb5-c3d0cc54ad60", "solution": "import numpy as np\n\nclass PSO_SA_Optimizer:\n    def __init__(self, budget, dim, num_particles=30, max_iterations=1000, inertia_weight=0.7, cognitive_weight=1.4, social_weight=1.4, initial_temperature=100, cooling_rate=0.95):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = num_particles\n        self.max_iterations = max_iterations\n        self.inertia_weight = inertia_weight\n        self.cognitive_weight = cognitive_weight\n        self.social_weight = social_weight\n        self.initial_temperature = initial_temperature\n        self.cooling_rate = cooling_rate\n\n    def __call__(self, func):\n        def pso_sa_helper():\n            # PSO initialization\n            # Dynamic adjustment of inertia weight based on exploration-exploitation balance\n            self.inertia_weight = np.random.uniform(0.4, 0.9)  # Update the inertia weight dynamically\n            # PSO optimization loop\n            # SA initialization\n            # SA optimization loop\n\n        return pso_sa_helper()", "name": "PSO_SA_Optimizer", "description": "Introducing dynamic adjustment of inertia weight in PSO to enhance exploration-exploitation balance and convergence speed.", "configspace": "", "generation": 29, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "aa1e84cc-0e18-4b11-aa3f-10c0a6eff3fa", "metadata": {"aucs": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, "mutation_prompt": null}
{"id": "8bc91454-b43e-4de2-9cfe-c9fcb348c083", "solution": "import numpy as np\n\nclass PSO_SA_Optimizer:\n    def __init__(self, budget, dim, num_particles=30, max_iterations=1000, inertia_weight=0.7, cognitive_weight=1.4, social_weight=1.4, initial_temperature=100, cooling_rate=0.95):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = num_particles\n        self.max_iterations = max_iterations\n        self.inertia_weight = inertia_weight\n        self.cognitive_weight = cognitive_weight\n        self.social_weight = social_weight\n        self.initial_temperature = initial_temperature\n        self.cooling_rate = cooling_rate\n\n    def __call__(self, func):\n        def pso_sa_helper():\n            # PSO initialization\n            # Dynamic adjustment of inertia weight based on exploration-exploitation balance\n            self.inertia_weight = np.random.uniform(0.4, 0.9)  # Update the inertia weight dynamically\n            # PSO optimization loop\n            # SA initialization\n            # SA optimization loop\n\n        return pso_sa_helper()", "name": "PSO_SA_Optimizer", "description": "Introducing dynamic adjustment of inertia weight in PSO to enhance exploration-exploitation balance and convergence speed.", "configspace": "", "generation": 29, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "aa1e84cc-0e18-4b11-aa3f-10c0a6eff3fa", "metadata": {"aucs": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, "mutation_prompt": null}
{"id": "b271067b-d045-4489-ad66-d40ad4aabd78", "solution": "import numpy as np\n\nclass PSO_SA_Optimizer_Enhanced:\n    def __init__(self, budget, dim, num_particles=30, max_iterations=1000, inertia_weight=0.7, cognitive_weight=1.4, social_weight=1.4, initial_temperature=100, cooling_rate=0.95):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = num_particles\n        self.max_iterations = max_iterations\n        self.inertia_weight = inertia_weight\n        self.cognitive_weight = cognitive_weight\n        self.social_weight = social_weight\n        self.initial_temperature = initial_temperature\n        self.cooling_rate = cooling_rate\n\n    def __call__(self, func):\n        def pso_sa_helper_enhanced():\n            # PSO initialization\n            # Dynamic adjustment of inertia weight based on exploration-exploitation balance\n            self.inertia_weight = np.random.uniform(0.4, 0.9)  # Update the inertia weight dynamically\n            # Enhanced PSO optimization loop with integrated Simulated Annealing\n            # SA initialization\n            # SA optimization loop\n\n        return pso_sa_helper_enhanced()", "name": "PSO_SA_Optimizer_Enhanced", "description": "Enhancing PSO with Simulated Annealing for improved global exploration and exploitation in optimization.", "configspace": "", "generation": 43, "fitness": 0.0, "feedback": "The algorithm PSO_SA_Optimizer_Enhanced got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.00 with standard deviation 0.00.", "error": "", "parent_id": "aa1e84cc-0e18-4b11-aa3f-10c0a6eff3fa", "metadata": {"aucs": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, "mutation_prompt": null}
{"id": "e9c23f3a-7995-4743-b1bf-a26f72e8e12a", "solution": "import numpy as np\n\nclass PSO_SA_Optimizer:\n    def __init__(self, budget, dim, num_particles=30, max_iterations=1000, inertia_weight=0.7, cognitive_weight=1.4, social_weight=1.4, initial_temperature=100, cooling_rate=0.95):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = num_particles\n        self.max_iterations = max_iterations\n        self.inertia_weight = inertia_weight\n        self.cognitive_weight = cognitive_weight\n        self.social_weight = social_weight\n        self.initial_temperature = initial_temperature\n        self.cooling_rate = cooling_rate\n\n    def __call__(self, func):\n        def pso_sa_helper():\n            # PSO initialization\n            # Dynamic adjustment of inertia weight based on exploration-exploitation balance\n            self.inertia_weight = np.random.uniform(0.4, 0.9)  # Update the inertia weight dynamically\n            # PSO optimization loop\n            # SA initialization\n            # SA optimization loop\n\n        return pso_sa_helper()", "name": "PSO_SA_Optimizer", "description": "Introducing dynamic adjustment of inertia weight in PSO to enhance exploration-exploitation balance and convergence speed.", "configspace": "", "generation": 29, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "aa1e84cc-0e18-4b11-aa3f-10c0a6eff3fa", "metadata": {"aucs": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, "mutation_prompt": null}
{"id": "98948ad9-f909-4181-be0e-06c93d899159", "solution": "import numpy as np\n\nclass PSO_SA_Optimizer:\n    def __init__(self, budget, dim, num_particles=30, max_iterations=1000, inertia_weight=0.7, cognitive_weight=1.4, social_weight=1.4, initial_temperature=100, cooling_rate=0.95):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = num_particles\n        self.max_iterations = max_iterations\n        self.inertia_weight = inertia_weight\n        self.cognitive_weight = cognitive_weight\n        self.social_weight = social_weight\n        self.initial_temperature = initial_temperature\n        self.cooling_rate = cooling_rate\n\n    def __call__(self, func):\n        def pso_sa_helper():\n            # PSO initialization\n            # Dynamic adjustment of inertia weight based on exploration-exploitation balance\n            self.inertia_weight = np.random.uniform(0.4, 0.9)  # Update the inertia weight dynamically\n            # PSO optimization loop\n            # SA initialization\n            # SA optimization loop\n\n        return pso_sa_helper()", "name": "PSO_SA_Optimizer", "description": "Introducing dynamic adjustment of inertia weight in PSO to enhance exploration-exploitation balance and convergence speed.", "configspace": "", "generation": 29, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "aa1e84cc-0e18-4b11-aa3f-10c0a6eff3fa", "metadata": {"aucs": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, "mutation_prompt": null}
{"id": "4e68572b-b18d-43b1-a8dc-1c2069a75d02", "solution": "import numpy as np\n\nclass PSO_SA_Optimizer:\n    def __init__(self, budget, dim, num_particles=30, max_iterations=1000, inertia_weight=0.7, cognitive_weight=1.4, social_weight=1.4, initial_temperature=100, cooling_rate=0.95):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = num_particles\n        self.max_iterations = max_iterations\n        self.inertia_weight = inertia_weight\n        self.cognitive_weight = cognitive_weight\n        self.social_weight = social_weight\n        self.initial_temperature = initial_temperature\n        self.cooling_rate = cooling_rate\n\n    def __call__(self, func):\n        def pso_sa_helper():\n            # PSO initialization\n            # Dynamic adjustment of inertia weight based on exploration-exploitation balance\n            self.inertia_weight = np.random.uniform(0.4, 0.9)  # Update the inertia weight dynamically\n            # PSO optimization loop\n            # SA initialization\n            # SA optimization loop\n\n        return pso_sa_helper()", "name": "PSO_SA_Optimizer", "description": "Introducing dynamic adjustment of inertia weight in PSO to enhance exploration-exploitation balance and convergence speed.", "configspace": "", "generation": 29, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "aa1e84cc-0e18-4b11-aa3f-10c0a6eff3fa", "metadata": {"aucs": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, "mutation_prompt": null}
{"id": "87896ef4-2630-432a-8100-a690151a1dd4", "solution": "import numpy as np\n\nclass PSO_SA_Optimizer:\n    def __init__(self, budget, dim, num_particles=30, max_iterations=1000, inertia_weight=0.7, cognitive_weight=1.4, social_weight=1.4, initial_temperature=100, cooling_rate=0.95):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = num_particles\n        self.max_iterations = max_iterations\n        self.inertia_weight = inertia_weight\n        self.cognitive_weight = cognitive_weight\n        self.social_weight = social_weight\n        self.initial_temperature = initial_temperature\n        self.cooling_rate = cooling_rate\n\n    def __call__(self, func):\n        def pso_sa_helper():\n            # PSO initialization\n            # Dynamic adjustment of inertia weight based on exploration-exploitation balance\n            self.inertia_weight = np.random.uniform(0.4, 0.9)  # Update the inertia weight dynamically\n            # PSO optimization loop\n            # Simulated Annealing (SA) initialization\n            current_solution = np.random.uniform(-5.0, 5.0, size=self.dim)\n            current_cost = func(current_solution)\n            best_solution = current_solution\n            best_cost = current_cost\n            # SA optimization loop\n            for _ in range(self.budget):\n                proposal_solution = current_solution + np.random.normal(0, 0.1, size=self.dim)\n                proposal_solution = np.clip(proposal_solution, -5.0, 5.0)\n                proposal_cost = func(proposal_solution)\n                if proposal_cost < current_cost or np.random.rand() < np.exp((current_cost - proposal_cost) / self.initial_temperature):\n                    current_solution = proposal_solution\n                    current_cost = proposal_cost\n                    if proposal_cost < best_cost:\n                        best_solution = proposal_solution\n                        best_cost = proposal_cost\n            return best_solution\n\n        return pso_sa_helper()", "name": "PSO_SA_Optimizer", "description": "Enhancing PSO with Simulated Annealing to improve exploration and exploitation balance for efficient optimization.", "configspace": "", "generation": 47, "fitness": 0.0906141788161689, "feedback": "The algorithm PSO_SA_Optimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09 with standard deviation 0.06.", "error": "", "parent_id": "aa1e84cc-0e18-4b11-aa3f-10c0a6eff3fa", "metadata": {"aucs": [0.1109206758385699, 0.1520753981016193, 0.12301304069111796, 0.04348555101520646, 0.0511593819118803, 0.11057429130455021, 0.12624342276343237, 0.08612291454307486, 0.20034429572616808, 0.08571421446881289, 0.002677960175269667, 0.0296730650429865, 0.05451558787234445, 9.999999999998899e-05, 0.025846221896836497, 0.04309102206703541, 9.999999999998899e-05, 0.02899715333905073, 0.02919438733851276, 0.04178404375036093, 0.007114481350629087, 0.059881586604391135, 0.07162612007153535, 0.048319473842314276, 0.015958929537915, 0.03137447003760829, 0.048913505453951456, 0.042790470606617226, 0.01470235077673221, 0.02447667220900096, 0.03927210843288087, 0.039092281296777776, 0.032457441038807655, 0.03361112606879435, 0.0779732385455949, 0.033562237277637186, 0.0379633743858625, 0.017959373283673807, 0.01900288351448509, 0.003250803622391274, 0.03394185641735603, 0.05451370037206671, 0.022181527388416944, 0.015100705922057589, 0.07951188617029525, 0.08283253067843743, 0.09838258822728896, 0.08178442998960822, 0.06164480538889472, 0.08809331064683412, 0.047001639280928376, 0.11502090264788745, 0.2025202329768665, 0.10370320964593793, 0.10992068632886642, 0.09127572130130535, 0.07022146388896144, 0.14325736874472061, 0.10196185704012695, 0.0897438605310612, 0.1082203409163014, 0.1612751354133405, 0.17580267743096833, 0.15501527072952082, 0.12963200354626847, 0.1206821081914684, 0.0992343447749039, 0.10992812534855545, 0.13079468940576222, 0.1108226197685086, 0.13788908713078452, 0.1401329619869075, 0.1369742340665423, 0.1303191413576471, 0.15889501157778396, 0.11156410529900551, 0.13595303914458978, 0.15933658299728104, 0.12642266361483978, 0.12022522835581329, 0.12589585592803365, 0.14869086718791347, 0.08064021761645634, 0.04704232657583196, 0.014168646933434004, 0.0318310098450163, 0.038139907104323356, 0.03680419133551005, 0.06427327868091104, 0.04852348645254645, 0.05284964994355745, 0.10681274691330289, 0.0813149565825364, 0.09435350654355446, 0.03697696249403215, 0.038788035049615255, 0.08300259487023065, 0.0709945530397923, 0.06644422266788241, 0.0154848804604657, 9.999999999998899e-05, 9.999999999998899e-05, 0.007529376153612044, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.004753524068185522, 0.035508756705108446, 0.023170770259052786, 0.0257034379663863, 0.02654540354440882, 0.015012088603499119, 0.030920744644118203, 0.03952806500164663, 0.055819707336851154, 0.08789091862708942, 0.14866990685599046, 0.12864898440764938, 0.08700971304363192, 0.10610893813880029, 0.07460975609194664, 0.15353572027709816, 0.1392508034102018, 0.1146688659868832, 0.02733793900660675, 0.06500551107494901, 0.03968381777648411, 0.04662535003860502, 0.0305807356126665, 0.038736267830364546, 0.051092538289376765, 0.022063903471743473, 0.04699558031917639, 0.17430559712214022, 0.14371516083960323, 0.11736252132534686, 0.13747013262964392, 0.1536497678561689, 0.11526030268002596, 0.1571485249749106, 0.1541867093355067, 0.14497734269037155, 0.14476105747791257, 0.1421143386030721, 0.13198877494988415, 0.16791732132418113, 0.09468299888597342, 0.10804718963186566, 0.15620764846450041, 0.11376219289478973, 0.10397121856090419, 0.08541061877743672, 0.12870508701304895, 0.04995812248419229, 0.09505337927204505, 0.10030957108758465, 0.09628517332224074, 0.08826799866095258, 0.06607715797930591, 0.09485341576115303, 0.16825939207626073, 0.12812771642167897, 0.14157764458846978, 0.17515304810423238, 0.15097193798261743, 0.15672022724147516, 0.17241220063627627, 0.13753453981528652, 0.13280486208822162, 0.18501495030970705, 0.17280906034872245, 0.19091866981125372, 0.17951612833893738, 0.18246250006531928, 0.1830961908269484, 0.18277216836925714, 0.2360300114733621, 0.17875142780717812, 0.131739083604426, 0.09540896515055997, 0.08346273859438602, 0.12747998211303424, 0.11261756677182932, 0.11805722401323548, 0.09370473733806906, 0.14727905323099788, 0.11837160926786039, 0.0706958024295603, 0.07769707310800145, 0.09317244322923923, 0.14472629253469604, 0.06380283717879465, 0.12508987977317754, 0.06990259016727818, 0.09130243015608797, 0.04859044901019116, 0.1801532758955795, 0.19727048764549104, 0.18890798155942223, 0.18349668917613582, 0.19616852667788487, 0.18595338125752192, 0.19456740222988622, 0.18204855410716192, 0.17826379439861162, 0.0520602473227626, 0.04582803371078703, 0.04644504026836116, 0.05205179137460869, 0.033026384259138064, 0.030501711465839576, 0.06134740524524718, 0.03968994993732233, 0.03699097796310957]}, "mutation_prompt": null}
{"id": "96546119-dce5-4018-8842-e963f1c9f7fe", "solution": "import numpy as np\n\nclass HybridPSO_SA_Optimizer:\n    def __init__(self, budget, dim, num_particles=30, max_iterations=1000, inertia_weight=0.7, cognitive_weight=1.4, social_weight=1.4, initial_temperature=100, cooling_rate=0.95):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = num_particles\n        self.max_iterations = max_iterations\n        self.inertia_weight = inertia_weight\n        self.cognitive_weight = cognitive_weight\n        self.social_weight = social_weight\n        self.initial_temperature = initial_temperature\n        self.cooling_rate = cooling_rate\n\n    def __call__(self, func):\n        def hybrid_pso_sa_helper():\n            # Hybrid PSO and SA optimization\n            # PSO initialization\n            self.inertia_weight = np.random.uniform(0.4, 0.9)  # Update the inertia weight dynamically\n            # Main optimization loop\n            current_solution = np.random.uniform(-5.0, 5.0, size=self.dim)\n            current_cost = func(current_solution)\n            best_solution = current_solution\n            best_cost = current_cost\n            # SA optimization loop\n            for _ in range(self.budget):\n                proposal_solution = current_solution + np.random.normal(0, 0.1, size=self.dim)\n                proposal_solution = np.clip(proposal_solution, -5.0, 5.0)\n                proposal_cost = func(proposal_solution)\n                if proposal_cost < current_cost or np.random.rand() < np.exp((current_cost - proposal_cost) / self.initial_temperature):\n                    current_solution = proposal_solution\n                    current_cost = proposal_cost\n                    if proposal_cost < best_cost:\n                        best_solution = proposal_solution\n                        best_cost = proposal_cost\n            return best_solution\n\n        return hybrid_pso_sa_helper()", "name": "HybridPSO_SA_Optimizer", "description": "Hybridizing PSO and Simulated Annealing with dynamic inertia weight adjustment for efficient optimization in a continuous search space.", "configspace": "", "generation": 48, "fitness": 0.0906141788161689, "feedback": "The algorithm HybridPSO_SA_Optimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09 with standard deviation 0.06.", "error": "", "parent_id": "87896ef4-2630-432a-8100-a690151a1dd4", "metadata": {"aucs": [0.1109206758385699, 0.1520753981016193, 0.12301304069111796, 0.04348555101520646, 0.0511593819118803, 0.11057429130455021, 0.12624342276343237, 0.08612291454307486, 0.20034429572616808, 0.08571421446881289, 0.002677960175269667, 0.0296730650429865, 0.05451558787234445, 9.999999999998899e-05, 0.025846221896836497, 0.04309102206703541, 9.999999999998899e-05, 0.02899715333905073, 0.02919438733851276, 0.04178404375036093, 0.007114481350629087, 0.059881586604391135, 0.07162612007153535, 0.048319473842314276, 0.015958929537915, 0.03137447003760829, 0.048913505453951456, 0.042790470606617226, 0.01470235077673221, 0.02447667220900096, 0.03927210843288087, 0.039092281296777776, 0.032457441038807655, 0.03361112606879435, 0.0779732385455949, 0.033562237277637186, 0.0379633743858625, 0.017959373283673807, 0.01900288351448509, 0.003250803622391274, 0.03394185641735603, 0.05451370037206671, 0.022181527388416944, 0.015100705922057589, 0.07951188617029525, 0.08283253067843743, 0.09838258822728896, 0.08178442998960822, 0.06164480538889472, 0.08809331064683412, 0.047001639280928376, 0.11502090264788745, 0.2025202329768665, 0.10370320964593793, 0.10992068632886642, 0.09127572130130535, 0.07022146388896144, 0.14325736874472061, 0.10196185704012695, 0.0897438605310612, 0.1082203409163014, 0.1612751354133405, 0.17580267743096833, 0.15501527072952082, 0.12963200354626847, 0.1206821081914684, 0.0992343447749039, 0.10992812534855545, 0.13079468940576222, 0.1108226197685086, 0.13788908713078452, 0.1401329619869075, 0.1369742340665423, 0.1303191413576471, 0.15889501157778396, 0.11156410529900551, 0.13595303914458978, 0.15933658299728104, 0.12642266361483978, 0.12022522835581329, 0.12589585592803365, 0.14869086718791347, 0.08064021761645634, 0.04704232657583196, 0.014168646933434004, 0.0318310098450163, 0.038139907104323356, 0.03680419133551005, 0.06427327868091104, 0.04852348645254645, 0.05284964994355745, 0.10681274691330289, 0.0813149565825364, 0.09435350654355446, 0.03697696249403215, 0.038788035049615255, 0.08300259487023065, 0.0709945530397923, 0.06644422266788241, 0.0154848804604657, 9.999999999998899e-05, 9.999999999998899e-05, 0.007529376153612044, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.004753524068185522, 0.035508756705108446, 0.023170770259052786, 0.0257034379663863, 0.02654540354440882, 0.015012088603499119, 0.030920744644118203, 0.03952806500164663, 0.055819707336851154, 0.08789091862708942, 0.14866990685599046, 0.12864898440764938, 0.08700971304363192, 0.10610893813880029, 0.07460975609194664, 0.15353572027709816, 0.1392508034102018, 0.1146688659868832, 0.02733793900660675, 0.06500551107494901, 0.03968381777648411, 0.04662535003860502, 0.0305807356126665, 0.038736267830364546, 0.051092538289376765, 0.022063903471743473, 0.04699558031917639, 0.17430559712214022, 0.14371516083960323, 0.11736252132534686, 0.13747013262964392, 0.1536497678561689, 0.11526030268002596, 0.1571485249749106, 0.1541867093355067, 0.14497734269037155, 0.14476105747791257, 0.1421143386030721, 0.13198877494988415, 0.16791732132418113, 0.09468299888597342, 0.10804718963186566, 0.15620764846450041, 0.11376219289478973, 0.10397121856090419, 0.08541061877743672, 0.12870508701304895, 0.04995812248419229, 0.09505337927204505, 0.10030957108758465, 0.09628517332224074, 0.08826799866095258, 0.06607715797930591, 0.09485341576115303, 0.16825939207626073, 0.12812771642167897, 0.14157764458846978, 0.17515304810423238, 0.15097193798261743, 0.15672022724147516, 0.17241220063627627, 0.13753453981528652, 0.13280486208822162, 0.18501495030970705, 0.17280906034872245, 0.19091866981125372, 0.17951612833893738, 0.18246250006531928, 0.1830961908269484, 0.18277216836925714, 0.2360300114733621, 0.17875142780717812, 0.131739083604426, 0.09540896515055997, 0.08346273859438602, 0.12747998211303424, 0.11261756677182932, 0.11805722401323548, 0.09370473733806906, 0.14727905323099788, 0.11837160926786039, 0.0706958024295603, 0.07769707310800145, 0.09317244322923923, 0.14472629253469604, 0.06380283717879465, 0.12508987977317754, 0.06990259016727818, 0.09130243015608797, 0.04859044901019116, 0.1801532758955795, 0.19727048764549104, 0.18890798155942223, 0.18349668917613582, 0.19616852667788487, 0.18595338125752192, 0.19456740222988622, 0.18204855410716192, 0.17826379439861162, 0.0520602473227626, 0.04582803371078703, 0.04644504026836116, 0.05205179137460869, 0.033026384259138064, 0.030501711465839576, 0.06134740524524718, 0.03968994993732233, 0.03699097796310957]}, "mutation_prompt": null}
{"id": "9628989e-f7d7-458d-bae3-cc482f923178", "solution": "import numpy as np\nfrom scipy.optimize import differential_evolution\n\nclass HybridPSO_SA_DE_Optimizer:\n    def __init__(self, budget, dim, num_particles=30, max_iterations=1000, inertia_weight=0.7, cognitive_weight=1.4, social_weight=1.4, initial_temperature=100, cooling_rate=0.95):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = num_particles\n        self.max_iterations = max_iterations\n        self.inertia_weight = inertia_weight\n        self.cognitive_weight = cognitive_weight\n        self.social_weight = social_weight\n        self.initial_temperature = initial_temperature\n        self.cooling_rate = cooling_rate\n\n    def __call__(self, func):\n        def hybrid_pso_sa_de_helper():\n            # PSO initialization\n            self.inertia_weight = np.random.uniform(0.4, 0.9)  # Update the inertia weight dynamically\n            # PSO optimization loop\n            # Simulated Annealing (SA) initialization\n            current_solution = np.random.uniform(-5.0, 5.0, size=self.dim)\n            current_cost = func(current_solution)\n            best_solution = current_solution\n            best_cost = current_cost\n            # SA optimization loop\n            for _ in range(self.budget):\n                proposal_solution = current_solution + np.random.normal(0, 0.1, size=self.dim)\n                proposal_solution = np.clip(proposal_solution, -5.0, 5.0)\n                proposal_cost = func(proposal_solution)\n                if proposal_cost < current_cost or np.random.rand() < np.exp((current_cost - proposal_cost) / self.initial_temperature):\n                    current_solution = proposal_solution\n                    current_cost = proposal_cost\n                    if proposal_cost < best_cost:\n                        best_solution = proposal_solution\n                        best_cost = proposal_cost\n            # Differential Evolution (DE) optimization\n            de_bounds = [(-5.0, 5.0)] * self.dim\n            result = differential_evolution(func, de_bounds, maxiter=self.budget)\n            if result.fun < best_cost:\n                return result.x\n            else:\n                return best_solution\n\n        return hybrid_pso_sa_de_helper()", "name": "HybridPSO_SA_DE_Optimizer", "description": "Hybridizing Particle Swarm Optimization (PSO) with Simulated Annealing (SA) and Differential Evolution (DE) for enhanced global exploration and exploitation in black box optimization.", "configspace": "", "generation": 49, "fitness": 0.0906141788161689, "feedback": "The algorithm HybridPSO_SA_DE_Optimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09 with standard deviation 0.06.", "error": "", "parent_id": "87896ef4-2630-432a-8100-a690151a1dd4", "metadata": {"aucs": [0.1109206758385699, 0.1520753981016193, 0.12301304069111796, 0.04348555101520646, 0.0511593819118803, 0.11057429130455021, 0.12624342276343237, 0.08612291454307486, 0.20034429572616808, 0.08571421446881289, 0.002677960175269667, 0.0296730650429865, 0.05451558787234445, 9.999999999998899e-05, 0.025846221896836497, 0.04309102206703541, 9.999999999998899e-05, 0.02899715333905073, 0.02919438733851276, 0.04178404375036093, 0.007114481350629087, 0.059881586604391135, 0.07162612007153535, 0.048319473842314276, 0.015958929537915, 0.03137447003760829, 0.048913505453951456, 0.042790470606617226, 0.01470235077673221, 0.02447667220900096, 0.03927210843288087, 0.039092281296777776, 0.032457441038807655, 0.03361112606879435, 0.0779732385455949, 0.033562237277637186, 0.0379633743858625, 0.017959373283673807, 0.01900288351448509, 0.003250803622391274, 0.03394185641735603, 0.05451370037206671, 0.022181527388416944, 0.015100705922057589, 0.07951188617029525, 0.08283253067843743, 0.09838258822728896, 0.08178442998960822, 0.06164480538889472, 0.08809331064683412, 0.047001639280928376, 0.11502090264788745, 0.2025202329768665, 0.10370320964593793, 0.10992068632886642, 0.09127572130130535, 0.07022146388896144, 0.14325736874472061, 0.10196185704012695, 0.0897438605310612, 0.1082203409163014, 0.1612751354133405, 0.17580267743096833, 0.15501527072952082, 0.12963200354626847, 0.1206821081914684, 0.0992343447749039, 0.10992812534855545, 0.13079468940576222, 0.1108226197685086, 0.13788908713078452, 0.1401329619869075, 0.1369742340665423, 0.1303191413576471, 0.15889501157778396, 0.11156410529900551, 0.13595303914458978, 0.15933658299728104, 0.12642266361483978, 0.12022522835581329, 0.12589585592803365, 0.14869086718791347, 0.08064021761645634, 0.04704232657583196, 0.014168646933434004, 0.0318310098450163, 0.038139907104323356, 0.03680419133551005, 0.06427327868091104, 0.04852348645254645, 0.05284964994355745, 0.10681274691330289, 0.0813149565825364, 0.09435350654355446, 0.03697696249403215, 0.038788035049615255, 0.08300259487023065, 0.0709945530397923, 0.06644422266788241, 0.0154848804604657, 9.999999999998899e-05, 9.999999999998899e-05, 0.007529376153612044, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.004753524068185522, 0.035508756705108446, 0.023170770259052786, 0.0257034379663863, 0.02654540354440882, 0.015012088603499119, 0.030920744644118203, 0.03952806500164663, 0.055819707336851154, 0.08789091862708942, 0.14866990685599046, 0.12864898440764938, 0.08700971304363192, 0.10610893813880029, 0.07460975609194664, 0.15353572027709816, 0.1392508034102018, 0.1146688659868832, 0.02733793900660675, 0.06500551107494901, 0.03968381777648411, 0.04662535003860502, 0.0305807356126665, 0.038736267830364546, 0.051092538289376765, 0.022063903471743473, 0.04699558031917639, 0.17430559712214022, 0.14371516083960323, 0.11736252132534686, 0.13747013262964392, 0.1536497678561689, 0.11526030268002596, 0.1571485249749106, 0.1541867093355067, 0.14497734269037155, 0.14476105747791257, 0.1421143386030721, 0.13198877494988415, 0.16791732132418113, 0.09468299888597342, 0.10804718963186566, 0.15620764846450041, 0.11376219289478973, 0.10397121856090419, 0.08541061877743672, 0.12870508701304895, 0.04995812248419229, 0.09505337927204505, 0.10030957108758465, 0.09628517332224074, 0.08826799866095258, 0.06607715797930591, 0.09485341576115303, 0.16825939207626073, 0.12812771642167897, 0.14157764458846978, 0.17515304810423238, 0.15097193798261743, 0.15672022724147516, 0.17241220063627627, 0.13753453981528652, 0.13280486208822162, 0.18501495030970705, 0.17280906034872245, 0.19091866981125372, 0.17951612833893738, 0.18246250006531928, 0.1830961908269484, 0.18277216836925714, 0.2360300114733621, 0.17875142780717812, 0.131739083604426, 0.09540896515055997, 0.08346273859438602, 0.12747998211303424, 0.11261756677182932, 0.11805722401323548, 0.09370473733806906, 0.14727905323099788, 0.11837160926786039, 0.0706958024295603, 0.07769707310800145, 0.09317244322923923, 0.14472629253469604, 0.06380283717879465, 0.12508987977317754, 0.06990259016727818, 0.09130243015608797, 0.04859044901019116, 0.1801532758955795, 0.19727048764549104, 0.18890798155942223, 0.18349668917613582, 0.19616852667788487, 0.18595338125752192, 0.19456740222988622, 0.18204855410716192, 0.17826379439861162, 0.0520602473227626, 0.04582803371078703, 0.04644504026836116, 0.05205179137460869, 0.033026384259138064, 0.030501711465839576, 0.06134740524524718, 0.03968994993732233, 0.03699097796310957]}, "mutation_prompt": null}
{"id": "c87f4fe3-a08d-49bb-9a74-d75ef0f19c0d", "solution": "import numpy as np\n\nclass PSO_SA_Optimizer:\n    def __init__(self, budget, dim, num_particles=30, max_iterations=1000, inertia_weight=0.7, cognitive_weight=1.4, social_weight=1.4, initial_temperature=100, cooling_rate=0.95):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = num_particles\n        self.max_iterations = max_iterations\n        self.inertia_weight = inertia_weight\n        self.cognitive_weight = cognitive_weight\n        self.social_weight = social_weight\n        self.initial_temperature = initial_temperature\n        self.cooling_rate = cooling_rate\n\n    def __call__(self, func):\n        def pso_sa_helper():\n            # PSO initialization\n            # Dynamic adjustment of inertia weight based on exploration-exploitation balance\n            self.inertia_weight = np.random.uniform(0.4, 0.9)  # Update the inertia weight dynamically\n            # PSO optimization loop\n            # Simulated Annealing (SA) initialization\n            current_solution = np.random.uniform(-5.0, 5.0, size=self.dim)\n            current_cost = func(current_solution)\n            best_solution = current_solution\n            best_cost = current_cost\n            # SA optimization loop\n            for _ in range(self.budget):\n                proposal_solution = current_solution + np.random.normal(0, 0.1, size=self.dim)\n                proposal_solution = np.clip(proposal_solution, -5.0, 5.0)\n                proposal_cost = func(proposal_solution)\n                if proposal_cost < current_cost or np.random.rand() < np.exp((current_cost - proposal_cost) / self.initial_temperature):\n                    current_solution = proposal_solution\n                    current_cost = proposal_cost\n                    if proposal_cost < best_cost:\n                        best_solution = proposal_solution\n                        best_cost = proposal_cost\n            return best_solution\n\n        return pso_sa_helper()", "name": "PSO_SA_Optimizer", "description": "Enhancing PSO with Simulated Annealing to improve exploration and exploitation balance for efficient optimization.", "configspace": "", "generation": 48, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "87896ef4-2630-432a-8100-a690151a1dd4", "metadata": {"aucs": [0.1109206758385699, 0.1520753981016193, 0.12301304069111796, 0.04348555101520646, 0.0511593819118803, 0.11057429130455021, 0.12624342276343237, 0.08612291454307486, 0.20034429572616808, 0.08571421446881289, 0.002677960175269667, 0.0296730650429865, 0.05451558787234445, 9.999999999998899e-05, 0.025846221896836497, 0.04309102206703541, 9.999999999998899e-05, 0.02899715333905073, 0.02919438733851276, 0.04178404375036093, 0.007114481350629087, 0.059881586604391135, 0.07162612007153535, 0.048319473842314276, 0.015958929537915, 0.03137447003760829, 0.048913505453951456, 0.042790470606617226, 0.01470235077673221, 0.02447667220900096, 0.03927210843288087, 0.039092281296777776, 0.032457441038807655, 0.03361112606879435, 0.0779732385455949, 0.033562237277637186, 0.0379633743858625, 0.017959373283673807, 0.01900288351448509, 0.003250803622391274, 0.03394185641735603, 0.05451370037206671, 0.022181527388416944, 0.015100705922057589, 0.07951188617029525, 0.08283253067843743, 0.09838258822728896, 0.08178442998960822, 0.06164480538889472, 0.08809331064683412, 0.047001639280928376, 0.11502090264788745, 0.2025202329768665, 0.10370320964593793, 0.10992068632886642, 0.09127572130130535, 0.07022146388896144, 0.14325736874472061, 0.10196185704012695, 0.0897438605310612, 0.1082203409163014, 0.1612751354133405, 0.17580267743096833, 0.15501527072952082, 0.12963200354626847, 0.1206821081914684, 0.0992343447749039, 0.10992812534855545, 0.13079468940576222, 0.1108226197685086, 0.13788908713078452, 0.1401329619869075, 0.1369742340665423, 0.1303191413576471, 0.15889501157778396, 0.11156410529900551, 0.13595303914458978, 0.15933658299728104, 0.12642266361483978, 0.12022522835581329, 0.12589585592803365, 0.14869086718791347, 0.08064021761645634, 0.04704232657583196, 0.014168646933434004, 0.0318310098450163, 0.038139907104323356, 0.03680419133551005, 0.06427327868091104, 0.04852348645254645, 0.05284964994355745, 0.10681274691330289, 0.0813149565825364, 0.09435350654355446, 0.03697696249403215, 0.038788035049615255, 0.08300259487023065, 0.0709945530397923, 0.06644422266788241, 0.0154848804604657, 9.999999999998899e-05, 9.999999999998899e-05, 0.007529376153612044, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.004753524068185522, 0.035508756705108446, 0.023170770259052786, 0.0257034379663863, 0.02654540354440882, 0.015012088603499119, 0.030920744644118203, 0.03952806500164663, 0.055819707336851154, 0.08789091862708942, 0.14866990685599046, 0.12864898440764938, 0.08700971304363192, 0.10610893813880029, 0.07460975609194664, 0.15353572027709816, 0.1392508034102018, 0.1146688659868832, 0.02733793900660675, 0.06500551107494901, 0.03968381777648411, 0.04662535003860502, 0.0305807356126665, 0.038736267830364546, 0.051092538289376765, 0.022063903471743473, 0.04699558031917639, 0.17430559712214022, 0.14371516083960323, 0.11736252132534686, 0.13747013262964392, 0.1536497678561689, 0.11526030268002596, 0.1571485249749106, 0.1541867093355067, 0.14497734269037155, 0.14476105747791257, 0.1421143386030721, 0.13198877494988415, 0.16791732132418113, 0.09468299888597342, 0.10804718963186566, 0.15620764846450041, 0.11376219289478973, 0.10397121856090419, 0.08541061877743672, 0.12870508701304895, 0.04995812248419229, 0.09505337927204505, 0.10030957108758465, 0.09628517332224074, 0.08826799866095258, 0.06607715797930591, 0.09485341576115303, 0.16825939207626073, 0.12812771642167897, 0.14157764458846978, 0.17515304810423238, 0.15097193798261743, 0.15672022724147516, 0.17241220063627627, 0.13753453981528652, 0.13280486208822162, 0.18501495030970705, 0.17280906034872245, 0.19091866981125372, 0.17951612833893738, 0.18246250006531928, 0.1830961908269484, 0.18277216836925714, 0.2360300114733621, 0.17875142780717812, 0.131739083604426, 0.09540896515055997, 0.08346273859438602, 0.12747998211303424, 0.11261756677182932, 0.11805722401323548, 0.09370473733806906, 0.14727905323099788, 0.11837160926786039, 0.0706958024295603, 0.07769707310800145, 0.09317244322923923, 0.14472629253469604, 0.06380283717879465, 0.12508987977317754, 0.06990259016727818, 0.09130243015608797, 0.04859044901019116, 0.1801532758955795, 0.19727048764549104, 0.18890798155942223, 0.18349668917613582, 0.19616852667788487, 0.18595338125752192, 0.19456740222988622, 0.18204855410716192, 0.17826379439861162, 0.0520602473227626, 0.04582803371078703, 0.04644504026836116, 0.05205179137460869, 0.033026384259138064, 0.030501711465839576, 0.06134740524524718, 0.03968994993732233, 0.03699097796310957]}, "mutation_prompt": null}
{"id": "656ed0e9-1574-4d1d-934a-d49428c9b0f2", "solution": "import numpy as np\n\nclass HybridPSO_SA_Optimizer:\n    def __init__(self, budget, dim, num_particles=30, max_iterations=1000, inertia_weight=0.7, cognitive_weight=1.4, social_weight=1.4, initial_temperature=100, cooling_rate=0.95):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = num_particles\n        self.max_iterations = max_iterations\n        self.inertia_weight = inertia_weight\n        self.cognitive_weight = cognitive_weight\n        self.social_weight = social_weight\n        self.initial_temperature = initial_temperature\n        self.cooling_rate = cooling_rate\n\n    def __call__(self, func):\n        def hybrid_pso_sa_helper():\n            # PSO initialization\n            # Dynamic adjustment of inertia weight based on exploration-exploitation balance\n            self.inertia_weight = np.random.uniform(0.4, 0.9)  # Update the inertia weight dynamically\n            # PSO optimization loop\n            # Simulated Annealing (SA) initialization\n            current_solution = np.random.uniform(-5.0, 5.0, size=self.dim)\n            current_cost = func(current_solution)\n            best_solution = current_solution\n            best_cost = current_cost\n            # SA optimization loop\n            for _ in range(self.budget):\n                proposal_solution = current_solution + np.random.normal(0, 0.1, size=self.dim)\n                proposal_solution = np.clip(proposal_solution, -5.0, 5.0)\n                proposal_cost = func(proposal_solution)\n                if proposal_cost < current_cost or np.random.rand() < np.exp((current_cost - proposal_cost) / self.initial_temperature):\n                    current_solution = proposal_solution\n                    current_cost = proposal_cost\n                    if proposal_cost < best_cost:\n                        best_solution = proposal_solution\n                        best_cost = proposal_cost\n            return best_solution\n\n        return hybrid_pso_sa_helper()", "name": "HybridPSO_SA_Optimizer", "description": "Hybridizing Particle Swarm Optimization (PSO) with Simulated Annealing (SA) by dynamically adjusting inertia weight and applying SA for local search to enhance exploration and exploitation balance for efficient optimization.", "configspace": "", "generation": 51, "fitness": 0.0906141788161689, "feedback": "The algorithm HybridPSO_SA_Optimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09 with standard deviation 0.06.", "error": "", "parent_id": "87896ef4-2630-432a-8100-a690151a1dd4", "metadata": {"aucs": [0.1109206758385699, 0.1520753981016193, 0.12301304069111796, 0.04348555101520646, 0.0511593819118803, 0.11057429130455021, 0.12624342276343237, 0.08612291454307486, 0.20034429572616808, 0.08571421446881289, 0.002677960175269667, 0.0296730650429865, 0.05451558787234445, 9.999999999998899e-05, 0.025846221896836497, 0.04309102206703541, 9.999999999998899e-05, 0.02899715333905073, 0.02919438733851276, 0.04178404375036093, 0.007114481350629087, 0.059881586604391135, 0.07162612007153535, 0.048319473842314276, 0.015958929537915, 0.03137447003760829, 0.048913505453951456, 0.042790470606617226, 0.01470235077673221, 0.02447667220900096, 0.03927210843288087, 0.039092281296777776, 0.032457441038807655, 0.03361112606879435, 0.0779732385455949, 0.033562237277637186, 0.0379633743858625, 0.017959373283673807, 0.01900288351448509, 0.003250803622391274, 0.03394185641735603, 0.05451370037206671, 0.022181527388416944, 0.015100705922057589, 0.07951188617029525, 0.08283253067843743, 0.09838258822728896, 0.08178442998960822, 0.06164480538889472, 0.08809331064683412, 0.047001639280928376, 0.11502090264788745, 0.2025202329768665, 0.10370320964593793, 0.10992068632886642, 0.09127572130130535, 0.07022146388896144, 0.14325736874472061, 0.10196185704012695, 0.0897438605310612, 0.1082203409163014, 0.1612751354133405, 0.17580267743096833, 0.15501527072952082, 0.12963200354626847, 0.1206821081914684, 0.0992343447749039, 0.10992812534855545, 0.13079468940576222, 0.1108226197685086, 0.13788908713078452, 0.1401329619869075, 0.1369742340665423, 0.1303191413576471, 0.15889501157778396, 0.11156410529900551, 0.13595303914458978, 0.15933658299728104, 0.12642266361483978, 0.12022522835581329, 0.12589585592803365, 0.14869086718791347, 0.08064021761645634, 0.04704232657583196, 0.014168646933434004, 0.0318310098450163, 0.038139907104323356, 0.03680419133551005, 0.06427327868091104, 0.04852348645254645, 0.05284964994355745, 0.10681274691330289, 0.0813149565825364, 0.09435350654355446, 0.03697696249403215, 0.038788035049615255, 0.08300259487023065, 0.0709945530397923, 0.06644422266788241, 0.0154848804604657, 9.999999999998899e-05, 9.999999999998899e-05, 0.007529376153612044, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.004753524068185522, 0.035508756705108446, 0.023170770259052786, 0.0257034379663863, 0.02654540354440882, 0.015012088603499119, 0.030920744644118203, 0.03952806500164663, 0.055819707336851154, 0.08789091862708942, 0.14866990685599046, 0.12864898440764938, 0.08700971304363192, 0.10610893813880029, 0.07460975609194664, 0.15353572027709816, 0.1392508034102018, 0.1146688659868832, 0.02733793900660675, 0.06500551107494901, 0.03968381777648411, 0.04662535003860502, 0.0305807356126665, 0.038736267830364546, 0.051092538289376765, 0.022063903471743473, 0.04699558031917639, 0.17430559712214022, 0.14371516083960323, 0.11736252132534686, 0.13747013262964392, 0.1536497678561689, 0.11526030268002596, 0.1571485249749106, 0.1541867093355067, 0.14497734269037155, 0.14476105747791257, 0.1421143386030721, 0.13198877494988415, 0.16791732132418113, 0.09468299888597342, 0.10804718963186566, 0.15620764846450041, 0.11376219289478973, 0.10397121856090419, 0.08541061877743672, 0.12870508701304895, 0.04995812248419229, 0.09505337927204505, 0.10030957108758465, 0.09628517332224074, 0.08826799866095258, 0.06607715797930591, 0.09485341576115303, 0.16825939207626073, 0.12812771642167897, 0.14157764458846978, 0.17515304810423238, 0.15097193798261743, 0.15672022724147516, 0.17241220063627627, 0.13753453981528652, 0.13280486208822162, 0.18501495030970705, 0.17280906034872245, 0.19091866981125372, 0.17951612833893738, 0.18246250006531928, 0.1830961908269484, 0.18277216836925714, 0.2360300114733621, 0.17875142780717812, 0.131739083604426, 0.09540896515055997, 0.08346273859438602, 0.12747998211303424, 0.11261756677182932, 0.11805722401323548, 0.09370473733806906, 0.14727905323099788, 0.11837160926786039, 0.0706958024295603, 0.07769707310800145, 0.09317244322923923, 0.14472629253469604, 0.06380283717879465, 0.12508987977317754, 0.06990259016727818, 0.09130243015608797, 0.04859044901019116, 0.1801532758955795, 0.19727048764549104, 0.18890798155942223, 0.18349668917613582, 0.19616852667788487, 0.18595338125752192, 0.19456740222988622, 0.18204855410716192, 0.17826379439861162, 0.0520602473227626, 0.04582803371078703, 0.04644504026836116, 0.05205179137460869, 0.033026384259138064, 0.030501711465839576, 0.06134740524524718, 0.03968994993732233, 0.03699097796310957]}, "mutation_prompt": null}
{"id": "33bc0e59-b8dc-4c1a-952b-7ded62a52034", "solution": "import numpy as np\n\nclass PSO_SA_Optimizer:\n    def __init__(self, budget, dim, num_particles=30, max_iterations=1000, inertia_weight=0.7, cognitive_weight=1.4, social_weight=1.4, initial_temperature=100, cooling_rate=0.95):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = num_particles\n        self.max_iterations = max_iterations\n        self.inertia_weight = inertia_weight\n        self.cognitive_weight = cognitive_weight\n        self.social_weight = social_weight\n        self.initial_temperature = initial_temperature\n        self.cooling_rate = cooling_rate\n\n    def __call__(self, func):\n        def pso_sa_helper():\n            # PSO initialization\n            # Dynamic adjustment of inertia weight based on exploration-exploitation balance\n            self.inertia_weight = np.random.uniform(0.4, 0.9)  # Update the inertia weight dynamically\n            # PSO optimization loop\n            # Simulated Annealing (SA) initialization\n            current_solution = np.random.uniform(-5.0, 5.0, size=self.dim)\n            current_cost = func(current_solution)\n            best_solution = current_solution\n            best_cost = current_cost\n            # SA optimization loop\n            for _ in range(self.budget):\n                proposal_solution = current_solution + np.random.normal(0, 0.1, size=self.dim)\n                proposal_solution = np.clip(proposal_solution, -5.0, 5.0)\n                proposal_cost = func(proposal_solution)\n                if proposal_cost < current_cost or np.random.rand() < np.exp((current_cost - proposal_cost) / self.initial_temperature):\n                    current_solution = proposal_solution\n                    current_cost = proposal_cost\n                    if proposal_cost < best_cost:\n                        best_solution = proposal_solution\n                        best_cost = proposal_cost\n            return best_solution\n\n        return pso_sa_helper()", "name": "PSO_SA_Optimizer", "description": "Enhancing PSO with Simulated Annealing to improve exploration and exploitation balance for efficient optimization.", "configspace": "", "generation": 48, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "87896ef4-2630-432a-8100-a690151a1dd4", "metadata": {"aucs": [0.1109206758385699, 0.1520753981016193, 0.12301304069111796, 0.04348555101520646, 0.0511593819118803, 0.11057429130455021, 0.12624342276343237, 0.08612291454307486, 0.20034429572616808, 0.08571421446881289, 0.002677960175269667, 0.0296730650429865, 0.05451558787234445, 9.999999999998899e-05, 0.025846221896836497, 0.04309102206703541, 9.999999999998899e-05, 0.02899715333905073, 0.02919438733851276, 0.04178404375036093, 0.007114481350629087, 0.059881586604391135, 0.07162612007153535, 0.048319473842314276, 0.015958929537915, 0.03137447003760829, 0.048913505453951456, 0.042790470606617226, 0.01470235077673221, 0.02447667220900096, 0.03927210843288087, 0.039092281296777776, 0.032457441038807655, 0.03361112606879435, 0.0779732385455949, 0.033562237277637186, 0.0379633743858625, 0.017959373283673807, 0.01900288351448509, 0.003250803622391274, 0.03394185641735603, 0.05451370037206671, 0.022181527388416944, 0.015100705922057589, 0.07951188617029525, 0.08283253067843743, 0.09838258822728896, 0.08178442998960822, 0.06164480538889472, 0.08809331064683412, 0.047001639280928376, 0.11502090264788745, 0.2025202329768665, 0.10370320964593793, 0.10992068632886642, 0.09127572130130535, 0.07022146388896144, 0.14325736874472061, 0.10196185704012695, 0.0897438605310612, 0.1082203409163014, 0.1612751354133405, 0.17580267743096833, 0.15501527072952082, 0.12963200354626847, 0.1206821081914684, 0.0992343447749039, 0.10992812534855545, 0.13079468940576222, 0.1108226197685086, 0.13788908713078452, 0.1401329619869075, 0.1369742340665423, 0.1303191413576471, 0.15889501157778396, 0.11156410529900551, 0.13595303914458978, 0.15933658299728104, 0.12642266361483978, 0.12022522835581329, 0.12589585592803365, 0.14869086718791347, 0.08064021761645634, 0.04704232657583196, 0.014168646933434004, 0.0318310098450163, 0.038139907104323356, 0.03680419133551005, 0.06427327868091104, 0.04852348645254645, 0.05284964994355745, 0.10681274691330289, 0.0813149565825364, 0.09435350654355446, 0.03697696249403215, 0.038788035049615255, 0.08300259487023065, 0.0709945530397923, 0.06644422266788241, 0.0154848804604657, 9.999999999998899e-05, 9.999999999998899e-05, 0.007529376153612044, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.004753524068185522, 0.035508756705108446, 0.023170770259052786, 0.0257034379663863, 0.02654540354440882, 0.015012088603499119, 0.030920744644118203, 0.03952806500164663, 0.055819707336851154, 0.08789091862708942, 0.14866990685599046, 0.12864898440764938, 0.08700971304363192, 0.10610893813880029, 0.07460975609194664, 0.15353572027709816, 0.1392508034102018, 0.1146688659868832, 0.02733793900660675, 0.06500551107494901, 0.03968381777648411, 0.04662535003860502, 0.0305807356126665, 0.038736267830364546, 0.051092538289376765, 0.022063903471743473, 0.04699558031917639, 0.17430559712214022, 0.14371516083960323, 0.11736252132534686, 0.13747013262964392, 0.1536497678561689, 0.11526030268002596, 0.1571485249749106, 0.1541867093355067, 0.14497734269037155, 0.14476105747791257, 0.1421143386030721, 0.13198877494988415, 0.16791732132418113, 0.09468299888597342, 0.10804718963186566, 0.15620764846450041, 0.11376219289478973, 0.10397121856090419, 0.08541061877743672, 0.12870508701304895, 0.04995812248419229, 0.09505337927204505, 0.10030957108758465, 0.09628517332224074, 0.08826799866095258, 0.06607715797930591, 0.09485341576115303, 0.16825939207626073, 0.12812771642167897, 0.14157764458846978, 0.17515304810423238, 0.15097193798261743, 0.15672022724147516, 0.17241220063627627, 0.13753453981528652, 0.13280486208822162, 0.18501495030970705, 0.17280906034872245, 0.19091866981125372, 0.17951612833893738, 0.18246250006531928, 0.1830961908269484, 0.18277216836925714, 0.2360300114733621, 0.17875142780717812, 0.131739083604426, 0.09540896515055997, 0.08346273859438602, 0.12747998211303424, 0.11261756677182932, 0.11805722401323548, 0.09370473733806906, 0.14727905323099788, 0.11837160926786039, 0.0706958024295603, 0.07769707310800145, 0.09317244322923923, 0.14472629253469604, 0.06380283717879465, 0.12508987977317754, 0.06990259016727818, 0.09130243015608797, 0.04859044901019116, 0.1801532758955795, 0.19727048764549104, 0.18890798155942223, 0.18349668917613582, 0.19616852667788487, 0.18595338125752192, 0.19456740222988622, 0.18204855410716192, 0.17826379439861162, 0.0520602473227626, 0.04582803371078703, 0.04644504026836116, 0.05205179137460869, 0.033026384259138064, 0.030501711465839576, 0.06134740524524718, 0.03968994993732233, 0.03699097796310957]}, "mutation_prompt": null}
{"id": "66f4fed4-3b1c-4774-9d2d-fd1606d41237", "solution": "import numpy as np\n\nclass PSO_SA_Optimizer:\n    def __init__(self, budget, dim, num_particles=30, max_iterations=1000, inertia_weight=0.7, cognitive_weight=1.4, social_weight=1.4, initial_temperature=100, cooling_rate=0.95):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = num_particles\n        self.max_iterations = max_iterations\n        self.inertia_weight = inertia_weight\n        self.cognitive_weight = cognitive_weight\n        self.social_weight = social_weight\n        self.initial_temperature = initial_temperature\n        self.cooling_rate = cooling_rate\n\n    def __call__(self, func):\n        def pso_sa_helper():\n            # PSO initialization\n            # Dynamic adjustment of inertia weight based on exploration-exploitation balance\n            self.inertia_weight = np.random.uniform(0.4, 0.9)  # Update the inertia weight dynamically\n            # PSO optimization loop\n            # Simulated Annealing (SA) initialization\n            current_solution = np.random.uniform(-5.0, 5.0, size=self.dim)\n            current_cost = func(current_solution)\n            best_solution = current_solution\n            best_cost = current_cost\n            # SA optimization loop\n            for _ in range(self.budget):\n                proposal_solution = current_solution + np.random.normal(0, 0.1, size=self.dim)\n                proposal_solution = np.clip(proposal_solution, -5.0, 5.0)\n                proposal_cost = func(proposal_solution)\n                if proposal_cost < current_cost or np.random.rand() < np.exp((current_cost - proposal_cost) / self.initial_temperature):\n                    current_solution = proposal_solution\n                    current_cost = proposal_cost\n                    if proposal_cost < best_cost:\n                        best_solution = proposal_solution\n                        best_cost = proposal_cost\n            return best_solution\n\n        return pso_sa_helper()", "name": "PSO_SA_Optimizer", "description": "Enhancing PSO with Simulated Annealing to improve exploration and exploitation balance for efficient optimization.", "configspace": "", "generation": 48, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "87896ef4-2630-432a-8100-a690151a1dd4", "metadata": {"aucs": [0.1109206758385699, 0.1520753981016193, 0.12301304069111796, 0.04348555101520646, 0.0511593819118803, 0.11057429130455021, 0.12624342276343237, 0.08612291454307486, 0.20034429572616808, 0.08571421446881289, 0.002677960175269667, 0.0296730650429865, 0.05451558787234445, 9.999999999998899e-05, 0.025846221896836497, 0.04309102206703541, 9.999999999998899e-05, 0.02899715333905073, 0.02919438733851276, 0.04178404375036093, 0.007114481350629087, 0.059881586604391135, 0.07162612007153535, 0.048319473842314276, 0.015958929537915, 0.03137447003760829, 0.048913505453951456, 0.042790470606617226, 0.01470235077673221, 0.02447667220900096, 0.03927210843288087, 0.039092281296777776, 0.032457441038807655, 0.03361112606879435, 0.0779732385455949, 0.033562237277637186, 0.0379633743858625, 0.017959373283673807, 0.01900288351448509, 0.003250803622391274, 0.03394185641735603, 0.05451370037206671, 0.022181527388416944, 0.015100705922057589, 0.07951188617029525, 0.08283253067843743, 0.09838258822728896, 0.08178442998960822, 0.06164480538889472, 0.08809331064683412, 0.047001639280928376, 0.11502090264788745, 0.2025202329768665, 0.10370320964593793, 0.10992068632886642, 0.09127572130130535, 0.07022146388896144, 0.14325736874472061, 0.10196185704012695, 0.0897438605310612, 0.1082203409163014, 0.1612751354133405, 0.17580267743096833, 0.15501527072952082, 0.12963200354626847, 0.1206821081914684, 0.0992343447749039, 0.10992812534855545, 0.13079468940576222, 0.1108226197685086, 0.13788908713078452, 0.1401329619869075, 0.1369742340665423, 0.1303191413576471, 0.15889501157778396, 0.11156410529900551, 0.13595303914458978, 0.15933658299728104, 0.12642266361483978, 0.12022522835581329, 0.12589585592803365, 0.14869086718791347, 0.08064021761645634, 0.04704232657583196, 0.014168646933434004, 0.0318310098450163, 0.038139907104323356, 0.03680419133551005, 0.06427327868091104, 0.04852348645254645, 0.05284964994355745, 0.10681274691330289, 0.0813149565825364, 0.09435350654355446, 0.03697696249403215, 0.038788035049615255, 0.08300259487023065, 0.0709945530397923, 0.06644422266788241, 0.0154848804604657, 9.999999999998899e-05, 9.999999999998899e-05, 0.007529376153612044, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.004753524068185522, 0.035508756705108446, 0.023170770259052786, 0.0257034379663863, 0.02654540354440882, 0.015012088603499119, 0.030920744644118203, 0.03952806500164663, 0.055819707336851154, 0.08789091862708942, 0.14866990685599046, 0.12864898440764938, 0.08700971304363192, 0.10610893813880029, 0.07460975609194664, 0.15353572027709816, 0.1392508034102018, 0.1146688659868832, 0.02733793900660675, 0.06500551107494901, 0.03968381777648411, 0.04662535003860502, 0.0305807356126665, 0.038736267830364546, 0.051092538289376765, 0.022063903471743473, 0.04699558031917639, 0.17430559712214022, 0.14371516083960323, 0.11736252132534686, 0.13747013262964392, 0.1536497678561689, 0.11526030268002596, 0.1571485249749106, 0.1541867093355067, 0.14497734269037155, 0.14476105747791257, 0.1421143386030721, 0.13198877494988415, 0.16791732132418113, 0.09468299888597342, 0.10804718963186566, 0.15620764846450041, 0.11376219289478973, 0.10397121856090419, 0.08541061877743672, 0.12870508701304895, 0.04995812248419229, 0.09505337927204505, 0.10030957108758465, 0.09628517332224074, 0.08826799866095258, 0.06607715797930591, 0.09485341576115303, 0.16825939207626073, 0.12812771642167897, 0.14157764458846978, 0.17515304810423238, 0.15097193798261743, 0.15672022724147516, 0.17241220063627627, 0.13753453981528652, 0.13280486208822162, 0.18501495030970705, 0.17280906034872245, 0.19091866981125372, 0.17951612833893738, 0.18246250006531928, 0.1830961908269484, 0.18277216836925714, 0.2360300114733621, 0.17875142780717812, 0.131739083604426, 0.09540896515055997, 0.08346273859438602, 0.12747998211303424, 0.11261756677182932, 0.11805722401323548, 0.09370473733806906, 0.14727905323099788, 0.11837160926786039, 0.0706958024295603, 0.07769707310800145, 0.09317244322923923, 0.14472629253469604, 0.06380283717879465, 0.12508987977317754, 0.06990259016727818, 0.09130243015608797, 0.04859044901019116, 0.1801532758955795, 0.19727048764549104, 0.18890798155942223, 0.18349668917613582, 0.19616852667788487, 0.18595338125752192, 0.19456740222988622, 0.18204855410716192, 0.17826379439861162, 0.0520602473227626, 0.04582803371078703, 0.04644504026836116, 0.05205179137460869, 0.033026384259138064, 0.030501711465839576, 0.06134740524524718, 0.03968994993732233, 0.03699097796310957]}, "mutation_prompt": null}
{"id": "dc483dc2-79bb-491f-9128-2d7caa6ff733", "solution": "import numpy as np\nfrom scipy.optimize import differential_evolution\n\nclass PSO_SA_DE_Optimizer:\n    def __init__(self, budget, dim, num_particles=30, max_iterations=1000, inertia_weight=0.7, cognitive_weight=1.4, social_weight=1.4, initial_temperature=100, cooling_rate=0.95):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = num_particles\n        self.max_iterations = max_iterations\n        self.inertia_weight = inertia_weight\n        self.cognitive_weight = cognitive_weight\n        self.social_weight = social_weight\n        self.initial_temperature = initial_temperature\n        self.cooling_rate = cooling_rate\n\n    def __call__(self, func):\n        def pso_sa_de_helper():\n            # PSO initialization\n            # Dynamic adjustment of inertia weight based on exploration-exploitation balance\n            self.inertia_weight = np.random.uniform(0.4, 0.9)  # Update the inertia weight dynamically\n            # PSO optimization loop\n            # Simulated Annealing (SA) initialization\n            current_solution = np.random.uniform(-5.0, 5.0, size=self.dim)\n            current_cost = func(current_solution)\n            best_solution = current_solution\n            best_cost = current_cost\n            # SA optimization loop\n            for _ in range(self.budget):\n                proposal_solution = current_solution + np.random.normal(0, 0.1, size=self.dim)\n                proposal_solution = np.clip(proposal_solution, -5.0, 5.0)\n                proposal_cost = func(proposal_solution)\n                if proposal_cost < current_cost or np.random.rand() < np.exp((current_cost - proposal_cost) / self.initial_temperature):\n                    current_solution = proposal_solution\n                    current_cost = proposal_cost\n                    if proposal_cost < best_cost:\n                        best_solution = proposal_solution\n                        best_cost = proposal_cost\n            # Differential Evolution (DE) to further refine the solution\n            bounds = [(-5.0, 5.0)] * self.dim\n            de_result = differential_evolution(func, bounds, maxiter=self.budget, popsize=10)\n            return de_result.x\n\n        return pso_sa_de_helper()", "name": "PSO_SA_DE_Optimizer", "description": "Enhancing PSO with Simulated Annealing and Differential Evolution to improve convergence and exploration for robust optimization.", "configspace": "", "generation": 54, "fitness": 0.0906141788161689, "feedback": "The algorithm PSO_SA_DE_Optimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09 with standard deviation 0.06.", "error": "", "parent_id": "87896ef4-2630-432a-8100-a690151a1dd4", "metadata": {"aucs": [0.1109206758385699, 0.1520753981016193, 0.12301304069111796, 0.04348555101520646, 0.0511593819118803, 0.11057429130455021, 0.12624342276343237, 0.08612291454307486, 0.20034429572616808, 0.08571421446881289, 0.002677960175269667, 0.0296730650429865, 0.05451558787234445, 9.999999999998899e-05, 0.025846221896836497, 0.04309102206703541, 9.999999999998899e-05, 0.02899715333905073, 0.02919438733851276, 0.04178404375036093, 0.007114481350629087, 0.059881586604391135, 0.07162612007153535, 0.048319473842314276, 0.015958929537915, 0.03137447003760829, 0.048913505453951456, 0.042790470606617226, 0.01470235077673221, 0.02447667220900096, 0.03927210843288087, 0.039092281296777776, 0.032457441038807655, 0.03361112606879435, 0.0779732385455949, 0.033562237277637186, 0.0379633743858625, 0.017959373283673807, 0.01900288351448509, 0.003250803622391274, 0.03394185641735603, 0.05451370037206671, 0.022181527388416944, 0.015100705922057589, 0.07951188617029525, 0.08283253067843743, 0.09838258822728896, 0.08178442998960822, 0.06164480538889472, 0.08809331064683412, 0.047001639280928376, 0.11502090264788745, 0.2025202329768665, 0.10370320964593793, 0.10992068632886642, 0.09127572130130535, 0.07022146388896144, 0.14325736874472061, 0.10196185704012695, 0.0897438605310612, 0.1082203409163014, 0.1612751354133405, 0.17580267743096833, 0.15501527072952082, 0.12963200354626847, 0.1206821081914684, 0.0992343447749039, 0.10992812534855545, 0.13079468940576222, 0.1108226197685086, 0.13788908713078452, 0.1401329619869075, 0.1369742340665423, 0.1303191413576471, 0.15889501157778396, 0.11156410529900551, 0.13595303914458978, 0.15933658299728104, 0.12642266361483978, 0.12022522835581329, 0.12589585592803365, 0.14869086718791347, 0.08064021761645634, 0.04704232657583196, 0.014168646933434004, 0.0318310098450163, 0.038139907104323356, 0.03680419133551005, 0.06427327868091104, 0.04852348645254645, 0.05284964994355745, 0.10681274691330289, 0.0813149565825364, 0.09435350654355446, 0.03697696249403215, 0.038788035049615255, 0.08300259487023065, 0.0709945530397923, 0.06644422266788241, 0.0154848804604657, 9.999999999998899e-05, 9.999999999998899e-05, 0.007529376153612044, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.004753524068185522, 0.035508756705108446, 0.023170770259052786, 0.0257034379663863, 0.02654540354440882, 0.015012088603499119, 0.030920744644118203, 0.03952806500164663, 0.055819707336851154, 0.08789091862708942, 0.14866990685599046, 0.12864898440764938, 0.08700971304363192, 0.10610893813880029, 0.07460975609194664, 0.15353572027709816, 0.1392508034102018, 0.1146688659868832, 0.02733793900660675, 0.06500551107494901, 0.03968381777648411, 0.04662535003860502, 0.0305807356126665, 0.038736267830364546, 0.051092538289376765, 0.022063903471743473, 0.04699558031917639, 0.17430559712214022, 0.14371516083960323, 0.11736252132534686, 0.13747013262964392, 0.1536497678561689, 0.11526030268002596, 0.1571485249749106, 0.1541867093355067, 0.14497734269037155, 0.14476105747791257, 0.1421143386030721, 0.13198877494988415, 0.16791732132418113, 0.09468299888597342, 0.10804718963186566, 0.15620764846450041, 0.11376219289478973, 0.10397121856090419, 0.08541061877743672, 0.12870508701304895, 0.04995812248419229, 0.09505337927204505, 0.10030957108758465, 0.09628517332224074, 0.08826799866095258, 0.06607715797930591, 0.09485341576115303, 0.16825939207626073, 0.12812771642167897, 0.14157764458846978, 0.17515304810423238, 0.15097193798261743, 0.15672022724147516, 0.17241220063627627, 0.13753453981528652, 0.13280486208822162, 0.18501495030970705, 0.17280906034872245, 0.19091866981125372, 0.17951612833893738, 0.18246250006531928, 0.1830961908269484, 0.18277216836925714, 0.2360300114733621, 0.17875142780717812, 0.131739083604426, 0.09540896515055997, 0.08346273859438602, 0.12747998211303424, 0.11261756677182932, 0.11805722401323548, 0.09370473733806906, 0.14727905323099788, 0.11837160926786039, 0.0706958024295603, 0.07769707310800145, 0.09317244322923923, 0.14472629253469604, 0.06380283717879465, 0.12508987977317754, 0.06990259016727818, 0.09130243015608797, 0.04859044901019116, 0.1801532758955795, 0.19727048764549104, 0.18890798155942223, 0.18349668917613582, 0.19616852667788487, 0.18595338125752192, 0.19456740222988622, 0.18204855410716192, 0.17826379439861162, 0.0520602473227626, 0.04582803371078703, 0.04644504026836116, 0.05205179137460869, 0.033026384259138064, 0.030501711465839576, 0.06134740524524718, 0.03968994993732233, 0.03699097796310957]}, "mutation_prompt": null}
{"id": "654477e0-4d3a-43cc-b192-f885349aa5cc", "solution": "import numpy as np\nfrom scipy.optimize import differential_evolution\n\nclass Hybrid_PSODE_SA_Optimizer:\n    def __init__(self, budget, dim, num_particles=30, max_iterations=1000, inertia_weight=0.7, cognitive_weight=1.4, social_weight=1.4, initial_temperature=100, cooling_rate=0.95):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = num_particles\n        self.max_iterations = max_iterations\n        self.inertia_weight = inertia_weight\n        self.cognitive_weight = cognitive_weight\n        self.social_weight = social_weight\n        self.initial_temperature = initial_temperature\n        self.cooling_rate = cooling_rate\n\n    def __call__(self, func):\n        def hybrid_pso_de_sa_helper():\n            # PSO initialization\n            # Dynamic adjustment of inertia weight based on exploration-exploitation balance\n            self.inertia_weight = np.random.uniform(0.4, 0.9)  # Update the inertia weight dynamically\n            # PSO optimization loop\n            # Simulated Annealing (SA) initialization\n            current_solution = np.random.uniform(-5.0, 5.0, size=self.dim)\n            current_cost = func(current_solution)\n            best_solution = current_solution\n            best_cost = current_cost\n            # SA optimization loop\n            for _ in range(int(self.budget*0.7)):  # Spend 70% of the budget on SA\n                proposal_solution = current_solution + np.random.normal(0, 0.1, size=self.dim)\n                proposal_solution = np.clip(proposal_solution, -5.0, 5.0)\n                proposal_cost = func(proposal_solution)\n                if proposal_cost < current_cost or np.random.rand() < np.exp((current_cost - proposal_cost) / self.initial_temperature):\n                    current_solution = proposal_solution\n                    current_cost = proposal_cost\n                    if proposal_cost < best_cost:\n                        best_solution = proposal_solution\n                        best_cost = proposal_cost\n            # Differential Evolution (DE) optimization\n            de_result = differential_evolution(func, bounds=[(-5.0, 5.0)]*self.dim, maxiter=int(self.budget*0.3))  # Spend 30% of the budget on DE\n            if de_result.fun < best_cost:\n                best_solution = de_result.x\n            return best_solution\n\n        return hybrid_pso_de_sa_helper()", "name": "Hybrid_PSODE_SA_Optimizer", "description": "Enhancing PSO with Simulated Annealing and Differential Evolution for a hybrid metaheuristic approach improving global exploration and local exploitation in optimization.", "configspace": "", "generation": 55, "fitness": 0.12523380220864708, "feedback": "The algorithm Hybrid_PSODE_SA_Optimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.13 with standard deviation 0.08.", "error": "", "parent_id": "87896ef4-2630-432a-8100-a690151a1dd4", "metadata": {"aucs": [0.28973149172408763, 0.3243343389857559, 0.3104742755271688, 0.2744396126493731, 0.2774458722826103, 0.31926094604124955, 0.32791907153315547, 0.28532970889671316, 0.39438341350319917, 0.12397055834086712, 0.04939007869361833, 0.09422327201410585, 0.10233794387117101, 0.05302389454059209, 0.06373305039398669, 0.10284165005858503, 0.03760881254179349, 0.055337717561820154, 0.04150802651161578, 0.05075537176052114, 0.03365153336430138, 0.06215022989286334, 0.07594145050725909, 0.06130811290555027, 0.03230559872452954, 0.04370902295136947, 0.055898722931636224, 0.06004933963820824, 0.03165736385000251, 0.039569653261228965, 0.048837084967940814, 0.044710756467256196, 0.04343610801577891, 0.038382042470198385, 0.07847093880912492, 0.04424343571404732, 0.07010485073933248, 0.05395409943193574, 0.05652898369471915, 0.18589990476891438, 0.23434793106584673, 0.2441959444329561, 0.06058360204785662, 0.05567752275458138, 0.1034196237728231, 0.11045008720817784, 0.11347123230506417, 0.09850697481016701, 0.10384130000541192, 0.08682883545352216, 0.07679838170617093, 0.17127603707329597, 0.21701579972591079, 0.14094456579986547, 0.1333888930933037, 0.12059374628619768, 0.11098051857991864, 0.16772864386416664, 0.11453810633532446, 0.12840024522631288, 0.1353829815106592, 0.17860860852578642, 0.1860939969876131, 0.178884109542495, 0.13983460442365558, 0.1333347659036701, 0.17538025757153897, 0.1696391932397846, 0.20898620252861944, 0.13966198754307735, 0.16681340951018842, 0.15435129490577815, 0.17259608440554453, 0.13418339668712487, 0.17750849571150318, 0.11859589278419369, 0.12904342847833394, 0.15933658299728104, 0.1690927229376843, 0.1206534886484677, 0.12878998561392474, 0.14869086718791347, 0.08064021761645634, 0.043298800064635046, 0.016498715065894287, 0.031701182988220666, 0.038139907104323356, 0.03680419133551005, 0.061512302278703634, 0.04852348645254645, 0.06805068862738084, 0.11292051162170702, 0.0957284223220689, 0.0960733407659844, 0.053021487728643346, 0.05364917729251395, 0.09389110008446189, 0.07903185320198969, 0.06729162809308142, 0.0154848804604657, 9.999999999998899e-05, 9.999999999998899e-05, 0.007595369039079203, 0.0025259036735233442, 0.0003589649686306373, 0.00018239067616576676, 9.999999999998899e-05, 9.999999999998899e-05, 0.02985117739490939, 0.043091437638419716, 0.0378348807083041, 0.039508122018701575, 0.036229829907631794, 0.0297712811357278, 0.03988485537509201, 0.048109500141186756, 0.06286111091181812, 0.2511888026644351, 0.24810992255575093, 0.24402358187859663, 0.23036495483298214, 0.24943361422814858, 0.23986209029161054, 0.30987712851975313, 0.2597461877557691, 0.27065104071735535, 0.04867112567335119, 0.06820909165177491, 0.051486197879023754, 0.050267593628658735, 0.040101542697310144, 0.04072859991481814, 0.054627671432407765, 0.032862792041209676, 0.05433701213911424, 0.17444936183953996, 0.14371516083960323, 0.12946589274416587, 0.13747013262964392, 0.1536497678561689, 0.12043448571044268, 0.1571485249749106, 0.14464974352974658, 0.14497734269037155, 0.1708139789123294, 0.1586413903771685, 0.148184360253665, 0.18311320569040113, 0.13184737136901292, 0.14460040537159025, 0.16124461403912715, 0.13020743501763787, 0.12221087930504726, 0.10940003030126377, 0.13797734420706276, 0.07769349794558622, 0.12007842078417075, 0.11420923196346888, 0.10861229835131148, 0.11155019915498798, 0.08474833677386073, 0.1158298687992636, 0.17113078066608556, 0.13682451244612515, 0.1399790605469592, 0.1776663608115603, 0.1534014260329143, 0.15712298818013637, 0.18413458292615092, 0.14145368570814953, 0.15261969021389632, 0.18858712514247244, 0.1730069271655862, 0.1803150320316319, 0.1818539411500747, 0.26293646454379493, 0.18380344683991567, 0.1811667670841648, 0.23671128241740202, 0.1842364488710415, 0.1349898987331568, 0.2013768030665769, 0.1037038311940447, 0.24798713808471506, 0.2044653147518426, 0.21455787443950325, 0.15664723064753927, 0.22191957410274776, 0.2486240179329362, 0.1616701842324415, 0.2133346743371719, 0.11696931124601961, 0.24286022215797276, 0.13932911821064042, 0.21664091937944374, 0.12566389461873628, 0.09967917194658626, 0.09438261678021043, 0.18024044736995637, 0.19727048764549104, 0.18890798155942223, 0.18383336497192415, 0.19616852667788487, 0.18595338125752192, 0.19266922079698612, 0.18204855410716192, 0.17826379439861162, 0.05273641752896874, 0.04897218547807569, 0.04648730377882149, 0.05683441875198225, 0.041714007709949374, 0.0345105000343352, 0.0626712221614758, 0.043885034159927505, 0.03633447168162529]}, "mutation_prompt": null}
{"id": "ad01e904-1bff-47c5-8eb4-227f58f44d95", "solution": "import numpy as np\nfrom scipy.optimize import differential_evolution\n\nclass Hybrid_PSODE_SA_Optimizer:\n    def __init__(self, budget, dim, num_particles=30, max_iterations=1000, inertia_weight=0.7, cognitive_weight=1.4, social_weight=1.4, initial_temperature=100, cooling_rate=0.95):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = num_particles\n        self.max_iterations = max_iterations\n        self.inertia_weight = inertia_weight\n        self.cognitive_weight = cognitive_weight\n        self.social_weight = social_weight\n        self.initial_temperature = initial_temperature\n        self.cooling_rate = cooling_rate\n\n    def __call__(self, func):\n        def hybrid_pso_de_sa_helper():\n            # PSO initialization\n            # Dynamic adjustment of inertia weight based on exploration-exploitation balance\n            self.inertia_weight = np.random.uniform(0.4, 0.9)  # Update the inertia weight dynamically\n            # PSO optimization loop\n            # Simulated Annealing (SA) initialization\n            current_solution = np.random.uniform(-5.0, 5.0, size=self.dim)\n            current_cost = func(current_solution)\n            best_solution = current_solution\n            best_cost = current_cost\n            # SA optimization loop\n            for _ in range(int(self.budget*0.7)):  # Spend 70% of the budget on SA\n                proposal_solution = current_solution + np.random.normal(0, 0.1, size=self.dim)\n                proposal_solution = np.clip(proposal_solution, -5.0, 5.0)\n                proposal_cost = func(proposal_solution)\n                if proposal_cost < current_cost or np.random.rand() < np.exp((current_cost - proposal_cost) / self.initial_temperature):\n                    current_solution = proposal_solution\n                    current_cost = proposal_cost\n                    if proposal_cost < best_cost:\n                        best_solution = proposal_solution\n                        best_cost = proposal_cost\n            # Differential Evolution (DE) optimization\n            de_result = differential_evolution(func, bounds=[(-5.0, 5.0)]*self.dim, maxiter=int(self.budget*0.3))  # Spend 30% of the budget on DE\n            if de_result.fun < best_cost:\n                best_solution = de_result.x\n            return best_solution\n\n        return hybrid_pso_de_sa_helper()", "name": "Hybrid_PSODE_SA_Optimizer", "description": "Enhancing PSO with Simulated Annealing and Differential Evolution for a hybrid metaheuristic approach improving global exploration and local exploitation in optimization.", "configspace": "", "generation": 56, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "654477e0-4d3a-43cc-b192-f885349aa5cc", "metadata": {"aucs": [0.28973149172408763, 0.3243343389857559, 0.3104742755271688, 0.2744396126493731, 0.2774458722826103, 0.31926094604124955, 0.32791907153315547, 0.28532970889671316, 0.39438341350319917, 0.12397055834086712, 0.04939007869361833, 0.09422327201410585, 0.10233794387117101, 0.05302389454059209, 0.06373305039398669, 0.10284165005858503, 0.03760881254179349, 0.055337717561820154, 0.04150802651161578, 0.05075537176052114, 0.03365153336430138, 0.06215022989286334, 0.07594145050725909, 0.06130811290555027, 0.03230559872452954, 0.04370902295136947, 0.055898722931636224, 0.06004933963820824, 0.03165736385000251, 0.039569653261228965, 0.048837084967940814, 0.044710756467256196, 0.04343610801577891, 0.038382042470198385, 0.07847093880912492, 0.04424343571404732, 0.07010485073933248, 0.05395409943193574, 0.05652898369471915, 0.18589990476891438, 0.23434793106584673, 0.2441959444329561, 0.06058360204785662, 0.05567752275458138, 0.1034196237728231, 0.11045008720817784, 0.11347123230506417, 0.09850697481016701, 0.10384130000541192, 0.08682883545352216, 0.07679838170617093, 0.17127603707329597, 0.21701579972591079, 0.14094456579986547, 0.1333888930933037, 0.12059374628619768, 0.11098051857991864, 0.16772864386416664, 0.11453810633532446, 0.12840024522631288, 0.1353829815106592, 0.17860860852578642, 0.1860939969876131, 0.178884109542495, 0.13983460442365558, 0.1333347659036701, 0.17538025757153897, 0.1696391932397846, 0.20898620252861944, 0.13966198754307735, 0.16681340951018842, 0.15435129490577815, 0.17259608440554453, 0.13418339668712487, 0.17750849571150318, 0.11859589278419369, 0.12904342847833394, 0.15933658299728104, 0.1690927229376843, 0.1206534886484677, 0.12878998561392474, 0.14869086718791347, 0.08064021761645634, 0.043298800064635046, 0.016498715065894287, 0.031701182988220666, 0.038139907104323356, 0.03680419133551005, 0.061512302278703634, 0.04852348645254645, 0.06805068862738084, 0.11292051162170702, 0.0957284223220689, 0.0960733407659844, 0.053021487728643346, 0.05364917729251395, 0.09389110008446189, 0.07903185320198969, 0.06729162809308142, 0.0154848804604657, 9.999999999998899e-05, 9.999999999998899e-05, 0.007595369039079203, 0.0025259036735233442, 0.0003589649686306373, 0.00018239067616576676, 9.999999999998899e-05, 9.999999999998899e-05, 0.02985117739490939, 0.043091437638419716, 0.0378348807083041, 0.039508122018701575, 0.036229829907631794, 0.0297712811357278, 0.03988485537509201, 0.048109500141186756, 0.06286111091181812, 0.2511888026644351, 0.24810992255575093, 0.24402358187859663, 0.23036495483298214, 0.24943361422814858, 0.23986209029161054, 0.30987712851975313, 0.2597461877557691, 0.27065104071735535, 0.04867112567335119, 0.06820909165177491, 0.051486197879023754, 0.050267593628658735, 0.040101542697310144, 0.04072859991481814, 0.054627671432407765, 0.032862792041209676, 0.05433701213911424, 0.17444936183953996, 0.14371516083960323, 0.12946589274416587, 0.13747013262964392, 0.1536497678561689, 0.12043448571044268, 0.1571485249749106, 0.14464974352974658, 0.14497734269037155, 0.1708139789123294, 0.1586413903771685, 0.148184360253665, 0.18311320569040113, 0.13184737136901292, 0.14460040537159025, 0.16124461403912715, 0.13020743501763787, 0.12221087930504726, 0.10940003030126377, 0.13797734420706276, 0.07769349794558622, 0.12007842078417075, 0.11420923196346888, 0.10861229835131148, 0.11155019915498798, 0.08474833677386073, 0.1158298687992636, 0.17113078066608556, 0.13682451244612515, 0.1399790605469592, 0.1776663608115603, 0.1534014260329143, 0.15712298818013637, 0.18413458292615092, 0.14145368570814953, 0.15261969021389632, 0.18858712514247244, 0.1730069271655862, 0.1803150320316319, 0.1818539411500747, 0.26293646454379493, 0.18380344683991567, 0.1811667670841648, 0.23671128241740202, 0.1842364488710415, 0.1349898987331568, 0.2013768030665769, 0.1037038311940447, 0.24798713808471506, 0.2044653147518426, 0.21455787443950325, 0.15664723064753927, 0.22191957410274776, 0.2486240179329362, 0.1616701842324415, 0.2133346743371719, 0.11696931124601961, 0.24286022215797276, 0.13932911821064042, 0.21664091937944374, 0.12566389461873628, 0.09967917194658626, 0.09438261678021043, 0.18024044736995637, 0.19727048764549104, 0.18890798155942223, 0.18383336497192415, 0.19616852667788487, 0.18595338125752192, 0.19266922079698612, 0.18204855410716192, 0.17826379439861162, 0.05273641752896874, 0.04897218547807569, 0.04648730377882149, 0.05683441875198225, 0.041714007709949374, 0.0345105000343352, 0.0626712221614758, 0.043885034159927505, 0.03633447168162529]}, "mutation_prompt": null}
{"id": "ae16ca35-d4d8-4fb9-bb79-a60ad17672f7", "solution": "import numpy as np\nfrom scipy.optimize import differential_evolution\n\nclass Novel_Hybrid_Metaheuristic:\n    def __init__(self, budget, dim, num_particles=30, max_iterations=1000, inertia_weight=0.7, cognitive_weight=1.4, social_weight=1.4, initial_temperature=100, cooling_rate=0.95):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = num_particles\n        self.max_iterations = max_iterations\n        self.inertia_weight = inertia_weight\n        self.cognitive_weight = cognitive_weight\n        self.social_weight = social_weight\n        self.initial_temperature = initial_temperature\n        self.cooling_rate = cooling_rate\n\n    def __call__(self, func):\n        def novel_hybrid_metaheuristic_helper():\n            # PSO initialization\n            # Dynamic adjustment of inertia weight based on exploration-exploitation balance\n            self.inertia_weight = np.random.uniform(0.4, 0.9)  # Update the inertia weight dynamically\n            # PSO optimization loop\n            # Simulated Annealing (SA) initialization\n            current_solution = np.random.uniform(-5.0, 5.0, size=self.dim)\n            current_cost = func(current_solution)\n            best_solution = current_solution\n            best_cost = current_cost\n            # SA optimization loop\n            for _ in range(int(self.budget*0.7)):  # Spend 70% of the budget on SA\n                proposal_solution = current_solution + np.random.normal(0, 0.1, size=self.dim)\n                proposal_solution = np.clip(proposal_solution, -5.0, 5.0)\n                proposal_cost = func(proposal_solution)\n                if proposal_cost < current_cost or np.random.rand() < np.exp((current_cost - proposal_cost) / self.initial_temperature):\n                    current_solution = proposal_solution\n                    current_cost = proposal_cost\n                    if proposal_cost < best_cost:\n                        best_solution = proposal_solution\n                        best_cost = proposal_cost\n            # Differential Evolution (DE) optimization\n            de_result = differential_evolution(func, bounds=[(-5.0, 5.0)]*self.dim, maxiter=int(self.budget*0.3))  # Spend 30% of the budget on DE\n            if de_result.fun < best_cost:\n                best_solution = de_result.x\n            return best_solution\n\n        return novel_hybrid_metaheuristic_helper()", "name": "Novel_Hybrid_Metaheuristic", "description": "A novel metaheuristic algorithm combining Particle Swarm Optimization (PSO), Simulated Annealing (SA), and Differential Evolution (DE) for black box optimization in a hybrid approach.", "configspace": "", "generation": 57, "fitness": 0.12523380220864708, "feedback": "The algorithm Novel_Hybrid_Metaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.13 with standard deviation 0.08.", "error": "", "parent_id": "654477e0-4d3a-43cc-b192-f885349aa5cc", "metadata": {"aucs": [0.28973149172408763, 0.3243343389857559, 0.3104742755271688, 0.2744396126493731, 0.2774458722826103, 0.31926094604124955, 0.32791907153315547, 0.28532970889671316, 0.39438341350319917, 0.12397055834086712, 0.04939007869361833, 0.09422327201410585, 0.10233794387117101, 0.05302389454059209, 0.06373305039398669, 0.10284165005858503, 0.03760881254179349, 0.055337717561820154, 0.04150802651161578, 0.05075537176052114, 0.03365153336430138, 0.06215022989286334, 0.07594145050725909, 0.06130811290555027, 0.03230559872452954, 0.04370902295136947, 0.055898722931636224, 0.06004933963820824, 0.03165736385000251, 0.039569653261228965, 0.048837084967940814, 0.044710756467256196, 0.04343610801577891, 0.038382042470198385, 0.07847093880912492, 0.04424343571404732, 0.07010485073933248, 0.05395409943193574, 0.05652898369471915, 0.18589990476891438, 0.23434793106584673, 0.2441959444329561, 0.06058360204785662, 0.05567752275458138, 0.1034196237728231, 0.11045008720817784, 0.11347123230506417, 0.09850697481016701, 0.10384130000541192, 0.08682883545352216, 0.07679838170617093, 0.17127603707329597, 0.21701579972591079, 0.14094456579986547, 0.1333888930933037, 0.12059374628619768, 0.11098051857991864, 0.16772864386416664, 0.11453810633532446, 0.12840024522631288, 0.1353829815106592, 0.17860860852578642, 0.1860939969876131, 0.178884109542495, 0.13983460442365558, 0.1333347659036701, 0.17538025757153897, 0.1696391932397846, 0.20898620252861944, 0.13966198754307735, 0.16681340951018842, 0.15435129490577815, 0.17259608440554453, 0.13418339668712487, 0.17750849571150318, 0.11859589278419369, 0.12904342847833394, 0.15933658299728104, 0.1690927229376843, 0.1206534886484677, 0.12878998561392474, 0.14869086718791347, 0.08064021761645634, 0.043298800064635046, 0.016498715065894287, 0.031701182988220666, 0.038139907104323356, 0.03680419133551005, 0.061512302278703634, 0.04852348645254645, 0.06805068862738084, 0.11292051162170702, 0.0957284223220689, 0.0960733407659844, 0.053021487728643346, 0.05364917729251395, 0.09389110008446189, 0.07903185320198969, 0.06729162809308142, 0.0154848804604657, 9.999999999998899e-05, 9.999999999998899e-05, 0.007595369039079203, 0.0025259036735233442, 0.0003589649686306373, 0.00018239067616576676, 9.999999999998899e-05, 9.999999999998899e-05, 0.02985117739490939, 0.043091437638419716, 0.0378348807083041, 0.039508122018701575, 0.036229829907631794, 0.0297712811357278, 0.03988485537509201, 0.048109500141186756, 0.06286111091181812, 0.2511888026644351, 0.24810992255575093, 0.24402358187859663, 0.23036495483298214, 0.24943361422814858, 0.23986209029161054, 0.30987712851975313, 0.2597461877557691, 0.27065104071735535, 0.04867112567335119, 0.06820909165177491, 0.051486197879023754, 0.050267593628658735, 0.040101542697310144, 0.04072859991481814, 0.054627671432407765, 0.032862792041209676, 0.05433701213911424, 0.17444936183953996, 0.14371516083960323, 0.12946589274416587, 0.13747013262964392, 0.1536497678561689, 0.12043448571044268, 0.1571485249749106, 0.14464974352974658, 0.14497734269037155, 0.1708139789123294, 0.1586413903771685, 0.148184360253665, 0.18311320569040113, 0.13184737136901292, 0.14460040537159025, 0.16124461403912715, 0.13020743501763787, 0.12221087930504726, 0.10940003030126377, 0.13797734420706276, 0.07769349794558622, 0.12007842078417075, 0.11420923196346888, 0.10861229835131148, 0.11155019915498798, 0.08474833677386073, 0.1158298687992636, 0.17113078066608556, 0.13682451244612515, 0.1399790605469592, 0.1776663608115603, 0.1534014260329143, 0.15712298818013637, 0.18413458292615092, 0.14145368570814953, 0.15261969021389632, 0.18858712514247244, 0.1730069271655862, 0.1803150320316319, 0.1818539411500747, 0.26293646454379493, 0.18380344683991567, 0.1811667670841648, 0.23671128241740202, 0.1842364488710415, 0.1349898987331568, 0.2013768030665769, 0.1037038311940447, 0.24798713808471506, 0.2044653147518426, 0.21455787443950325, 0.15664723064753927, 0.22191957410274776, 0.2486240179329362, 0.1616701842324415, 0.2133346743371719, 0.11696931124601961, 0.24286022215797276, 0.13932911821064042, 0.21664091937944374, 0.12566389461873628, 0.09967917194658626, 0.09438261678021043, 0.18024044736995637, 0.19727048764549104, 0.18890798155942223, 0.18383336497192415, 0.19616852667788487, 0.18595338125752192, 0.19266922079698612, 0.18204855410716192, 0.17826379439861162, 0.05273641752896874, 0.04897218547807569, 0.04648730377882149, 0.05683441875198225, 0.041714007709949374, 0.0345105000343352, 0.0626712221614758, 0.043885034159927505, 0.03633447168162529]}, "mutation_prompt": null}
{"id": "4865fff0-5911-4756-b600-2c3e37940c74", "solution": "import numpy as np\nfrom scipy.optimize import differential_evolution\n\nclass Hybrid_PSODE_SA_Optimizer:\n    def __init__(self, budget, dim, num_particles=30, max_iterations=1000, inertia_weight=0.7, cognitive_weight=1.4, social_weight=1.4, initial_temperature=100, cooling_rate=0.95):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = num_particles\n        self.max_iterations = max_iterations\n        self.inertia_weight = inertia_weight\n        self.cognitive_weight = cognitive_weight\n        self.social_weight = social_weight\n        self.initial_temperature = initial_temperature\n        self.cooling_rate = cooling_rate\n\n    def __call__(self, func):\n        def hybrid_pso_de_sa_helper():\n            # PSO initialization\n            # Dynamic adjustment of inertia weight based on exploration-exploitation balance\n            self.inertia_weight = np.random.uniform(0.4, 0.9)  # Update the inertia weight dynamically\n            # PSO optimization loop\n            # Simulated Annealing (SA) initialization\n            current_solution = np.random.uniform(-5.0, 5.0, size=self.dim)\n            current_cost = func(current_solution)\n            best_solution = current_solution\n            best_cost = current_cost\n            # SA optimization loop\n            for _ in range(int(self.budget*0.7)):  # Spend 70% of the budget on SA\n                proposal_solution = current_solution + np.random.normal(0, 0.1, size=self.dim)\n                proposal_solution = np.clip(proposal_solution, -5.0, 5.0)\n                proposal_cost = func(proposal_solution)\n                if proposal_cost < current_cost or np.random.rand() < np.exp((current_cost - proposal_cost) / self.initial_temperature):\n                    current_solution = proposal_solution\n                    current_cost = proposal_cost\n                    if proposal_cost < best_cost:\n                        best_solution = proposal_solution\n                        best_cost = proposal_cost\n            # Differential Evolution (DE) optimization\n            de_result = differential_evolution(func, bounds=[(-5.0, 5.0)]*self.dim, maxiter=int(self.budget*0.3))  # Spend 30% of the budget on DE\n            if de_result.fun < best_cost:\n                best_solution = de_result.x\n            return best_solution\n\n        return hybrid_pso_de_sa_helper()", "name": "Hybrid_PSODE_SA_Optimizer", "description": "Enhancing PSO with Simulated Annealing and Differential Evolution for a hybrid metaheuristic approach improving global exploration and local exploitation in optimization.", "configspace": "", "generation": 56, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "654477e0-4d3a-43cc-b192-f885349aa5cc", "metadata": {"aucs": [0.28973149172408763, 0.3243343389857559, 0.3104742755271688, 0.2744396126493731, 0.2774458722826103, 0.31926094604124955, 0.32791907153315547, 0.28532970889671316, 0.39438341350319917, 0.12397055834086712, 0.04939007869361833, 0.09422327201410585, 0.10233794387117101, 0.05302389454059209, 0.06373305039398669, 0.10284165005858503, 0.03760881254179349, 0.055337717561820154, 0.04150802651161578, 0.05075537176052114, 0.03365153336430138, 0.06215022989286334, 0.07594145050725909, 0.06130811290555027, 0.03230559872452954, 0.04370902295136947, 0.055898722931636224, 0.06004933963820824, 0.03165736385000251, 0.039569653261228965, 0.048837084967940814, 0.044710756467256196, 0.04343610801577891, 0.038382042470198385, 0.07847093880912492, 0.04424343571404732, 0.07010485073933248, 0.05395409943193574, 0.05652898369471915, 0.18589990476891438, 0.23434793106584673, 0.2441959444329561, 0.06058360204785662, 0.05567752275458138, 0.1034196237728231, 0.11045008720817784, 0.11347123230506417, 0.09850697481016701, 0.10384130000541192, 0.08682883545352216, 0.07679838170617093, 0.17127603707329597, 0.21701579972591079, 0.14094456579986547, 0.1333888930933037, 0.12059374628619768, 0.11098051857991864, 0.16772864386416664, 0.11453810633532446, 0.12840024522631288, 0.1353829815106592, 0.17860860852578642, 0.1860939969876131, 0.178884109542495, 0.13983460442365558, 0.1333347659036701, 0.17538025757153897, 0.1696391932397846, 0.20898620252861944, 0.13966198754307735, 0.16681340951018842, 0.15435129490577815, 0.17259608440554453, 0.13418339668712487, 0.17750849571150318, 0.11859589278419369, 0.12904342847833394, 0.15933658299728104, 0.1690927229376843, 0.1206534886484677, 0.12878998561392474, 0.14869086718791347, 0.08064021761645634, 0.043298800064635046, 0.016498715065894287, 0.031701182988220666, 0.038139907104323356, 0.03680419133551005, 0.061512302278703634, 0.04852348645254645, 0.06805068862738084, 0.11292051162170702, 0.0957284223220689, 0.0960733407659844, 0.053021487728643346, 0.05364917729251395, 0.09389110008446189, 0.07903185320198969, 0.06729162809308142, 0.0154848804604657, 9.999999999998899e-05, 9.999999999998899e-05, 0.007595369039079203, 0.0025259036735233442, 0.0003589649686306373, 0.00018239067616576676, 9.999999999998899e-05, 9.999999999998899e-05, 0.02985117739490939, 0.043091437638419716, 0.0378348807083041, 0.039508122018701575, 0.036229829907631794, 0.0297712811357278, 0.03988485537509201, 0.048109500141186756, 0.06286111091181812, 0.2511888026644351, 0.24810992255575093, 0.24402358187859663, 0.23036495483298214, 0.24943361422814858, 0.23986209029161054, 0.30987712851975313, 0.2597461877557691, 0.27065104071735535, 0.04867112567335119, 0.06820909165177491, 0.051486197879023754, 0.050267593628658735, 0.040101542697310144, 0.04072859991481814, 0.054627671432407765, 0.032862792041209676, 0.05433701213911424, 0.17444936183953996, 0.14371516083960323, 0.12946589274416587, 0.13747013262964392, 0.1536497678561689, 0.12043448571044268, 0.1571485249749106, 0.14464974352974658, 0.14497734269037155, 0.1708139789123294, 0.1586413903771685, 0.148184360253665, 0.18311320569040113, 0.13184737136901292, 0.14460040537159025, 0.16124461403912715, 0.13020743501763787, 0.12221087930504726, 0.10940003030126377, 0.13797734420706276, 0.07769349794558622, 0.12007842078417075, 0.11420923196346888, 0.10861229835131148, 0.11155019915498798, 0.08474833677386073, 0.1158298687992636, 0.17113078066608556, 0.13682451244612515, 0.1399790605469592, 0.1776663608115603, 0.1534014260329143, 0.15712298818013637, 0.18413458292615092, 0.14145368570814953, 0.15261969021389632, 0.18858712514247244, 0.1730069271655862, 0.1803150320316319, 0.1818539411500747, 0.26293646454379493, 0.18380344683991567, 0.1811667670841648, 0.23671128241740202, 0.1842364488710415, 0.1349898987331568, 0.2013768030665769, 0.1037038311940447, 0.24798713808471506, 0.2044653147518426, 0.21455787443950325, 0.15664723064753927, 0.22191957410274776, 0.2486240179329362, 0.1616701842324415, 0.2133346743371719, 0.11696931124601961, 0.24286022215797276, 0.13932911821064042, 0.21664091937944374, 0.12566389461873628, 0.09967917194658626, 0.09438261678021043, 0.18024044736995637, 0.19727048764549104, 0.18890798155942223, 0.18383336497192415, 0.19616852667788487, 0.18595338125752192, 0.19266922079698612, 0.18204855410716192, 0.17826379439861162, 0.05273641752896874, 0.04897218547807569, 0.04648730377882149, 0.05683441875198225, 0.041714007709949374, 0.0345105000343352, 0.0626712221614758, 0.043885034159927505, 0.03633447168162529]}, "mutation_prompt": null}
{"id": "383eced4-be37-499a-b2e2-70f177eceafc", "solution": "import numpy as np\nfrom scipy.optimize import differential_evolution\n\nclass Novel_Metaheuristic_Optimizer:\n    def __init__(self, budget, dim, num_particles=30, max_iterations=1000, inertia_weight=0.7, cognitive_weight=1.4, social_weight=1.4, initial_temperature=100, cooling_rate=0.95):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        def novel_metaheuristic_helper():\n            # PSO initialization\n            # Dynamic adjustment of inertia weight based on exploration-exploitation balance\n            inertia_weight = np.random.uniform(0.4, 0.9)  # Update the inertia weight dynamically\n            # PSO optimization loop\n            # Simulated Annealing (SA) initialization\n            current_solution = np.random.uniform(-5.0, 5.0, size=self.dim)\n            current_cost = func(current_solution)\n            best_solution = current_solution\n            best_cost = current_cost\n            # SA optimization loop\n            for _ in range(int(self.budget*0.7)):  # Spend 70% of the budget on SA\n                proposal_solution = current_solution + np.random.normal(0, 0.1, size=self.dim)\n                proposal_solution = np.clip(proposal_solution, -5.0, 5.0)\n                proposal_cost = func(proposal_solution)\n                if proposal_cost < current_cost or np.random.rand() < np.exp((current_cost - proposal_cost) / 100):\n                    current_solution = proposal_solution\n                    current_cost = proposal_cost\n                    if proposal_cost < best_cost:\n                        best_solution = proposal_solution\n                        best_cost = proposal_cost\n            # Differential Evolution (DE) optimization\n            de_result = differential_evolution(func, bounds=[(-5.0, 5.0)]*self.dim, maxiter=int(self.budget*0.3))  # Spend 30% of the budget on DE\n            if de_result.fun < best_cost:\n                best_solution = de_result.x\n            return best_solution\n\n        return novel_metaheuristic_helper()", "name": "Novel_Metaheuristic_Optimizer", "description": "A novel metaheuristic algorithm combining Particle Swarm Optimization (PSO), Simulated Annealing (SA), and Differential Evolution (DE) for enhanced global exploration and local exploitation in black box optimization.", "configspace": "", "generation": 59, "fitness": 0.12523380220864708, "feedback": "The algorithm Novel_Metaheuristic_Optimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.13 with standard deviation 0.08.", "error": "", "parent_id": "654477e0-4d3a-43cc-b192-f885349aa5cc", "metadata": {"aucs": [0.28973149172408763, 0.3243343389857559, 0.3104742755271688, 0.2744396126493731, 0.2774458722826103, 0.31926094604124955, 0.32791907153315547, 0.28532970889671316, 0.39438341350319917, 0.12397055834086712, 0.04939007869361833, 0.09422327201410585, 0.10233794387117101, 0.05302389454059209, 0.06373305039398669, 0.10284165005858503, 0.03760881254179349, 0.055337717561820154, 0.04150802651161578, 0.05075537176052114, 0.03365153336430138, 0.06215022989286334, 0.07594145050725909, 0.06130811290555027, 0.03230559872452954, 0.04370902295136947, 0.055898722931636224, 0.06004933963820824, 0.03165736385000251, 0.039569653261228965, 0.048837084967940814, 0.044710756467256196, 0.04343610801577891, 0.038382042470198385, 0.07847093880912492, 0.04424343571404732, 0.07010485073933248, 0.05395409943193574, 0.05652898369471915, 0.18589990476891438, 0.23434793106584673, 0.2441959444329561, 0.06058360204785662, 0.05567752275458138, 0.1034196237728231, 0.11045008720817784, 0.11347123230506417, 0.09850697481016701, 0.10384130000541192, 0.08682883545352216, 0.07679838170617093, 0.17127603707329597, 0.21701579972591079, 0.14094456579986547, 0.1333888930933037, 0.12059374628619768, 0.11098051857991864, 0.16772864386416664, 0.11453810633532446, 0.12840024522631288, 0.1353829815106592, 0.17860860852578642, 0.1860939969876131, 0.178884109542495, 0.13983460442365558, 0.1333347659036701, 0.17538025757153897, 0.1696391932397846, 0.20898620252861944, 0.13966198754307735, 0.16681340951018842, 0.15435129490577815, 0.17259608440554453, 0.13418339668712487, 0.17750849571150318, 0.11859589278419369, 0.12904342847833394, 0.15933658299728104, 0.1690927229376843, 0.1206534886484677, 0.12878998561392474, 0.14869086718791347, 0.08064021761645634, 0.043298800064635046, 0.016498715065894287, 0.031701182988220666, 0.038139907104323356, 0.03680419133551005, 0.061512302278703634, 0.04852348645254645, 0.06805068862738084, 0.11292051162170702, 0.0957284223220689, 0.0960733407659844, 0.053021487728643346, 0.05364917729251395, 0.09389110008446189, 0.07903185320198969, 0.06729162809308142, 0.0154848804604657, 9.999999999998899e-05, 9.999999999998899e-05, 0.007595369039079203, 0.0025259036735233442, 0.0003589649686306373, 0.00018239067616576676, 9.999999999998899e-05, 9.999999999998899e-05, 0.02985117739490939, 0.043091437638419716, 0.0378348807083041, 0.039508122018701575, 0.036229829907631794, 0.0297712811357278, 0.03988485537509201, 0.048109500141186756, 0.06286111091181812, 0.2511888026644351, 0.24810992255575093, 0.24402358187859663, 0.23036495483298214, 0.24943361422814858, 0.23986209029161054, 0.30987712851975313, 0.2597461877557691, 0.27065104071735535, 0.04867112567335119, 0.06820909165177491, 0.051486197879023754, 0.050267593628658735, 0.040101542697310144, 0.04072859991481814, 0.054627671432407765, 0.032862792041209676, 0.05433701213911424, 0.17444936183953996, 0.14371516083960323, 0.12946589274416587, 0.13747013262964392, 0.1536497678561689, 0.12043448571044268, 0.1571485249749106, 0.14464974352974658, 0.14497734269037155, 0.1708139789123294, 0.1586413903771685, 0.148184360253665, 0.18311320569040113, 0.13184737136901292, 0.14460040537159025, 0.16124461403912715, 0.13020743501763787, 0.12221087930504726, 0.10940003030126377, 0.13797734420706276, 0.07769349794558622, 0.12007842078417075, 0.11420923196346888, 0.10861229835131148, 0.11155019915498798, 0.08474833677386073, 0.1158298687992636, 0.17113078066608556, 0.13682451244612515, 0.1399790605469592, 0.1776663608115603, 0.1534014260329143, 0.15712298818013637, 0.18413458292615092, 0.14145368570814953, 0.15261969021389632, 0.18858712514247244, 0.1730069271655862, 0.1803150320316319, 0.1818539411500747, 0.26293646454379493, 0.18380344683991567, 0.1811667670841648, 0.23671128241740202, 0.1842364488710415, 0.1349898987331568, 0.2013768030665769, 0.1037038311940447, 0.24798713808471506, 0.2044653147518426, 0.21455787443950325, 0.15664723064753927, 0.22191957410274776, 0.2486240179329362, 0.1616701842324415, 0.2133346743371719, 0.11696931124601961, 0.24286022215797276, 0.13932911821064042, 0.21664091937944374, 0.12566389461873628, 0.09967917194658626, 0.09438261678021043, 0.18024044736995637, 0.19727048764549104, 0.18890798155942223, 0.18383336497192415, 0.19616852667788487, 0.18595338125752192, 0.19266922079698612, 0.18204855410716192, 0.17826379439861162, 0.05273641752896874, 0.04897218547807569, 0.04648730377882149, 0.05683441875198225, 0.041714007709949374, 0.0345105000343352, 0.0626712221614758, 0.043885034159927505, 0.03633447168162529]}, "mutation_prompt": null}
{"id": "cb69ff01-a0a0-4d31-adf9-89ee6e61c445", "solution": "import numpy as np\nfrom scipy.optimize import differential_evolution\n\nclass Hybrid_PSODE_SA_Optimizer:\n    def __init__(self, budget, dim, num_particles=30, max_iterations=1000, inertia_weight=0.7, cognitive_weight=1.4, social_weight=1.4, initial_temperature=100, cooling_rate=0.95):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = num_particles\n        self.max_iterations = max_iterations\n        self.inertia_weight = inertia_weight\n        self.cognitive_weight = cognitive_weight\n        self.social_weight = social_weight\n        self.initial_temperature = initial_temperature\n        self.cooling_rate = cooling_rate\n\n    def __call__(self, func):\n        def hybrid_pso_de_sa_helper():\n            # PSO initialization\n            # Dynamic adjustment of inertia weight based on exploration-exploitation balance\n            self.inertia_weight = np.random.uniform(0.4, 0.9)  # Update the inertia weight dynamically\n            # PSO optimization loop\n            # Simulated Annealing (SA) initialization\n            current_solution = np.random.uniform(-5.0, 5.0, size=self.dim)\n            current_cost = func(current_solution)\n            best_solution = current_solution\n            best_cost = current_cost\n            # SA optimization loop\n            for _ in range(int(self.budget*0.7)):  # Spend 70% of the budget on SA\n                proposal_solution = current_solution + np.random.normal(0, 0.1, size=self.dim)\n                proposal_solution = np.clip(proposal_solution, -5.0, 5.0)\n                proposal_cost = func(proposal_solution)\n                if proposal_cost < current_cost or np.random.rand() < np.exp((current_cost - proposal_cost) / self.initial_temperature):\n                    current_solution = proposal_solution\n                    current_cost = proposal_cost\n                    if proposal_cost < best_cost:\n                        best_solution = proposal_solution\n                        best_cost = proposal_cost\n            # Differential Evolution (DE) optimization\n            de_result = differential_evolution(func, bounds=[(-5.0, 5.0)]*self.dim, maxiter=int(self.budget*0.3))  # Spend 30% of the budget on DE\n            if de_result.fun < best_cost:\n                best_solution = de_result.x\n            return best_solution\n\n        return hybrid_pso_de_sa_helper()", "name": "Hybrid_PSODE_SA_Optimizer", "description": "Enhancing PSO with Simulated Annealing and Differential Evolution for a hybrid metaheuristic approach improving global exploration and local exploitation in optimization.", "configspace": "", "generation": 56, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "654477e0-4d3a-43cc-b192-f885349aa5cc", "metadata": {"aucs": [0.28973149172408763, 0.3243343389857559, 0.3104742755271688, 0.2744396126493731, 0.2774458722826103, 0.31926094604124955, 0.32791907153315547, 0.28532970889671316, 0.39438341350319917, 0.12397055834086712, 0.04939007869361833, 0.09422327201410585, 0.10233794387117101, 0.05302389454059209, 0.06373305039398669, 0.10284165005858503, 0.03760881254179349, 0.055337717561820154, 0.04150802651161578, 0.05075537176052114, 0.03365153336430138, 0.06215022989286334, 0.07594145050725909, 0.06130811290555027, 0.03230559872452954, 0.04370902295136947, 0.055898722931636224, 0.06004933963820824, 0.03165736385000251, 0.039569653261228965, 0.048837084967940814, 0.044710756467256196, 0.04343610801577891, 0.038382042470198385, 0.07847093880912492, 0.04424343571404732, 0.07010485073933248, 0.05395409943193574, 0.05652898369471915, 0.18589990476891438, 0.23434793106584673, 0.2441959444329561, 0.06058360204785662, 0.05567752275458138, 0.1034196237728231, 0.11045008720817784, 0.11347123230506417, 0.09850697481016701, 0.10384130000541192, 0.08682883545352216, 0.07679838170617093, 0.17127603707329597, 0.21701579972591079, 0.14094456579986547, 0.1333888930933037, 0.12059374628619768, 0.11098051857991864, 0.16772864386416664, 0.11453810633532446, 0.12840024522631288, 0.1353829815106592, 0.17860860852578642, 0.1860939969876131, 0.178884109542495, 0.13983460442365558, 0.1333347659036701, 0.17538025757153897, 0.1696391932397846, 0.20898620252861944, 0.13966198754307735, 0.16681340951018842, 0.15435129490577815, 0.17259608440554453, 0.13418339668712487, 0.17750849571150318, 0.11859589278419369, 0.12904342847833394, 0.15933658299728104, 0.1690927229376843, 0.1206534886484677, 0.12878998561392474, 0.14869086718791347, 0.08064021761645634, 0.043298800064635046, 0.016498715065894287, 0.031701182988220666, 0.038139907104323356, 0.03680419133551005, 0.061512302278703634, 0.04852348645254645, 0.06805068862738084, 0.11292051162170702, 0.0957284223220689, 0.0960733407659844, 0.053021487728643346, 0.05364917729251395, 0.09389110008446189, 0.07903185320198969, 0.06729162809308142, 0.0154848804604657, 9.999999999998899e-05, 9.999999999998899e-05, 0.007595369039079203, 0.0025259036735233442, 0.0003589649686306373, 0.00018239067616576676, 9.999999999998899e-05, 9.999999999998899e-05, 0.02985117739490939, 0.043091437638419716, 0.0378348807083041, 0.039508122018701575, 0.036229829907631794, 0.0297712811357278, 0.03988485537509201, 0.048109500141186756, 0.06286111091181812, 0.2511888026644351, 0.24810992255575093, 0.24402358187859663, 0.23036495483298214, 0.24943361422814858, 0.23986209029161054, 0.30987712851975313, 0.2597461877557691, 0.27065104071735535, 0.04867112567335119, 0.06820909165177491, 0.051486197879023754, 0.050267593628658735, 0.040101542697310144, 0.04072859991481814, 0.054627671432407765, 0.032862792041209676, 0.05433701213911424, 0.17444936183953996, 0.14371516083960323, 0.12946589274416587, 0.13747013262964392, 0.1536497678561689, 0.12043448571044268, 0.1571485249749106, 0.14464974352974658, 0.14497734269037155, 0.1708139789123294, 0.1586413903771685, 0.148184360253665, 0.18311320569040113, 0.13184737136901292, 0.14460040537159025, 0.16124461403912715, 0.13020743501763787, 0.12221087930504726, 0.10940003030126377, 0.13797734420706276, 0.07769349794558622, 0.12007842078417075, 0.11420923196346888, 0.10861229835131148, 0.11155019915498798, 0.08474833677386073, 0.1158298687992636, 0.17113078066608556, 0.13682451244612515, 0.1399790605469592, 0.1776663608115603, 0.1534014260329143, 0.15712298818013637, 0.18413458292615092, 0.14145368570814953, 0.15261969021389632, 0.18858712514247244, 0.1730069271655862, 0.1803150320316319, 0.1818539411500747, 0.26293646454379493, 0.18380344683991567, 0.1811667670841648, 0.23671128241740202, 0.1842364488710415, 0.1349898987331568, 0.2013768030665769, 0.1037038311940447, 0.24798713808471506, 0.2044653147518426, 0.21455787443950325, 0.15664723064753927, 0.22191957410274776, 0.2486240179329362, 0.1616701842324415, 0.2133346743371719, 0.11696931124601961, 0.24286022215797276, 0.13932911821064042, 0.21664091937944374, 0.12566389461873628, 0.09967917194658626, 0.09438261678021043, 0.18024044736995637, 0.19727048764549104, 0.18890798155942223, 0.18383336497192415, 0.19616852667788487, 0.18595338125752192, 0.19266922079698612, 0.18204855410716192, 0.17826379439861162, 0.05273641752896874, 0.04897218547807569, 0.04648730377882149, 0.05683441875198225, 0.041714007709949374, 0.0345105000343352, 0.0626712221614758, 0.043885034159927505, 0.03633447168162529]}, "mutation_prompt": null}
{"id": "6f85ca1f-9db9-4e89-929d-6e6718a512d0", "solution": "import numpy as np\nfrom scipy.optimize import differential_evolution\n\nclass Hybrid_PSODE_SA_DE_Optimizer:\n    def __init__(self, budget, dim, num_particles=30, max_iterations=1000, inertia_weight=0.7, cognitive_weight=1.4, social_weight=1.4, initial_temperature=100, cooling_rate=0.95):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = num_particles\n        self.max_iterations = max_iterations\n        self.inertia_weight = inertia_weight\n        self.cognitive_weight = cognitive_weight\n        self.social_weight = social_weight\n        self.initial_temperature = initial_temperature\n        self.cooling_rate = cooling_rate\n\n    def __call__(self, func):\n        def hybrid_pso_de_sa_helper():\n            self.inertia_weight = np.random.uniform(0.4, 0.9)\n            current_solution = np.random.uniform(-5.0, 5.0, size=self.dim)\n            current_cost = func(current_solution)\n            best_solution = current_solution\n            best_cost = current_cost\n            for _ in range(int(self.budget*0.7)):\n                proposal_solution = current_solution + np.random.normal(0, 0.1, size=self.dim)\n                proposal_solution = np.clip(proposal_solution, -5.0, 5.0)\n                proposal_cost = func(proposal_solution)\n                if proposal_cost < current_cost or np.random.rand() < np.exp((current_cost - proposal_cost) / self.initial_temperature):\n                    current_solution = proposal_solution\n                    current_cost = proposal_cost\n                    if proposal_cost < best_cost:\n                        best_solution = proposal_solution\n                        best_cost = proposal_cost\n            de_result = differential_evolution(func, bounds=[(-5.0, 5.0)]*self.dim, maxiter=int(self.budget*0.3))\n            if de_result.fun < best_cost:\n                best_solution = de_result.x\n            return best_solution\n\n        return hybrid_pso_de_sa_helper()", "name": "Hybrid_PSODE_SA_DE_Optimizer", "description": "Integrating Particle Swarm Optimization (PSO), Simulated Annealing (SA), and Differential Evolution (DE) in a hybrid metaheuristic to balance exploration and exploitation for black box optimization.", "configspace": "", "generation": 61, "fitness": 0.12523380220864708, "feedback": "The algorithm Hybrid_PSODE_SA_DE_Optimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.13 with standard deviation 0.08.", "error": "", "parent_id": "654477e0-4d3a-43cc-b192-f885349aa5cc", "metadata": {"aucs": [0.28973149172408763, 0.3243343389857559, 0.3104742755271688, 0.2744396126493731, 0.2774458722826103, 0.31926094604124955, 0.32791907153315547, 0.28532970889671316, 0.39438341350319917, 0.12397055834086712, 0.04939007869361833, 0.09422327201410585, 0.10233794387117101, 0.05302389454059209, 0.06373305039398669, 0.10284165005858503, 0.03760881254179349, 0.055337717561820154, 0.04150802651161578, 0.05075537176052114, 0.03365153336430138, 0.06215022989286334, 0.07594145050725909, 0.06130811290555027, 0.03230559872452954, 0.04370902295136947, 0.055898722931636224, 0.06004933963820824, 0.03165736385000251, 0.039569653261228965, 0.048837084967940814, 0.044710756467256196, 0.04343610801577891, 0.038382042470198385, 0.07847093880912492, 0.04424343571404732, 0.07010485073933248, 0.05395409943193574, 0.05652898369471915, 0.18589990476891438, 0.23434793106584673, 0.2441959444329561, 0.06058360204785662, 0.05567752275458138, 0.1034196237728231, 0.11045008720817784, 0.11347123230506417, 0.09850697481016701, 0.10384130000541192, 0.08682883545352216, 0.07679838170617093, 0.17127603707329597, 0.21701579972591079, 0.14094456579986547, 0.1333888930933037, 0.12059374628619768, 0.11098051857991864, 0.16772864386416664, 0.11453810633532446, 0.12840024522631288, 0.1353829815106592, 0.17860860852578642, 0.1860939969876131, 0.178884109542495, 0.13983460442365558, 0.1333347659036701, 0.17538025757153897, 0.1696391932397846, 0.20898620252861944, 0.13966198754307735, 0.16681340951018842, 0.15435129490577815, 0.17259608440554453, 0.13418339668712487, 0.17750849571150318, 0.11859589278419369, 0.12904342847833394, 0.15933658299728104, 0.1690927229376843, 0.1206534886484677, 0.12878998561392474, 0.14869086718791347, 0.08064021761645634, 0.043298800064635046, 0.016498715065894287, 0.031701182988220666, 0.038139907104323356, 0.03680419133551005, 0.061512302278703634, 0.04852348645254645, 0.06805068862738084, 0.11292051162170702, 0.0957284223220689, 0.0960733407659844, 0.053021487728643346, 0.05364917729251395, 0.09389110008446189, 0.07903185320198969, 0.06729162809308142, 0.0154848804604657, 9.999999999998899e-05, 9.999999999998899e-05, 0.007595369039079203, 0.0025259036735233442, 0.0003589649686306373, 0.00018239067616576676, 9.999999999998899e-05, 9.999999999998899e-05, 0.02985117739490939, 0.043091437638419716, 0.0378348807083041, 0.039508122018701575, 0.036229829907631794, 0.0297712811357278, 0.03988485537509201, 0.048109500141186756, 0.06286111091181812, 0.2511888026644351, 0.24810992255575093, 0.24402358187859663, 0.23036495483298214, 0.24943361422814858, 0.23986209029161054, 0.30987712851975313, 0.2597461877557691, 0.27065104071735535, 0.04867112567335119, 0.06820909165177491, 0.051486197879023754, 0.050267593628658735, 0.040101542697310144, 0.04072859991481814, 0.054627671432407765, 0.032862792041209676, 0.05433701213911424, 0.17444936183953996, 0.14371516083960323, 0.12946589274416587, 0.13747013262964392, 0.1536497678561689, 0.12043448571044268, 0.1571485249749106, 0.14464974352974658, 0.14497734269037155, 0.1708139789123294, 0.1586413903771685, 0.148184360253665, 0.18311320569040113, 0.13184737136901292, 0.14460040537159025, 0.16124461403912715, 0.13020743501763787, 0.12221087930504726, 0.10940003030126377, 0.13797734420706276, 0.07769349794558622, 0.12007842078417075, 0.11420923196346888, 0.10861229835131148, 0.11155019915498798, 0.08474833677386073, 0.1158298687992636, 0.17113078066608556, 0.13682451244612515, 0.1399790605469592, 0.1776663608115603, 0.1534014260329143, 0.15712298818013637, 0.18413458292615092, 0.14145368570814953, 0.15261969021389632, 0.18858712514247244, 0.1730069271655862, 0.1803150320316319, 0.1818539411500747, 0.26293646454379493, 0.18380344683991567, 0.1811667670841648, 0.23671128241740202, 0.1842364488710415, 0.1349898987331568, 0.2013768030665769, 0.1037038311940447, 0.24798713808471506, 0.2044653147518426, 0.21455787443950325, 0.15664723064753927, 0.22191957410274776, 0.2486240179329362, 0.1616701842324415, 0.2133346743371719, 0.11696931124601961, 0.24286022215797276, 0.13932911821064042, 0.21664091937944374, 0.12566389461873628, 0.09967917194658626, 0.09438261678021043, 0.18024044736995637, 0.19727048764549104, 0.18890798155942223, 0.18383336497192415, 0.19616852667788487, 0.18595338125752192, 0.19266922079698612, 0.18204855410716192, 0.17826379439861162, 0.05273641752896874, 0.04897218547807569, 0.04648730377882149, 0.05683441875198225, 0.041714007709949374, 0.0345105000343352, 0.0626712221614758, 0.043885034159927505, 0.03633447168162529]}, "mutation_prompt": null}
{"id": "cc8a9cf4-7b99-47dd-a35c-89337bd394fb", "solution": "import numpy as np\nfrom scipy.optimize import differential_evolution\n\nclass NovelHybridMetaheuristic:\n    def __init__(self, budget, dim, num_particles=30, max_iterations=1000, inertia_weight=0.7, cognitive_weight=1.4, social_weight=1.4, initial_temperature=100, cooling_rate=0.95):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = num_particles\n        self.max_iterations = max_iterations\n        self.inertia_weight = inertia_weight\n        self.cognitive_weight = cognitive_weight\n        self.social_weight = social_weight\n        self.initial_temperature = initial_temperature\n        self.cooling_rate = cooling_rate\n\n    def __call__(self, func):\n        def novel_hybrid_metaheuristic_helper():\n            # PSO initialization\n            # Dynamic adjustment of inertia weight based on exploration-exploitation balance\n            self.inertia_weight = np.random.uniform(0.4, 0.9)  # Update the inertia weight dynamically\n            # PSO optimization loop\n            # Simulated Annealing (SA) initialization\n            current_solution = np.random.uniform(-5.0, 5.0, size=self.dim)\n            current_cost = func(current_solution)\n            best_solution = current_solution\n            best_cost = current_cost\n            # SA optimization loop\n            for _ in range(int(self.budget*0.7)):  # Spend 70% of the budget on SA\n                proposal_solution = current_solution + np.random.normal(0, 0.1, size=self.dim)\n                proposal_solution = np.clip(proposal_solution, -5.0, 5.0)\n                proposal_cost = func(proposal_solution)\n                if proposal_cost < current_cost or np.random.rand() < np.exp((current_cost - proposal_cost) / self.initial_temperature):\n                    current_solution = proposal_solution\n                    current_cost = proposal_cost\n                    if proposal_cost < best_cost:\n                        best_solution = proposal_solution\n                        best_cost = proposal_cost\n            # Differential Evolution (DE) optimization\n            de_result = differential_evolution(func, bounds=[(-5.0, 5.0)]*self.dim, maxiter=int(self.budget*0.3))  # Spend 30% of the budget on DE\n            if de_result.fun < best_cost:\n                best_solution = de_result.x\n            return best_solution\n\n        return novel_hybrid_metaheuristic_helper()", "name": "NovelHybridMetaheuristic", "description": "Hybridizing Particle Swarm Optimization (PSO) with Simulated Annealing (SA) and Differential Evolution (DE) to achieve a balance between exploration and exploitation for global optimization.", "configspace": "", "generation": 62, "fitness": 0.12523380220864708, "feedback": "The algorithm NovelHybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.13 with standard deviation 0.08.", "error": "", "parent_id": "654477e0-4d3a-43cc-b192-f885349aa5cc", "metadata": {"aucs": [0.28973149172408763, 0.3243343389857559, 0.3104742755271688, 0.2744396126493731, 0.2774458722826103, 0.31926094604124955, 0.32791907153315547, 0.28532970889671316, 0.39438341350319917, 0.12397055834086712, 0.04939007869361833, 0.09422327201410585, 0.10233794387117101, 0.05302389454059209, 0.06373305039398669, 0.10284165005858503, 0.03760881254179349, 0.055337717561820154, 0.04150802651161578, 0.05075537176052114, 0.03365153336430138, 0.06215022989286334, 0.07594145050725909, 0.06130811290555027, 0.03230559872452954, 0.04370902295136947, 0.055898722931636224, 0.06004933963820824, 0.03165736385000251, 0.039569653261228965, 0.048837084967940814, 0.044710756467256196, 0.04343610801577891, 0.038382042470198385, 0.07847093880912492, 0.04424343571404732, 0.07010485073933248, 0.05395409943193574, 0.05652898369471915, 0.18589990476891438, 0.23434793106584673, 0.2441959444329561, 0.06058360204785662, 0.05567752275458138, 0.1034196237728231, 0.11045008720817784, 0.11347123230506417, 0.09850697481016701, 0.10384130000541192, 0.08682883545352216, 0.07679838170617093, 0.17127603707329597, 0.21701579972591079, 0.14094456579986547, 0.1333888930933037, 0.12059374628619768, 0.11098051857991864, 0.16772864386416664, 0.11453810633532446, 0.12840024522631288, 0.1353829815106592, 0.17860860852578642, 0.1860939969876131, 0.178884109542495, 0.13983460442365558, 0.1333347659036701, 0.17538025757153897, 0.1696391932397846, 0.20898620252861944, 0.13966198754307735, 0.16681340951018842, 0.15435129490577815, 0.17259608440554453, 0.13418339668712487, 0.17750849571150318, 0.11859589278419369, 0.12904342847833394, 0.15933658299728104, 0.1690927229376843, 0.1206534886484677, 0.12878998561392474, 0.14869086718791347, 0.08064021761645634, 0.043298800064635046, 0.016498715065894287, 0.031701182988220666, 0.038139907104323356, 0.03680419133551005, 0.061512302278703634, 0.04852348645254645, 0.06805068862738084, 0.11292051162170702, 0.0957284223220689, 0.0960733407659844, 0.053021487728643346, 0.05364917729251395, 0.09389110008446189, 0.07903185320198969, 0.06729162809308142, 0.0154848804604657, 9.999999999998899e-05, 9.999999999998899e-05, 0.007595369039079203, 0.0025259036735233442, 0.0003589649686306373, 0.00018239067616576676, 9.999999999998899e-05, 9.999999999998899e-05, 0.02985117739490939, 0.043091437638419716, 0.0378348807083041, 0.039508122018701575, 0.036229829907631794, 0.0297712811357278, 0.03988485537509201, 0.048109500141186756, 0.06286111091181812, 0.2511888026644351, 0.24810992255575093, 0.24402358187859663, 0.23036495483298214, 0.24943361422814858, 0.23986209029161054, 0.30987712851975313, 0.2597461877557691, 0.27065104071735535, 0.04867112567335119, 0.06820909165177491, 0.051486197879023754, 0.050267593628658735, 0.040101542697310144, 0.04072859991481814, 0.054627671432407765, 0.032862792041209676, 0.05433701213911424, 0.17444936183953996, 0.14371516083960323, 0.12946589274416587, 0.13747013262964392, 0.1536497678561689, 0.12043448571044268, 0.1571485249749106, 0.14464974352974658, 0.14497734269037155, 0.1708139789123294, 0.1586413903771685, 0.148184360253665, 0.18311320569040113, 0.13184737136901292, 0.14460040537159025, 0.16124461403912715, 0.13020743501763787, 0.12221087930504726, 0.10940003030126377, 0.13797734420706276, 0.07769349794558622, 0.12007842078417075, 0.11420923196346888, 0.10861229835131148, 0.11155019915498798, 0.08474833677386073, 0.1158298687992636, 0.17113078066608556, 0.13682451244612515, 0.1399790605469592, 0.1776663608115603, 0.1534014260329143, 0.15712298818013637, 0.18413458292615092, 0.14145368570814953, 0.15261969021389632, 0.18858712514247244, 0.1730069271655862, 0.1803150320316319, 0.1818539411500747, 0.26293646454379493, 0.18380344683991567, 0.1811667670841648, 0.23671128241740202, 0.1842364488710415, 0.1349898987331568, 0.2013768030665769, 0.1037038311940447, 0.24798713808471506, 0.2044653147518426, 0.21455787443950325, 0.15664723064753927, 0.22191957410274776, 0.2486240179329362, 0.1616701842324415, 0.2133346743371719, 0.11696931124601961, 0.24286022215797276, 0.13932911821064042, 0.21664091937944374, 0.12566389461873628, 0.09967917194658626, 0.09438261678021043, 0.18024044736995637, 0.19727048764549104, 0.18890798155942223, 0.18383336497192415, 0.19616852667788487, 0.18595338125752192, 0.19266922079698612, 0.18204855410716192, 0.17826379439861162, 0.05273641752896874, 0.04897218547807569, 0.04648730377882149, 0.05683441875198225, 0.041714007709949374, 0.0345105000343352, 0.0626712221614758, 0.043885034159927505, 0.03633447168162529]}, "mutation_prompt": null}
{"id": "193261a1-a812-4c1a-bfce-58a9df8ed1d3", "solution": "import numpy as np\nfrom scipy.optimize import differential_evolution\n\nclass Novel_Metaheuristic_Optimizer:\n    def __init__(self, budget, dim, num_particles=30, max_iterations=1000, inertia_weight=0.7, cognitive_weight=1.4, social_weight=1.4, initial_temperature=100, cooling_rate=0.95):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = num_particles\n        self.max_iterations = max_iterations\n        self.inertia_weight = inertia_weight\n        self.cognitive_weight = cognitive_weight\n        self.social_weight = social_weight\n        self.initial_temperature = initial_temperature\n        self.cooling_rate = cooling_rate\n\n    def __call__(self, func):\n        def novel_metaheuristic_helper():\n            # PSO initialization with dynamic inertia weight\n            self.inertia_weight = np.random.uniform(0.4, 0.9)\n            # PSO optimization loop\n            # Simulated Annealing (SA) initialization\n            current_solution = np.random.uniform(-5.0, 5.0, size=self.dim)\n            current_cost = func(current_solution)\n            best_solution = current_solution\n            best_cost = current_cost\n            # SA optimization loop\n            for _ in range(int(self.budget*0.7)):  # Spend 70% budget on SA\n                proposal_solution = current_solution + np.random.normal(0, 0.1, size=self.dim)\n                proposal_solution = np.clip(proposal_solution, -5.0, 5.0)\n                proposal_cost = func(proposal_solution)\n                if proposal_cost < current_cost or np.random.rand() < np.exp((current_cost - proposal_cost) / self.initial_temperature):\n                    current_solution = proposal_solution\n                    current_cost = proposal_cost\n                    if proposal_cost < best_cost:\n                        best_solution = proposal_solution\n                        best_cost = proposal_cost\n            # Differential Evolution (DE) optimization\n            de_result = differential_evolution(func, bounds=[(-5.0, 5.0)]*self.dim, maxiter=int(self.budget*0.3))  # Spend 30% budget on DE\n            if de_result.fun < best_cost:\n                best_solution = de_result.x\n            return best_solution\n\n        return novel_metaheuristic_helper()", "name": "Novel_Metaheuristic_Optimizer", "description": "A novel algorithm combining Particle Swarm Optimization (PSO), Simulated Annealing (SA), and Differential Evolution (DE) for black box optimization across BBOB test functions.", "configspace": "", "generation": 63, "fitness": 0.12523380220864708, "feedback": "The algorithm Novel_Metaheuristic_Optimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.13 with standard deviation 0.08.", "error": "", "parent_id": "654477e0-4d3a-43cc-b192-f885349aa5cc", "metadata": {"aucs": [0.28973149172408763, 0.3243343389857559, 0.3104742755271688, 0.2744396126493731, 0.2774458722826103, 0.31926094604124955, 0.32791907153315547, 0.28532970889671316, 0.39438341350319917, 0.12397055834086712, 0.04939007869361833, 0.09422327201410585, 0.10233794387117101, 0.05302389454059209, 0.06373305039398669, 0.10284165005858503, 0.03760881254179349, 0.055337717561820154, 0.04150802651161578, 0.05075537176052114, 0.03365153336430138, 0.06215022989286334, 0.07594145050725909, 0.06130811290555027, 0.03230559872452954, 0.04370902295136947, 0.055898722931636224, 0.06004933963820824, 0.03165736385000251, 0.039569653261228965, 0.048837084967940814, 0.044710756467256196, 0.04343610801577891, 0.038382042470198385, 0.07847093880912492, 0.04424343571404732, 0.07010485073933248, 0.05395409943193574, 0.05652898369471915, 0.18589990476891438, 0.23434793106584673, 0.2441959444329561, 0.06058360204785662, 0.05567752275458138, 0.1034196237728231, 0.11045008720817784, 0.11347123230506417, 0.09850697481016701, 0.10384130000541192, 0.08682883545352216, 0.07679838170617093, 0.17127603707329597, 0.21701579972591079, 0.14094456579986547, 0.1333888930933037, 0.12059374628619768, 0.11098051857991864, 0.16772864386416664, 0.11453810633532446, 0.12840024522631288, 0.1353829815106592, 0.17860860852578642, 0.1860939969876131, 0.178884109542495, 0.13983460442365558, 0.1333347659036701, 0.17538025757153897, 0.1696391932397846, 0.20898620252861944, 0.13966198754307735, 0.16681340951018842, 0.15435129490577815, 0.17259608440554453, 0.13418339668712487, 0.17750849571150318, 0.11859589278419369, 0.12904342847833394, 0.15933658299728104, 0.1690927229376843, 0.1206534886484677, 0.12878998561392474, 0.14869086718791347, 0.08064021761645634, 0.043298800064635046, 0.016498715065894287, 0.031701182988220666, 0.038139907104323356, 0.03680419133551005, 0.061512302278703634, 0.04852348645254645, 0.06805068862738084, 0.11292051162170702, 0.0957284223220689, 0.0960733407659844, 0.053021487728643346, 0.05364917729251395, 0.09389110008446189, 0.07903185320198969, 0.06729162809308142, 0.0154848804604657, 9.999999999998899e-05, 9.999999999998899e-05, 0.007595369039079203, 0.0025259036735233442, 0.0003589649686306373, 0.00018239067616576676, 9.999999999998899e-05, 9.999999999998899e-05, 0.02985117739490939, 0.043091437638419716, 0.0378348807083041, 0.039508122018701575, 0.036229829907631794, 0.0297712811357278, 0.03988485537509201, 0.048109500141186756, 0.06286111091181812, 0.2511888026644351, 0.24810992255575093, 0.24402358187859663, 0.23036495483298214, 0.24943361422814858, 0.23986209029161054, 0.30987712851975313, 0.2597461877557691, 0.27065104071735535, 0.04867112567335119, 0.06820909165177491, 0.051486197879023754, 0.050267593628658735, 0.040101542697310144, 0.04072859991481814, 0.054627671432407765, 0.032862792041209676, 0.05433701213911424, 0.17444936183953996, 0.14371516083960323, 0.12946589274416587, 0.13747013262964392, 0.1536497678561689, 0.12043448571044268, 0.1571485249749106, 0.14464974352974658, 0.14497734269037155, 0.1708139789123294, 0.1586413903771685, 0.148184360253665, 0.18311320569040113, 0.13184737136901292, 0.14460040537159025, 0.16124461403912715, 0.13020743501763787, 0.12221087930504726, 0.10940003030126377, 0.13797734420706276, 0.07769349794558622, 0.12007842078417075, 0.11420923196346888, 0.10861229835131148, 0.11155019915498798, 0.08474833677386073, 0.1158298687992636, 0.17113078066608556, 0.13682451244612515, 0.1399790605469592, 0.1776663608115603, 0.1534014260329143, 0.15712298818013637, 0.18413458292615092, 0.14145368570814953, 0.15261969021389632, 0.18858712514247244, 0.1730069271655862, 0.1803150320316319, 0.1818539411500747, 0.26293646454379493, 0.18380344683991567, 0.1811667670841648, 0.23671128241740202, 0.1842364488710415, 0.1349898987331568, 0.2013768030665769, 0.1037038311940447, 0.24798713808471506, 0.2044653147518426, 0.21455787443950325, 0.15664723064753927, 0.22191957410274776, 0.2486240179329362, 0.1616701842324415, 0.2133346743371719, 0.11696931124601961, 0.24286022215797276, 0.13932911821064042, 0.21664091937944374, 0.12566389461873628, 0.09967917194658626, 0.09438261678021043, 0.18024044736995637, 0.19727048764549104, 0.18890798155942223, 0.18383336497192415, 0.19616852667788487, 0.18595338125752192, 0.19266922079698612, 0.18204855410716192, 0.17826379439861162, 0.05273641752896874, 0.04897218547807569, 0.04648730377882149, 0.05683441875198225, 0.041714007709949374, 0.0345105000343352, 0.0626712221614758, 0.043885034159927505, 0.03633447168162529]}, "mutation_prompt": null}
{"id": "3351e87a-39bd-4b81-8169-e9eab76db944", "solution": "import numpy as np\nfrom scipy.optimize import differential_evolution\n\nclass Hybrid_PSODE_SA_Optimizer:\n    def __init__(self, budget, dim, num_particles=30, max_iterations=1000, inertia_weight=0.7, cognitive_weight=1.4, social_weight=1.4, initial_temperature=100, cooling_rate=0.95):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = num_particles\n        self.max_iterations = max_iterations\n        self.inertia_weight = inertia_weight\n        self.cognitive_weight = cognitive_weight\n        self.social_weight = social_weight\n        self.initial_temperature = initial_temperature\n        self.cooling_rate = cooling_rate\n\n    def __call__(self, func):\n        def hybrid_pso_de_sa_helper():\n            # PSO initialization\n            # Dynamic adjustment of inertia weight based on exploration-exploitation balance\n            self.inertia_weight = np.random.uniform(0.4, 0.9)  # Update the inertia weight dynamically\n            # PSO optimization loop\n            # Simulated Annealing (SA) initialization\n            current_solution = np.random.uniform(-5.0, 5.0, size=self.dim)\n            current_cost = func(current_solution)\n            best_solution = current_solution\n            best_cost = current_cost\n            # SA optimization loop\n            for _ in range(int(self.budget*0.7)):  # Spend 70% of the budget on SA\n                proposal_solution = current_solution + np.random.normal(0, 0.1, size=self.dim)\n                proposal_solution = np.clip(proposal_solution, -5.0, 5.0)\n                proposal_cost = func(proposal_solution)\n                if proposal_cost < current_cost or np.random.rand() < np.exp((current_cost - proposal_cost) / self.initial_temperature):\n                    current_solution = proposal_solution\n                    current_cost = proposal_cost\n                    if proposal_cost < best_cost:\n                        best_solution = proposal_solution\n                        best_cost = proposal_cost\n            # Differential Evolution (DE) optimization\n            de_result = differential_evolution(func, bounds=[(-5.0, 5.0)]*self.dim, maxiter=int(self.budget*0.3))  # Spend 30% of the budget on DE\n            if de_result.fun < best_cost:\n                best_solution = de_result.x\n            return best_solution\n\n        return hybrid_pso_de_sa_helper()", "name": "Hybrid_PSODE_SA_Optimizer", "description": "Enhancing PSO with Simulated Annealing and Differential Evolution for a hybrid metaheuristic approach improving global exploration and local exploitation in optimization.", "configspace": "", "generation": 56, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "654477e0-4d3a-43cc-b192-f885349aa5cc", "metadata": {"aucs": [0.28973149172408763, 0.3243343389857559, 0.3104742755271688, 0.2744396126493731, 0.2774458722826103, 0.31926094604124955, 0.32791907153315547, 0.28532970889671316, 0.39438341350319917, 0.12397055834086712, 0.04939007869361833, 0.09422327201410585, 0.10233794387117101, 0.05302389454059209, 0.06373305039398669, 0.10284165005858503, 0.03760881254179349, 0.055337717561820154, 0.04150802651161578, 0.05075537176052114, 0.03365153336430138, 0.06215022989286334, 0.07594145050725909, 0.06130811290555027, 0.03230559872452954, 0.04370902295136947, 0.055898722931636224, 0.06004933963820824, 0.03165736385000251, 0.039569653261228965, 0.048837084967940814, 0.044710756467256196, 0.04343610801577891, 0.038382042470198385, 0.07847093880912492, 0.04424343571404732, 0.07010485073933248, 0.05395409943193574, 0.05652898369471915, 0.18589990476891438, 0.23434793106584673, 0.2441959444329561, 0.06058360204785662, 0.05567752275458138, 0.1034196237728231, 0.11045008720817784, 0.11347123230506417, 0.09850697481016701, 0.10384130000541192, 0.08682883545352216, 0.07679838170617093, 0.17127603707329597, 0.21701579972591079, 0.14094456579986547, 0.1333888930933037, 0.12059374628619768, 0.11098051857991864, 0.16772864386416664, 0.11453810633532446, 0.12840024522631288, 0.1353829815106592, 0.17860860852578642, 0.1860939969876131, 0.178884109542495, 0.13983460442365558, 0.1333347659036701, 0.17538025757153897, 0.1696391932397846, 0.20898620252861944, 0.13966198754307735, 0.16681340951018842, 0.15435129490577815, 0.17259608440554453, 0.13418339668712487, 0.17750849571150318, 0.11859589278419369, 0.12904342847833394, 0.15933658299728104, 0.1690927229376843, 0.1206534886484677, 0.12878998561392474, 0.14869086718791347, 0.08064021761645634, 0.043298800064635046, 0.016498715065894287, 0.031701182988220666, 0.038139907104323356, 0.03680419133551005, 0.061512302278703634, 0.04852348645254645, 0.06805068862738084, 0.11292051162170702, 0.0957284223220689, 0.0960733407659844, 0.053021487728643346, 0.05364917729251395, 0.09389110008446189, 0.07903185320198969, 0.06729162809308142, 0.0154848804604657, 9.999999999998899e-05, 9.999999999998899e-05, 0.007595369039079203, 0.0025259036735233442, 0.0003589649686306373, 0.00018239067616576676, 9.999999999998899e-05, 9.999999999998899e-05, 0.02985117739490939, 0.043091437638419716, 0.0378348807083041, 0.039508122018701575, 0.036229829907631794, 0.0297712811357278, 0.03988485537509201, 0.048109500141186756, 0.06286111091181812, 0.2511888026644351, 0.24810992255575093, 0.24402358187859663, 0.23036495483298214, 0.24943361422814858, 0.23986209029161054, 0.30987712851975313, 0.2597461877557691, 0.27065104071735535, 0.04867112567335119, 0.06820909165177491, 0.051486197879023754, 0.050267593628658735, 0.040101542697310144, 0.04072859991481814, 0.054627671432407765, 0.032862792041209676, 0.05433701213911424, 0.17444936183953996, 0.14371516083960323, 0.12946589274416587, 0.13747013262964392, 0.1536497678561689, 0.12043448571044268, 0.1571485249749106, 0.14464974352974658, 0.14497734269037155, 0.1708139789123294, 0.1586413903771685, 0.148184360253665, 0.18311320569040113, 0.13184737136901292, 0.14460040537159025, 0.16124461403912715, 0.13020743501763787, 0.12221087930504726, 0.10940003030126377, 0.13797734420706276, 0.07769349794558622, 0.12007842078417075, 0.11420923196346888, 0.10861229835131148, 0.11155019915498798, 0.08474833677386073, 0.1158298687992636, 0.17113078066608556, 0.13682451244612515, 0.1399790605469592, 0.1776663608115603, 0.1534014260329143, 0.15712298818013637, 0.18413458292615092, 0.14145368570814953, 0.15261969021389632, 0.18858712514247244, 0.1730069271655862, 0.1803150320316319, 0.1818539411500747, 0.26293646454379493, 0.18380344683991567, 0.1811667670841648, 0.23671128241740202, 0.1842364488710415, 0.1349898987331568, 0.2013768030665769, 0.1037038311940447, 0.24798713808471506, 0.2044653147518426, 0.21455787443950325, 0.15664723064753927, 0.22191957410274776, 0.2486240179329362, 0.1616701842324415, 0.2133346743371719, 0.11696931124601961, 0.24286022215797276, 0.13932911821064042, 0.21664091937944374, 0.12566389461873628, 0.09967917194658626, 0.09438261678021043, 0.18024044736995637, 0.19727048764549104, 0.18890798155942223, 0.18383336497192415, 0.19616852667788487, 0.18595338125752192, 0.19266922079698612, 0.18204855410716192, 0.17826379439861162, 0.05273641752896874, 0.04897218547807569, 0.04648730377882149, 0.05683441875198225, 0.041714007709949374, 0.0345105000343352, 0.0626712221614758, 0.043885034159927505, 0.03633447168162529]}, "mutation_prompt": null}
{"id": "43cba62d-8a6a-4a20-9069-f7a385c33e14", "solution": "# import numpy as np\nfrom scipy.optimize import differential_evolution\n\nclass Hybrid_PSO_SA_DE_Optimizer:\n    def __init__(self, budget, dim, num_particles=30, max_iterations=1000, inertia_weight=0.7, cognitive_weight=1.4, social_weight=1.4, initial_temperature=100, cooling_rate=0.95):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = num_particles\n        self.max_iterations = max_iterations\n        self.inertia_weight = inertia_weight\n        self.cognitive_weight = cognitive_weight\n        self.social_weight = social_weight\n        self.initial_temperature = initial_temperature\n        self.cooling_rate = cooling_rate\n\n    def __call__(self, func):\n        def hybrid_pso_sa_de_helper():\n            # PSO initialization\n            self.inertia_weight = np.random.uniform(0.4, 0.9)\n            # PSO optimization\n            # SA initialization\n            current_solution = np.random.uniform(-5.0, 5.0, size=self.dim)\n            current_cost = func(current_solution)\n            best_solution = current_solution\n            best_cost = current_cost\n            # SA optimization\n            for _ in range(int(self.budget*0.7)):\n                proposal_solution = current_solution + np.random.normal(0, 0.1, size=self.dim)\n                proposal_solution = np.clip(proposal_solution, -5.0, 5.0)\n                proposal_cost = func(proposal_solution)\n                if proposal_cost < current_cost or np.random.rand() < np.exp((current_cost - proposal_cost) / self.initial_temperature):\n                    current_solution = proposal_solution\n                    current_cost = proposal_cost\n                    if proposal_cost < best_cost:\n                        best_solution = proposal_solution\n                        best_cost = proposal_cost\n            # DE optimization\n            de_result = differential_evolution(func, bounds=[(-5.0, 5.0)]*self.dim, maxiter=int(self.budget*0.3))\n            if de_result.fun < best_cost:\n                best_solution = de_result.x\n            return best_solution\n\n        return hybrid_pso_sa_de_helper()", "name": "Hybrid_PSO_SA_DE_Optimizer", "description": "A novel algorithm combining Particle Swarm Optimization (PSO), Simulated Annealing (SA), and Differential Evolution (DE) for hybrid exploration and exploitation in optimization with dynamic parameter adjustments.", "configspace": "", "generation": 65, "fitness": 0.12523380220864708, "feedback": "The algorithm Hybrid_PSO_SA_DE_Optimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.13 with standard deviation 0.08.", "error": "", "parent_id": "654477e0-4d3a-43cc-b192-f885349aa5cc", "metadata": {"aucs": [0.28973149172408763, 0.3243343389857559, 0.3104742755271688, 0.2744396126493731, 0.2774458722826103, 0.31926094604124955, 0.32791907153315547, 0.28532970889671316, 0.39438341350319917, 0.12397055834086712, 0.04939007869361833, 0.09422327201410585, 0.10233794387117101, 0.05302389454059209, 0.06373305039398669, 0.10284165005858503, 0.03760881254179349, 0.055337717561820154, 0.04150802651161578, 0.05075537176052114, 0.03365153336430138, 0.06215022989286334, 0.07594145050725909, 0.06130811290555027, 0.03230559872452954, 0.04370902295136947, 0.055898722931636224, 0.06004933963820824, 0.03165736385000251, 0.039569653261228965, 0.048837084967940814, 0.044710756467256196, 0.04343610801577891, 0.038382042470198385, 0.07847093880912492, 0.04424343571404732, 0.07010485073933248, 0.05395409943193574, 0.05652898369471915, 0.18589990476891438, 0.23434793106584673, 0.2441959444329561, 0.06058360204785662, 0.05567752275458138, 0.1034196237728231, 0.11045008720817784, 0.11347123230506417, 0.09850697481016701, 0.10384130000541192, 0.08682883545352216, 0.07679838170617093, 0.17127603707329597, 0.21701579972591079, 0.14094456579986547, 0.1333888930933037, 0.12059374628619768, 0.11098051857991864, 0.16772864386416664, 0.11453810633532446, 0.12840024522631288, 0.1353829815106592, 0.17860860852578642, 0.1860939969876131, 0.178884109542495, 0.13983460442365558, 0.1333347659036701, 0.17538025757153897, 0.1696391932397846, 0.20898620252861944, 0.13966198754307735, 0.16681340951018842, 0.15435129490577815, 0.17259608440554453, 0.13418339668712487, 0.17750849571150318, 0.11859589278419369, 0.12904342847833394, 0.15933658299728104, 0.1690927229376843, 0.1206534886484677, 0.12878998561392474, 0.14869086718791347, 0.08064021761645634, 0.043298800064635046, 0.016498715065894287, 0.031701182988220666, 0.038139907104323356, 0.03680419133551005, 0.061512302278703634, 0.04852348645254645, 0.06805068862738084, 0.11292051162170702, 0.0957284223220689, 0.0960733407659844, 0.053021487728643346, 0.05364917729251395, 0.09389110008446189, 0.07903185320198969, 0.06729162809308142, 0.0154848804604657, 9.999999999998899e-05, 9.999999999998899e-05, 0.007595369039079203, 0.0025259036735233442, 0.0003589649686306373, 0.00018239067616576676, 9.999999999998899e-05, 9.999999999998899e-05, 0.02985117739490939, 0.043091437638419716, 0.0378348807083041, 0.039508122018701575, 0.036229829907631794, 0.0297712811357278, 0.03988485537509201, 0.048109500141186756, 0.06286111091181812, 0.2511888026644351, 0.24810992255575093, 0.24402358187859663, 0.23036495483298214, 0.24943361422814858, 0.23986209029161054, 0.30987712851975313, 0.2597461877557691, 0.27065104071735535, 0.04867112567335119, 0.06820909165177491, 0.051486197879023754, 0.050267593628658735, 0.040101542697310144, 0.04072859991481814, 0.054627671432407765, 0.032862792041209676, 0.05433701213911424, 0.17444936183953996, 0.14371516083960323, 0.12946589274416587, 0.13747013262964392, 0.1536497678561689, 0.12043448571044268, 0.1571485249749106, 0.14464974352974658, 0.14497734269037155, 0.1708139789123294, 0.1586413903771685, 0.148184360253665, 0.18311320569040113, 0.13184737136901292, 0.14460040537159025, 0.16124461403912715, 0.13020743501763787, 0.12221087930504726, 0.10940003030126377, 0.13797734420706276, 0.07769349794558622, 0.12007842078417075, 0.11420923196346888, 0.10861229835131148, 0.11155019915498798, 0.08474833677386073, 0.1158298687992636, 0.17113078066608556, 0.13682451244612515, 0.1399790605469592, 0.1776663608115603, 0.1534014260329143, 0.15712298818013637, 0.18413458292615092, 0.14145368570814953, 0.15261969021389632, 0.18858712514247244, 0.1730069271655862, 0.1803150320316319, 0.1818539411500747, 0.26293646454379493, 0.18380344683991567, 0.1811667670841648, 0.23671128241740202, 0.1842364488710415, 0.1349898987331568, 0.2013768030665769, 0.1037038311940447, 0.24798713808471506, 0.2044653147518426, 0.21455787443950325, 0.15664723064753927, 0.22191957410274776, 0.2486240179329362, 0.1616701842324415, 0.2133346743371719, 0.11696931124601961, 0.24286022215797276, 0.13932911821064042, 0.21664091937944374, 0.12566389461873628, 0.09967917194658626, 0.09438261678021043, 0.18024044736995637, 0.19727048764549104, 0.18890798155942223, 0.18383336497192415, 0.19616852667788487, 0.18595338125752192, 0.19266922079698612, 0.18204855410716192, 0.17826379439861162, 0.05273641752896874, 0.04897218547807569, 0.04648730377882149, 0.05683441875198225, 0.041714007709949374, 0.0345105000343352, 0.0626712221614758, 0.043885034159927505, 0.03633447168162529]}, "mutation_prompt": null}
{"id": "44fe08be-8ffc-49c7-a919-686092073b28", "solution": "import numpy as np\nfrom scipy.optimize import differential_evolution\n\nclass Enhanced_Hybrid_PSODE_SA_Optimizer:\n    def __init__(self, budget, dim, num_particles=30, max_iterations=1000, inertia_weight=0.7, cognitive_weight=1.4, social_weight=1.4, initial_temperature=100, cooling_rate=0.95):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = num_particles\n        self.max_iterations = max_iterations\n        self.inertia_weight = inertia_weight\n        self.cognitive_weight = cognitive_weight\n        self.social_weight = social_weight\n        self.initial_temperature = initial_temperature\n        self.cooling_rate = cooling_rate\n\n    def __call__(self, func):\n        def enhanced_hybrid_pso_de_sa_helper():\n            # Enhanced PSO initialization\n            self.inertia_weight = np.random.uniform(0.4, 0.9)  # Dynamically adjust inertia weight\n            # Enhanced PSO optimization loop\n            # Simulated Annealing (SA) enhancement\n            current_solution = np.random.uniform(-5.0, 5.0, size=self.dim)\n            current_cost = func(current_solution)\n            best_solution = current_solution\n            best_cost = current_cost\n            # SA enhancement loop\n            for _ in range(int(self.budget*0.7)):  # Spend 70% of the budget on SA\n                proposal_solution = current_solution + np.random.normal(0, 0.1, size=self.dim)\n                proposal_solution = np.clip(proposal_solution, -5.0, 5.0)\n                proposal_cost = func(proposal_solution)\n                if proposal_cost < current_cost or np.random.rand() < np.exp((current_cost - proposal_cost) / self.initial_temperature):\n                    current_solution = proposal_solution\n                    current_cost = proposal_cost\n                    if proposal_cost < best_cost:\n                        best_solution = proposal_solution\n                        best_cost = proposal_cost\n            # Differential Evolution (DE) optimization\n            de_result = differential_evolution(func, bounds=[(-5.0, 5.0)]*self.dim, maxiter=int(self.budget*0.3))  # Spend 30% of the budget on DE\n            if de_result.fun < best_cost:\n                best_solution = de_result.x\n            return best_solution\n\n        return enhanced_hybrid_pso_de_sa_helper()", "name": "Enhanced_Hybrid_PSODE_SA_Optimizer", "description": "Improving the PSO-DE-SA hybrid metaheuristic by dynamically adjusting the inertia weight and refining the Simulated Annealing strategy.", "configspace": "", "generation": 66, "fitness": 0.12523380220864708, "feedback": "The algorithm Enhanced_Hybrid_PSODE_SA_Optimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.13 with standard deviation 0.08.", "error": "", "parent_id": "654477e0-4d3a-43cc-b192-f885349aa5cc", "metadata": {"aucs": [0.28973149172408763, 0.3243343389857559, 0.3104742755271688, 0.2744396126493731, 0.2774458722826103, 0.31926094604124955, 0.32791907153315547, 0.28532970889671316, 0.39438341350319917, 0.12397055834086712, 0.04939007869361833, 0.09422327201410585, 0.10233794387117101, 0.05302389454059209, 0.06373305039398669, 0.10284165005858503, 0.03760881254179349, 0.055337717561820154, 0.04150802651161578, 0.05075537176052114, 0.03365153336430138, 0.06215022989286334, 0.07594145050725909, 0.06130811290555027, 0.03230559872452954, 0.04370902295136947, 0.055898722931636224, 0.06004933963820824, 0.03165736385000251, 0.039569653261228965, 0.048837084967940814, 0.044710756467256196, 0.04343610801577891, 0.038382042470198385, 0.07847093880912492, 0.04424343571404732, 0.07010485073933248, 0.05395409943193574, 0.05652898369471915, 0.18589990476891438, 0.23434793106584673, 0.2441959444329561, 0.06058360204785662, 0.05567752275458138, 0.1034196237728231, 0.11045008720817784, 0.11347123230506417, 0.09850697481016701, 0.10384130000541192, 0.08682883545352216, 0.07679838170617093, 0.17127603707329597, 0.21701579972591079, 0.14094456579986547, 0.1333888930933037, 0.12059374628619768, 0.11098051857991864, 0.16772864386416664, 0.11453810633532446, 0.12840024522631288, 0.1353829815106592, 0.17860860852578642, 0.1860939969876131, 0.178884109542495, 0.13983460442365558, 0.1333347659036701, 0.17538025757153897, 0.1696391932397846, 0.20898620252861944, 0.13966198754307735, 0.16681340951018842, 0.15435129490577815, 0.17259608440554453, 0.13418339668712487, 0.17750849571150318, 0.11859589278419369, 0.12904342847833394, 0.15933658299728104, 0.1690927229376843, 0.1206534886484677, 0.12878998561392474, 0.14869086718791347, 0.08064021761645634, 0.043298800064635046, 0.016498715065894287, 0.031701182988220666, 0.038139907104323356, 0.03680419133551005, 0.061512302278703634, 0.04852348645254645, 0.06805068862738084, 0.11292051162170702, 0.0957284223220689, 0.0960733407659844, 0.053021487728643346, 0.05364917729251395, 0.09389110008446189, 0.07903185320198969, 0.06729162809308142, 0.0154848804604657, 9.999999999998899e-05, 9.999999999998899e-05, 0.007595369039079203, 0.0025259036735233442, 0.0003589649686306373, 0.00018239067616576676, 9.999999999998899e-05, 9.999999999998899e-05, 0.02985117739490939, 0.043091437638419716, 0.0378348807083041, 0.039508122018701575, 0.036229829907631794, 0.0297712811357278, 0.03988485537509201, 0.048109500141186756, 0.06286111091181812, 0.2511888026644351, 0.24810992255575093, 0.24402358187859663, 0.23036495483298214, 0.24943361422814858, 0.23986209029161054, 0.30987712851975313, 0.2597461877557691, 0.27065104071735535, 0.04867112567335119, 0.06820909165177491, 0.051486197879023754, 0.050267593628658735, 0.040101542697310144, 0.04072859991481814, 0.054627671432407765, 0.032862792041209676, 0.05433701213911424, 0.17444936183953996, 0.14371516083960323, 0.12946589274416587, 0.13747013262964392, 0.1536497678561689, 0.12043448571044268, 0.1571485249749106, 0.14464974352974658, 0.14497734269037155, 0.1708139789123294, 0.1586413903771685, 0.148184360253665, 0.18311320569040113, 0.13184737136901292, 0.14460040537159025, 0.16124461403912715, 0.13020743501763787, 0.12221087930504726, 0.10940003030126377, 0.13797734420706276, 0.07769349794558622, 0.12007842078417075, 0.11420923196346888, 0.10861229835131148, 0.11155019915498798, 0.08474833677386073, 0.1158298687992636, 0.17113078066608556, 0.13682451244612515, 0.1399790605469592, 0.1776663608115603, 0.1534014260329143, 0.15712298818013637, 0.18413458292615092, 0.14145368570814953, 0.15261969021389632, 0.18858712514247244, 0.1730069271655862, 0.1803150320316319, 0.1818539411500747, 0.26293646454379493, 0.18380344683991567, 0.1811667670841648, 0.23671128241740202, 0.1842364488710415, 0.1349898987331568, 0.2013768030665769, 0.1037038311940447, 0.24798713808471506, 0.2044653147518426, 0.21455787443950325, 0.15664723064753927, 0.22191957410274776, 0.2486240179329362, 0.1616701842324415, 0.2133346743371719, 0.11696931124601961, 0.24286022215797276, 0.13932911821064042, 0.21664091937944374, 0.12566389461873628, 0.09967917194658626, 0.09438261678021043, 0.18024044736995637, 0.19727048764549104, 0.18890798155942223, 0.18383336497192415, 0.19616852667788487, 0.18595338125752192, 0.19266922079698612, 0.18204855410716192, 0.17826379439861162, 0.05273641752896874, 0.04897218547807569, 0.04648730377882149, 0.05683441875198225, 0.041714007709949374, 0.0345105000343352, 0.0626712221614758, 0.043885034159927505, 0.03633447168162529]}, "mutation_prompt": null}
{"id": "bcd00cbf-213b-4d4f-ac9f-b9c4e099cc7a", "solution": "import numpy as np\nfrom scipy.optimize import differential_evolution\n\nclass Hybrid_PSODE_SA_Optimizer:\n    def __init__(self, budget, dim, num_particles=30, max_iterations=1000, inertia_weight=0.7, cognitive_weight=1.4, social_weight=1.4, initial_temperature=100, cooling_rate=0.95):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = num_particles\n        self.max_iterations = max_iterations\n        self.inertia_weight = inertia_weight\n        self.cognitive_weight = cognitive_weight\n        self.social_weight = social_weight\n        self.initial_temperature = initial_temperature\n        self.cooling_rate = cooling_rate\n\n    def __call__(self, func):\n        def hybrid_pso_de_sa_helper():\n            # PSO initialization\n            # Dynamic adjustment of inertia weight based on exploration-exploitation balance\n            self.inertia_weight = np.random.uniform(0.4, 0.9)  # Update the inertia weight dynamically\n            # PSO optimization loop\n            # Simulated Annealing (SA) initialization\n            current_solution = np.random.uniform(-5.0, 5.0, size=self.dim)\n            current_cost = func(current_solution)\n            best_solution = current_solution\n            best_cost = current_cost\n            # SA optimization loop\n            for _ in range(int(self.budget*0.7)):  # Spend 70% of the budget on SA\n                proposal_solution = current_solution + np.random.normal(0, 0.1, size=self.dim)\n                proposal_solution = np.clip(proposal_solution, -5.0, 5.0)\n                proposal_cost = func(proposal_solution)\n                if proposal_cost < current_cost or np.random.rand() < np.exp((current_cost - proposal_cost) / self.initial_temperature):\n                    current_solution = proposal_solution\n                    current_cost = proposal_cost\n                    if proposal_cost < best_cost:\n                        best_solution = proposal_solution\n                        best_cost = proposal_cost\n            # Differential Evolution (DE) optimization\n            de_result = differential_evolution(func, bounds=[(-5.0, 5.0)]*self.dim, maxiter=int(self.budget*0.3))  # Spend 30% of the budget on DE\n            if de_result.fun < best_cost:\n                best_solution = de_result.x\n            return best_solution\n\n        return hybrid_pso_de_sa_helper()", "name": "Hybrid_PSODE_SA_Optimizer", "description": "Enhancing PSO with Simulated Annealing and Differential Evolution for a hybrid metaheuristic approach improving global exploration and local exploitation in optimization.", "configspace": "", "generation": 56, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "654477e0-4d3a-43cc-b192-f885349aa5cc", "metadata": {"aucs": [0.28973149172408763, 0.3243343389857559, 0.3104742755271688, 0.2744396126493731, 0.2774458722826103, 0.31926094604124955, 0.32791907153315547, 0.28532970889671316, 0.39438341350319917, 0.12397055834086712, 0.04939007869361833, 0.09422327201410585, 0.10233794387117101, 0.05302389454059209, 0.06373305039398669, 0.10284165005858503, 0.03760881254179349, 0.055337717561820154, 0.04150802651161578, 0.05075537176052114, 0.03365153336430138, 0.06215022989286334, 0.07594145050725909, 0.06130811290555027, 0.03230559872452954, 0.04370902295136947, 0.055898722931636224, 0.06004933963820824, 0.03165736385000251, 0.039569653261228965, 0.048837084967940814, 0.044710756467256196, 0.04343610801577891, 0.038382042470198385, 0.07847093880912492, 0.04424343571404732, 0.07010485073933248, 0.05395409943193574, 0.05652898369471915, 0.18589990476891438, 0.23434793106584673, 0.2441959444329561, 0.06058360204785662, 0.05567752275458138, 0.1034196237728231, 0.11045008720817784, 0.11347123230506417, 0.09850697481016701, 0.10384130000541192, 0.08682883545352216, 0.07679838170617093, 0.17127603707329597, 0.21701579972591079, 0.14094456579986547, 0.1333888930933037, 0.12059374628619768, 0.11098051857991864, 0.16772864386416664, 0.11453810633532446, 0.12840024522631288, 0.1353829815106592, 0.17860860852578642, 0.1860939969876131, 0.178884109542495, 0.13983460442365558, 0.1333347659036701, 0.17538025757153897, 0.1696391932397846, 0.20898620252861944, 0.13966198754307735, 0.16681340951018842, 0.15435129490577815, 0.17259608440554453, 0.13418339668712487, 0.17750849571150318, 0.11859589278419369, 0.12904342847833394, 0.15933658299728104, 0.1690927229376843, 0.1206534886484677, 0.12878998561392474, 0.14869086718791347, 0.08064021761645634, 0.043298800064635046, 0.016498715065894287, 0.031701182988220666, 0.038139907104323356, 0.03680419133551005, 0.061512302278703634, 0.04852348645254645, 0.06805068862738084, 0.11292051162170702, 0.0957284223220689, 0.0960733407659844, 0.053021487728643346, 0.05364917729251395, 0.09389110008446189, 0.07903185320198969, 0.06729162809308142, 0.0154848804604657, 9.999999999998899e-05, 9.999999999998899e-05, 0.007595369039079203, 0.0025259036735233442, 0.0003589649686306373, 0.00018239067616576676, 9.999999999998899e-05, 9.999999999998899e-05, 0.02985117739490939, 0.043091437638419716, 0.0378348807083041, 0.039508122018701575, 0.036229829907631794, 0.0297712811357278, 0.03988485537509201, 0.048109500141186756, 0.06286111091181812, 0.2511888026644351, 0.24810992255575093, 0.24402358187859663, 0.23036495483298214, 0.24943361422814858, 0.23986209029161054, 0.30987712851975313, 0.2597461877557691, 0.27065104071735535, 0.04867112567335119, 0.06820909165177491, 0.051486197879023754, 0.050267593628658735, 0.040101542697310144, 0.04072859991481814, 0.054627671432407765, 0.032862792041209676, 0.05433701213911424, 0.17444936183953996, 0.14371516083960323, 0.12946589274416587, 0.13747013262964392, 0.1536497678561689, 0.12043448571044268, 0.1571485249749106, 0.14464974352974658, 0.14497734269037155, 0.1708139789123294, 0.1586413903771685, 0.148184360253665, 0.18311320569040113, 0.13184737136901292, 0.14460040537159025, 0.16124461403912715, 0.13020743501763787, 0.12221087930504726, 0.10940003030126377, 0.13797734420706276, 0.07769349794558622, 0.12007842078417075, 0.11420923196346888, 0.10861229835131148, 0.11155019915498798, 0.08474833677386073, 0.1158298687992636, 0.17113078066608556, 0.13682451244612515, 0.1399790605469592, 0.1776663608115603, 0.1534014260329143, 0.15712298818013637, 0.18413458292615092, 0.14145368570814953, 0.15261969021389632, 0.18858712514247244, 0.1730069271655862, 0.1803150320316319, 0.1818539411500747, 0.26293646454379493, 0.18380344683991567, 0.1811667670841648, 0.23671128241740202, 0.1842364488710415, 0.1349898987331568, 0.2013768030665769, 0.1037038311940447, 0.24798713808471506, 0.2044653147518426, 0.21455787443950325, 0.15664723064753927, 0.22191957410274776, 0.2486240179329362, 0.1616701842324415, 0.2133346743371719, 0.11696931124601961, 0.24286022215797276, 0.13932911821064042, 0.21664091937944374, 0.12566389461873628, 0.09967917194658626, 0.09438261678021043, 0.18024044736995637, 0.19727048764549104, 0.18890798155942223, 0.18383336497192415, 0.19616852667788487, 0.18595338125752192, 0.19266922079698612, 0.18204855410716192, 0.17826379439861162, 0.05273641752896874, 0.04897218547807569, 0.04648730377882149, 0.05683441875198225, 0.041714007709949374, 0.0345105000343352, 0.0626712221614758, 0.043885034159927505, 0.03633447168162529]}, "mutation_prompt": null}
{"id": "2fe39f1b-83d5-41fd-94c4-6b1c0e894590", "solution": "import numpy as np\nfrom scipy.optimize import differential_evolution\n\nclass Hybrid_PSODE_SA_Optimizer:\n    def __init__(self, budget, dim, num_particles=30, max_iterations=1000, inertia_weight=0.7, cognitive_weight=1.4, social_weight=1.4, initial_temperature=100, cooling_rate=0.95):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = num_particles\n        self.max_iterations = max_iterations\n        self.inertia_weight = inertia_weight\n        self.cognitive_weight = cognitive_weight\n        self.social_weight = social_weight\n        self.initial_temperature = initial_temperature\n        self.cooling_rate = cooling_rate\n\n    def __call__(self, func):\n        def hybrid_pso_de_sa_helper():\n            # PSO initialization\n            # Dynamic adjustment of inertia weight based on exploration-exploitation balance\n            self.inertia_weight = np.random.uniform(0.4, 0.9)  # Update the inertia weight dynamically\n            # PSO optimization loop\n            # Simulated Annealing (SA) initialization\n            current_solution = np.random.uniform(-5.0, 5.0, size=self.dim)\n            current_cost = func(current_solution)\n            best_solution = current_solution\n            best_cost = current_cost\n            # SA optimization loop\n            for _ in range(int(self.budget*0.7)):  # Spend 70% of the budget on SA\n                proposal_solution = current_solution + np.random.normal(0, 0.1, size=self.dim)\n                proposal_solution = np.clip(proposal_solution, -5.0, 5.0)\n                proposal_cost = func(proposal_solution)\n                if proposal_cost < current_cost or np.random.rand() < np.exp((current_cost - proposal_cost) / self.initial_temperature):\n                    current_solution = proposal_solution\n                    current_cost = proposal_cost\n                    if proposal_cost < best_cost:\n                        best_solution = proposal_solution\n                        best_cost = proposal_cost\n            # Differential Evolution (DE) optimization\n            de_result = differential_evolution(func, bounds=[(-5.0, 5.0)]*self.dim, maxiter=int(self.budget*0.3))  # Spend 30% of the budget on DE\n            if de_result.fun < best_cost:\n                best_solution = de_result.x\n            return best_solution\n\n        return hybrid_pso_de_sa_helper()", "name": "Hybrid_PSODE_SA_Optimizer", "description": "Enhancing PSO with Simulated Annealing and Differential Evolution for a hybrid metaheuristic approach improving global exploration and local exploitation in optimization.", "configspace": "", "generation": 56, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "654477e0-4d3a-43cc-b192-f885349aa5cc", "metadata": {"aucs": [0.28973149172408763, 0.3243343389857559, 0.3104742755271688, 0.2744396126493731, 0.2774458722826103, 0.31926094604124955, 0.32791907153315547, 0.28532970889671316, 0.39438341350319917, 0.12397055834086712, 0.04939007869361833, 0.09422327201410585, 0.10233794387117101, 0.05302389454059209, 0.06373305039398669, 0.10284165005858503, 0.03760881254179349, 0.055337717561820154, 0.04150802651161578, 0.05075537176052114, 0.03365153336430138, 0.06215022989286334, 0.07594145050725909, 0.06130811290555027, 0.03230559872452954, 0.04370902295136947, 0.055898722931636224, 0.06004933963820824, 0.03165736385000251, 0.039569653261228965, 0.048837084967940814, 0.044710756467256196, 0.04343610801577891, 0.038382042470198385, 0.07847093880912492, 0.04424343571404732, 0.07010485073933248, 0.05395409943193574, 0.05652898369471915, 0.18589990476891438, 0.23434793106584673, 0.2441959444329561, 0.06058360204785662, 0.05567752275458138, 0.1034196237728231, 0.11045008720817784, 0.11347123230506417, 0.09850697481016701, 0.10384130000541192, 0.08682883545352216, 0.07679838170617093, 0.17127603707329597, 0.21701579972591079, 0.14094456579986547, 0.1333888930933037, 0.12059374628619768, 0.11098051857991864, 0.16772864386416664, 0.11453810633532446, 0.12840024522631288, 0.1353829815106592, 0.17860860852578642, 0.1860939969876131, 0.178884109542495, 0.13983460442365558, 0.1333347659036701, 0.17538025757153897, 0.1696391932397846, 0.20898620252861944, 0.13966198754307735, 0.16681340951018842, 0.15435129490577815, 0.17259608440554453, 0.13418339668712487, 0.17750849571150318, 0.11859589278419369, 0.12904342847833394, 0.15933658299728104, 0.1690927229376843, 0.1206534886484677, 0.12878998561392474, 0.14869086718791347, 0.08064021761645634, 0.043298800064635046, 0.016498715065894287, 0.031701182988220666, 0.038139907104323356, 0.03680419133551005, 0.061512302278703634, 0.04852348645254645, 0.06805068862738084, 0.11292051162170702, 0.0957284223220689, 0.0960733407659844, 0.053021487728643346, 0.05364917729251395, 0.09389110008446189, 0.07903185320198969, 0.06729162809308142, 0.0154848804604657, 9.999999999998899e-05, 9.999999999998899e-05, 0.007595369039079203, 0.0025259036735233442, 0.0003589649686306373, 0.00018239067616576676, 9.999999999998899e-05, 9.999999999998899e-05, 0.02985117739490939, 0.043091437638419716, 0.0378348807083041, 0.039508122018701575, 0.036229829907631794, 0.0297712811357278, 0.03988485537509201, 0.048109500141186756, 0.06286111091181812, 0.2511888026644351, 0.24810992255575093, 0.24402358187859663, 0.23036495483298214, 0.24943361422814858, 0.23986209029161054, 0.30987712851975313, 0.2597461877557691, 0.27065104071735535, 0.04867112567335119, 0.06820909165177491, 0.051486197879023754, 0.050267593628658735, 0.040101542697310144, 0.04072859991481814, 0.054627671432407765, 0.032862792041209676, 0.05433701213911424, 0.17444936183953996, 0.14371516083960323, 0.12946589274416587, 0.13747013262964392, 0.1536497678561689, 0.12043448571044268, 0.1571485249749106, 0.14464974352974658, 0.14497734269037155, 0.1708139789123294, 0.1586413903771685, 0.148184360253665, 0.18311320569040113, 0.13184737136901292, 0.14460040537159025, 0.16124461403912715, 0.13020743501763787, 0.12221087930504726, 0.10940003030126377, 0.13797734420706276, 0.07769349794558622, 0.12007842078417075, 0.11420923196346888, 0.10861229835131148, 0.11155019915498798, 0.08474833677386073, 0.1158298687992636, 0.17113078066608556, 0.13682451244612515, 0.1399790605469592, 0.1776663608115603, 0.1534014260329143, 0.15712298818013637, 0.18413458292615092, 0.14145368570814953, 0.15261969021389632, 0.18858712514247244, 0.1730069271655862, 0.1803150320316319, 0.1818539411500747, 0.26293646454379493, 0.18380344683991567, 0.1811667670841648, 0.23671128241740202, 0.1842364488710415, 0.1349898987331568, 0.2013768030665769, 0.1037038311940447, 0.24798713808471506, 0.2044653147518426, 0.21455787443950325, 0.15664723064753927, 0.22191957410274776, 0.2486240179329362, 0.1616701842324415, 0.2133346743371719, 0.11696931124601961, 0.24286022215797276, 0.13932911821064042, 0.21664091937944374, 0.12566389461873628, 0.09967917194658626, 0.09438261678021043, 0.18024044736995637, 0.19727048764549104, 0.18890798155942223, 0.18383336497192415, 0.19616852667788487, 0.18595338125752192, 0.19266922079698612, 0.18204855410716192, 0.17826379439861162, 0.05273641752896874, 0.04897218547807569, 0.04648730377882149, 0.05683441875198225, 0.041714007709949374, 0.0345105000343352, 0.0626712221614758, 0.043885034159927505, 0.03633447168162529]}, "mutation_prompt": null}
{"id": "cf4670ee-9579-4902-af38-0e14422668e8", "solution": "import numpy as np\nfrom scipy.optimize import differential_evolution\n\nclass Novel_Metaheuristic_Optimizer:\n    def __init__(self, budget, dim, num_particles=30, max_iterations=1000, inertia_weight=0.7, cognitive_weight=1.4, social_weight=1.4, initial_temperature=100, cooling_rate=0.95):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = num_particles\n        self.max_iterations = max_iterations\n        self.inertia_weight = inertia_weight\n        self.cognitive_weight = cognitive_weight\n        self.social_weight = social_weight\n        self.initial_temperature = initial_temperature\n        self.cooling_rate = cooling_rate\n\n    def __call__(self, func):\n        def novel_metaheuristic_helper():\n            # PSO initialization\n            # Dynamic adjustment of inertia weight based on exploration-exploitation balance\n            self.inertia_weight = np.random.uniform(0.4, 0.9)  # Update the inertia weight dynamically\n\n            # PSO optimization loop\n\n            # Simulated Annealing (SA) initialization\n            current_solution = np.random.uniform(-5.0, 5.0, size=self.dim)\n            current_cost = func(current_solution)\n            best_solution = current_solution\n            best_cost = current_cost\n\n            # SA optimization loop\n            for _ in range(int(self.budget*0.7)):  # Spend 70% of the budget on SA\n                proposal_solution = current_solution + np.random.normal(0, 0.1, size=self.dim)\n                proposal_solution = np.clip(proposal_solution, -5.0, 5.0)\n                proposal_cost = func(proposal_solution)\n                if proposal_cost < current_cost or np.random.rand() < np.exp((current_cost - proposal_cost) / self.initial_temperature):\n                    current_solution = proposal_solution\n                    current_cost = proposal_cost\n                    if proposal_cost < best_cost:\n                        best_solution = proposal_solution\n                        best_cost = proposal_cost\n\n            # Differential Evolution (DE) optimization\n            de_result = differential_evolution(func, bounds=[(-5.0, 5.0)]*self.dim, maxiter=int(self.budget*0.3))  # Spend 30% of the budget on DE\n            if de_result.fun < best_cost:\n                best_solution = de_result.x\n\n            return best_solution\n\n        return novel_metaheuristic_helper()", "name": "Novel_Metaheuristic_Optimizer", "description": "A novel metaheuristic algorithm combining Particle Swarm Optimization (PSO), Simulated Annealing (SA), and Differential Evolution (DE) for global exploration and local exploitation in black box optimization.", "configspace": "", "generation": 69, "fitness": 0.12523380220864708, "feedback": "The algorithm Novel_Metaheuristic_Optimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.13 with standard deviation 0.08.", "error": "", "parent_id": "654477e0-4d3a-43cc-b192-f885349aa5cc", "metadata": {"aucs": [0.28973149172408763, 0.3243343389857559, 0.3104742755271688, 0.2744396126493731, 0.2774458722826103, 0.31926094604124955, 0.32791907153315547, 0.28532970889671316, 0.39438341350319917, 0.12397055834086712, 0.04939007869361833, 0.09422327201410585, 0.10233794387117101, 0.05302389454059209, 0.06373305039398669, 0.10284165005858503, 0.03760881254179349, 0.055337717561820154, 0.04150802651161578, 0.05075537176052114, 0.03365153336430138, 0.06215022989286334, 0.07594145050725909, 0.06130811290555027, 0.03230559872452954, 0.04370902295136947, 0.055898722931636224, 0.06004933963820824, 0.03165736385000251, 0.039569653261228965, 0.048837084967940814, 0.044710756467256196, 0.04343610801577891, 0.038382042470198385, 0.07847093880912492, 0.04424343571404732, 0.07010485073933248, 0.05395409943193574, 0.05652898369471915, 0.18589990476891438, 0.23434793106584673, 0.2441959444329561, 0.06058360204785662, 0.05567752275458138, 0.1034196237728231, 0.11045008720817784, 0.11347123230506417, 0.09850697481016701, 0.10384130000541192, 0.08682883545352216, 0.07679838170617093, 0.17127603707329597, 0.21701579972591079, 0.14094456579986547, 0.1333888930933037, 0.12059374628619768, 0.11098051857991864, 0.16772864386416664, 0.11453810633532446, 0.12840024522631288, 0.1353829815106592, 0.17860860852578642, 0.1860939969876131, 0.178884109542495, 0.13983460442365558, 0.1333347659036701, 0.17538025757153897, 0.1696391932397846, 0.20898620252861944, 0.13966198754307735, 0.16681340951018842, 0.15435129490577815, 0.17259608440554453, 0.13418339668712487, 0.17750849571150318, 0.11859589278419369, 0.12904342847833394, 0.15933658299728104, 0.1690927229376843, 0.1206534886484677, 0.12878998561392474, 0.14869086718791347, 0.08064021761645634, 0.043298800064635046, 0.016498715065894287, 0.031701182988220666, 0.038139907104323356, 0.03680419133551005, 0.061512302278703634, 0.04852348645254645, 0.06805068862738084, 0.11292051162170702, 0.0957284223220689, 0.0960733407659844, 0.053021487728643346, 0.05364917729251395, 0.09389110008446189, 0.07903185320198969, 0.06729162809308142, 0.0154848804604657, 9.999999999998899e-05, 9.999999999998899e-05, 0.007595369039079203, 0.0025259036735233442, 0.0003589649686306373, 0.00018239067616576676, 9.999999999998899e-05, 9.999999999998899e-05, 0.02985117739490939, 0.043091437638419716, 0.0378348807083041, 0.039508122018701575, 0.036229829907631794, 0.0297712811357278, 0.03988485537509201, 0.048109500141186756, 0.06286111091181812, 0.2511888026644351, 0.24810992255575093, 0.24402358187859663, 0.23036495483298214, 0.24943361422814858, 0.23986209029161054, 0.30987712851975313, 0.2597461877557691, 0.27065104071735535, 0.04867112567335119, 0.06820909165177491, 0.051486197879023754, 0.050267593628658735, 0.040101542697310144, 0.04072859991481814, 0.054627671432407765, 0.032862792041209676, 0.05433701213911424, 0.17444936183953996, 0.14371516083960323, 0.12946589274416587, 0.13747013262964392, 0.1536497678561689, 0.12043448571044268, 0.1571485249749106, 0.14464974352974658, 0.14497734269037155, 0.1708139789123294, 0.1586413903771685, 0.148184360253665, 0.18311320569040113, 0.13184737136901292, 0.14460040537159025, 0.16124461403912715, 0.13020743501763787, 0.12221087930504726, 0.10940003030126377, 0.13797734420706276, 0.07769349794558622, 0.12007842078417075, 0.11420923196346888, 0.10861229835131148, 0.11155019915498798, 0.08474833677386073, 0.1158298687992636, 0.17113078066608556, 0.13682451244612515, 0.1399790605469592, 0.1776663608115603, 0.1534014260329143, 0.15712298818013637, 0.18413458292615092, 0.14145368570814953, 0.15261969021389632, 0.18858712514247244, 0.1730069271655862, 0.1803150320316319, 0.1818539411500747, 0.26293646454379493, 0.18380344683991567, 0.1811667670841648, 0.23671128241740202, 0.1842364488710415, 0.1349898987331568, 0.2013768030665769, 0.1037038311940447, 0.24798713808471506, 0.2044653147518426, 0.21455787443950325, 0.15664723064753927, 0.22191957410274776, 0.2486240179329362, 0.1616701842324415, 0.2133346743371719, 0.11696931124601961, 0.24286022215797276, 0.13932911821064042, 0.21664091937944374, 0.12566389461873628, 0.09967917194658626, 0.09438261678021043, 0.18024044736995637, 0.19727048764549104, 0.18890798155942223, 0.18383336497192415, 0.19616852667788487, 0.18595338125752192, 0.19266922079698612, 0.18204855410716192, 0.17826379439861162, 0.05273641752896874, 0.04897218547807569, 0.04648730377882149, 0.05683441875198225, 0.041714007709949374, 0.0345105000343352, 0.0626712221614758, 0.043885034159927505, 0.03633447168162529]}, "mutation_prompt": null}
{"id": "c58f7d94-507e-41c5-adfe-3a5279532092", "solution": "import numpy as np\nfrom scipy.optimize import differential_evolution\n\nclass Novel_Metaheuristic_Optimizer:\n    def __init__(self, budget, dim, num_particles=30, max_iterations=1000, inertia_weight=0.7, cognitive_weight=1.4, social_weight=1.4, initial_temperature=100, cooling_rate=0.95):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = num_particles\n        self.max_iterations = max_iterations\n        self.inertia_weight = inertia_weight\n        self.cognitive_weight = cognitive_weight\n        self.social_weight = social_weight\n        self.initial_temperature = initial_temperature\n        self.cooling_rate = cooling_rate\n\n    def __call__(self, func):\n        def novel_metaheuristic_helper():\n            # Particle Swarm Optimization (PSO) initialization\n            # Dynamic adjustment of inertia weight based on exploration-exploitation balance\n            self.inertia_weight = np.random.uniform(0.4, 0.9)  # Update the inertia weight dynamically\n            # PSO optimization loop\n            # Simulated Annealing (SA) initialization\n            current_solution = np.random.uniform(-5.0, 5.0, size=self.dim)\n            current_cost = func(current_solution)\n            best_solution = current_solution\n            best_cost = current_cost\n            # SA optimization loop\n            for _ in range(int(self.budget*0.7)):  # Spend 70% of the budget on SA\n                proposal_solution = current_solution + np.random.normal(0, 0.1, size=self.dim)\n                proposal_solution = np.clip(proposal_solution, -5.0, 5.0)\n                proposal_cost = func(proposal_solution)\n                if proposal_cost < current_cost or np.random.rand() < np.exp((current_cost - proposal_cost) / self.initial_temperature):\n                    current_solution = proposal_solution\n                    current_cost = proposal_cost\n                    if proposal_cost < best_cost:\n                        best_solution = proposal_solution\n                        best_cost = proposal_cost\n            # Differential Evolution (DE) optimization\n            de_result = differential_evolution(func, bounds=[(-5.0, 5.0)]*self.dim, maxiter=int(self.budget*0.3))  # Spend 30% of the budget on DE\n            if de_result.fun < best_cost:\n                best_solution = de_result.x\n            return best_solution\n\n        return novel_metaheuristic_helper()", "name": "Novel_Metaheuristic_Optimizer", "description": "A novel Metaheuristic Algorithm combining Particle Swarm Optimization, Simulated Annealing, and Differential Evolution with dynamic parameters adjustment for better global exploration and local exploitation in optimization.", "configspace": "", "generation": 70, "fitness": 0.12523380220864708, "feedback": "The algorithm Novel_Metaheuristic_Optimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.13 with standard deviation 0.08.", "error": "", "parent_id": "654477e0-4d3a-43cc-b192-f885349aa5cc", "metadata": {"aucs": [0.28973149172408763, 0.3243343389857559, 0.3104742755271688, 0.2744396126493731, 0.2774458722826103, 0.31926094604124955, 0.32791907153315547, 0.28532970889671316, 0.39438341350319917, 0.12397055834086712, 0.04939007869361833, 0.09422327201410585, 0.10233794387117101, 0.05302389454059209, 0.06373305039398669, 0.10284165005858503, 0.03760881254179349, 0.055337717561820154, 0.04150802651161578, 0.05075537176052114, 0.03365153336430138, 0.06215022989286334, 0.07594145050725909, 0.06130811290555027, 0.03230559872452954, 0.04370902295136947, 0.055898722931636224, 0.06004933963820824, 0.03165736385000251, 0.039569653261228965, 0.048837084967940814, 0.044710756467256196, 0.04343610801577891, 0.038382042470198385, 0.07847093880912492, 0.04424343571404732, 0.07010485073933248, 0.05395409943193574, 0.05652898369471915, 0.18589990476891438, 0.23434793106584673, 0.2441959444329561, 0.06058360204785662, 0.05567752275458138, 0.1034196237728231, 0.11045008720817784, 0.11347123230506417, 0.09850697481016701, 0.10384130000541192, 0.08682883545352216, 0.07679838170617093, 0.17127603707329597, 0.21701579972591079, 0.14094456579986547, 0.1333888930933037, 0.12059374628619768, 0.11098051857991864, 0.16772864386416664, 0.11453810633532446, 0.12840024522631288, 0.1353829815106592, 0.17860860852578642, 0.1860939969876131, 0.178884109542495, 0.13983460442365558, 0.1333347659036701, 0.17538025757153897, 0.1696391932397846, 0.20898620252861944, 0.13966198754307735, 0.16681340951018842, 0.15435129490577815, 0.17259608440554453, 0.13418339668712487, 0.17750849571150318, 0.11859589278419369, 0.12904342847833394, 0.15933658299728104, 0.1690927229376843, 0.1206534886484677, 0.12878998561392474, 0.14869086718791347, 0.08064021761645634, 0.043298800064635046, 0.016498715065894287, 0.031701182988220666, 0.038139907104323356, 0.03680419133551005, 0.061512302278703634, 0.04852348645254645, 0.06805068862738084, 0.11292051162170702, 0.0957284223220689, 0.0960733407659844, 0.053021487728643346, 0.05364917729251395, 0.09389110008446189, 0.07903185320198969, 0.06729162809308142, 0.0154848804604657, 9.999999999998899e-05, 9.999999999998899e-05, 0.007595369039079203, 0.0025259036735233442, 0.0003589649686306373, 0.00018239067616576676, 9.999999999998899e-05, 9.999999999998899e-05, 0.02985117739490939, 0.043091437638419716, 0.0378348807083041, 0.039508122018701575, 0.036229829907631794, 0.0297712811357278, 0.03988485537509201, 0.048109500141186756, 0.06286111091181812, 0.2511888026644351, 0.24810992255575093, 0.24402358187859663, 0.23036495483298214, 0.24943361422814858, 0.23986209029161054, 0.30987712851975313, 0.2597461877557691, 0.27065104071735535, 0.04867112567335119, 0.06820909165177491, 0.051486197879023754, 0.050267593628658735, 0.040101542697310144, 0.04072859991481814, 0.054627671432407765, 0.032862792041209676, 0.05433701213911424, 0.17444936183953996, 0.14371516083960323, 0.12946589274416587, 0.13747013262964392, 0.1536497678561689, 0.12043448571044268, 0.1571485249749106, 0.14464974352974658, 0.14497734269037155, 0.1708139789123294, 0.1586413903771685, 0.148184360253665, 0.18311320569040113, 0.13184737136901292, 0.14460040537159025, 0.16124461403912715, 0.13020743501763787, 0.12221087930504726, 0.10940003030126377, 0.13797734420706276, 0.07769349794558622, 0.12007842078417075, 0.11420923196346888, 0.10861229835131148, 0.11155019915498798, 0.08474833677386073, 0.1158298687992636, 0.17113078066608556, 0.13682451244612515, 0.1399790605469592, 0.1776663608115603, 0.1534014260329143, 0.15712298818013637, 0.18413458292615092, 0.14145368570814953, 0.15261969021389632, 0.18858712514247244, 0.1730069271655862, 0.1803150320316319, 0.1818539411500747, 0.26293646454379493, 0.18380344683991567, 0.1811667670841648, 0.23671128241740202, 0.1842364488710415, 0.1349898987331568, 0.2013768030665769, 0.1037038311940447, 0.24798713808471506, 0.2044653147518426, 0.21455787443950325, 0.15664723064753927, 0.22191957410274776, 0.2486240179329362, 0.1616701842324415, 0.2133346743371719, 0.11696931124601961, 0.24286022215797276, 0.13932911821064042, 0.21664091937944374, 0.12566389461873628, 0.09967917194658626, 0.09438261678021043, 0.18024044736995637, 0.19727048764549104, 0.18890798155942223, 0.18383336497192415, 0.19616852667788487, 0.18595338125752192, 0.19266922079698612, 0.18204855410716192, 0.17826379439861162, 0.05273641752896874, 0.04897218547807569, 0.04648730377882149, 0.05683441875198225, 0.041714007709949374, 0.0345105000343352, 0.0626712221614758, 0.043885034159927505, 0.03633447168162529]}, "mutation_prompt": null}
{"id": "9c77c786-06e9-42c8-a20b-50e292c02695", "solution": "import numpy as np\nfrom scipy.optimize import differential_evolution\n\nclass Hybrid_PSODE_SA_Optimizer:\n    def __init__(self, budget, dim, num_particles=30, max_iterations=1000, inertia_weight=0.7, cognitive_weight=1.4, social_weight=1.4, initial_temperature=100, cooling_rate=0.95):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = num_particles\n        self.max_iterations = max_iterations\n        self.inertia_weight = inertia_weight\n        self.cognitive_weight = cognitive_weight\n        self.social_weight = social_weight\n        self.initial_temperature = initial_temperature\n        self.cooling_rate = cooling_rate\n\n    def __call__(self, func):\n        def hybrid_pso_de_sa_helper():\n            # PSO initialization\n            # Dynamic adjustment of inertia weight based on exploration-exploitation balance\n            self.inertia_weight = np.random.uniform(0.4, 0.9)  # Update the inertia weight dynamically\n            # PSO optimization loop\n            # Simulated Annealing (SA) initialization\n            current_solution = np.random.uniform(-5.0, 5.0, size=self.dim)\n            current_cost = func(current_solution)\n            best_solution = current_solution\n            best_cost = current_cost\n            # SA optimization loop\n            for _ in range(int(self.budget*0.7)):  # Spend 70% of the budget on SA\n                proposal_solution = current_solution + np.random.normal(0, 0.1, size=self.dim)\n                proposal_solution = np.clip(proposal_solution, -5.0, 5.0)\n                proposal_cost = func(proposal_solution)\n                if proposal_cost < current_cost or np.random.rand() < np.exp((current_cost - proposal_cost) / self.initial_temperature):\n                    current_solution = proposal_solution\n                    current_cost = proposal_cost\n                    if proposal_cost < best_cost:\n                        best_solution = proposal_solution\n                        best_cost = proposal_cost\n            # Differential Evolution (DE) optimization\n            de_result = differential_evolution(func, bounds=[(-5.0, 5.0)]*self.dim, maxiter=int(self.budget*0.3))  # Spend 30% of the budget on DE\n            if de_result.fun < best_cost:\n                best_solution = de_result.x\n            return best_solution\n\n        return hybrid_pso_de_sa_helper()", "name": "Hybrid_PSODE_SA_Optimizer", "description": "Enhancing PSO with Simulated Annealing and Differential Evolution for a hybrid metaheuristic approach improving global exploration and local exploitation in optimization.", "configspace": "", "generation": 56, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "654477e0-4d3a-43cc-b192-f885349aa5cc", "metadata": {"aucs": [0.28973149172408763, 0.3243343389857559, 0.3104742755271688, 0.2744396126493731, 0.2774458722826103, 0.31926094604124955, 0.32791907153315547, 0.28532970889671316, 0.39438341350319917, 0.12397055834086712, 0.04939007869361833, 0.09422327201410585, 0.10233794387117101, 0.05302389454059209, 0.06373305039398669, 0.10284165005858503, 0.03760881254179349, 0.055337717561820154, 0.04150802651161578, 0.05075537176052114, 0.03365153336430138, 0.06215022989286334, 0.07594145050725909, 0.06130811290555027, 0.03230559872452954, 0.04370902295136947, 0.055898722931636224, 0.06004933963820824, 0.03165736385000251, 0.039569653261228965, 0.048837084967940814, 0.044710756467256196, 0.04343610801577891, 0.038382042470198385, 0.07847093880912492, 0.04424343571404732, 0.07010485073933248, 0.05395409943193574, 0.05652898369471915, 0.18589990476891438, 0.23434793106584673, 0.2441959444329561, 0.06058360204785662, 0.05567752275458138, 0.1034196237728231, 0.11045008720817784, 0.11347123230506417, 0.09850697481016701, 0.10384130000541192, 0.08682883545352216, 0.07679838170617093, 0.17127603707329597, 0.21701579972591079, 0.14094456579986547, 0.1333888930933037, 0.12059374628619768, 0.11098051857991864, 0.16772864386416664, 0.11453810633532446, 0.12840024522631288, 0.1353829815106592, 0.17860860852578642, 0.1860939969876131, 0.178884109542495, 0.13983460442365558, 0.1333347659036701, 0.17538025757153897, 0.1696391932397846, 0.20898620252861944, 0.13966198754307735, 0.16681340951018842, 0.15435129490577815, 0.17259608440554453, 0.13418339668712487, 0.17750849571150318, 0.11859589278419369, 0.12904342847833394, 0.15933658299728104, 0.1690927229376843, 0.1206534886484677, 0.12878998561392474, 0.14869086718791347, 0.08064021761645634, 0.043298800064635046, 0.016498715065894287, 0.031701182988220666, 0.038139907104323356, 0.03680419133551005, 0.061512302278703634, 0.04852348645254645, 0.06805068862738084, 0.11292051162170702, 0.0957284223220689, 0.0960733407659844, 0.053021487728643346, 0.05364917729251395, 0.09389110008446189, 0.07903185320198969, 0.06729162809308142, 0.0154848804604657, 9.999999999998899e-05, 9.999999999998899e-05, 0.007595369039079203, 0.0025259036735233442, 0.0003589649686306373, 0.00018239067616576676, 9.999999999998899e-05, 9.999999999998899e-05, 0.02985117739490939, 0.043091437638419716, 0.0378348807083041, 0.039508122018701575, 0.036229829907631794, 0.0297712811357278, 0.03988485537509201, 0.048109500141186756, 0.06286111091181812, 0.2511888026644351, 0.24810992255575093, 0.24402358187859663, 0.23036495483298214, 0.24943361422814858, 0.23986209029161054, 0.30987712851975313, 0.2597461877557691, 0.27065104071735535, 0.04867112567335119, 0.06820909165177491, 0.051486197879023754, 0.050267593628658735, 0.040101542697310144, 0.04072859991481814, 0.054627671432407765, 0.032862792041209676, 0.05433701213911424, 0.17444936183953996, 0.14371516083960323, 0.12946589274416587, 0.13747013262964392, 0.1536497678561689, 0.12043448571044268, 0.1571485249749106, 0.14464974352974658, 0.14497734269037155, 0.1708139789123294, 0.1586413903771685, 0.148184360253665, 0.18311320569040113, 0.13184737136901292, 0.14460040537159025, 0.16124461403912715, 0.13020743501763787, 0.12221087930504726, 0.10940003030126377, 0.13797734420706276, 0.07769349794558622, 0.12007842078417075, 0.11420923196346888, 0.10861229835131148, 0.11155019915498798, 0.08474833677386073, 0.1158298687992636, 0.17113078066608556, 0.13682451244612515, 0.1399790605469592, 0.1776663608115603, 0.1534014260329143, 0.15712298818013637, 0.18413458292615092, 0.14145368570814953, 0.15261969021389632, 0.18858712514247244, 0.1730069271655862, 0.1803150320316319, 0.1818539411500747, 0.26293646454379493, 0.18380344683991567, 0.1811667670841648, 0.23671128241740202, 0.1842364488710415, 0.1349898987331568, 0.2013768030665769, 0.1037038311940447, 0.24798713808471506, 0.2044653147518426, 0.21455787443950325, 0.15664723064753927, 0.22191957410274776, 0.2486240179329362, 0.1616701842324415, 0.2133346743371719, 0.11696931124601961, 0.24286022215797276, 0.13932911821064042, 0.21664091937944374, 0.12566389461873628, 0.09967917194658626, 0.09438261678021043, 0.18024044736995637, 0.19727048764549104, 0.18890798155942223, 0.18383336497192415, 0.19616852667788487, 0.18595338125752192, 0.19266922079698612, 0.18204855410716192, 0.17826379439861162, 0.05273641752896874, 0.04897218547807569, 0.04648730377882149, 0.05683441875198225, 0.041714007709949374, 0.0345105000343352, 0.0626712221614758, 0.043885034159927505, 0.03633447168162529]}, "mutation_prompt": null}
{"id": "8d4f2e07-22a4-4236-acf0-6c9053a059f4", "solution": "import numpy as np\nfrom scipy.optimize import differential_evolution\n\nclass Enhanced_Hybrid_Optimizer:\n    def __init__(self, budget, dim, num_particles=30, max_iterations=1000, inertia_weight=0.7, cognitive_weight=1.4, social_weight=1.4, initial_temperature=100, cooling_rate=0.95):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = num_particles\n        self.max_iterations = max_iterations\n        self.inertia_weight = inertia_weight\n        self.cognitive_weight = cognitive_weight\n        self.social_weight = social_weight\n        self.initial_temperature = initial_temperature\n        self.cooling_rate = cooling_rate\n\n    def __call__(self, func):\n        def enhanced_hybrid_optimizer_helper():\n            # PSO initialization\n            self.inertia_weight = np.random.uniform(0.4, 0.9)\n            # PSO optimization loop\n            # SA initialization\n            current_solution = np.random.uniform(-5.0, 5.0, size=self.dim)\n            current_cost = func(current_solution)\n            best_solution = current_solution\n            best_cost = current_cost\n            # SA optimization loop\n            for _ in range(int(self.budget*0.7)):\n                proposal_solution = current_solution + np.random.normal(0, 0.1, size=self.dim)\n                proposal_solution = np.clip(proposal_solution, -5.0, 5.0)\n                proposal_cost = func(proposal_solution)\n                if proposal_cost < current_cost or np.random.rand() < np.exp((current_cost - proposal_cost) / self.initial_temperature):\n                    current_solution = proposal_solution\n                    current_cost = proposal_cost\n                    if proposal_cost < best_cost:\n                        best_solution = proposal_solution\n                        best_cost = proposal_cost\n            # DE optimization\n            de_result = differential_evolution(func, bounds=[(-5.0, 5.0)]*self.dim, maxiter=int(self.budget*0.3))\n            if de_result.fun < best_cost:\n                best_solution = de_result.x\n            return best_solution\n\n        return enhanced_hybrid_optimizer_helper()", "name": "Enhanced_Hybrid_Optimizer", "description": "Integrating Particle Swarm Optimization (PSO), Simulated Annealing (SA), and Differential Evolution (DE) to balance exploration and exploitation for efficient black box function optimization.", "configspace": "", "generation": 72, "fitness": 0.12523380220864708, "feedback": "The algorithm Enhanced_Hybrid_Optimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.13 with standard deviation 0.08.", "error": "", "parent_id": "654477e0-4d3a-43cc-b192-f885349aa5cc", "metadata": {"aucs": [0.28973149172408763, 0.3243343389857559, 0.3104742755271688, 0.2744396126493731, 0.2774458722826103, 0.31926094604124955, 0.32791907153315547, 0.28532970889671316, 0.39438341350319917, 0.12397055834086712, 0.04939007869361833, 0.09422327201410585, 0.10233794387117101, 0.05302389454059209, 0.06373305039398669, 0.10284165005858503, 0.03760881254179349, 0.055337717561820154, 0.04150802651161578, 0.05075537176052114, 0.03365153336430138, 0.06215022989286334, 0.07594145050725909, 0.06130811290555027, 0.03230559872452954, 0.04370902295136947, 0.055898722931636224, 0.06004933963820824, 0.03165736385000251, 0.039569653261228965, 0.048837084967940814, 0.044710756467256196, 0.04343610801577891, 0.038382042470198385, 0.07847093880912492, 0.04424343571404732, 0.07010485073933248, 0.05395409943193574, 0.05652898369471915, 0.18589990476891438, 0.23434793106584673, 0.2441959444329561, 0.06058360204785662, 0.05567752275458138, 0.1034196237728231, 0.11045008720817784, 0.11347123230506417, 0.09850697481016701, 0.10384130000541192, 0.08682883545352216, 0.07679838170617093, 0.17127603707329597, 0.21701579972591079, 0.14094456579986547, 0.1333888930933037, 0.12059374628619768, 0.11098051857991864, 0.16772864386416664, 0.11453810633532446, 0.12840024522631288, 0.1353829815106592, 0.17860860852578642, 0.1860939969876131, 0.178884109542495, 0.13983460442365558, 0.1333347659036701, 0.17538025757153897, 0.1696391932397846, 0.20898620252861944, 0.13966198754307735, 0.16681340951018842, 0.15435129490577815, 0.17259608440554453, 0.13418339668712487, 0.17750849571150318, 0.11859589278419369, 0.12904342847833394, 0.15933658299728104, 0.1690927229376843, 0.1206534886484677, 0.12878998561392474, 0.14869086718791347, 0.08064021761645634, 0.043298800064635046, 0.016498715065894287, 0.031701182988220666, 0.038139907104323356, 0.03680419133551005, 0.061512302278703634, 0.04852348645254645, 0.06805068862738084, 0.11292051162170702, 0.0957284223220689, 0.0960733407659844, 0.053021487728643346, 0.05364917729251395, 0.09389110008446189, 0.07903185320198969, 0.06729162809308142, 0.0154848804604657, 9.999999999998899e-05, 9.999999999998899e-05, 0.007595369039079203, 0.0025259036735233442, 0.0003589649686306373, 0.00018239067616576676, 9.999999999998899e-05, 9.999999999998899e-05, 0.02985117739490939, 0.043091437638419716, 0.0378348807083041, 0.039508122018701575, 0.036229829907631794, 0.0297712811357278, 0.03988485537509201, 0.048109500141186756, 0.06286111091181812, 0.2511888026644351, 0.24810992255575093, 0.24402358187859663, 0.23036495483298214, 0.24943361422814858, 0.23986209029161054, 0.30987712851975313, 0.2597461877557691, 0.27065104071735535, 0.04867112567335119, 0.06820909165177491, 0.051486197879023754, 0.050267593628658735, 0.040101542697310144, 0.04072859991481814, 0.054627671432407765, 0.032862792041209676, 0.05433701213911424, 0.17444936183953996, 0.14371516083960323, 0.12946589274416587, 0.13747013262964392, 0.1536497678561689, 0.12043448571044268, 0.1571485249749106, 0.14464974352974658, 0.14497734269037155, 0.1708139789123294, 0.1586413903771685, 0.148184360253665, 0.18311320569040113, 0.13184737136901292, 0.14460040537159025, 0.16124461403912715, 0.13020743501763787, 0.12221087930504726, 0.10940003030126377, 0.13797734420706276, 0.07769349794558622, 0.12007842078417075, 0.11420923196346888, 0.10861229835131148, 0.11155019915498798, 0.08474833677386073, 0.1158298687992636, 0.17113078066608556, 0.13682451244612515, 0.1399790605469592, 0.1776663608115603, 0.1534014260329143, 0.15712298818013637, 0.18413458292615092, 0.14145368570814953, 0.15261969021389632, 0.18858712514247244, 0.1730069271655862, 0.1803150320316319, 0.1818539411500747, 0.26293646454379493, 0.18380344683991567, 0.1811667670841648, 0.23671128241740202, 0.1842364488710415, 0.1349898987331568, 0.2013768030665769, 0.1037038311940447, 0.24798713808471506, 0.2044653147518426, 0.21455787443950325, 0.15664723064753927, 0.22191957410274776, 0.2486240179329362, 0.1616701842324415, 0.2133346743371719, 0.11696931124601961, 0.24286022215797276, 0.13932911821064042, 0.21664091937944374, 0.12566389461873628, 0.09967917194658626, 0.09438261678021043, 0.18024044736995637, 0.19727048764549104, 0.18890798155942223, 0.18383336497192415, 0.19616852667788487, 0.18595338125752192, 0.19266922079698612, 0.18204855410716192, 0.17826379439861162, 0.05273641752896874, 0.04897218547807569, 0.04648730377882149, 0.05683441875198225, 0.041714007709949374, 0.0345105000343352, 0.0626712221614758, 0.043885034159927505, 0.03633447168162529]}, "mutation_prompt": null}
{"id": "68e1bd4b-8d62-44e8-8784-2dd7488b1af9", "solution": "import numpy as np\nfrom scipy.optimize import differential_evolution\n\nclass Hybrid_PSODE_SA_Optimizer:\n    def __init__(self, budget, dim, num_particles=30, max_iterations=1000, inertia_weight=0.7, cognitive_weight=1.4, social_weight=1.4, initial_temperature=100, cooling_rate=0.95):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = num_particles\n        self.max_iterations = max_iterations\n        self.inertia_weight = inertia_weight\n        self.cognitive_weight = cognitive_weight\n        self.social_weight = social_weight\n        self.initial_temperature = initial_temperature\n        self.cooling_rate = cooling_rate\n\n    def __call__(self, func):\n        def hybrid_pso_de_sa_helper():\n            # PSO initialization\n            # Dynamic adjustment of inertia weight based on exploration-exploitation balance\n            self.inertia_weight = np.random.uniform(0.4, 0.9)  # Update the inertia weight dynamically\n            # PSO optimization loop\n            # Simulated Annealing (SA) initialization\n            current_solution = np.random.uniform(-5.0, 5.0, size=self.dim)\n            current_cost = func(current_solution)\n            best_solution = current_solution\n            best_cost = current_cost\n            # SA optimization loop\n            for _ in range(int(self.budget*0.7)):  # Spend 70% of the budget on SA\n                proposal_solution = current_solution + np.random.normal(0, 0.1, size=self.dim)\n                proposal_solution = np.clip(proposal_solution, -5.0, 5.0)\n                proposal_cost = func(proposal_solution)\n                if proposal_cost < current_cost or np.random.rand() < np.exp((current_cost - proposal_cost) / self.initial_temperature):\n                    current_solution = proposal_solution\n                    current_cost = proposal_cost\n                    if proposal_cost < best_cost:\n                        best_solution = proposal_solution\n                        best_cost = proposal_cost\n            # Differential Evolution (DE) optimization\n            de_result = differential_evolution(func, bounds=[(-5.0, 5.0)]*self.dim, maxiter=int(self.budget*0.3))  # Spend 30% of the budget on DE\n            if de_result.fun < best_cost:\n                best_solution = de_result.x\n            return best_solution\n\n        return hybrid_pso_de_sa_helper()", "name": "Hybrid_PSODE_SA_Optimizer", "description": "Enhancing PSO with Simulated Annealing and Differential Evolution for a hybrid metaheuristic approach improving global exploration and local exploitation in optimization.", "configspace": "", "generation": 56, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "654477e0-4d3a-43cc-b192-f885349aa5cc", "metadata": {"aucs": [0.28973149172408763, 0.3243343389857559, 0.3104742755271688, 0.2744396126493731, 0.2774458722826103, 0.31926094604124955, 0.32791907153315547, 0.28532970889671316, 0.39438341350319917, 0.12397055834086712, 0.04939007869361833, 0.09422327201410585, 0.10233794387117101, 0.05302389454059209, 0.06373305039398669, 0.10284165005858503, 0.03760881254179349, 0.055337717561820154, 0.04150802651161578, 0.05075537176052114, 0.03365153336430138, 0.06215022989286334, 0.07594145050725909, 0.06130811290555027, 0.03230559872452954, 0.04370902295136947, 0.055898722931636224, 0.06004933963820824, 0.03165736385000251, 0.039569653261228965, 0.048837084967940814, 0.044710756467256196, 0.04343610801577891, 0.038382042470198385, 0.07847093880912492, 0.04424343571404732, 0.07010485073933248, 0.05395409943193574, 0.05652898369471915, 0.18589990476891438, 0.23434793106584673, 0.2441959444329561, 0.06058360204785662, 0.05567752275458138, 0.1034196237728231, 0.11045008720817784, 0.11347123230506417, 0.09850697481016701, 0.10384130000541192, 0.08682883545352216, 0.07679838170617093, 0.17127603707329597, 0.21701579972591079, 0.14094456579986547, 0.1333888930933037, 0.12059374628619768, 0.11098051857991864, 0.16772864386416664, 0.11453810633532446, 0.12840024522631288, 0.1353829815106592, 0.17860860852578642, 0.1860939969876131, 0.178884109542495, 0.13983460442365558, 0.1333347659036701, 0.17538025757153897, 0.1696391932397846, 0.20898620252861944, 0.13966198754307735, 0.16681340951018842, 0.15435129490577815, 0.17259608440554453, 0.13418339668712487, 0.17750849571150318, 0.11859589278419369, 0.12904342847833394, 0.15933658299728104, 0.1690927229376843, 0.1206534886484677, 0.12878998561392474, 0.14869086718791347, 0.08064021761645634, 0.043298800064635046, 0.016498715065894287, 0.031701182988220666, 0.038139907104323356, 0.03680419133551005, 0.061512302278703634, 0.04852348645254645, 0.06805068862738084, 0.11292051162170702, 0.0957284223220689, 0.0960733407659844, 0.053021487728643346, 0.05364917729251395, 0.09389110008446189, 0.07903185320198969, 0.06729162809308142, 0.0154848804604657, 9.999999999998899e-05, 9.999999999998899e-05, 0.007595369039079203, 0.0025259036735233442, 0.0003589649686306373, 0.00018239067616576676, 9.999999999998899e-05, 9.999999999998899e-05, 0.02985117739490939, 0.043091437638419716, 0.0378348807083041, 0.039508122018701575, 0.036229829907631794, 0.0297712811357278, 0.03988485537509201, 0.048109500141186756, 0.06286111091181812, 0.2511888026644351, 0.24810992255575093, 0.24402358187859663, 0.23036495483298214, 0.24943361422814858, 0.23986209029161054, 0.30987712851975313, 0.2597461877557691, 0.27065104071735535, 0.04867112567335119, 0.06820909165177491, 0.051486197879023754, 0.050267593628658735, 0.040101542697310144, 0.04072859991481814, 0.054627671432407765, 0.032862792041209676, 0.05433701213911424, 0.17444936183953996, 0.14371516083960323, 0.12946589274416587, 0.13747013262964392, 0.1536497678561689, 0.12043448571044268, 0.1571485249749106, 0.14464974352974658, 0.14497734269037155, 0.1708139789123294, 0.1586413903771685, 0.148184360253665, 0.18311320569040113, 0.13184737136901292, 0.14460040537159025, 0.16124461403912715, 0.13020743501763787, 0.12221087930504726, 0.10940003030126377, 0.13797734420706276, 0.07769349794558622, 0.12007842078417075, 0.11420923196346888, 0.10861229835131148, 0.11155019915498798, 0.08474833677386073, 0.1158298687992636, 0.17113078066608556, 0.13682451244612515, 0.1399790605469592, 0.1776663608115603, 0.1534014260329143, 0.15712298818013637, 0.18413458292615092, 0.14145368570814953, 0.15261969021389632, 0.18858712514247244, 0.1730069271655862, 0.1803150320316319, 0.1818539411500747, 0.26293646454379493, 0.18380344683991567, 0.1811667670841648, 0.23671128241740202, 0.1842364488710415, 0.1349898987331568, 0.2013768030665769, 0.1037038311940447, 0.24798713808471506, 0.2044653147518426, 0.21455787443950325, 0.15664723064753927, 0.22191957410274776, 0.2486240179329362, 0.1616701842324415, 0.2133346743371719, 0.11696931124601961, 0.24286022215797276, 0.13932911821064042, 0.21664091937944374, 0.12566389461873628, 0.09967917194658626, 0.09438261678021043, 0.18024044736995637, 0.19727048764549104, 0.18890798155942223, 0.18383336497192415, 0.19616852667788487, 0.18595338125752192, 0.19266922079698612, 0.18204855410716192, 0.17826379439861162, 0.05273641752896874, 0.04897218547807569, 0.04648730377882149, 0.05683441875198225, 0.041714007709949374, 0.0345105000343352, 0.0626712221614758, 0.043885034159927505, 0.03633447168162529]}, "mutation_prompt": null}
{"id": "4b1002c5-f29c-4f0e-bb77-0b9bb4b42cca", "solution": "import numpy as np\nfrom scipy.optimize import differential_evolution\n\nclass Novel_Metaheuristic_Optimizer:\n    def __init__(self, budget, dim, num_particles=30, max_iterations=1000, inertia_weight=0.7, cognitive_weight=1.4, social_weight=1.4, initial_temperature=100, cooling_rate=0.95):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = num_particles\n        self.max_iterations = max_iterations\n        self.inertia_weight = inertia_weight\n        self.cognitive_weight = cognitive_weight\n        self.social_weight = social_weight\n        self.initial_temperature = initial_temperature\n        self.cooling_rate = cooling_rate\n\n    def __call__(self, func):\n        def novel_metaheuristic_helper():\n            # PSO initialization\n            # Dynamic adjustment of inertia weight based on exploration-exploitation balance\n            self.inertia_weight = np.random.uniform(0.4, 0.9)  # Update the inertia weight dynamically\n            # PSO optimization loop\n            # Simulated Annealing (SA) initialization\n            current_solution = np.random.uniform(-5.0, 5.0, size=self.dim)\n            current_cost = func(current_solution)\n            best_solution = current_solution\n            best_cost = current_cost\n            # SA optimization loop\n            for _ in range(int(self.budget*0.7)):  # Spend 70% of the budget on SA\n                proposal_solution = current_solution + np.random.normal(0, 0.1, size=self.dim)\n                proposal_solution = np.clip(proposal_solution, -5.0, 5.0)\n                proposal_cost = func(proposal_solution)\n                if proposal_cost < current_cost or np.random.rand() < np.exp((current_cost - proposal_cost) / self.initial_temperature):\n                    current_solution = proposal_solution\n                    current_cost = proposal_cost\n                    if proposal_cost < best_cost:\n                        best_solution = proposal_solution\n                        best_cost = proposal_cost\n            # Differential Evolution (DE) optimization\n            de_result = differential_evolution(func, bounds=[(-5.0, 5.0)]*self.dim, maxiter=int(self.budget*0.3))  # Spend 30% of the budget on DE\n            if de_result.fun < best_cost:\n                best_solution = de_result.x\n            return best_solution\n\n        return novel_metaheuristic_helper()", "name": "Novel_Metaheuristic_Optimizer", "description": "A novel metaheuristic algorithm that combines Particle Swarm Optimization (PSO), Simulated Annealing (SA), and Differential Evolution (DE) for enhanced global exploration and local exploitation in black box optimization tasks.", "configspace": "", "generation": 74, "fitness": 0.12523380220864708, "feedback": "The algorithm Novel_Metaheuristic_Optimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.13 with standard deviation 0.08.", "error": "", "parent_id": "654477e0-4d3a-43cc-b192-f885349aa5cc", "metadata": {"aucs": [0.28973149172408763, 0.3243343389857559, 0.3104742755271688, 0.2744396126493731, 0.2774458722826103, 0.31926094604124955, 0.32791907153315547, 0.28532970889671316, 0.39438341350319917, 0.12397055834086712, 0.04939007869361833, 0.09422327201410585, 0.10233794387117101, 0.05302389454059209, 0.06373305039398669, 0.10284165005858503, 0.03760881254179349, 0.055337717561820154, 0.04150802651161578, 0.05075537176052114, 0.03365153336430138, 0.06215022989286334, 0.07594145050725909, 0.06130811290555027, 0.03230559872452954, 0.04370902295136947, 0.055898722931636224, 0.06004933963820824, 0.03165736385000251, 0.039569653261228965, 0.048837084967940814, 0.044710756467256196, 0.04343610801577891, 0.038382042470198385, 0.07847093880912492, 0.04424343571404732, 0.07010485073933248, 0.05395409943193574, 0.05652898369471915, 0.18589990476891438, 0.23434793106584673, 0.2441959444329561, 0.06058360204785662, 0.05567752275458138, 0.1034196237728231, 0.11045008720817784, 0.11347123230506417, 0.09850697481016701, 0.10384130000541192, 0.08682883545352216, 0.07679838170617093, 0.17127603707329597, 0.21701579972591079, 0.14094456579986547, 0.1333888930933037, 0.12059374628619768, 0.11098051857991864, 0.16772864386416664, 0.11453810633532446, 0.12840024522631288, 0.1353829815106592, 0.17860860852578642, 0.1860939969876131, 0.178884109542495, 0.13983460442365558, 0.1333347659036701, 0.17538025757153897, 0.1696391932397846, 0.20898620252861944, 0.13966198754307735, 0.16681340951018842, 0.15435129490577815, 0.17259608440554453, 0.13418339668712487, 0.17750849571150318, 0.11859589278419369, 0.12904342847833394, 0.15933658299728104, 0.1690927229376843, 0.1206534886484677, 0.12878998561392474, 0.14869086718791347, 0.08064021761645634, 0.043298800064635046, 0.016498715065894287, 0.031701182988220666, 0.038139907104323356, 0.03680419133551005, 0.061512302278703634, 0.04852348645254645, 0.06805068862738084, 0.11292051162170702, 0.0957284223220689, 0.0960733407659844, 0.053021487728643346, 0.05364917729251395, 0.09389110008446189, 0.07903185320198969, 0.06729162809308142, 0.0154848804604657, 9.999999999998899e-05, 9.999999999998899e-05, 0.007595369039079203, 0.0025259036735233442, 0.0003589649686306373, 0.00018239067616576676, 9.999999999998899e-05, 9.999999999998899e-05, 0.02985117739490939, 0.043091437638419716, 0.0378348807083041, 0.039508122018701575, 0.036229829907631794, 0.0297712811357278, 0.03988485537509201, 0.048109500141186756, 0.06286111091181812, 0.2511888026644351, 0.24810992255575093, 0.24402358187859663, 0.23036495483298214, 0.24943361422814858, 0.23986209029161054, 0.30987712851975313, 0.2597461877557691, 0.27065104071735535, 0.04867112567335119, 0.06820909165177491, 0.051486197879023754, 0.050267593628658735, 0.040101542697310144, 0.04072859991481814, 0.054627671432407765, 0.032862792041209676, 0.05433701213911424, 0.17444936183953996, 0.14371516083960323, 0.12946589274416587, 0.13747013262964392, 0.1536497678561689, 0.12043448571044268, 0.1571485249749106, 0.14464974352974658, 0.14497734269037155, 0.1708139789123294, 0.1586413903771685, 0.148184360253665, 0.18311320569040113, 0.13184737136901292, 0.14460040537159025, 0.16124461403912715, 0.13020743501763787, 0.12221087930504726, 0.10940003030126377, 0.13797734420706276, 0.07769349794558622, 0.12007842078417075, 0.11420923196346888, 0.10861229835131148, 0.11155019915498798, 0.08474833677386073, 0.1158298687992636, 0.17113078066608556, 0.13682451244612515, 0.1399790605469592, 0.1776663608115603, 0.1534014260329143, 0.15712298818013637, 0.18413458292615092, 0.14145368570814953, 0.15261969021389632, 0.18858712514247244, 0.1730069271655862, 0.1803150320316319, 0.1818539411500747, 0.26293646454379493, 0.18380344683991567, 0.1811667670841648, 0.23671128241740202, 0.1842364488710415, 0.1349898987331568, 0.2013768030665769, 0.1037038311940447, 0.24798713808471506, 0.2044653147518426, 0.21455787443950325, 0.15664723064753927, 0.22191957410274776, 0.2486240179329362, 0.1616701842324415, 0.2133346743371719, 0.11696931124601961, 0.24286022215797276, 0.13932911821064042, 0.21664091937944374, 0.12566389461873628, 0.09967917194658626, 0.09438261678021043, 0.18024044736995637, 0.19727048764549104, 0.18890798155942223, 0.18383336497192415, 0.19616852667788487, 0.18595338125752192, 0.19266922079698612, 0.18204855410716192, 0.17826379439861162, 0.05273641752896874, 0.04897218547807569, 0.04648730377882149, 0.05683441875198225, 0.041714007709949374, 0.0345105000343352, 0.0626712221614758, 0.043885034159927505, 0.03633447168162529]}, "mutation_prompt": null}
{"id": "1c0e68dc-055b-4aa7-b535-8e925598cf86", "solution": "import numpy as np\nfrom scipy.optimize import differential_evolution\n\nclass Refined_Hybrid_PSODE_SA_Optimizer(Hybrid_PSODE_SA_Optimizer):\n    def __init__(self, budget, dim, num_particles=30, max_iterations=1000, inertia_weight=0.7, cognitive_weight=1.4, social_weight=1.4, initial_temperature=100, cooling_rate=0.95):\n        super().__init__(budget, dim, num_particles, max_iterations, inertia_weight, cognitive_weight, social_weight, initial_temperature, cooling_rate)\n\n    def __call__(self, func):\n        def refined_hybrid_pso_de_sa_helper():\n            # Refined PSO initialization\n            self.inertia_weight = np.random.uniform(0.5, 0.8)  # Refined inertia weight update\n            # Refined SA optimization loop\n            for _ in range(int(self.budget*0.65)):  # Refined 65% budget allocation for SA\n                proposal_solution = current_solution + np.random.normal(0, 0.15, size=self.dim)  # Refined SA proposal generation\n                proposal_solution = np.clip(proposal_solution, -4.5, 4.5)  # Refined solution clipping\n                proposal_cost = func(proposal_solution)\n            # Refined DE optimization\n            de_result = differential_evolution(func, bounds=[(-4.5, 4.5)]*self.dim, maxiter=int(self.budget*0.35))  # Refined 35% budget allocation for DE\n            return de_result.x if de_result.fun < best_cost else best_solution\n\n        return refined_hybrid_pso_de_sa_helper()", "name": "Refined_Hybrid_PSODE_SA_Optimizer", "description": "Enhancing PSO with Simulated Annealing and Differential Evolution for a hybrid metaheuristic approach improving global exploration and local exploitation in optimization with refined strategies.", "configspace": "", "generation": 75, "fitness": -Infinity, "feedback": "An exception occurred: NameError(\"name 'current_solution' is not defined\").", "error": "NameError(\"name 'current_solution' is not defined\")", "parent_id": "654477e0-4d3a-43cc-b192-f885349aa5cc", "metadata": {}, "mutation_prompt": null}
{"id": "dfe55465-c75d-466f-bcf2-6a5f4cd08645", "solution": "import numpy as np\nfrom scipy.optimize import differential_evolution\n\nclass Hybrid_PSODE_SA_Optimizer:\n    def __init__(self, budget, dim, num_particles=30, max_iterations=1000, inertia_weight=0.7, cognitive_weight=1.4, social_weight=1.4, initial_temperature=100, cooling_rate=0.95):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = num_particles\n        self.max_iterations = max_iterations\n        self.inertia_weight = inertia_weight\n        self.cognitive_weight = cognitive_weight\n        self.social_weight = social_weight\n        self.initial_temperature = initial_temperature\n        self.cooling_rate = cooling_rate\n\n    def __call__(self, func):\n        def hybrid_pso_de_sa_helper():\n            # PSO initialization\n            # Dynamic adjustment of inertia weight based on exploration-exploitation balance\n            self.inertia_weight = np.random.uniform(0.4, 0.9)  # Update the inertia weight dynamically\n            # PSO optimization loop\n            # Simulated Annealing (SA) initialization\n            current_solution = np.random.uniform(-5.0, 5.0, size=self.dim)\n            current_cost = func(current_solution)\n            best_solution = current_solution\n            best_cost = current_cost\n            # SA optimization loop\n            for _ in range(int(self.budget*0.7)):  # Spend 70% of the budget on SA\n                proposal_solution = current_solution + np.random.normal(0, 0.1, size=self.dim)\n                proposal_solution = np.clip(proposal_solution, -5.0, 5.0)\n                proposal_cost = func(proposal_solution)\n                if proposal_cost < current_cost or np.random.rand() < np.exp((current_cost - proposal_cost) / self.initial_temperature):\n                    current_solution = proposal_solution\n                    current_cost = proposal_cost\n                    if proposal_cost < best_cost:\n                        best_solution = proposal_solution\n                        best_cost = proposal_cost\n            # Differential Evolution (DE) optimization\n            de_result = differential_evolution(func, bounds=[(-5.0, 5.0)]*self.dim, maxiter=int(self.budget*0.3))  # Spend 30% of the budget on DE\n            if de_result.fun < best_cost:\n                best_solution = de_result.x\n            return best_solution\n\n        return hybrid_pso_de_sa_helper()", "name": "Hybrid_PSODE_SA_Optimizer", "description": "Enhancing PSO with Simulated Annealing and Differential Evolution for a hybrid metaheuristic approach improving global exploration and local exploitation in optimization.", "configspace": "", "generation": 56, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "654477e0-4d3a-43cc-b192-f885349aa5cc", "metadata": {"aucs": [0.28973149172408763, 0.3243343389857559, 0.3104742755271688, 0.2744396126493731, 0.2774458722826103, 0.31926094604124955, 0.32791907153315547, 0.28532970889671316, 0.39438341350319917, 0.12397055834086712, 0.04939007869361833, 0.09422327201410585, 0.10233794387117101, 0.05302389454059209, 0.06373305039398669, 0.10284165005858503, 0.03760881254179349, 0.055337717561820154, 0.04150802651161578, 0.05075537176052114, 0.03365153336430138, 0.06215022989286334, 0.07594145050725909, 0.06130811290555027, 0.03230559872452954, 0.04370902295136947, 0.055898722931636224, 0.06004933963820824, 0.03165736385000251, 0.039569653261228965, 0.048837084967940814, 0.044710756467256196, 0.04343610801577891, 0.038382042470198385, 0.07847093880912492, 0.04424343571404732, 0.07010485073933248, 0.05395409943193574, 0.05652898369471915, 0.18589990476891438, 0.23434793106584673, 0.2441959444329561, 0.06058360204785662, 0.05567752275458138, 0.1034196237728231, 0.11045008720817784, 0.11347123230506417, 0.09850697481016701, 0.10384130000541192, 0.08682883545352216, 0.07679838170617093, 0.17127603707329597, 0.21701579972591079, 0.14094456579986547, 0.1333888930933037, 0.12059374628619768, 0.11098051857991864, 0.16772864386416664, 0.11453810633532446, 0.12840024522631288, 0.1353829815106592, 0.17860860852578642, 0.1860939969876131, 0.178884109542495, 0.13983460442365558, 0.1333347659036701, 0.17538025757153897, 0.1696391932397846, 0.20898620252861944, 0.13966198754307735, 0.16681340951018842, 0.15435129490577815, 0.17259608440554453, 0.13418339668712487, 0.17750849571150318, 0.11859589278419369, 0.12904342847833394, 0.15933658299728104, 0.1690927229376843, 0.1206534886484677, 0.12878998561392474, 0.14869086718791347, 0.08064021761645634, 0.043298800064635046, 0.016498715065894287, 0.031701182988220666, 0.038139907104323356, 0.03680419133551005, 0.061512302278703634, 0.04852348645254645, 0.06805068862738084, 0.11292051162170702, 0.0957284223220689, 0.0960733407659844, 0.053021487728643346, 0.05364917729251395, 0.09389110008446189, 0.07903185320198969, 0.06729162809308142, 0.0154848804604657, 9.999999999998899e-05, 9.999999999998899e-05, 0.007595369039079203, 0.0025259036735233442, 0.0003589649686306373, 0.00018239067616576676, 9.999999999998899e-05, 9.999999999998899e-05, 0.02985117739490939, 0.043091437638419716, 0.0378348807083041, 0.039508122018701575, 0.036229829907631794, 0.0297712811357278, 0.03988485537509201, 0.048109500141186756, 0.06286111091181812, 0.2511888026644351, 0.24810992255575093, 0.24402358187859663, 0.23036495483298214, 0.24943361422814858, 0.23986209029161054, 0.30987712851975313, 0.2597461877557691, 0.27065104071735535, 0.04867112567335119, 0.06820909165177491, 0.051486197879023754, 0.050267593628658735, 0.040101542697310144, 0.04072859991481814, 0.054627671432407765, 0.032862792041209676, 0.05433701213911424, 0.17444936183953996, 0.14371516083960323, 0.12946589274416587, 0.13747013262964392, 0.1536497678561689, 0.12043448571044268, 0.1571485249749106, 0.14464974352974658, 0.14497734269037155, 0.1708139789123294, 0.1586413903771685, 0.148184360253665, 0.18311320569040113, 0.13184737136901292, 0.14460040537159025, 0.16124461403912715, 0.13020743501763787, 0.12221087930504726, 0.10940003030126377, 0.13797734420706276, 0.07769349794558622, 0.12007842078417075, 0.11420923196346888, 0.10861229835131148, 0.11155019915498798, 0.08474833677386073, 0.1158298687992636, 0.17113078066608556, 0.13682451244612515, 0.1399790605469592, 0.1776663608115603, 0.1534014260329143, 0.15712298818013637, 0.18413458292615092, 0.14145368570814953, 0.15261969021389632, 0.18858712514247244, 0.1730069271655862, 0.1803150320316319, 0.1818539411500747, 0.26293646454379493, 0.18380344683991567, 0.1811667670841648, 0.23671128241740202, 0.1842364488710415, 0.1349898987331568, 0.2013768030665769, 0.1037038311940447, 0.24798713808471506, 0.2044653147518426, 0.21455787443950325, 0.15664723064753927, 0.22191957410274776, 0.2486240179329362, 0.1616701842324415, 0.2133346743371719, 0.11696931124601961, 0.24286022215797276, 0.13932911821064042, 0.21664091937944374, 0.12566389461873628, 0.09967917194658626, 0.09438261678021043, 0.18024044736995637, 0.19727048764549104, 0.18890798155942223, 0.18383336497192415, 0.19616852667788487, 0.18595338125752192, 0.19266922079698612, 0.18204855410716192, 0.17826379439861162, 0.05273641752896874, 0.04897218547807569, 0.04648730377882149, 0.05683441875198225, 0.041714007709949374, 0.0345105000343352, 0.0626712221614758, 0.043885034159927505, 0.03633447168162529]}, "mutation_prompt": null}
{"id": "eb5a5c64-5e99-4ee5-8802-2c94f11b433f", "solution": "import numpy as np\nfrom scipy.optimize import differential_evolution\n\nclass Hybrid_PSODE_SA_Optimizer:\n    def __init__(self, budget, dim, num_particles=30, max_iterations=1000, inertia_weight=0.7, cognitive_weight=1.4, social_weight=1.4, initial_temperature=100, cooling_rate=0.95):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = num_particles\n        self.max_iterations = max_iterations\n        self.inertia_weight = inertia_weight\n        self.cognitive_weight = cognitive_weight\n        self.social_weight = social_weight\n        self.initial_temperature = initial_temperature\n        self.cooling_rate = cooling_rate\n\n    def __call__(self, func):\n        def hybrid_pso_de_sa_helper():\n            # PSO initialization\n            # Dynamic adjustment of inertia weight based on exploration-exploitation balance\n            self.inertia_weight = np.random.uniform(0.4, 0.9)  # Update the inertia weight dynamically\n            # PSO optimization loop\n            # Simulated Annealing (SA) initialization\n            current_solution = np.random.uniform(-5.0, 5.0, size=self.dim)\n            current_cost = func(current_solution)\n            best_solution = current_solution\n            best_cost = current_cost\n            # SA optimization loop\n            for _ in range(int(self.budget*0.7)):  # Spend 70% of the budget on SA\n                proposal_solution = current_solution + np.random.normal(0, 0.1, size=self.dim)\n                proposal_solution = np.clip(proposal_solution, -5.0, 5.0)\n                proposal_cost = func(proposal_solution)\n                if proposal_cost < current_cost or np.random.rand() < np.exp((current_cost - proposal_cost) / self.initial_temperature):\n                    current_solution = proposal_solution\n                    current_cost = proposal_cost\n                    if proposal_cost < best_cost:\n                        best_solution = proposal_solution\n                        best_cost = proposal_cost\n            # Differential Evolution (DE) optimization\n            de_result = differential_evolution(func, bounds=[(-5.0, 5.0)]*self.dim, maxiter=int(self.budget*0.3))  # Spend 30% of the budget on DE\n            if de_result.fun < best_cost:\n                best_solution = de_result.x\n            return best_solution\n\n        return hybrid_pso_de_sa_helper()", "name": "Hybrid_PSODE_SA_Optimizer", "description": "Enhancing PSO with Simulated Annealing and Differential Evolution for a hybrid metaheuristic approach improving global exploration and local exploitation in optimization.", "configspace": "", "generation": 56, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "654477e0-4d3a-43cc-b192-f885349aa5cc", "metadata": {"aucs": [0.28973149172408763, 0.3243343389857559, 0.3104742755271688, 0.2744396126493731, 0.2774458722826103, 0.31926094604124955, 0.32791907153315547, 0.28532970889671316, 0.39438341350319917, 0.12397055834086712, 0.04939007869361833, 0.09422327201410585, 0.10233794387117101, 0.05302389454059209, 0.06373305039398669, 0.10284165005858503, 0.03760881254179349, 0.055337717561820154, 0.04150802651161578, 0.05075537176052114, 0.03365153336430138, 0.06215022989286334, 0.07594145050725909, 0.06130811290555027, 0.03230559872452954, 0.04370902295136947, 0.055898722931636224, 0.06004933963820824, 0.03165736385000251, 0.039569653261228965, 0.048837084967940814, 0.044710756467256196, 0.04343610801577891, 0.038382042470198385, 0.07847093880912492, 0.04424343571404732, 0.07010485073933248, 0.05395409943193574, 0.05652898369471915, 0.18589990476891438, 0.23434793106584673, 0.2441959444329561, 0.06058360204785662, 0.05567752275458138, 0.1034196237728231, 0.11045008720817784, 0.11347123230506417, 0.09850697481016701, 0.10384130000541192, 0.08682883545352216, 0.07679838170617093, 0.17127603707329597, 0.21701579972591079, 0.14094456579986547, 0.1333888930933037, 0.12059374628619768, 0.11098051857991864, 0.16772864386416664, 0.11453810633532446, 0.12840024522631288, 0.1353829815106592, 0.17860860852578642, 0.1860939969876131, 0.178884109542495, 0.13983460442365558, 0.1333347659036701, 0.17538025757153897, 0.1696391932397846, 0.20898620252861944, 0.13966198754307735, 0.16681340951018842, 0.15435129490577815, 0.17259608440554453, 0.13418339668712487, 0.17750849571150318, 0.11859589278419369, 0.12904342847833394, 0.15933658299728104, 0.1690927229376843, 0.1206534886484677, 0.12878998561392474, 0.14869086718791347, 0.08064021761645634, 0.043298800064635046, 0.016498715065894287, 0.031701182988220666, 0.038139907104323356, 0.03680419133551005, 0.061512302278703634, 0.04852348645254645, 0.06805068862738084, 0.11292051162170702, 0.0957284223220689, 0.0960733407659844, 0.053021487728643346, 0.05364917729251395, 0.09389110008446189, 0.07903185320198969, 0.06729162809308142, 0.0154848804604657, 9.999999999998899e-05, 9.999999999998899e-05, 0.007595369039079203, 0.0025259036735233442, 0.0003589649686306373, 0.00018239067616576676, 9.999999999998899e-05, 9.999999999998899e-05, 0.02985117739490939, 0.043091437638419716, 0.0378348807083041, 0.039508122018701575, 0.036229829907631794, 0.0297712811357278, 0.03988485537509201, 0.048109500141186756, 0.06286111091181812, 0.2511888026644351, 0.24810992255575093, 0.24402358187859663, 0.23036495483298214, 0.24943361422814858, 0.23986209029161054, 0.30987712851975313, 0.2597461877557691, 0.27065104071735535, 0.04867112567335119, 0.06820909165177491, 0.051486197879023754, 0.050267593628658735, 0.040101542697310144, 0.04072859991481814, 0.054627671432407765, 0.032862792041209676, 0.05433701213911424, 0.17444936183953996, 0.14371516083960323, 0.12946589274416587, 0.13747013262964392, 0.1536497678561689, 0.12043448571044268, 0.1571485249749106, 0.14464974352974658, 0.14497734269037155, 0.1708139789123294, 0.1586413903771685, 0.148184360253665, 0.18311320569040113, 0.13184737136901292, 0.14460040537159025, 0.16124461403912715, 0.13020743501763787, 0.12221087930504726, 0.10940003030126377, 0.13797734420706276, 0.07769349794558622, 0.12007842078417075, 0.11420923196346888, 0.10861229835131148, 0.11155019915498798, 0.08474833677386073, 0.1158298687992636, 0.17113078066608556, 0.13682451244612515, 0.1399790605469592, 0.1776663608115603, 0.1534014260329143, 0.15712298818013637, 0.18413458292615092, 0.14145368570814953, 0.15261969021389632, 0.18858712514247244, 0.1730069271655862, 0.1803150320316319, 0.1818539411500747, 0.26293646454379493, 0.18380344683991567, 0.1811667670841648, 0.23671128241740202, 0.1842364488710415, 0.1349898987331568, 0.2013768030665769, 0.1037038311940447, 0.24798713808471506, 0.2044653147518426, 0.21455787443950325, 0.15664723064753927, 0.22191957410274776, 0.2486240179329362, 0.1616701842324415, 0.2133346743371719, 0.11696931124601961, 0.24286022215797276, 0.13932911821064042, 0.21664091937944374, 0.12566389461873628, 0.09967917194658626, 0.09438261678021043, 0.18024044736995637, 0.19727048764549104, 0.18890798155942223, 0.18383336497192415, 0.19616852667788487, 0.18595338125752192, 0.19266922079698612, 0.18204855410716192, 0.17826379439861162, 0.05273641752896874, 0.04897218547807569, 0.04648730377882149, 0.05683441875198225, 0.041714007709949374, 0.0345105000343352, 0.0626712221614758, 0.043885034159927505, 0.03633447168162529]}, "mutation_prompt": null}
{"id": "e1c4fa8d-5510-4a31-8047-26201372bb09", "solution": "import numpy as np\nfrom scipy.optimize import differential_evolution\n\nclass Hybrid_PSODE_SA_Optimizer:\n    def __init__(self, budget, dim, num_particles=30, max_iterations=1000, inertia_weight=0.7, cognitive_weight=1.4, social_weight=1.4, initial_temperature=100, cooling_rate=0.95):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = num_particles\n        self.max_iterations = max_iterations\n        self.inertia_weight = inertia_weight\n        self.cognitive_weight = cognitive_weight\n        self.social_weight = social_weight\n        self.initial_temperature = initial_temperature\n        self.cooling_rate = cooling_rate\n\n    def __call__(self, func):\n        def hybrid_pso_de_sa_helper():\n            # PSO initialization\n            # Dynamic adjustment of inertia weight based on exploration-exploitation balance\n            self.inertia_weight = np.random.uniform(0.4, 0.9)  # Update the inertia weight dynamically\n            # PSO optimization loop\n            # Simulated Annealing (SA) initialization\n            current_solution = np.random.uniform(-5.0, 5.0, size=self.dim)\n            current_cost = func(current_solution)\n            best_solution = current_solution\n            best_cost = current_cost\n            # SA optimization loop\n            for _ in range(int(self.budget*0.7)):  # Spend 70% of the budget on SA\n                proposal_solution = current_solution + np.random.normal(0, 0.1, size=self.dim)\n                proposal_solution = np.clip(proposal_solution, -5.0, 5.0)\n                proposal_cost = func(proposal_solution)\n                if proposal_cost < current_cost or np.random.rand() < np.exp((current_cost - proposal_cost) / self.initial_temperature):\n                    current_solution = proposal_solution\n                    current_cost = proposal_cost\n                    if proposal_cost < best_cost:\n                        best_solution = proposal_solution\n                        best_cost = proposal_cost\n            # Differential Evolution (DE) optimization\n            de_result = differential_evolution(func, bounds=[(-5.0, 5.0)]*self.dim, maxiter=int(self.budget*0.3))  # Spend 30% of the budget on DE\n            if de_result.fun < best_cost:\n                best_solution = de_result.x\n            return best_solution\n\n        return hybrid_pso_de_sa_helper()", "name": "Hybrid_PSODE_SA_Optimizer", "description": "Enhancing PSO with Simulated Annealing and Differential Evolution for a hybrid metaheuristic approach improving global exploration and local exploitation in optimization.", "configspace": "", "generation": 56, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "654477e0-4d3a-43cc-b192-f885349aa5cc", "metadata": {"aucs": [0.28973149172408763, 0.3243343389857559, 0.3104742755271688, 0.2744396126493731, 0.2774458722826103, 0.31926094604124955, 0.32791907153315547, 0.28532970889671316, 0.39438341350319917, 0.12397055834086712, 0.04939007869361833, 0.09422327201410585, 0.10233794387117101, 0.05302389454059209, 0.06373305039398669, 0.10284165005858503, 0.03760881254179349, 0.055337717561820154, 0.04150802651161578, 0.05075537176052114, 0.03365153336430138, 0.06215022989286334, 0.07594145050725909, 0.06130811290555027, 0.03230559872452954, 0.04370902295136947, 0.055898722931636224, 0.06004933963820824, 0.03165736385000251, 0.039569653261228965, 0.048837084967940814, 0.044710756467256196, 0.04343610801577891, 0.038382042470198385, 0.07847093880912492, 0.04424343571404732, 0.07010485073933248, 0.05395409943193574, 0.05652898369471915, 0.18589990476891438, 0.23434793106584673, 0.2441959444329561, 0.06058360204785662, 0.05567752275458138, 0.1034196237728231, 0.11045008720817784, 0.11347123230506417, 0.09850697481016701, 0.10384130000541192, 0.08682883545352216, 0.07679838170617093, 0.17127603707329597, 0.21701579972591079, 0.14094456579986547, 0.1333888930933037, 0.12059374628619768, 0.11098051857991864, 0.16772864386416664, 0.11453810633532446, 0.12840024522631288, 0.1353829815106592, 0.17860860852578642, 0.1860939969876131, 0.178884109542495, 0.13983460442365558, 0.1333347659036701, 0.17538025757153897, 0.1696391932397846, 0.20898620252861944, 0.13966198754307735, 0.16681340951018842, 0.15435129490577815, 0.17259608440554453, 0.13418339668712487, 0.17750849571150318, 0.11859589278419369, 0.12904342847833394, 0.15933658299728104, 0.1690927229376843, 0.1206534886484677, 0.12878998561392474, 0.14869086718791347, 0.08064021761645634, 0.043298800064635046, 0.016498715065894287, 0.031701182988220666, 0.038139907104323356, 0.03680419133551005, 0.061512302278703634, 0.04852348645254645, 0.06805068862738084, 0.11292051162170702, 0.0957284223220689, 0.0960733407659844, 0.053021487728643346, 0.05364917729251395, 0.09389110008446189, 0.07903185320198969, 0.06729162809308142, 0.0154848804604657, 9.999999999998899e-05, 9.999999999998899e-05, 0.007595369039079203, 0.0025259036735233442, 0.0003589649686306373, 0.00018239067616576676, 9.999999999998899e-05, 9.999999999998899e-05, 0.02985117739490939, 0.043091437638419716, 0.0378348807083041, 0.039508122018701575, 0.036229829907631794, 0.0297712811357278, 0.03988485537509201, 0.048109500141186756, 0.06286111091181812, 0.2511888026644351, 0.24810992255575093, 0.24402358187859663, 0.23036495483298214, 0.24943361422814858, 0.23986209029161054, 0.30987712851975313, 0.2597461877557691, 0.27065104071735535, 0.04867112567335119, 0.06820909165177491, 0.051486197879023754, 0.050267593628658735, 0.040101542697310144, 0.04072859991481814, 0.054627671432407765, 0.032862792041209676, 0.05433701213911424, 0.17444936183953996, 0.14371516083960323, 0.12946589274416587, 0.13747013262964392, 0.1536497678561689, 0.12043448571044268, 0.1571485249749106, 0.14464974352974658, 0.14497734269037155, 0.1708139789123294, 0.1586413903771685, 0.148184360253665, 0.18311320569040113, 0.13184737136901292, 0.14460040537159025, 0.16124461403912715, 0.13020743501763787, 0.12221087930504726, 0.10940003030126377, 0.13797734420706276, 0.07769349794558622, 0.12007842078417075, 0.11420923196346888, 0.10861229835131148, 0.11155019915498798, 0.08474833677386073, 0.1158298687992636, 0.17113078066608556, 0.13682451244612515, 0.1399790605469592, 0.1776663608115603, 0.1534014260329143, 0.15712298818013637, 0.18413458292615092, 0.14145368570814953, 0.15261969021389632, 0.18858712514247244, 0.1730069271655862, 0.1803150320316319, 0.1818539411500747, 0.26293646454379493, 0.18380344683991567, 0.1811667670841648, 0.23671128241740202, 0.1842364488710415, 0.1349898987331568, 0.2013768030665769, 0.1037038311940447, 0.24798713808471506, 0.2044653147518426, 0.21455787443950325, 0.15664723064753927, 0.22191957410274776, 0.2486240179329362, 0.1616701842324415, 0.2133346743371719, 0.11696931124601961, 0.24286022215797276, 0.13932911821064042, 0.21664091937944374, 0.12566389461873628, 0.09967917194658626, 0.09438261678021043, 0.18024044736995637, 0.19727048764549104, 0.18890798155942223, 0.18383336497192415, 0.19616852667788487, 0.18595338125752192, 0.19266922079698612, 0.18204855410716192, 0.17826379439861162, 0.05273641752896874, 0.04897218547807569, 0.04648730377882149, 0.05683441875198225, 0.041714007709949374, 0.0345105000343352, 0.0626712221614758, 0.043885034159927505, 0.03633447168162529]}, "mutation_prompt": null}
{"id": "98db56be-60b6-4b8b-8f48-918ae4c440d5", "solution": "import numpy as np\nfrom scipy.optimize import differential_evolution\n\nclass Enhanced_Hybrid_PSODE_SA_Optimizer:\n    def __init__(self, budget, dim, num_particles=30, max_iterations=1000, inertia_weight=0.7, cognitive_weight=1.4, social_weight=1.4, initial_temperature=100, cooling_rate=0.95):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = num_particles\n        self.max_iterations = max_iterations\n        self.inertia_weight = inertia_weight\n        self.cognitive_weight = cognitive_weight\n        self.social_weight = social_weight\n        self.initial_temperature = initial_temperature\n        self.cooling_rate = cooling_rate\n\n    def __call__(self, func):\n        def enhanced_hybrid_pso_de_sa_helper():\n            # PSO initialization\n            self.inertia_weight = np.random.uniform(0.4, 0.9)\n            # PSO optimization loop\n            # Simulated Annealing (SA) initialization\n            current_solution = np.random.uniform(-5.0, 5.0, size=self.dim)\n            current_cost = func(current_solution)\n            best_solution = current_solution\n            best_cost = current_cost\n            # SA optimization loop\n            for _ in range(int(self.budget*0.7)):\n                proposal_solution = current_solution + np.random.normal(0, 0.1, size=self.dim)\n                proposal_solution = np.clip(proposal_solution, -5.0, 5.0)\n                proposal_cost = func(proposal_solution)\n                if proposal_cost < current_cost or np.random.rand() < np.exp((current_cost - proposal_cost) / self.initial_temperature):\n                    current_solution = proposal_solution\n                    current_cost = proposal_cost\n                    if proposal_cost < best_cost:\n                        best_solution = proposal_solution\n                        best_cost = proposal_cost\n            # Differential Evolution (DE) optimization\n            de_result = differential_evolution(func, bounds=[(-5.0, 5.0)]*self.dim, maxiter=int(self.budget*0.3))\n            if de_result.fun < best_cost:\n                best_solution = de_result.x\n            return best_solution\n\n        return enhanced_hybrid_pso_de_sa_helper()", "name": "Enhanced_Hybrid_PSODE_SA_Optimizer", "description": "Hybridizing Particle Swarm Optimization with Simulated Annealing and Differential Evolution for enhanced global exploration and local exploitation in black box optimization.", "configspace": "", "generation": 79, "fitness": 0.12523380220864708, "feedback": "The algorithm Enhanced_Hybrid_PSODE_SA_Optimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.13 with standard deviation 0.08.", "error": "", "parent_id": "654477e0-4d3a-43cc-b192-f885349aa5cc", "metadata": {"aucs": [0.28973149172408763, 0.3243343389857559, 0.3104742755271688, 0.2744396126493731, 0.2774458722826103, 0.31926094604124955, 0.32791907153315547, 0.28532970889671316, 0.39438341350319917, 0.12397055834086712, 0.04939007869361833, 0.09422327201410585, 0.10233794387117101, 0.05302389454059209, 0.06373305039398669, 0.10284165005858503, 0.03760881254179349, 0.055337717561820154, 0.04150802651161578, 0.05075537176052114, 0.03365153336430138, 0.06215022989286334, 0.07594145050725909, 0.06130811290555027, 0.03230559872452954, 0.04370902295136947, 0.055898722931636224, 0.06004933963820824, 0.03165736385000251, 0.039569653261228965, 0.048837084967940814, 0.044710756467256196, 0.04343610801577891, 0.038382042470198385, 0.07847093880912492, 0.04424343571404732, 0.07010485073933248, 0.05395409943193574, 0.05652898369471915, 0.18589990476891438, 0.23434793106584673, 0.2441959444329561, 0.06058360204785662, 0.05567752275458138, 0.1034196237728231, 0.11045008720817784, 0.11347123230506417, 0.09850697481016701, 0.10384130000541192, 0.08682883545352216, 0.07679838170617093, 0.17127603707329597, 0.21701579972591079, 0.14094456579986547, 0.1333888930933037, 0.12059374628619768, 0.11098051857991864, 0.16772864386416664, 0.11453810633532446, 0.12840024522631288, 0.1353829815106592, 0.17860860852578642, 0.1860939969876131, 0.178884109542495, 0.13983460442365558, 0.1333347659036701, 0.17538025757153897, 0.1696391932397846, 0.20898620252861944, 0.13966198754307735, 0.16681340951018842, 0.15435129490577815, 0.17259608440554453, 0.13418339668712487, 0.17750849571150318, 0.11859589278419369, 0.12904342847833394, 0.15933658299728104, 0.1690927229376843, 0.1206534886484677, 0.12878998561392474, 0.14869086718791347, 0.08064021761645634, 0.043298800064635046, 0.016498715065894287, 0.031701182988220666, 0.038139907104323356, 0.03680419133551005, 0.061512302278703634, 0.04852348645254645, 0.06805068862738084, 0.11292051162170702, 0.0957284223220689, 0.0960733407659844, 0.053021487728643346, 0.05364917729251395, 0.09389110008446189, 0.07903185320198969, 0.06729162809308142, 0.0154848804604657, 9.999999999998899e-05, 9.999999999998899e-05, 0.007595369039079203, 0.0025259036735233442, 0.0003589649686306373, 0.00018239067616576676, 9.999999999998899e-05, 9.999999999998899e-05, 0.02985117739490939, 0.043091437638419716, 0.0378348807083041, 0.039508122018701575, 0.036229829907631794, 0.0297712811357278, 0.03988485537509201, 0.048109500141186756, 0.06286111091181812, 0.2511888026644351, 0.24810992255575093, 0.24402358187859663, 0.23036495483298214, 0.24943361422814858, 0.23986209029161054, 0.30987712851975313, 0.2597461877557691, 0.27065104071735535, 0.04867112567335119, 0.06820909165177491, 0.051486197879023754, 0.050267593628658735, 0.040101542697310144, 0.04072859991481814, 0.054627671432407765, 0.032862792041209676, 0.05433701213911424, 0.17444936183953996, 0.14371516083960323, 0.12946589274416587, 0.13747013262964392, 0.1536497678561689, 0.12043448571044268, 0.1571485249749106, 0.14464974352974658, 0.14497734269037155, 0.1708139789123294, 0.1586413903771685, 0.148184360253665, 0.18311320569040113, 0.13184737136901292, 0.14460040537159025, 0.16124461403912715, 0.13020743501763787, 0.12221087930504726, 0.10940003030126377, 0.13797734420706276, 0.07769349794558622, 0.12007842078417075, 0.11420923196346888, 0.10861229835131148, 0.11155019915498798, 0.08474833677386073, 0.1158298687992636, 0.17113078066608556, 0.13682451244612515, 0.1399790605469592, 0.1776663608115603, 0.1534014260329143, 0.15712298818013637, 0.18413458292615092, 0.14145368570814953, 0.15261969021389632, 0.18858712514247244, 0.1730069271655862, 0.1803150320316319, 0.1818539411500747, 0.26293646454379493, 0.18380344683991567, 0.1811667670841648, 0.23671128241740202, 0.1842364488710415, 0.1349898987331568, 0.2013768030665769, 0.1037038311940447, 0.24798713808471506, 0.2044653147518426, 0.21455787443950325, 0.15664723064753927, 0.22191957410274776, 0.2486240179329362, 0.1616701842324415, 0.2133346743371719, 0.11696931124601961, 0.24286022215797276, 0.13932911821064042, 0.21664091937944374, 0.12566389461873628, 0.09967917194658626, 0.09438261678021043, 0.18024044736995637, 0.19727048764549104, 0.18890798155942223, 0.18383336497192415, 0.19616852667788487, 0.18595338125752192, 0.19266922079698612, 0.18204855410716192, 0.17826379439861162, 0.05273641752896874, 0.04897218547807569, 0.04648730377882149, 0.05683441875198225, 0.041714007709949374, 0.0345105000343352, 0.0626712221614758, 0.043885034159927505, 0.03633447168162529]}, "mutation_prompt": null}
{"id": "d82bc67e-0c68-42f5-b876-0815e26c45c3", "solution": "import numpy as np\nfrom scipy.optimize import differential_evolution\n\nclass Hybrid_PSODE_SA_DE_Optimizer:\n    def __init__(self, budget, dim, num_particles=30, max_iterations=1000, inertia_weight=0.7, cognitive_weight=1.4, social_weight=1.4, initial_temperature=100, cooling_rate=0.95):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = num_particles\n        self.max_iterations = max_iterations\n        self.inertia_weight = inertia_weight\n        self.cognitive_weight = cognitive_weight\n        self.social_weight = social_weight\n        self.initial_temperature = initial_temperature\n        self.cooling_rate = cooling_rate\n\n    def __call__(self, func):\n        def hybrid_pso_sa_de_helper():\n            # PSO initialization\n            self.inertia_weight = np.random.uniform(0.4, 0.9)\n            # PSO optimization loop\n            # Simulated Annealing (SA) initialization\n            current_solution = np.random.uniform(-5.0, 5.0, size=self.dim)\n            current_cost = func(current_solution)\n            best_solution = current_solution\n            best_cost = current_cost\n            # SA optimization loop\n            for _ in range(int(self.budget*0.5)):  # Spend 50% of the budget on SA\n                proposal_solution = current_solution + np.random.normal(0, 0.1, size=self.dim)\n                proposal_solution = np.clip(proposal_solution, -5.0, 5.0)\n                proposal_cost = func(proposal_solution)\n                if proposal_cost < current_cost or np.random.rand() < np.exp((current_cost - proposal_cost) / self.initial_temperature):\n                    current_solution = proposal_solution\n                    current_cost = proposal_cost\n                    if proposal_cost < best_cost:\n                        best_solution = proposal_solution\n                        best_cost = proposal_cost\n            # Differential Evolution (DE) optimization\n            de_result = differential_evolution(func, bounds=[(-5.0, 5.0)]*self.dim, maxiter=int(self.budget*0.5))  # Spend 50% of the budget on DE\n            if de_result.fun < best_cost:\n                best_solution = de_result.x\n            return best_solution\n\n        return hybrid_pso_sa_de_helper()\n\noptimizer = Hybrid_PSODE_SA_DE_Optimizer(budget=1000, dim=10)", "name": "Hybrid_PSODE_SA_DE_Optimizer", "description": "Combining Particle Swarm Optimization (PSO), Simulated Annealing (SA), and Differential Evolution (DE) for a hybrid metaheuristic approach to optimize black box functions efficiently within a given budget.", "configspace": "", "generation": 80, "fitness": 0.1824295991437497, "feedback": "The algorithm Hybrid_PSODE_SA_DE_Optimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.18 with standard deviation 0.12.", "error": "", "parent_id": "654477e0-4d3a-43cc-b192-f885349aa5cc", "metadata": {"aucs": [0.48456837961059296, 0.5105538177594323, 0.47507057601350744, 0.47136751809518973, 0.467732558082386, 0.4996677345119218, 0.48477110876648977, 0.47038103501661366, 0.545176959239378, 0.18273261797874463, 0.19429757105452794, 0.21506122537384254, 0.19451909647024235, 0.12146371433165404, 0.24079244109696885, 0.27263541071937736, 0.13427678057795633, 0.1949133209598758, 0.06820291502883957, 0.06966638517654022, 0.12395381192411059, 0.08289695025266608, 0.08296512362937791, 0.0704014936562467, 0.161588407958278, 0.06945382781693155, 0.0722924237460717, 0.09232046452458575, 0.05330572800163913, 0.07118382690440017, 0.08917587563413543, 0.06361582817029554, 0.05932679852603273, 0.0635086490190615, 0.08265301708915862, 0.05814508982599631, 0.12721453922183246, 0.11706329861822595, 0.10630696636794656, 0.4174408512262425, 0.41873076422615274, 0.44278493340983627, 0.2123470216737544, 0.21871732615763728, 0.2200105933850952, 0.22008000655410365, 0.16538357465335463, 0.13387199369701974, 0.16636815324180299, 0.14716754841340518, 0.12746030363756877, 0.21459892081009235, 0.3376740437342389, 0.21157406285358538, 0.16947644277671503, 0.1644703645228397, 0.14361446048647464, 0.1907397339836573, 0.1695538471210719, 0.17152511706131024, 0.18809197229177876, 0.23240056220857597, 0.35739257552103, 0.30711331026701605, 0.35800000372394647, 0.27430480307612815, 0.10107003287669303, 0.38461052355102554, 0.40748602519626975, 0.3329604914039255, 0.3311750758329405, 0.3502616151401581, 0.3634412917063622, 0.3529535045867639, 0.34852116366747576, 0.2831203133435162, 0.24473953266556592, 0.16649180289750054, 0.29474586603190134, 0.1625909842331552, 0.31696283773167133, 0.14869086718791347, 0.07212689495988767, 0.06175901597576361, 0.020191522623446145, 0.03572120668890544, 0.038188410930948935, 0.0538084588160711, 0.06082424657643004, 0.03914242214484198, 0.07873098788749833, 0.1280191711671197, 0.11311878725947622, 0.12625932608897728, 0.08535519357748123, 0.09378270625489715, 0.12059452416952898, 0.10471029393977482, 0.12530781396926283, 0.021758851949409297, 0.01118865514291989, 0.01630852195620791, 0.009717848689135056, 0.005687385866965822, 0.007543861114835626, 0.01032720173348356, 0.01648145006013546, 0.005373827784677321, 0.07782528772435504, 0.07330077990565553, 0.06166601747141709, 0.08055000157538439, 0.07934930144482333, 0.062084647386244196, 0.10632178558194105, 0.07001529129837158, 0.10518737552427082, 0.35100325129559173, 0.37523474412177293, 0.37672805793904796, 0.36548934896301644, 0.3784362034739134, 0.29606697411473326, 0.38347943212156954, 0.36879532140862226, 0.4156318141657802, 0.06382786247443584, 0.07027068159229422, 0.06624128936599916, 0.05655454515347169, 0.05191204552758877, 0.05506194647616636, 0.05754263107754798, 0.044978851032902445, 0.06453824649042239, 0.17430559712214022, 0.14371516083960323, 0.13051352534288252, 0.1486892368088053, 0.14242895630840868, 0.11916859449762929, 0.1571485249749106, 0.14360241955760045, 0.14497734269037155, 0.20741927828451423, 0.19714385027183123, 0.17307556592616313, 0.21943769638530608, 0.17629886886715318, 0.1763960630323771, 0.17415519128369839, 0.14610377103098293, 0.13604240414071722, 0.15761735523911335, 0.17384670425875282, 0.12931732912620886, 0.15729233192226133, 0.15298296199388528, 0.15387607639362466, 0.13204630169711196, 0.1106239180094315, 0.10232685023347143, 0.18187502715462378, 0.15122193339174406, 0.1491135876626546, 0.17632443556245903, 0.1601654742865336, 0.15051735448101877, 0.19316221984484072, 0.1751426705706377, 0.186300441145674, 0.18717676149004225, 0.17272682736838418, 0.18311183184071178, 0.17713748478276103, 0.19446933698451196, 0.2035937959690537, 0.3837886709773012, 0.23595571520615266, 0.18984625380959352, 0.1372146125602216, 0.3375473553526086, 0.3751022808034383, 0.34124726872015887, 0.20497501473067492, 0.3041174875603694, 0.12875970971203898, 0.3946650494814252, 0.1387322605317557, 0.3577015084168791, 0.3406424644901558, 0.14168063322299607, 0.3218763830477227, 0.27719104026340924, 0.13076117137094367, 0.2911946769569581, 0.25316828466381325, 0.20684626425743513, 0.17951653794217437, 0.19543027414880643, 0.1898356726589886, 0.17357588525115408, 0.19616852667788487, 0.18477683563372382, 0.1910169861343758, 0.17449898452055512, 0.16900911271993313, 0.055545142414739845, 0.05029678957846151, 0.0451835029003681, 0.07009360399369924, 0.04843630757627959, 0.05027907031284684, 0.07167402092987196, 0.057166150127647364, 0.05062401906399627]}, "mutation_prompt": null}
{"id": "0092724a-a95a-475c-9ea7-cb414f9047bc", "solution": "import numpy as np\nfrom scipy.optimize import differential_evolution\n\nclass Hybrid_PSODE_SA_DE_Optimizer:\n    def __init__(self, budget, dim, num_particles=30, max_iterations=1000, inertia_weight=0.7, cognitive_weight=1.4, social_weight=1.4, initial_temperature=100, cooling_rate=0.95):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = num_particles\n        self.max_iterations = max_iterations\n        self.inertia_weight = inertia_weight\n        self.cognitive_weight = cognitive_weight\n        self.social_weight = social_weight\n        self.initial_temperature = initial_temperature\n        self.cooling_rate = cooling_rate\n\n    def __call__(self, func):\n        def hybrid_pso_sa_de_helper():\n            # PSO initialization\n            self.inertia_weight = np.random.uniform(0.4, 0.9)\n            # PSO optimization loop\n            # Simulated Annealing (SA) initialization\n            current_solution = np.random.uniform(-5.0, 5.0, size=self.dim)\n            current_cost = func(current_solution)\n            best_solution = current_solution\n            best_cost = current_cost\n            # SA optimization loop\n            for _ in range(int(self.budget*0.5)):  # Spend 50% of the budget on SA\n                proposal_solution = current_solution + np.random.normal(0, 0.1, size=self.dim)\n                proposal_solution = np.clip(proposal_solution, -5.0, 5.0)\n                proposal_cost = func(proposal_solution)\n                if proposal_cost < current_cost or np.random.rand() < np.exp((current_cost - proposal_cost) / self.initial_temperature):\n                    current_solution = proposal_solution\n                    current_cost = proposal_cost\n                    if proposal_cost < best_cost:\n                        best_solution = proposal_solution\n                        best_cost = proposal_cost\n            # Differential Evolution (DE) optimization\n            de_result = differential_evolution(func, bounds=[(-5.0, 5.0)]*self.dim, maxiter=int(self.budget*0.5))  # Spend 50% of the budget on DE\n            if de_result.fun < best_cost:\n                best_solution = de_result.x\n            return best_solution\n\n        return hybrid_pso_sa_de_helper()\n\noptimizer = Hybrid_PSODE_SA_DE_Optimizer(budget=1000, dim=10)", "name": "Hybrid_PSODE_SA_DE_Optimizer", "description": "Combining Particle Swarm Optimization (PSO), Simulated Annealing (SA), and Differential Evolution (DE) for a hybrid metaheuristic approach to optimize black box functions efficiently within a given budget.", "configspace": "", "generation": 81, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "d82bc67e-0c68-42f5-b876-0815e26c45c3", "metadata": {"aucs": [0.48456837961059296, 0.5105538177594323, 0.47507057601350744, 0.47136751809518973, 0.467732558082386, 0.4996677345119218, 0.48477110876648977, 0.47038103501661366, 0.545176959239378, 0.18273261797874463, 0.19429757105452794, 0.21506122537384254, 0.19451909647024235, 0.12146371433165404, 0.24079244109696885, 0.27263541071937736, 0.13427678057795633, 0.1949133209598758, 0.06820291502883957, 0.06966638517654022, 0.12395381192411059, 0.08289695025266608, 0.08296512362937791, 0.0704014936562467, 0.161588407958278, 0.06945382781693155, 0.0722924237460717, 0.09232046452458575, 0.05330572800163913, 0.07118382690440017, 0.08917587563413543, 0.06361582817029554, 0.05932679852603273, 0.0635086490190615, 0.08265301708915862, 0.05814508982599631, 0.12721453922183246, 0.11706329861822595, 0.10630696636794656, 0.4174408512262425, 0.41873076422615274, 0.44278493340983627, 0.2123470216737544, 0.21871732615763728, 0.2200105933850952, 0.22008000655410365, 0.16538357465335463, 0.13387199369701974, 0.16636815324180299, 0.14716754841340518, 0.12746030363756877, 0.21459892081009235, 0.3376740437342389, 0.21157406285358538, 0.16947644277671503, 0.1644703645228397, 0.14361446048647464, 0.1907397339836573, 0.1695538471210719, 0.17152511706131024, 0.18809197229177876, 0.23240056220857597, 0.35739257552103, 0.30711331026701605, 0.35800000372394647, 0.27430480307612815, 0.10107003287669303, 0.38461052355102554, 0.40748602519626975, 0.3329604914039255, 0.3311750758329405, 0.3502616151401581, 0.3634412917063622, 0.3529535045867639, 0.34852116366747576, 0.2831203133435162, 0.24473953266556592, 0.16649180289750054, 0.29474586603190134, 0.1625909842331552, 0.31696283773167133, 0.14869086718791347, 0.07212689495988767, 0.06175901597576361, 0.020191522623446145, 0.03572120668890544, 0.038188410930948935, 0.0538084588160711, 0.06082424657643004, 0.03914242214484198, 0.07873098788749833, 0.1280191711671197, 0.11311878725947622, 0.12625932608897728, 0.08535519357748123, 0.09378270625489715, 0.12059452416952898, 0.10471029393977482, 0.12530781396926283, 0.021758851949409297, 0.01118865514291989, 0.01630852195620791, 0.009717848689135056, 0.005687385866965822, 0.007543861114835626, 0.01032720173348356, 0.01648145006013546, 0.005373827784677321, 0.07782528772435504, 0.07330077990565553, 0.06166601747141709, 0.08055000157538439, 0.07934930144482333, 0.062084647386244196, 0.10632178558194105, 0.07001529129837158, 0.10518737552427082, 0.35100325129559173, 0.37523474412177293, 0.37672805793904796, 0.36548934896301644, 0.3784362034739134, 0.29606697411473326, 0.38347943212156954, 0.36879532140862226, 0.4156318141657802, 0.06382786247443584, 0.07027068159229422, 0.06624128936599916, 0.05655454515347169, 0.05191204552758877, 0.05506194647616636, 0.05754263107754798, 0.044978851032902445, 0.06453824649042239, 0.17430559712214022, 0.14371516083960323, 0.13051352534288252, 0.1486892368088053, 0.14242895630840868, 0.11916859449762929, 0.1571485249749106, 0.14360241955760045, 0.14497734269037155, 0.20741927828451423, 0.19714385027183123, 0.17307556592616313, 0.21943769638530608, 0.17629886886715318, 0.1763960630323771, 0.17415519128369839, 0.14610377103098293, 0.13604240414071722, 0.15761735523911335, 0.17384670425875282, 0.12931732912620886, 0.15729233192226133, 0.15298296199388528, 0.15387607639362466, 0.13204630169711196, 0.1106239180094315, 0.10232685023347143, 0.18187502715462378, 0.15122193339174406, 0.1491135876626546, 0.17632443556245903, 0.1601654742865336, 0.15051735448101877, 0.19316221984484072, 0.1751426705706377, 0.186300441145674, 0.18717676149004225, 0.17272682736838418, 0.18311183184071178, 0.17713748478276103, 0.19446933698451196, 0.2035937959690537, 0.3837886709773012, 0.23595571520615266, 0.18984625380959352, 0.1372146125602216, 0.3375473553526086, 0.3751022808034383, 0.34124726872015887, 0.20497501473067492, 0.3041174875603694, 0.12875970971203898, 0.3946650494814252, 0.1387322605317557, 0.3577015084168791, 0.3406424644901558, 0.14168063322299607, 0.3218763830477227, 0.27719104026340924, 0.13076117137094367, 0.2911946769569581, 0.25316828466381325, 0.20684626425743513, 0.17951653794217437, 0.19543027414880643, 0.1898356726589886, 0.17357588525115408, 0.19616852667788487, 0.18477683563372382, 0.1910169861343758, 0.17449898452055512, 0.16900911271993313, 0.055545142414739845, 0.05029678957846151, 0.0451835029003681, 0.07009360399369924, 0.04843630757627959, 0.05027907031284684, 0.07167402092987196, 0.057166150127647364, 0.05062401906399627]}, "mutation_prompt": null}
{"id": "65f0ac4c-d46b-4ab2-aab2-612d980a92b4", "solution": "import numpy as np\nfrom scipy.optimize import differential_evolution\n\nclass Novel_Metaheuristic_Optimizer:\n    def __init__(self, budget, dim, num_particles=30, max_iterations=1000, inertia_weight=0.7, cognitive_weight=1.4, social_weight=1.4, initial_temperature=100, cooling_rate=0.95):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = num_particles\n        self.max_iterations = max_iterations\n        self.inertia_weight = inertia_weight\n        self.cognitive_weight = cognitive_weight\n        self.social_weight = social_weight\n        self.initial_temperature = initial_temperature\n        self.cooling_rate = cooling_rate\n\n    def __call__(self, func):\n        def novel_metaheuristic_helper():\n            # PSO initialization\n            self.inertia_weight = np.random.uniform(0.4, 0.9)\n            # PSO optimization loop\n            # Simulated Annealing (SA) initialization\n            current_solution = np.random.uniform(-5.0, 5.0, size=self.dim)\n            current_cost = func(current_solution)\n            best_solution = current_solution\n            best_cost = current_cost\n            # SA optimization loop\n            for _ in range(int(self.budget*0.5)):  # Spend 50% of the budget on SA\n                proposal_solution = current_solution + np.random.normal(0, 0.1, size=self.dim)\n                proposal_solution = np.clip(proposal_solution, -5.0, 5.0)\n                proposal_cost = func(proposal_solution)\n                if proposal_cost < current_cost or np.random.rand() < np.exp((current_cost - proposal_cost) / self.initial_temperature):\n                    current_solution = proposal_solution\n                    current_cost = proposal_cost\n                    if proposal_cost < best_cost:\n                        best_solution = proposal_solution\n                        best_cost = proposal_cost\n            # Differential Evolution (DE) optimization\n            de_result = differential_evolution(func, bounds=[(-5.0, 5.0)]*self.dim, maxiter=int(self.budget*0.5))  # Spend 50% of the budget on DE\n            if de_result.fun < best_cost:\n                best_solution = de_result.x\n            return best_solution\n\n        return novel_metaheuristic_helper()\n\noptimizer = Novel_Metaheuristic_Optimizer(budget=1000, dim=10)", "name": "Novel_Metaheuristic_Optimizer", "description": "A novel optimization algorithm that combines Particle Swarm Optimization (PSO), Simulated Annealing (SA), and Differential Evolution (DE) to efficiently optimize black box functions within a given budget by dynamically adjusting key parameters and spending the budget on different optimization techniques.", "configspace": "", "generation": 82, "fitness": 0.1824295991437497, "feedback": "The algorithm Novel_Metaheuristic_Optimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.18 with standard deviation 0.12.", "error": "", "parent_id": "d82bc67e-0c68-42f5-b876-0815e26c45c3", "metadata": {"aucs": [0.48456837961059296, 0.5105538177594323, 0.47507057601350744, 0.47136751809518973, 0.467732558082386, 0.4996677345119218, 0.48477110876648977, 0.47038103501661366, 0.545176959239378, 0.18273261797874463, 0.19429757105452794, 0.21506122537384254, 0.19451909647024235, 0.12146371433165404, 0.24079244109696885, 0.27263541071937736, 0.13427678057795633, 0.1949133209598758, 0.06820291502883957, 0.06966638517654022, 0.12395381192411059, 0.08289695025266608, 0.08296512362937791, 0.0704014936562467, 0.161588407958278, 0.06945382781693155, 0.0722924237460717, 0.09232046452458575, 0.05330572800163913, 0.07118382690440017, 0.08917587563413543, 0.06361582817029554, 0.05932679852603273, 0.0635086490190615, 0.08265301708915862, 0.05814508982599631, 0.12721453922183246, 0.11706329861822595, 0.10630696636794656, 0.4174408512262425, 0.41873076422615274, 0.44278493340983627, 0.2123470216737544, 0.21871732615763728, 0.2200105933850952, 0.22008000655410365, 0.16538357465335463, 0.13387199369701974, 0.16636815324180299, 0.14716754841340518, 0.12746030363756877, 0.21459892081009235, 0.3376740437342389, 0.21157406285358538, 0.16947644277671503, 0.1644703645228397, 0.14361446048647464, 0.1907397339836573, 0.1695538471210719, 0.17152511706131024, 0.18809197229177876, 0.23240056220857597, 0.35739257552103, 0.30711331026701605, 0.35800000372394647, 0.27430480307612815, 0.10107003287669303, 0.38461052355102554, 0.40748602519626975, 0.3329604914039255, 0.3311750758329405, 0.3502616151401581, 0.3634412917063622, 0.3529535045867639, 0.34852116366747576, 0.2831203133435162, 0.24473953266556592, 0.16649180289750054, 0.29474586603190134, 0.1625909842331552, 0.31696283773167133, 0.14869086718791347, 0.07212689495988767, 0.06175901597576361, 0.020191522623446145, 0.03572120668890544, 0.038188410930948935, 0.0538084588160711, 0.06082424657643004, 0.03914242214484198, 0.07873098788749833, 0.1280191711671197, 0.11311878725947622, 0.12625932608897728, 0.08535519357748123, 0.09378270625489715, 0.12059452416952898, 0.10471029393977482, 0.12530781396926283, 0.021758851949409297, 0.01118865514291989, 0.01630852195620791, 0.009717848689135056, 0.005687385866965822, 0.007543861114835626, 0.01032720173348356, 0.01648145006013546, 0.005373827784677321, 0.07782528772435504, 0.07330077990565553, 0.06166601747141709, 0.08055000157538439, 0.07934930144482333, 0.062084647386244196, 0.10632178558194105, 0.07001529129837158, 0.10518737552427082, 0.35100325129559173, 0.37523474412177293, 0.37672805793904796, 0.36548934896301644, 0.3784362034739134, 0.29606697411473326, 0.38347943212156954, 0.36879532140862226, 0.4156318141657802, 0.06382786247443584, 0.07027068159229422, 0.06624128936599916, 0.05655454515347169, 0.05191204552758877, 0.05506194647616636, 0.05754263107754798, 0.044978851032902445, 0.06453824649042239, 0.17430559712214022, 0.14371516083960323, 0.13051352534288252, 0.1486892368088053, 0.14242895630840868, 0.11916859449762929, 0.1571485249749106, 0.14360241955760045, 0.14497734269037155, 0.20741927828451423, 0.19714385027183123, 0.17307556592616313, 0.21943769638530608, 0.17629886886715318, 0.1763960630323771, 0.17415519128369839, 0.14610377103098293, 0.13604240414071722, 0.15761735523911335, 0.17384670425875282, 0.12931732912620886, 0.15729233192226133, 0.15298296199388528, 0.15387607639362466, 0.13204630169711196, 0.1106239180094315, 0.10232685023347143, 0.18187502715462378, 0.15122193339174406, 0.1491135876626546, 0.17632443556245903, 0.1601654742865336, 0.15051735448101877, 0.19316221984484072, 0.1751426705706377, 0.186300441145674, 0.18717676149004225, 0.17272682736838418, 0.18311183184071178, 0.17713748478276103, 0.19446933698451196, 0.2035937959690537, 0.3837886709773012, 0.23595571520615266, 0.18984625380959352, 0.1372146125602216, 0.3375473553526086, 0.3751022808034383, 0.34124726872015887, 0.20497501473067492, 0.3041174875603694, 0.12875970971203898, 0.3946650494814252, 0.1387322605317557, 0.3577015084168791, 0.3406424644901558, 0.14168063322299607, 0.3218763830477227, 0.27719104026340924, 0.13076117137094367, 0.2911946769569581, 0.25316828466381325, 0.20684626425743513, 0.17951653794217437, 0.19543027414880643, 0.1898356726589886, 0.17357588525115408, 0.19616852667788487, 0.18477683563372382, 0.1910169861343758, 0.17449898452055512, 0.16900911271993313, 0.055545142414739845, 0.05029678957846151, 0.0451835029003681, 0.07009360399369924, 0.04843630757627959, 0.05027907031284684, 0.07167402092987196, 0.057166150127647364, 0.05062401906399627]}, "mutation_prompt": null}
{"id": "86f411a2-8ebf-4fc3-9a9d-25571d3f6099", "solution": "import numpy as np\nfrom scipy.optimize import differential_evolution\n\nclass Enhanced_Hybrid_PSODE_SA_DE_Optimizer:\n    def __init__(self, budget, dim, num_particles=30, max_iterations=1000, cognitive_weight=1.4, social_weight=1.4, initial_temperature=100, cooling_rate=0.95):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = num_particles\n        self.max_iterations = max_iterations\n        self.cognitive_weight = cognitive_weight\n        self.social_weight = social_weight\n        self.initial_temperature = initial_temperature\n        self.cooling_rate = cooling_rate\n\n    def __call__(self, func):\n        def enhanced_hybrid_pso_sa_de_helper():\n            # Enhanced PSO with dynamic inertia weight\n            inertia_weight = np.random.uniform(0.4, 0.9)\n            # PSO optimization loop\n            for _ in range(int(self.budget*0.25)):  # Spend 25% of the budget on PSO\n                # Update inertia weight dynamically based on progress\n                if np.random.rand() < 0.15:  # Adjust inertia weight with 15% probability\n                    inertia_weight = np.clip(inertia_weight + np.random.uniform(-0.1, 0.1), 0.4, 0.9)\n                # PSO algorithm implementation here\n\n            # Simulated Annealing (SA) and Differential Evolution (DE) optimization similar to the previous implementation\n\n            return best_solution\n\n        return enhanced_hybrid_pso_sa_de_helper()\n\noptimizer = Enhanced_Hybrid_PSODE_SA_DE_Optimizer(budget=1000, dim=10)", "name": "Enhanced_Hybrid_PSODE_SA_DE_Optimizer", "description": "Enhancing the Hybrid_PSODE_SA_DE_Optimizer algorithm by dynamically adjusting the inertia weight in PSO for improved convergence.", "configspace": "", "generation": 83, "fitness": -Infinity, "feedback": "An exception occurred: NameError(\"name 'best_solution' is not defined\").", "error": "NameError(\"name 'best_solution' is not defined\")", "parent_id": "d82bc67e-0c68-42f5-b876-0815e26c45c3", "metadata": {}, "mutation_prompt": null}
{"id": "aab2686a-8875-46f3-a6d7-77b43e99c104", "solution": "import numpy as np\nfrom scipy.optimize import differential_evolution\n\nclass Hybrid_PSODE_SA_DE_Optimizer:\n    def __init__(self, budget, dim, num_particles=30, max_iterations=1000, inertia_weight=0.7, cognitive_weight=1.4, social_weight=1.4, initial_temperature=100, cooling_rate=0.95):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = num_particles\n        self.max_iterations = max_iterations\n        self.inertia_weight = inertia_weight\n        self.cognitive_weight = cognitive_weight\n        self.social_weight = social_weight\n        self.initial_temperature = initial_temperature\n        self.cooling_rate = cooling_rate\n\n    def __call__(self, func):\n        def hybrid_pso_sa_de_helper():\n            # PSO initialization\n            self.inertia_weight = np.random.uniform(0.4, 0.9)\n            # PSO optimization loop\n            # Simulated Annealing (SA) initialization\n            current_solution = np.random.uniform(-5.0, 5.0, size=self.dim)\n            current_cost = func(current_solution)\n            best_solution = current_solution\n            best_cost = current_cost\n            # SA optimization loop\n            for _ in range(int(self.budget*0.5)):  # Spend 50% of the budget on SA\n                proposal_solution = current_solution + np.random.normal(0, 0.1, size=self.dim)\n                proposal_solution = np.clip(proposal_solution, -5.0, 5.0)\n                proposal_cost = func(proposal_solution)\n                if proposal_cost < current_cost or np.random.rand() < np.exp((current_cost - proposal_cost) / self.initial_temperature):\n                    current_solution = proposal_solution\n                    current_cost = proposal_cost\n                    if proposal_cost < best_cost:\n                        best_solution = proposal_solution\n                        best_cost = proposal_cost\n            # Differential Evolution (DE) optimization\n            de_result = differential_evolution(func, bounds=[(-5.0, 5.0)]*self.dim, maxiter=int(self.budget*0.5))  # Spend 50% of the budget on DE\n            if de_result.fun < best_cost:\n                best_solution = de_result.x\n            return best_solution\n\n        return hybrid_pso_sa_de_helper()\n\noptimizer = Hybrid_PSODE_SA_DE_Optimizer(budget=1000, dim=10)", "name": "Hybrid_PSODE_SA_DE_Optimizer", "description": "Combining Particle Swarm Optimization (PSO), Simulated Annealing (SA), and Differential Evolution (DE) for a hybrid metaheuristic approach to optimize black box functions efficiently within a given budget.", "configspace": "", "generation": 81, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "d82bc67e-0c68-42f5-b876-0815e26c45c3", "metadata": {"aucs": [0.48456837961059296, 0.5105538177594323, 0.47507057601350744, 0.47136751809518973, 0.467732558082386, 0.4996677345119218, 0.48477110876648977, 0.47038103501661366, 0.545176959239378, 0.18273261797874463, 0.19429757105452794, 0.21506122537384254, 0.19451909647024235, 0.12146371433165404, 0.24079244109696885, 0.27263541071937736, 0.13427678057795633, 0.1949133209598758, 0.06820291502883957, 0.06966638517654022, 0.12395381192411059, 0.08289695025266608, 0.08296512362937791, 0.0704014936562467, 0.161588407958278, 0.06945382781693155, 0.0722924237460717, 0.09232046452458575, 0.05330572800163913, 0.07118382690440017, 0.08917587563413543, 0.06361582817029554, 0.05932679852603273, 0.0635086490190615, 0.08265301708915862, 0.05814508982599631, 0.12721453922183246, 0.11706329861822595, 0.10630696636794656, 0.4174408512262425, 0.41873076422615274, 0.44278493340983627, 0.2123470216737544, 0.21871732615763728, 0.2200105933850952, 0.22008000655410365, 0.16538357465335463, 0.13387199369701974, 0.16636815324180299, 0.14716754841340518, 0.12746030363756877, 0.21459892081009235, 0.3376740437342389, 0.21157406285358538, 0.16947644277671503, 0.1644703645228397, 0.14361446048647464, 0.1907397339836573, 0.1695538471210719, 0.17152511706131024, 0.18809197229177876, 0.23240056220857597, 0.35739257552103, 0.30711331026701605, 0.35800000372394647, 0.27430480307612815, 0.10107003287669303, 0.38461052355102554, 0.40748602519626975, 0.3329604914039255, 0.3311750758329405, 0.3502616151401581, 0.3634412917063622, 0.3529535045867639, 0.34852116366747576, 0.2831203133435162, 0.24473953266556592, 0.16649180289750054, 0.29474586603190134, 0.1625909842331552, 0.31696283773167133, 0.14869086718791347, 0.07212689495988767, 0.06175901597576361, 0.020191522623446145, 0.03572120668890544, 0.038188410930948935, 0.0538084588160711, 0.06082424657643004, 0.03914242214484198, 0.07873098788749833, 0.1280191711671197, 0.11311878725947622, 0.12625932608897728, 0.08535519357748123, 0.09378270625489715, 0.12059452416952898, 0.10471029393977482, 0.12530781396926283, 0.021758851949409297, 0.01118865514291989, 0.01630852195620791, 0.009717848689135056, 0.005687385866965822, 0.007543861114835626, 0.01032720173348356, 0.01648145006013546, 0.005373827784677321, 0.07782528772435504, 0.07330077990565553, 0.06166601747141709, 0.08055000157538439, 0.07934930144482333, 0.062084647386244196, 0.10632178558194105, 0.07001529129837158, 0.10518737552427082, 0.35100325129559173, 0.37523474412177293, 0.37672805793904796, 0.36548934896301644, 0.3784362034739134, 0.29606697411473326, 0.38347943212156954, 0.36879532140862226, 0.4156318141657802, 0.06382786247443584, 0.07027068159229422, 0.06624128936599916, 0.05655454515347169, 0.05191204552758877, 0.05506194647616636, 0.05754263107754798, 0.044978851032902445, 0.06453824649042239, 0.17430559712214022, 0.14371516083960323, 0.13051352534288252, 0.1486892368088053, 0.14242895630840868, 0.11916859449762929, 0.1571485249749106, 0.14360241955760045, 0.14497734269037155, 0.20741927828451423, 0.19714385027183123, 0.17307556592616313, 0.21943769638530608, 0.17629886886715318, 0.1763960630323771, 0.17415519128369839, 0.14610377103098293, 0.13604240414071722, 0.15761735523911335, 0.17384670425875282, 0.12931732912620886, 0.15729233192226133, 0.15298296199388528, 0.15387607639362466, 0.13204630169711196, 0.1106239180094315, 0.10232685023347143, 0.18187502715462378, 0.15122193339174406, 0.1491135876626546, 0.17632443556245903, 0.1601654742865336, 0.15051735448101877, 0.19316221984484072, 0.1751426705706377, 0.186300441145674, 0.18717676149004225, 0.17272682736838418, 0.18311183184071178, 0.17713748478276103, 0.19446933698451196, 0.2035937959690537, 0.3837886709773012, 0.23595571520615266, 0.18984625380959352, 0.1372146125602216, 0.3375473553526086, 0.3751022808034383, 0.34124726872015887, 0.20497501473067492, 0.3041174875603694, 0.12875970971203898, 0.3946650494814252, 0.1387322605317557, 0.3577015084168791, 0.3406424644901558, 0.14168063322299607, 0.3218763830477227, 0.27719104026340924, 0.13076117137094367, 0.2911946769569581, 0.25316828466381325, 0.20684626425743513, 0.17951653794217437, 0.19543027414880643, 0.1898356726589886, 0.17357588525115408, 0.19616852667788487, 0.18477683563372382, 0.1910169861343758, 0.17449898452055512, 0.16900911271993313, 0.055545142414739845, 0.05029678957846151, 0.0451835029003681, 0.07009360399369924, 0.04843630757627959, 0.05027907031284684, 0.07167402092987196, 0.057166150127647364, 0.05062401906399627]}, "mutation_prompt": null}
{"id": "d9867c32-1b46-412f-976e-35bacc866740", "solution": "import numpy as np\nfrom scipy.optimize import differential_evolution\n\nclass Hybrid_PSODE_SA_DE_Optimizer:\n    def __init__(self, budget, dim, num_particles=30, max_iterations=1000, inertia_weight=0.7, cognitive_weight=1.4, social_weight=1.4, initial_temperature=100, cooling_rate=0.95):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = num_particles\n        self.max_iterations = max_iterations\n        self.inertia_weight = inertia_weight\n        self.cognitive_weight = cognitive_weight\n        self.social_weight = social_weight\n        self.initial_temperature = initial_temperature\n        self.cooling_rate = cooling_rate\n\n    def __call__(self, func):\n        def hybrid_pso_sa_de_helper():\n            # PSO initialization\n            self.inertia_weight = np.random.uniform(0.4, 0.9)\n            # PSO optimization loop\n            # Simulated Annealing (SA) initialization\n            current_solution = np.random.uniform(-5.0, 5.0, size=self.dim)\n            current_cost = func(current_solution)\n            best_solution = current_solution\n            best_cost = current_cost\n            # SA optimization loop\n            for _ in range(int(self.budget*0.5)):  # Spend 50% of the budget on SA\n                proposal_solution = current_solution + np.random.normal(0, 0.1, size=self.dim)\n                proposal_solution = np.clip(proposal_solution, -5.0, 5.0)\n                proposal_cost = func(proposal_solution)\n                if proposal_cost < current_cost or np.random.rand() < np.exp((current_cost - proposal_cost) / self.initial_temperature):\n                    current_solution = proposal_solution\n                    current_cost = proposal_cost\n                    if proposal_cost < best_cost:\n                        best_solution = proposal_solution\n                        best_cost = proposal_cost\n            # Differential Evolution (DE) optimization\n            de_result = differential_evolution(func, bounds=[(-5.0, 5.0)]*self.dim, maxiter=int(self.budget*0.5))  # Spend 50% of the budget on DE\n            if de_result.fun < best_cost:\n                best_solution = de_result.x\n            return best_solution\n\n        return hybrid_pso_sa_de_helper()\n\noptimizer = Hybrid_PSODE_SA_DE_Optimizer(budget=1000, dim=10)", "name": "Hybrid_PSODE_SA_DE_Optimizer", "description": "Combining Particle Swarm Optimization (PSO), Simulated Annealing (SA), and Differential Evolution (DE) for a hybrid metaheuristic approach to optimize black box functions efficiently within a given budget.", "configspace": "", "generation": 81, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "d82bc67e-0c68-42f5-b876-0815e26c45c3", "metadata": {"aucs": [0.48456837961059296, 0.5105538177594323, 0.47507057601350744, 0.47136751809518973, 0.467732558082386, 0.4996677345119218, 0.48477110876648977, 0.47038103501661366, 0.545176959239378, 0.18273261797874463, 0.19429757105452794, 0.21506122537384254, 0.19451909647024235, 0.12146371433165404, 0.24079244109696885, 0.27263541071937736, 0.13427678057795633, 0.1949133209598758, 0.06820291502883957, 0.06966638517654022, 0.12395381192411059, 0.08289695025266608, 0.08296512362937791, 0.0704014936562467, 0.161588407958278, 0.06945382781693155, 0.0722924237460717, 0.09232046452458575, 0.05330572800163913, 0.07118382690440017, 0.08917587563413543, 0.06361582817029554, 0.05932679852603273, 0.0635086490190615, 0.08265301708915862, 0.05814508982599631, 0.12721453922183246, 0.11706329861822595, 0.10630696636794656, 0.4174408512262425, 0.41873076422615274, 0.44278493340983627, 0.2123470216737544, 0.21871732615763728, 0.2200105933850952, 0.22008000655410365, 0.16538357465335463, 0.13387199369701974, 0.16636815324180299, 0.14716754841340518, 0.12746030363756877, 0.21459892081009235, 0.3376740437342389, 0.21157406285358538, 0.16947644277671503, 0.1644703645228397, 0.14361446048647464, 0.1907397339836573, 0.1695538471210719, 0.17152511706131024, 0.18809197229177876, 0.23240056220857597, 0.35739257552103, 0.30711331026701605, 0.35800000372394647, 0.27430480307612815, 0.10107003287669303, 0.38461052355102554, 0.40748602519626975, 0.3329604914039255, 0.3311750758329405, 0.3502616151401581, 0.3634412917063622, 0.3529535045867639, 0.34852116366747576, 0.2831203133435162, 0.24473953266556592, 0.16649180289750054, 0.29474586603190134, 0.1625909842331552, 0.31696283773167133, 0.14869086718791347, 0.07212689495988767, 0.06175901597576361, 0.020191522623446145, 0.03572120668890544, 0.038188410930948935, 0.0538084588160711, 0.06082424657643004, 0.03914242214484198, 0.07873098788749833, 0.1280191711671197, 0.11311878725947622, 0.12625932608897728, 0.08535519357748123, 0.09378270625489715, 0.12059452416952898, 0.10471029393977482, 0.12530781396926283, 0.021758851949409297, 0.01118865514291989, 0.01630852195620791, 0.009717848689135056, 0.005687385866965822, 0.007543861114835626, 0.01032720173348356, 0.01648145006013546, 0.005373827784677321, 0.07782528772435504, 0.07330077990565553, 0.06166601747141709, 0.08055000157538439, 0.07934930144482333, 0.062084647386244196, 0.10632178558194105, 0.07001529129837158, 0.10518737552427082, 0.35100325129559173, 0.37523474412177293, 0.37672805793904796, 0.36548934896301644, 0.3784362034739134, 0.29606697411473326, 0.38347943212156954, 0.36879532140862226, 0.4156318141657802, 0.06382786247443584, 0.07027068159229422, 0.06624128936599916, 0.05655454515347169, 0.05191204552758877, 0.05506194647616636, 0.05754263107754798, 0.044978851032902445, 0.06453824649042239, 0.17430559712214022, 0.14371516083960323, 0.13051352534288252, 0.1486892368088053, 0.14242895630840868, 0.11916859449762929, 0.1571485249749106, 0.14360241955760045, 0.14497734269037155, 0.20741927828451423, 0.19714385027183123, 0.17307556592616313, 0.21943769638530608, 0.17629886886715318, 0.1763960630323771, 0.17415519128369839, 0.14610377103098293, 0.13604240414071722, 0.15761735523911335, 0.17384670425875282, 0.12931732912620886, 0.15729233192226133, 0.15298296199388528, 0.15387607639362466, 0.13204630169711196, 0.1106239180094315, 0.10232685023347143, 0.18187502715462378, 0.15122193339174406, 0.1491135876626546, 0.17632443556245903, 0.1601654742865336, 0.15051735448101877, 0.19316221984484072, 0.1751426705706377, 0.186300441145674, 0.18717676149004225, 0.17272682736838418, 0.18311183184071178, 0.17713748478276103, 0.19446933698451196, 0.2035937959690537, 0.3837886709773012, 0.23595571520615266, 0.18984625380959352, 0.1372146125602216, 0.3375473553526086, 0.3751022808034383, 0.34124726872015887, 0.20497501473067492, 0.3041174875603694, 0.12875970971203898, 0.3946650494814252, 0.1387322605317557, 0.3577015084168791, 0.3406424644901558, 0.14168063322299607, 0.3218763830477227, 0.27719104026340924, 0.13076117137094367, 0.2911946769569581, 0.25316828466381325, 0.20684626425743513, 0.17951653794217437, 0.19543027414880643, 0.1898356726589886, 0.17357588525115408, 0.19616852667788487, 0.18477683563372382, 0.1910169861343758, 0.17449898452055512, 0.16900911271993313, 0.055545142414739845, 0.05029678957846151, 0.0451835029003681, 0.07009360399369924, 0.04843630757627959, 0.05027907031284684, 0.07167402092987196, 0.057166150127647364, 0.05062401906399627]}, "mutation_prompt": null}
{"id": "299fe6e7-66b6-4e82-a97e-27019fbcf6a2", "solution": "import numpy as np\nfrom scipy.optimize import differential_evolution\n\nclass Hybrid_PSODE_SA_DE_Optimizer:\n    def __init__(self, budget, dim, num_particles=30, max_iterations=1000, inertia_weight=0.7, cognitive_weight=1.4, social_weight=1.4, initial_temperature=100, cooling_rate=0.95):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = num_particles\n        self.max_iterations = max_iterations\n        self.inertia_weight = inertia_weight\n        self.cognitive_weight = cognitive_weight\n        self.social_weight = social_weight\n        self.initial_temperature = initial_temperature\n        self.cooling_rate = cooling_rate\n\n    def __call__(self, func):\n        def hybrid_pso_sa_de_helper():\n            # PSO initialization\n            self.inertia_weight = np.random.uniform(0.4, 0.9)\n            # PSO optimization loop\n            # Simulated Annealing (SA) initialization\n            current_solution = np.random.uniform(-5.0, 5.0, size=self.dim)\n            current_cost = func(current_solution)\n            best_solution = current_solution\n            best_cost = current_cost\n            # SA optimization loop\n            for _ in range(int(self.budget*0.5)):  # Spend 50% of the budget on SA\n                proposal_solution = current_solution + np.random.normal(0, 0.1, size=self.dim)\n                proposal_solution = np.clip(proposal_solution, -5.0, 5.0)\n                proposal_cost = func(proposal_solution)\n                if proposal_cost < current_cost or np.random.rand() < np.exp((current_cost - proposal_cost) / self.initial_temperature):\n                    current_solution = proposal_solution\n                    current_cost = proposal_cost\n                    if proposal_cost < best_cost:\n                        best_solution = proposal_solution\n                        best_cost = proposal_cost\n            # Differential Evolution (DE) optimization\n            de_result = differential_evolution(func, bounds=[(-5.0, 5.0)]*self.dim, maxiter=int(self.budget*0.5))  # Spend 50% of the budget on DE\n            if de_result.fun < best_cost:\n                best_solution = de_result.x\n            return best_solution\n\n        return hybrid_pso_sa_de_helper()\n\noptimizer = Hybrid_PSODE_SA_DE_Optimizer(budget=1000, dim=10)", "name": "Hybrid_PSODE_SA_DE_Optimizer", "description": "Combining Particle Swarm Optimization (PSO), Simulated Annealing (SA), and Differential Evolution (DE) for a hybrid metaheuristic approach to optimize black box functions efficiently within a given budget.", "configspace": "", "generation": 81, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "d82bc67e-0c68-42f5-b876-0815e26c45c3", "metadata": {"aucs": [0.48456837961059296, 0.5105538177594323, 0.47507057601350744, 0.47136751809518973, 0.467732558082386, 0.4996677345119218, 0.48477110876648977, 0.47038103501661366, 0.545176959239378, 0.18273261797874463, 0.19429757105452794, 0.21506122537384254, 0.19451909647024235, 0.12146371433165404, 0.24079244109696885, 0.27263541071937736, 0.13427678057795633, 0.1949133209598758, 0.06820291502883957, 0.06966638517654022, 0.12395381192411059, 0.08289695025266608, 0.08296512362937791, 0.0704014936562467, 0.161588407958278, 0.06945382781693155, 0.0722924237460717, 0.09232046452458575, 0.05330572800163913, 0.07118382690440017, 0.08917587563413543, 0.06361582817029554, 0.05932679852603273, 0.0635086490190615, 0.08265301708915862, 0.05814508982599631, 0.12721453922183246, 0.11706329861822595, 0.10630696636794656, 0.4174408512262425, 0.41873076422615274, 0.44278493340983627, 0.2123470216737544, 0.21871732615763728, 0.2200105933850952, 0.22008000655410365, 0.16538357465335463, 0.13387199369701974, 0.16636815324180299, 0.14716754841340518, 0.12746030363756877, 0.21459892081009235, 0.3376740437342389, 0.21157406285358538, 0.16947644277671503, 0.1644703645228397, 0.14361446048647464, 0.1907397339836573, 0.1695538471210719, 0.17152511706131024, 0.18809197229177876, 0.23240056220857597, 0.35739257552103, 0.30711331026701605, 0.35800000372394647, 0.27430480307612815, 0.10107003287669303, 0.38461052355102554, 0.40748602519626975, 0.3329604914039255, 0.3311750758329405, 0.3502616151401581, 0.3634412917063622, 0.3529535045867639, 0.34852116366747576, 0.2831203133435162, 0.24473953266556592, 0.16649180289750054, 0.29474586603190134, 0.1625909842331552, 0.31696283773167133, 0.14869086718791347, 0.07212689495988767, 0.06175901597576361, 0.020191522623446145, 0.03572120668890544, 0.038188410930948935, 0.0538084588160711, 0.06082424657643004, 0.03914242214484198, 0.07873098788749833, 0.1280191711671197, 0.11311878725947622, 0.12625932608897728, 0.08535519357748123, 0.09378270625489715, 0.12059452416952898, 0.10471029393977482, 0.12530781396926283, 0.021758851949409297, 0.01118865514291989, 0.01630852195620791, 0.009717848689135056, 0.005687385866965822, 0.007543861114835626, 0.01032720173348356, 0.01648145006013546, 0.005373827784677321, 0.07782528772435504, 0.07330077990565553, 0.06166601747141709, 0.08055000157538439, 0.07934930144482333, 0.062084647386244196, 0.10632178558194105, 0.07001529129837158, 0.10518737552427082, 0.35100325129559173, 0.37523474412177293, 0.37672805793904796, 0.36548934896301644, 0.3784362034739134, 0.29606697411473326, 0.38347943212156954, 0.36879532140862226, 0.4156318141657802, 0.06382786247443584, 0.07027068159229422, 0.06624128936599916, 0.05655454515347169, 0.05191204552758877, 0.05506194647616636, 0.05754263107754798, 0.044978851032902445, 0.06453824649042239, 0.17430559712214022, 0.14371516083960323, 0.13051352534288252, 0.1486892368088053, 0.14242895630840868, 0.11916859449762929, 0.1571485249749106, 0.14360241955760045, 0.14497734269037155, 0.20741927828451423, 0.19714385027183123, 0.17307556592616313, 0.21943769638530608, 0.17629886886715318, 0.1763960630323771, 0.17415519128369839, 0.14610377103098293, 0.13604240414071722, 0.15761735523911335, 0.17384670425875282, 0.12931732912620886, 0.15729233192226133, 0.15298296199388528, 0.15387607639362466, 0.13204630169711196, 0.1106239180094315, 0.10232685023347143, 0.18187502715462378, 0.15122193339174406, 0.1491135876626546, 0.17632443556245903, 0.1601654742865336, 0.15051735448101877, 0.19316221984484072, 0.1751426705706377, 0.186300441145674, 0.18717676149004225, 0.17272682736838418, 0.18311183184071178, 0.17713748478276103, 0.19446933698451196, 0.2035937959690537, 0.3837886709773012, 0.23595571520615266, 0.18984625380959352, 0.1372146125602216, 0.3375473553526086, 0.3751022808034383, 0.34124726872015887, 0.20497501473067492, 0.3041174875603694, 0.12875970971203898, 0.3946650494814252, 0.1387322605317557, 0.3577015084168791, 0.3406424644901558, 0.14168063322299607, 0.3218763830477227, 0.27719104026340924, 0.13076117137094367, 0.2911946769569581, 0.25316828466381325, 0.20684626425743513, 0.17951653794217437, 0.19543027414880643, 0.1898356726589886, 0.17357588525115408, 0.19616852667788487, 0.18477683563372382, 0.1910169861343758, 0.17449898452055512, 0.16900911271993313, 0.055545142414739845, 0.05029678957846151, 0.0451835029003681, 0.07009360399369924, 0.04843630757627959, 0.05027907031284684, 0.07167402092987196, 0.057166150127647364, 0.05062401906399627]}, "mutation_prompt": null}
{"id": "ee54357c-75b7-4b1d-9f6c-e2eed46148b2", "solution": "import numpy as np\nfrom scipy.optimize import differential_evolution\n\nclass Refined_Hybrid_PSODE_SA_DE_Optimizer:\n    def __init__(self, budget, dim, num_particles=30, max_iterations=1000, inertia_weight=0.7, cognitive_weight=1.4, social_weight=1.4, initial_temperature=100, cooling_rate=0.95):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = num_particles\n        self.max_iterations = max_iterations\n        self.inertia_weight = inertia_weight\n        self.cognitive_weight = cognitive_weight\n        self.social_weight = social_weight\n        self.initial_temperature = initial_temperature\n        self.cooling_rate = cooling_rate\n\n    def __call__(self, func):\n        def refined_hybrid_pso_sa_de_helper():\n            # Combination of PSO, SA, and DE with refined strategies\n            # PSO initialization\n            self.inertia_weight = np.random.uniform(0.4, 0.9)\n            # PSO optimization loop\n            # Simulated Annealing (SA) initialization\n            current_solution = np.random.uniform(-5.0, 5.0, size=self.dim)\n            current_cost = func(current_solution)\n            best_solution = current_solution\n            best_cost = current_cost\n            # Refined SA optimization loop\n            for _ in range(int(self.budget*0.5)):  # Spend 50% of the budget on SA\n                proposal_solution = current_solution + np.random.normal(0, 0.1, size=self.dim)\n                proposal_solution = np.clip(proposal_solution, -5.0, 5.0)\n                proposal_cost = func(proposal_solution)\n                if proposal_cost < current_cost or np.random.rand() < np.exp((current_cost - proposal_cost) / self.initial_temperature):\n                    current_solution = proposal_solution\n                    current_cost = proposal_cost\n                    if proposal_cost < best_cost:\n                        best_solution = proposal_solution\n                        best_cost = proposal_cost\n            # Differential Evolution (DE) optimization\n            de_result = differential_evolution(func, bounds=[(-5.0, 5.0)]*self.dim, maxiter=int(self.budget*0.5))  # Spend 50% of the budget on DE\n            if de_result.fun < best_cost:\n                best_solution = de_result.x\n            return best_solution\n\n        return refined_hybrid_pso_sa_de_helper()\n\noptimizer = Refined_Hybrid_PSODE_SA_DE_Optimizer(budget=1000, dim=10)", "name": "Refined_Hybrid_PSODE_SA_DE_Optimizer", "description": "Combining Particle Swarm Optimization (PSO), Simulated Annealing (SA), and Differential Evolution (DE) for a hybrid metaheuristic approach to optimize black box functions efficiently within a given budget, with refined strategies.", "configspace": "", "generation": 87, "fitness": 0.1824295991437497, "feedback": "The algorithm Refined_Hybrid_PSODE_SA_DE_Optimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.18 with standard deviation 0.12.", "error": "", "parent_id": "d82bc67e-0c68-42f5-b876-0815e26c45c3", "metadata": {"aucs": [0.48456837961059296, 0.5105538177594323, 0.47507057601350744, 0.47136751809518973, 0.467732558082386, 0.4996677345119218, 0.48477110876648977, 0.47038103501661366, 0.545176959239378, 0.18273261797874463, 0.19429757105452794, 0.21506122537384254, 0.19451909647024235, 0.12146371433165404, 0.24079244109696885, 0.27263541071937736, 0.13427678057795633, 0.1949133209598758, 0.06820291502883957, 0.06966638517654022, 0.12395381192411059, 0.08289695025266608, 0.08296512362937791, 0.0704014936562467, 0.161588407958278, 0.06945382781693155, 0.0722924237460717, 0.09232046452458575, 0.05330572800163913, 0.07118382690440017, 0.08917587563413543, 0.06361582817029554, 0.05932679852603273, 0.0635086490190615, 0.08265301708915862, 0.05814508982599631, 0.12721453922183246, 0.11706329861822595, 0.10630696636794656, 0.4174408512262425, 0.41873076422615274, 0.44278493340983627, 0.2123470216737544, 0.21871732615763728, 0.2200105933850952, 0.22008000655410365, 0.16538357465335463, 0.13387199369701974, 0.16636815324180299, 0.14716754841340518, 0.12746030363756877, 0.21459892081009235, 0.3376740437342389, 0.21157406285358538, 0.16947644277671503, 0.1644703645228397, 0.14361446048647464, 0.1907397339836573, 0.1695538471210719, 0.17152511706131024, 0.18809197229177876, 0.23240056220857597, 0.35739257552103, 0.30711331026701605, 0.35800000372394647, 0.27430480307612815, 0.10107003287669303, 0.38461052355102554, 0.40748602519626975, 0.3329604914039255, 0.3311750758329405, 0.3502616151401581, 0.3634412917063622, 0.3529535045867639, 0.34852116366747576, 0.2831203133435162, 0.24473953266556592, 0.16649180289750054, 0.29474586603190134, 0.1625909842331552, 0.31696283773167133, 0.14869086718791347, 0.07212689495988767, 0.06175901597576361, 0.020191522623446145, 0.03572120668890544, 0.038188410930948935, 0.0538084588160711, 0.06082424657643004, 0.03914242214484198, 0.07873098788749833, 0.1280191711671197, 0.11311878725947622, 0.12625932608897728, 0.08535519357748123, 0.09378270625489715, 0.12059452416952898, 0.10471029393977482, 0.12530781396926283, 0.021758851949409297, 0.01118865514291989, 0.01630852195620791, 0.009717848689135056, 0.005687385866965822, 0.007543861114835626, 0.01032720173348356, 0.01648145006013546, 0.005373827784677321, 0.07782528772435504, 0.07330077990565553, 0.06166601747141709, 0.08055000157538439, 0.07934930144482333, 0.062084647386244196, 0.10632178558194105, 0.07001529129837158, 0.10518737552427082, 0.35100325129559173, 0.37523474412177293, 0.37672805793904796, 0.36548934896301644, 0.3784362034739134, 0.29606697411473326, 0.38347943212156954, 0.36879532140862226, 0.4156318141657802, 0.06382786247443584, 0.07027068159229422, 0.06624128936599916, 0.05655454515347169, 0.05191204552758877, 0.05506194647616636, 0.05754263107754798, 0.044978851032902445, 0.06453824649042239, 0.17430559712214022, 0.14371516083960323, 0.13051352534288252, 0.1486892368088053, 0.14242895630840868, 0.11916859449762929, 0.1571485249749106, 0.14360241955760045, 0.14497734269037155, 0.20741927828451423, 0.19714385027183123, 0.17307556592616313, 0.21943769638530608, 0.17629886886715318, 0.1763960630323771, 0.17415519128369839, 0.14610377103098293, 0.13604240414071722, 0.15761735523911335, 0.17384670425875282, 0.12931732912620886, 0.15729233192226133, 0.15298296199388528, 0.15387607639362466, 0.13204630169711196, 0.1106239180094315, 0.10232685023347143, 0.18187502715462378, 0.15122193339174406, 0.1491135876626546, 0.17632443556245903, 0.1601654742865336, 0.15051735448101877, 0.19316221984484072, 0.1751426705706377, 0.186300441145674, 0.18717676149004225, 0.17272682736838418, 0.18311183184071178, 0.17713748478276103, 0.19446933698451196, 0.2035937959690537, 0.3837886709773012, 0.23595571520615266, 0.18984625380959352, 0.1372146125602216, 0.3375473553526086, 0.3751022808034383, 0.34124726872015887, 0.20497501473067492, 0.3041174875603694, 0.12875970971203898, 0.3946650494814252, 0.1387322605317557, 0.3577015084168791, 0.3406424644901558, 0.14168063322299607, 0.3218763830477227, 0.27719104026340924, 0.13076117137094367, 0.2911946769569581, 0.25316828466381325, 0.20684626425743513, 0.17951653794217437, 0.19543027414880643, 0.1898356726589886, 0.17357588525115408, 0.19616852667788487, 0.18477683563372382, 0.1910169861343758, 0.17449898452055512, 0.16900911271993313, 0.055545142414739845, 0.05029678957846151, 0.0451835029003681, 0.07009360399369924, 0.04843630757627959, 0.05027907031284684, 0.07167402092987196, 0.057166150127647364, 0.05062401906399627]}, "mutation_prompt": null}
{"id": "5ed84889-27f6-4381-83c8-7c8a0495921d", "solution": "import numpy as np\nfrom scipy.optimize import differential_evolution\n\nclass Hybrid_PSODE_SA_DE_Optimizer:\n    def __init__(self, budget, dim, num_particles=30, max_iterations=1000, inertia_weight=0.7, cognitive_weight=1.4, social_weight=1.4, initial_temperature=100, cooling_rate=0.95):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = num_particles\n        self.max_iterations = max_iterations\n        self.inertia_weight = inertia_weight\n        self.cognitive_weight = cognitive_weight\n        self.social_weight = social_weight\n        self.initial_temperature = initial_temperature\n        self.cooling_rate = cooling_rate\n\n    def __call__(self, func):\n        def hybrid_pso_sa_de_helper():\n            # PSO initialization\n            self.inertia_weight = np.random.uniform(0.4, 0.9)\n            # PSO optimization loop\n            # Simulated Annealing (SA) initialization\n            current_solution = np.random.uniform(-5.0, 5.0, size=self.dim)\n            current_cost = func(current_solution)\n            best_solution = current_solution\n            best_cost = current_cost\n            # SA optimization loop\n            for _ in range(int(self.budget*0.5)):  # Spend 50% of the budget on SA\n                proposal_solution = current_solution + np.random.normal(0, 0.1, size=self.dim)\n                proposal_solution = np.clip(proposal_solution, -5.0, 5.0)\n                proposal_cost = func(proposal_solution)\n                if proposal_cost < current_cost or np.random.rand() < np.exp((current_cost - proposal_cost) / self.initial_temperature):\n                    current_solution = proposal_solution\n                    current_cost = proposal_cost\n                    if proposal_cost < best_cost:\n                        best_solution = proposal_solution\n                        best_cost = proposal_cost\n            # Differential Evolution (DE) optimization\n            de_result = differential_evolution(func, bounds=[(-5.0, 5.0)]*self.dim, maxiter=int(self.budget*0.5))  # Spend 50% of the budget on DE\n            if de_result.fun < best_cost:\n                best_solution = de_result.x\n            return best_solution\n\n        return hybrid_pso_sa_de_helper()\n\noptimizer = Hybrid_PSODE_SA_DE_Optimizer(budget=1000, dim=10)", "name": "Hybrid_PSODE_SA_DE_Optimizer", "description": "Combining Particle Swarm Optimization (PSO), Simulated Annealing (SA), and Differential Evolution (DE) for a hybrid metaheuristic approach to optimize black box functions efficiently within a given budget.", "configspace": "", "generation": 81, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "d82bc67e-0c68-42f5-b876-0815e26c45c3", "metadata": {"aucs": [0.48456837961059296, 0.5105538177594323, 0.47507057601350744, 0.47136751809518973, 0.467732558082386, 0.4996677345119218, 0.48477110876648977, 0.47038103501661366, 0.545176959239378, 0.18273261797874463, 0.19429757105452794, 0.21506122537384254, 0.19451909647024235, 0.12146371433165404, 0.24079244109696885, 0.27263541071937736, 0.13427678057795633, 0.1949133209598758, 0.06820291502883957, 0.06966638517654022, 0.12395381192411059, 0.08289695025266608, 0.08296512362937791, 0.0704014936562467, 0.161588407958278, 0.06945382781693155, 0.0722924237460717, 0.09232046452458575, 0.05330572800163913, 0.07118382690440017, 0.08917587563413543, 0.06361582817029554, 0.05932679852603273, 0.0635086490190615, 0.08265301708915862, 0.05814508982599631, 0.12721453922183246, 0.11706329861822595, 0.10630696636794656, 0.4174408512262425, 0.41873076422615274, 0.44278493340983627, 0.2123470216737544, 0.21871732615763728, 0.2200105933850952, 0.22008000655410365, 0.16538357465335463, 0.13387199369701974, 0.16636815324180299, 0.14716754841340518, 0.12746030363756877, 0.21459892081009235, 0.3376740437342389, 0.21157406285358538, 0.16947644277671503, 0.1644703645228397, 0.14361446048647464, 0.1907397339836573, 0.1695538471210719, 0.17152511706131024, 0.18809197229177876, 0.23240056220857597, 0.35739257552103, 0.30711331026701605, 0.35800000372394647, 0.27430480307612815, 0.10107003287669303, 0.38461052355102554, 0.40748602519626975, 0.3329604914039255, 0.3311750758329405, 0.3502616151401581, 0.3634412917063622, 0.3529535045867639, 0.34852116366747576, 0.2831203133435162, 0.24473953266556592, 0.16649180289750054, 0.29474586603190134, 0.1625909842331552, 0.31696283773167133, 0.14869086718791347, 0.07212689495988767, 0.06175901597576361, 0.020191522623446145, 0.03572120668890544, 0.038188410930948935, 0.0538084588160711, 0.06082424657643004, 0.03914242214484198, 0.07873098788749833, 0.1280191711671197, 0.11311878725947622, 0.12625932608897728, 0.08535519357748123, 0.09378270625489715, 0.12059452416952898, 0.10471029393977482, 0.12530781396926283, 0.021758851949409297, 0.01118865514291989, 0.01630852195620791, 0.009717848689135056, 0.005687385866965822, 0.007543861114835626, 0.01032720173348356, 0.01648145006013546, 0.005373827784677321, 0.07782528772435504, 0.07330077990565553, 0.06166601747141709, 0.08055000157538439, 0.07934930144482333, 0.062084647386244196, 0.10632178558194105, 0.07001529129837158, 0.10518737552427082, 0.35100325129559173, 0.37523474412177293, 0.37672805793904796, 0.36548934896301644, 0.3784362034739134, 0.29606697411473326, 0.38347943212156954, 0.36879532140862226, 0.4156318141657802, 0.06382786247443584, 0.07027068159229422, 0.06624128936599916, 0.05655454515347169, 0.05191204552758877, 0.05506194647616636, 0.05754263107754798, 0.044978851032902445, 0.06453824649042239, 0.17430559712214022, 0.14371516083960323, 0.13051352534288252, 0.1486892368088053, 0.14242895630840868, 0.11916859449762929, 0.1571485249749106, 0.14360241955760045, 0.14497734269037155, 0.20741927828451423, 0.19714385027183123, 0.17307556592616313, 0.21943769638530608, 0.17629886886715318, 0.1763960630323771, 0.17415519128369839, 0.14610377103098293, 0.13604240414071722, 0.15761735523911335, 0.17384670425875282, 0.12931732912620886, 0.15729233192226133, 0.15298296199388528, 0.15387607639362466, 0.13204630169711196, 0.1106239180094315, 0.10232685023347143, 0.18187502715462378, 0.15122193339174406, 0.1491135876626546, 0.17632443556245903, 0.1601654742865336, 0.15051735448101877, 0.19316221984484072, 0.1751426705706377, 0.186300441145674, 0.18717676149004225, 0.17272682736838418, 0.18311183184071178, 0.17713748478276103, 0.19446933698451196, 0.2035937959690537, 0.3837886709773012, 0.23595571520615266, 0.18984625380959352, 0.1372146125602216, 0.3375473553526086, 0.3751022808034383, 0.34124726872015887, 0.20497501473067492, 0.3041174875603694, 0.12875970971203898, 0.3946650494814252, 0.1387322605317557, 0.3577015084168791, 0.3406424644901558, 0.14168063322299607, 0.3218763830477227, 0.27719104026340924, 0.13076117137094367, 0.2911946769569581, 0.25316828466381325, 0.20684626425743513, 0.17951653794217437, 0.19543027414880643, 0.1898356726589886, 0.17357588525115408, 0.19616852667788487, 0.18477683563372382, 0.1910169861343758, 0.17449898452055512, 0.16900911271993313, 0.055545142414739845, 0.05029678957846151, 0.0451835029003681, 0.07009360399369924, 0.04843630757627959, 0.05027907031284684, 0.07167402092987196, 0.057166150127647364, 0.05062401906399627]}, "mutation_prompt": null}
{"id": "1929e17f-2393-404c-bc72-0bf08808203d", "solution": "import numpy as np\nfrom scipy.optimize import differential_evolution\n\nclass Hybrid_PSODE_SA_DE_Optimizer:\n    def __init__(self, budget, dim, num_particles=30, max_iterations=1000, inertia_weight=0.7, cognitive_weight=1.4, social_weight=1.4, initial_temperature=100, cooling_rate=0.95):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = num_particles\n        self.max_iterations = max_iterations\n        self.inertia_weight = inertia_weight\n        self.cognitive_weight = cognitive_weight\n        self.social_weight = social_weight\n        self.initial_temperature = initial_temperature\n        self.cooling_rate = cooling_rate\n\n    def __call__(self, func):\n        def hybrid_pso_sa_de_helper():\n            # PSO initialization\n            self.inertia_weight = np.random.uniform(0.4, 0.9)\n            # PSO optimization loop\n            # Simulated Annealing (SA) initialization\n            current_solution = np.random.uniform(-5.0, 5.0, size=self.dim)\n            current_cost = func(current_solution)\n            best_solution = current_solution\n            best_cost = current_cost\n            # SA optimization loop\n            for _ in range(int(self.budget*0.5)):  # Spend 50% of the budget on SA\n                proposal_solution = current_solution + np.random.normal(0, 0.1, size=self.dim)\n                proposal_solution = np.clip(proposal_solution, -5.0, 5.0)\n                proposal_cost = func(proposal_solution)\n                if proposal_cost < current_cost or np.random.rand() < np.exp((current_cost - proposal_cost) / self.initial_temperature):\n                    current_solution = proposal_solution\n                    current_cost = proposal_cost\n                    if proposal_cost < best_cost:\n                        best_solution = proposal_solution\n                        best_cost = proposal_cost\n            # Differential Evolution (DE) optimization\n            de_result = differential_evolution(func, bounds=[(-5.0, 5.0)]*self.dim, maxiter=int(self.budget*0.5))  # Spend 50% of the budget on DE\n            if de_result.fun < best_cost:\n                best_solution = de_result.x\n            return best_solution\n\n        return hybrid_pso_sa_de_helper()\n\noptimizer = Hybrid_PSODE_SA_DE_Optimizer(budget=1000, dim=10)", "name": "Hybrid_PSODE_SA_DE_Optimizer", "description": "Combining Particle Swarm Optimization (PSO), Simulated Annealing (SA), and Differential Evolution (DE) for a hybrid metaheuristic approach to optimize black box functions efficiently within a given budget.", "configspace": "", "generation": 81, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "d82bc67e-0c68-42f5-b876-0815e26c45c3", "metadata": {"aucs": [0.48456837961059296, 0.5105538177594323, 0.47507057601350744, 0.47136751809518973, 0.467732558082386, 0.4996677345119218, 0.48477110876648977, 0.47038103501661366, 0.545176959239378, 0.18273261797874463, 0.19429757105452794, 0.21506122537384254, 0.19451909647024235, 0.12146371433165404, 0.24079244109696885, 0.27263541071937736, 0.13427678057795633, 0.1949133209598758, 0.06820291502883957, 0.06966638517654022, 0.12395381192411059, 0.08289695025266608, 0.08296512362937791, 0.0704014936562467, 0.161588407958278, 0.06945382781693155, 0.0722924237460717, 0.09232046452458575, 0.05330572800163913, 0.07118382690440017, 0.08917587563413543, 0.06361582817029554, 0.05932679852603273, 0.0635086490190615, 0.08265301708915862, 0.05814508982599631, 0.12721453922183246, 0.11706329861822595, 0.10630696636794656, 0.4174408512262425, 0.41873076422615274, 0.44278493340983627, 0.2123470216737544, 0.21871732615763728, 0.2200105933850952, 0.22008000655410365, 0.16538357465335463, 0.13387199369701974, 0.16636815324180299, 0.14716754841340518, 0.12746030363756877, 0.21459892081009235, 0.3376740437342389, 0.21157406285358538, 0.16947644277671503, 0.1644703645228397, 0.14361446048647464, 0.1907397339836573, 0.1695538471210719, 0.17152511706131024, 0.18809197229177876, 0.23240056220857597, 0.35739257552103, 0.30711331026701605, 0.35800000372394647, 0.27430480307612815, 0.10107003287669303, 0.38461052355102554, 0.40748602519626975, 0.3329604914039255, 0.3311750758329405, 0.3502616151401581, 0.3634412917063622, 0.3529535045867639, 0.34852116366747576, 0.2831203133435162, 0.24473953266556592, 0.16649180289750054, 0.29474586603190134, 0.1625909842331552, 0.31696283773167133, 0.14869086718791347, 0.07212689495988767, 0.06175901597576361, 0.020191522623446145, 0.03572120668890544, 0.038188410930948935, 0.0538084588160711, 0.06082424657643004, 0.03914242214484198, 0.07873098788749833, 0.1280191711671197, 0.11311878725947622, 0.12625932608897728, 0.08535519357748123, 0.09378270625489715, 0.12059452416952898, 0.10471029393977482, 0.12530781396926283, 0.021758851949409297, 0.01118865514291989, 0.01630852195620791, 0.009717848689135056, 0.005687385866965822, 0.007543861114835626, 0.01032720173348356, 0.01648145006013546, 0.005373827784677321, 0.07782528772435504, 0.07330077990565553, 0.06166601747141709, 0.08055000157538439, 0.07934930144482333, 0.062084647386244196, 0.10632178558194105, 0.07001529129837158, 0.10518737552427082, 0.35100325129559173, 0.37523474412177293, 0.37672805793904796, 0.36548934896301644, 0.3784362034739134, 0.29606697411473326, 0.38347943212156954, 0.36879532140862226, 0.4156318141657802, 0.06382786247443584, 0.07027068159229422, 0.06624128936599916, 0.05655454515347169, 0.05191204552758877, 0.05506194647616636, 0.05754263107754798, 0.044978851032902445, 0.06453824649042239, 0.17430559712214022, 0.14371516083960323, 0.13051352534288252, 0.1486892368088053, 0.14242895630840868, 0.11916859449762929, 0.1571485249749106, 0.14360241955760045, 0.14497734269037155, 0.20741927828451423, 0.19714385027183123, 0.17307556592616313, 0.21943769638530608, 0.17629886886715318, 0.1763960630323771, 0.17415519128369839, 0.14610377103098293, 0.13604240414071722, 0.15761735523911335, 0.17384670425875282, 0.12931732912620886, 0.15729233192226133, 0.15298296199388528, 0.15387607639362466, 0.13204630169711196, 0.1106239180094315, 0.10232685023347143, 0.18187502715462378, 0.15122193339174406, 0.1491135876626546, 0.17632443556245903, 0.1601654742865336, 0.15051735448101877, 0.19316221984484072, 0.1751426705706377, 0.186300441145674, 0.18717676149004225, 0.17272682736838418, 0.18311183184071178, 0.17713748478276103, 0.19446933698451196, 0.2035937959690537, 0.3837886709773012, 0.23595571520615266, 0.18984625380959352, 0.1372146125602216, 0.3375473553526086, 0.3751022808034383, 0.34124726872015887, 0.20497501473067492, 0.3041174875603694, 0.12875970971203898, 0.3946650494814252, 0.1387322605317557, 0.3577015084168791, 0.3406424644901558, 0.14168063322299607, 0.3218763830477227, 0.27719104026340924, 0.13076117137094367, 0.2911946769569581, 0.25316828466381325, 0.20684626425743513, 0.17951653794217437, 0.19543027414880643, 0.1898356726589886, 0.17357588525115408, 0.19616852667788487, 0.18477683563372382, 0.1910169861343758, 0.17449898452055512, 0.16900911271993313, 0.055545142414739845, 0.05029678957846151, 0.0451835029003681, 0.07009360399369924, 0.04843630757627959, 0.05027907031284684, 0.07167402092987196, 0.057166150127647364, 0.05062401906399627]}, "mutation_prompt": null}
{"id": "c18bce62-32a9-4146-9cfd-f201af2df83e", "solution": "import numpy as np\nfrom scipy.optimize import differential_evolution\n\nclass Hybrid_PSODE_SA_DE_Optimizer:\n    def __init__(self, budget, dim, num_particles=30, max_iterations=1000, inertia_weight=0.7, cognitive_weight=1.4, social_weight=1.4, initial_temperature=100, cooling_rate=0.95):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = num_particles\n        self.max_iterations = max_iterations\n        self.inertia_weight = inertia_weight\n        self.cognitive_weight = cognitive_weight\n        self.social_weight = social_weight\n        self.initial_temperature = initial_temperature\n        self.cooling_rate = cooling_rate\n\n    def __call__(self, func):\n        def hybrid_pso_sa_de_helper():\n            # PSO initialization\n            self.inertia_weight = np.random.uniform(0.4, 0.9)\n            # PSO optimization loop\n            # Simulated Annealing (SA) initialization\n            current_solution = np.random.uniform(-5.0, 5.0, size=self.dim)\n            current_cost = func(current_solution)\n            best_solution = current_solution\n            best_cost = current_cost\n            # SA optimization loop\n            for _ in range(int(self.budget*0.5)):  # Spend 50% of the budget on SA\n                proposal_solution = current_solution + np.random.normal(0, 0.1, size=self.dim)\n                proposal_solution = np.clip(proposal_solution, -5.0, 5.0)\n                proposal_cost = func(proposal_solution)\n                if proposal_cost < current_cost or np.random.rand() < np.exp((current_cost - proposal_cost) / self.initial_temperature):\n                    current_solution = proposal_solution\n                    current_cost = proposal_cost\n                    if proposal_cost < best_cost:\n                        best_solution = proposal_solution\n                        best_cost = proposal_cost\n            # Differential Evolution (DE) optimization\n            de_result = differential_evolution(func, bounds=[(-5.0, 5.0)]*self.dim, maxiter=int(self.budget*0.5))  # Spend 50% of the budget on DE\n            if de_result.fun < best_cost:\n                best_solution = de_result.x\n            return best_solution\n\n        return hybrid_pso_sa_de_helper()\n\noptimizer = Hybrid_PSODE_SA_DE_Optimizer(budget=1000, dim=10)", "name": "Hybrid_PSODE_SA_DE_Optimizer", "description": "Combining Particle Swarm Optimization (PSO), Simulated Annealing (SA), and Differential Evolution (DE) for a hybrid metaheuristic approach to optimize black box functions efficiently within a given budget.", "configspace": "", "generation": 81, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "d82bc67e-0c68-42f5-b876-0815e26c45c3", "metadata": {"aucs": [0.48456837961059296, 0.5105538177594323, 0.47507057601350744, 0.47136751809518973, 0.467732558082386, 0.4996677345119218, 0.48477110876648977, 0.47038103501661366, 0.545176959239378, 0.18273261797874463, 0.19429757105452794, 0.21506122537384254, 0.19451909647024235, 0.12146371433165404, 0.24079244109696885, 0.27263541071937736, 0.13427678057795633, 0.1949133209598758, 0.06820291502883957, 0.06966638517654022, 0.12395381192411059, 0.08289695025266608, 0.08296512362937791, 0.0704014936562467, 0.161588407958278, 0.06945382781693155, 0.0722924237460717, 0.09232046452458575, 0.05330572800163913, 0.07118382690440017, 0.08917587563413543, 0.06361582817029554, 0.05932679852603273, 0.0635086490190615, 0.08265301708915862, 0.05814508982599631, 0.12721453922183246, 0.11706329861822595, 0.10630696636794656, 0.4174408512262425, 0.41873076422615274, 0.44278493340983627, 0.2123470216737544, 0.21871732615763728, 0.2200105933850952, 0.22008000655410365, 0.16538357465335463, 0.13387199369701974, 0.16636815324180299, 0.14716754841340518, 0.12746030363756877, 0.21459892081009235, 0.3376740437342389, 0.21157406285358538, 0.16947644277671503, 0.1644703645228397, 0.14361446048647464, 0.1907397339836573, 0.1695538471210719, 0.17152511706131024, 0.18809197229177876, 0.23240056220857597, 0.35739257552103, 0.30711331026701605, 0.35800000372394647, 0.27430480307612815, 0.10107003287669303, 0.38461052355102554, 0.40748602519626975, 0.3329604914039255, 0.3311750758329405, 0.3502616151401581, 0.3634412917063622, 0.3529535045867639, 0.34852116366747576, 0.2831203133435162, 0.24473953266556592, 0.16649180289750054, 0.29474586603190134, 0.1625909842331552, 0.31696283773167133, 0.14869086718791347, 0.07212689495988767, 0.06175901597576361, 0.020191522623446145, 0.03572120668890544, 0.038188410930948935, 0.0538084588160711, 0.06082424657643004, 0.03914242214484198, 0.07873098788749833, 0.1280191711671197, 0.11311878725947622, 0.12625932608897728, 0.08535519357748123, 0.09378270625489715, 0.12059452416952898, 0.10471029393977482, 0.12530781396926283, 0.021758851949409297, 0.01118865514291989, 0.01630852195620791, 0.009717848689135056, 0.005687385866965822, 0.007543861114835626, 0.01032720173348356, 0.01648145006013546, 0.005373827784677321, 0.07782528772435504, 0.07330077990565553, 0.06166601747141709, 0.08055000157538439, 0.07934930144482333, 0.062084647386244196, 0.10632178558194105, 0.07001529129837158, 0.10518737552427082, 0.35100325129559173, 0.37523474412177293, 0.37672805793904796, 0.36548934896301644, 0.3784362034739134, 0.29606697411473326, 0.38347943212156954, 0.36879532140862226, 0.4156318141657802, 0.06382786247443584, 0.07027068159229422, 0.06624128936599916, 0.05655454515347169, 0.05191204552758877, 0.05506194647616636, 0.05754263107754798, 0.044978851032902445, 0.06453824649042239, 0.17430559712214022, 0.14371516083960323, 0.13051352534288252, 0.1486892368088053, 0.14242895630840868, 0.11916859449762929, 0.1571485249749106, 0.14360241955760045, 0.14497734269037155, 0.20741927828451423, 0.19714385027183123, 0.17307556592616313, 0.21943769638530608, 0.17629886886715318, 0.1763960630323771, 0.17415519128369839, 0.14610377103098293, 0.13604240414071722, 0.15761735523911335, 0.17384670425875282, 0.12931732912620886, 0.15729233192226133, 0.15298296199388528, 0.15387607639362466, 0.13204630169711196, 0.1106239180094315, 0.10232685023347143, 0.18187502715462378, 0.15122193339174406, 0.1491135876626546, 0.17632443556245903, 0.1601654742865336, 0.15051735448101877, 0.19316221984484072, 0.1751426705706377, 0.186300441145674, 0.18717676149004225, 0.17272682736838418, 0.18311183184071178, 0.17713748478276103, 0.19446933698451196, 0.2035937959690537, 0.3837886709773012, 0.23595571520615266, 0.18984625380959352, 0.1372146125602216, 0.3375473553526086, 0.3751022808034383, 0.34124726872015887, 0.20497501473067492, 0.3041174875603694, 0.12875970971203898, 0.3946650494814252, 0.1387322605317557, 0.3577015084168791, 0.3406424644901558, 0.14168063322299607, 0.3218763830477227, 0.27719104026340924, 0.13076117137094367, 0.2911946769569581, 0.25316828466381325, 0.20684626425743513, 0.17951653794217437, 0.19543027414880643, 0.1898356726589886, 0.17357588525115408, 0.19616852667788487, 0.18477683563372382, 0.1910169861343758, 0.17449898452055512, 0.16900911271993313, 0.055545142414739845, 0.05029678957846151, 0.0451835029003681, 0.07009360399369924, 0.04843630757627959, 0.05027907031284684, 0.07167402092987196, 0.057166150127647364, 0.05062401906399627]}, "mutation_prompt": null}
{"id": "98d0c61a-6f34-4736-bf8c-c82e6fb25664", "solution": "import numpy as np\nfrom scipy.optimize import differential_evolution\n\nclass Hybrid_PSODE_SA_DE_Optimizer:\n    def __init__(self, budget, dim, num_particles=30, max_iterations=1000, inertia_weight=0.7, cognitive_weight=1.4, social_weight=1.4, initial_temperature=100, cooling_rate=0.95):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = num_particles\n        self.max_iterations = max_iterations\n        self.inertia_weight = inertia_weight\n        self.cognitive_weight = cognitive_weight\n        self.social_weight = social_weight\n        self.initial_temperature = initial_temperature\n        self.cooling_rate = cooling_rate\n\n    def __call__(self, func):\n        def hybrid_pso_sa_de_helper():\n            # PSO initialization\n            self.inertia_weight = np.random.uniform(0.4, 0.9)\n            # PSO optimization loop\n            # Simulated Annealing (SA) initialization\n            current_solution = np.random.uniform(-5.0, 5.0, size=self.dim)\n            current_cost = func(current_solution)\n            best_solution = current_solution\n            best_cost = current_cost\n            # SA optimization loop\n            for _ in range(int(self.budget*0.5)):  # Spend 50% of the budget on SA\n                proposal_solution = current_solution + np.random.normal(0, 0.1, size=self.dim)\n                proposal_solution = np.clip(proposal_solution, -5.0, 5.0)\n                proposal_cost = func(proposal_solution)\n                if proposal_cost < current_cost or np.random.rand() < np.exp((current_cost - proposal_cost) / self.initial_temperature):\n                    current_solution = proposal_solution\n                    current_cost = proposal_cost\n                    if proposal_cost < best_cost:\n                        best_solution = proposal_solution\n                        best_cost = proposal_cost\n            # Differential Evolution (DE) optimization\n            de_result = differential_evolution(func, bounds=[(-5.0, 5.0)]*self.dim, maxiter=int(self.budget*0.5))  # Spend 50% of the budget on DE\n            if de_result.fun < best_cost:\n                best_solution = de_result.x\n            return best_solution\n\n        return hybrid_pso_sa_de_helper()\n\noptimizer = Hybrid_PSODE_SA_DE_Optimizer(budget=1000, dim=10)", "name": "Hybrid_PSODE_SA_DE_Optimizer", "description": "Combining Particle Swarm Optimization (PSO), Simulated Annealing (SA), and Differential Evolution (DE) for a hybrid metaheuristic approach to optimize black box functions efficiently within a given budget.", "configspace": "", "generation": 81, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "d82bc67e-0c68-42f5-b876-0815e26c45c3", "metadata": {"aucs": [0.48456837961059296, 0.5105538177594323, 0.47507057601350744, 0.47136751809518973, 0.467732558082386, 0.4996677345119218, 0.48477110876648977, 0.47038103501661366, 0.545176959239378, 0.18273261797874463, 0.19429757105452794, 0.21506122537384254, 0.19451909647024235, 0.12146371433165404, 0.24079244109696885, 0.27263541071937736, 0.13427678057795633, 0.1949133209598758, 0.06820291502883957, 0.06966638517654022, 0.12395381192411059, 0.08289695025266608, 0.08296512362937791, 0.0704014936562467, 0.161588407958278, 0.06945382781693155, 0.0722924237460717, 0.09232046452458575, 0.05330572800163913, 0.07118382690440017, 0.08917587563413543, 0.06361582817029554, 0.05932679852603273, 0.0635086490190615, 0.08265301708915862, 0.05814508982599631, 0.12721453922183246, 0.11706329861822595, 0.10630696636794656, 0.4174408512262425, 0.41873076422615274, 0.44278493340983627, 0.2123470216737544, 0.21871732615763728, 0.2200105933850952, 0.22008000655410365, 0.16538357465335463, 0.13387199369701974, 0.16636815324180299, 0.14716754841340518, 0.12746030363756877, 0.21459892081009235, 0.3376740437342389, 0.21157406285358538, 0.16947644277671503, 0.1644703645228397, 0.14361446048647464, 0.1907397339836573, 0.1695538471210719, 0.17152511706131024, 0.18809197229177876, 0.23240056220857597, 0.35739257552103, 0.30711331026701605, 0.35800000372394647, 0.27430480307612815, 0.10107003287669303, 0.38461052355102554, 0.40748602519626975, 0.3329604914039255, 0.3311750758329405, 0.3502616151401581, 0.3634412917063622, 0.3529535045867639, 0.34852116366747576, 0.2831203133435162, 0.24473953266556592, 0.16649180289750054, 0.29474586603190134, 0.1625909842331552, 0.31696283773167133, 0.14869086718791347, 0.07212689495988767, 0.06175901597576361, 0.020191522623446145, 0.03572120668890544, 0.038188410930948935, 0.0538084588160711, 0.06082424657643004, 0.03914242214484198, 0.07873098788749833, 0.1280191711671197, 0.11311878725947622, 0.12625932608897728, 0.08535519357748123, 0.09378270625489715, 0.12059452416952898, 0.10471029393977482, 0.12530781396926283, 0.021758851949409297, 0.01118865514291989, 0.01630852195620791, 0.009717848689135056, 0.005687385866965822, 0.007543861114835626, 0.01032720173348356, 0.01648145006013546, 0.005373827784677321, 0.07782528772435504, 0.07330077990565553, 0.06166601747141709, 0.08055000157538439, 0.07934930144482333, 0.062084647386244196, 0.10632178558194105, 0.07001529129837158, 0.10518737552427082, 0.35100325129559173, 0.37523474412177293, 0.37672805793904796, 0.36548934896301644, 0.3784362034739134, 0.29606697411473326, 0.38347943212156954, 0.36879532140862226, 0.4156318141657802, 0.06382786247443584, 0.07027068159229422, 0.06624128936599916, 0.05655454515347169, 0.05191204552758877, 0.05506194647616636, 0.05754263107754798, 0.044978851032902445, 0.06453824649042239, 0.17430559712214022, 0.14371516083960323, 0.13051352534288252, 0.1486892368088053, 0.14242895630840868, 0.11916859449762929, 0.1571485249749106, 0.14360241955760045, 0.14497734269037155, 0.20741927828451423, 0.19714385027183123, 0.17307556592616313, 0.21943769638530608, 0.17629886886715318, 0.1763960630323771, 0.17415519128369839, 0.14610377103098293, 0.13604240414071722, 0.15761735523911335, 0.17384670425875282, 0.12931732912620886, 0.15729233192226133, 0.15298296199388528, 0.15387607639362466, 0.13204630169711196, 0.1106239180094315, 0.10232685023347143, 0.18187502715462378, 0.15122193339174406, 0.1491135876626546, 0.17632443556245903, 0.1601654742865336, 0.15051735448101877, 0.19316221984484072, 0.1751426705706377, 0.186300441145674, 0.18717676149004225, 0.17272682736838418, 0.18311183184071178, 0.17713748478276103, 0.19446933698451196, 0.2035937959690537, 0.3837886709773012, 0.23595571520615266, 0.18984625380959352, 0.1372146125602216, 0.3375473553526086, 0.3751022808034383, 0.34124726872015887, 0.20497501473067492, 0.3041174875603694, 0.12875970971203898, 0.3946650494814252, 0.1387322605317557, 0.3577015084168791, 0.3406424644901558, 0.14168063322299607, 0.3218763830477227, 0.27719104026340924, 0.13076117137094367, 0.2911946769569581, 0.25316828466381325, 0.20684626425743513, 0.17951653794217437, 0.19543027414880643, 0.1898356726589886, 0.17357588525115408, 0.19616852667788487, 0.18477683563372382, 0.1910169861343758, 0.17449898452055512, 0.16900911271993313, 0.055545142414739845, 0.05029678957846151, 0.0451835029003681, 0.07009360399369924, 0.04843630757627959, 0.05027907031284684, 0.07167402092987196, 0.057166150127647364, 0.05062401906399627]}, "mutation_prompt": null}
{"id": "3cace4a8-a6d3-4355-ab80-690ad5e8a55e", "solution": "import numpy as np\nfrom scipy.optimize import differential_evolution\n\nclass Dynamic_Hybrid_PSODE_SA_DE_Optimizer:\n    def __init__(self, budget, dim, num_particles=30, max_iterations=1000, inertia_weight=0.7, cognitive_weight=1.4, social_weight=1.4, initial_temperature=100, cooling_rate=0.95):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = num_particles\n        self.max_iterations = max_iterations\n        self.inertia_weight = inertia_weight\n        self.cognitive_weight = cognitive_weight\n        self.social_weight = social_weight\n        self.initial_temperature = initial_temperature\n        self.cooling_rate = cooling_rate\n\n    def __call__(self, func):\n        def dynamic_hybrid_pso_sa_de_helper():\n            # PSO initialization\n            inertia_weight = np.random.uniform(0.4, 0.9)\n            cognitive_weight = np.random.uniform(0.5, 1.5)  # Dynamic cognitive weight\n            social_weight = np.random.uniform(0.5, 1.5)  # Dynamic social weight\n            # PSO optimization loop with dynamic weights\n            # Simulated Annealing (SA) initialization\n            current_solution = np.random.uniform(-5.0, 5.0, size=self.dim)\n            current_cost = func(current_solution)\n            best_solution = current_solution\n            best_cost = current_cost\n            # SA optimization loop\n            for _ in range(int(self.budget*0.5)):  # Spend 50% of the budget on SA\n                proposal_solution = current_solution + np.random.normal(0, 0.1, size=self.dim)\n                proposal_solution = np.clip(proposal_solution, -5.0, 5.0)\n                proposal_cost = func(proposal_solution)\n                if proposal_cost < current_cost or np.random.rand() < np.exp((current_cost - proposal_cost) / self.initial_temperature):\n                    current_solution = proposal_solution\n                    current_cost = proposal_cost\n                    if proposal_cost < best_cost:\n                        best_solution = proposal_solution\n                        best_cost = proposal_cost\n            # Differential Evolution (DE) optimization\n            de_result = differential_evolution(func, bounds=[(-5.0, 5.0)]*self.dim, maxiter=int(self.budget*0.5))  # Spend 50% of the budget on DE\n            if de_result.fun < best_cost:\n                best_solution = de_result.x\n            return best_solution\n\n        return dynamic_hybrid_pso_sa_de_helper()\n\noptimizer = Dynamic_Hybrid_PSODE_SA_DE_Optimizer(budget=1000, dim=10)", "name": "Dynamic_Hybrid_PSODE_SA_DE_Optimizer", "description": "Enhancing the Hybrid_PSODE_SA_DE_Optimizer by dynamically adjusting the weights of PSO, SA, and DE based on their performance to improve convergence speed and solution quality.", "configspace": "", "generation": 92, "fitness": 0.17538759521570832, "feedback": "The algorithm Dynamic_Hybrid_PSODE_SA_DE_Optimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.18 with standard deviation 0.12.", "error": "", "parent_id": "d82bc67e-0c68-42f5-b876-0815e26c45c3", "metadata": {"aucs": [0.46010766260697733, 0.4701212461769323, 0.46685541619646087, 0.4701368834177254, 0.47256051095636364, 0.46736763381707735, 0.47544195155736113, 0.48127719919214795, 0.48265601718568485, 0.21591803381972663, 0.24389645413744931, 0.15841588297748277, 0.1402781685660517, 0.23148582590227595, 0.2246802292970167, 0.2254859276075849, 0.13292700963131088, 0.1920241790982945, 0.08444758090286142, 0.07549783265284282, 0.05106057604335823, 0.06466749851277231, 0.08881691969632577, 0.06188778551415253, 0.07897798357763963, 0.06924742762413683, 0.06371509382039253, 0.06676854359931894, 0.05532039374486608, 0.055260683291984636, 0.05948649235149439, 0.06333198126609474, 0.06500407783792328, 0.05323306712496145, 0.05549992290674999, 0.05393425305114741, 0.1056468454478563, 0.10827600143690475, 0.1009949690365145, 0.4173318165496124, 0.42569263096058974, 0.41139391454586616, 0.1808978143446689, 0.18797274603609593, 0.234839294985292, 0.14307824666462599, 0.14889447214949092, 0.12935344963489748, 0.14246931503207128, 0.16898176961748068, 0.12063042981095873, 0.20917010543043602, 0.2330713632958531, 0.1594103554736439, 0.15539504046431507, 0.216682923018661, 0.18076254790275637, 0.16897577096654115, 0.17766559833937423, 0.1824670035651922, 0.3293194609556911, 0.2534990603247643, 0.22336304332755064, 0.32592422375229746, 0.35726242137786157, 0.3198531416972511, 0.2902239269118835, 0.3023684990432176, 0.10660481234275665, 0.35594510135163104, 0.30084951223928025, 0.3422361652923165, 0.34863061973575327, 0.32255997275962744, 0.32220817862419404, 0.22264023637513775, 0.11811724970459725, 0.22989645475056153, 0.3089642191507552, 0.19020452805046895, 0.25476889811055003, 0.010098752980133185, 0.015164620740529933, 0.06561970244367255, 0.006754104239328096, 0.06250737195159095, 0.05818465289976105, 0.03485812524642384, 0.07072551840191377, 0.02959973250182124, 0.09963708636100566, 0.10752401435700087, 0.09304415971587932, 0.11163381354058022, 0.06189871247224488, 0.0903548300328263, 0.13388346154575015, 0.10219406562808175, 0.11289043170845237, 0.006652125400330222, 0.0011857574793044945, 0.0018061609991265604, 0.015551029114673653, 0.010935611408406265, 0.01744849153224637, 0.0151275168148306, 0.0014930384159108634, 0.01809800840375153, 0.06940088131211519, 0.08559132089776211, 0.06717437498212331, 0.08216079108474095, 0.09254609885955944, 0.06851826924045601, 0.10736566244952794, 0.12059305224658112, 0.10017450575826592, 0.35710367317277647, 0.3188060943313118, 0.41175864717073496, 0.39647249055984957, 0.32249853570118325, 0.354767009180764, 0.39936676397740056, 0.4281353361151513, 0.441627744603724, 0.06456484189352996, 0.08399458549839911, 0.07936852555638219, 0.051846191435743316, 0.09608622400492473, 0.07325899172143013, 0.04659226406694528, 0.10619663170519311, 0.06706210722458905, 0.15187846682882267, 0.14559220802466077, 0.136496363959142, 0.1438127083305284, 0.13137231025413199, 0.14153363371851646, 0.1496642160989894, 0.1569611224694749, 0.16062691032691678, 0.19186114026938716, 0.20627735395775182, 0.2177225847247377, 0.21567992631537714, 0.20378154895573564, 0.19921687451889525, 0.14412812822137, 0.1751050064974723, 0.1496385053551832, 0.13248529054894587, 0.14115387092311704, 0.13933182518119158, 0.16316404279402053, 0.13649179243869147, 0.12175149658503992, 0.11931404965347392, 0.11432629430535035, 0.10497562011396733, 0.1842569690594844, 0.16652904526490142, 0.14508862647753462, 0.17287437180165977, 0.16827051145923266, 0.17439516382467557, 0.18831727629080286, 0.1776130380043962, 0.14922563158818847, 0.18980110023452046, 0.18413717560753362, 0.19449976612702358, 0.19938133201102815, 0.1956101323762568, 0.1795969258668092, 0.20233023567340014, 0.18803455548625403, 0.18179426359255757, 0.13374949449246631, 0.1360871520600485, 0.13487353243669442, 0.3292124938240295, 0.35927962339637276, 0.44525718807387615, 0.1286074707832049, 0.3368238945379076, 0.3624844860970785, 0.25733142585600455, 0.28206951133290015, 0.11563424345173667, 0.12798972215407134, 0.12492263077908938, 0.10684261100052028, 0.36556715397140627, 0.2773699254048838, 0.28255778040039004, 0.18646502383093255, 0.1775219787898179, 0.19472073741182427, 0.17241219075911318, 0.19349487164816204, 0.1911437729518014, 0.18213655316161947, 0.17879386708608325, 0.19009985022931852, 0.060848311951669176, 0.0705307303004371, 0.05640714044201289, 0.05467882144091696, 0.058309256592701475, 0.054903559979943006, 0.058349267562368134, 0.04802479081836519, 0.05092908212854341]}, "mutation_prompt": null}
{"id": "652f21bf-2f26-4b42-ae61-9202d0de8dc7", "solution": "import numpy as np\nfrom scipy.optimize import differential_evolution\n\nclass Hybrid_PSODE_SA_DE_Optimizer:\n    def __init__(self, budget, dim, num_particles=30, max_iterations=1000, inertia_weight=0.7, cognitive_weight=1.4, social_weight=1.4, initial_temperature=100, cooling_rate=0.95):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = num_particles\n        self.max_iterations = max_iterations\n        self.inertia_weight = inertia_weight\n        self.cognitive_weight = cognitive_weight\n        self.social_weight = social_weight\n        self.initial_temperature = initial_temperature\n        self.cooling_rate = cooling_rate\n\n    def __call__(self, func):\n        def hybrid_pso_sa_de_helper():\n            # PSO initialization\n            self.inertia_weight = np.random.uniform(0.4, 0.9)\n            # PSO optimization loop\n            # Simulated Annealing (SA) initialization\n            current_solution = np.random.uniform(-5.0, 5.0, size=self.dim)\n            current_cost = func(current_solution)\n            best_solution = current_solution\n            best_cost = current_cost\n            # SA optimization loop\n            for _ in range(int(self.budget*0.5)):  # Spend 50% of the budget on SA\n                proposal_solution = current_solution + np.random.normal(0, 0.1, size=self.dim)\n                proposal_solution = np.clip(proposal_solution, -5.0, 5.0)\n                proposal_cost = func(proposal_solution)\n                if proposal_cost < current_cost or np.random.rand() < np.exp((current_cost - proposal_cost) / self.initial_temperature):\n                    current_solution = proposal_solution\n                    current_cost = proposal_cost\n                    if proposal_cost < best_cost:\n                        best_solution = proposal_solution\n                        best_cost = proposal_cost\n            # Differential Evolution (DE) optimization\n            de_result = differential_evolution(func, bounds=[(-5.0, 5.0)]*self.dim, maxiter=int(self.budget*0.5))  # Spend 50% of the budget on DE\n            if de_result.fun < best_cost:\n                best_solution = de_result.x\n            return best_solution\n\n        return hybrid_pso_sa_de_helper()\n\noptimizer = Hybrid_PSODE_SA_DE_Optimizer(budget=1000, dim=10)", "name": "Hybrid_PSODE_SA_DE_Optimizer", "description": "Combining Particle Swarm Optimization (PSO), Simulated Annealing (SA), and Differential Evolution (DE) for a hybrid metaheuristic approach to optimize black box functions efficiently within a given budget.", "configspace": "", "generation": 81, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "d82bc67e-0c68-42f5-b876-0815e26c45c3", "metadata": {"aucs": [0.48456837961059296, 0.5105538177594323, 0.47507057601350744, 0.47136751809518973, 0.467732558082386, 0.4996677345119218, 0.48477110876648977, 0.47038103501661366, 0.545176959239378, 0.18273261797874463, 0.19429757105452794, 0.21506122537384254, 0.19451909647024235, 0.12146371433165404, 0.24079244109696885, 0.27263541071937736, 0.13427678057795633, 0.1949133209598758, 0.06820291502883957, 0.06966638517654022, 0.12395381192411059, 0.08289695025266608, 0.08296512362937791, 0.0704014936562467, 0.161588407958278, 0.06945382781693155, 0.0722924237460717, 0.09232046452458575, 0.05330572800163913, 0.07118382690440017, 0.08917587563413543, 0.06361582817029554, 0.05932679852603273, 0.0635086490190615, 0.08265301708915862, 0.05814508982599631, 0.12721453922183246, 0.11706329861822595, 0.10630696636794656, 0.4174408512262425, 0.41873076422615274, 0.44278493340983627, 0.2123470216737544, 0.21871732615763728, 0.2200105933850952, 0.22008000655410365, 0.16538357465335463, 0.13387199369701974, 0.16636815324180299, 0.14716754841340518, 0.12746030363756877, 0.21459892081009235, 0.3376740437342389, 0.21157406285358538, 0.16947644277671503, 0.1644703645228397, 0.14361446048647464, 0.1907397339836573, 0.1695538471210719, 0.17152511706131024, 0.18809197229177876, 0.23240056220857597, 0.35739257552103, 0.30711331026701605, 0.35800000372394647, 0.27430480307612815, 0.10107003287669303, 0.38461052355102554, 0.40748602519626975, 0.3329604914039255, 0.3311750758329405, 0.3502616151401581, 0.3634412917063622, 0.3529535045867639, 0.34852116366747576, 0.2831203133435162, 0.24473953266556592, 0.16649180289750054, 0.29474586603190134, 0.1625909842331552, 0.31696283773167133, 0.14869086718791347, 0.07212689495988767, 0.06175901597576361, 0.020191522623446145, 0.03572120668890544, 0.038188410930948935, 0.0538084588160711, 0.06082424657643004, 0.03914242214484198, 0.07873098788749833, 0.1280191711671197, 0.11311878725947622, 0.12625932608897728, 0.08535519357748123, 0.09378270625489715, 0.12059452416952898, 0.10471029393977482, 0.12530781396926283, 0.021758851949409297, 0.01118865514291989, 0.01630852195620791, 0.009717848689135056, 0.005687385866965822, 0.007543861114835626, 0.01032720173348356, 0.01648145006013546, 0.005373827784677321, 0.07782528772435504, 0.07330077990565553, 0.06166601747141709, 0.08055000157538439, 0.07934930144482333, 0.062084647386244196, 0.10632178558194105, 0.07001529129837158, 0.10518737552427082, 0.35100325129559173, 0.37523474412177293, 0.37672805793904796, 0.36548934896301644, 0.3784362034739134, 0.29606697411473326, 0.38347943212156954, 0.36879532140862226, 0.4156318141657802, 0.06382786247443584, 0.07027068159229422, 0.06624128936599916, 0.05655454515347169, 0.05191204552758877, 0.05506194647616636, 0.05754263107754798, 0.044978851032902445, 0.06453824649042239, 0.17430559712214022, 0.14371516083960323, 0.13051352534288252, 0.1486892368088053, 0.14242895630840868, 0.11916859449762929, 0.1571485249749106, 0.14360241955760045, 0.14497734269037155, 0.20741927828451423, 0.19714385027183123, 0.17307556592616313, 0.21943769638530608, 0.17629886886715318, 0.1763960630323771, 0.17415519128369839, 0.14610377103098293, 0.13604240414071722, 0.15761735523911335, 0.17384670425875282, 0.12931732912620886, 0.15729233192226133, 0.15298296199388528, 0.15387607639362466, 0.13204630169711196, 0.1106239180094315, 0.10232685023347143, 0.18187502715462378, 0.15122193339174406, 0.1491135876626546, 0.17632443556245903, 0.1601654742865336, 0.15051735448101877, 0.19316221984484072, 0.1751426705706377, 0.186300441145674, 0.18717676149004225, 0.17272682736838418, 0.18311183184071178, 0.17713748478276103, 0.19446933698451196, 0.2035937959690537, 0.3837886709773012, 0.23595571520615266, 0.18984625380959352, 0.1372146125602216, 0.3375473553526086, 0.3751022808034383, 0.34124726872015887, 0.20497501473067492, 0.3041174875603694, 0.12875970971203898, 0.3946650494814252, 0.1387322605317557, 0.3577015084168791, 0.3406424644901558, 0.14168063322299607, 0.3218763830477227, 0.27719104026340924, 0.13076117137094367, 0.2911946769569581, 0.25316828466381325, 0.20684626425743513, 0.17951653794217437, 0.19543027414880643, 0.1898356726589886, 0.17357588525115408, 0.19616852667788487, 0.18477683563372382, 0.1910169861343758, 0.17449898452055512, 0.16900911271993313, 0.055545142414739845, 0.05029678957846151, 0.0451835029003681, 0.07009360399369924, 0.04843630757627959, 0.05027907031284684, 0.07167402092987196, 0.057166150127647364, 0.05062401906399627]}, "mutation_prompt": null}
{"id": "74bcf08c-7a48-42db-8da2-05e946c89242", "solution": "import numpy as np\nfrom scipy.optimize import differential_evolution\n\nclass Enhanced_Hybrid_PSODE_SA_DE_Optimizer:\n    def __init__(self, budget, dim, num_particles=30, max_iterations=1000, inertia_weight=0.7, cognitive_weight=1.4, social_weight=1.4, initial_temperature=100, cooling_rate=0.95):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = num_particles\n        self.max_iterations = max_iterations\n        self.inertia_weight = inertia_weight\n        self.cognitive_weight = cognitive_weight\n        self.social_weight = social_weight\n        self.initial_temperature = initial_temperature\n        self.cooling_rate = cooling_rate\n\n    def __call__(self, func):\n        def enhanced_hybrid_optimizer():\n            # PSO initialization with dynamic inertia weight\n            self.inertia_weight = np.random.uniform(0.4, 0.9)\n            # PSO optimization loop\n            # Simulated Annealing (SA) initialization with novel mutation operator\n            current_solution = np.random.uniform(-5.0, 5.0, size=self.dim)\n            current_cost = func(current_solution)\n            best_solution = current_solution\n            best_cost = current_cost\n            # SA optimization loop\n            for _ in range(int(self.budget*0.5)):  # Spend 50% of the budget on SA\n                mutation_strength = 0.1 * np.exp(-_ / (self.budget*0.5))  # Dynamic mutation strength\n                proposal_solution = current_solution + np.random.normal(0, mutation_strength, size=self.dim)\n                proposal_solution = np.clip(proposal_solution, -5.0, 5.0)\n                proposal_cost = func(proposal_solution)\n                if proposal_cost < current_cost or np.random.rand() < np.exp((current_cost - proposal_cost) / self.initial_temperature):\n                    current_solution = proposal_solution\n                    current_cost = proposal_cost\n                    if proposal_cost < best_cost:\n                        best_solution = proposal_solution\n                        best_cost = proposal_cost\n            # Differential Evolution (DE) optimization\n            de_result = differential_evolution(func, bounds=[(-5.0, 5.0)]*self.dim, maxiter=int(self.budget*0.5))  # Spend 50% of the budget on DE\n            if de_result.fun < best_cost:\n                best_solution = de_result.x\n            return best_solution\n\n        return enhanced_hybrid_optimizer()\n\nenhanced_optimizer = Enhanced_Hybrid_PSODE_SA_DE_Optimizer(budget=1000, dim=10)", "name": "Enhanced_Hybrid_PSODE_SA_DE_Optimizer", "description": "Enhancing the Hybrid_PSODE_SA_DE_Optimizer by dynamically adjusting the PSO inertia weight and incorporating a novel mutation operator in the Simulated Annealing phase to improve solution exploration.", "configspace": "", "generation": 94, "fitness": 0.1741871964276338, "feedback": "The algorithm Enhanced_Hybrid_PSODE_SA_DE_Optimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.17 with standard deviation 0.12.", "error": "", "parent_id": "d82bc67e-0c68-42f5-b876-0815e26c45c3", "metadata": {"aucs": [0.478476196747768, 0.47361395589689825, 0.4679779689171393, 0.4528574559095224, 0.4464427635405167, 0.5026337648874222, 0.45899146806106383, 0.4657947497770998, 0.551718017653919, 0.29728138471852805, 0.1745048408845571, 0.25647520120661116, 0.2781670489321605, 0.21918679213034864, 0.2571654396759756, 0.26049061367881743, 0.22498082340358905, 0.23948738648194157, 0.07032374668238917, 0.06286939854317697, 0.05318186483935894, 0.0699204879647588, 0.05759165143378431, 0.06963768893644373, 0.07496642291224687, 0.06674872486191086, 0.055623001569966024, 0.07318518560484988, 0.055917166171048804, 0.054668922008361154, 0.05296783365162516, 0.07385480126481447, 0.05623895918326982, 0.05904679453441042, 0.05987399596319465, 0.05277821664245963, 0.11532730749761178, 0.10501087372815021, 0.11542425416414903, 0.417280804666882, 0.4144584450500032, 0.4280584718648811, 0.20846189056312536, 0.18817194208558563, 0.29133967422181295, 0.1856354074811275, 0.15089209708575912, 0.11607855969638625, 0.17038750284955073, 0.18096230797374024, 0.1209579205742386, 0.16550621467686377, 0.2868194467441546, 0.1775904072424882, 0.1653769808558505, 0.1314857863506288, 0.16855141292485476, 0.17133571489232813, 0.12298048962365826, 0.15206698796640117, 0.2887614334706091, 0.32047470530951827, 0.17519655884154473, 0.23875217265159931, 0.34774855017715467, 0.26123240330681075, 0.3410889040998708, 0.4085413603382517, 0.36017840820052405, 0.3093735037832147, 0.3021702971198723, 0.21832295055716477, 0.2898956342606851, 0.27117492478248706, 0.37285368306704325, 0.26768802711491446, 0.2933613348132911, 0.2776033831780851, 0.3053441703436559, 0.29301881540026, 0.2766717751640484, 0.08358457533113706, 0.05840242191901801, 0.04312088513569434, 0.004301730889366251, 0.054989366752731805, 0.03707164764527593, 0.05968731172507191, 0.056394179878223816, 0.02182861761299104, 0.08712551579580063, 0.10282911403526873, 0.16269103708228072, 0.11597350913848825, 0.060126715480390724, 0.08545823878775938, 0.13650046729150056, 0.11207832957613595, 0.11193624180337391, 0.02855011231350113, 0.03650904979044012, 0.016312155226897773, 0.0024190447812263427, 0.02257520184275441, 0.014682559950282448, 0.021653983525968812, 0.01768353832675751, 0.0038242540978984385, 0.05917173814835819, 0.05928398119162104, 0.06641083726355357, 0.057497093442250846, 0.08363270071942464, 0.06708018834776519, 0.053464086338478545, 0.1650989004540041, 0.06848513123327649, 0.3150795755736616, 0.38219570970012917, 0.3364045002172694, 0.3428318924707998, 0.3869317509128507, 0.37264807160464586, 0.4600496194151418, 0.40217599099580714, 0.34275877510248887, 0.05790959200712131, 0.06318624477809043, 0.044442842468315535, 0.052148462375041005, 0.059562679419019715, 0.055970752508669785, 0.05997967210194943, 0.05472205971754507, 0.055285268601833026, 0.16229164551389352, 0.1380229371552144, 0.13835178951348093, 0.12292352637023973, 0.14505546333706865, 0.14220996923086582, 0.12273148833673708, 0.12231126966276717, 0.12171525763705193, 0.20173033937219886, 0.1942750788512274, 0.18885946325042335, 0.19016210019229962, 0.18486409030807793, 0.17481428539961963, 0.15891642614705104, 0.15119563257585888, 0.1394242791107153, 0.13103980273245963, 0.1683971993712503, 0.1142332121347579, 0.16547793162358826, 0.11292939320803763, 0.137183198115168, 0.12023387069383529, 0.11247509077852158, 0.08639463641549794, 0.179509896591602, 0.15240292518596998, 0.15008031320371484, 0.18453288651423827, 0.13616585116697533, 0.17323726849779453, 0.20568053162819433, 0.12403584733121786, 0.150545872564799, 0.18834039204745578, 0.17440244448603048, 0.18363121270784155, 0.18317080726359358, 0.18038837623584603, 0.18544998129174028, 0.20777905125367813, 0.21883511711119286, 0.21179097680461845, 0.12169211921943102, 0.3219225366160916, 0.27249199084984055, 0.3576628445293174, 0.15141537113714365, 0.12625074994339536, 0.3136292703968634, 0.15090290476581647, 0.3250570559129383, 0.13031934329709238, 0.15097946934072237, 0.1389371531707082, 0.11372786146699365, 0.29774675411924434, 0.13426024574342266, 0.26856234066065165, 0.09338786413995126, 0.09754839466604825, 0.19605374349718208, 0.18921479704977573, 0.19712544026286138, 0.20013952376200628, 0.18378950760869261, 0.18958626324879335, 0.18268495843481702, 0.18724693278760485, 0.1859366296276237, 0.07223933818607953, 0.05696505333002111, 0.06349988774483428, 0.052492833925026905, 0.0544910907432824, 0.04275438765801087, 0.07878335489006927, 0.053857308775008494, 0.05490742305899654]}, "mutation_prompt": null}
{"id": "36287df6-564f-4d76-a952-ac293d9d325e", "solution": "import numpy as np\nfrom scipy.optimize import differential_evolution\n\nclass Hybrid_PSODE_SA_DE_Optimizer:\n    def __init__(self, budget, dim, num_particles=30, max_iterations=1000, inertia_weight=0.7, cognitive_weight=1.4, social_weight=1.4, initial_temperature=100, cooling_rate=0.95):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = num_particles\n        self.max_iterations = max_iterations\n        self.inertia_weight = inertia_weight\n        self.cognitive_weight = cognitive_weight\n        self.social_weight = social_weight\n        self.initial_temperature = initial_temperature\n        self.cooling_rate = cooling_rate\n\n    def __call__(self, func):\n        def hybrid_pso_sa_de_helper():\n            # PSO initialization\n            self.inertia_weight = np.random.uniform(0.4, 0.9)\n            # PSO optimization loop\n            # Simulated Annealing (SA) initialization\n            current_solution = np.random.uniform(-5.0, 5.0, size=self.dim)\n            current_cost = func(current_solution)\n            best_solution = current_solution\n            best_cost = current_cost\n            # SA optimization loop\n            for _ in range(int(self.budget*0.5)):  # Spend 50% of the budget on SA\n                proposal_solution = current_solution + np.random.normal(0, 0.1, size=self.dim)\n                proposal_solution = np.clip(proposal_solution, -5.0, 5.0)\n                proposal_cost = func(proposal_solution)\n                if proposal_cost < current_cost or np.random.rand() < np.exp((current_cost - proposal_cost) / self.initial_temperature):\n                    current_solution = proposal_solution\n                    current_cost = proposal_cost\n                    if proposal_cost < best_cost:\n                        best_solution = proposal_solution\n                        best_cost = proposal_cost\n            # Differential Evolution (DE) optimization\n            de_result = differential_evolution(func, bounds=[(-5.0, 5.0)]*self.dim, maxiter=int(self.budget*0.5))  # Spend 50% of the budget on DE\n            if de_result.fun < best_cost:\n                best_solution = de_result.x\n            return best_solution\n\n        return hybrid_pso_sa_de_helper()\n\noptimizer = Hybrid_PSODE_SA_DE_Optimizer(budget=1000, dim=10)", "name": "Hybrid_PSODE_SA_DE_Optimizer", "description": "Combining Particle Swarm Optimization (PSO), Simulated Annealing (SA), and Differential Evolution (DE) for a hybrid metaheuristic approach to optimize black box functions efficiently within a given budget.", "configspace": "", "generation": 81, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "d82bc67e-0c68-42f5-b876-0815e26c45c3", "metadata": {"aucs": [0.48456837961059296, 0.5105538177594323, 0.47507057601350744, 0.47136751809518973, 0.467732558082386, 0.4996677345119218, 0.48477110876648977, 0.47038103501661366, 0.545176959239378, 0.18273261797874463, 0.19429757105452794, 0.21506122537384254, 0.19451909647024235, 0.12146371433165404, 0.24079244109696885, 0.27263541071937736, 0.13427678057795633, 0.1949133209598758, 0.06820291502883957, 0.06966638517654022, 0.12395381192411059, 0.08289695025266608, 0.08296512362937791, 0.0704014936562467, 0.161588407958278, 0.06945382781693155, 0.0722924237460717, 0.09232046452458575, 0.05330572800163913, 0.07118382690440017, 0.08917587563413543, 0.06361582817029554, 0.05932679852603273, 0.0635086490190615, 0.08265301708915862, 0.05814508982599631, 0.12721453922183246, 0.11706329861822595, 0.10630696636794656, 0.4174408512262425, 0.41873076422615274, 0.44278493340983627, 0.2123470216737544, 0.21871732615763728, 0.2200105933850952, 0.22008000655410365, 0.16538357465335463, 0.13387199369701974, 0.16636815324180299, 0.14716754841340518, 0.12746030363756877, 0.21459892081009235, 0.3376740437342389, 0.21157406285358538, 0.16947644277671503, 0.1644703645228397, 0.14361446048647464, 0.1907397339836573, 0.1695538471210719, 0.17152511706131024, 0.18809197229177876, 0.23240056220857597, 0.35739257552103, 0.30711331026701605, 0.35800000372394647, 0.27430480307612815, 0.10107003287669303, 0.38461052355102554, 0.40748602519626975, 0.3329604914039255, 0.3311750758329405, 0.3502616151401581, 0.3634412917063622, 0.3529535045867639, 0.34852116366747576, 0.2831203133435162, 0.24473953266556592, 0.16649180289750054, 0.29474586603190134, 0.1625909842331552, 0.31696283773167133, 0.14869086718791347, 0.07212689495988767, 0.06175901597576361, 0.020191522623446145, 0.03572120668890544, 0.038188410930948935, 0.0538084588160711, 0.06082424657643004, 0.03914242214484198, 0.07873098788749833, 0.1280191711671197, 0.11311878725947622, 0.12625932608897728, 0.08535519357748123, 0.09378270625489715, 0.12059452416952898, 0.10471029393977482, 0.12530781396926283, 0.021758851949409297, 0.01118865514291989, 0.01630852195620791, 0.009717848689135056, 0.005687385866965822, 0.007543861114835626, 0.01032720173348356, 0.01648145006013546, 0.005373827784677321, 0.07782528772435504, 0.07330077990565553, 0.06166601747141709, 0.08055000157538439, 0.07934930144482333, 0.062084647386244196, 0.10632178558194105, 0.07001529129837158, 0.10518737552427082, 0.35100325129559173, 0.37523474412177293, 0.37672805793904796, 0.36548934896301644, 0.3784362034739134, 0.29606697411473326, 0.38347943212156954, 0.36879532140862226, 0.4156318141657802, 0.06382786247443584, 0.07027068159229422, 0.06624128936599916, 0.05655454515347169, 0.05191204552758877, 0.05506194647616636, 0.05754263107754798, 0.044978851032902445, 0.06453824649042239, 0.17430559712214022, 0.14371516083960323, 0.13051352534288252, 0.1486892368088053, 0.14242895630840868, 0.11916859449762929, 0.1571485249749106, 0.14360241955760045, 0.14497734269037155, 0.20741927828451423, 0.19714385027183123, 0.17307556592616313, 0.21943769638530608, 0.17629886886715318, 0.1763960630323771, 0.17415519128369839, 0.14610377103098293, 0.13604240414071722, 0.15761735523911335, 0.17384670425875282, 0.12931732912620886, 0.15729233192226133, 0.15298296199388528, 0.15387607639362466, 0.13204630169711196, 0.1106239180094315, 0.10232685023347143, 0.18187502715462378, 0.15122193339174406, 0.1491135876626546, 0.17632443556245903, 0.1601654742865336, 0.15051735448101877, 0.19316221984484072, 0.1751426705706377, 0.186300441145674, 0.18717676149004225, 0.17272682736838418, 0.18311183184071178, 0.17713748478276103, 0.19446933698451196, 0.2035937959690537, 0.3837886709773012, 0.23595571520615266, 0.18984625380959352, 0.1372146125602216, 0.3375473553526086, 0.3751022808034383, 0.34124726872015887, 0.20497501473067492, 0.3041174875603694, 0.12875970971203898, 0.3946650494814252, 0.1387322605317557, 0.3577015084168791, 0.3406424644901558, 0.14168063322299607, 0.3218763830477227, 0.27719104026340924, 0.13076117137094367, 0.2911946769569581, 0.25316828466381325, 0.20684626425743513, 0.17951653794217437, 0.19543027414880643, 0.1898356726589886, 0.17357588525115408, 0.19616852667788487, 0.18477683563372382, 0.1910169861343758, 0.17449898452055512, 0.16900911271993313, 0.055545142414739845, 0.05029678957846151, 0.0451835029003681, 0.07009360399369924, 0.04843630757627959, 0.05027907031284684, 0.07167402092987196, 0.057166150127647364, 0.05062401906399627]}, "mutation_prompt": null}
{"id": "290d0384-cc1b-4f5c-a8d5-d6a760db20ca", "solution": "import numpy as np\nfrom scipy.optimize import differential_evolution\n\nclass Hybrid_PSODE_SA_DE_Optimizer:\n    def __init__(self, budget, dim, num_particles=30, max_iterations=1000, inertia_weight=0.7, cognitive_weight=1.4, social_weight=1.4, initial_temperature=100, cooling_rate=0.95):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = num_particles\n        self.max_iterations = max_iterations\n        self.inertia_weight = inertia_weight\n        self.cognitive_weight = cognitive_weight\n        self.social_weight = social_weight\n        self.initial_temperature = initial_temperature\n        self.cooling_rate = cooling_rate\n\n    def __call__(self, func):\n        def hybrid_pso_sa_de_helper():\n            # PSO initialization\n            self.inertia_weight = np.random.uniform(0.4, 0.9)\n            # PSO optimization loop\n            # Simulated Annealing (SA) initialization\n            current_solution = np.random.uniform(-5.0, 5.0, size=self.dim)\n            current_cost = func(current_solution)\n            best_solution = current_solution\n            best_cost = current_cost\n            # SA optimization loop\n            for _ in range(int(self.budget*0.5)):  # Spend 50% of the budget on SA\n                proposal_solution = current_solution + np.random.normal(0, 0.1, size=self.dim)\n                proposal_solution = np.clip(proposal_solution, -5.0, 5.0)\n                proposal_cost = func(proposal_solution)\n                if proposal_cost < current_cost or np.random.rand() < np.exp((current_cost - proposal_cost) / self.initial_temperature):\n                    current_solution = proposal_solution\n                    current_cost = proposal_cost\n                    if proposal_cost < best_cost:\n                        best_solution = proposal_solution\n                        best_cost = proposal_cost\n            # Differential Evolution (DE) optimization\n            de_result = differential_evolution(func, bounds=[(-5.0, 5.0)]*self.dim, maxiter=int(self.budget*0.5))  # Spend 50% of the budget on DE\n            if de_result.fun < best_cost:\n                best_solution = de_result.x\n            return best_solution\n\n        return hybrid_pso_sa_de_helper()\n\noptimizer = Hybrid_PSODE_SA_DE_Optimizer(budget=1000, dim=10)", "name": "Hybrid_PSODE_SA_DE_Optimizer", "description": "Combining Particle Swarm Optimization (PSO), Simulated Annealing (SA), and Differential Evolution (DE) for a hybrid metaheuristic approach to optimize black box functions efficiently within a given budget.", "configspace": "", "generation": 81, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "d82bc67e-0c68-42f5-b876-0815e26c45c3", "metadata": {"aucs": [0.48456837961059296, 0.5105538177594323, 0.47507057601350744, 0.47136751809518973, 0.467732558082386, 0.4996677345119218, 0.48477110876648977, 0.47038103501661366, 0.545176959239378, 0.18273261797874463, 0.19429757105452794, 0.21506122537384254, 0.19451909647024235, 0.12146371433165404, 0.24079244109696885, 0.27263541071937736, 0.13427678057795633, 0.1949133209598758, 0.06820291502883957, 0.06966638517654022, 0.12395381192411059, 0.08289695025266608, 0.08296512362937791, 0.0704014936562467, 0.161588407958278, 0.06945382781693155, 0.0722924237460717, 0.09232046452458575, 0.05330572800163913, 0.07118382690440017, 0.08917587563413543, 0.06361582817029554, 0.05932679852603273, 0.0635086490190615, 0.08265301708915862, 0.05814508982599631, 0.12721453922183246, 0.11706329861822595, 0.10630696636794656, 0.4174408512262425, 0.41873076422615274, 0.44278493340983627, 0.2123470216737544, 0.21871732615763728, 0.2200105933850952, 0.22008000655410365, 0.16538357465335463, 0.13387199369701974, 0.16636815324180299, 0.14716754841340518, 0.12746030363756877, 0.21459892081009235, 0.3376740437342389, 0.21157406285358538, 0.16947644277671503, 0.1644703645228397, 0.14361446048647464, 0.1907397339836573, 0.1695538471210719, 0.17152511706131024, 0.18809197229177876, 0.23240056220857597, 0.35739257552103, 0.30711331026701605, 0.35800000372394647, 0.27430480307612815, 0.10107003287669303, 0.38461052355102554, 0.40748602519626975, 0.3329604914039255, 0.3311750758329405, 0.3502616151401581, 0.3634412917063622, 0.3529535045867639, 0.34852116366747576, 0.2831203133435162, 0.24473953266556592, 0.16649180289750054, 0.29474586603190134, 0.1625909842331552, 0.31696283773167133, 0.14869086718791347, 0.07212689495988767, 0.06175901597576361, 0.020191522623446145, 0.03572120668890544, 0.038188410930948935, 0.0538084588160711, 0.06082424657643004, 0.03914242214484198, 0.07873098788749833, 0.1280191711671197, 0.11311878725947622, 0.12625932608897728, 0.08535519357748123, 0.09378270625489715, 0.12059452416952898, 0.10471029393977482, 0.12530781396926283, 0.021758851949409297, 0.01118865514291989, 0.01630852195620791, 0.009717848689135056, 0.005687385866965822, 0.007543861114835626, 0.01032720173348356, 0.01648145006013546, 0.005373827784677321, 0.07782528772435504, 0.07330077990565553, 0.06166601747141709, 0.08055000157538439, 0.07934930144482333, 0.062084647386244196, 0.10632178558194105, 0.07001529129837158, 0.10518737552427082, 0.35100325129559173, 0.37523474412177293, 0.37672805793904796, 0.36548934896301644, 0.3784362034739134, 0.29606697411473326, 0.38347943212156954, 0.36879532140862226, 0.4156318141657802, 0.06382786247443584, 0.07027068159229422, 0.06624128936599916, 0.05655454515347169, 0.05191204552758877, 0.05506194647616636, 0.05754263107754798, 0.044978851032902445, 0.06453824649042239, 0.17430559712214022, 0.14371516083960323, 0.13051352534288252, 0.1486892368088053, 0.14242895630840868, 0.11916859449762929, 0.1571485249749106, 0.14360241955760045, 0.14497734269037155, 0.20741927828451423, 0.19714385027183123, 0.17307556592616313, 0.21943769638530608, 0.17629886886715318, 0.1763960630323771, 0.17415519128369839, 0.14610377103098293, 0.13604240414071722, 0.15761735523911335, 0.17384670425875282, 0.12931732912620886, 0.15729233192226133, 0.15298296199388528, 0.15387607639362466, 0.13204630169711196, 0.1106239180094315, 0.10232685023347143, 0.18187502715462378, 0.15122193339174406, 0.1491135876626546, 0.17632443556245903, 0.1601654742865336, 0.15051735448101877, 0.19316221984484072, 0.1751426705706377, 0.186300441145674, 0.18717676149004225, 0.17272682736838418, 0.18311183184071178, 0.17713748478276103, 0.19446933698451196, 0.2035937959690537, 0.3837886709773012, 0.23595571520615266, 0.18984625380959352, 0.1372146125602216, 0.3375473553526086, 0.3751022808034383, 0.34124726872015887, 0.20497501473067492, 0.3041174875603694, 0.12875970971203898, 0.3946650494814252, 0.1387322605317557, 0.3577015084168791, 0.3406424644901558, 0.14168063322299607, 0.3218763830477227, 0.27719104026340924, 0.13076117137094367, 0.2911946769569581, 0.25316828466381325, 0.20684626425743513, 0.17951653794217437, 0.19543027414880643, 0.1898356726589886, 0.17357588525115408, 0.19616852667788487, 0.18477683563372382, 0.1910169861343758, 0.17449898452055512, 0.16900911271993313, 0.055545142414739845, 0.05029678957846151, 0.0451835029003681, 0.07009360399369924, 0.04843630757627959, 0.05027907031284684, 0.07167402092987196, 0.057166150127647364, 0.05062401906399627]}, "mutation_prompt": null}
{"id": "916fcba8-07b0-4f4e-a5b8-4ff42cd3f53d", "solution": "import numpy as np\nfrom scipy.optimize import differential_evolution\n\nclass Hybrid_PSODE_SA_DE_Optimizer:\n    def __init__(self, budget, dim, num_particles=30, max_iterations=1000, inertia_weight=0.7, cognitive_weight=1.4, social_weight=1.4, initial_temperature=100, cooling_rate=0.95):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = num_particles\n        self.max_iterations = max_iterations\n        self.inertia_weight = inertia_weight\n        self.cognitive_weight = cognitive_weight\n        self.social_weight = social_weight\n        self.initial_temperature = initial_temperature\n        self.cooling_rate = cooling_rate\n\n    def __call__(self, func):\n        def hybrid_pso_sa_de_helper():\n            # PSO initialization\n            self.inertia_weight = np.random.uniform(0.4, 0.9)\n            # PSO optimization loop\n            # Simulated Annealing (SA) initialization\n            current_solution = np.random.uniform(-5.0, 5.0, size=self.dim)\n            current_cost = func(current_solution)\n            best_solution = current_solution\n            best_cost = current_cost\n            # SA optimization loop\n            for _ in range(int(self.budget*0.5)):  # Spend 50% of the budget on SA\n                proposal_solution = current_solution + np.random.normal(0, 0.1, size=self.dim)\n                proposal_solution = np.clip(proposal_solution, -5.0, 5.0)\n                proposal_cost = func(proposal_solution)\n                if proposal_cost < current_cost or np.random.rand() < np.exp((current_cost - proposal_cost) / self.initial_temperature):\n                    current_solution = proposal_solution\n                    current_cost = proposal_cost\n                    if proposal_cost < best_cost:\n                        best_solution = proposal_solution\n                        best_cost = proposal_cost\n            # Differential Evolution (DE) optimization\n            de_result = differential_evolution(func, bounds=[(-5.0, 5.0)]*self.dim, maxiter=int(self.budget*0.5))  # Spend 50% of the budget on DE\n            if de_result.fun < best_cost:\n                best_solution = de_result.x\n            return best_solution\n\n        return hybrid_pso_sa_de_helper()\n\noptimizer = Hybrid_PSODE_SA_DE_Optimizer(budget=1000, dim=10)", "name": "Hybrid_PSODE_SA_DE_Optimizer", "description": "Combining Particle Swarm Optimization (PSO), Simulated Annealing (SA), and Differential Evolution (DE) for a hybrid metaheuristic approach to optimize black box functions efficiently within a given budget.", "configspace": "", "generation": 81, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "d82bc67e-0c68-42f5-b876-0815e26c45c3", "metadata": {"aucs": [0.48456837961059296, 0.5105538177594323, 0.47507057601350744, 0.47136751809518973, 0.467732558082386, 0.4996677345119218, 0.48477110876648977, 0.47038103501661366, 0.545176959239378, 0.18273261797874463, 0.19429757105452794, 0.21506122537384254, 0.19451909647024235, 0.12146371433165404, 0.24079244109696885, 0.27263541071937736, 0.13427678057795633, 0.1949133209598758, 0.06820291502883957, 0.06966638517654022, 0.12395381192411059, 0.08289695025266608, 0.08296512362937791, 0.0704014936562467, 0.161588407958278, 0.06945382781693155, 0.0722924237460717, 0.09232046452458575, 0.05330572800163913, 0.07118382690440017, 0.08917587563413543, 0.06361582817029554, 0.05932679852603273, 0.0635086490190615, 0.08265301708915862, 0.05814508982599631, 0.12721453922183246, 0.11706329861822595, 0.10630696636794656, 0.4174408512262425, 0.41873076422615274, 0.44278493340983627, 0.2123470216737544, 0.21871732615763728, 0.2200105933850952, 0.22008000655410365, 0.16538357465335463, 0.13387199369701974, 0.16636815324180299, 0.14716754841340518, 0.12746030363756877, 0.21459892081009235, 0.3376740437342389, 0.21157406285358538, 0.16947644277671503, 0.1644703645228397, 0.14361446048647464, 0.1907397339836573, 0.1695538471210719, 0.17152511706131024, 0.18809197229177876, 0.23240056220857597, 0.35739257552103, 0.30711331026701605, 0.35800000372394647, 0.27430480307612815, 0.10107003287669303, 0.38461052355102554, 0.40748602519626975, 0.3329604914039255, 0.3311750758329405, 0.3502616151401581, 0.3634412917063622, 0.3529535045867639, 0.34852116366747576, 0.2831203133435162, 0.24473953266556592, 0.16649180289750054, 0.29474586603190134, 0.1625909842331552, 0.31696283773167133, 0.14869086718791347, 0.07212689495988767, 0.06175901597576361, 0.020191522623446145, 0.03572120668890544, 0.038188410930948935, 0.0538084588160711, 0.06082424657643004, 0.03914242214484198, 0.07873098788749833, 0.1280191711671197, 0.11311878725947622, 0.12625932608897728, 0.08535519357748123, 0.09378270625489715, 0.12059452416952898, 0.10471029393977482, 0.12530781396926283, 0.021758851949409297, 0.01118865514291989, 0.01630852195620791, 0.009717848689135056, 0.005687385866965822, 0.007543861114835626, 0.01032720173348356, 0.01648145006013546, 0.005373827784677321, 0.07782528772435504, 0.07330077990565553, 0.06166601747141709, 0.08055000157538439, 0.07934930144482333, 0.062084647386244196, 0.10632178558194105, 0.07001529129837158, 0.10518737552427082, 0.35100325129559173, 0.37523474412177293, 0.37672805793904796, 0.36548934896301644, 0.3784362034739134, 0.29606697411473326, 0.38347943212156954, 0.36879532140862226, 0.4156318141657802, 0.06382786247443584, 0.07027068159229422, 0.06624128936599916, 0.05655454515347169, 0.05191204552758877, 0.05506194647616636, 0.05754263107754798, 0.044978851032902445, 0.06453824649042239, 0.17430559712214022, 0.14371516083960323, 0.13051352534288252, 0.1486892368088053, 0.14242895630840868, 0.11916859449762929, 0.1571485249749106, 0.14360241955760045, 0.14497734269037155, 0.20741927828451423, 0.19714385027183123, 0.17307556592616313, 0.21943769638530608, 0.17629886886715318, 0.1763960630323771, 0.17415519128369839, 0.14610377103098293, 0.13604240414071722, 0.15761735523911335, 0.17384670425875282, 0.12931732912620886, 0.15729233192226133, 0.15298296199388528, 0.15387607639362466, 0.13204630169711196, 0.1106239180094315, 0.10232685023347143, 0.18187502715462378, 0.15122193339174406, 0.1491135876626546, 0.17632443556245903, 0.1601654742865336, 0.15051735448101877, 0.19316221984484072, 0.1751426705706377, 0.186300441145674, 0.18717676149004225, 0.17272682736838418, 0.18311183184071178, 0.17713748478276103, 0.19446933698451196, 0.2035937959690537, 0.3837886709773012, 0.23595571520615266, 0.18984625380959352, 0.1372146125602216, 0.3375473553526086, 0.3751022808034383, 0.34124726872015887, 0.20497501473067492, 0.3041174875603694, 0.12875970971203898, 0.3946650494814252, 0.1387322605317557, 0.3577015084168791, 0.3406424644901558, 0.14168063322299607, 0.3218763830477227, 0.27719104026340924, 0.13076117137094367, 0.2911946769569581, 0.25316828466381325, 0.20684626425743513, 0.17951653794217437, 0.19543027414880643, 0.1898356726589886, 0.17357588525115408, 0.19616852667788487, 0.18477683563372382, 0.1910169861343758, 0.17449898452055512, 0.16900911271993313, 0.055545142414739845, 0.05029678957846151, 0.0451835029003681, 0.07009360399369924, 0.04843630757627959, 0.05027907031284684, 0.07167402092987196, 0.057166150127647364, 0.05062401906399627]}, "mutation_prompt": null}
{"id": "9e61e3aa-31d7-40ec-a7ca-49806788f877", "solution": "import numpy as np\nfrom scipy.optimize import differential_evolution\n\nclass Hybrid_PSODE_SA_DE_Optimizer:\n    def __init__(self, budget, dim, num_particles=30, max_iterations=1000, inertia_weight=0.7, cognitive_weight=1.4, social_weight=1.4, initial_temperature=100, cooling_rate=0.95):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = num_particles\n        self.max_iterations = max_iterations\n        self.inertia_weight = inertia_weight\n        self.cognitive_weight = cognitive_weight\n        self.social_weight = social_weight\n        self.initial_temperature = initial_temperature\n        self.cooling_rate = cooling_rate\n\n    def __call__(self, func):\n        def hybrid_pso_sa_de_helper():\n            # PSO initialization\n            self.inertia_weight = np.random.uniform(0.4, 0.9)\n            # PSO optimization loop\n            # Simulated Annealing (SA) initialization\n            current_solution = np.random.uniform(-5.0, 5.0, size=self.dim)\n            current_cost = func(current_solution)\n            best_solution = current_solution\n            best_cost = current_cost\n            # SA optimization loop\n            for _ in range(int(self.budget*0.5)):  # Spend 50% of the budget on SA\n                proposal_solution = current_solution + np.random.normal(0, 0.1, size=self.dim)\n                proposal_solution = np.clip(proposal_solution, -5.0, 5.0)\n                proposal_cost = func(proposal_solution)\n                if proposal_cost < current_cost or np.random.rand() < np.exp((current_cost - proposal_cost) / self.initial_temperature):\n                    current_solution = proposal_solution\n                    current_cost = proposal_cost\n                    if proposal_cost < best_cost:\n                        best_solution = proposal_solution\n                        best_cost = proposal_cost\n            # Differential Evolution (DE) optimization\n            de_result = differential_evolution(func, bounds=[(-5.0, 5.0)]*self.dim, maxiter=int(self.budget*0.5))  # Spend 50% of the budget on DE\n            if de_result.fun < best_cost:\n                best_solution = de_result.x\n            return best_solution\n\n        return hybrid_pso_sa_de_helper()\n\noptimizer = Hybrid_PSODE_SA_DE_Optimizer(budget=1000, dim=10)", "name": "Hybrid_PSODE_SA_DE_Optimizer", "description": "Combining Particle Swarm Optimization (PSO), Simulated Annealing (SA), and Differential Evolution (DE) for a hybrid metaheuristic approach to optimize black box functions efficiently within a given budget.", "configspace": "", "generation": 81, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "d82bc67e-0c68-42f5-b876-0815e26c45c3", "metadata": {"aucs": [0.48456837961059296, 0.5105538177594323, 0.47507057601350744, 0.47136751809518973, 0.467732558082386, 0.4996677345119218, 0.48477110876648977, 0.47038103501661366, 0.545176959239378, 0.18273261797874463, 0.19429757105452794, 0.21506122537384254, 0.19451909647024235, 0.12146371433165404, 0.24079244109696885, 0.27263541071937736, 0.13427678057795633, 0.1949133209598758, 0.06820291502883957, 0.06966638517654022, 0.12395381192411059, 0.08289695025266608, 0.08296512362937791, 0.0704014936562467, 0.161588407958278, 0.06945382781693155, 0.0722924237460717, 0.09232046452458575, 0.05330572800163913, 0.07118382690440017, 0.08917587563413543, 0.06361582817029554, 0.05932679852603273, 0.0635086490190615, 0.08265301708915862, 0.05814508982599631, 0.12721453922183246, 0.11706329861822595, 0.10630696636794656, 0.4174408512262425, 0.41873076422615274, 0.44278493340983627, 0.2123470216737544, 0.21871732615763728, 0.2200105933850952, 0.22008000655410365, 0.16538357465335463, 0.13387199369701974, 0.16636815324180299, 0.14716754841340518, 0.12746030363756877, 0.21459892081009235, 0.3376740437342389, 0.21157406285358538, 0.16947644277671503, 0.1644703645228397, 0.14361446048647464, 0.1907397339836573, 0.1695538471210719, 0.17152511706131024, 0.18809197229177876, 0.23240056220857597, 0.35739257552103, 0.30711331026701605, 0.35800000372394647, 0.27430480307612815, 0.10107003287669303, 0.38461052355102554, 0.40748602519626975, 0.3329604914039255, 0.3311750758329405, 0.3502616151401581, 0.3634412917063622, 0.3529535045867639, 0.34852116366747576, 0.2831203133435162, 0.24473953266556592, 0.16649180289750054, 0.29474586603190134, 0.1625909842331552, 0.31696283773167133, 0.14869086718791347, 0.07212689495988767, 0.06175901597576361, 0.020191522623446145, 0.03572120668890544, 0.038188410930948935, 0.0538084588160711, 0.06082424657643004, 0.03914242214484198, 0.07873098788749833, 0.1280191711671197, 0.11311878725947622, 0.12625932608897728, 0.08535519357748123, 0.09378270625489715, 0.12059452416952898, 0.10471029393977482, 0.12530781396926283, 0.021758851949409297, 0.01118865514291989, 0.01630852195620791, 0.009717848689135056, 0.005687385866965822, 0.007543861114835626, 0.01032720173348356, 0.01648145006013546, 0.005373827784677321, 0.07782528772435504, 0.07330077990565553, 0.06166601747141709, 0.08055000157538439, 0.07934930144482333, 0.062084647386244196, 0.10632178558194105, 0.07001529129837158, 0.10518737552427082, 0.35100325129559173, 0.37523474412177293, 0.37672805793904796, 0.36548934896301644, 0.3784362034739134, 0.29606697411473326, 0.38347943212156954, 0.36879532140862226, 0.4156318141657802, 0.06382786247443584, 0.07027068159229422, 0.06624128936599916, 0.05655454515347169, 0.05191204552758877, 0.05506194647616636, 0.05754263107754798, 0.044978851032902445, 0.06453824649042239, 0.17430559712214022, 0.14371516083960323, 0.13051352534288252, 0.1486892368088053, 0.14242895630840868, 0.11916859449762929, 0.1571485249749106, 0.14360241955760045, 0.14497734269037155, 0.20741927828451423, 0.19714385027183123, 0.17307556592616313, 0.21943769638530608, 0.17629886886715318, 0.1763960630323771, 0.17415519128369839, 0.14610377103098293, 0.13604240414071722, 0.15761735523911335, 0.17384670425875282, 0.12931732912620886, 0.15729233192226133, 0.15298296199388528, 0.15387607639362466, 0.13204630169711196, 0.1106239180094315, 0.10232685023347143, 0.18187502715462378, 0.15122193339174406, 0.1491135876626546, 0.17632443556245903, 0.1601654742865336, 0.15051735448101877, 0.19316221984484072, 0.1751426705706377, 0.186300441145674, 0.18717676149004225, 0.17272682736838418, 0.18311183184071178, 0.17713748478276103, 0.19446933698451196, 0.2035937959690537, 0.3837886709773012, 0.23595571520615266, 0.18984625380959352, 0.1372146125602216, 0.3375473553526086, 0.3751022808034383, 0.34124726872015887, 0.20497501473067492, 0.3041174875603694, 0.12875970971203898, 0.3946650494814252, 0.1387322605317557, 0.3577015084168791, 0.3406424644901558, 0.14168063322299607, 0.3218763830477227, 0.27719104026340924, 0.13076117137094367, 0.2911946769569581, 0.25316828466381325, 0.20684626425743513, 0.17951653794217437, 0.19543027414880643, 0.1898356726589886, 0.17357588525115408, 0.19616852667788487, 0.18477683563372382, 0.1910169861343758, 0.17449898452055512, 0.16900911271993313, 0.055545142414739845, 0.05029678957846151, 0.0451835029003681, 0.07009360399369924, 0.04843630757627959, 0.05027907031284684, 0.07167402092987196, 0.057166150127647364, 0.05062401906399627]}, "mutation_prompt": null}
{"id": "f3d338db-f18c-4664-b680-540ccdd90353", "solution": "import numpy as np\nfrom scipy.optimize import differential_evolution\n\nclass Novel_Metaheuristic_Optimizer:\n    def __init__(self, budget, dim, num_particles=30, max_iterations=1000, inertia_weight=0.7, cognitive_weight=1.4, social_weight=1.4, initial_temperature=100, cooling_rate=0.95):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = num_particles\n        self.max_iterations = max_iterations\n        self.inertia_weight = inertia_weight\n        self.cognitive_weight = cognitive_weight\n        self.social_weight = social_weight\n        self.initial_temperature = initial_temperature\n        self.cooling_rate = cooling_rate\n\n    def __call__(self, func):\n        def novel_metaheuristic_helper():\n            # PSO initialization\n            self.inertia_weight = np.random.uniform(0.4, 0.9)\n            # PSO optimization loop\n            # Simulated Annealing (SA) initialization\n            current_solution = np.random.uniform(-5.0, 5.0, size=self.dim)\n            current_cost = func(current_solution)\n            best_solution = current_solution\n            best_cost = current_cost\n            # SA optimization loop\n            for _ in range(int(self.budget*0.5)):  # Spend 50% of the budget on SA\n                proposal_solution = current_solution + np.random.normal(0, 0.1, size=self.dim)\n                proposal_solution = np.clip(proposal_solution, -5.0, 5.0)\n                proposal_cost = func(proposal_solution)\n                if proposal_cost < current_cost or np.random.rand() < np.exp((current_cost - proposal_cost) / self.initial_temperature):\n                    current_solution = proposal_solution\n                    current_cost = proposal_cost\n                    if proposal_cost < best_cost:\n                        best_solution = proposal_solution\n                        best_cost = proposal_cost\n            # Differential Evolution (DE) optimization\n            de_result = differential_evolution(func, bounds=[(-5.0, 5.0)]*self.dim, maxiter=int(self.budget*0.5))  # Spend 50% of the budget on DE\n            if de_result.fun < best_cost:\n                best_solution = de_result.x\n            return best_solution\n\n        return novel_metaheuristic_helper()\n\noptimizer = Novel_Metaheuristic_Optimizer(budget=1000, dim=10)", "name": "Novel_Metaheuristic_Optimizer", "description": "A novel metaheuristic algorithm that combines Particle Swarm Optimization (PSO), Simulated Annealing (SA), and Differential Evolution (DE) in a dynamic and adaptive manner to optimize black box functions efficiently within a given budget, with line-by-line refinement based on a probability of 0.15.", "configspace": "", "generation": 99, "fitness": 0.1824295991437497, "feedback": "The algorithm Novel_Metaheuristic_Optimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.18 with standard deviation 0.12.", "error": "", "parent_id": "d82bc67e-0c68-42f5-b876-0815e26c45c3", "metadata": {"aucs": [0.48456837961059296, 0.5105538177594323, 0.47507057601350744, 0.47136751809518973, 0.467732558082386, 0.4996677345119218, 0.48477110876648977, 0.47038103501661366, 0.545176959239378, 0.18273261797874463, 0.19429757105452794, 0.21506122537384254, 0.19451909647024235, 0.12146371433165404, 0.24079244109696885, 0.27263541071937736, 0.13427678057795633, 0.1949133209598758, 0.06820291502883957, 0.06966638517654022, 0.12395381192411059, 0.08289695025266608, 0.08296512362937791, 0.0704014936562467, 0.161588407958278, 0.06945382781693155, 0.0722924237460717, 0.09232046452458575, 0.05330572800163913, 0.07118382690440017, 0.08917587563413543, 0.06361582817029554, 0.05932679852603273, 0.0635086490190615, 0.08265301708915862, 0.05814508982599631, 0.12721453922183246, 0.11706329861822595, 0.10630696636794656, 0.4174408512262425, 0.41873076422615274, 0.44278493340983627, 0.2123470216737544, 0.21871732615763728, 0.2200105933850952, 0.22008000655410365, 0.16538357465335463, 0.13387199369701974, 0.16636815324180299, 0.14716754841340518, 0.12746030363756877, 0.21459892081009235, 0.3376740437342389, 0.21157406285358538, 0.16947644277671503, 0.1644703645228397, 0.14361446048647464, 0.1907397339836573, 0.1695538471210719, 0.17152511706131024, 0.18809197229177876, 0.23240056220857597, 0.35739257552103, 0.30711331026701605, 0.35800000372394647, 0.27430480307612815, 0.10107003287669303, 0.38461052355102554, 0.40748602519626975, 0.3329604914039255, 0.3311750758329405, 0.3502616151401581, 0.3634412917063622, 0.3529535045867639, 0.34852116366747576, 0.2831203133435162, 0.24473953266556592, 0.16649180289750054, 0.29474586603190134, 0.1625909842331552, 0.31696283773167133, 0.14869086718791347, 0.07212689495988767, 0.06175901597576361, 0.020191522623446145, 0.03572120668890544, 0.038188410930948935, 0.0538084588160711, 0.06082424657643004, 0.03914242214484198, 0.07873098788749833, 0.1280191711671197, 0.11311878725947622, 0.12625932608897728, 0.08535519357748123, 0.09378270625489715, 0.12059452416952898, 0.10471029393977482, 0.12530781396926283, 0.021758851949409297, 0.01118865514291989, 0.01630852195620791, 0.009717848689135056, 0.005687385866965822, 0.007543861114835626, 0.01032720173348356, 0.01648145006013546, 0.005373827784677321, 0.07782528772435504, 0.07330077990565553, 0.06166601747141709, 0.08055000157538439, 0.07934930144482333, 0.062084647386244196, 0.10632178558194105, 0.07001529129837158, 0.10518737552427082, 0.35100325129559173, 0.37523474412177293, 0.37672805793904796, 0.36548934896301644, 0.3784362034739134, 0.29606697411473326, 0.38347943212156954, 0.36879532140862226, 0.4156318141657802, 0.06382786247443584, 0.07027068159229422, 0.06624128936599916, 0.05655454515347169, 0.05191204552758877, 0.05506194647616636, 0.05754263107754798, 0.044978851032902445, 0.06453824649042239, 0.17430559712214022, 0.14371516083960323, 0.13051352534288252, 0.1486892368088053, 0.14242895630840868, 0.11916859449762929, 0.1571485249749106, 0.14360241955760045, 0.14497734269037155, 0.20741927828451423, 0.19714385027183123, 0.17307556592616313, 0.21943769638530608, 0.17629886886715318, 0.1763960630323771, 0.17415519128369839, 0.14610377103098293, 0.13604240414071722, 0.15761735523911335, 0.17384670425875282, 0.12931732912620886, 0.15729233192226133, 0.15298296199388528, 0.15387607639362466, 0.13204630169711196, 0.1106239180094315, 0.10232685023347143, 0.18187502715462378, 0.15122193339174406, 0.1491135876626546, 0.17632443556245903, 0.1601654742865336, 0.15051735448101877, 0.19316221984484072, 0.1751426705706377, 0.186300441145674, 0.18717676149004225, 0.17272682736838418, 0.18311183184071178, 0.17713748478276103, 0.19446933698451196, 0.2035937959690537, 0.3837886709773012, 0.23595571520615266, 0.18984625380959352, 0.1372146125602216, 0.3375473553526086, 0.3751022808034383, 0.34124726872015887, 0.20497501473067492, 0.3041174875603694, 0.12875970971203898, 0.3946650494814252, 0.1387322605317557, 0.3577015084168791, 0.3406424644901558, 0.14168063322299607, 0.3218763830477227, 0.27719104026340924, 0.13076117137094367, 0.2911946769569581, 0.25316828466381325, 0.20684626425743513, 0.17951653794217437, 0.19543027414880643, 0.1898356726589886, 0.17357588525115408, 0.19616852667788487, 0.18477683563372382, 0.1910169861343758, 0.17449898452055512, 0.16900911271993313, 0.055545142414739845, 0.05029678957846151, 0.0451835029003681, 0.07009360399369924, 0.04843630757627959, 0.05027907031284684, 0.07167402092987196, 0.057166150127647364, 0.05062401906399627]}, "mutation_prompt": null}
