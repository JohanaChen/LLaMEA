{"id": "bb9cbb65-5c4d-45a7-b81d-3ee61b89267c", "solution": "import numpy as np\nimport random\n\nclass ADDE_SAFM:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.F = 0.5  # initial frequency\n        self.CR = 0.5  # initial crossover rate\n        self.pop_size = 50  # initial population size\n        self.population = np.random.uniform(-5.0, 5.0, (self.pop_size, self.dim))\n        self.best_solution = np.inf\n\n    def evaluate(self, x):\n        return self.func(x)\n\n    def func(self, x):\n        # BBOB test suite of 24 noiseless functions\n        # Here we use the first function f1(x) = sum(x_i^2)\n        return np.sum(x**2)\n\n    def frequency_modulation(self, x):\n        # frequency modulation function\n        return self.F * np.sin(2 * np.pi * self.F * x)\n\n    def adaptive_frequency_modulation(self):\n        # adaptive frequency modulation\n        self.F *= 1.1  # increase frequency\n        if self.evaluate(self.population) > self.evaluate(self.population * self.frequency_modulation()):\n            self.F *= 0.9  # decrease frequency\n        return self.F\n\n    def crossover(self, x1, x2):\n        # crossover operation\n        r = np.random.rand(self.dim)\n        return x1 + r * (x2 - x1)\n\n    def selection(self, x1, x2):\n        # selection operation\n        if self.evaluate(x2) < self.evaluate(x1):\n            return x2\n        else:\n            return x1\n\n    def optimize(self, func):\n        for _ in range(self.budget):\n            # evaluate population\n            fitness = [self.evaluate(x) for x in self.population]\n            # get best solution\n            self.best_solution = min(fitness)\n            best_index = fitness.index(self.best_solution)\n            # adaptive frequency modulation\n            self.F = self.adaptive_frequency_modulation()\n            # differential evolution\n            for i in range(self.pop_size):\n                # generate trial vector\n                trial = self.crossover(self.population[i], self.population[random.randint(0, self.pop_size - 1)])\n                # evaluate trial vector\n                trial_fitness = self.evaluate(trial)\n                # selection\n                self.population[i] = self.selection(self.population[i], trial)\n                # update best solution\n                if trial_fitness < self.evaluate(self.population[i]):\n                    self.population[i] = trial\n        return self.best_solution\n\n    def __call__(self, func):\n        self.func = func\n        return self.optimize(func)\n\n# example usage\nbudget = 1000\ndim = 10\noptimizer = ADDE_SAFM(budget, dim)\nbest_solution = optimizer(lambda x: np.sum(x**2))\nprint(\"Best solution:\", best_solution)", "name": "ADDE_SAFM", "description": "Adaptive Discrete Differential Evolution with Self-Adaptive Frequency Modulation (ADDE-SAFM) - a novel hybridization of differential evolution and frequency modulation for adaptive exploration-exploitation trade-off.", "configspace": "", "generation": 0, "fitness": -Infinity, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/gpfs/scratch1/shared/hyin/LLaMEA/llamea/llamea.py\", line 180, in initialize_single\n    new_individual = self.evaluate_fitness(new_individual)\n  File \"/gpfs/scratch1/shared/hyin/LLaMEA/llamea/llamea.py\", line 257, in evaluate_fitness\n    updated_individual = self.f(individual, self.logger)\n  File \"/gpfs/scratch1/shared/hyin/LLaMEA/mutation_exp.py\", line 24, in evaluateBBOB\n    exec(code, globals())\n  File \"<string>\", line 75, in <module>\n  File \"<string>\", line 69, in __call__\n  File \"<string>\", line 53, in optimize\n  File \"<string>\", line 29, in adaptive_frequency_modulation\nTypeError: frequency_modulation() missing 1 required positional argument: 'x'\n.", "error": "TypeError(\"frequency_modulation() missing 1 required positional argument: 'x'\")Traceback (most recent call last):\n  File \"/gpfs/scratch1/shared/hyin/LLaMEA/llamea/llamea.py\", line 180, in initialize_single\n    new_individual = self.evaluate_fitness(new_individual)\n  File \"/gpfs/scratch1/shared/hyin/LLaMEA/llamea/llamea.py\", line 257, in evaluate_fitness\n    updated_individual = self.f(individual, self.logger)\n  File \"/gpfs/scratch1/shared/hyin/LLaMEA/mutation_exp.py\", line 24, in evaluateBBOB\n    exec(code, globals())\n  File \"<string>\", line 75, in <module>\n  File \"<string>\", line 69, in __call__\n  File \"<string>\", line 53, in optimize\n  File \"<string>\", line 29, in adaptive_frequency_modulation\nTypeError: frequency_modulation() missing 1 required positional argument: 'x'\n", "parent_id": null, "metadata": {}, "mutation_prompt": null}
{"id": "73a78172-2559-47ff-83b5-d9cc8b477369", "solution": "import numpy as np\nimport random\n\nclass ADDE_SAFM:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.F = 0.5  # initial frequency\n        self.CR = 0.5  # initial crossover rate\n        self.pop_size = 50  # initial population size\n        self.population = np.random.uniform(-5.0, 5.0, (self.pop_size, self.dim))\n        self.best_solution = np.inf\n        self.fm = lambda x: self.F * np.sin(2 * np.pi * self.F * x)\n\n    def evaluate(self, x):\n        return self.func(x)\n\n    def func(self, x):\n        # BBOB test suite of 24 noiseless functions\n        # Here we use the first function f1(x) = sum(x_i^2)\n        return np.sum(x**2)\n\n    def adaptive_frequency_modulation(self):\n        # adaptive frequency modulation\n        self.F *= 1.1  # increase frequency\n        if self.evaluate(self.population) > self.evaluate(self.population * self.fm()):\n            self.F *= 0.9  # decrease frequency\n        return self.F\n\n    def crossover(self, x1, x2):\n        # crossover operation\n        r = np.random.rand(self.dim)\n        return x1 + r * (x2 - x1)\n\n    def selection(self, x1, x2):\n        # selection operation\n        if self.evaluate(x2) < self.evaluate(x1):\n            return x2\n        else:\n            return x1\n\n    def optimize(self, func):\n        self.fm = lambda x: self.F * np.sin(2 * np.pi * self.F * x)\n        for _ in range(self.budget):\n            # evaluate population\n            fitness = [self.evaluate(x) for x in self.population]\n            # get best solution\n            self.best_solution = min(fitness)\n            best_index = fitness.index(self.best_solution)\n            # adaptive frequency modulation\n            self.F = self.adaptive_frequency_modulation()\n            # differential evolution\n            for i in range(self.pop_size):\n                # generate trial vector\n                trial = self.crossover(self.population[i], self.population[random.randint(0, self.pop_size - 1)])\n                # evaluate trial vector\n                trial_fitness = self.evaluate(trial)\n                # selection\n                self.population[i] = self.selection(self.population[i], trial)\n                # update best solution\n                if trial_fitness < self.evaluate(self.population[i]):\n                    self.population[i] = trial\n        return self.best_solution\n\n    def __call__(self, func):\n        self.func = func\n        return self.optimize(func)\n\n# example usage\nbudget = 1000\ndim = 10\noptimizer = ADDE_SAFM(budget, dim)\nbest_solution = optimizer(lambda x: np.sum(x**2))\nprint(\"Best solution:\", best_solution)", "name": "ADDE_SAFM", "description": "A novel hybridization of differential evolution and frequency modulation with adaptive exploration-exploitation trade-off, self-adaptive frequency modulation, and crossover selection.", "configspace": "", "generation": 1, "fitness": -Infinity, "feedback": "An exception occurred: TypeError(\"<lambda>() missing 1 required positional argument: 'x'\").", "error": "TypeError(\"<lambda>() missing 1 required positional argument: 'x'\")", "parent_id": "bb9cbb65-5c4d-45a7-b81d-3ee61b89267c", "metadata": {}, "mutation_prompt": null}
{"id": "63a9936a-7861-4646-a979-3d63a9ac1fdf", "solution": "import numpy as np\nimport random\n\nclass AFMLF:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.F = 0.5  # initial frequency\n        self.CR = 0.5  # initial crossover rate\n        self.pop_size = 50  # initial population size\n        self.population = np.random.uniform(-5.0, 5.0, (self.pop_size, self.dim))\n        self.best_solution = np.inf\n\n    def evaluate(self, x):\n        return self.func(x)\n\n    def func(self, x):\n        # BBOB test suite of 24 noiseless functions\n        # Here we use the first function f1(x) = sum(x_i^2)\n        return np.sum(x**2)\n\n    def frequency_modulation(self, x):\n        # frequency modulation function\n        return self.F * np.sin(2 * np.pi * self.F * x)\n\n    def adaptive_frequency_modulation(self):\n        # adaptive frequency modulation\n        self.F *= 1.1  # increase frequency\n        if self.evaluate(self.population) > self.evaluate(self.population * self.frequency_modulation()):\n            self.F *= 0.9  # decrease frequency\n        return self.F\n\n    def levy_flight(self, x):\n        # Levy flight function\n        sigma = 1\n        step_size = np.random.normal(0, sigma)\n        return x + step_size * np.random.uniform(-1, 1, self.dim)\n\n    def crossover(self, x1, x2):\n        # crossover operation\n        r = np.random.rand(self.dim)\n        return x1 + r * (x2 - x1)\n\n    def selection(self, x1, x2):\n        # selection operation\n        if self.evaluate(x2) < self.evaluate(x1):\n            return x2\n        else:\n            return x1\n\n    def optimize(self, func):\n        for _ in range(self.budget):\n            # evaluate population\n            fitness = [self.evaluate(x) for x in self.population]\n            # get best solution\n            self.best_solution = min(fitness)\n            best_index = fitness.index(self.best_solution)\n            # adaptive frequency modulation\n            self.F = self.adaptive_frequency_modulation()\n            # Levy flights\n            for i in range(self.pop_size):\n                # generate trial vector using Levy flight\n                trial = self.population[i] + self.levy_flight(self.population[i])\n                # evaluate trial vector\n                trial_fitness = self.evaluate(trial)\n                # selection\n                self.population[i] = self.selection(self.population[i], trial)\n                # update best solution\n                if trial_fitness < self.evaluate(self.population[i]):\n                    self.population[i] = trial\n        return self.best_solution\n\n    def __call__(self, func):\n        self.func = func\n        return self.optimize(func)\n\n# example usage\nbudget = 1000\ndim = 10\noptimizer = AFMLF(budget, dim)\nbest_solution = optimizer(lambda x: np.sum(x**2))\nprint(\"Best solution:\", best_solution)", "name": "AFMLF", "description": "A novel metaheuristic algorithm called \"Adaptive Frequency Modulation with Levy Flights\" (AFMLF) that combines frequency modulation with Levy flights for efficient exploration and exploitation of the search space.", "configspace": "", "generation": 2, "fitness": -Infinity, "feedback": "An exception occurred: TypeError(\"frequency_modulation() missing 1 required positional argument: 'x'\").", "error": "TypeError(\"frequency_modulation() missing 1 required positional argument: 'x'\")", "parent_id": "bb9cbb65-5c4d-45a7-b81d-3ee61b89267c", "metadata": {}, "mutation_prompt": null}
{"id": "90f488a0-7445-4ee6-a7a1-db53017678ae", "solution": "import numpy as np\nimport random\n\nclass ADDE_SAFM_PBAR:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.F = 0.5  # initial frequency\n        self.CR = 0.5  # initial crossover rate\n        self.pop_size = 50  # initial population size\n        self.population = np.random.uniform(-5.0, 5.0, (self.pop_size, self.dim))\n        self.best_solution = np.inf\n        self.probability = 0.5  # initial probability for adaptive crossover rate\n\n    def evaluate(self, x):\n        return self.func(x)\n\n    def func(self, x):\n        # BBOB test suite of 24 noiseless functions\n        # Here we use the first function f1(x) = sum(x_i^2)\n        return np.sum(x**2)\n\n    def frequency_modulation(self, x):\n        # frequency modulation function\n        return self.F * np.sin(2 * np.pi * self.F * x)\n\n    def adaptive_frequency_modulation(self):\n        # adaptive frequency modulation\n        self.F *= 1.1  # increase frequency\n        if self.evaluate(self.population) > self.evaluate(self.population * self.frequency_modulation()):\n            self.F *= 0.9  # decrease frequency\n        return self.F\n\n    def adaptive_crossover_rate(self):\n        # adaptive crossover rate\n        if np.random.rand() < self.probability:\n            self.CR *= 1.1  # increase crossover rate\n        else:\n            self.CR *= 0.9  # decrease crossover rate\n        return self.CR\n\n    def crossover(self, x1, x2):\n        # crossover operation\n        r = np.random.rand(self.dim)\n        if np.random.rand() < self.CR:\n            return x1 + r * (x2 - x1)\n        else:\n            return x1\n\n    def selection(self, x1, x2):\n        # selection operation\n        if self.evaluate(x2) < self.evaluate(x1):\n            return x2\n        else:\n            return x1\n\n    def optimize(self, func):\n        for _ in range(self.budget):\n            # evaluate population\n            fitness = [self.evaluate(x) for x in self.population]\n            # get best solution\n            self.best_solution = min(fitness)\n            best_index = fitness.index(self.best_solution)\n            # adaptive frequency modulation\n            self.F = self.adaptive_frequency_modulation()\n            # adaptive crossover rate\n            self.CR = self.adaptive_crossover_rate()\n            # differential evolution\n            for i in range(self.pop_size):\n                # generate trial vector\n                trial = self.crossover(self.population[i], self.population[random.randint(0, self.pop_size - 1)])\n                # evaluate trial vector\n                trial_fitness = self.evaluate(trial)\n                # selection\n                self.population[i] = self.selection(self.population[i], trial)\n                # update best solution\n                if trial_fitness < self.evaluate(self.population[i]):\n                    self.population[i] = trial\n        return self.best_solution\n\n    def __call__(self, func):\n        self.func = func\n        return self.optimize(func)\n\n# example usage\nbudget = 1000\ndim = 10\noptimizer = ADDE_SAFM_PBAR(budget, dim)\nbest_solution = optimizer(lambda x: np.sum(x**2))\nprint(\"Best solution:\", best_solution)", "name": "ADDE_SAFM_PBAR", "description": "Adaptive Discrete Differential Evolution with Self-Adaptive Frequency Modulation and Probability-Based Adaptive Crossover Rate (ADDE_SAFM-PBAR)", "configspace": "", "generation": 3, "fitness": -Infinity, "feedback": "An exception occurred: TypeError(\"frequency_modulation() missing 1 required positional argument: 'x'\").", "error": "TypeError(\"frequency_modulation() missing 1 required positional argument: 'x'\")", "parent_id": "bb9cbb65-5c4d-45a7-b81d-3ee61b89267c", "metadata": {}, "mutation_prompt": null}
{"id": "ed37a148-43db-46ec-b4f2-954dd885c849", "solution": "import numpy as np\nimport random\n\nclass ADDE_SAFM_COPA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.F = 0.5  # initial frequency\n        self.CR = 0.5  # initial crossover rate\n        self.pop_size = 50  # initial population size\n        self.population = np.random.uniform(-5.0, 5.0, (self.pop_size, self.dim))\n        self.best_solution = np.inf\n        self.CR_adaptation_rate = 0.01  # rate of CR adaptation\n\n    def evaluate(self, x):\n        return self.func(x)\n\n    def func(self, x):\n        # BBOB test suite of 24 noiseless functions\n        # Here we use the first function f1(x) = sum(x_i^2)\n        return np.sum(x**2)\n\n    def frequency_modulation(self, x):\n        # frequency modulation function\n        return self.F * np.sin(2 * np.pi * self.F * x)\n\n    def adaptive_frequency_modulation(self):\n        # adaptive frequency modulation\n        self.F *= 1.1  # increase frequency\n        if self.evaluate(self.population) > self.evaluate(self.population * self.frequency_modulation()):\n            self.F *= 0.9  # decrease frequency\n        return self.F\n\n    def crossover(self, x1, x2):\n        # crossover operation\n        r = np.random.rand(self.dim)\n        return x1 + r * (x2 - x1)\n\n    def selection(self, x1, x2):\n        # selection operation\n        if self.evaluate(x2) < self.evaluate(x1):\n            return x2\n        else:\n            return x1\n\n    def CR_adaptation(self):\n        # adaptive crossover probability\n        self.CR += self.CR_adaptation_rate\n        if self.CR > 1:\n            self.CR = 1\n        elif self.CR < 0:\n            self.CR = 0\n\n    def optimize(self, func):\n        for _ in range(self.budget):\n            # evaluate population\n            fitness = [self.evaluate(x) for x in self.population]\n            # get best solution\n            self.best_solution = min(fitness)\n            best_index = fitness.index(self.best_solution)\n            # adaptive frequency modulation\n            self.F = self.adaptive_frequency_modulation()\n            # adaptive crossover probability\n            self.CR_adaptation()\n            # differential evolution\n            for i in range(self.pop_size):\n                # generate trial vector\n                trial = self.crossover(self.population[i], self.population[random.randint(0, self.pop_size - 1)])\n                # evaluate trial vector\n                trial_fitness = self.evaluate(trial)\n                # selection\n                self.population[i] = self.selection(self.population[i], trial)\n                # update best solution\n                if trial_fitness < self.evaluate(self.population[i]):\n                    self.population[i] = trial\n        return self.best_solution\n\n    def __call__(self, func):\n        self.func = func\n        return self.optimize(func)\n\n# example usage\nbudget = 1000\ndim = 10\noptimizer = ADDE_SAFM_COPA(budget, dim)\nbest_solution = optimizer(lambda x: np.sum(x**2))\nprint(\"Best solution:\", best_solution)", "name": "ADDE_SAFM_COPA", "description": "Adaptive Discrete Differential Evolution with Self-Adaptive Frequency Modulation and Crossover Probability Adaptation (ADDE_SAFM-COPA) - a novel hybridization of differential evolution and frequency modulation with adaptive crossover probability for enhanced exploration-exploitation trade-off.", "configspace": "", "generation": 4, "fitness": -Infinity, "feedback": "An exception occurred: TypeError(\"frequency_modulation() missing 1 required positional argument: 'x'\").", "error": "TypeError(\"frequency_modulation() missing 1 required positional argument: 'x'\")", "parent_id": "bb9cbb65-5c4d-45a7-b81d-3ee61b89267c", "metadata": {}, "mutation_prompt": null}
{"id": "eea91d5b-dadc-4d9c-9068-ec446ecc9837", "solution": "import numpy as np\nimport random\n\nclass ADDE_SAFM:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.F = 0.5  # initial frequency\n        self.CR = 0.5  # initial crossover rate\n        self.pop_size = 50  # initial population size\n        self.population = np.random.uniform(-5.0, 5.0, (self.pop_size, self.dim))\n        self.best_solution = np.inf\n        self.p = 0.17  # probability of mutation\n\n    def evaluate(self, x):\n        return self.func(x)\n\n    def func(self, x):\n        # BBOB test suite of 24 noiseless functions\n        # Here we use the first function f1(x) = sum(x_i^2)\n        return np.sum(x**2)\n\n    def frequency_modulation(self, x):\n        # frequency modulation function\n        return self.F * np.sin(2 * np.pi * self.F * x)\n\n    def adaptive_frequency_modulation(self):\n        # adaptive frequency modulation\n        self.F *= 1.1  # increase frequency\n        if self.evaluate(self.population) > self.evaluate(self.population * self.frequency_modulation()):\n            self.F *= 0.9  # decrease frequency\n        return self.F\n\n    def crossover(self, x1, x2):\n        # crossover operation\n        r = np.random.rand(self.dim)\n        return x1 + r * (x2 - x1)\n\n    def selection(self, x1, x2):\n        # selection operation\n        if self.evaluate(x2) < self.evaluate(x1):\n            return x2\n        else:\n            return x1\n\n    def mutation(self, x):\n        # probability-based mutation\n        if np.random.rand() < self.p:\n            return x + np.random.uniform(-1, 1, self.dim)\n        else:\n            return x\n\n    def optimize(self, func):\n        for _ in range(self.budget):\n            # evaluate population\n            fitness = [self.evaluate(x) for x in self.population]\n            # get best solution\n            self.best_solution = min(fitness)\n            best_index = fitness.index(self.best_solution)\n            # adaptive frequency modulation\n            self.F = self.adaptive_frequency_modulation()\n            # differential evolution\n            for i in range(self.pop_size):\n                # generate trial vector\n                trial = self.crossover(self.population[i], self.population[random.randint(0, self.pop_size - 1)])\n                # apply mutation\n                trial = self.mutation(trial)\n                # evaluate trial vector\n                trial_fitness = self.evaluate(trial)\n                # selection\n                self.population[i] = self.selection(self.population[i], trial)\n                # update best solution\n                if trial_fitness < self.evaluate(self.population[i]):\n                    self.population[i] = trial\n        return self.best_solution\n\n    def __call__(self, func):\n        self.func = func\n        return self.optimize(func)\n\n# example usage\nbudget = 1000\ndim = 10\noptimizer = ADDE_SAFM(budget, dim)\nbest_solution = optimizer(lambda x: np.sum(x**2))\nprint(\"Best solution:\", best_solution)", "name": "ADDE_SAFM", "description": "Novel hybridization of differential evolution and frequency modulation with adaptive exploration-exploitation trade-off and probability-based mutation strategy.", "configspace": "", "generation": 5, "fitness": -Infinity, "feedback": "An exception occurred: TypeError(\"frequency_modulation() missing 1 required positional argument: 'x'\").", "error": "TypeError(\"frequency_modulation() missing 1 required positional argument: 'x'\")", "parent_id": "bb9cbb65-5c4d-45a7-b81d-3ee61b89267c", "metadata": {}, "mutation_prompt": null}
{"id": "f4cb4cb9-b9ba-458e-893a-5ba935bba448", "solution": "import numpy as np\nimport random\n\nclass ADDE_SAFM:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.F = 0.5  # initial frequency\n        self.CR = 0.5  # initial crossover rate\n        self.pop_size = 50  # initial population size\n        self.population = np.random.uniform(-5.0, 5.0, (self.pop_size, self.dim))\n        self.best_solution = np.inf\n        self.probability = 0.1  # probability of probabilistic exploration\n\n    def evaluate(self, x):\n        return self.func(x)\n\n    def func(self, x):\n        # BBOB test suite of 24 noiseless functions\n        # Here we use the first function f1(x) = sum(x_i^2)\n        return np.sum(x**2)\n\n    def frequency_modulation(self, x):\n        # frequency modulation function\n        return self.F * np.sin(2 * np.pi * self.F * x)\n\n    def adaptive_frequency_modulation(self):\n        # adaptive frequency modulation\n        self.F *= 1.1  # increase frequency\n        if random.random() < self.probability:\n            self.F *= 0.9  # decrease frequency with probability\n        return self.F\n\n    def crossover(self, x1, x2):\n        # crossover operation\n        r = np.random.rand(self.dim)\n        return x1 + r * (x2 - x1)\n\n    def selection(self, x1, x2):\n        # selection operation\n        if self.evaluate(x2) < self.evaluate(x1):\n            return x2\n        else:\n            return x1\n\n    def optimize(self, func):\n        for _ in range(self.budget):\n            # evaluate population\n            fitness = [self.evaluate(x) for x in self.population]\n            # get best solution\n            self.best_solution = min(fitness)\n            best_index = fitness.index(self.best_solution)\n            # adaptive frequency modulation\n            self.F = self.adaptive_frequency_modulation()\n            # differential evolution\n            for i in range(self.pop_size):\n                # generate trial vector\n                trial = self.crossover(self.population[i], self.population[random.randint(0, self.pop_size - 1)])\n                # evaluate trial vector\n                trial_fitness = self.evaluate(trial)\n                # selection\n                self.population[i] = self.selection(self.population[i], trial)\n                # update best solution\n                if trial_fitness < self.evaluate(self.population[i]):\n                    self.population[i] = trial\n        return self.best_solution\n\n    def __call__(self, func):\n        self.func = func\n        return self.optimize(func)\n\n# example usage\nbudget = 1000\ndim = 10\noptimizer = ADDE_SAFM(budget, dim)\nbest_solution = optimizer(lambda x: np.sum(x**2))\nprint(\"Best solution:\", best_solution)", "name": "ADDE_SAFM", "description": "Adaptive Discrete Differential Evolution with Self-Adaptive Frequency Modulation and Probabilistic Exploration-Exploitation Trade-off", "configspace": "", "generation": 6, "fitness": 0.10215919638362855, "feedback": "The algorithm ADDE_SAFM got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.10 with standard deviation 0.08.", "error": "", "parent_id": "bb9cbb65-5c4d-45a7-b81d-3ee61b89267c", "metadata": {"aucs": [0.257228451992248, 0.2474280601179376, 0.20877067787282733, 0.12630387145902933, 0.14212356606000298, 0.1486677166220428, 0.14706162600381212, 0.16553032027194825, 0.11988291498353776, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.07968824309141309, 0.08720330236158791, 0.08077757262842189, 0.0777704137157823, 0.05008455550642632, 0.06972212624429686, 0.07876230812720264, 0.0717553941580118, 0.06043373706474375, 0.052396606626647135, 0.05708610806695691, 0.05240809075323072, 0.06509944298337922, 0.03922715928231646, 0.04260720926906758, 0.04577233516028445, 0.03437523273508181, 0.013772055219368529, 0.051922104455408014, 0.05083043929555586, 0.04721270332046201, 0.05510617993665856, 0.05027593253860463, 0.049487404794003154, 0.06661476449066428, 0.05346266873488803, 0.05163416156882683, 0.06281446110420874, 0.03385057695639715, 0.03432938407343322, 0.06072283000304668, 0.05593875820446359, 0.059406735798636645, 9.999999999998899e-05, 0.032119148007989096, 9.999999999998899e-05, 0.17096059897815208, 0.20206958017636378, 0.13791010289017613, 0.15132479253111852, 0.20139766352615274, 0.10369344555979476, 0.12522833642636289, 0.09034209955935768, 0.16307622664922905, 0.008607212026728539, 0.02359262494210601, 9.999999999998899e-05, 9.999999999998899e-05, 0.004904597174603453, 9.999999999998899e-05, 0.12189784642175583, 0.03577394754243246, 9.999999999998899e-05, 0.09045212642990885, 0.057574119426946124, 0.05065770211580711, 0.0897490384870604, 0.13304591051147596, 0.09933481639710406, 0.10262810044506165, 0.11903039662090686, 0.05830705670119407, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.00041586112098956907, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.008050668799856386, 0.09777948198691522, 0.11320387500985907, 0.07865878613644872, 0.04831116195121554, 0.05487659455044813, 0.03421319938130907, 0.09525924417725495, 0.10032029041314128, 0.09043121887141892, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.32642223710821894, 0.2449193742527246, 0.22029327285213796, 0.23139465681311167, 0.17470386315253073, 0.19365680951945619, 0.23406516138895606, 0.23447863249477519, 0.23846169438276343, 0.05099641799403709, 0.021812298996978785, 0.02562441977329344, 0.06757125940053033, 0.048249567434446416, 0.06815131232928084, 0.07394129853724629, 0.05657491907479195, 0.0714660196672452, 0.11910574422167697, 0.12995518378802862, 0.12679855996169753, 0.10830489464609216, 0.14861274644588762, 0.11192696390984269, 0.14554901304234535, 0.11604127424167598, 0.12151741767534296, 0.17525343829062712, 0.18006404977736234, 0.1850663962495892, 0.20912607293452679, 0.2518933465450848, 0.23682143320695082, 0.16267179825074396, 0.2076414789498049, 0.18257121476125948, 0.11692090748849637, 0.09585553793242474, 0.12075839883051409, 0.15189310307919368, 0.18535335703841171, 0.17699108648278594, 0.11298897394938956, 0.14521158446257632, 0.10423064587787989, 0.21541550948318722, 0.18887601827212352, 0.20205010266734924, 0.19383633259269284, 0.19044642751733531, 0.21350958356035776, 0.18642832536082765, 0.19689287158818114, 0.20053027524383726, 0.16307358737067568, 0.15475137935940952, 0.1654427705965632, 0.1515827450356313, 0.16280300685245552, 0.1646979440807862, 0.15036692733959411, 0.15731513343097958, 0.15741766069239715, 0.1682951634419294, 0.17909146281746602, 0.14731818129993335, 0.36289548948549755, 0.21177055045472104, 0.354768760626771, 0.14117168018382165, 0.18176671991614946, 0.2480600568364627, 0.21040936600847904, 0.25295159042029614, 0.207168018210528, 0.12437658882890701, 0.1674679587442255, 0.09582516267612118, 0.17426155577705327, 0.15785730040822454, 0.125397671727966, 0.16116222518902013, 0.1644783044954642, 0.16119148566505315, 0.16089856559788185, 0.16643529131025048, 0.18249312894048597, 0.16884953962472804, 0.1683631593049284, 0.17639972375344837, 0.05417025531147279, 0.057370043851155406, 0.06389536318991573, 0.06601110825342571, 0.07550050197223468, 0.06224266202304096, 0.06537078750866698, 0.083298582857678, 0.0853118982937352]}, "mutation_prompt": null}
{"id": "351caa0a-d67a-464e-8407-9e3f2bdbf792", "solution": "import numpy as np\nimport random\n\nclass ADDE_SAFM_MPDS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.F = 0.5  # initial frequency\n        self.CR = 0.5  # initial crossover rate\n        self.pop_size = 50  # initial population size\n        self.population = np.random.uniform(-5.0, 5.0, (self.pop_size, self.dim))\n        self.best_solution = np.inf\n        self.probability = 0.1  # probability of probabilistic exploration\n        self.phases = 5  # number of optimization phases\n        self.phase = 1  # current optimization phase\n        self.phase_duration = self.budget // self.phases  # duration of each phase\n        self.population_size_history = [self.pop_size] * self.phases  # population size history\n\n    def evaluate(self, x):\n        return self.func(x)\n\n    def func(self, x):\n        # BBOB test suite of 24 noiseless functions\n        # Here we use the first function f1(x) = sum(x_i^2)\n        return np.sum(x**2)\n\n    def frequency_modulation(self, x):\n        # frequency modulation function\n        return self.F * np.sin(2 * np.pi * self.F * x)\n\n    def adaptive_frequency_modulation(self):\n        # adaptive frequency modulation\n        self.F *= 1.1  # increase frequency\n        if random.random() < self.probability:\n            self.F *= 0.9  # decrease frequency with probability\n        return self.F\n\n    def crossover(self, x1, x2):\n        # crossover operation\n        r = np.random.rand(self.dim)\n        return x1 + r * (x2 - x1)\n\n    def selection(self, x1, x2):\n        # selection operation\n        if self.evaluate(x2) < self.evaluate(x1):\n            return x2\n        else:\n            return x1\n\n    def optimize(self, func):\n        for _ in range(self.budget):\n            # evaluate population\n            fitness = [self.evaluate(x) for x in self.population]\n            # get best solution\n            self.best_solution = min(fitness)\n            best_index = fitness.index(self.best_solution)\n            # adaptive frequency modulation\n            self.F = self.adaptive_frequency_modulation()\n            # differential evolution\n            for i in range(self.pop_size):\n                # generate trial vector\n                trial = self.crossover(self.population[i], self.population[random.randint(0, self.pop_size - 1)])\n                # evaluate trial vector\n                trial_fitness = self.evaluate(trial)\n                # selection\n                self.population[i] = self.selection(self.population[i], trial)\n                # update best solution\n                if trial_fitness < self.evaluate(self.population[i]):\n                    self.population[i] = trial\n            # dynamic population size\n            if self.phase < self.phases:\n                if self.phase == 1:\n                    self.population_size_history[self.phase] = self.pop_size\n                elif self.phase == 2:\n                    self.population_size_history[self.phase] = self.pop_size // 2\n                elif self.phase == 3:\n                    self.population_size_history[self.phase] = self.pop_size // 3\n                elif self.phase == 4:\n                    self.population_size_history[self.phase] = self.pop_size // 4\n                self.pop_size = self.population_size_history[self.phase]\n                self.population = np.random.uniform(-5.0, 5.0, (self.pop_size, self.dim))\n            # phase transition\n            if _ % self.phase_duration == 0 and self.phase < self.phases:\n                self.phase += 1\n        return self.best_solution\n\n    def __call__(self, func):\n        self.func = func\n        return self.optimize(func)\n\n# example usage\nbudget = 1000\ndim = 10\noptimizer = ADDE_SAFM_MPDS(budget, dim)\nbest_solution = optimizer(lambda x: np.sum(x**2))\nprint(\"Best solution:\", best_solution)", "name": "ADDE_SAFM_MPDS", "description": "Adaptive Discrete Differential Evolution with Self-Adaptive Frequency Modulation and Probabilistic Exploration-Exploitation Trade-off with Multi-Phase Optimization and Dynamic Population Size", "configspace": "", "generation": 7, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('min() arg is an empty sequence').", "error": "ValueError('min() arg is an empty sequence')", "parent_id": "f4cb4cb9-b9ba-458e-893a-5ba935bba448", "metadata": {}, "mutation_prompt": null}
{"id": "f39fbdbc-f3bd-44c5-9c29-099893fb60ad", "solution": "import numpy as np\nimport random\n\nclass ADDE_SAFM_DPS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.F = 0.5  # initial frequency\n        self.CR = 0.5  # initial crossover rate\n        self.pop_size = 50  # initial population size\n        self.population = np.random.uniform(-5.0, 5.0, (self.pop_size, self.dim))\n        self.best_solution = np.inf\n        self.probability = 0.1  # probability of probabilistic exploration\n        self.min_pop_size = 20\n        self.max_pop_size = 100\n\n    def evaluate(self, x):\n        return self.func(x)\n\n    def func(self, x):\n        # BBOB test suite of 24 noiseless functions\n        # Here we use the first function f1(x) = sum(x_i^2)\n        return np.sum(x**2)\n\n    def frequency_modulation(self, x):\n        # frequency modulation function\n        return self.F * np.sin(2 * np.pi * self.F * x)\n\n    def adaptive_frequency_modulation(self):\n        # adaptive frequency modulation\n        self.F *= 1.1  # increase frequency\n        if random.random() < self.probability:\n            self.F *= 0.9  # decrease frequency with probability\n        return self.F\n\n    def crossover(self, x1, x2):\n        # crossover operation\n        r = np.random.rand(self.dim)\n        return x1 + r * (x2 - x1)\n\n    def selection(self, x1, x2):\n        # selection operation\n        if self.evaluate(x2) < self.evaluate(x1):\n            return x2\n        else:\n            return x1\n\n    def dynamic_population_size_adjustment(self):\n        # adjust population size based on convergence\n        if self.budget > 200 and np.mean([self.evaluate(x) for x in self.population]) < self.best_solution:\n            self.pop_size = min(self.pop_size + 10, self.max_pop_size)\n        elif self.budget > 200 and np.mean([self.evaluate(x) for x in self.population]) >= self.best_solution:\n            self.pop_size = max(self.pop_size - 10, self.min_pop_size)\n        return self.pop_size\n\n    def optimize(self, func):\n        for _ in range(self.budget):\n            # evaluate population\n            fitness = [self.evaluate(x) for x in self.population]\n            # get best solution\n            self.best_solution = min(fitness)\n            best_index = fitness.index(self.best_solution)\n            # adaptive frequency modulation\n            self.F = self.adaptive_frequency_modulation()\n            # differential evolution\n            self.pop_size = self.dynamic_population_size_adjustment()\n            for i in range(self.pop_size):\n                # generate trial vector\n                trial = self.crossover(self.population[i], self.population[random.randint(0, self.pop_size - 1)])\n                # evaluate trial vector\n                trial_fitness = self.evaluate(trial)\n                # selection\n                self.population[i] = self.selection(self.population[i], trial)\n                # update best solution\n                if trial_fitness < self.evaluate(self.population[i]):\n                    self.population[i] = trial\n        return self.best_solution\n\n    def __call__(self, func):\n        self.func = func\n        return self.optimize(func)\n\n# example usage\nbudget = 1000\ndim = 10\noptimizer = ADDE_SAFM_DPS(budget, dim)\nbest_solution = optimizer(lambda x: np.sum(x**2))\nprint(\"Best solution:\", best_solution)", "name": "ADDE_SAFM_DPS", "description": "Adaptive Differential Evolution with Self-Adaptive Frequency Modulation, Probabilistic Exploration-Exploitation Trade-off, and Dynamic Population Size Adjustment.", "configspace": "", "generation": 8, "fitness": 0.09254745928892044, "feedback": "The algorithm ADDE_SAFM_DPS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09 with standard deviation 0.08.", "error": "", "parent_id": "f4cb4cb9-b9ba-458e-893a-5ba935bba448", "metadata": {"aucs": [0.31145808711427403, 0.186622360059997, 0.29105257709428145, 0.09431231635004056, 0.10750000009520266, 0.12729602952996666, 0.16532476503693183, 0.17328817424992216, 0.0994687854704156, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.05176568098855949, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.0396789624260242, 0.05648068841768161, 0.023341631528313478, 0.08295753682991613, 0.05582496249973845, 0.06281735705965197, 0.07796271254373499, 0.06833158526857075, 0.05700191341299743, 0.039772244674994406, 0.044198238220555686, 0.037874546312927526, 0.053941875094680514, 0.04753706532488955, 0.0431045366615701, 0.06323663519723521, 0.028146068540179314, 0.014165321029633526, 0.05140529277059902, 0.04734202292195444, 0.04415698380632205, 0.05848664439336648, 0.05050588740784134, 0.05046320978585339, 0.06392262248154279, 0.049121585756495056, 0.04593329549536029, 0.06274794416174423, 0.05304332941355383, 0.055674198659942165, 0.05046375508533785, 0.016492338137845786, 0.06948836433831218, 0.044183020153744645, 0.015591066676106324, 9.999999999998899e-05, 0.14717434250657047, 0.1367956927787749, 0.13386135963368584, 0.13341007434531404, 0.09154397869603459, 0.054141821504978704, 0.09874046168494588, 0.13447939962419497, 0.12640342601361332, 0.09294869051445964, 0.003479820588933591, 9.999999999998899e-05, 9.999999999998899e-05, 0.0024452968757087135, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.10311391778811341, 0.10164750093246966, 0.09923953980023237, 0.10581529635590015, 0.03201086312847545, 0.07652017297668823, 0.161975882137938, 0.05126377102706703, 0.06268416760278739, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.06014453238871542, 0.0953561080499501, 0.08109838721183005, 0.03991536815204222, 0.027661076340444346, 0.020776568219876945, 0.08055332276798655, 0.06908143815947754, 0.057354970397543004, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.005048294572393752, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.35852765137796805, 0.20605841569683303, 0.18227355892272956, 0.2098762537943326, 0.13629313498217432, 0.17024373423021388, 0.20759370156691848, 0.19609561852413493, 0.2074537697915586, 0.020709108116962693, 0.010361296635203354, 0.02198412468535549, 0.04943573213011587, 0.03318009482068407, 0.06584956348976156, 0.050413171440406024, 0.03639432747190352, 0.07248156753818236, 0.0995291338592369, 0.1218989009387067, 0.1773948188603397, 0.11793722694661102, 0.11528874534688116, 0.11106593969762213, 0.12032176990215282, 0.11646855870737383, 0.10218805264829645, 0.13398763383405077, 0.16360493184069091, 0.1613932299901245, 0.19151995509442754, 0.22124473165813652, 0.1710666148718235, 0.1474762899998956, 0.20366656654524762, 0.1255839411145756, 0.09589972013969672, 0.15044570856460637, 0.09519154126159823, 0.13676175495458243, 0.18102846252667715, 0.12883428326241264, 0.0936584301835095, 0.13381558016226613, 0.07422364698150319, 0.18755377664597594, 0.1992737498135606, 0.16725739354197322, 0.18432448630211107, 0.17144809003170725, 0.22000131033158576, 0.17691935001345171, 0.17797222379798439, 0.17852581118267985, 0.16365240422237493, 0.16226363928020715, 0.1554061631858651, 0.17030312667775993, 0.17243200692221794, 0.15979678472116743, 0.1560273533043397, 0.15195741355761205, 0.13538840363799698, 0.15715948829648418, 0.15814480275536424, 0.10189390411521104, 0.1756569510063205, 0.13788853217599406, 0.1283770827827928, 0.10928802785381575, 0.25544817935055597, 0.11211704612528706, 0.20897820094309605, 0.21413617338122182, 0.12645363000958865, 0.26302752471934765, 0.09114537147113877, 0.3142430416335956, 0.16697895676366314, 0.20579544993433507, 0.08683483086138422, 0.17060037118639615, 0.17321868438441979, 0.17466108603333463, 0.16072260739907995, 0.1903158683078432, 0.1723453934902911, 0.201583035831584, 0.17379335270985607, 0.15757758954406287, 0.06490584094355167, 0.05198009964195749, 0.054407043894195484, 0.06923152633660357, 0.05256668866773073, 0.08659349739319566, 0.06453567113836456, 0.0465711930621906, 0.06958021872795128]}, "mutation_prompt": null}
{"id": "ab5e47e4-83a2-4861-9801-ce57b6ca8146", "solution": "import numpy as np\nimport random\n\nclass ADDE_SAFM_ES:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.F = 0.5  # initial frequency\n        self.CR = 0.5  # initial crossover rate\n        self.pop_size = 50  # initial population size\n        self.population = np.random.uniform(-5.0, 5.0, (self.pop_size, self.dim))\n        self.best_solution = np.inf\n        self.probability = 0.1  # probability of probabilistic exploration\n        self.temperature = 100  # initial temperature for simulated annealing\n        self.covariance_matrix = np.eye(self.dim)  # initial covariance matrix\n        self.adaptive_step_size = 0.1  # initial adaptive step size for covariance matrix adaptation\n\n    def evaluate(self, x):\n        return self.func(x)\n\n    def func(self, x):\n        # BBOB test suite of 24 noiseless functions\n        # Here we use the first function f1(x) = sum(x_i^2)\n        return np.sum(x**2)\n\n    def frequency_modulation(self, x):\n        # frequency modulation function\n        return self.F * np.sin(2 * np.pi * self.F * x)\n\n    def adaptive_frequency_modulation(self):\n        # adaptive frequency modulation\n        self.F *= 1.1  # increase frequency\n        if random.random() < self.probability:\n            self.F *= 0.9  # decrease frequency with probability\n        return self.F\n\n    def crossover(self, x1, x2):\n        # crossover operation\n        r = np.random.rand(self.dim)\n        return x1 + r * (x2 - x1)\n\n    def selection(self, x1, x2):\n        # selection operation\n        if self.evaluate(x2) < self.evaluate(x1):\n            return x2\n        else:\n            return x1\n\n    def simulated_annealing(self, x):\n        # simulated annealing\n        new_x = x + np.random.normal(0, 1, self.dim)\n        new_x = np.clip(new_x, -5.0, 5.0)\n        if self.evaluate(new_x) < self.evaluate(x):\n            return new_x\n        else:\n            probability = np.exp(-(self.evaluate(new_x) - self.evaluate(x)) / self.temperature)\n            if random.random() < probability:\n                return new_x\n            else:\n                return x\n        self.temperature *= 0.99  # decrease temperature\n\n    def covariance_matrix_adaptation(self):\n        # covariance matrix adaptation\n        self.covariance_matrix = (1 - self.adaptive_step_size) * self.covariance_matrix + self.adaptive_step_size * np.eye(self.dim)\n        return self.covariance_matrix\n\n    def optimize(self, func):\n        for _ in range(self.budget):\n            # evaluate population\n            fitness = [self.evaluate(x) for x in self.population]\n            # get best solution\n            self.best_solution = min(fitness)\n            best_index = fitness.index(self.best_solution)\n            # adaptive frequency modulation\n            self.F = self.adaptive_frequency_modulation()\n            # differential evolution\n            for i in range(self.pop_size):\n                # generate trial vector\n                trial = self.crossover(self.population[i], self.population[random.randint(0, self.pop_size - 1)])\n                # evaluate trial vector\n                trial_fitness = self.evaluate(trial)\n                # selection\n                self.population[i] = self.selection(self.population[i], trial)\n                # update best solution\n                if trial_fitness < self.evaluate(self.population[i]):\n                    self.population[i] = trial\n            # simulated annealing\n            for i in range(self.pop_size):\n                self.population[i] = self.simulated_annealing(self.population[i])\n            # covariance matrix adaptation\n            self.covariance_matrix = self.covariance_matrix_adaptation()\n        return self.best_solution\n\n    def __call__(self, func):\n        self.func = func\n        return self.optimize(func)\n\n# example usage\nbudget = 1000\ndim = 10\noptimizer = ADDE_SAFM_ES(budget, dim)\nbest_solution = optimizer(lambda x: np.sum(x**2))\nprint(\"Best solution:\", best_solution)", "name": "ADDE_SAFM_ES", "description": "Adaptive Discrete Differential Evolution with Self-Adaptive Frequency Modulation, Probabilistic Exploration-Exploitation Trade-off, and Enhanced Local Search using Simulated Annealing and Covariance Matrix Adaptation.", "configspace": "", "generation": 9, "fitness": 0.1104600223237032, "feedback": "The algorithm ADDE_SAFM_ES got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.11 with standard deviation 0.07.", "error": "", "parent_id": "f4cb4cb9-b9ba-458e-893a-5ba935bba448", "metadata": {"aucs": [0.23906187872904183, 0.2510843148945291, 0.22111091660977722, 0.20560847666060422, 0.18995176305576855, 0.2004169123314158, 0.21487534349591253, 0.20303223090415146, 0.21514755926328633, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.05355610146399992, 0.06079630563891947, 0.052811287323377476, 0.07042642328183979, 0.06405891671910391, 0.06469768930817499, 0.0568496858432056, 0.06499835824111744, 0.0716721300706602, 0.05459890442222637, 0.05753881067641764, 0.05193894557038059, 0.06494128902282414, 0.05279462452491057, 0.04138734223611151, 0.05668166343376957, 0.05103300424634927, 0.045115762845026275, 0.24099989289043955, 0.1461542150275611, 0.10290293882513035, 0.14387155482311087, 0.10880333358461503, 0.12609504732735766, 0.18359237046393895, 0.10031632770312904, 0.1333438960133808, 0.10154350104793775, 0.08197870036485111, 0.1184807648111148, 0.06796853714755802, 0.08814687182280412, 0.08871034399417443, 0.08973779847587193, 0.0885806554386721, 0.09816396955643725, 0.18706290943465786, 0.17454616223645913, 0.15317754310826326, 0.17015123178186664, 0.19906586228324108, 0.1900851584027874, 0.1557314153955407, 0.1474432663043297, 0.16981651618070048, 0.03398469523481307, 0.06609483212236922, 0.0731609719880082, 0.06090512717953189, 0.08209140121999403, 0.06588942499060901, 0.11176959569528022, 0.09252995861750624, 0.0831226343042647, 0.10540750373710439, 0.11625416955376, 0.10963455063226979, 0.1006762039280602, 0.11215446611580882, 0.09390116379131219, 0.11005440458711802, 0.11377567623660001, 0.09956160417661652, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.00010062234526553304, 0.014380030820433265, 0.08104180493266344, 0.07401395404256306, 0.07967078354134904, 0.03304102565510458, 0.048983528037553015, 0.03989362622807935, 0.07752749366277689, 0.11724885483229186, 0.09034500399773315, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.032712427014472634, 0.03203690830314221, 0.025216084315066833, 0.02373470752973894, 0.03695549435104095, 0.04847471867960618, 0.041371804524372835, 0.02699129589573801, 0.04357890636254924, 0.21477908743418228, 0.24678704374231142, 0.2279536568745566, 0.22426617508870128, 0.21336295878428124, 0.21211651448687918, 0.2261896545399923, 0.2359464415211069, 0.23988103659751214, 0.04623806423613064, 0.05803051653040636, 0.0565095983228443, 0.06764338582946439, 0.05818672360285615, 0.059345525633996976, 0.07135692951950512, 0.05768157529790474, 0.07037554695137227, 0.1080302574858435, 0.12044006313981903, 0.11894688272068865, 0.10489348091650186, 0.1246213326696185, 0.11799240861041871, 0.11436537968607152, 0.13937766466111967, 0.16335583616340632, 0.17440932969559486, 0.1714717032863029, 0.1800930571325976, 0.172143472181703, 0.18047366765821493, 0.18332027308529242, 0.17183148187388286, 0.17892019668928805, 0.17404367454512393, 0.11704954976885551, 0.12064756118306108, 0.1184622949697659, 0.14780342705369953, 0.1383214023751772, 0.12349144134016132, 0.11201811292433672, 0.11334532934772212, 0.12018202256499866, 0.18357488817335788, 0.15818709710181933, 0.17444369459568532, 0.1742373871898344, 0.18759725156589613, 0.17333162904557498, 0.17580110366267132, 0.16817084462423715, 0.18125241251459556, 0.16129252518839843, 0.15872595871289563, 0.16631285228369563, 0.16886909892960977, 0.16964250031854045, 0.16165075363198245, 0.15673818152670316, 0.16275574932819203, 0.16336225763690582, 0.163193983403074, 0.15323205246866523, 0.20810123492396804, 0.21549422031563636, 0.2390886186305311, 0.18335580136619567, 0.16638141691306774, 0.18921728389036974, 0.1490772244014128, 0.18388642723920812, 0.14331616653978274, 0.12117647060402015, 0.14461488481670481, 0.26781293329124534, 0.14931749706008834, 0.11285254316320847, 0.14696125409265748, 0.12412672739595454, 0.16652592966196422, 0.16094663334590964, 0.18902278875639045, 0.16180496459540905, 0.16916463679389382, 0.16250602617514198, 0.16154208343537324, 0.17429747387452643, 0.17311370187882336, 0.0728717840785299, 0.07559610654915538, 0.06807200057251195, 0.06174451703299033, 0.06465088812744746, 0.062325507451185924, 0.06813060684104699, 0.06332298976528405, 0.06642663327325171]}, "mutation_prompt": null}
{"id": "33d9afe2-8391-4a4d-bfa5-b51d58e98eac", "solution": "import numpy as np\nimport random\n\nclass ADDE_SAFM_ES:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.F = 0.5  # initial frequency\n        self.CR = 0.5  # initial crossover rate\n        self.pop_size = 50  # initial population size\n        self.population = np.random.uniform(-5.0, 5.0, (self.pop_size, self.dim))\n        self.best_solution = np.inf\n        self.probability = 0.1  # probability of probabilistic exploration\n        self.temperature = 100  # initial temperature for simulated annealing\n        self.covariance_matrix = np.eye(self.dim)  # initial covariance matrix\n        self.adaptive_step_size = 0.1  # initial adaptive step size for covariance matrix adaptation\n        self.phase = 1  # initial phase\n\n    def evaluate(self, x):\n        return self.func(x)\n\n    def func(self, x):\n        # BBOB test suite of 24 noiseless functions\n        # Here we use the first function f1(x) = sum(x_i^2)\n        return np.sum(x**2)\n\n    def frequency_modulation(self, x):\n        # frequency modulation function\n        return self.F * np.sin(2 * np.pi * self.F * x)\n\n    def adaptive_frequency_modulation(self):\n        # adaptive frequency modulation\n        self.F *= 1.1  # increase frequency\n        if random.random() < self.probability:\n            self.F *= 0.9  # decrease frequency with probability\n        return self.F\n\n    def crossover(self, x1, x2):\n        # crossover operation\n        r = np.random.rand(self.dim)\n        return x1 + r * (x2 - x1)\n\n    def selection(self, x1, x2):\n        # selection operation\n        if self.evaluate(x2) < self.evaluate(x1):\n            return x2\n        else:\n            return x1\n\n    def simulated_annealing(self, x):\n        # simulated annealing\n        new_x = x + np.random.normal(0, 1, self.dim)\n        new_x = np.clip(new_x, -5.0, 5.0)\n        if self.evaluate(new_x) < self.evaluate(x):\n            return new_x\n        else:\n            probability = np.exp(-(self.evaluate(new_x) - self.evaluate(x)) / self.temperature)\n            if random.random() < probability:\n                return new_x\n            else:\n                return x\n        self.temperature *= 0.99  # decrease temperature\n\n    def covariance_matrix_adaptation(self):\n        # covariance matrix adaptation\n        self.covariance_matrix = (1 - self.adaptive_step_size) * self.covariance_matrix + self.adaptive_step_size * np.eye(self.dim)\n        return self.covariance_matrix\n\n    def optimize(self, func):\n        for _ in range(self.budget):\n            # evaluate population\n            fitness = [self.evaluate(x) for x in self.population]\n            # get best solution\n            self.best_solution = min(fitness)\n            best_index = fitness.index(self.best_solution)\n            # adaptive frequency modulation\n            self.F = self.adaptive_frequency_modulation()\n            # differential evolution\n            for i in range(self.pop_size):\n                # generate trial vector\n                trial = self.crossover(self.population[i], self.population[random.randint(0, self.pop_size - 1)])\n                # evaluate trial vector\n                trial_fitness = self.evaluate(trial)\n                # selection\n                self.population[i] = self.selection(self.population[i], trial)\n                # update best solution\n                if trial_fitness < self.evaluate(self.population[i]):\n                    self.population[i] = trial\n            # simulated annealing\n            for i in range(self.pop_size):\n                self.population[i] = self.simulated_annealing(self.population[i])\n            # covariance matrix adaptation\n            self.covariance_matrix = self.covariance_matrix_adaptation()\n            # phase change\n            if self.phase == 1 and np.mean(fitness) < 0.1:\n                self.phase = 2\n                self.population = np.random.uniform(-5.0, 5.0, (self.pop_size, self.dim))\n                self.best_solution = np.inf\n            elif self.phase == 2 and np.mean(fitness) < 0.01:\n                self.phase = 3\n                self.population = np.random.uniform(-5.0, 5.0, (self.pop_size, self.dim))\n                self.best_solution = np.inf\n            elif self.phase == 3 and np.mean(fitness) < 0.001:\n                self.phase = 1\n                self.population = np.random.uniform(-5.0, 5.0, (self.pop_size, self.dim))\n                self.best_solution = np.inf\n        return self.best_solution\n\n    def __call__(self, func):\n        self.func = func\n        return self.optimize(func)\n\n# example usage\nbudget = 1000\ndim = 10\noptimizer = ADDE_SAFM_ES(budget, dim)\nbest_solution = optimizer(lambda x: np.sum(x**2))\nprint(\"Best solution:\", best_solution)", "name": "ADDE_SAFM_ES", "description": "Adaptive Differential Evolution with Self-Adaptive Frequency Modulation, Probabilistic Exploration-Exploitation Trade-off, and Enhanced Local Search using Simulated Annealing and Covariance Matrix Adaptation with a Novel Multi-Phase Strategy.", "configspace": "", "generation": 10, "fitness": 0.10630554423805115, "feedback": "The algorithm ADDE_SAFM_ES got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.11 with standard deviation 0.07.", "error": "", "parent_id": "ab5e47e4-83a2-4861-9801-ce57b6ca8146", "metadata": {"aucs": [0.23657509181512693, 0.2204703707981669, 0.2186857721849499, 0.18405357509199538, 0.2146275158122135, 0.19582105029338592, 0.14790546228282198, 0.14592958545451384, 0.17957774372304391, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.039168867798635265, 0.07051179737798119, 0.03672383071018026, 0.06333989260108897, 0.06900784789088776, 0.061853706911423445, 0.08322600867053875, 0.08490925460325638, 0.05734127663488109, 0.03302619201037382, 0.045432761951399336, 0.04521527716369689, 0.054879682642274163, 0.04950319382223711, 0.04883520040556555, 0.06487740804750863, 0.044608919835884664, 0.048360044642406175, 0.1533272175749114, 0.11000609313511756, 0.09939571658855162, 0.12194642335897699, 0.09956738663933118, 0.11006622438624414, 0.17628293497895342, 0.11055269547028057, 0.4218422406936505, 0.12398623901378159, 0.08844216250828196, 0.09342711755664568, 0.0865923152743644, 0.07820828585238926, 0.07904432645635351, 0.0974422066962416, 0.1046291564207642, 0.0901035630121223, 0.15258303713747923, 0.16746163724353935, 0.14682242765221598, 0.16342013210892326, 0.18764067081798252, 0.13788783921299796, 0.15587926862457158, 0.1419905665137703, 0.18295825031118385, 0.08843141087834283, 0.04646190790069127, 0.0747962470599145, 0.02474263993471848, 0.03418242643231073, 0.007298295763345952, 0.08933552071057238, 0.0594015190227305, 0.0977593871868001, 0.10124123636608506, 0.10918022435751407, 0.0846008769029144, 0.10547848233169388, 0.13047040989751857, 0.09785695021975283, 0.12760675241434927, 0.10329445015811456, 0.09114001029247232, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.06811172774301921, 0.05725968783989843, 0.07442735417447421, 0.03700448305986481, 0.03609392381487142, 0.024115300973516773, 0.06273219936404872, 0.10289179170957574, 0.04675282640224576, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.01800092434574141, 0.024052307763240743, 0.027033915459004132, 0.03388164390611781, 0.031256235273738664, 0.038061402272689726, 0.02482767396831076, 0.02097759227246332, 0.013105357099519876, 0.2125698748694892, 0.18702563836166142, 0.19664057508096033, 0.20639667844418697, 0.17133619610268402, 0.16871953330417822, 0.22849621051385516, 0.2200957080153253, 0.23165154348650885, 0.04976470234100383, 0.05322905021500124, 0.051888688601009925, 0.08102076065421993, 0.08848669212070326, 0.06251293761110688, 0.05652560250168226, 0.06548398111610931, 0.05925026168229597, 0.10795776646209343, 0.12397346256258679, 0.12066184540628277, 0.12054392416091209, 0.1224904141782095, 0.12358796318790632, 0.10544182343828556, 0.13730005151085878, 0.12449398076187368, 0.1600137354688611, 0.16253063986625715, 0.16255193367064202, 0.17489177757344665, 0.178173261394043, 0.17976873692037842, 0.16354885419030563, 0.1794762277759312, 0.17372365245559263, 0.11809565235084396, 0.12145159925263105, 0.11969642576565587, 0.14575087688218136, 0.17510640384477316, 0.1293023360472002, 0.10413098606264015, 0.12382348519657449, 0.10134198835518471, 0.16328046661337192, 0.15998185875431903, 0.17471306837069445, 0.18001499366231133, 0.24542707605187475, 0.18714328295696392, 0.19626532288170906, 0.15990563402593538, 0.15598903604230308, 0.16575794742345318, 0.14756265544754676, 0.1509800850316254, 0.18163327905139393, 0.1586969780169395, 0.1668736730436332, 0.1600870711850828, 0.17182290049150084, 0.1561355008680082, 0.1465371395465569, 0.16818811883097984, 0.1560127948733021, 0.1926164551935834, 0.24567662199753837, 0.1694189154318133, 0.20647411777999036, 0.17606395428959698, 0.25972892991047714, 0.17990558049593075, 0.14531810213439134, 0.11453976811574973, 0.12235735701383221, 0.1391513426775033, 0.15519578269859668, 0.14312794505672932, 0.14207559143125525, 0.13297006731155303, 0.1672156036626773, 0.1673240621532055, 0.16471385782078563, 0.1853136052844757, 0.16575416646703156, 0.17666149831030253, 0.1648904880794072, 0.16986830709465162, 0.1746434854316241, 0.06121078791219092, 0.0683692541469183, 0.06761256094640833, 0.06306766558300647, 0.05658163455584164, 0.05306352349509047, 0.0610778160682961, 0.06631825634312882, 0.05721856590358132]}, "mutation_prompt": null}
{"id": "da3e015e-e1ca-42e2-808a-c0af6dc4899f", "solution": "import numpy as np\nimport random\n\nclass ADDE_SAFM_ES_PPS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.F = 0.5  # initial frequency\n        self.CR = 0.5  # initial crossover rate\n        self.pop_size = 50  # initial population size\n        self.population = np.random.uniform(-5.0, 5.0, (self.pop_size, self.dim))\n        self.best_solution = np.inf\n        self.probability = 0.1  # probability of probabilistic exploration\n        self.temperature = 100  # initial temperature for simulated annealing\n        self.covariance_matrix = np.eye(self.dim)  # initial covariance matrix\n        self.adaptive_step_size = 0.1  # initial adaptive step size for covariance matrix adaptation\n        self.PPS_weight = 1.0  # initial weight for Probability Proportional to Size selection\n\n    def evaluate(self, x):\n        return self.func(x)\n\n    def func(self, x):\n        # BBOB test suite of 24 noiseless functions\n        # Here we use the first function f1(x) = sum(x_i^2)\n        return np.sum(x**2)\n\n    def frequency_modulation(self, x):\n        # frequency modulation function\n        return self.F * np.sin(2 * np.pi * self.F * x)\n\n    def adaptive_frequency_modulation(self):\n        # adaptive frequency modulation\n        self.F *= 1.1  # increase frequency\n        if random.random() < self.probability:\n            self.F *= 0.9  # decrease frequency with probability\n        return self.F\n\n    def crossover(self, x1, x2):\n        # crossover operation\n        r = np.random.rand(self.dim)\n        return x1 + r * (x2 - x1)\n\n    def selection(self, x1, x2):\n        # selection operation\n        if self.evaluate(x2) < self.evaluate(x1):\n            return x2\n        else:\n            return x1\n\n    def simulated_annealing(self, x):\n        # simulated annealing\n        new_x = x + np.random.normal(0, 1, self.dim)\n        new_x = np.clip(new_x, -5.0, 5.0)\n        if self.evaluate(new_x) < self.evaluate(x):\n            return new_x\n        else:\n            probability = np.exp(-(self.evaluate(new_x) - self.evaluate(x)) / self.temperature)\n            if random.random() < probability:\n                return new_x\n            else:\n                return x\n        self.temperature *= 0.99  # decrease temperature\n\n    def covariance_matrix_adaptation(self):\n        # covariance matrix adaptation\n        self.covariance_matrix = (1 - self.adaptive_step_size) * self.covariance_matrix + self.adaptive_step_size * np.eye(self.dim)\n        return self.covariance_matrix\n\n    def PPS_selection(self):\n        # Probability Proportional to Size selection\n        fitness = [self.evaluate(x) for x in self.population]\n        weights = [1 / (1 + f) for f in fitness]\n        weights = [w / sum(weights) for w in weights]\n        selected_indices = np.random.choice(self.pop_size, size=self.pop_size, replace=True, p=weights)\n        selected_population = [self.population[i] for i in selected_indices]\n        return selected_population\n\n    def optimize(self, func):\n        for _ in range(self.budget):\n            # evaluate population\n            fitness = [self.evaluate(x) for x in self.population]\n            # get best solution\n            self.best_solution = min(fitness)\n            best_index = fitness.index(self.best_solution)\n            # adaptive frequency modulation\n            self.F = self.adaptive_frequency_modulation()\n            # differential evolution\n            for i in range(self.pop_size):\n                # generate trial vector\n                trial = self.crossover(self.population[i], self.population[random.randint(0, self.pop_size - 1)])\n                # evaluate trial vector\n                trial_fitness = self.evaluate(trial)\n                # selection\n                self.population[i] = self.selection(self.population[i], trial)\n                # update best solution\n                if trial_fitness < self.evaluate(self.population[i]):\n                    self.population[i] = trial\n            # PPS selection\n            self.population = self.PPS_selection()\n            # simulated annealing\n            for i in range(self.pop_size):\n                self.population[i] = self.simulated_annealing(self.population[i])\n            # covariance matrix adaptation\n            self.covariance_matrix = self.covariance_matrix_adaptation()\n        return self.best_solution\n\n    def __call__(self, func):\n        self.func = func\n        return self.optimize(func)\n\n# example usage\nbudget = 1000\ndim = 10\noptimizer = ADDE_SAFM_ES_PPS(budget, dim)\nbest_solution = optimizer(lambda x: np.sum(x**2))\nprint(\"Best solution:\", best_solution)", "name": "ADDE_SAFM_ES_PPS", "description": "Adaptive Discrete Differential Evolution with Self-Adaptive Frequency Modulation, Probabilistic Exploration-Exploitation Trade-off, and Enhanced Local Search using Simulated Annealing and Covariance Matrix Adaptation, refined with Probability Proportional to Size (PPS) selection and Self-Adaptive Differential Evolution with Local Search.", "configspace": "", "generation": 11, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('probabilities are not non-negative').", "error": "ValueError('probabilities are not non-negative')", "parent_id": "ab5e47e4-83a2-4861-9801-ce57b6ca8146", "metadata": {}, "mutation_prompt": null}
{"id": "df117d13-1611-400f-aadc-c21c3dfd8dd8", "solution": "import numpy as np\nimport random\n\nclass Adaptive_Differential_Evolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.F = 0.5  # initial frequency\n        self.CR = 0.5  # initial crossover rate\n        self.pop_size = 50  # initial population size\n        self.population = np.random.uniform(-5.0, 5.0, (self.pop_size, self.dim))\n        self.best_solution = np.inf\n        self.probability = 0.1  # probability of probabilistic exploration\n        self.temperature = 100  # initial temperature for simulated annealing\n        self.covariance_matrix = np.eye(self.dim)  # initial covariance matrix\n        self.adaptive_step_size = 0.1  # initial adaptive step size for covariance matrix adaptation\n        self.step_size_learning_rate = 0.001  # learning rate for adaptive step size\n\n    def evaluate(self, x):\n        return self.func(x)\n\n    def func(self, x):\n        # BBOB test suite of 24 noiseless functions\n        # Here we use the first function f1(x) = sum(x_i^2)\n        return np.sum(x**2)\n\n    def frequency_modulation(self, x):\n        # frequency modulation function\n        return self.F * np.sin(2 * np.pi * self.F * x)\n\n    def adaptive_frequency_modulation(self):\n        # adaptive frequency modulation\n        self.F *= 1.1  # increase frequency\n        if random.random() < self.probability:\n            self.F *= 0.9  # decrease frequency with probability\n        return self.F\n\n    def crossover(self, x1, x2):\n        # crossover operation\n        r = np.random.rand(self.dim)\n        return x1 + r * (x2 - x1)\n\n    def selection(self, x1, x2):\n        # selection operation\n        if self.evaluate(x2) < self.evaluate(x1):\n            return x2\n        else:\n            return x1\n\n    def simulated_annealing(self, x):\n        # simulated annealing\n        new_x = x + np.random.normal(0, 1, self.dim)\n        new_x = np.clip(new_x, -5.0, 5.0)\n        if self.evaluate(new_x) < self.evaluate(x):\n            return new_x\n        else:\n            probability = np.exp(-(self.evaluate(new_x) - self.evaluate(x)) / self.temperature)\n            if random.random() < probability:\n                return new_x\n            else:\n                return x\n        self.temperature *= 0.99  # decrease temperature\n\n    def covariance_matrix_adaptation(self):\n        # covariance matrix adaptation\n        self.covariance_matrix = (1 - self.adaptive_step_size) * self.covariance_matrix + self.adaptive_step_size * np.eye(self.dim)\n        return self.covariance_matrix\n\n    def adaptive_step_size_learning(self):\n        # adaptive step size learning\n        self.adaptive_step_size += self.step_size_learning_rate * (self.evaluate(self.population[0]) - self.evaluate(self.population[-1]))\n        return self.adaptive_step_size\n\n    def optimize(self, func):\n        for _ in range(self.budget):\n            # evaluate population\n            fitness = [self.evaluate(x) for x in self.population]\n            # get best solution\n            self.best_solution = min(fitness)\n            best_index = fitness.index(self.best_solution)\n            # adaptive frequency modulation\n            self.F = self.adaptive_frequency_modulation()\n            # differential evolution\n            for i in range(self.pop_size):\n                # generate trial vector\n                trial = self.crossover(self.population[i], self.population[random.randint(0, self.pop_size - 1)])\n                # evaluate trial vector\n                trial_fitness = self.evaluate(trial)\n                # selection\n                self.population[i] = self.selection(self.population[i], trial)\n                # update best solution\n                if trial_fitness < self.evaluate(self.population[i]):\n                    self.population[i] = trial\n            # simulated annealing\n            for i in range(self.pop_size):\n                self.population[i] = self.simulated_annealing(self.population[i])\n            # covariance matrix adaptation\n            self.covariance_matrix = self.covariance_matrix_adaptation()\n            # adaptive step size learning\n            self.adaptive_step_size = self.adaptive_step_size_learning()\n        return self.best_solution\n\n    def __call__(self, func):\n        self.func = func\n        return self.optimize(func)\n\n# example usage\nbudget = 1000\ndim = 10\noptimizer = Adaptive_Differential_Evolution(budget, dim)\nbest_solution = optimizer(lambda x: np.sum(x**2))\nprint(\"Best solution:\", best_solution)", "name": "Adaptive_Differential_Evolution", "description": "Adaptive Differential Evolution with Self-Adaptive Frequency Modulation, Probabilistic Exploration-Exploitation Trade-off, and Enhanced Local Search using Simulated Annealing and Covariance Matrix Adaptation with Adaptive Step Size Learning and Probability-Based Frequency Modulation.", "configspace": "", "generation": 12, "fitness": 0.11134206739919167, "feedback": "The algorithm Adaptive_Differential_Evolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.11 with standard deviation 0.07.", "error": "", "parent_id": "ab5e47e4-83a2-4861-9801-ce57b6ca8146", "metadata": {"aucs": [0.2431409386091541, 0.23009349430623682, 0.21590902513207189, 0.19848028296775233, 0.19670233120308178, 0.2361783984439758, 0.20835542898172943, 0.21761885172512108, 0.20704549853208398, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.05634609167540072, 0.06947766296291025, 0.05849895205412592, 0.07466642600383422, 0.07235938137166664, 0.07291628465542122, 0.06908592467186159, 0.070308475988064, 0.07398628404622087, 0.052049766458211355, 0.062315474067236254, 0.05495779336826867, 0.053526287288546914, 0.049544098013648896, 0.04697999876854597, 0.05671448933584089, 0.04773481289706338, 0.05363391836371012, 0.17151467354751626, 0.10445406887481734, 0.10277173810206752, 0.12171355656651195, 0.11117000087119855, 0.12421910524165647, 0.1792936998973126, 0.10796746050169459, 0.1344708603515561, 0.10358308620771783, 0.11659432340824327, 0.09560139108031784, 0.0646742836598071, 0.06808759587914814, 0.09108857264377057, 0.11050830496445474, 0.09838630588747677, 0.12543911357834836, 0.1769080448785877, 0.1699810415625539, 0.14745626000190948, 0.18448933110395627, 0.17195700665428248, 0.1594491600343717, 0.15236951767960272, 0.17887522912685472, 0.15395817664278422, 0.06967865164498876, 0.04302637124452757, 0.07271944585073686, 0.05503427074567735, 0.05488559235796697, 0.015929140481927062, 0.07120347159250906, 0.06606531458370557, 0.07808770823610789, 0.11654512322839039, 0.09527351728486211, 0.09178337050082552, 0.09362865470271409, 0.09957995482523307, 0.10090673217163804, 0.1038350309510131, 0.10263231922772087, 0.0963024300525005, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.09066135579029555, 0.053460068707942954, 0.11730594121401139, 0.07111000517744392, 0.04696423676267858, 0.05760010411058192, 0.09225203814320482, 0.10807780524000987, 0.04782588668384413, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.035054707026648035, 0.03233853355888461, 0.01722381451925392, 0.025716286632439966, 0.03785085191454585, 0.04012768393461974, 0.0374615865394784, 0.02761302936435628, 0.035601082329765665, 0.23859474809064274, 0.22883333868826383, 0.23285787293656512, 0.1920088938319785, 0.2190282842617508, 0.21338170001570778, 0.23113540036966174, 0.21295406033012731, 0.2270637721444143, 0.04859022095458587, 0.05743094450116404, 0.04987072298908968, 0.06783992846321307, 0.06289382707348956, 0.05640928106043119, 0.0625423089381053, 0.0647810363953657, 0.06377586255786005, 0.10304446201756745, 0.12502607611146266, 0.12560375082812336, 0.11649124078953044, 0.12980570784852663, 0.13229080431825402, 0.12876899765905603, 0.13378763999533216, 0.09391087353581029, 0.1766317252134595, 0.19685486779941708, 0.16605280389686616, 0.19622434159104618, 0.1769402036798976, 0.1967135652484564, 0.1600902970569501, 0.18357197835774486, 0.1709340840556104, 0.13296082693517197, 0.11155391833911654, 0.1274246975779022, 0.13527732399846992, 0.13235269028679364, 0.1446335167354531, 0.0962630111927627, 0.12554235315914597, 0.11364652794151409, 0.17264778616283782, 0.18050889269424208, 0.1835895155300573, 0.17840568070185436, 0.20343782701273083, 0.2038827352315461, 0.1825740151368469, 0.17329079175113127, 0.1718210413456338, 0.15637688765276647, 0.15874493006020185, 0.16143392595200678, 0.18120746039363134, 0.15941580276198164, 0.16994639078874896, 0.16237591663533224, 0.16065178933526147, 0.15895392375136308, 0.15343628069379933, 0.16286882435923722, 0.14120080540933955, 0.2620104012222114, 0.21153413429601908, 0.16643979831591038, 0.18434946065938218, 0.20211734008063997, 0.33236175624859066, 0.14757779064110377, 0.2188876204778898, 0.14363080673230433, 0.26931629737816043, 0.13641619626511947, 0.20687993382485204, 0.22566592916415273, 0.15105100494039037, 0.10968035462638603, 0.1670901976779352, 0.17778460140453622, 0.18472824925198184, 0.17624344326932695, 0.17095412870881266, 0.1825202526655284, 0.18607739526400924, 0.17404126871561298, 0.1730660010214624, 0.06454429185757904, 0.0626970869880562, 0.062439856938715455, 0.07103310296182697, 0.06343295686451633, 0.05864917758484267, 0.06607268892215756, 0.059427843311346984, 0.06233307863827808]}, "mutation_prompt": null}
{"id": "a259ae06-f4e1-498d-8b7a-55286c067997", "solution": "import numpy as np\nimport random\n\nclass Enhanced_Adaptive_Differential_Evolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.F = 0.5  # initial frequency\n        self.CR = 0.5  # initial crossover rate\n        self.pop_size = 50  # initial population size\n        self.population = np.random.uniform(-5.0, 5.0, (self.pop_size, self.dim))\n        self.best_solution = np.inf\n        self.probability = 0.2  # increased probability of probabilistic exploration\n        self.temperature = 100  # initial temperature for simulated annealing\n        self.covariance_matrix = np.eye(self.dim)  # initial covariance matrix\n        self.adaptive_step_size = 0.1  # initial adaptive step size for covariance matrix adaptation\n        self.step_size_learning_rate = 0.001  # learning rate for adaptive step size\n\n    def evaluate(self, x):\n        return self.func(x)\n\n    def func(self, x):\n        # BBOB test suite of 24 noiseless functions\n        # Here we use the first function f1(x) = sum(x_i^2)\n        return np.sum(x**2)\n\n    def frequency_modulation(self, x):\n        # frequency modulation function\n        return self.F * np.sin(2 * np.pi * self.F * x)\n\n    def adaptive_frequency_modulation(self):\n        # adaptive frequency modulation\n        self.F *= 1.1  # increase frequency\n        if random.random() < self.probability:\n            self.F *= 0.9  # decrease frequency with probability\n        return self.F\n\n    def crossover(self, x1, x2):\n        # crossover operation\n        r = np.random.rand(self.dim)\n        return x1 + r * (x2 - x1)\n\n    def selection(self, x1, x2):\n        # selection operation\n        if self.evaluate(x2) < self.evaluate(x1):\n            return x2\n        else:\n            return x1\n\n    def simulated_annealing(self, x):\n        # simulated annealing\n        new_x = x + np.random.normal(0, 1, self.dim)\n        new_x = np.clip(new_x, -5.0, 5.0)\n        if self.evaluate(new_x) < self.evaluate(x):\n            return new_x\n        else:\n            probability = np.exp(-(self.evaluate(new_x) - self.evaluate(x)) / self.temperature)\n            if random.random() < probability:\n                return new_x\n            else:\n                return x\n        self.temperature *= 0.99  # decrease temperature with a higher rate\n\n    def covariance_matrix_adaptation(self):\n        # covariance matrix adaptation\n        self.covariance_matrix = (1 - self.adaptive_step_size) * self.covariance_matrix + self.adaptive_step_size * np.eye(self.dim)\n        return self.covariance_matrix\n\n    def adaptive_step_size_learning(self):\n        # adaptive step size learning\n        self.adaptive_step_size += self.step_size_learning_rate * (self.evaluate(self.population[0]) - self.evaluate(self.population[-1]))\n        return self.adaptive_step_size\n\n    def optimize(self, func):\n        for _ in range(self.budget):\n            # evaluate population\n            fitness = [self.evaluate(x) for x in self.population]\n            # get best solution\n            self.best_solution = min(fitness)\n            best_index = fitness.index(self.best_solution)\n            # adaptive frequency modulation\n            self.F = self.adaptive_frequency_modulation()\n            # differential evolution\n            for i in range(self.pop_size):\n                # generate trial vector\n                trial = self.crossover(self.population[i], self.population[random.randint(0, self.pop_size - 1)])\n                # evaluate trial vector\n                trial_fitness = self.evaluate(trial)\n                # selection\n                self.population[i] = self.selection(self.population[i], trial)\n                # update best solution\n                if trial_fitness < self.evaluate(self.population[i]):\n                    self.population[i] = trial\n            # simulated annealing\n            for i in range(self.pop_size):\n                self.population[i] = self.simulated_annealing(self.population[i])\n            # covariance matrix adaptation\n            self.covariance_matrix = self.covariance_matrix_adaptation()\n            # adaptive step size learning\n            self.adaptive_step_size = self.adaptive_step_size_learning()\n        return self.best_solution\n\n    def __call__(self, func):\n        self.func = func\n        return self.optimize(func)\n\n# example usage\nbudget = 1000\ndim = 10\noptimizer = Enhanced_Adaptive_Differential_Evolution(budget, dim)\nbest_solution = optimizer(lambda x: np.sum(x**2))\nprint(\"Best solution:\", best_solution)", "name": "Enhanced_Adaptive_Differential_Evolution", "description": "Enhanced Adaptive Differential Evolution with Probabilistic Frequency Modulation, Covariance Matrix Adaptation, and Simulated Annealing with Adaptive Temperature Cooling Schedule.", "configspace": "", "generation": 13, "fitness": 0.11202378259652745, "feedback": "The algorithm Enhanced_Adaptive_Differential_Evolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.11 with standard deviation 0.07.", "error": "", "parent_id": "df117d13-1611-400f-aadc-c21c3dfd8dd8", "metadata": {"aucs": [0.23302422449897808, 0.22393072083520782, 0.23705828497997605, 0.20117422812388708, 0.19369713259044807, 0.2139113107806465, 0.21914135344842467, 0.18770822806501153, 0.2049416638841669, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.05704690922325917, 0.06776362488679566, 0.05520646858987077, 0.07809912075874603, 0.0719134900950581, 0.06269118603049006, 0.06910267082759747, 0.06828442112399591, 0.07365080232361065, 0.05299814203307529, 0.05157257142303373, 0.049465575341912316, 0.05585446202304123, 0.05943621662008225, 0.05549637737081192, 0.061698330152560765, 0.057251621434651345, 0.05177783572937411, 0.1098602905913938, 0.13436739837010936, 0.10226466873661888, 0.24855428006989677, 0.1327847327837799, 0.13283425794653503, 0.18325225790868693, 0.1115378145484246, 0.20108437229120013, 0.10084512005831947, 0.09492112877701786, 0.07954811011347374, 0.09400832082774868, 0.08729676700616096, 0.09209799724603018, 0.10963818059764963, 0.10834489131088632, 0.09963141694312194, 0.16267521329234513, 0.15506528831297295, 0.1686352438626898, 0.19745609172109402, 0.1880991766207709, 0.1561106824343904, 0.17611047893825393, 0.16818073170103476, 0.18873340919244186, 0.03406842915691688, 0.060060623596756346, 0.05002883925311363, 0.0516074340399133, 0.07666475321208488, 0.06291916958681854, 0.09722188355765016, 0.08138082651286038, 0.09732993966325432, 0.09532369255372997, 0.09261500678021473, 0.10808873998196233, 0.11563765923648739, 0.10207813824580791, 0.10797352627394907, 0.12029312879472398, 0.09521477054473815, 0.1059872061370577, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.05270469587748816, 0.07561225180392994, 0.07638220488479164, 0.0563133786594866, 0.051664534304048715, 0.040510668692816054, 0.06546207622895583, 0.06212922508010543, 0.07603336808316286, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.01135462062598469, 0.045437535468959456, 0.027856903282928558, 0.02679960084210775, 0.03789796139060708, 0.03786362595756454, 0.034560760420411896, 0.02271734737699993, 0.041576999099561696, 0.2236609736906855, 0.235112012396836, 0.23031401812146268, 0.21373825111787492, 0.2067174028147044, 0.22291672264257945, 0.26474747232712026, 0.22529026998281132, 0.20984280954480417, 0.05152557000599045, 0.05492067256214139, 0.05085692541899134, 0.059119035008149856, 0.05418746130337826, 0.07486282376079145, 0.061771229771834024, 0.05899753553561893, 0.06917276308735476, 0.12143283756091228, 0.12008124720835, 0.12699045371325335, 0.13649550245660136, 0.11695565723014656, 0.11579581629170932, 0.11450540355508365, 0.11779155202742386, 0.16404018309411095, 0.16817332225714687, 0.1691199059261853, 0.19149870397170465, 0.19218449309467545, 0.18979787206420173, 0.18462236070699978, 0.16222664967140987, 0.18063290702909485, 0.15300878887235647, 0.11554996208530843, 0.12520829901580988, 0.10497035034496494, 0.11462365755714365, 0.13202731695678394, 0.14563825159856458, 0.11661740365464612, 0.12194679862105096, 0.11781953374259102, 0.16594793709908573, 0.1702232666329193, 0.1640894367495661, 0.19420064583840257, 0.17516218234721093, 0.17278863049326976, 0.17358762796082594, 0.18724364571596985, 0.17014172768912428, 0.1554573786207304, 0.15939853155760342, 0.15418344158346842, 0.15070607795836333, 0.15987642439468253, 0.17118992112593934, 0.15712563151781977, 0.15808700702368683, 0.16030306009002193, 0.16100841388402387, 0.15247469513910383, 0.21980565098139648, 0.17899010168695317, 0.19976064950837769, 0.23594438160367914, 0.34153887905838276, 0.16617028867024242, 0.1718482057359635, 0.19299656159630363, 0.26398962072516163, 0.18727705396172267, 0.20249272947147656, 0.21256716679982268, 0.15990356695458796, 0.15108065274880866, 0.18452384661621613, 0.1613411360190634, 0.16709078009543699, 0.15963529185608571, 0.17002589621593656, 0.173296193691587, 0.1669595477938648, 0.16545520396750635, 0.17412066794869985, 0.16465305091338467, 0.17176937340838472, 0.05759165539725275, 0.06150673504010906, 0.05586368657252572, 0.0619493323209912, 0.06548474331857379, 0.06839081392434332, 0.0634451145561774, 0.06332086967447592, 0.057735908587549734]}, "mutation_prompt": null}
{"id": "d01b5f73-445e-4efc-9378-9698537b7b53", "solution": "import numpy as np\nimport random\n\nclass Enhanced_Metaheuristic_Algorithm:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.F = 0.5  # initial frequency\n        self.CR = 0.5  # initial crossover rate\n        self.pop_size = 50  # initial population size\n        self.population = np.random.uniform(-5.0, 5.0, (self.pop_size, self.dim))\n        self.best_solution = np.inf\n        self.probability = 0.2  # increased probability of probabilistic exploration\n        self.temperature = 100  # initial temperature for simulated annealing\n        self.covariance_matrix = np.eye(self.dim)  # initial covariance matrix\n        self.adaptive_step_size = 0.1  # initial adaptive step size for covariance matrix adaptation\n        self.step_size_learning_rate = 0.001  # learning rate for adaptive step size\n        self.alpha = 0.5  # initial weight for adaptive differential evolution\n\n    def evaluate(self, x):\n        return self.func(x)\n\n    def func(self, x):\n        # BBOB test suite of 24 noiseless functions\n        # Here we use the first function f1(x) = sum(x_i^2)\n        return np.sum(x**2)\n\n    def frequency_modulation(self, x):\n        # frequency modulation function\n        return self.F * np.sin(2 * np.pi * self.F * x)\n\n    def adaptive_frequency_modulation(self):\n        # adaptive frequency modulation\n        self.F *= 1.1  # increase frequency\n        if random.random() < self.probability:\n            self.F *= 0.9  # decrease frequency with probability\n        return self.F\n\n    def crossover(self, x1, x2):\n        # crossover operation\n        r = np.random.rand(self.dim)\n        return x1 + r * (x2 - x1)\n\n    def selection(self, x1, x2):\n        # selection operation\n        if self.evaluate(x2) < self.evaluate(x1):\n            return x2\n        else:\n            return x1\n\n    def simulated_annealing(self, x):\n        # simulated annealing\n        new_x = x + np.random.normal(0, 1, self.dim)\n        new_x = np.clip(new_x, -5.0, 5.0)\n        if self.evaluate(new_x) < self.evaluate(x):\n            return new_x\n        else:\n            probability = np.exp(-(self.evaluate(new_x) - self.evaluate(x)) / self.temperature)\n            if random.random() < probability:\n                return new_x\n            else:\n                return x\n        self.temperature *= 0.99  # decrease temperature with a higher rate\n\n    def covariance_matrix_adaptation(self):\n        # covariance matrix adaptation\n        self.covariance_matrix = (1 - self.adaptive_step_size) * self.covariance_matrix + self.adaptive_step_size * np.eye(self.dim)\n        return self.covariance_matrix\n\n    def adaptive_step_size_learning(self):\n        # adaptive step size learning\n        self.adaptive_step_size += self.step_size_learning_rate * (self.evaluate(self.population[0]) - self.evaluate(self.population[-1]))\n        return self.adaptive_step_size\n\n    def adaptive_differential_evolution(self):\n        # adaptive differential evolution\n        for i in range(self.pop_size):\n            # generate trial vector\n            trial = self.crossover(self.population[i], self.population[random.randint(0, self.pop_size - 1)])\n            # evaluate trial vector\n            trial_fitness = self.evaluate(trial)\n            # selection\n            self.population[i] = self.selection(self.population[i], trial)\n            # update best solution\n            if trial_fitness < self.evaluate(self.population[i]):\n                self.population[i] = trial\n            # adaptive frequency modulation\n            self.F = self.adaptive_frequency_modulation()\n            # update best solution with adaptive frequency modulation\n            if self.evaluate(self.population[i]) < self.evaluate(self.population[0]):\n                self.population[0] = self.population[i]\n\n    def optimize(self, func):\n        for _ in range(self.budget):\n            # evaluate population\n            fitness = [self.evaluate(x) for x in self.population]\n            # get best solution\n            self.best_solution = min(fitness)\n            best_index = fitness.index(self.best_solution)\n            # simulated annealing\n            for i in range(self.pop_size):\n                self.population[i] = self.simulated_annealing(self.population[i])\n            # covariance matrix adaptation\n            self.covariance_matrix = self.covariance_matrix_adaptation()\n            # adaptive step size learning\n            self.adaptive_step_size = self.adaptive_step_size_learning()\n            # adaptive differential evolution\n            self.adaptive_differential_evolution()\n        return self.best_solution\n\n    def __call__(self, func):\n        self.func = func\n        return self.optimize(func)\n\n# example usage\nbudget = 1000\ndim = 10\noptimizer = Enhanced_Metaheuristic_Algorithm(budget, dim)\nbest_solution = optimizer(lambda x: np.sum(x**2))\nprint(\"Best solution:\", best_solution)", "name": "Enhanced_Metaheuristic_Algorithm", "description": "Novel Metaheuristic Algorithm: Enhanced Adaptive Frequency Modulation with Covariance Matrix Adaptation and Simulated Annealing with Adaptive Temperature Cooling Schedule and Adaptive Differential Evolution with Probabilistic Frequency Modulation and Covariance Matrix Adaptation.", "configspace": "", "generation": 14, "fitness": 0.10702399616003923, "feedback": "The algorithm Enhanced_Metaheuristic_Algorithm got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.11 with standard deviation 0.07.", "error": "", "parent_id": "a259ae06-f4e1-498d-8b7a-55286c067997", "metadata": {"aucs": [0.229842599991659, 0.23429111348671927, 0.2284688512844124, 0.21160036259871573, 0.192369916707134, 0.19253520527399282, 0.20796620504771346, 0.20321530036672852, 0.23367393861324304, 9.999999999998899e-05, 0.002791092646579285, 9.999999999998899e-05, 9.999999999998899e-05, 0.001571093921600042, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.048696121108116164, 0.06558108711014898, 0.06178925852880912, 0.051930669800632545, 0.06804964606704234, 0.0672002650295539, 0.08177730657555626, 0.06296730341809942, 0.08304904680493475, 0.04554965052479132, 0.047138644549279185, 0.05227998075479867, 0.06992916974518193, 0.04781956546689692, 0.04121032893294263, 0.04270109328866334, 0.05238538637952239, 0.04004085611670338, 0.0836398698671521, 0.10839142365212606, 0.10314532498507945, 0.12825662367366109, 0.09725164319798896, 0.10115942474232875, 0.12185324731878133, 0.11420435909542248, 0.09436550946177269, 0.10847600347843123, 0.09593620392269675, 0.09044941740382217, 0.0993142084108869, 0.07384010560370058, 0.10106020863191334, 0.12404154960916147, 0.08694668836039166, 0.12105154686256303, 0.18965089284487224, 0.16117173399911244, 0.15627833134498637, 0.15814829817318155, 0.18986279284699448, 0.1716869343927978, 0.13120567741411382, 0.15080076068971682, 0.1920032913974744, 0.0698841191265146, 0.058376079261889924, 0.02859974214418326, 0.03555583641724003, 0.05711814699320117, 0.06050272244829158, 0.10939714981974435, 0.07211158384617355, 0.09819334738468988, 0.08766565607331323, 0.09565594176797843, 0.10386188930761275, 0.10765622607810876, 0.10096000550679396, 0.10484996374008881, 0.09398228682307908, 0.09616323374604874, 0.07499774621745459, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.08365701326277464, 0.11930193009031209, 0.08180223772177941, 0.02872466528184703, 0.04143836673448298, 0.025152066060178502, 0.07972194467326776, 0.13001386333849363, 0.06964743925060068, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.03627700460538852, 0.010242050492170751, 0.024312107633736924, 0.019362856772193515, 0.02704305754313774, 0.03428711500276371, 0.030921500692117077, 0.03235577852425486, 0.029225267921292652, 0.23600180457156517, 0.24584650410272413, 0.2247180079964638, 0.2207384123632945, 0.18352828398813248, 0.23367445870912573, 0.2360784181680421, 0.24773633261266725, 0.22045313233887687, 0.049104656709921035, 0.0495935577965978, 0.04913229208022096, 0.06943586188459339, 0.03886324707677524, 0.05459979743894805, 0.06173212131840666, 0.08091544741192591, 0.0659834477919552, 0.12438354369329974, 0.12782724441449222, 0.11842175776965747, 0.1522635590011261, 0.1537226938207965, 0.11080166843850059, 0.10737166358165373, 0.13494635210874384, 0.13436935370147696, 0.16644991899684503, 0.17572604632075395, 0.17629999080497705, 0.193574498203138, 0.18064862317618424, 0.17508712153642958, 0.15995949188044556, 0.17659185082733475, 0.17083454359185457, 0.12265226256171669, 0.1071222246072846, 0.11893217093994846, 0.12385015087118756, 0.13836108592309249, 0.1424771089925343, 0.10992533391937465, 0.13340113348326232, 0.1287372683661333, 0.17157278700038625, 0.17128054075039656, 0.17345101306567812, 0.16564215430057339, 0.1819698906778463, 0.16690717594854, 0.17307148439583375, 0.17672915869389982, 0.16950431074572336, 0.15213643993923998, 0.1475775894381013, 0.16387851522988606, 0.1508788483607837, 0.1546522258440668, 0.1614242555455887, 0.16085482121868555, 0.1535915175745184, 0.1602784015326988, 0.14794832319606832, 0.15155902230032936, 0.15824900875472048, 0.2585735182373058, 0.2029311750903794, 0.1437818993470622, 0.19905003023136492, 0.18716384086876725, 0.16440976753587022, 0.17661018484699031, 0.16496324235725313, 0.2098231345009689, 0.15562857100507466, 0.12311125850047688, 0.1396642253516739, 0.16234272393231164, 0.12666683095440512, 0.11407334551875714, 0.16410295074152292, 0.1766427264755901, 0.16629577610746438, 0.17342903624485406, 0.1678133040580465, 0.17375390150495407, 0.16124033975069196, 0.15896472891326863, 0.16316277226044185, 0.06276911064197721, 0.05930390638674454, 0.054195793998881525, 0.06410861817444224, 0.060282422240342415, 0.062393961695593525, 0.06686828248263332, 0.0711917009182953, 0.05170708546352565]}, "mutation_prompt": null}
{"id": "04967be9-1d3b-4856-a6e4-af38b5b7bee6", "solution": "import numpy as np\nimport random\n\nclass Adaptive_Frequency_Modulation_Covariance_Matrix_Adaptation:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.F = 0.5  # initial frequency\n        self.CR = 0.5  # initial crossover rate\n        self.pop_size = 50  # initial population size\n        self.population = np.random.uniform(-5.0, 5.0, (self.pop_size, self.dim))\n        self.best_solution = np.inf\n        self.probability = 0.4  # increased probability of probabilistic exploration\n        self.temperature = 100  # initial temperature for simulated annealing\n        self.covariance_matrix = np.eye(self.dim)  # initial covariance matrix\n        self.adaptive_step_size = 0.1  # initial adaptive step size for covariance matrix adaptation\n        self.step_size_learning_rate = 0.001  # learning rate for adaptive step size\n\n    def evaluate(self, x):\n        return self.func(x)\n\n    def func(self, x):\n        # BBOB test suite of 24 noiseless functions\n        # Here we use the first function f1(x) = sum(x_i^2)\n        return np.sum(x**2)\n\n    def frequency_modulation(self, x):\n        # frequency modulation function\n        return self.F * np.sin(2 * np.pi * self.F * x)\n\n    def adaptive_frequency_modulation(self):\n        # adaptive frequency modulation\n        self.F *= 1.1  # increase frequency\n        if random.random() < self.probability:\n            self.F *= 0.9  # decrease frequency with probability\n        return self.F\n\n    def crossover(self, x1, x2):\n        # crossover operation\n        r = np.random.rand(self.dim)\n        return x1 + r * (x2 - x1)\n\n    def selection(self, x1, x2):\n        # selection operation\n        if self.evaluate(x2) < self.evaluate(x1):\n            return x2\n        else:\n            return x1\n\n    def simulated_annealing(self, x):\n        # simulated annealing\n        new_x = x + np.random.normal(0, 1, self.dim)\n        new_x = np.clip(new_x, -5.0, 5.0)\n        if self.evaluate(new_x) < self.evaluate(x):\n            return new_x\n        else:\n            probability = np.exp(-(self.evaluate(new_x) - self.evaluate(x)) / self.temperature)\n            if random.random() < probability:\n                return new_x\n            else:\n                return x\n        self.temperature *= 0.99  # decrease temperature with a higher rate\n\n    def covariance_matrix_adaptation(self):\n        # covariance matrix adaptation\n        self.covariance_matrix = (1 - self.adaptive_step_size) * self.covariance_matrix + self.adaptive_step_size * np.eye(self.dim)\n        return self.covariance_matrix\n\n    def adaptive_step_size_learning(self):\n        # adaptive step size learning\n        self.adaptive_step_size += self.step_size_learning_rate * (self.evaluate(self.population[0]) - self.evaluate(self.population[-1]))\n        return self.adaptive_step_size\n\n    def optimize(self, func):\n        for _ in range(self.budget):\n            # evaluate population\n            fitness = [self.evaluate(x) for x in self.population]\n            # get best solution\n            self.best_solution = min(fitness)\n            best_index = fitness.index(self.best_solution)\n            # adaptive frequency modulation\n            self.F = self.adaptive_frequency_modulation()\n            # differential evolution\n            for i in range(self.pop_size):\n                # generate trial vector\n                trial = self.crossover(self.population[i], self.population[random.randint(0, self.pop_size - 1)])\n                # evaluate trial vector\n                trial_fitness = self.evaluate(trial)\n                # selection\n                self.population[i] = self.selection(self.population[i], trial)\n                # update best solution\n                if trial_fitness < self.evaluate(self.population[i]):\n                    self.population[i] = trial\n            # simulated annealing\n            for i in range(self.pop_size):\n                self.population[i] = self.simulated_annealing(self.population[i])\n            # covariance matrix adaptation\n            self.covariance_matrix = self.covariance_matrix_adaptation()\n            # adaptive step size learning\n            self.adaptive_step_size = self.adaptive_step_size_learning()\n            # update population with new frequency modulation\n            for i in range(self.pop_size):\n                self.population[i] = self.frequency_modulation(self.population[i])\n        return self.best_solution\n\n    def __call__(self, func):\n        self.func = func\n        return self.optimize(func)\n\n# example usage\nbudget = 1000\ndim = 10\noptimizer = Adaptive_Frequency_Modulation_Covariance_Matrix_Adaptation(budget, dim)\nbest_solution = optimizer(lambda x: np.sum(x**2))\nprint(\"Best solution:\", best_solution)", "name": "Adaptive_Frequency_Modulation_Covariance_Matrix_Adaptation", "description": "Adaptive Frequency Modulation and Covariance Matrix Adaptation with Probabilistic Frequency Modulation and Adaptive Step Size Learning", "configspace": "", "generation": 15, "fitness": 0.08377236249397225, "feedback": "The algorithm Adaptive_Frequency_Modulation_Covariance_Matrix_Adaptation got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.08 with standard deviation 0.07.", "error": "", "parent_id": "a259ae06-f4e1-498d-8b7a-55286c067997", "metadata": {"aucs": [0.22817335367546643, 0.18254235941869956, 0.21414479346205706, 0.11894836861252878, 0.10750000009520266, 0.15158619335952406, 0.1273375437089197, 0.1129396895282141, 0.09500303078871308, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.02374846190063995, 0.03924490625881294, 0.02997073213252388, 0.033440005757762625, 0.02718705930654508, 0.027376929171493924, 0.029080242524802236, 0.026142155072610196, 0.03330767757125486, 0.02161303546303539, 0.030573507939140154, 0.037348531658991435, 0.03181704749613434, 0.037958674451669894, 0.03334246523892215, 0.015846662354214835, 0.012463377010902832, 0.01302362891290998, 0.07448906857391446, 0.04569625358842, 0.05256803175301361, 0.06796248002318206, 0.055371294330787024, 0.049103426776751835, 0.07799236313728419, 0.06447217821572215, 0.04283808767450592, 0.06274794416174423, 0.02317712491622914, 0.055224131266640275, 0.08557427525180883, 0.05361214215105614, 0.04777929938043657, 9.999999999998899e-05, 0.011293333694524565, 0.02072088616875445, 0.16421176535568938, 0.16754123191318882, 0.16361044957747384, 0.0667108215001172, 0.07285119553694963, 0.0357323746722128, 0.07421038922175793, 0.08611262390464458, 0.06312250065617808, 9.999999999998899e-05, 0.0037952153408410405, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.00025322658708915036, 9.999999999998899e-05, 0.11835094425009607, 0.12911461940025204, 0.10498085513576183, 0.1366633614178383, 0.1110524149130615, 0.14650921669874128, 0.13574799673687343, 0.11717034099540302, 0.13923560533143353, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.0753819587614809, 0.023416676289500504, 0.016571040057750563, 0.04780052724758477, 0.023790688000471016, 0.024377185199953022, 0.04507109934110265, 0.011234029682620661, 0.06583018873446922, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.2079468094528808, 0.17434863713498527, 0.19037051073011124, 0.13654580770717917, 0.13334273857435586, 0.13353835887455867, 0.21810278290483442, 0.19470963304038946, 0.2143997767574296, 0.016111939985129498, 0.010830009510591432, 0.02198412468535549, 0.035263209454384215, 0.03184451624090523, 0.04758144975120926, 0.026879521818081065, 0.02507005876056756, 0.08030164759188974, 0.10166934729222354, 0.10841081714004808, 0.14161704293142519, 0.12173509101811575, 0.12417537128725453, 0.11160124152811846, 0.09927320670880668, 0.10467058921218175, 0.13098750426770844, 0.13748051507412962, 0.13697356116988413, 0.1645074430369836, 0.17125024077146211, 0.1712114447618318, 0.17336741864207839, 0.13581192436450695, 0.17377774300845283, 0.1404989700430349, 0.08597053749053107, 0.07387736959906543, 0.08398582550377798, 0.11612763888661704, 0.12178434661896032, 0.1281462132385781, 0.08841913820168579, 0.0997711199450243, 0.0740110444545149, 0.20420799801676415, 0.2019403764951757, 0.2146076246377583, 0.2223277590337932, 0.21451926638015162, 0.21591944508385275, 0.24822027975508654, 0.22642024873188105, 0.20877198149472587, 0.14593057790381003, 0.14112367514747426, 0.150267447842384, 0.14815621387392897, 0.1659056153714844, 0.1467879401630553, 0.16186659536423098, 0.1471426064017629, 0.14564070153020325, 0.12592149238273775, 0.15355497380038818, 0.1437290960097053, 0.21101908445668116, 0.1706906960414144, 0.10445623185729913, 0.10713614336874999, 0.10655818300504649, 0.10754947608513798, 0.15284918779906764, 0.18142423071284652, 0.14467314538872977, 0.11980116605251478, 0.09120097402089467, 0.07868793367986093, 0.11096437457869435, 0.11345194593200592, 0.10765279060412813, 0.1677721503103743, 0.16580857355988732, 0.1747578768102116, 0.17383043431908396, 0.18821387758793617, 0.19097672180630565, 0.18606182799005933, 0.18394230417279012, 0.16341812466101657, 0.05886663452939034, 0.06206838436221751, 0.056952420129700254, 0.059336005830291305, 0.06008278051434823, 0.055438893099910236, 0.0628624735636737, 0.06109427072795748, 0.05485153015058475]}, "mutation_prompt": null}
{"id": "b840b597-86cf-439c-9944-316b54b119a2", "solution": "import numpy as np\nimport random\n\nclass Enhanced_Adaptive_Differential_Evolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.F = 0.5  # initial frequency\n        self.CR = 0.5  # initial crossover rate\n        self.pop_size = 50  # initial population size\n        self.population = np.random.uniform(-5.0, 5.0, (self.pop_size, self.dim))\n        self.best_solution = np.inf\n        self.probability = 0.3  # increased probability of probabilistic exploration\n        self.temperature = 100  # initial temperature for simulated annealing\n        self.covariance_matrix = np.eye(self.dim)  # initial covariance matrix\n        self.adaptive_step_size = 0.1  # initial adaptive step size for covariance matrix adaptation\n        self.step_size_learning_rate = 0.005  # increased learning rate for adaptive step size\n\n    def evaluate(self, x):\n        return self.func(x)\n\n    def func(self, x):\n        # BBOB test suite of 24 noiseless functions\n        # Here we use the first function f1(x) = sum(x_i^2)\n        return np.sum(x**2)\n\n    def frequency_modulation(self, x):\n        # frequency modulation function\n        return self.F * np.sin(2 * np.pi * self.F * x)\n\n    def adaptive_frequency_modulation(self):\n        # adaptive frequency modulation\n        self.F *= 1.1  # increase frequency\n        if random.random() < self.probability:\n            self.F *= 0.9  # decrease frequency with probability\n        return self.F\n\n    def crossover(self, x1, x2):\n        # crossover operation\n        r = np.random.rand(self.dim)\n        return x1 + r * (x2 - x1)\n\n    def selection(self, x1, x2):\n        # selection operation\n        if self.evaluate(x2) < self.evaluate(x1):\n            return x2\n        else:\n            return x1\n\n    def simulated_annealing(self, x):\n        # simulated annealing\n        new_x = x + np.random.normal(0, 1, self.dim)\n        new_x = np.clip(new_x, -5.0, 5.0)\n        if self.evaluate(new_x) < self.evaluate(x):\n            return new_x\n        else:\n            probability = np.exp(-(self.evaluate(new_x) - self.evaluate(x)) / self.temperature)\n            if random.random() < probability:\n                return new_x\n            else:\n                return x\n        self.temperature *= 0.99  # decrease temperature with a higher rate\n\n    def covariance_matrix_adaptation(self):\n        # covariance matrix adaptation\n        self.covariance_matrix = (1 - self.adaptive_step_size) * self.covariance_matrix + self.adaptive_step_size * np.eye(self.dim)\n        return self.covariance_matrix\n\n    def adaptive_step_size_learning(self):\n        # adaptive step size learning\n        self.adaptive_step_size += self.step_size_learning_rate * (self.evaluate(self.population[0]) - self.evaluate(self.population[-1]))\n        return self.adaptive_step_size\n\n    def optimize(self, func):\n        for _ in range(self.budget):\n            # evaluate population\n            fitness = [self.evaluate(x) for x in self.population]\n            # get best solution\n            self.best_solution = min(fitness)\n            best_index = fitness.index(self.best_solution)\n            # adaptive frequency modulation\n            self.F = self.adaptive_frequency_modulation()\n            # differential evolution\n            for i in range(self.pop_size):\n                # generate trial vector\n                trial = self.crossover(self.population[i], self.population[random.randint(0, self.pop_size - 1)])\n                # evaluate trial vector\n                trial_fitness = self.evaluate(trial)\n                # selection\n                self.population[i] = self.selection(self.population[i], trial)\n                # update best solution\n                if trial_fitness < self.evaluate(self.population[i]):\n                    self.population[i] = trial\n            # simulated annealing\n            for i in range(self.pop_size):\n                self.population[i] = self.simulated_annealing(self.population[i])\n            # covariance matrix adaptation\n            self.covariance_matrix = self.covariance_matrix_adaptation()\n            # adaptive step size learning\n            self.adaptive_step_size = self.adaptive_step_size_learning()\n        return self.best_solution\n\n    def __call__(self, func):\n        self.func = func\n        return self.optimize(func)\n\n# example usage\nbudget = 1000\ndim = 10\noptimizer = Enhanced_Adaptive_Differential_Evolution(budget, dim)\nbest_solution = optimizer(lambda x: np.sum(x**2))\nprint(\"Best solution:\", best_solution)", "name": "Enhanced_Adaptive_Differential_Evolution", "description": "Adaptive Frequency Modulation with Covariance Matrix Adaptation and Simulated Annealing with Adaptive Temperature Cooling Schedule, Improved by Increasing the Probability of Probabilistic Exploration and Learning Rate for Adaptive Step Size.", "configspace": "", "generation": 16, "fitness": 0.11063918588168734, "feedback": "The algorithm Enhanced_Adaptive_Differential_Evolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.11 with standard deviation 0.07.", "error": "", "parent_id": "a259ae06-f4e1-498d-8b7a-55286c067997", "metadata": {"aucs": [0.25321145406109635, 0.22208698860531062, 0.2311426475536844, 0.21901348405852605, 0.2098220936420251, 0.2223399559329695, 0.2078062139192598, 0.21547370074285532, 0.20670329503239715, 0.002064278780738782, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.02701171263884372, 9.999999999998899e-05, 9.999999999998899e-05, 0.0037657857952894114, 9.999999999998899e-05, 0.05832730527417895, 0.06541312144867684, 0.05626486642487749, 0.06082138586908492, 0.06804649558384956, 0.07035887998407375, 0.09036089150483073, 0.06527438296133148, 0.07120468833255478, 0.04654201828131099, 0.06031723554597024, 0.04245043932499071, 0.05624221861408185, 0.053100336463533515, 0.04924667721218834, 0.046097057092081895, 0.04877378285000267, 0.049487700452869876, 0.0975237381757188, 0.13470929719241553, 0.0897064045571222, 0.16015401376286953, 0.5256708516592801, 0.13014457559331405, 0.2315443370872342, 0.11244926763360674, 0.12226028371067532, 0.09011390527044316, 0.10941514721361478, 0.09242921993822728, 0.08929397920639115, 0.08275384208795278, 0.09275152760539407, 0.10292601911689159, 0.1086913287686393, 0.07745134876349657, 0.14668126749694055, 0.22710149843524197, 0.14324471308865794, 0.16776475937497548, 0.2042322583200803, 0.15399621850184064, 0.16793350432840015, 0.12246979405440939, 0.12901957560538835, 0.08477503475700465, 0.04779991615926682, 0.04290213503628748, 0.07240132762153628, 0.05967535956590986, 0.05182708705933581, 0.08168447118988498, 0.07714615578012529, 0.07622432549653424, 0.11096567606501861, 0.08673401259814395, 0.0844525724868056, 0.10677668173527965, 0.10172025089123271, 0.09305265174832544, 0.10904195366394853, 0.10316611306331447, 0.10177873602704968, 9.999999999998899e-05, 0.0003093460200374043, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.10467583897660149, 0.03943631293969807, 0.0946052468194779, 0.02335175225912789, 0.04134976153341685, 0.035197276971563385, 0.08169296635403622, 0.08831842816070878, 0.07241681890318541, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.026132371532169185, 0.01528555074738669, 0.02303901201352021, 0.011998019911042324, 0.036977313031843595, 0.03784495199458071, 0.0383159344164673, 0.041181204986667774, 0.04328510733986146, 0.24447989587891794, 0.23891278069219557, 0.22179983336937248, 0.20285911797934741, 0.22507377138174012, 0.21590580118798042, 0.23406938161814927, 0.22567732362266102, 0.23957128269872696, 0.056925318814830095, 0.05460336957576495, 0.05131821757657817, 0.06758601049176682, 0.06117678676127469, 0.06353581077165416, 0.06020478108912941, 0.05558516956409998, 0.05495344146407988, 0.12732444875632287, 0.1189413249702106, 0.13570872078551255, 0.14357445819510672, 0.1463730292259584, 0.11636760782064814, 0.12328824752142087, 0.12687744282391578, 0.1079783264201496, 0.16247619384914946, 0.17498931636108206, 0.17390809261386952, 0.16558466937630145, 0.1804224793430138, 0.18100803861639503, 0.15703089192164754, 0.17890708362251517, 0.15649078748387835, 0.11369471572191492, 0.12188257213204345, 0.12305413028700152, 0.12337260757374025, 0.11975852738715242, 0.12983875388430077, 0.10223363652971074, 0.11890805432486362, 0.11055151692493059, 0.17206215212193188, 0.17538113384568055, 0.17030479898711637, 0.17789376299886117, 0.18025596084655449, 0.18771665579491026, 0.17485855171197473, 0.18705876060722637, 0.1591222262767098, 0.15691943531001162, 0.1636787695339812, 0.1511202193376996, 0.1680119681776845, 0.1661606574161011, 0.1546146360936933, 0.15694564696588276, 0.15869589063614986, 0.15387137981568633, 0.1491268645199061, 0.16708340599294202, 0.16561578448697578, 0.16409813374359616, 0.1755784997916806, 0.19000375448644335, 0.1424459119899535, 0.20361049683829935, 0.17110383823302766, 0.1725031492601422, 0.25021368629824214, 0.27161353527535936, 0.13994298839268726, 0.16277514148324823, 0.15920501354496996, 0.12537634258631147, 0.1890217063828079, 0.13163133478975908, 0.181281270412749, 0.17445443037208874, 0.16700375718493465, 0.16136920050940495, 0.16216752550741587, 0.15099497726779554, 0.17961617736816837, 0.16343142616936435, 0.1776536548188722, 0.05874103103903927, 0.07052297008183084, 0.0613410679356734, 0.07638283617992181, 0.06406061438765687, 0.062445719821845325, 0.05775470856986742, 0.0570761609446816, 0.07031921189881962]}, "mutation_prompt": null}
{"id": "2157d141-a47b-4e89-bf88-8319545281dd", "solution": "import numpy as np\nimport random\n\nclass Enhanced_Adaptive_Differential_Evolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.F = 0.5  # initial frequency\n        self.CR = 0.5  # initial crossover rate\n        self.pop_size = 50  # initial population size\n        self.population = np.random.uniform(-5.0, 5.0, (self.pop_size, self.dim))\n        self.best_solution = np.inf\n        self.probability = 0.2  # increased probability of probabilistic exploration\n        self.temperature = 100  # initial temperature for simulated annealing\n        self.covariance_matrix = np.eye(self.dim)  # initial covariance matrix\n        self.adaptive_step_size = 0.1  # initial adaptive step size for covariance matrix adaptation\n        self.step_size_learning_rate = 0.001  # learning rate for adaptive step size\n        self.mutation_rate = 0.1  # initial mutation rate\n\n    def evaluate(self, x):\n        return self.func(x)\n\n    def func(self, x):\n        # BBOB test suite of 24 noiseless functions\n        # Here we use the first function f1(x) = sum(x_i^2)\n        return np.sum(x**2)\n\n    def frequency_modulation(self, x):\n        # frequency modulation function\n        return self.F * np.sin(2 * np.pi * self.F * x)\n\n    def adaptive_frequency_modulation(self):\n        # adaptive frequency modulation\n        self.F *= 1.1  # increase frequency\n        if random.random() < self.probability:\n            self.F *= 0.9  # decrease frequency with probability\n        return self.F\n\n    def mutation(self, x):\n        # mutation operation\n        return x + np.random.normal(0, 1, self.dim)\n\n    def crossover(self, x1, x2):\n        # crossover operation\n        r = np.random.rand(self.dim)\n        return x1 + r * (x2 - x1)\n\n    def selection(self, x1, x2):\n        # selection operation\n        if self.evaluate(x2) < self.evaluate(x1):\n            return x2\n        else:\n            return x1\n\n    def simulated_annealing(self, x):\n        # simulated annealing\n        new_x = x + np.random.normal(0, 1, self.dim)\n        new_x = np.clip(new_x, -5.0, 5.0)\n        if self.evaluate(new_x) < self.evaluate(x):\n            return new_x\n        else:\n            probability = np.exp(-(self.evaluate(new_x) - self.evaluate(x)) / self.temperature)\n            if random.random() < probability:\n                return new_x\n            else:\n                return x\n        self.temperature *= 0.99  # decrease temperature with a higher rate\n\n    def covariance_matrix_adaptation(self):\n        # covariance matrix adaptation\n        self.covariance_matrix = (1 - self.adaptive_step_size) * self.covariance_matrix + self.adaptive_step_size * np.eye(self.dim)\n        return self.covariance_matrix\n\n    def adaptive_step_size_learning(self):\n        # adaptive step size learning\n        self.adaptive_step_size += self.step_size_learning_rate * (self.evaluate(self.population[0]) - self.evaluate(self.population[-1]))\n        return self.adaptive_step_size\n\n    def optimize(self, func):\n        for _ in range(self.budget):\n            # evaluate population\n            fitness = [self.evaluate(x) for x in self.population]\n            # get best solution\n            self.best_solution = min(fitness)\n            best_index = fitness.index(self.best_solution)\n            # adaptive frequency modulation\n            self.F = self.adaptive_frequency_modulation()\n            # differential evolution\n            for i in range(self.pop_size):\n                # generate trial vector\n                trial = self.mutation(self.population[i])\n                trial = self.crossover(self.population[i], self.population[random.randint(0, self.pop_size - 1)])\n                # evaluate trial vector\n                trial_fitness = self.evaluate(trial)\n                # selection\n                self.population[i] = self.selection(self.population[i], trial)\n                # update best solution\n                if trial_fitness < self.evaluate(self.population[i]):\n                    self.population[i] = trial\n            # simulated annealing\n            for i in range(self.pop_size):\n                self.population[i] = self.simulated_annealing(self.population[i])\n            # covariance matrix adaptation\n            self.covariance_matrix = self.covariance_matrix_adaptation()\n            # adaptive step size learning\n            self.adaptive_step_size = self.adaptive_step_size_learning()\n            # mutation\n            for i in range(int(self.pop_size * self.mutation_rate)):\n                self.population[i] = self.mutation(self.population[i])\n        return self.best_solution\n\n    def __call__(self, func):\n        self.func = func\n        return self.optimize(func)\n\n# example usage\nbudget = 1000\ndim = 10\noptimizer = Enhanced_Adaptive_Differential_Evolution(budget, dim)\nbest_solution = optimizer(lambda x: np.sum(x**2))\nprint(\"Best solution:\", best_solution)", "name": "Enhanced_Adaptive_Differential_Evolution", "description": "Enhanced Adaptive Differential Evolution with Probabilistic Frequency Modulation, Covariance Matrix Adaptation, and Simulated Annealing with Adaptive Temperature Cooling Schedule, now with a probability 1 to change the individual lines of the selected solution to refine its strategy.", "configspace": "", "generation": 17, "fitness": 0.10896721985514522, "feedback": "The algorithm Enhanced_Adaptive_Differential_Evolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.11 with standard deviation 0.07.", "error": "", "parent_id": "a259ae06-f4e1-498d-8b7a-55286c067997", "metadata": {"aucs": [0.22143007702522277, 0.24720473993297143, 0.2742295675048889, 0.18887663786323705, 0.25048644401583176, 0.2041644853850335, 0.21445275011570286, 0.2505841813102244, 0.2068721439784299, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.023614070087193295, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.05990237725738479, 0.07554568274428552, 0.05709653277260973, 0.07494371911497366, 0.074087491600489, 0.07191321353344116, 0.06464589777360108, 0.060555903777732456, 0.0726517311265481, 0.052671012707937725, 0.05643704792183668, 0.03890251163610792, 0.04786653203255831, 0.057855587186017465, 0.059969788569843474, 0.05389154804377816, 0.057459071794257355, 0.052688729440862114, 0.12411577167354537, 0.12409474590714331, 0.11301007691836074, 0.10890783054483999, 0.14946266339623848, 0.12647789224934658, 0.10973395933087415, 0.1298071601686248, 0.11750389315767429, 0.0754773358952554, 0.09282355617266436, 0.09379307878363108, 0.07905762722322396, 0.07588512630651689, 0.08193333821813997, 0.09340314331518618, 0.09186297325425241, 0.0983611174054817, 0.17739890753422027, 0.17618937258437184, 0.17984351391427722, 0.17038745843864744, 0.16544158793690056, 0.1640668867215621, 0.13295788284289012, 0.14018877197871293, 0.17160755038658848, 0.054506213595002406, 0.08280910646193274, 0.06751674238355498, 0.06339574050378582, 0.05149049378822823, 0.05023605026613698, 0.0995328561607226, 0.08499905698665822, 0.07913562947992248, 0.12045656827869533, 0.08471960288770464, 0.10262456875082249, 0.08928994466140117, 0.12140826118215664, 0.08137296156742246, 0.08288966004154241, 0.09200620134871129, 0.10149179133956299, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.08503582581735902, 0.07043649148006903, 0.04937095944568359, 0.03789800776607277, 0.03910532834592362, 0.04278146035495933, 0.08945097365766264, 0.07797867667924196, 0.06581989434595092, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.026689566537355658, 0.01673141718835558, 0.016957426033578527, 0.035533895765670565, 0.02884034131453206, 0.0365236147319703, 0.033810012317515015, 0.0299107039396711, 0.053651818255975425, 0.24573173280884297, 0.25079191817449364, 0.22543041045738155, 0.2236481453508028, 0.2050989198222133, 0.22922296873052217, 0.2348417062247239, 0.2252375449074594, 0.2649472992281562, 0.0459787590327736, 0.04727229528527732, 0.06344079306744155, 0.06735465070671254, 0.04972187753028323, 0.0667621997956851, 0.06139812167244307, 0.0660082125752548, 0.06237520205086167, 0.12013553411776157, 0.11184113202893664, 0.12574065163778636, 0.10579658204357179, 0.13337655004027815, 0.14506441358210775, 0.1278116756781319, 0.11166714300820202, 0.13130679949702573, 0.1667864354269898, 0.166482049545834, 0.16875350378656806, 0.20000576876059584, 0.18587292631220953, 0.177777356053619, 0.16711325433075797, 0.21437113266466634, 0.1640402651658176, 0.12072711285620985, 0.12150408265606727, 0.12174568602852487, 0.14194712454646718, 0.13459100905748655, 0.14719998680633983, 0.12479649413177363, 0.12873849636598822, 0.132423744502164, 0.18341263040826072, 0.16297260187421325, 0.17680687235759673, 0.1766488061400049, 0.16771212647982248, 0.1686706539184636, 0.21041785815836034, 0.18629070947046644, 0.1694105876233225, 0.15398697059309396, 0.16726185262195725, 0.16492260909956913, 0.16478401568211265, 0.15939100845701137, 0.16048814565261216, 0.15892950874404288, 0.160773067871398, 0.17828168169316394, 0.15579823387980962, 0.21040679776311622, 0.17031944665137522, 0.16084503936372507, 0.1336334020501201, 0.21185457020143394, 0.1779501567394265, 0.18021324163578278, 0.13769872682073037, 0.20166061473862895, 0.15859702050223679, 0.16827745570142194, 0.15124982871566295, 0.1579124888987239, 0.18439357277053314, 0.11164503584375396, 0.15564284067446588, 0.11287813306440808, 0.16590703349127056, 0.19496110887087004, 0.1736879001101912, 0.15894213762231246, 0.16081176092620886, 0.17162705863459582, 0.1740195944792713, 0.18222282173525628, 0.16365804811540274, 0.052980170561507056, 0.06904676271437271, 0.06351745508885398, 0.06342385087080527, 0.0648302430819434, 0.06165340619389714, 0.05857203798657551, 0.0641802784944373, 0.06405927664587618]}, "mutation_prompt": null}
{"id": "6ca45b9d-41ab-461d-aea1-c900ac522fce", "solution": "import numpy as np\nimport random\n\nclass Adaptive_Frequency_Modulation_Covariance_Matrix_Simulated_Annealing:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.F = 0.5  # initial frequency\n        self.CR = 0.5  # initial crossover rate\n        self.pop_size = 50  # initial population size\n        self.population = np.random.uniform(-5.0, 5.0, (self.pop_size, self.dim))\n        self.best_solution = np.inf\n        self.probability = 0.3  # increased probability of probabilistic exploration\n        self.temperature = 100  # initial temperature for simulated annealing\n        self.covariance_matrix = np.eye(self.dim)  # initial covariance matrix\n        self.adaptive_step_size = 0.1  # initial adaptive step size for covariance matrix adaptation\n        self.step_size_learning_rate = 0.001  # learning rate for adaptive step size\n        self.adaptive_temperature_cooling_rate = 0.999  # adaptive temperature cooling rate\n\n    def evaluate(self, x):\n        return self.func(x)\n\n    def func(self, x):\n        # BBOB test suite of 24 noiseless functions\n        # Here we use the first function f1(x) = sum(x_i^2)\n        return np.sum(x**2)\n\n    def frequency_modulation(self, x):\n        # frequency modulation function\n        return self.F * np.sin(2 * np.pi * self.F * x)\n\n    def adaptive_frequency_modulation(self):\n        # adaptive frequency modulation\n        self.F *= 1.1  # increase frequency\n        if random.random() < self.probability:\n            self.F *= 0.9  # decrease frequency with probability\n        return self.F\n\n    def crossover(self, x1, x2):\n        # crossover operation\n        r = np.random.rand(self.dim)\n        return x1 + r * (x2 - x1)\n\n    def selection(self, x1, x2):\n        # selection operation\n        if self.evaluate(x2) < self.evaluate(x1):\n            return x2\n        else:\n            return x1\n\n    def simulated_annealing(self, x):\n        # simulated annealing\n        new_x = x + np.random.normal(0, 1, self.dim)\n        new_x = np.clip(new_x, -5.0, 5.0)\n        if self.evaluate(new_x) < self.evaluate(x):\n            return new_x\n        else:\n            probability = np.exp(-(self.evaluate(new_x) - self.evaluate(x)) / self.temperature)\n            if random.random() < probability:\n                return new_x\n            else:\n                return x\n        self.temperature *= self.adaptive_temperature_cooling_rate  # decrease temperature with adaptive cooling rate\n\n    def covariance_matrix_adaptation(self):\n        # covariance matrix adaptation\n        self.covariance_matrix = (1 - self.adaptive_step_size) * self.covariance_matrix + self.adaptive_step_size * np.eye(self.dim)\n        return self.covariance_matrix\n\n    def adaptive_step_size_learning(self):\n        # adaptive step size learning\n        self.adaptive_step_size += self.step_size_learning_rate * (self.evaluate(self.population[0]) - self.evaluate(self.population[-1]))\n        return self.adaptive_step_size\n\n    def optimize(self, func):\n        for _ in range(self.budget):\n            # evaluate population\n            fitness = [self.evaluate(x) for x in self.population]\n            # get best solution\n            self.best_solution = min(fitness)\n            best_index = fitness.index(self.best_solution)\n            # adaptive frequency modulation\n            self.F = self.adaptive_frequency_modulation()\n            # differential evolution\n            for i in range(self.pop_size):\n                # generate trial vector\n                trial = self.crossover(self.population[i], self.population[random.randint(0, self.pop_size - 1)])\n                # evaluate trial vector\n                trial_fitness = self.evaluate(trial)\n                # selection\n                self.population[i] = self.selection(self.population[i], trial)\n                # update best solution\n                if trial_fitness < self.evaluate(self.population[i]):\n                    self.population[i] = trial\n            # simulated annealing\n            for i in range(self.pop_size):\n                self.population[i] = self.simulated_annealing(self.population[i])\n            # covariance matrix adaptation\n            self.covariance_matrix = self.covariance_matrix_adaptation()\n            # adaptive step size learning\n            self.adaptive_step_size = self.adaptive_step_size_learning()\n        return self.best_solution\n\n    def __call__(self, func):\n        self.func = func\n        return self.optimize(func)\n\n# example usage\nbudget = 1000\ndim = 10\noptimizer = Adaptive_Frequency_Modulation_Covariance_Matrix_Simulated_Annealing(budget, dim)\nbest_solution = optimizer(lambda x: np.sum(x**2))\nprint(\"Best solution:\", best_solution)", "name": "Adaptive_Frequency_Modulation_Covariance_Matrix_Simulated_Annealing", "description": "Adaptive Frequency Modulation with Covariance Matrix Adaptation and Simulated Annealing with Adaptive Temperature Cooling Schedule and Increased Probability of Probabilistic Exploration.", "configspace": "", "generation": 18, "fitness": 0.11024362409319771, "feedback": "The algorithm Adaptive_Frequency_Modulation_Covariance_Matrix_Simulated_Annealing got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.11 with standard deviation 0.07.", "error": "", "parent_id": "a259ae06-f4e1-498d-8b7a-55286c067997", "metadata": {"aucs": [0.23978465208533462, 0.2713128794257055, 0.2195966450392035, 0.19342266920501172, 0.2101369063203491, 0.19351608442101909, 0.21215382626436108, 0.2177211740884456, 0.209238892551867, 9.999999999998899e-05, 0.006503124584455899, 9.999999999998899e-05, 9.999999999998899e-05, 0.0001605185311039925, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.06277472948990104, 0.061108299193934346, 0.05845714527193213, 0.06414079614403545, 0.06987746051917898, 0.07513995211376745, 0.07031112741142764, 0.07332303630924675, 0.06296461715753043, 0.05063359079323382, 0.04666069635521508, 0.04623294750880358, 0.04830767144327075, 0.04864255473195411, 0.041068715786101495, 0.04961922206355962, 0.05108710305937225, 0.054523908996953696, 0.1661626108468508, 0.10579095481416112, 0.08152757900666929, 0.1384635946378705, 0.13504234397668846, 0.1355381462379217, 0.1754993661363271, 0.10527477517355166, 0.16052402050622472, 0.08908033706215324, 0.08559184187524371, 0.09706879678811564, 0.08612552518133143, 0.07799667398164878, 0.08540782457474339, 0.08982363316354536, 0.09415029170913658, 0.10983100456747674, 0.14225663563549662, 0.16031367055080192, 0.16542749764411058, 0.21046751964247146, 0.17570781660833945, 0.15397289469631925, 0.1573469176136132, 0.1457801421163376, 0.1540892781158445, 0.048904199644812074, 0.06999583509522433, 0.08316065414510454, 0.04619796754639405, 0.0339025871344536, 0.07899927051085176, 0.10115833229662785, 0.0798563444218422, 0.08738283381145462, 0.10390531588148755, 0.08630819292159775, 0.10440998037723148, 0.10502466375300523, 0.0998353508045211, 0.09328073714797736, 0.0931208776685799, 0.11878425118810698, 0.10908607850215157, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.03292272694544385, 9.999999999998899e-05, 0.0006360652838282022, 0.09355978794566111, 0.045141195693986225, 0.08006333277168654, 0.042416835397790065, 0.049680649701416724, 0.03940302001422746, 0.08892218635268256, 0.05235130351524253, 0.061507443460652955, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.02266788279846421, 0.019641600367872547, 0.045980194386799456, 0.028100415552463343, 0.040410598762193106, 0.05430539963478165, 0.035173276471607506, 0.040177617988986114, 0.03559005457544406, 0.2424545558478738, 0.2578748823212208, 0.2420297146189636, 0.242355682842046, 0.2218723151038855, 0.21200324692330463, 0.2556996899764393, 0.22988679008302038, 0.24379183563880324, 0.06086098641714088, 0.052000068723423776, 0.04647995638705593, 0.06662329380876786, 0.06797098215654807, 0.05799425911967082, 0.056891644823318877, 0.07416323745196096, 0.06566249331356377, 0.12580021656610374, 0.1116035670024732, 0.1372413927692021, 0.12402669539480271, 0.11862257859899417, 0.14593680076458093, 0.13501009016365806, 0.12170595101506021, 0.13500998796891184, 0.16062277878443942, 0.17823489785919122, 0.16634239266760997, 0.16715836098564507, 0.19521627647011053, 0.18513895857782725, 0.17098870682442158, 0.18711644061629673, 0.16537648574422237, 0.10618624697551149, 0.12075978397082143, 0.12502466183804883, 0.1352916414222911, 0.14530967150597085, 0.1488926298919565, 0.10993575629120422, 0.118659126128139, 0.12910840976083582, 0.2152228551574764, 0.17681614906048237, 0.17826871228466246, 0.1765489183542739, 0.17490514431995252, 0.16949621182698393, 0.24344936890134072, 0.15529151470600155, 0.18322944303663502, 0.15229097878098985, 0.16371620059103242, 0.1559674854151808, 0.1584273161865205, 0.16916902767849096, 0.15970443565527026, 0.1668065570114613, 0.15934551982059342, 0.16312421394000087, 0.15461161274931057, 0.1640683533061401, 0.15938522053814907, 0.2565890166711239, 0.17067020564842006, 0.1342917187356416, 0.18748602782492974, 0.19122136697844883, 0.14985585837073923, 0.15710935046157504, 0.18363932572668962, 0.16451688882082438, 0.2120807647196431, 0.16324860581573863, 0.17874448206761684, 0.13024085354510118, 0.16421493583639057, 0.15618269456407563, 0.17087060984439884, 0.16099921969601438, 0.18902278875639045, 0.16373736159266827, 0.18117527209516104, 0.16265959217253367, 0.17380193561426427, 0.17528894936137673, 0.17630742610548733, 0.0639087839388438, 0.06073496994343508, 0.06757923694921175, 0.06093070334707262, 0.057500867723324856, 0.05763792353975572, 0.06532848287971116, 0.05627294512787906, 0.05504455801800101]}, "mutation_prompt": null}
{"id": "83c33b72-6288-41c3-b4f1-7cf3d4cb71d2", "solution": "import numpy as np\nimport random\n\nclass Adaptive_Frequency_Modulation_Covariance_Matrix_Adaptation_Simulated_Annealing_Probabilistic_Exploration:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.F = 0.5  # initial frequency\n        self.CR = 0.5  # initial crossover rate\n        self.pop_size = 50  # initial population size\n        self.population = np.random.uniform(-5.0, 5.0, (self.pop_size, self.dim))\n        self.best_solution = np.inf\n        self.probability = 0.2  # increased probability of probabilistic exploration\n        self.temperature = 100  # initial temperature for simulated annealing\n        self.covariance_matrix = np.eye(self.dim)  # initial covariance matrix\n        self.adaptive_step_size = 0.1  # initial adaptive step size for covariance matrix adaptation\n        self.step_size_learning_rate = 0.001  # learning rate for adaptive step size\n        self.probability_learning_rate = 0.001  # learning rate for probability\n\n    def evaluate(self, x):\n        return self.func(x)\n\n    def func(self, x):\n        # BBOB test suite of 24 noiseless functions\n        # Here we use the first function f1(x) = sum(x_i^2)\n        return np.sum(x**2)\n\n    def frequency_modulation(self, x):\n        # frequency modulation function\n        return self.F * np.sin(2 * np.pi * self.F * x)\n\n    def adaptive_frequency_modulation(self):\n        # adaptive frequency modulation\n        self.F *= 1.1  # increase frequency\n        if random.random() < self.probability:\n            self.F *= 0.9  # decrease frequency with probability\n        return self.F\n\n    def adaptive_probability_modulation(self):\n        # adaptive probability modulation\n        self.probability += self.probability_learning_rate  # increase probability\n        if random.random() < self.probability:\n            self.probability *= 0.9  # decrease probability with probability\n        return self.probability\n\n    def crossover(self, x1, x2):\n        # crossover operation\n        r = np.random.rand(self.dim)\n        return x1 + r * (x2 - x1)\n\n    def selection(self, x1, x2):\n        # selection operation\n        if self.evaluate(x2) < self.evaluate(x1):\n            return x2\n        else:\n            return x1\n\n    def simulated_annealing(self, x):\n        # simulated annealing\n        new_x = x + np.random.normal(0, 1, self.dim)\n        new_x = np.clip(new_x, -5.0, 5.0)\n        if self.evaluate(new_x) < self.evaluate(x):\n            return new_x\n        else:\n            probability = np.exp(-(self.evaluate(new_x) - self.evaluate(x)) / self.temperature)\n            if random.random() < probability:\n                return new_x\n            else:\n                return x\n        self.temperature *= 0.99  # decrease temperature with a higher rate\n\n    def covariance_matrix_adaptation(self):\n        # covariance matrix adaptation\n        self.covariance_matrix = (1 - self.adaptive_step_size) * self.covariance_matrix + self.adaptive_step_size * np.eye(self.dim)\n        return self.covariance_matrix\n\n    def adaptive_step_size_learning(self):\n        # adaptive step size learning\n        self.adaptive_step_size += self.step_size_learning_rate * (self.evaluate(self.population[0]) - self.evaluate(self.population[-1]))\n        return self.adaptive_step_size\n\n    def probability_learning(self):\n        # probability learning\n        self.probability_learning_rate += self.step_size_learning_rate * (self.evaluate(self.population[0]) - self.evaluate(self.population[-1]))\n        return self.probability_learning_rate\n\n    def optimize(self, func):\n        for _ in range(self.budget):\n            # evaluate population\n            fitness = [self.evaluate(x) for x in self.population]\n            # get best solution\n            self.best_solution = min(fitness)\n            best_index = fitness.index(self.best_solution)\n            # adaptive frequency modulation\n            self.F = self.adaptive_frequency_modulation()\n            # adaptive probability modulation\n            self.probability = self.adaptive_probability_modulation()\n            # differential evolution\n            for i in range(self.pop_size):\n                # generate trial vector\n                trial = self.crossover(self.population[i], self.population[random.randint(0, self.pop_size - 1)])\n                # evaluate trial vector\n                trial_fitness = self.evaluate(trial)\n                # selection\n                self.population[i] = self.selection(self.population[i], trial)\n                # update best solution\n                if trial_fitness < self.evaluate(self.population[i]):\n                    self.population[i] = trial\n            # simulated annealing\n            for i in range(self.pop_size):\n                self.population[i] = self.simulated_annealing(self.population[i])\n            # covariance matrix adaptation\n            self.covariance_matrix = self.covariance_matrix_adaptation()\n            # adaptive step size learning\n            self.adaptive_step_size = self.adaptive_step_size_learning()\n            # probability learning\n            self.probability_learning_rate = self.probability_learning()\n        return self.best_solution\n\n    def __call__(self, func):\n        self.func = func\n        return self.optimize(func)\n\n# example usage\nbudget = 1000\ndim = 10\noptimizer = Adaptive_Frequency_Modulation_Covariance_Matrix_Adaptation_Simulated_Annealing_Probabilistic_Exploration(budget, dim)\nbest_solution = optimizer(lambda x: np.sum(x**2))\nprint(\"Best solution:\", best_solution)", "name": "Adaptive_Frequency_Modulation_Covariance_Matrix_Adaptation_Simulated_Annealing_Probabilistic_Exploration", "description": "Adaptive Frequency Modulation and Covariance Matrix Adaptation with Simulated Annealing and Probabilistic Exploration (AFMCASPE)", "configspace": "", "generation": 19, "fitness": 0.1110557705894416, "feedback": "The algorithm Adaptive_Frequency_Modulation_Covariance_Matrix_Adaptation_Simulated_Annealing_Probabilistic_Exploration got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.11 with standard deviation 0.07.", "error": "", "parent_id": "a259ae06-f4e1-498d-8b7a-55286c067997", "metadata": {"aucs": [0.2667646211938831, 0.2471674979000894, 0.22738783422305875, 0.19667469685299788, 0.20390394221778452, 0.2004886141058848, 0.22968136717583054, 0.19163858094117592, 0.2084589691836266, 9.999999999998899e-05, 0.001607203031814941, 9.999999999998899e-05, 9.999999999998899e-05, 0.0015486342335770953, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.05727346498261299, 0.0643648220284645, 0.04601642816464502, 0.07000648727483005, 0.07507557651667285, 0.06948559335971916, 0.06837754998762358, 0.07149496997330118, 0.06910902093332005, 0.050278743672827475, 0.047712462362569985, 0.07303444529249103, 0.05737668931144113, 0.052170096245688136, 0.05186567473714243, 0.05045291599988033, 0.06535686753587089, 0.04353563626863344, 0.20130464061757092, 0.11339012312074481, 0.09271598980696749, 0.12358508657974088, 0.17724840400362885, 0.11842478997943129, 0.1668959187790542, 0.11898333154582275, 0.4165199662600476, 0.11119268464850318, 0.09123785599293188, 0.07496194560676073, 0.07480527525070446, 0.08803416662721941, 0.09284679897228876, 0.10538168855897634, 0.09868234121576991, 0.09897494627266101, 0.14239259138870275, 0.1783544962061192, 0.18307038097206818, 0.20333147871308366, 0.17067512339900748, 0.15795372891166004, 0.13676330664468528, 0.15318227227302628, 0.1480714375379545, 0.05278656644581026, 0.08564061009870294, 0.04854598288393863, 0.06356009394286688, 0.051714264346362504, 0.042325993278575114, 0.08359400677219242, 0.13370130706760663, 0.08080804836647626, 0.12182870140804569, 0.1021597357014623, 0.06893193939236752, 0.10489366190530558, 0.09979355996216632, 0.11008512477850041, 0.1030324693520912, 0.09278900853741456, 0.09929444749835492, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.06613133354392775, 0.05798616386283262, 0.07574432753605265, 0.03205409839461626, 0.05520706336630132, 0.04352495941852086, 0.05797976636807867, 0.08923396087965307, 0.07148233398218895, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.0034971797033648278, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.032265708662576054, 0.030152346074442504, 0.02673300591710981, 0.029419034590140614, 0.0481032797336004, 0.03414391706450026, 0.0326051229458022, 0.03226570024715525, 0.03394107141087099, 0.2298727800527296, 0.2225681843083619, 0.21801095352248223, 0.2225578330160668, 0.21451495953369826, 0.202417593061056, 0.24166878430627725, 0.23228370819376865, 0.23577063668194542, 0.05844558279223888, 0.05500600725719429, 0.053931380205511026, 0.06142849861968991, 0.07598980853344406, 0.06756176988863827, 0.060027436048137095, 0.0638181957779016, 0.059956132358049086, 0.12419918114573358, 0.1216953265732329, 0.12650884115001937, 0.11230382815027717, 0.11067349112672853, 0.1186261082712281, 0.1445019134958646, 0.15919463502566777, 0.0968193206290836, 0.17416420821663992, 0.17307288731806691, 0.1767833311441962, 0.1958834508745121, 0.18622277673132703, 0.1839882583023178, 0.15477879715230758, 0.17844129393106412, 0.16960319144941527, 0.10971979917220287, 0.1043589251455953, 0.13817799195586777, 0.1402795600480362, 0.14618982525437618, 0.1520878508413167, 0.1264311102383926, 0.1418593032367902, 0.11580991807286123, 0.18067476367018276, 0.16648922822305634, 0.18667598370494765, 0.17433615119800505, 0.17135354689864657, 0.16364151974727736, 0.1808532240666103, 0.16141289461745278, 0.1633325289105474, 0.16181594297552715, 0.15797344427508053, 0.1706412772847823, 0.16542877234327102, 0.15552012521907632, 0.15798118075409928, 0.16484530538019393, 0.1653017180039129, 0.15331858520621977, 0.16242999568171212, 0.1968881446741063, 0.1602430838650991, 0.24480689161264313, 0.22124846182717173, 0.20574141508192256, 0.22387690912727276, 0.15822757226612594, 0.15220088448748648, 0.19213704046406188, 0.23997203385494492, 0.16172342992997601, 0.1524898457013536, 0.1277532880659975, 0.12295281398310087, 0.12176189815508187, 0.18357759083715108, 0.1238365493797865, 0.17442744888916184, 0.17045135300965797, 0.18033043072948607, 0.16521976146012762, 0.16882588778931717, 0.17088065248539008, 0.16763094249470012, 0.16278465864477798, 0.17579537285588154, 0.07538539939359234, 0.06003812511196993, 0.0520655065038611, 0.05318810602144164, 0.05052876327419731, 0.05315520170175292, 0.06765290396053047, 0.06379921287156687, 0.06657123880521265]}, "mutation_prompt": null}
{"id": "77c8f75d-37e0-4953-b10d-1f88b22b415c", "solution": "import numpy as np\nimport random\n\nclass Adaptive_Frequency_Modulation_Enhanced_Adaptive_Differential_Evolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.F = 0.5  # initial frequency\n        self.CR = 0.5  # initial crossover rate\n        self.pop_size = 50  # initial population size\n        self.population = np.random.uniform(-5.0, 5.0, (self.pop_size, self.dim))\n        self.best_solution = np.inf\n        self.probability = 0.2  # increased probability of probabilistic exploration\n        self.temperature = 100  # initial temperature for simulated annealing\n        self.covariance_matrix = np.eye(self.dim)  # initial covariance matrix\n        self.adaptive_step_size = 0.1  # initial adaptive step size for covariance matrix adaptation\n        self.step_size_learning_rate = 0.001  # learning rate for adaptive step size\n        self.adaptive_frequency_modulation_rate = 0.01  # rate of change for adaptive frequency modulation\n\n    def evaluate(self, x):\n        return self.func(x)\n\n    def func(self, x):\n        # BBOB test suite of 24 noiseless functions\n        # Here we use the first function f1(x) = sum(x_i^2)\n        return np.sum(x**2)\n\n    def frequency_modulation(self, x):\n        # frequency modulation function\n        return self.F * np.sin(2 * np.pi * self.F * x)\n\n    def adaptive_frequency_modulation(self):\n        # adaptive frequency modulation\n        self.F += self.adaptive_frequency_modulation_rate * np.sin(2 * np.pi * self.adaptive_frequency_modulation_rate * self.budget)  # increase frequency\n        if random.random() < self.probability:\n            self.F *= 0.9  # decrease frequency with probability\n        return self.F\n\n    def crossover(self, x1, x2):\n        # crossover operation\n        r = np.random.rand(self.dim)\n        return x1 + r * (x2 - x1)\n\n    def selection(self, x1, x2):\n        # selection operation\n        if self.evaluate(x2) < self.evaluate(x1):\n            return x2\n        else:\n            return x1\n\n    def simulated_annealing(self, x):\n        # simulated annealing\n        new_x = x + np.random.normal(0, 1, self.dim)\n        new_x = np.clip(new_x, -5.0, 5.0)\n        if self.evaluate(new_x) < self.evaluate(x):\n            return new_x\n        else:\n            probability = np.exp(-(self.evaluate(new_x) - self.evaluate(x)) / self.temperature)\n            if random.random() < probability:\n                return new_x\n            else:\n                return x\n        self.temperature *= 0.99  # decrease temperature with a higher rate\n\n    def covariance_matrix_adaptation(self):\n        # covariance matrix adaptation\n        self.covariance_matrix = (1 - self.adaptive_step_size) * self.covariance_matrix + self.adaptive_step_size * np.eye(self.dim)\n        return self.covariance_matrix\n\n    def adaptive_step_size_learning(self):\n        # adaptive step size learning\n        self.adaptive_step_size += self.step_size_learning_rate * (self.evaluate(self.population[0]) - self.evaluate(self.population[-1]))\n        return self.adaptive_step_size\n\n    def optimize(self, func):\n        for _ in range(self.budget):\n            # evaluate population\n            fitness = [self.evaluate(x) for x in self.population]\n            # get best solution\n            self.best_solution = min(fitness)\n            best_index = fitness.index(self.best_solution)\n            # adaptive frequency modulation\n            self.F = self.adaptive_frequency_modulation()\n            # differential evolution\n            for i in range(self.pop_size):\n                # generate trial vector\n                trial = self.crossover(self.population[i], self.population[random.randint(0, self.pop_size - 1)])\n                # evaluate trial vector\n                trial_fitness = self.evaluate(trial)\n                # selection\n                self.population[i] = self.selection(self.population[i], trial)\n                # update best solution\n                if trial_fitness < self.evaluate(self.population[i]):\n                    self.population[i] = trial\n            # simulated annealing\n            for i in range(self.pop_size):\n                self.population[i] = self.simulated_annealing(self.population[i])\n            # covariance matrix adaptation\n            self.covariance_matrix = self.covariance_matrix_adaptation()\n            # adaptive step size learning\n            self.adaptive_step_size = self.adaptive_step_size_learning()\n        return self.best_solution\n\n    def __call__(self, func):\n        self.func = func\n        return self.optimize(func)\n\n# example usage\nbudget = 1000\ndim = 10\noptimizer = Adaptive_Frequency_Modulation_Enhanced_Adaptive_Differential_Evolution(budget, dim)\nbest_solution = optimizer(lambda x: np.sum(x**2))\nprint(\"Best solution:\", best_solution)", "name": "Adaptive_Frequency_Modulation_Enhanced_Adaptive_Differential_Evolution", "description": "Adaptive Frequency Modulation Enhanced Adaptive Differential Evolution with Covariance Matrix Adaptation and Simulated Annealing with Adaptive Temperature Cooling Schedule", "configspace": "", "generation": 20, "fitness": 0.11047431272618427, "feedback": "The algorithm Adaptive_Frequency_Modulation_Enhanced_Adaptive_Differential_Evolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.11 with standard deviation 0.07.", "error": "", "parent_id": "a259ae06-f4e1-498d-8b7a-55286c067997", "metadata": {"aucs": [0.2425856925438955, 0.2256120040389098, 0.22880092127691642, 0.1989117115907748, 0.21542253083401586, 0.1961596339678694, 0.21948739645486914, 0.26897651881916496, 0.20473143424752993, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.06685302481239852, 0.05776145199623883, 0.0632004298730795, 0.0703700483748767, 0.05486075772461807, 0.07507062563166322, 0.07996486459575392, 0.05966289212135456, 0.057078747181813716, 0.04331632411511788, 0.054471104413794436, 0.04690035747627208, 0.049397161543787704, 0.04865815047826105, 0.0496875914325432, 0.056646401291560844, 0.0568888466311499, 0.05932549761845074, 0.1087665635652163, 0.11875123283338052, 0.0860317387003644, 0.13948395840627614, 0.11978274771925956, 0.11353120074409129, 0.12841529165761967, 0.13633557742004454, 0.4121817945592615, 0.0952059647057566, 0.09014182675551996, 0.09334984314044192, 0.08833647975176007, 0.0708834523400057, 0.08282445417327855, 0.09666519100906135, 0.09728725179365816, 0.09327864728001645, 0.1602752185238282, 0.18901633689869768, 0.13583824376154752, 0.17637302804105948, 0.1690508903796928, 0.1820344940090991, 0.1341206868738074, 0.16190776324263356, 0.18053506139330033, 0.06905214312848151, 0.06019695505053424, 0.0781875314858016, 0.024204316536215464, 0.05307371591245724, 0.020083907369152998, 0.06314193702428217, 0.051251075636216004, 0.05971409327613075, 0.09580918848337605, 0.13111742462747333, 0.08383877680412133, 0.1029230324103717, 0.1005991043424338, 0.1042733739377929, 0.11375401405970786, 0.11224920501905222, 0.10420189324691487, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.08774468106634781, 0.07763207265086769, 0.06715698957146066, 0.048838663720854325, 0.05605101814161595, 0.05247886303736626, 0.08428026454438864, 0.06612866502471149, 0.05711532077175374, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.04003113852755236, 0.01861796329959231, 0.017750164346925668, 0.017952649608028337, 0.032082557992760274, 0.05067759472373834, 0.03342079619439631, 0.03822609184607062, 0.0478110008549687, 0.2331442486946127, 0.25128154911470746, 0.22904564405569916, 0.2131799895530132, 0.2158888462249755, 0.2273528019002592, 0.24662342040489937, 0.2180054619218904, 0.21913408310473392, 0.04687539220093817, 0.061031295072118685, 0.05502571073059259, 0.05736388614069987, 0.0575065513764641, 0.08037494083129038, 0.07799266384422354, 0.06175228880111916, 0.08357869187882094, 0.16116908415188624, 0.11769598703347217, 0.14152783711885264, 0.11763146583567519, 0.12114064344033726, 0.11184780281715079, 0.12165474877429183, 0.13433590362162717, 0.15245106529770946, 0.15894547181662633, 0.18102562246000198, 0.17272211237764168, 0.1873108753572168, 0.17844645426555794, 0.1716363927117085, 0.1661617967874397, 0.18196878370483283, 0.1676301001837428, 0.11216254199205034, 0.12596271389475877, 0.12893787083694463, 0.1277597834412738, 0.12575761871173308, 0.13356505496733118, 0.12030620776311896, 0.12908352562755554, 0.11643593430744914, 0.16372159242117557, 0.1733482324094291, 0.18303045524805872, 0.19429105615357434, 0.17748331250510652, 0.1708432301095263, 0.17329870499025712, 0.16438031839810197, 0.17483929438707624, 0.1608164553192386, 0.16108385875068998, 0.15871781283222386, 0.15289867412157365, 0.16369057557812738, 0.15725622114611593, 0.15541859476680298, 0.16152617708879302, 0.16096718252128828, 0.17123212937855403, 0.16256276326588737, 0.16997726710856353, 0.20170506257270793, 0.19547472419126766, 0.25573047550002015, 0.2119846311829292, 0.18783028129288049, 0.13659635478600685, 0.15510634157189374, 0.19353300130562523, 0.18495384365733603, 0.16381807757440003, 0.15624028506654886, 0.13566224274223782, 0.20011551830741658, 0.1617319776750903, 0.14058913519331595, 0.15929102740895684, 0.16741791477470092, 0.16015761490903457, 0.17545750493877876, 0.19300635429311697, 0.16234432280227773, 0.17182684961080663, 0.19019562872108087, 0.1614553947879913, 0.06337592530532687, 0.055174865790729055, 0.060006151325918466, 0.08147609787841337, 0.07498884289340513, 0.0715180371228884, 0.06341810384461166, 0.06186328138563513, 0.06101565975396983]}, "mutation_prompt": null}
{"id": "d1016990-e76d-4c24-91df-a2d15cdfb04f", "solution": "import numpy as np\nimport random\n\nclass Novel_Adaptive_Differential_Evolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.F = 0.5  # initial frequency\n        self.CR = 0.5  # initial crossover rate\n        self.pop_size = 50  # initial population size\n        self.population = np.random.uniform(-5.0, 5.0, (self.pop_size, self.dim))\n        self.best_solution = np.inf\n        self.probability = 0.2  # increased probability of probabilistic exploration\n        self.temperature = 100  # initial temperature for simulated annealing\n        self.covariance_matrix = np.eye(self.dim)  # initial covariance matrix\n        self.adaptive_step_size = 0.1  # initial adaptive step size for covariance matrix adaptation\n        self.step_size_learning_rate = 0.001  # learning rate for adaptive step size\n        self.mutation_rate = 0.1  # mutation rate for multi-objective optimization\n        self.archive_size = 10  # archive size for multi-objective optimization\n\n    def evaluate(self, x):\n        return self.func(x)\n\n    def func(self, x):\n        # BBOB test suite of 24 noiseless functions\n        # Here we use the first function f1(x) = sum(x_i^2)\n        return np.sum(x**2)\n\n    def frequency_modulation(self, x):\n        # frequency modulation function\n        return self.F * np.sin(2 * np.pi * self.F * x)\n\n    def adaptive_frequency_modulation(self):\n        # adaptive frequency modulation\n        self.F *= 1.1  # increase frequency\n        if random.random() < self.probability:\n            self.F *= 0.9  # decrease frequency with probability\n        return self.F\n\n    def crossover(self, x1, x2):\n        # crossover operation\n        r = np.random.rand(self.dim)\n        return x1 + r * (x2 - x1)\n\n    def selection(self, x1, x2):\n        # selection operation\n        if self.evaluate(x2) < self.evaluate(x1):\n            return x2\n        else:\n            return x1\n\n    def simulated_annealing(self, x):\n        # simulated annealing\n        new_x = x + np.random.normal(0, 1, self.dim)\n        new_x = np.clip(new_x, -5.0, 5.0)\n        if self.evaluate(new_x) < self.evaluate(x):\n            return new_x\n        else:\n            probability = np.exp(-(self.evaluate(new_x) - self.evaluate(x)) / self.temperature)\n            if random.random() < probability:\n                return new_x\n            else:\n                return x\n        self.temperature *= 0.99  # decrease temperature with a higher rate\n\n    def covariance_matrix_adaptation(self):\n        # covariance matrix adaptation\n        self.covariance_matrix = (1 - self.adaptive_step_size) * self.covariance_matrix + self.adaptive_step_size * np.eye(self.dim)\n        return self.covariance_matrix\n\n    def adaptive_step_size_learning(self):\n        # adaptive step size learning\n        self.adaptive_step_size += self.step_size_learning_rate * (self.evaluate(self.population[0]) - self.evaluate(self.population[-1]))\n        return self.adaptive_step_size\n\n    def mutation(self, x):\n        # mutation operation for multi-objective optimization\n        return x + np.random.normal(0, 1, self.dim) * self.mutation_rate\n\n    def archive(self, x):\n        # archive operation for multi-objective optimization\n        if len(self.archive) < self.archive_size:\n            self.archive.append(x)\n        else:\n            self.archive.sort(key=lambda x: self.evaluate(x))\n            if self.evaluate(x) < self.evaluate(self.archive[-1]):\n                self.archive[-1] = x\n\n    def optimize(self, func):\n        self.archive = []\n        for _ in range(self.budget):\n            # evaluate population\n            fitness = [self.evaluate(x) for x in self.population]\n            # get best solution\n            self.best_solution = min(fitness)\n            best_index = fitness.index(self.best_solution)\n            # adaptive frequency modulation\n            self.F = self.adaptive_frequency_modulation()\n            # differential evolution\n            for i in range(self.pop_size):\n                # generate trial vector\n                trial = self.crossover(self.population[i], self.population[random.randint(0, self.pop_size - 1)])\n                # evaluate trial vector\n                trial_fitness = self.evaluate(trial)\n                # selection\n                self.population[i] = self.selection(self.population[i], trial)\n                # update best solution\n                if trial_fitness < self.evaluate(self.population[i]):\n                    self.population[i] = trial\n            # simulated annealing\n            for i in range(self.pop_size):\n                self.population[i] = self.simulated_annealing(self.population[i])\n            # covariance matrix adaptation\n            self.covariance_matrix = self.covariance_matrix_adaptation()\n            # adaptive step size learning\n            self.adaptive_step_size = self.adaptive_step_size_learning()\n            # mutation and archive\n            for i in range(self.pop_size):\n                trial = self.mutation(self.population[i])\n                self.archive(trial)\n                if len(self.archive) > self.archive_size:\n                    self.archive.pop(0)\n        return self.best_solution\n\n    def __call__(self, func):\n        self.func = func\n        return self.optimize(func)\n\n# example usage\nbudget = 1000\ndim = 10\noptimizer = Novel_Adaptive_Differential_Evolution(budget, dim)\nbest_solution = optimizer(lambda x: np.sum(x**2))\nprint(\"Best solution:\", best_solution)", "name": "Novel_Adaptive_Differential_Evolution", "description": "Novel Adaptive Differential Evolution with Probabilistic Frequency Modulation, Covariance Matrix Adaptation, Simulated Annealing, and Adaptive Temperature Cooling Schedule, with an additional Multi-Objective Optimization inspired by the concept of Pareto optimality to handle multiple conflicting objectives.", "configspace": "", "generation": 21, "fitness": -Infinity, "feedback": "An exception occurred: TypeError(\"'list' object is not callable\").", "error": "TypeError(\"'list' object is not callable\")", "parent_id": "a259ae06-f4e1-498d-8b7a-55286c067997", "metadata": {}, "mutation_prompt": null}
{"id": "1da4f0fd-be72-4c52-901d-628e7f41884d", "solution": "import numpy as np\nimport random\n\nclass Adaptive_Frequency_Modulation_with_Probabilistic_Exploration:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.F = 0.5  # initial frequency\n        self.CR = 0.5  # initial crossover rate\n        self.pop_size = 50  # initial population size\n        self.population = np.random.uniform(-5.0, 5.0, (self.pop_size, self.dim))\n        self.best_solution = np.inf\n        self.probability = 0.3  # increased probability of probabilistic exploration\n        self.temperature = 100  # initial temperature for simulated annealing\n        self.covariance_matrix = np.eye(self.dim)  # initial covariance matrix\n        self.adaptive_step_size = 0.1  # initial adaptive step size for covariance matrix adaptation\n        self.step_size_learning_rate = 0.001  # learning rate for adaptive step size\n        self.exploration_rate = 0.2  # rate of probabilistic exploration\n\n    def evaluate(self, x):\n        return self.func(x)\n\n    def func(self, x):\n        # BBOB test suite of 24 noiseless functions\n        # Here we use the first function f1(x) = sum(x_i^2)\n        return np.sum(x**2)\n\n    def frequency_modulation(self, x):\n        # frequency modulation function\n        return self.F * np.sin(2 * np.pi * self.F * x)\n\n    def adaptive_frequency_modulation(self):\n        # adaptive frequency modulation\n        self.F *= 1.1  # increase frequency\n        if random.random() < self.probability:\n            self.F *= 0.9  # decrease frequency with probability\n        return self.F\n\n    def crossover(self, x1, x2):\n        # crossover operation\n        r = np.random.rand(self.dim)\n        return x1 + r * (x2 - x1)\n\n    def selection(self, x1, x2):\n        # selection operation\n        if self.evaluate(x2) < self.evaluate(x1):\n            return x2\n        else:\n            return x1\n\n    def simulated_annealing(self, x):\n        # simulated annealing\n        new_x = x + np.random.normal(0, 1, self.dim)\n        new_x = np.clip(new_x, -5.0, 5.0)\n        if self.evaluate(new_x) < self.evaluate(x):\n            return new_x\n        else:\n            probability = np.exp(-(self.evaluate(new_x) - self.evaluate(x)) / self.temperature)\n            if random.random() < probability:\n                return new_x\n            else:\n                return x\n        self.temperature *= 0.99  # decrease temperature with a higher rate\n\n    def covariance_matrix_adaptation(self):\n        # covariance matrix adaptation\n        self.covariance_matrix = (1 - self.adaptive_step_size) * self.covariance_matrix + self.adaptive_step_size * np.eye(self.dim)\n        return self.covariance_matrix\n\n    def adaptive_step_size_learning(self):\n        # adaptive step size learning\n        self.adaptive_step_size += self.step_size_learning_rate * (self.evaluate(self.population[0]) - self.evaluate(self.population[-1]))\n        return self.adaptive_step_size\n\n    def probabilistic_exploration(self):\n        # probabilistic exploration\n        if random.random() < self.exploration_rate:\n            new_solution = np.random.uniform(-5.0, 5.0, self.dim)\n            if self.evaluate(new_solution) < self.evaluate(self.population[0]):\n                self.population[0] = new_solution\n                return True\n        return False\n\n    def optimize(self, func):\n        for _ in range(self.budget):\n            # evaluate population\n            fitness = [self.evaluate(x) for x in self.population]\n            # get best solution\n            self.best_solution = min(fitness)\n            best_index = fitness.index(self.best_solution)\n            # adaptive frequency modulation\n            self.F = self.adaptive_frequency_modulation()\n            # differential evolution\n            for i in range(self.pop_size):\n                # generate trial vector\n                trial = self.crossover(self.population[i], self.population[random.randint(0, self.pop_size - 1)])\n                # evaluate trial vector\n                trial_fitness = self.evaluate(trial)\n                # selection\n                self.population[i] = self.selection(self.population[i], trial)\n                # update best solution\n                if trial_fitness < self.evaluate(self.population[i]):\n                    self.population[i] = trial\n            # probabilistic exploration\n            for i in range(self.pop_size):\n                self.probabilistic_exploration()\n            # simulated annealing\n            for i in range(self.pop_size):\n                self.population[i] = self.simulated_annealing(self.population[i])\n            # covariance matrix adaptation\n            self.covariance_matrix = self.covariance_matrix_adaptation()\n            # adaptive step size learning\n            self.adaptive_step_size = self.adaptive_step_size_learning()\n        return self.best_solution\n\n    def __call__(self, func):\n        self.func = func\n        return self.optimize(func)\n\n# example usage\nbudget = 1000\ndim = 10\noptimizer = Adaptive_Frequency_Modulation_with_Probabilistic_Exploration(budget, dim)\nbest_solution = optimizer(lambda x: np.sum(x**2))\nprint(\"Best solution:\", best_solution)", "name": "Adaptive_Frequency_Modulation_with_Probabilistic_Exploration", "description": "Adaptive Frequency Modulation with Probabilistic Exploration and Covariance Matrix Adaptation using a Novel Adaptive Step Size Learning Mechanism", "configspace": "", "generation": 22, "fitness": 0.10816400106749635, "feedback": "The algorithm Adaptive_Frequency_Modulation_with_Probabilistic_Exploration got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.11 with standard deviation 0.07.", "error": "", "parent_id": "a259ae06-f4e1-498d-8b7a-55286c067997", "metadata": {"aucs": [0.23921471781293535, 0.22739127280809424, 0.2245890023521454, 0.17906977245680755, 0.20584624328901513, 0.19843058586192586, 0.20535894999107374, 0.2380133893689692, 0.202368998527645, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.005804896524547631, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.05727184594044432, 0.07963756328687654, 0.06018338847042981, 0.06573373191353493, 0.05462443817786966, 0.06762267030307101, 0.05893232977034002, 0.0742838458890519, 0.07911966473664689, 0.051528292942277454, 0.05646876147759483, 0.06262690848498786, 0.04811168287741929, 0.06123403523286519, 0.054592936961251115, 0.06369225854708127, 0.05165130048611377, 0.04274460565589433, 0.1164467255763535, 0.11565075657254253, 0.0975859123788494, 0.13960362884848365, 0.10529798523245792, 0.10589167691274559, 0.12490217477630106, 0.08589676088904508, 0.10060309248417065, 0.08659676447844411, 0.0869574509951051, 0.1076987202085713, 0.08915614074960776, 0.07630760185564034, 0.10226221909493793, 0.09100431094387129, 0.09358110134626096, 0.10444026012698837, 0.18830886304824934, 0.19827564964437472, 0.14709598294770654, 0.15303730308447916, 0.1728785925751405, 0.14767028036947938, 0.1480943700224976, 0.1532762894233881, 0.16215365093575218, 0.07631517161087154, 0.016406447901437082, 0.0627351852270831, 0.08528985264518574, 0.047952753392996206, 0.02966649729339421, 0.07342201619806998, 0.08403022620134182, 0.08903470616268383, 0.10339376678884205, 0.11624164832683315, 0.10555927217721095, 0.11285339481687517, 0.10735329884715339, 0.10985781330542, 0.07931982963935702, 0.09914710344438304, 0.10974608497217486, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.08203708294594414, 0.05351760332678568, 0.08026975921057056, 0.021832000538205443, 0.06091817402686717, 0.04295903542822921, 0.06485153147779787, 0.10113162786993801, 0.03774623753178108, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.011937470083278834, 0.014242548472139016, 0.019061605614021926, 0.027447832191570654, 0.044958669816120844, 0.03065383867611271, 0.037429848883701156, 0.02627299670403993, 0.039562889394124445, 0.2315635567371016, 0.2342955922515706, 0.24962725721385992, 0.2257887589085118, 0.2293526918045593, 0.20380448422910913, 0.21927019963412142, 0.23196142083337368, 0.2400379590634356, 0.0487592803282485, 0.050376553668435964, 0.05202987047746499, 0.06812032023418013, 0.054511076762133626, 0.06256674783625626, 0.08120252675110895, 0.06924982082568609, 0.0632149730308258, 0.14568139188646456, 0.10851962604100296, 0.13084854155831493, 0.11401907219735352, 0.12061127132356508, 0.1313971387740277, 0.11695211586179644, 0.11877751930608826, 0.13756813589295036, 0.16816139349769943, 0.1770176237898584, 0.1801504696165579, 0.18122689486688326, 0.18287858798747136, 0.18538912468543056, 0.15895549996361702, 0.17434605192438157, 0.16811572452691403, 0.10785230356581177, 0.12903114221832934, 0.12727882826368164, 0.1357704182236541, 0.12669182701275217, 0.15115290981621132, 0.12027595646437639, 0.12047614169946919, 0.1439193875289404, 0.20088724210145303, 0.17080135695926602, 0.17773192170155794, 0.18404365499049535, 0.20507679984261662, 0.1748455383248998, 0.18811804112384278, 0.16412891466376678, 0.21086887084641082, 0.15013723587482197, 0.1551801042817489, 0.15762086760319793, 0.16119836197998816, 0.15562644980272655, 0.15817028920698872, 0.14514020944654493, 0.1554684439969949, 0.16707248883401427, 0.14624986885234825, 0.15359186917512402, 0.15286096480255673, 0.17962194716884383, 0.18728980011486296, 0.1940647835226077, 0.13790988890036682, 0.20019793294464605, 0.11553800504512057, 0.1943466714898654, 0.23429843398463712, 0.2058906079798113, 0.2560299009511281, 0.1381967059447592, 0.13901772083134623, 0.16093140714821674, 0.16096359587522424, 0.1500004186727898, 0.16757069977966088, 0.16476389283115955, 0.17744876338922444, 0.16940886945344025, 0.15563538544185085, 0.17758403236752673, 0.17269746171101774, 0.17367064709635305, 0.1755257506543787, 0.05913253514693806, 0.05703790480269977, 0.06488133590279754, 0.06588294476829526, 0.05603201530268709, 0.05431863669169723, 0.06120022830529581, 0.06316035438958045, 0.05497909166453574]}, "mutation_prompt": null}
{"id": "8c23b7a6-af1a-4c7f-b0af-80281ab6776e", "solution": "import numpy as np\nimport random\n\nclass Enhanced_Adaptive_Differential_Evolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.F = 0.5  # initial frequency\n        self.CR = 0.5  # initial crossover rate\n        self.pop_size = 50  # initial population size\n        self.population = np.random.uniform(-5.0, 5.0, (self.pop_size, self.dim))\n        self.best_solution = np.inf\n        self.probability = 0.2  # increased probability of probabilistic exploration\n        self.temperature = 100  # initial temperature for simulated annealing\n        self.covariance_matrix = np.eye(self.dim)  # initial covariance matrix\n        self.adaptive_step_size = 0.1  # initial adaptive step size for covariance matrix adaptation\n        self.step_size_learning_rate = 0.001  # learning rate for adaptive step size\n        self.probability_increase_rate = 0.001  # increase probability of probabilistic exploration\n\n    def evaluate(self, x):\n        return self.func(x)\n\n    def func(self, x):\n        # BBOB test suite of 24 noiseless functions\n        # Here we use the first function f1(x) = sum(x_i^2)\n        return np.sum(x**2)\n\n    def frequency_modulation(self, x):\n        # frequency modulation function\n        return self.F * np.sin(2 * np.pi * self.F * x)\n\n    def adaptive_frequency_modulation(self):\n        # adaptive frequency modulation\n        self.F *= 1.1  # increase frequency\n        if random.random() < self.probability:\n            self.F *= 0.9  # decrease frequency with probability\n        if self.probability < 0.9:\n            self.probability += self.probability_increase_rate  # increase probability of probabilistic exploration\n        return self.F\n\n    def crossover(self, x1, x2):\n        # crossover operation\n        r = np.random.rand(self.dim)\n        return x1 + r * (x2 - x1)\n\n    def selection(self, x1, x2):\n        # selection operation\n        if self.evaluate(x2) < self.evaluate(x1):\n            return x2\n        else:\n            return x1\n\n    def simulated_annealing(self, x):\n        # simulated annealing\n        new_x = x + np.random.normal(0, 1, self.dim)\n        new_x = np.clip(new_x, -5.0, 5.0)\n        if self.evaluate(new_x) < self.evaluate(x):\n            return new_x\n        else:\n            probability = np.exp(-(self.evaluate(new_x) - self.evaluate(x)) / self.temperature)\n            if random.random() < probability:\n                return new_x\n            else:\n                return x\n        self.temperature *= 0.99  # decrease temperature with a higher rate\n\n    def covariance_matrix_adaptation(self):\n        # covariance matrix adaptation\n        self.covariance_matrix = (1 - self.adaptive_step_size) * self.covariance_matrix + self.adaptive_step_size * np.eye(self.dim)\n        return self.covariance_matrix\n\n    def adaptive_step_size_learning(self):\n        # adaptive step size learning\n        self.adaptive_step_size += self.step_size_learning_rate * (self.evaluate(self.population[0]) - self.evaluate(self.population[-1]))\n        return self.adaptive_step_size\n\n    def optimize(self, func):\n        for _ in range(self.budget):\n            # evaluate population\n            fitness = [self.evaluate(x) for x in self.population]\n            # get best solution\n            self.best_solution = min(fitness)\n            best_index = fitness.index(self.best_solution)\n            # adaptive frequency modulation\n            self.F = self.adaptive_frequency_modulation()\n            # differential evolution\n            for i in range(self.pop_size):\n                # generate trial vector\n                trial = self.crossover(self.population[i], self.population[random.randint(0, self.pop_size - 1)])\n                # evaluate trial vector\n                trial_fitness = self.evaluate(trial)\n                # selection\n                self.population[i] = self.selection(self.population[i], trial)\n                # update best solution\n                if trial_fitness < self.evaluate(self.population[i]):\n                    self.population[i] = trial\n            # simulated annealing\n            for i in range(self.pop_size):\n                self.population[i] = self.simulated_annealing(self.population[i])\n            # covariance matrix adaptation\n            self.covariance_matrix = self.covariance_matrix_adaptation()\n            # adaptive step size learning\n            self.adaptive_step_size = self.adaptive_step_size_learning()\n        return self.best_solution\n\n    def __call__(self, func):\n        self.func = func\n        return self.optimize(func)\n\n# example usage\nbudget = 1000\ndim = 10\noptimizer = Enhanced_Adaptive_Differential_Evolution(budget, dim)\nbest_solution = optimizer(lambda x: np.sum(x**2))\nprint(\"Best solution:\", best_solution)", "name": "Enhanced_Adaptive_Differential_Evolution", "description": "Enhanced Adaptive Differential Evolution with Probabilistic Frequency Modulation, Covariance Matrix Adaptation, and Simulated Annealing with Adaptive Temperature Cooling Schedule, refined with a probability 1 to change the individual lines of the selected solution to refine its strategy.", "configspace": "", "generation": 23, "fitness": 0.10874082506833888, "feedback": "The algorithm Enhanced_Adaptive_Differential_Evolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.11 with standard deviation 0.07.", "error": "", "parent_id": "a259ae06-f4e1-498d-8b7a-55286c067997", "metadata": {"aucs": [0.23804477811988922, 0.21356219951423183, 0.22170081866833014, 0.18814667368770877, 0.20307074300714767, 0.21418357824885192, 0.2090730405903669, 0.18941683981095014, 0.2048302685765253, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.03784001429784822, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.0608480607206946, 0.07396402747560116, 0.04976048550130707, 0.0798390336359387, 0.07515291314305106, 0.05668449914242224, 0.07019128659444684, 0.06738000582294901, 0.06592013963380816, 0.04572018325266258, 0.06468764997153498, 0.04859944167381747, 0.0641056033981835, 0.0550104699048396, 0.04286564763215861, 0.06979546312088769, 0.058234587900318324, 0.04721782399802765, 0.11571408591809262, 0.12401099768146162, 0.13319990817134097, 0.16446500877474934, 0.12509139585190765, 0.12067866677953498, 0.17334119344072063, 0.11712496481314139, 0.15739800244633118, 0.08370632115799492, 0.0859728781208563, 0.09826533000609183, 0.09863610898478314, 0.09121123740875758, 0.07960465339661493, 0.1045794678920895, 0.11203378354463767, 0.11989517163691543, 0.134424141287097, 0.1516667020990481, 0.13827253747515855, 0.17731706205872344, 0.16615938740105451, 0.15716842085638272, 0.12498296312701695, 0.15328753390079586, 0.13492509087280236, 0.09277807394306836, 0.0670547038262419, 0.06984598139328191, 0.028311918182298168, 0.03875636603975674, 0.04742231146132436, 0.11104029148022188, 0.07167708659919114, 0.09991246397270548, 0.09826079185932013, 0.10005552766420445, 0.08095881931086091, 0.11643710787398576, 0.09786543998930552, 0.10692532897554496, 0.1040459909153304, 0.07648769092926211, 0.12087930495659016, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.07987693277739072, 0.05308340718851401, 0.07913869462967416, 0.039675029506670345, 0.05128970572659619, 0.0283860802426108, 0.08996511522447292, 0.05652455038988757, 0.04197094352803943, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.010069742505556811, 0.032657304692474076, 0.035037064793404404, 0.03262497869938419, 0.04309398675343068, 0.03401586846158422, 0.04073970115917813, 0.039808182376206425, 0.04554402908545274, 0.249550988362512, 0.23105045128571344, 0.2260504205946, 0.21634008166435525, 0.19910192202874544, 0.23917986224934018, 0.23018763928134078, 0.24121537675670235, 0.2724454993840284, 0.04442842210381348, 0.06512154293063921, 0.04877280157678432, 0.06748092182273235, 0.07089727467196194, 0.06258003829669656, 0.0782650670672318, 0.07440487108352367, 0.06319213301156268, 0.12104472248883869, 0.11249469734675877, 0.14888879663338506, 0.10741087452553788, 0.13543762905160817, 0.12000239390263123, 0.11032162211423258, 0.12131349069848107, 0.1258264504484179, 0.16346024577825846, 0.177922901990768, 0.1699418565771983, 0.19249122648083616, 0.1998768990883414, 0.1944483518075364, 0.190875226833362, 0.18659928758903077, 0.16451946643841642, 0.1108130022677839, 0.11042911277192102, 0.12869731771326276, 0.12278005550435878, 0.12852071881542237, 0.11836172165242698, 0.1124813804796213, 0.1276059376303712, 0.12621402839344698, 0.1713771371867251, 0.171423612321564, 0.16392450731214814, 0.17040730788324332, 0.1880223100746693, 0.17027767771133162, 0.17851069522434448, 0.1781174825034728, 0.18364406351295293, 0.15946450892993547, 0.15287943678748384, 0.1518066618646775, 0.15907000904071977, 0.16063342881334175, 0.15818490909880145, 0.16636449746479665, 0.14883005711255848, 0.15871723175609553, 0.1641696969119958, 0.14881434282249606, 0.14844618791647246, 0.2291980499871593, 0.20070370070872368, 0.19121931928620683, 0.2096448120141432, 0.21153227854419032, 0.16125621790719125, 0.164314274847968, 0.20422300293545614, 0.16788262845229807, 0.12797371814215586, 0.11880235839007813, 0.2053422535237701, 0.13279497128280704, 0.1898746102484713, 0.11259626889480512, 0.16633514273714844, 0.16211037547835672, 0.17076843853432622, 0.16375705607669766, 0.15457253522557823, 0.1836545682397276, 0.1724095333286796, 0.1527134232345856, 0.17861368582980408, 0.06354409923960425, 0.05904416399230794, 0.058702440701988934, 0.06572184989846797, 0.06397880435664516, 0.06769358536211734, 0.0727115466129924, 0.05649405597981527, 0.06190814406094436]}, "mutation_prompt": null}
{"id": "6af81c7b-449b-4d4d-a84b-828c02aacbe7", "solution": "import numpy as np\nimport random\n\nclass Adaptive_Frequency_Modulation_with_Covariance_Matrix_Adaptation_and_Simulated_Annealing:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.F = 0.5  # initial frequency\n        self.CR = 0.5  # initial crossover rate\n        self.pop_size = 50  # initial population size\n        self.population = np.random.uniform(-5.0, 5.0, (self.pop_size, self.dim))\n        self.best_solution = np.inf\n        self.probability = 0.2  # increased probability of probabilistic exploration\n        self.temperature = 100  # initial temperature for simulated annealing\n        self.covariance_matrix = np.eye(self.dim)  # initial covariance matrix\n        self.adaptive_step_size = 0.1  # initial adaptive step size for covariance matrix adaptation\n        self.step_size_learning_rate = 0.001  # learning rate for adaptive step size\n        self.probability_learning_rate = 0.0001  # learning rate for probability\n\n    def evaluate(self, x):\n        return self.func(x)\n\n    def func(self, x):\n        # BBOB test suite of 24 noiseless functions\n        # Here we use the first function f1(x) = sum(x_i^2)\n        return np.sum(x**2)\n\n    def frequency_modulation(self, x):\n        # frequency modulation function\n        return self.F * np.sin(2 * np.pi * self.F * x)\n\n    def adaptive_frequency_modulation(self):\n        # adaptive frequency modulation\n        self.F *= 1.1  # increase frequency\n        if random.random() < self.probability:\n            self.F *= 0.9  # decrease frequency with probability\n        return self.F\n\n    def probability_modulation(self):\n        # probability modulation\n        self.probability += self.probability_learning_rate\n        if random.random() < self.probability:\n            self.probability *= 0.9  # decrease probability with probability\n        return self.probability\n\n    def crossover(self, x1, x2):\n        # crossover operation\n        r = np.random.rand(self.dim)\n        return x1 + r * (x2 - x1)\n\n    def selection(self, x1, x2):\n        # selection operation\n        if self.evaluate(x2) < self.evaluate(x1):\n            return x2\n        else:\n            return x1\n\n    def simulated_annealing(self, x):\n        # simulated annealing\n        new_x = x + np.random.normal(0, 1, self.dim)\n        new_x = np.clip(new_x, -5.0, 5.0)\n        if self.evaluate(new_x) < self.evaluate(x):\n            return new_x\n        else:\n            probability = np.exp(-(self.evaluate(new_x) - self.evaluate(x)) / self.temperature)\n            if random.random() < probability:\n                return new_x\n            else:\n                return x\n        self.temperature *= 0.99  # decrease temperature with a higher rate\n\n    def covariance_matrix_adaptation(self):\n        # covariance matrix adaptation\n        self.covariance_matrix = (1 - self.adaptive_step_size) * self.covariance_matrix + self.adaptive_step_size * np.eye(self.dim)\n        return self.covariance_matrix\n\n    def adaptive_step_size_learning(self):\n        # adaptive step size learning\n        self.adaptive_step_size += self.step_size_learning_rate * (self.evaluate(self.population[0]) - self.evaluate(self.population[-1]))\n        return self.adaptive_step_size\n\n    def probability_learning(self):\n        # probability learning\n        self.probability_learning_rate += self.step_size_learning_rate * (self.evaluate(self.population[0]) - self.evaluate(self.population[-1]))\n        return self.probability_learning_rate\n\n    def optimize(self, func):\n        for _ in range(self.budget):\n            # evaluate population\n            fitness = [self.evaluate(x) for x in self.population]\n            # get best solution\n            self.best_solution = min(fitness)\n            best_index = fitness.index(self.best_solution)\n            # adaptive frequency modulation\n            self.F = self.adaptive_frequency_modulation()\n            # probability modulation\n            self.probability = self.probability_modulation()\n            # differential evolution\n            for i in range(self.pop_size):\n                # generate trial vector\n                trial = self.crossover(self.population[i], self.population[random.randint(0, self.pop_size - 1)])\n                # evaluate trial vector\n                trial_fitness = self.evaluate(trial)\n                # selection\n                self.population[i] = self.selection(self.population[i], trial)\n                # update best solution\n                if trial_fitness < self.evaluate(self.population[i]):\n                    self.population[i] = trial\n            # simulated annealing\n            for i in range(self.pop_size):\n                self.population[i] = self.simulated_annealing(self.population[i])\n            # covariance matrix adaptation\n            self.covariance_matrix = self.covariance_matrix_adaptation()\n            # adaptive step size learning\n            self.adaptive_step_size = self.adaptive_step_size_learning()\n            # probability learning\n            self.probability_learning_rate = self.probability_learning()\n        return self.best_solution\n\n    def __call__(self, func):\n        self.func = func\n        return self.optimize(func)\n\n# example usage\nbudget = 1000\ndim = 10\noptimizer = Adaptive_Frequency_Modulation_with_Covariance_Matrix_Adaptation_and_Simulated_Annealing(budget, dim)\nbest_solution = optimizer(lambda x: np.sum(x**2))\nprint(\"Best solution:\", best_solution)", "name": "Adaptive_Frequency_Modulation_with_Covariance_Matrix_Adaptation_and_Simulated_Annealing", "description": "Adaptive Frequency Modulation with Covariance Matrix Adaptation and Simulated Annealing with Adaptive Temperature Cooling Schedule using Probability-Informed Probabilistic Frequency Modulation and Covariance Matrix Adaptation.", "configspace": "", "generation": 24, "fitness": 0.10893838388476379, "feedback": "The algorithm Adaptive_Frequency_Modulation_with_Covariance_Matrix_Adaptation_and_Simulated_Annealing got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.11 with standard deviation 0.07.", "error": "", "parent_id": "a259ae06-f4e1-498d-8b7a-55286c067997", "metadata": {"aucs": [0.2443051982575295, 0.21074266128611385, 0.21468233517434787, 0.1960888241202745, 0.2050774243633786, 0.2252295598030991, 0.21801034350562298, 0.19952051737988097, 0.21657283852001863, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.03288680082233386, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.06317891789077135, 0.06049247739015351, 0.0537693760551321, 0.06299227343898306, 0.06568561499093739, 0.06520769077210675, 0.06870424205445469, 0.05620493097202617, 0.0646386282660587, 0.03592126635775128, 0.044244006912677825, 0.04602436143897359, 0.05799525999491195, 0.06019083522746482, 0.04649734071875811, 0.05268689583528341, 0.06904666179119745, 0.049201027748118964, 0.10792858308775288, 0.09960180520288264, 0.09819344651142836, 0.2047629800806473, 0.11186644549566149, 0.1310849358421765, 0.16691624022742968, 0.10399226716600185, 0.12744832269848794, 0.11863242864865475, 0.08333550593387662, 0.08547017148865865, 0.09104141859988735, 0.06508006750778328, 0.08617473712716162, 0.1130972097379781, 0.09015214082537537, 0.1045821736121132, 0.1546376340903447, 0.19171771192982712, 0.13723452436066763, 0.17115982985701594, 0.18385797023378325, 0.16889421692925388, 0.13982460440942646, 0.1499206272553325, 0.13325310237716526, 0.059145487324698154, 0.031198911644202743, 0.07462584652927562, 0.01771548429655212, 0.019047104888363142, 0.03837955754424238, 0.0885150254427799, 0.07752334191591037, 0.07760319776131286, 0.11658492012863864, 0.10776258317487486, 0.09703311117788982, 0.11074234516413273, 0.10623787928971629, 0.0922784223887595, 0.09023480181455434, 0.11031389928641422, 0.10142659394116726, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.05286319222113611, 0.11967012960734824, 0.09644188641974671, 0.02658219896569136, 0.05563545569880568, 0.06512119629835234, 0.07713891419461671, 0.05156581152892137, 0.08249708119455412, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.03463521583926599, 0.02039932343568507, 0.028449007793203807, 0.02246905692005896, 0.041059474640469995, 0.03816543830733521, 0.023981148431358124, 0.038296239580564384, 0.06695379614699082, 0.21718690753905712, 0.23637473833928857, 0.21434486649252982, 0.21939797147139284, 0.21584576048799042, 0.23004701375566505, 0.23674880339744286, 0.21423867298799126, 0.21989569286328747, 0.04586980891675396, 0.062458662006335874, 0.04857548210462359, 0.06849373643932999, 0.05233282342747769, 0.06005922319027357, 0.07382534304623223, 0.06151393420371498, 0.08135377417635592, 0.1380989042736288, 0.11334604016174543, 0.12560078948679398, 0.13650369914217586, 0.11958416401799854, 0.14541030380084286, 0.11136699379908077, 0.1153951038539317, 0.12870072526724374, 0.1552672269639488, 0.16956345850664678, 0.17293180311936796, 0.1939274711804021, 0.18961727729997468, 0.17926416874457696, 0.16602024087524936, 0.18530034458266254, 0.16022910725895456, 0.12677813171795138, 0.11497103531396824, 0.11505819943127515, 0.142520340025879, 0.13379529183416694, 0.13765150453917108, 0.10676052028475169, 0.1278384007952652, 0.1169273978419827, 0.1850635172189493, 0.16985173079908544, 0.16998982482727876, 0.1874297875926232, 0.1692734748059158, 0.17570970187658985, 0.16770890130122806, 0.16666796683444773, 0.15932048749839234, 0.15333872959014994, 0.1519546006882211, 0.1627500467975438, 0.15524821003291944, 0.15524817034482297, 0.15006345521431996, 0.14936047345887704, 0.15451267430049098, 0.18494220132929917, 0.1585343576374758, 0.15294243564979537, 0.14773149323066204, 0.2314387013961382, 0.17711219932332445, 0.19336312586484183, 0.29194271831313856, 0.21481808090392707, 0.13659583467270098, 0.24117150507527263, 0.177124041966074, 0.19738674273707135, 0.2149899821309167, 0.13121009614506363, 0.15284015338939372, 0.15207501458892692, 0.15320376385747436, 0.17636049375499463, 0.16573669814111802, 0.17812416121928987, 0.17781367655776792, 0.1695083932023581, 0.16191939886085815, 0.1838938694853789, 0.17125165851301993, 0.16690230989703136, 0.16154631897675842, 0.05384007504645161, 0.07255153054260643, 0.07030419714996883, 0.0715583314194822, 0.06434744456531549, 0.060778790955986595, 0.056091613388256856, 0.0586935198611469, 0.06300988394972751]}, "mutation_prompt": null}
{"id": "62f0f28d-b1f3-482e-a59e-689769ae92f8", "solution": "import numpy as np\nimport random\n\nclass Enhanced_Adaptive_Differential_Evolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.F = 0.5  # initial frequency\n        self.CR = 0.5  # initial crossover rate\n        self.pop_size = 50  # initial population size\n        self.population = np.random.uniform(-5.0, 5.0, (self.pop_size, self.dim))\n        self.best_solution = np.inf\n        self.probability = 0.2  # increased probability of probabilistic exploration\n        self.temperature = 100  # initial temperature for simulated annealing\n        self.covariance_matrix = np.eye(self.dim)  # initial covariance matrix\n        self.adaptive_step_size = 0.1  # initial adaptive step size for covariance matrix adaptation\n        self.step_size_learning_rate = 0.001  # learning rate for adaptive step size\n        self.history = np.zeros((self.budget, self.dim))  # history of best solutions\n\n    def evaluate(self, x):\n        return self.func(x)\n\n    def func(self, x):\n        # BBOB test suite of 24 noiseless functions\n        # Here we use the first function f1(x) = sum(x_i^2)\n        return np.sum(x**2)\n\n    def frequency_modulation(self, x):\n        # frequency modulation function\n        return self.F * np.sin(2 * np.pi * self.F * x)\n\n    def adaptive_frequency_modulation(self):\n        # adaptive frequency modulation\n        self.F *= 1.1  # increase frequency\n        if random.random() < self.probability:\n            self.F *= 0.9  # decrease frequency with probability\n        return self.F\n\n    def crossover(self, x1, x2):\n        # crossover operation\n        r = np.random.rand(self.dim)\n        return x1 + r * (x2 - x1)\n\n    def selection(self, x1, x2):\n        # selection operation\n        if self.evaluate(x2) < self.evaluate(x1):\n            return x2\n        else:\n            return x1\n\n    def simulated_annealing(self, x):\n        # simulated annealing\n        new_x = x + np.random.normal(0, 1, self.dim)\n        new_x = np.clip(new_x, -5.0, 5.0)\n        if self.evaluate(new_x) < self.evaluate(x):\n            return new_x\n        else:\n            probability = np.exp(-(self.evaluate(new_x) - self.evaluate(x)) / self.temperature)\n            if random.random() < probability:\n                return new_x\n            else:\n                return x\n        self.temperature *= 0.99  # decrease temperature with a higher rate\n\n    def covariance_matrix_adaptation(self):\n        # covariance matrix adaptation\n        self.covariance_matrix = (1 - self.adaptive_step_size) * self.covariance_matrix + self.adaptive_step_size * np.eye(self.dim)\n        return self.covariance_matrix\n\n    def adaptive_step_size_learning(self):\n        # adaptive step size learning\n        self.adaptive_step_size += self.step_size_learning_rate * (self.evaluate(self.population[0]) - self.evaluate(self.population[-1]))\n        return self.adaptive_step_size\n\n    def history_learning(self):\n        # history learning\n        self.history = np.vstack((self.history, self.population[0]))\n\n    def optimize(self, func):\n        for i in range(self.budget):\n            # evaluate population\n            fitness = [self.evaluate(x) for x in self.population]\n            # get best solution\n            self.best_solution = min(fitness)\n            best_index = fitness.index(self.best_solution)\n            # adaptive frequency modulation\n            self.F = self.adaptive_frequency_modulation()\n            # differential evolution\n            for j in range(self.pop_size):\n                # generate trial vector\n                trial = self.crossover(self.population[j], self.population[random.randint(0, self.pop_size - 1)])\n                # evaluate trial vector\n                trial_fitness = self.evaluate(trial)\n                # selection\n                self.population[j] = self.selection(self.population[j], trial)\n                # update best solution\n                if trial_fitness < self.evaluate(self.population[j]):\n                    self.population[j] = trial\n            # simulated annealing\n            for j in range(self.pop_size):\n                self.population[j] = self.simulated_annealing(self.population[j])\n            # covariance matrix adaptation\n            self.covariance_matrix = self.covariance_matrix_adaptation()\n            # adaptive step size learning\n            self.adaptive_step_size = self.adaptive_step_size_learning()\n            # history learning\n            self.history_learning()\n        return self.best_solution, self.history\n\n    def __call__(self, func):\n        self.func = func\n        return self.optimize(func)\n\n# example usage\nbudget = 1000\ndim = 10\noptimizer = Enhanced_Adaptive_Differential_Evolution(budget, dim)\nbest_solution, history = optimizer(lambda x: np.sum(x**2))\nprint(\"Best solution:\", best_solution)\nprint(\"History:\", history)", "name": "Enhanced_Adaptive_Differential_Evolution", "description": "Enhanced Adaptive Differential Evolution with Probabilistic Frequency Modulation, Covariance Matrix Adaptation, and Simulated Annealing with Adaptive Temperature Cooling Schedule and Adaptive Step Size Learning.", "configspace": "", "generation": 25, "fitness": 0.11349104386716621, "feedback": "The algorithm Enhanced_Adaptive_Differential_Evolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.11 with standard deviation 0.08.", "error": "", "parent_id": "a259ae06-f4e1-498d-8b7a-55286c067997", "metadata": {"aucs": [0.23769731172917852, 0.23720457413485296, 0.23398524340821303, 0.20549460010352794, 0.20555116867582168, 0.20130651801061838, 0.2111034242676071, 0.2007516942743227, 0.2116305929248098, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.006392611173892893, 9.999999999998899e-05, 0.059900294776183416, 0.07012785618662798, 0.05419817263252702, 0.0795014722763715, 0.06935901025400615, 0.07305402795587912, 0.07197872740953881, 0.06459557626532919, 0.06996007361196876, 0.053111208791695175, 0.053566490408007095, 0.045790153649368226, 0.08521993086965163, 0.0694819433532079, 0.04350696073483884, 0.060257967750235175, 0.05189063106084635, 0.04649729526248825, 0.14662710504050225, 0.12405344652255246, 0.10395021756261902, 0.5379579998710504, 0.16193852016424048, 0.11791330384339704, 0.18518522693319261, 0.10037714751656313, 0.4283067989696846, 0.09280746828909836, 0.08837625468069787, 0.07937852657802957, 0.07854238359383503, 0.08252944843027832, 0.11767926985163069, 0.1205915686158825, 0.0852113029185213, 0.09530169292310442, 0.17330123385194474, 0.1802252971353444, 0.1525284743121933, 0.1555795762620088, 0.17867323713658245, 0.17229723458815238, 0.16781672295818117, 0.15990674428772478, 0.16337899010881085, 0.08900179420138443, 0.03865389075710812, 0.08160113257791912, 0.03927597651265646, 0.026075257529084794, 0.054926825543406665, 0.09841040259303824, 0.06333280723678636, 0.07418265885714515, 0.09796359396919452, 0.09649198930530256, 0.1100308792533109, 0.10331574585163705, 0.10284233020551181, 0.09918302484193209, 0.10784997913383398, 0.10545554298550341, 0.08743879185238912, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.06531471720729043, 0.06322776317563727, 0.10764868144200157, 0.04234868722028973, 0.06891894424860334, 0.11389557244681159, 0.08902233063245635, 0.04959990216821064, 0.06503624669867603, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.019302053195070723, 0.02569447570168837, 0.02218694043020275, 0.030389077976176515, 0.05011639472944973, 0.026064793258674057, 0.03964242745950375, 0.03523032723265518, 0.04990912829843397, 0.26045759541142244, 0.22792046210319727, 0.24789883372762955, 0.22782461580683844, 0.20323172004362933, 0.21785474159582874, 0.24977361138643406, 0.22257910459292318, 0.23487337912939432, 0.047406160946790954, 0.06913972508922595, 0.04628131465091223, 0.06565901983375388, 0.055748382497863735, 0.0633631736000938, 0.06015180734300485, 0.0755272335023841, 0.06646283240006434, 0.12119223920814659, 0.12470214300559834, 0.14030068848721566, 0.11557895408650731, 0.1328869344681134, 0.12954047545064928, 0.12548124776483527, 0.11082883532147259, 0.11613807783079211, 0.1646706089612735, 0.17402029364751725, 0.169957030806709, 0.17721735438905684, 0.18977506642230046, 0.18552711273919076, 0.16008964310468943, 0.18975185260984473, 0.16153221359268788, 0.11588258721205402, 0.12741740017735936, 0.13192378680608652, 0.1228000957149119, 0.12838293347600316, 0.1273759751841037, 0.10303660034216178, 0.12869446988955147, 0.11553907992403178, 0.1591644210830242, 0.190149261923249, 0.16711753718862765, 0.1942395294204997, 0.2132047975084318, 0.15491365322932638, 0.1620218121660999, 0.16842459346413563, 0.19539101481697096, 0.1574308748762071, 0.15527046019467394, 0.16117056355370862, 0.1507179142739733, 0.16571259637988223, 0.1572215990897441, 0.1537357474934712, 0.15304501824019534, 0.17085057300372697, 0.14647342296828503, 0.16458182587567938, 0.1475369256476301, 0.24951655367433845, 0.1706906960414144, 0.22975424671538114, 0.1468282263746179, 0.18998475289801375, 0.19872740316007598, 0.17967694151965607, 0.2095248766304756, 0.16531236969298302, 0.17673728075527995, 0.1573563880111336, 0.16079461976257647, 0.13630563294661702, 0.18943475494288797, 0.15794340341591473, 0.1694964781221654, 0.17584746439259435, 0.1675693088119269, 0.16675380106618054, 0.16310744584612002, 0.17812108928443993, 0.16876649757309747, 0.16601458261862279, 0.18324856573202208, 0.0679542418940462, 0.06107766964719452, 0.0634038621543137, 0.0589167546534245, 0.06826269804045038, 0.051810492248762996, 0.06457290160761553, 0.0671718679707437, 0.06687823879460175]}, "mutation_prompt": null}
{"id": "2c5da637-e485-42f2-bdc0-82d00983b2d4", "solution": "import numpy as np\nimport random\n\nclass Enhanced_Adaptive_Differential_Evolution_2:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.F = 0.5  # initial frequency\n        self.CR = 0.5  # initial crossover rate\n        self.pop_size = 50  # initial population size\n        self.population = np.random.uniform(-5.0, 5.0, (self.pop_size, self.dim))\n        self.best_solution = np.inf\n        self.probability = 0.4  # increased probability of probabilistic exploration\n        self.temperature = 100  # initial temperature for simulated annealing\n        self.covariance_matrix = np.eye(self.dim)  # initial covariance matrix\n        self.adaptive_step_size = 0.05  # initial adaptive step size for covariance matrix adaptation\n        self.step_size_learning_rate = 0.001  # learning rate for adaptive step size\n        self.history = np.zeros((self.budget, self.dim))  # history of best solutions\n        self.aooc_score = 0.11  # initial AOOC score\n\n    def evaluate(self, x):\n        return self.func(x)\n\n    def func(self, x):\n        # BBOB test suite of 24 noiseless functions\n        # Here we use the first function f1(x) = sum(x_i^2)\n        return np.sum(x**2)\n\n    def frequency_modulation(self, x):\n        # frequency modulation function\n        return self.F * np.sin(2 * np.pi * self.F * x)\n\n    def adaptive_frequency_modulation(self):\n        # adaptive frequency modulation\n        self.F *= 1.1  # increase frequency\n        if random.random() < self.probability:\n            self.F *= 0.9  # decrease frequency with probability\n        return self.F\n\n    def crossover(self, x1, x2):\n        # crossover operation\n        r = np.random.rand(self.dim)\n        return x1 + r * (x2 - x1)\n\n    def selection(self, x1, x2):\n        # selection operation\n        if self.evaluate(x2) < self.evaluate(x1):\n            return x2\n        else:\n            return x1\n\n    def simulated_annealing(self, x):\n        # simulated annealing\n        new_x = x + np.random.normal(0, 1, self.dim)\n        new_x = np.clip(new_x, -5.0, 5.0)\n        if self.evaluate(new_x) < self.evaluate(x):\n            return new_x\n        else:\n            probability = np.exp(-(self.evaluate(new_x) - self.evaluate(x)) / self.temperature)\n            if random.random() < probability:\n                return new_x\n            else:\n                return x\n        self.temperature *= 0.995  # decrease temperature with a higher rate\n\n    def covariance_matrix_adaptation(self):\n        # covariance matrix adaptation\n        self.covariance_matrix = (1 - self.adaptive_step_size) * self.covariance_matrix + self.adaptive_step_size * np.eye(self.dim)\n        return self.covariance_matrix\n\n    def adaptive_step_size_learning(self):\n        # adaptive step size learning\n        self.adaptive_step_size += self.step_size_learning_rate * (self.evaluate(self.population[0]) - self.evaluate(self.population[-1]))\n        return self.adaptive_step_size\n\n    def history_learning(self):\n        # history learning\n        self.history = np.vstack((self.history, self.population[0]))\n\n    def optimize(self, func):\n        for i in range(self.budget):\n            # evaluate population\n            fitness = [self.evaluate(x) for x in self.population]\n            # get best solution\n            self.best_solution = min(fitness)\n            best_index = fitness.index(self.best_solution)\n            # adaptive frequency modulation\n            self.F = self.adaptive_frequency_modulation()\n            # differential evolution\n            for j in range(self.pop_size):\n                # generate trial vector\n                trial = self.crossover(self.population[j], self.population[random.randint(0, self.pop_size - 1)])\n                # evaluate trial vector\n                trial_fitness = self.evaluate(trial)\n                # selection\n                self.population[j] = self.selection(self.population[j], trial)\n                # update best solution\n                if trial_fitness < self.evaluate(self.population[j]):\n                    self.population[j] = trial\n            # simulated annealing\n            for j in range(self.pop_size):\n                self.population[j] = self.simulated_annealing(self.population[j])\n            # covariance matrix adaptation\n            self.covariance_matrix = self.covariance_matrix_adaptation()\n            # adaptive step size learning\n            self.adaptive_step_size = self.adaptive_step_size_learning()\n            # history learning\n            self.history_learning()\n        return self.best_solution, self.history\n\n    def __call__(self, func):\n        self.func = func\n        return self.optimize(func)\n\n# example usage\nbudget = 1000\ndim = 10\noptimizer = Enhanced_Adaptive_Differential_Evolution_2(budget, dim)\nbest_solution, history = optimizer(lambda x: np.sum(x**2))\nprint(\"Best solution:\", best_solution)\nprint(\"History:\", history)", "name": "Enhanced_Adaptive_Differential_Evolution_2", "description": "Hybridization of Enhanced Adaptive Differential Evolution with Simulated Annealing and Covariance Matrix Adaptation using a Novel Frequency Modulation Strategy and Adaptive Step Size Learning.", "configspace": "", "generation": 26, "fitness": 0.1103018205235429, "feedback": "The algorithm Enhanced_Adaptive_Differential_Evolution_2 got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.11 with standard deviation 0.07.", "error": "", "parent_id": "62f0f28d-b1f3-482e-a59e-689769ae92f8", "metadata": {"aucs": [0.25425552739596635, 0.24935898233092746, 0.21957874124648358, 0.22758038758578492, 0.19115212106881596, 0.22032065309898852, 0.20942900418308652, 0.1980268944544702, 0.21221502223106015, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.0004025145186152823, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.05599428428796327, 0.05592429032971136, 0.061587417424629476, 0.06345513128916846, 0.06262618536603226, 0.07410984026730072, 0.0786229780188129, 0.06578186046864287, 0.06499659122015822, 0.047416205793575084, 0.053766877821172954, 0.061451639148945136, 0.06426717441665264, 0.05826445699357141, 0.045776441045762506, 0.0524910303735594, 0.04727932391750178, 0.0583058812687024, 0.10850001262705977, 0.13364294306316338, 0.09532673880088571, 0.11770167045276525, 0.13198325815339418, 0.13278554295808298, 0.163742789673601, 0.12202934179331482, 0.41924029864196977, 0.0862107221449242, 0.09145698989418993, 0.07878855419058495, 0.08579686641757689, 0.0720588188220962, 0.07577098782933867, 0.1166812549855909, 0.08777671389394948, 0.09520840972351174, 0.1533957686901527, 0.17706972403563936, 0.15176880413407579, 0.17335112687232546, 0.20465236198568404, 0.15169428015128228, 0.16372653145214577, 0.14271331835767342, 0.15236233400850197, 0.0672700317604622, 0.032097902869202, 0.06745579010452074, 0.09004792474595924, 0.03692025680736766, 0.027150189115922707, 0.1128822395869532, 0.07193635658546338, 0.07778418897940409, 0.08997278574179857, 0.08926146171242455, 0.11423143238451017, 0.10718606080763382, 0.1031814779397846, 0.10190400618185924, 0.10388857418153452, 0.10960796760722802, 0.10368299833413708, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.07129404651299465, 0.05414443531054114, 0.18171256966236593, 0.04770390149401971, 0.07094882594355023, 0.04685049211038528, 0.07027279830869537, 0.08165433692796631, 0.06898123576536086, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.02835803716685059, 0.02338046608498845, 0.02839271700635304, 0.030716323494884623, 0.021924228876433527, 0.037009206274320805, 0.023712382694782375, 0.02978307486918852, 0.03723909355317334, 0.26676803494638124, 0.2453334459150751, 0.241753724431774, 0.2090669604272538, 0.21400001513074174, 0.2207886303443195, 0.23724454092046054, 0.22865082124237546, 0.22229513882383767, 0.04554559052460394, 0.04816248817464297, 0.042466678456809515, 0.05830959815046588, 0.05374737441574762, 0.0678923464575455, 0.07359220850688453, 0.08140482313951036, 0.06735953768859326, 0.12382829909088722, 0.14397350664940245, 0.11923812403081957, 0.14984510052682654, 0.12063450850066204, 0.12494293503966547, 0.11810763011086622, 0.14066711303647095, 0.11351073771903064, 0.1507535085513949, 0.19009978636815317, 0.16510322755563756, 0.17737288917734517, 0.1813304051093686, 0.18436643958250054, 0.16718051417271929, 0.17623169013191986, 0.18119192488358937, 0.1207060673540693, 0.14929898800467634, 0.1230633758066868, 0.1244362439894825, 0.13719838909540927, 0.1369516135070109, 0.10762726264760936, 0.12929935405407922, 0.12676661797465116, 0.18264880676612716, 0.1932786381876197, 0.1657886864256507, 0.17534450279645153, 0.17270648427257984, 0.16932241461858266, 0.17446842513001626, 0.1714336137286453, 0.1729304696137538, 0.1512624683076017, 0.15755540930515, 0.1695976867850708, 0.1610791077343583, 0.15860316276281283, 0.15303128177728054, 0.15718398197649586, 0.15910744514704456, 0.1795722395844196, 0.1438626941038036, 0.18150547180486765, 0.1503414366887208, 0.180406637356325, 0.17068045084491712, 0.16923180943332494, 0.17678949779334163, 0.19283472875256935, 0.22187041586061174, 0.23967376833001386, 0.17800507855704018, 0.1368655587039519, 0.16405905847586455, 0.1263345230887042, 0.139659251711033, 0.10999279718281707, 0.19090864295663146, 0.149543643117673, 0.16326799919270563, 0.16747215168168295, 0.17414413024550301, 0.16570558498390042, 0.16870050630151434, 0.17178726976051717, 0.16505538805933395, 0.16845541889696958, 0.1602404935685321, 0.06459015871724205, 0.06341505003616077, 0.05923673032348231, 0.06723559945260327, 0.05693035741741592, 0.07148259048805738, 0.06662873947752723, 0.05431546480531724, 0.05792766080440526]}, "mutation_prompt": null}
{"id": "cb037f8e-90be-49a7-b8fd-b2c3da1f615b", "solution": "import numpy as np\nimport random\n\nclass Enhanced_Adaptive_Frequency_Modulated_Differential_Evolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.F = 0.5  # initial frequency\n        self.CR = 0.5  # initial crossover rate\n        self.pop_size = 50  # initial population size\n        self.population = np.random.uniform(-5.0, 5.0, (self.pop_size, self.dim))\n        self.best_solution = np.inf\n        self.probability = 0.25  # increased probability of probabilistic exploration\n        self.temperature = 100  # initial temperature for simulated annealing\n        self.covariance_matrix = np.eye(self.dim)  # initial covariance matrix\n        self.adaptive_step_size = 0.05  # initial adaptive step size for covariance matrix adaptation\n        self.step_size_learning_rate = 0.001  # learning rate for adaptive step size\n        self.history = np.zeros((self.budget, self.dim))  # history of best solutions\n        self.frequency_modulation_frequency = 0.5  # frequency of frequency modulation\n\n    def evaluate(self, x):\n        return self.func(x)\n\n    def func(self, x):\n        # BBOB test suite of 24 noiseless functions\n        # Here we use the first function f1(x) = sum(x_i^2)\n        return np.sum(x**2)\n\n    def frequency_modulation(self, x):\n        # frequency modulation function\n        return self.F * np.sin(2 * np.pi * self.F * x)\n\n    def adaptive_frequency_modulation(self):\n        # adaptive frequency modulation\n        self.F *= 1.1  # increase frequency\n        if random.random() < self.probability:\n            self.F *= 0.9  # decrease frequency with probability\n        return self.F\n\n    def crossover(self, x1, x2):\n        # crossover operation\n        r = np.random.rand(self.dim)\n        return x1 + r * (x2 - x1)\n\n    def selection(self, x1, x2):\n        # selection operation\n        if self.evaluate(x2) < self.evaluate(x1):\n            return x2\n        else:\n            return x1\n\n    def simulated_annealing(self, x):\n        # simulated annealing\n        new_x = x + np.random.normal(0, 1, self.dim)\n        new_x = np.clip(new_x, -5.0, 5.0)\n        if self.evaluate(new_x) < self.evaluate(x):\n            return new_x\n        else:\n            probability = np.exp(-(self.evaluate(new_x) - self.evaluate(x)) / self.temperature)\n            if random.random() < probability:\n                return new_x\n            else:\n                return x\n        self.temperature *= 0.99  # decrease temperature with a higher rate\n\n    def covariance_matrix_adaptation(self):\n        # covariance matrix adaptation\n        self.covariance_matrix = (1 - self.adaptive_step_size) * self.covariance_matrix + self.adaptive_step_size * np.eye(self.dim)\n        return self.covariance_matrix\n\n    def adaptive_step_size_learning(self):\n        # adaptive step size learning\n        self.adaptive_step_size += self.step_size_learning_rate * (self.evaluate(self.population[0]) - self.evaluate(self.population[-1]))\n        return self.adaptive_step_size\n\n    def history_learning(self):\n        # history learning\n        self.history = np.vstack((self.history, self.population[0]))\n\n    def optimize(self, func):\n        for i in range(self.budget):\n            # evaluate population\n            fitness = [self.evaluate(x) for x in self.population]\n            # get best solution\n            self.best_solution = min(fitness)\n            best_index = fitness.index(self.best_solution)\n            # adaptive frequency modulation\n            self.F = self.adaptive_frequency_modulation()\n            # frequency modulation\n            if random.random() < self.frequency_modulation_frequency:\n                self.population = self.frequency_modulation(self.population)\n            # differential evolution\n            for j in range(self.pop_size):\n                # generate trial vector\n                trial = self.crossover(self.population[j], self.population[random.randint(0, self.pop_size - 1)])\n                # evaluate trial vector\n                trial_fitness = self.evaluate(trial)\n                # selection\n                self.population[j] = self.selection(self.population[j], trial)\n                # update best solution\n                if trial_fitness < self.evaluate(self.population[j]):\n                    self.population[j] = trial\n            # simulated annealing\n            for j in range(self.pop_size):\n                self.population[j] = self.simulated_annealing(self.population[j])\n            # covariance matrix adaptation\n            self.covariance_matrix = self.covariance_matrix_adaptation()\n            # adaptive step size learning\n            self.adaptive_step_size = self.adaptive_step_size_learning()\n            # history learning\n            self.history_learning()\n            # adaptive frequency modulation frequency update\n            if i % 10 == 0:\n                self.frequency_modulation_frequency *= 1.1\n                if self.frequency_modulation_frequency > 1:\n                    self.frequency_modulation_frequency = 1\n        return self.best_solution, self.history\n\n    def __call__(self, func):\n        self.func = func\n        return self.optimize(func)\n\n# example usage\nbudget = 1000\ndim = 10\noptimizer = Enhanced_Adaptive_Frequency_Modulated_Differential_Evolution(budget, dim)\nbest_solution, history = optimizer(lambda x: np.sum(x**2))\nprint(\"Best solution:\", best_solution)\nprint(\"History:\", history)", "name": "Enhanced_Adaptive_Frequency_Modulated_Differential_Evolution", "description": "Enhanced Adaptive Frequency Modulated Differential Evolution with Simulated Annealing and Covariance Matrix Adaptation using Adaptive Frequency Modulation and History Learning with Adaptive Temperature Cooling Schedule and Adaptive Step Size Learning.", "configspace": "", "generation": 27, "fitness": 0.08843682556178468, "feedback": "The algorithm Enhanced_Adaptive_Frequency_Modulated_Differential_Evolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09 with standard deviation 0.07.", "error": "", "parent_id": "62f0f28d-b1f3-482e-a59e-689769ae92f8", "metadata": {"aucs": [0.1983367793989822, 0.1992896934644185, 0.18837122480123902, 0.10630158860754324, 0.22951727978053027, 0.14441318072662035, 0.129519674662672, 0.12387598227539132, 0.11512050280459463, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.03625871486208143, 0.03870704107078948, 0.027351998809660882, 0.05482985380168881, 0.04184815141167564, 0.02525348400132621, 0.02572005855934789, 0.047636654851712645, 0.036592221578330864, 0.035186281768309846, 0.04469881727625091, 0.034575798002305036, 0.027699453776889316, 0.03126361135334954, 0.02320183435370693, 0.03609150254154525, 0.02752071274015866, 0.03224613809277321, 0.07448906857391446, 0.045385788542351646, 0.04289798075468243, 0.053340261193588256, 0.07459234581923391, 0.04245519455714453, 0.07799236313728419, 0.06447217821572215, 0.06050364967302413, 0.06274794416174423, 0.049861647872385007, 0.047338525198587944, 0.08053303879767171, 0.07368351013541996, 0.043903618923968324, 0.059083946906509976, 0.039342318780436814, 9.999999999998899e-05, 0.16317694130545846, 0.16329120682678422, 0.1603197381136684, 0.1507084380199678, 0.07692237234401289, 0.06403019127858078, 0.10686230468534885, 0.06910756580688615, 0.06441712055478943, 0.007132894298617631, 0.004487661022165912, 0.0015706348705062378, 9.999999999998899e-05, 0.00994841535600266, 9.999999999998899e-05, 0.0013455745865780244, 9.999999999998899e-05, 9.999999999998899e-05, 0.12340906600416912, 0.09770992193028527, 0.14058225880643838, 0.1354547432026133, 0.1329151208824969, 0.12943642680810574, 0.16683590023571693, 0.13425688562572702, 0.1236257853293593, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.023066166243934094, 0.020833711562334045, 0.06548973325028984, 0.022206568977189423, 0.05471251951859979, 0.029881846225638053, 0.0912728335116979, 0.048029487800701154, 0.018994219256823652, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.19690728341327646, 0.1828941712638935, 0.186652524065945, 0.15554189664370077, 0.14698998753676973, 0.17355488049851664, 0.20779306142862375, 0.19558198140601013, 0.20786252813098172, 0.016109709679977047, 0.016406246375279654, 0.023086871931832897, 0.054237091008148997, 0.0260305262020476, 0.04371166607595278, 0.039836084406556616, 0.02906026862455069, 0.04740676600320659, 0.12893524996880334, 0.12548892749151386, 0.11842175776965747, 0.11648358569963002, 0.12337556566693697, 0.1126954731168327, 0.12045413724806875, 0.10651784158178523, 0.11831950247201095, 0.1481853714252619, 0.14734120017034125, 0.16596699414193028, 0.18005308992337254, 0.19222959660942807, 0.19036936362495893, 0.13895954712926795, 0.17377774300845283, 0.13711684662749835, 0.08274619225742941, 0.08819301083727638, 0.09304251334307878, 0.1255314542020275, 0.11907891050719865, 0.1345005702728953, 0.08820108419770678, 0.0997711199450243, 0.09744404478249402, 0.21658380612410366, 0.19706626215847445, 0.20219587445642606, 0.2062522125500994, 0.18871277460698732, 0.22374758418866614, 0.20920436116674168, 0.2204291734970062, 0.20959193221653716, 0.15063147478114647, 0.14601772115318412, 0.15979138806934823, 0.14990616816279267, 0.15208367695684444, 0.14518084772722972, 0.15364486247005427, 0.14594098851458315, 0.15177851944055598, 0.16642705243897504, 0.15955350455150763, 0.14941301757304248, 0.19769875950452687, 0.1706906960414144, 0.17306437808363428, 0.12820175725226346, 0.1243503939669699, 0.14308171325037766, 0.16263515371463688, 0.1961443671141626, 0.19335201066273544, 0.1610155006365881, 0.09227097917153626, 0.10390470591006729, 0.14299550925442095, 0.12114275277770081, 0.1285388496278409, 0.17164519995620742, 0.16497887161147717, 0.1705230179808186, 0.17148493995548286, 0.17344600613120087, 0.16779037831309618, 0.17724568128693086, 0.1683492578911775, 0.16908034382186454, 0.062241807847734365, 0.053778226965573994, 0.06455859912261497, 0.06161402591115872, 0.05445295022209651, 0.06617882162292976, 0.06006459732041003, 0.06211546217602082, 0.05651529386031029]}, "mutation_prompt": null}
{"id": "938dd138-9f00-4f19-b268-31a21182129e", "solution": "import numpy as np\nimport random\n\nclass Enhanced_Adaptive_Differential_Evolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.F = 0.5  # initial frequency\n        self.CR = 0.5  # initial crossover rate\n        self.pop_size = 50  # initial population size\n        self.population = np.random.uniform(-5.0, 5.0, (self.pop_size, self.dim))\n        self.best_solution = np.inf\n        self.probability = 0.2  # increased probability of probabilistic exploration\n        self.temperature = 100  # initial temperature for simulated annealing\n        self.covariance_matrix = np.eye(self.dim)  # initial covariance matrix\n        self.adaptive_step_size = 0.1  # initial adaptive step size for covariance matrix adaptation\n        self.step_size_learning_rate = 0.001  # learning rate for adaptive step size\n        self.history = np.zeros((self.budget, self.dim))  # history of best solutions\n        self.multi_modal = False  # flag for multi-modal optimization\n\n    def evaluate(self, x):\n        return self.func(x)\n\n    def func(self, x):\n        # BBOB test suite of 24 noiseless functions\n        # Here we use the first function f1(x) = sum(x_i^2)\n        return np.sum(x**2)\n\n    def frequency_modulation(self, x):\n        # frequency modulation function\n        return self.F * np.sin(2 * np.pi * self.F * x)\n\n    def adaptive_frequency_modulation(self):\n        # adaptive frequency modulation\n        self.F *= 1.1  # increase frequency\n        if random.random() < self.probability:\n            self.F *= 0.9  # decrease frequency with probability\n        return self.F\n\n    def multi_modal_frequency_modulation(self):\n        # multi-modal frequency modulation\n        if self.multi_modal:\n            self.F = np.random.uniform(0.1, 10)\n        return self.F\n\n    def crossover(self, x1, x2):\n        # crossover operation\n        r = np.random.rand(self.dim)\n        return x1 + r * (x2 - x1)\n\n    def selection(self, x1, x2):\n        # selection operation\n        if self.evaluate(x2) < self.evaluate(x1):\n            return x2\n        else:\n            return x1\n\n    def simulated_annealing(self, x):\n        # simulated annealing\n        new_x = x + np.random.normal(0, 1, self.dim)\n        new_x = np.clip(new_x, -5.0, 5.0)\n        if self.evaluate(new_x) < self.evaluate(x):\n            return new_x\n        else:\n            probability = np.exp(-(self.evaluate(new_x) - self.evaluate(x)) / self.temperature)\n            if random.random() < probability:\n                return new_x\n            else:\n                return x\n        self.temperature *= 0.99  # decrease temperature with a higher rate\n\n    def covariance_matrix_adaptation(self):\n        # covariance matrix adaptation\n        self.covariance_matrix = (1 - self.adaptive_step_size) * self.covariance_matrix + self.adaptive_step_size * np.eye(self.dim)\n        return self.covariance_matrix\n\n    def adaptive_step_size_learning(self):\n        # adaptive step size learning\n        self.adaptive_step_size += self.step_size_learning_rate * (self.evaluate(self.population[0]) - self.evaluate(self.population[-1]))\n        return self.adaptive_step_size\n\n    def history_learning(self):\n        # history learning\n        self.history = np.vstack((self.history, self.population[0]))\n\n    def optimize(self, func):\n        for i in range(self.budget):\n            # evaluate population\n            fitness = [self.evaluate(x) for x in self.population]\n            # get best solution\n            self.best_solution = min(fitness)\n            best_index = fitness.index(self.best_solution)\n            # adaptive frequency modulation\n            self.F = self.multi_modal_frequency_modulation()\n            # differential evolution\n            for j in range(self.pop_size):\n                # generate trial vector\n                trial = self.crossover(self.population[j], self.population[random.randint(0, self.pop_size - 1)])\n                # evaluate trial vector\n                trial_fitness = self.evaluate(trial)\n                # selection\n                self.population[j] = self.selection(self.population[j], trial)\n                # update best solution\n                if trial_fitness < self.evaluate(self.population[j]):\n                    self.population[j] = trial\n            # simulated annealing\n            for j in range(self.pop_size):\n                self.population[j] = self.simulated_annealing(self.population[j])\n            # covariance matrix adaptation\n            self.covariance_matrix = self.covariance_matrix_adaptation()\n            # adaptive step size learning\n            self.adaptive_step_size = self.adaptive_step_size_learning()\n            # history learning\n            self.history_learning()\n            # multi-modal optimization\n            if self.multi_modal:\n                self.population = np.random.uniform(-5.0, 5.0, (self.pop_size, self.dim))\n        return self.best_solution, self.history\n\n    def __call__(self, func):\n        self.func = func\n        self.multi_modal = True\n        return self.optimize(func)\n\n# example usage\nbudget = 1000\ndim = 10\noptimizer = Enhanced_Adaptive_Differential_Evolution(budget, dim)\nbest_solution, history = optimizer(lambda x: np.sum(x**2))\nprint(\"Best solution:\", best_solution)\nprint(\"History:\", history)", "name": "Enhanced_Adaptive_Differential_Evolution", "description": "Enhanced Adaptive Frequency Modulation and Covariance Matrix Adaptation with Multi-Modal Optimization and Adaptive Step Size Learning.", "configspace": "", "generation": 28, "fitness": 0.08705293518234851, "feedback": "The algorithm Enhanced_Adaptive_Differential_Evolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09 with standard deviation 0.07.", "error": "", "parent_id": "62f0f28d-b1f3-482e-a59e-689769ae92f8", "metadata": {"aucs": [0.1695152690825722, 0.17076236595918415, 0.1914565960041883, 0.1458253101967948, 0.1335529244626339, 0.14159595312748585, 0.14129543484331974, 0.1576709438662065, 0.15077887517959532, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.03993901673577327, 0.07365184080913256, 0.04158175008293641, 0.04399439763920998, 0.04465038281468314, 0.04541303685016462, 0.0534980431049038, 0.03642278898764273, 0.04040528557886025, 0.0330537725452863, 0.03733214203584667, 0.08161956123709002, 0.04138508982707667, 0.028035310303106575, 0.031050977952847836, 0.03775283729361545, 0.03137494686751674, 0.03298968323990448, 0.06434612489784053, 0.08180718689653799, 0.08031594322329838, 0.07497135059035043, 0.08175748113870718, 0.07881025901550842, 0.07708360554072402, 0.07521105818758334, 0.1216669756491785, 0.1129892474673646, 0.05002455900494074, 0.048869264497100495, 0.06359766809945833, 0.06112685616223967, 0.046554733469692966, 0.04544570480598342, 0.03159370184762078, 0.05373641456500011, 0.1295664777004758, 0.12586798227336438, 0.11245827863773517, 0.1325206701738536, 0.1099682506388685, 0.1112158916739181, 0.14099604562686485, 0.135298319015543, 0.11611900467561265, 0.002001263839809919, 9.999999999998899e-05, 0.0010182278382734333, 0.023208666601126193, 9.999999999998899e-05, 9.999999999998899e-05, 0.011972517329936982, 9.999999999998899e-05, 9.999999999998899e-05, 0.00213929826495185, 0.02000227731453541, 0.01113100448250337, 0.0163739917447443, 0.03929840196259593, 0.02347052842774866, 0.0557930951087916, 0.042444652965320295, 0.004506659758486409, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.033433060910201706, 0.048792790960175636, 0.08110834330449757, 0.0360912431179955, 0.041873833973575914, 0.03245528024308586, 0.06418997157809558, 0.048455093918317704, 0.03886300032051637, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.005360198774082003, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.19392302463299427, 0.1956736311145243, 0.22044178019258187, 0.1941253655684908, 0.16833136456822595, 0.22923698392546155, 0.20290296966425514, 0.20225586182260358, 0.19528604361795143, 0.020133552395876886, 0.03033247165880948, 0.037188311562561416, 0.038579007774889984, 0.03620752834793928, 0.05327025353676873, 0.05099344628672442, 0.035491776782309525, 0.034876808073210475, 0.11958348941108343, 0.11287875400340763, 0.13191513132474153, 0.13925100697547899, 0.12477960162490198, 0.12737241592332882, 0.13377829281926934, 0.1853398848176543, 0.12012411672505718, 0.15464406144927345, 0.15723512918401394, 0.1613932299901245, 0.15449527270025987, 0.21031235101361967, 0.1820200860327552, 0.16242247342346838, 0.17377774300845283, 0.1784877494929824, 0.0998510331711443, 0.11936664777529116, 0.11785585567511081, 0.12322357920492888, 0.14344492055390734, 0.11099521792500544, 0.12867013776627667, 0.12042758724892721, 0.10237355437743045, 0.15813250848873484, 0.15071238689061173, 0.17794942132614144, 0.163682068197345, 0.1474052686749424, 0.15079480173639948, 0.1555263443940348, 0.14775708797526055, 0.1451134082525336, 0.143995021542496, 0.15523724573604214, 0.15383921162958591, 0.15920988765541388, 0.14039874943072506, 0.14293884598334483, 0.13423524852596946, 0.14861628504693747, 0.15695177431698126, 0.15588893571964124, 0.15820320159319523, 0.18817234464337862, 0.2195534941308277, 0.1550191806352108, 0.1501078581474723, 0.17332669306676163, 0.13709950176791752, 0.15489912839594122, 0.15862762264209052, 0.12258459472716243, 0.17351872996472006, 0.1578754562386594, 0.1851441178412161, 0.17873255954654965, 0.11993461235535419, 0.14468203494030263, 0.17399262650738168, 0.16883620348787987, 0.18036433488489712, 0.16649576086347317, 0.19704195679687309, 0.16397902936496234, 0.16030249345113556, 0.16310067389694527, 0.17261763435333854, 0.16145070844826281, 0.05378811685270268, 0.05289229026721365, 0.04381815395047839, 0.04846475809232109, 0.050974335179976515, 0.0495038548900113, 0.048756002303247836, 0.05126508874180291, 0.06614784490808667]}, "mutation_prompt": null}
{"id": "425d2914-1895-4f90-b73b-2d761beca7b2", "solution": "import numpy as np\nimport random\n\nclass Enhanced_Adaptive_Differential_Evolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.F = 0.5  # initial frequency\n        self.CR = 0.5  # initial crossover rate\n        self.pop_size = 50  # initial population size\n        self.population = np.random.uniform(-5.0, 5.0, (self.pop_size, self.dim))\n        self.best_solution = np.inf\n        self.probability = 0.25  # increased probability of probabilistic exploration\n        self.temperature = 100  # initial temperature for simulated annealing\n        self.covariance_matrix = np.eye(self.dim)  # initial covariance matrix\n        self.adaptive_step_size = 0.05  # initial adaptive step size for covariance matrix adaptation\n        self.step_size_learning_rate = 0.002  # learning rate for adaptive step size\n        self.history = np.zeros((self.budget, self.dim))  # history of best solutions\n\n    def evaluate(self, x):\n        return self.func(x)\n\n    def func(self, x):\n        # BBOB test suite of 24 noiseless functions\n        # Here we use the first function f1(x) = sum(x_i^2)\n        return np.sum(x**2)\n\n    def frequency_modulation(self, x):\n        # frequency modulation function\n        return self.F * np.sin(2 * np.pi * self.F * x)\n\n    def adaptive_frequency_modulation(self):\n        # adaptive frequency modulation\n        self.F *= 1.1  # increase frequency\n        if random.random() < self.probability:\n            self.F *= 0.9  # decrease frequency with probability\n        return self.F\n\n    def crossover(self, x1, x2):\n        # crossover operation\n        r = np.random.rand(self.dim)\n        return x1 + r * (x2 - x1)\n\n    def selection(self, x1, x2):\n        # selection operation\n        if self.evaluate(x2) < self.evaluate(x1):\n            return x2\n        else:\n            return x1\n\n    def simulated_annealing(self, x):\n        # simulated annealing\n        new_x = x + np.random.normal(0, 1, self.dim)\n        new_x = np.clip(new_x, -5.0, 5.0)\n        if self.evaluate(new_x) < self.evaluate(x):\n            return new_x\n        else:\n            probability = np.exp(-(self.evaluate(new_x) - self.evaluate(x)) / self.temperature)\n            if random.random() < probability:\n                return new_x\n            else:\n                return x\n        self.temperature *= 0.99  # decrease temperature with a higher rate\n\n    def covariance_matrix_adaptation(self):\n        # covariance matrix adaptation\n        self.covariance_matrix = (1 - self.adaptive_step_size) * self.covariance_matrix + self.adaptive_step_size * np.eye(self.dim)\n        return self.covariance_matrix\n\n    def adaptive_step_size_learning(self):\n        # adaptive step size learning\n        self.adaptive_step_size += self.step_size_learning_rate * (self.evaluate(self.population[0]) - self.evaluate(self.population[-1]))\n        return self.adaptive_step_size\n\n    def history_learning(self):\n        # history learning\n        self.history = np.vstack((self.history, self.population[0]))\n\n    def optimize(self, func):\n        for i in range(self.budget):\n            # evaluate population\n            fitness = [self.evaluate(x) for x in self.population]\n            # get best solution\n            self.best_solution = min(fitness)\n            best_index = fitness.index(self.best_solution)\n            # adaptive frequency modulation\n            self.F = self.adaptive_frequency_modulation()\n            # differential evolution\n            for j in range(self.pop_size):\n                # generate trial vector\n                trial = self.crossover(self.population[j], self.population[random.randint(0, self.pop_size - 1)])\n                # evaluate trial vector\n                trial_fitness = self.evaluate(trial)\n                # selection\n                self.population[j] = self.selection(self.population[j], trial)\n                # update best solution\n                if trial_fitness < self.evaluate(self.population[j]):\n                    self.population[j] = trial\n            # simulated annealing\n            for j in range(self.pop_size):\n                self.population[j] = self.simulated_annealing(self.population[j])\n            # covariance matrix adaptation\n            self.covariance_matrix = self.covariance_matrix_adaptation()\n            # adaptive step size learning\n            self.adaptive_step_size = self.adaptive_step_size_learning()\n            # history learning\n            self.history_learning()\n            # add noise to best solution\n            self.best_solution += np.random.normal(0, 0.1, self.dim)\n            self.best_solution = np.clip(self.best_solution, -5.0, 5.0)\n        return self.best_solution, self.history\n\n    def __call__(self, func):\n        self.func = func\n        return self.optimize(func)\n\n# example usage\nbudget = 1000\ndim = 10\noptimizer = Enhanced_Adaptive_Differential_Evolution(budget, dim)\nbest_solution, history = optimizer(lambda x: np.sum(x**2))\nprint(\"Best solution:\", best_solution)\nprint(\"History:\", history)", "name": "Enhanced_Adaptive_Differential_Evolution", "description": "Enhanced Adaptive Frequency Modulation with Adaptive Covariance Matrix, Simulated Annealing, and History Learning for Black Box Optimization", "configspace": "", "generation": 29, "fitness": 0.10867823460154764, "feedback": "The algorithm Enhanced_Adaptive_Differential_Evolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.11 with standard deviation 0.07.", "error": "", "parent_id": "62f0f28d-b1f3-482e-a59e-689769ae92f8", "metadata": {"aucs": [0.23227395524217342, 0.27857175574296567, 0.2484250764093049, 0.20558139812637966, 0.21198896267042144, 0.2070040797529057, 0.20356123615714805, 0.21406321224382008, 0.2243245337130083, 9.999999999998899e-05, 0.004345673730034694, 9.999999999998899e-05, 9.999999999998899e-05, 0.0008964114944743917, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.05007232654990734, 0.06280791577692002, 0.05678635309279567, 0.06780728658442026, 0.06441437951103923, 0.05857972435915926, 0.08462211592628777, 0.06049282349229346, 0.06189696637786091, 0.04422437324660167, 0.05268434228084751, 0.0484341683330598, 0.04891522611386845, 0.054052900079256005, 0.04612020300645103, 0.05026632892443306, 0.05133774943709635, 0.04099426366072201, 0.1271809125658987, 0.09753815696299917, 0.10815626364945075, 0.09430718965543472, 0.12253832772524942, 0.2164411234679362, 0.12577437743718456, 0.10578364754857905, 0.1372989123026951, 0.09787436555141027, 0.07721586078746201, 0.08226244312847719, 0.07246542622818708, 0.07040045139553841, 0.0939837434309938, 0.08588951557024616, 0.06488479241695932, 0.12582620293681468, 0.15486037018323784, 0.15295981476252218, 0.15976803773323722, 0.17554897214341858, 0.1666930129037939, 0.16824699725904357, 0.13211982552472779, 0.15338629409682014, 0.13995423026834597, 0.07570060540969792, 0.07149801051460869, 0.09817634646494011, 0.05393851245047443, 0.06469169949060638, 0.042829192487202405, 0.08655167136957331, 0.08858092381826099, 0.08849779661507051, 0.09086456317587455, 0.12113483167786165, 0.10458803225914526, 0.10938415487849484, 0.09906341221717962, 0.1169614005302354, 0.09681926742592839, 0.10581044576354792, 0.11024358461467765, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.08468713841325148, 0.056929928664603335, 0.07560652739384599, 0.056401762719181736, 0.036781295575056716, 0.042612698483412204, 0.06802619612630989, 0.0558401510245341, 0.04637631493146133, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.01468305290269345, 0.023262530765805645, 0.020593350176869363, 0.02736942835491174, 0.03851610797499305, 0.04820284332582836, 0.042916745291221914, 0.030355827170696847, 0.02864204015902483, 0.22719538060960354, 0.23291015067466825, 0.24141651904109096, 0.22234162269432622, 0.2429067522933237, 0.22121727864950125, 0.24049754954625369, 0.234456027433681, 0.250865633806173, 0.05420282812059962, 0.05219543601607746, 0.07110488287580918, 0.07611941093269248, 0.055979656938136446, 0.07165674131027389, 0.07320565805933943, 0.062105243435166435, 0.06998506273402749, 0.1163973721953282, 0.10643848744903606, 0.11842175776965747, 0.12565426883749553, 0.1577057695554609, 0.11790504892957965, 0.11128537672990313, 0.11260425887760916, 0.10383654816147336, 0.1652947050949526, 0.1653590932247967, 0.17415112494090812, 0.18030236704539349, 0.18667168010890678, 0.20412279932593913, 0.15622829224750057, 0.17439938602769312, 0.15955834630412336, 0.12984518424683178, 0.11423950800973193, 0.11968212622672347, 0.13786183520815487, 0.1295308522869838, 0.15672822300424327, 0.12440208274988895, 0.12833493260045048, 0.13011344408707481, 0.1608197047479032, 0.15398363916341729, 0.18454007247473336, 0.18120721318845368, 0.1919823850045449, 0.17270027354938888, 0.17916366206368384, 0.1736677193208772, 0.1734316386390421, 0.15913680061512658, 0.1630010161346387, 0.16181457083606365, 0.16029440379506155, 0.15829775483871977, 0.1553796985089111, 0.15079344999242772, 0.15985629658446765, 0.15591956953453923, 0.1780221250881917, 0.1640727578514285, 0.15171498849340315, 0.2654415300188241, 0.17188263405819804, 0.22311145522455933, 0.17919516162011917, 0.17691330633892122, 0.12803750602664365, 0.14175116013160172, 0.1575490438480074, 0.20674039000589806, 0.17658616663941573, 0.13040657685837798, 0.17290752763255002, 0.14535381432302874, 0.14484484335508674, 0.14026843437982084, 0.17763670377281415, 0.1817772984676842, 0.17891933288586137, 0.17400432976301494, 0.17174901055218217, 0.16679035421291954, 0.18894380710404357, 0.16474417233245153, 0.17054667043359417, 0.05576379546128174, 0.06433037407434405, 0.06019623173913513, 0.07632231390495703, 0.06090335425789162, 0.05663329153557606, 0.06841478911808119, 0.05775880869539485, 0.07029463803100156]}, "mutation_prompt": null}
{"id": "ce491266-b3d7-43cc-9b2d-8337663d7b41", "solution": "import numpy as np\nimport random\n\nclass Probabilistic_Frequency_Modulation_Enhanced_Adaptive_Differential_Evolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.F = 0.5  # initial frequency\n        self.CR = 0.5  # initial crossover rate\n        self.pop_size = 50  # initial population size\n        self.population = np.random.uniform(-5.0, 5.0, (self.pop_size, self.dim))\n        self.best_solution = np.inf\n        self.probability = 0.25  # increased probability of probabilistic exploration\n        self.temperature = 100  # initial temperature for simulated annealing\n        self.covariance_matrix = np.eye(self.dim)  # initial covariance matrix\n        self.adaptive_step_size = 0.1  # initial adaptive step size for covariance matrix adaptation\n        self.step_size_learning_rate = 0.001  # learning rate for adaptive step size\n        self.history = np.zeros((self.budget, self.dim))  # history of best solutions\n        self.history_index = 0  # index for history learning\n\n    def evaluate(self, x):\n        return self.func(x)\n\n    def func(self, x):\n        # BBOB test suite of 24 noiseless functions\n        # Here we use the first function f1(x) = sum(x_i^2)\n        return np.sum(x**2)\n\n    def frequency_modulation(self, x):\n        # frequency modulation function\n        return self.F * np.sin(2 * np.pi * self.F * x)\n\n    def adaptive_frequency_modulation(self):\n        # adaptive frequency modulation\n        self.F *= 1.1  # increase frequency\n        if random.random() < self.probability:\n            self.F *= 0.9  # decrease frequency with probability\n        return self.F\n\n    def crossover(self, x1, x2):\n        # crossover operation\n        r = np.random.rand(self.dim)\n        return x1 + r * (x2 - x1)\n\n    def selection(self, x1, x2):\n        # selection operation\n        if self.evaluate(x2) < self.evaluate(x1):\n            return x2\n        else:\n            return x1\n\n    def simulated_annealing(self, x):\n        # simulated annealing\n        new_x = x + np.random.normal(0, 1, self.dim)\n        new_x = np.clip(new_x, -5.0, 5.0)\n        if self.evaluate(new_x) < self.evaluate(x):\n            return new_x\n        else:\n            probability = np.exp(-(self.evaluate(new_x) - self.evaluate(x)) / self.temperature)\n            if random.random() < probability:\n                return new_x\n            else:\n                return x\n        self.temperature *= 0.995  # decrease temperature with a higher rate\n\n    def covariance_matrix_adaptation(self):\n        # covariance matrix adaptation\n        self.covariance_matrix = (1 - self.adaptive_step_size) * self.covariance_matrix + self.adaptive_step_size * np.eye(self.dim)\n        return self.covariance_matrix\n\n    def adaptive_step_size_learning(self):\n        # adaptive step size learning\n        self.adaptive_step_size += self.step_size_learning_rate * (self.evaluate(self.population[0]) - self.evaluate(self.population[-1]))\n        return self.adaptive_step_size\n\n    def history_learning(self):\n        # history learning\n        self.history[self.history_index] = self.population[0]\n        self.history_index = (self.history_index + 1) % self.budget\n\n    def optimize(self, func):\n        for i in range(self.budget):\n            # evaluate population\n            fitness = [self.evaluate(x) for x in self.population]\n            # get best solution\n            self.best_solution = min(fitness)\n            best_index = fitness.index(self.best_solution)\n            # adaptive frequency modulation\n            self.F = self.adaptive_frequency_modulation()\n            # differential evolution\n            for j in range(self.pop_size):\n                # generate trial vector\n                trial = self.crossover(self.population[j], self.population[random.randint(0, self.pop_size - 1)])\n                # evaluate trial vector\n                trial_fitness = self.evaluate(trial)\n                # selection\n                self.population[j] = self.selection(self.population[j], trial)\n                # update best solution\n                if trial_fitness < self.evaluate(self.population[j]):\n                    self.population[j] = trial\n            # simulated annealing\n            for j in range(self.pop_size):\n                self.population[j] = self.simulated_annealing(self.population[j])\n            # covariance matrix adaptation\n            self.covariance_matrix = self.covariance_matrix_adaptation()\n            # adaptive step size learning\n            self.adaptive_step_size = self.adaptive_step_size_learning()\n            # history learning\n            self.history_learning()\n        return self.best_solution, self.history\n\n    def __call__(self, func):\n        self.func = func\n        return self.optimize(func)\n\n# example usage\nbudget = 1000\ndim = 10\noptimizer = Probabilistic_Frequency_Modulation_Enhanced_Adaptive_Differential_Evolution(budget, dim)\nbest_solution, history = optimizer(lambda x: np.sum(x**2))\nprint(\"Best solution:\", best_solution)\nprint(\"History:\", history)", "name": "Probabilistic_Frequency_Modulation_Enhanced_Adaptive_Differential_Evolution", "description": "Probabilistic Frequency Modulation Enhanced Adaptive Differential Evolution with Covariance Matrix Adaptation and Adaptive Step Size Learning, Simulated Annealing with Adaptive Temperature Cooling Schedule and Adaptive Step Size Learning, and History Learning.", "configspace": "", "generation": 30, "fitness": 0.11132444619371283, "feedback": "The algorithm Probabilistic_Frequency_Modulation_Enhanced_Adaptive_Differential_Evolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.11 with standard deviation 0.07.", "error": "", "parent_id": "62f0f28d-b1f3-482e-a59e-689769ae92f8", "metadata": {"aucs": [0.25845246873291305, 0.2466517368624701, 0.2340987316169515, 0.19302636270745466, 0.20888255446058102, 0.21142630043830601, 0.2289201547598836, 0.20022220701940996, 0.21063133626205344, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.053446749095371726, 0.058617098614016094, 0.08505214518448578, 0.08407096674253534, 0.07077887641066116, 0.06619495996594071, 0.059799297097854254, 0.054772485045762376, 0.06838842214830909, 0.05107942526068987, 0.04636657105797837, 0.05206703103970267, 0.05731718164941979, 0.05387557213504024, 0.04487560470536878, 0.05148287563275866, 0.0611713305046776, 0.04950514036137432, 0.1365892138171957, 0.1134291777333456, 0.09553306819080642, 0.1680893784436568, 0.12197061223203032, 0.1155296820381424, 0.17232053022059757, 0.13379316317591294, 0.16541357558360215, 0.1122234787869254, 0.08634778763924644, 0.07723305615877507, 0.09311366508248042, 0.07490533054894533, 0.06303031739806952, 0.10353290434811191, 0.09047086599501142, 0.09364293615572827, 0.1527445334017662, 0.172765699511495, 0.13900695830866427, 0.17008981153868152, 0.16316252387616137, 0.15203060896843112, 0.171837882011323, 0.13843382228016288, 0.1340205353897549, 0.05666481469821172, 0.046886070699136306, 0.07164196847506898, 0.047105339600565066, 0.03990856466923087, 0.07629768011496485, 0.08751041106570268, 0.10179840544856944, 0.06608179033061712, 0.12155517452058917, 0.11386391201852131, 0.10004136139973197, 0.10471959829690036, 0.10365205354189944, 0.12684734762846583, 0.08862033663910074, 0.10231328626865888, 0.10328626456939605, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.0646719070481313, 0.08194303029891037, 0.0815270745822334, 0.0719196027446738, 0.04887805524521993, 0.037689445742171146, 0.07637645058207576, 0.05576577047918596, 0.0689291078961729, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.019534304150330195, 0.019430302603283933, 0.04481424929072908, 0.02581728418024054, 0.025559381456569663, 0.03875644187197902, 0.03060300444566455, 0.027107863194921422, 0.046305339500742226, 0.23429360349539363, 0.2626693831245118, 0.22171437139828654, 0.22612956792023253, 0.19915851384780459, 0.23748672221558587, 0.2248945152556202, 0.21788994307439769, 0.25915976048344624, 0.054784433802761434, 0.06142697813906306, 0.05104245302409216, 0.069573111060374, 0.06843001600353327, 0.05926171885533371, 0.07002987565693763, 0.0606705571312246, 0.06858658693564634, 0.11200513015638203, 0.11827893274379364, 0.12560523149878788, 0.1371924893002905, 0.11910122318119543, 0.12088808506534021, 0.14359744538050367, 0.14233711447044373, 0.11785190654684718, 0.16201557812805856, 0.1665815816416263, 0.18067500642995726, 0.1805661857775649, 0.19570407764944975, 0.18235231617418612, 0.16833349988870994, 0.17377774300845283, 0.16444227920524335, 0.11754364090950842, 0.13341562939461094, 0.11703323649708963, 0.12672045720735314, 0.1250787439309956, 0.1369329078464362, 0.11626763033932752, 0.12681042076263438, 0.11805756804842327, 0.19382128240632368, 0.1822762112048072, 0.17740139067643845, 0.17953849704893587, 0.19127377016379155, 0.18497495911922146, 0.18122471583309796, 0.2014060120135418, 0.19681609195145566, 0.1633973406135738, 0.15978006440054937, 0.1692118817208913, 0.15911917421720223, 0.15727119799291978, 0.1601322188877995, 0.1616458491980871, 0.1617935720546695, 0.15453389904920056, 0.20810230596255586, 0.1511441635550953, 0.16831361142351653, 0.22659650156311073, 0.2038437151347271, 0.20651011074915226, 0.21802296255082387, 0.2040582079292007, 0.16378770881922577, 0.20855573297854968, 0.21633111739956723, 0.21662618428679115, 0.18074449133304338, 0.13994045894140528, 0.2545332584056722, 0.11514779416618948, 0.17884206735472163, 0.1456895236423239, 0.16205727970322947, 0.16921957602484783, 0.16536202933880173, 0.1665030925693345, 0.16168467871050363, 0.19214311230950643, 0.16513789800170275, 0.17356549111049158, 0.21350314602248177, 0.05539087414933552, 0.07315986610425351, 0.05522799248276966, 0.06261928055291777, 0.054325029983253614, 0.06266879340169329, 0.059678715207397204, 0.06495907284945912, 0.07054053231879731]}, "mutation_prompt": null}
{"id": "ff8e9f56-adb1-4a4a-8cc2-9a16058dbf18", "solution": "import numpy as np\nimport random\n\nclass Adaptive_Frequency_Modulation_Enhanced_Differential_Evolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.F = 0.5  # initial frequency\n        self.CR = 0.5  # initial crossover rate\n        self.pop_size = 50  # initial population size\n        self.population = np.random.uniform(-5.0, 5.0, (self.pop_size, self.dim))\n        self.best_solution = np.inf\n        self.probability = 0.2  # increased probability of probabilistic exploration\n        self.temperature = 100  # initial temperature for simulated annealing\n        self.covariance_matrix = np.eye(self.dim)  # initial covariance matrix\n        self.adaptive_step_size = 0.1  # initial adaptive step size for covariance matrix adaptation\n        self.step_size_learning_rate = 0.001  # learning rate for adaptive step size\n        self.history = np.zeros((self.budget, self.dim))  # history of best solutions\n        self.history_learning_rate = 0.05  # learning rate for history learning\n        self.history_adaptive_frequency_modulation = 0.1  # adaptive frequency modulation for history learning\n\n    def evaluate(self, x):\n        return self.func(x)\n\n    def func(self, x):\n        # BBOB test suite of 24 noiseless functions\n        # Here we use the first function f1(x) = sum(x_i^2)\n        return np.sum(x**2)\n\n    def frequency_modulation(self, x):\n        # frequency modulation function\n        return self.F * np.sin(2 * np.pi * self.F * x)\n\n    def adaptive_frequency_modulation(self):\n        # adaptive frequency modulation\n        self.F *= 1.1  # increase frequency\n        if random.random() < self.probability:\n            self.F *= 0.9  # decrease frequency with probability\n        return self.F\n\n    def history_adaptive_frequency_modulation(self):\n        # adaptive frequency modulation for history learning\n        self.history_adaptive_frequency_modulation *= 1.1  # increase frequency\n        if random.random() < self.history_adaptive_frequency_modulation:\n            self.history_adaptive_frequency_modulation *= 0.9  # decrease frequency with probability\n        return self.history_adaptive_frequency_modulation\n\n    def crossover(self, x1, x2):\n        # crossover operation\n        r = np.random.rand(self.dim)\n        return x1 + r * (x2 - x1)\n\n    def selection(self, x1, x2):\n        # selection operation\n        if self.evaluate(x2) < self.evaluate(x1):\n            return x2\n        else:\n            return x1\n\n    def simulated_annealing(self, x):\n        # simulated annealing\n        new_x = x + np.random.normal(0, 1, self.dim)\n        new_x = np.clip(new_x, -5.0, 5.0)\n        if self.evaluate(new_x) < self.evaluate(x):\n            return new_x\n        else:\n            probability = np.exp(-(self.evaluate(new_x) - self.evaluate(x)) / self.temperature)\n            if random.random() < probability:\n                return new_x\n            else:\n                return x\n        self.temperature *= 0.99  # decrease temperature with a higher rate\n\n    def covariance_matrix_adaptation(self):\n        # covariance matrix adaptation\n        self.covariance_matrix = (1 - self.adaptive_step_size) * self.covariance_matrix + self.adaptive_step_size * np.eye(self.dim)\n        return self.covariance_matrix\n\n    def adaptive_step_size_learning(self):\n        # adaptive step size learning\n        self.adaptive_step_size += self.step_size_learning_rate * (self.evaluate(self.population[0]) - self.evaluate(self.population[-1]))\n        return self.adaptive_step_size\n\n    def history_learning(self):\n        # history learning\n        self.history = np.vstack((self.history, self.population[0]))\n        self.history *= (1 - self.history_learning_rate) + self.history_learning_rate * self.history_adaptive_frequency_modulation()\n        return self.history\n\n    def optimize(self, func):\n        for i in range(self.budget):\n            # evaluate population\n            fitness = [self.evaluate(x) for x in self.population]\n            # get best solution\n            self.best_solution = min(fitness)\n            best_index = fitness.index(self.best_solution)\n            # adaptive frequency modulation\n            self.F = self.adaptive_frequency_modulation()\n            # differential evolution\n            for j in range(self.pop_size):\n                # generate trial vector\n                trial = self.crossover(self.population[j], self.population[random.randint(0, self.pop_size - 1)])\n                # evaluate trial vector\n                trial_fitness = self.evaluate(trial)\n                # selection\n                self.population[j] = self.selection(self.population[j], trial)\n                # update best solution\n                if trial_fitness < self.evaluate(self.population[j]):\n                    self.population[j] = trial\n            # simulated annealing\n            for j in range(self.pop_size):\n                self.population[j] = self.simulated_annealing(self.population[j])\n            # covariance matrix adaptation\n            self.covariance_matrix = self.covariance_matrix_adaptation()\n            # adaptive step size learning\n            self.adaptive_step_size = self.adaptive_step_size_learning()\n            # history learning\n            self.history = self.history_learning()\n        return self.best_solution, self.history\n\n    def __call__(self, func):\n        self.func = func\n        return self.optimize(func)\n\n# example usage\nbudget = 1000\ndim = 10\noptimizer = Adaptive_Frequency_Modulation_Enhanced_Differential_Evolution(budget, dim)\nbest_solution, history = optimizer(lambda x: np.sum(x**2))\nprint(\"Best solution:\", best_solution)\nprint(\"History:\", history)", "name": "Adaptive_Frequency_Modulation_Enhanced_Differential_Evolution", "description": "Novel Metaheuristic Algorithm: Adaptive Frequency Modulation Enhanced Differential Evolution with Covariance Matrix Adaptation and Simulated Annealing with Adaptive Temperature Cooling Schedule and Adaptive Step Size Learning, which incorporates a history learning mechanism to refine the strategy.", "configspace": "", "generation": 31, "fitness": -Infinity, "feedback": "An exception occurred: TypeError(\"'float' object is not callable\").", "error": "TypeError(\"'float' object is not callable\")", "parent_id": "62f0f28d-b1f3-482e-a59e-689769ae92f8", "metadata": {}, "mutation_prompt": null}
{"id": "b402887e-a86f-4b10-af2f-fea377e0aad0", "solution": "import numpy as np\nimport random\n\nclass Novel_Metaheuristic_Algorithm:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.F = 0.5  # initial frequency\n        self.CR = 0.5  # initial crossover rate\n        self.pop_size = 50  # initial population size\n        self.population = np.random.uniform(-5.0, 5.0, (self.pop_size, self.dim))\n        self.best_solution = np.inf\n        self.probability = 0.2  # increased probability of probabilistic exploration\n        self.temperature = 100  # initial temperature for simulated annealing\n        self.covariance_matrix = np.eye(self.dim)  # initial covariance matrix\n        self.adaptive_step_size = 0.1  # initial adaptive step size for covariance matrix adaptation\n        self.step_size_learning_rate = 0.001  # learning rate for adaptive step size\n        self.history = np.zeros((self.budget, self.dim))  # history of best solutions\n        self.ensemble_size = 5  # size of ensemble for frequency modulation\n        self.ensemble = np.zeros((self.ensemble_size, self.dim))  # ensemble for frequency modulation\n        self.ensemble_indices = np.arange(self.ensemble_size)  # indices of ensemble\n\n    def evaluate(self, x):\n        return self.func(x)\n\n    def func(self, x):\n        # BBOB test suite of 24 noiseless functions\n        # Here we use the first function f1(x) = sum(x_i^2)\n        return np.sum(x**2)\n\n    def frequency_modulation(self, x):\n        # frequency modulation function\n        return self.F * np.sin(2 * np.pi * self.F * x)\n\n    def adaptive_frequency_modulation(self):\n        # adaptive frequency modulation\n        self.F *= 1.1  # increase frequency\n        if random.random() < self.probability:\n            self.F *= 0.9  # decrease frequency with probability\n        return self.F\n\n    def ensemble_frequency_modulation(self):\n        # ensemble frequency modulation\n        self.ensemble = np.zeros((self.ensemble_size, self.dim))\n        for i in range(self.ensemble_size):\n            self.ensemble[i] = self.frequency_modulation(self.population[random.randint(0, self.pop_size - 1)])\n        return self.ensemble\n\n    def crossover(self, x1, x2):\n        # crossover operation\n        r = np.random.rand(self.dim)\n        return x1 + r * (x2 - x1)\n\n    def selection(self, x1, x2):\n        # selection operation\n        if self.evaluate(x2) < self.evaluate(x1):\n            return x2\n        else:\n            return x1\n\n    def simulated_annealing(self, x):\n        # simulated annealing\n        new_x = x + np.random.normal(0, 1, self.dim)\n        new_x = np.clip(new_x, -5.0, 5.0)\n        if self.evaluate(new_x) < self.evaluate(x):\n            return new_x\n        else:\n            probability = np.exp(-(self.evaluate(new_x) - self.evaluate(x)) / self.temperature)\n            if random.random() < probability:\n                return new_x\n            else:\n                return x\n        self.temperature *= 0.99  # decrease temperature with a higher rate\n\n    def covariance_matrix_adaptation(self):\n        # covariance matrix adaptation\n        self.covariance_matrix = (1 - self.adaptive_step_size) * self.covariance_matrix + self.adaptive_step_size * np.eye(self.dim)\n        return self.covariance_matrix\n\n    def adaptive_step_size_learning(self):\n        # adaptive step size learning\n        self.adaptive_step_size += self.step_size_learning_rate * (self.evaluate(self.population[0]) - self.evaluate(self.population[-1]))\n        return self.adaptive_step_size\n\n    def history_learning(self):\n        # history learning\n        self.history = np.vstack((self.history, self.population[0]))\n\n    def ensemble_learning(self):\n        # ensemble learning\n        self.ensemble_indices = np.random.choice(self.pop_size, self.ensemble_size, replace=False)\n        self.ensemble = np.zeros((self.ensemble_size, self.dim))\n        for i in range(self.ensemble_size):\n            self.ensemble[i] = self.population[self.ensemble_indices[i]]\n\n    def optimize(self, func):\n        for i in range(self.budget):\n            # evaluate population\n            fitness = [self.evaluate(x) for x in self.population]\n            # get best solution\n            self.best_solution = min(fitness)\n            best_index = fitness.index(self.best_solution)\n            # ensemble frequency modulation\n            self.ensemble = self.ensemble_frequency_modulation()\n            # differential evolution\n            for j in range(self.pop_size):\n                # generate trial vector\n                trial = self.crossover(self.population[j], self.population[random.randint(0, self.pop_size - 1)])\n                # evaluate trial vector\n                trial_fitness = self.evaluate(trial)\n                # selection\n                self.population[j] = self.selection(self.population[j], trial)\n                # update best solution\n                if trial_fitness < self.evaluate(self.population[j]):\n                    self.population[j] = trial\n            # simulated annealing\n            for j in range(self.pop_size):\n                self.population[j] = self.simulated_annealing(self.population[j])\n            # covariance matrix adaptation\n            self.covariance_matrix = self.covariance_matrix_adaptation()\n            # adaptive step size learning\n            self.adaptive_step_size = self.adaptive_step_size_learning()\n            # history learning\n            self.history_learning()\n            # ensemble learning\n            self.ensemble_learning()\n        return self.best_solution, self.history\n\n    def __call__(self, func):\n        self.func = func\n        return self.optimize(func)\n\n# example usage\nbudget = 1000\ndim = 10\noptimizer = Novel_Metaheuristic_Algorithm(budget, dim)\nbest_solution, history = optimizer(lambda x: np.sum(x**2))\nprint(\"Best solution:\", best_solution)\nprint(\"History:\", history)", "name": "Novel_Metaheuristic_Algorithm", "description": "A Novel Metaheuristic Algorithm that Combines Frequency Modulation, Differential Evolution, Simulated Annealing, and Covariance Matrix Adaptation with Adaptive Frequency Modulation, Adaptive Step Size Learning, and History Learning.", "configspace": "", "generation": 32, "fitness": 0.1092194105649216, "feedback": "The algorithm Novel_Metaheuristic_Algorithm got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.11 with standard deviation 0.07.", "error": "", "parent_id": "62f0f28d-b1f3-482e-a59e-689769ae92f8", "metadata": {"aucs": [0.2214787961215341, 0.2198459600032341, 0.2474776910412163, 0.21748897797413558, 0.20292888723158298, 0.2012276103823275, 0.22098449053809943, 0.21657151205888103, 0.22025659631525762, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.06531537415488653, 0.06957501902867147, 0.05634863336104534, 0.05677570693363887, 0.06282788945204143, 0.07010244989832404, 0.06716621560198877, 0.06883741850912051, 0.057567087016613394, 0.045820313234961385, 0.06378018679255004, 0.04376799878281401, 0.0637079925316415, 0.07165584458740848, 0.04820144780098112, 0.06094975654692003, 0.052246965694846215, 0.04353526037594224, 0.10816082744370747, 0.09346476840312312, 0.10712510514870732, 0.10439707422000122, 0.12848005441721178, 0.10199331974387937, 0.12783002741079474, 0.12996323519623854, 0.12225854195512598, 0.09407699487286703, 0.10508580297631664, 0.08658438368888655, 0.10243242679246745, 0.09480706380368753, 0.08545705969059525, 0.0967776950367546, 0.09999964129740269, 0.09625408423378268, 0.1805541411973336, 0.1747855553987714, 0.1347346185446966, 0.15334281316208032, 0.18699868828720334, 0.17676985094554076, 0.13635327708282852, 0.1493190789067741, 0.17692750769566923, 0.049050866082558375, 0.048349323655556886, 0.09272234370769239, 0.09759178183640504, 0.03724158317136883, 0.036880007359990086, 0.07466134745139974, 0.0608489597213675, 0.09183771839126897, 0.10692177762987909, 0.12135580730769213, 0.09218367022574547, 0.12464701339139284, 0.10690130641945306, 0.08850579715793361, 0.0974039075515235, 0.1061267024698922, 0.09550816287601205, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.0962820200997907, 0.09489947607489035, 0.08690713032336916, 0.033955074422091114, 0.032126589605177336, 0.023822451243281306, 0.05994253219001966, 0.08744393267483208, 0.08132900889368955, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.01799823137402723, 0.036019951334409606, 0.030533327899394336, 0.05263800627305404, 0.03430629382332029, 0.037342015662502503, 0.03643734433525514, 0.03286503590789902, 0.03547821892122616, 0.25045930879139755, 0.24032716838949386, 0.27084986006822276, 0.22207217310138494, 0.20198859325310703, 0.20057784688478475, 0.2144334840759594, 0.2287585386714578, 0.25597069021148333, 0.05293098259023732, 0.053652429350718234, 0.05524003948651224, 0.06409724783416881, 0.0487457831561825, 0.05720083077067484, 0.09007137394743114, 0.057029625573253306, 0.06894106950075163, 0.09812875527936049, 0.1093193960880835, 0.1403876301422351, 0.11377966949408147, 0.121727310461894, 0.1293815171706567, 0.1502925170960252, 0.10951734999303042, 0.11867916488789432, 0.16603029394789248, 0.16196229214208835, 0.16744340965251014, 0.19812736419956334, 0.1781857118346819, 0.19101285470491525, 0.17328077868828895, 0.19633412175222176, 0.16389006104248016, 0.11260768834477186, 0.1387217920241366, 0.12896975466720229, 0.12947133686181356, 0.13838385716853463, 0.15092949820782664, 0.11979762432317043, 0.1467423844778094, 0.1305851019009835, 0.1658865025570878, 0.1867491112011055, 0.1791378152708044, 0.16614944417234623, 0.178970208454028, 0.18313729589209682, 0.22114508495126572, 0.20919740168556722, 0.1741569006360001, 0.1561920012911684, 0.1573869504880686, 0.16709135545057163, 0.16307389069263445, 0.1752025658597406, 0.1581650309198097, 0.17204604740027352, 0.15884505248850456, 0.16558835004839279, 0.15766040767578737, 0.1634045348923171, 0.1604347099535185, 0.19889551356737234, 0.2035945159403716, 0.19230670779001235, 0.20551512633899438, 0.1704701019041147, 0.13178983402303324, 0.20020764006863434, 0.1910502099950243, 0.19840594107609721, 0.15578486095613242, 0.16880930839235808, 0.16152596543524456, 0.12318573903682484, 0.11610950469577841, 0.12547875375422624, 0.18197456530679068, 0.17100416123861817, 0.1633739850302236, 0.16050311577675236, 0.16569717535451234, 0.17112559721714038, 0.1676867316283911, 0.18071331832056525, 0.16588802696052918, 0.05740551042037367, 0.0556874631855786, 0.0561763070458986, 0.06646911598405691, 0.06649436546354104, 0.05421291638909986, 0.060931750986616295, 0.06122652626987424, 0.07189494528935048]}, "mutation_prompt": null}
{"id": "7340ba9b-b17b-4e2e-bb27-db576a71db35", "solution": "import numpy as np\nimport random\n\nclass Enhanced_Metaheuristic_Optimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.F = 0.5  # initial frequency\n        self.CR = 0.5  # initial crossover rate\n        self.pop_size = 50  # initial population size\n        self.population = np.random.uniform(-5.0, 5.0, (self.pop_size, self.dim))\n        self.best_solution = np.inf\n        self.probability = 0.2  # increased probability of probabilistic exploration\n        self.temperature = 100  # initial temperature for simulated annealing\n        self.covariance_matrix = np.eye(self.dim)  # initial covariance matrix\n        self.adaptive_step_size = 0.1  # initial adaptive step size for covariance matrix adaptation\n        self.step_size_learning_rate = 0.001  # learning rate for adaptive step size\n        self.history = np.zeros((self.budget, self.dim))  # history of best solutions\n        self.history_learning_rate = 0.1  # learning rate for history learning\n\n    def evaluate(self, x):\n        return self.func(x)\n\n    def func(self, x):\n        # BBOB test suite of 24 noiseless functions\n        # Here we use the first function f1(x) = sum(x_i^2)\n        return np.sum(x**2)\n\n    def frequency_modulation(self, x):\n        # frequency modulation function\n        return self.F * np.sin(2 * np.pi * self.F * x)\n\n    def adaptive_frequency_modulation(self):\n        # adaptive frequency modulation\n        self.F *= 1.1  # increase frequency\n        if random.random() < self.probability:\n            self.F *= 0.9  # decrease frequency with probability\n        return self.F\n\n    def crossover(self, x1, x2):\n        # crossover operation\n        r = np.random.rand(self.dim)\n        return x1 + r * (x2 - x1)\n\n    def selection(self, x1, x2):\n        # selection operation\n        if self.evaluate(x2) < self.evaluate(x1):\n            return x2\n        else:\n            return x1\n\n    def simulated_annealing(self, x):\n        # simulated annealing\n        new_x = x + np.random.normal(0, 1, self.dim)\n        new_x = np.clip(new_x, -5.0, 5.0)\n        if self.evaluate(new_x) < self.evaluate(x):\n            return new_x\n        else:\n            probability = np.exp(-(self.evaluate(new_x) - self.evaluate(x)) / self.temperature)\n            if random.random() < probability:\n                return new_x\n            else:\n                return x\n        self.temperature *= 0.99  # decrease temperature with a higher rate\n\n    def covariance_matrix_adaptation(self):\n        # covariance matrix adaptation\n        self.covariance_matrix = (1 - self.adaptive_step_size) * self.covariance_matrix + self.adaptive_step_size * np.eye(self.dim)\n        return self.covariance_matrix\n\n    def adaptive_step_size_learning(self):\n        # adaptive step size learning\n        self.adaptive_step_size += self.step_size_learning_rate * (self.evaluate(self.population[0]) - self.evaluate(self.population[-1]))\n        return self.adaptive_step_size\n\n    def history_learning(self):\n        # history learning\n        self.history = (1 - self.history_learning_rate) * self.history + self.history_learning_rate * self.population[0]\n\n    def optimize(self, func):\n        for i in range(self.budget):\n            # evaluate population\n            fitness = [self.evaluate(x) for x in self.population]\n            # get best solution\n            self.best_solution = min(fitness)\n            best_index = fitness.index(self.best_solution)\n            # adaptive frequency modulation\n            self.F = self.adaptive_frequency_modulation()\n            # differential evolution\n            for j in range(self.pop_size):\n                # generate trial vector\n                trial = self.crossover(self.population[j], self.population[random.randint(0, self.pop_size - 1)])\n                # evaluate trial vector\n                trial_fitness = self.evaluate(trial)\n                # selection\n                self.population[j] = self.selection(self.population[j], trial)\n                # update best solution\n                if trial_fitness < self.evaluate(self.population[j]):\n                    self.population[j] = trial\n            # simulated annealing\n            for j in range(self.pop_size):\n                self.population[j] = self.simulated_annealing(self.population[j])\n            # covariance matrix adaptation\n            self.covariance_matrix = self.covariance_matrix_adaptation()\n            # adaptive step size learning\n            self.adaptive_step_size = self.adaptive_step_size_learning()\n            # history learning\n            self.history_learning()\n        return self.best_solution, self.history\n\n    def __call__(self, func):\n        self.func = func\n        return self.optimize(func)\n\n# example usage\nbudget = 1000\ndim = 10\noptimizer = Enhanced_Metaheuristic_Optimizer(budget, dim)\nbest_solution, history = optimizer(lambda x: np.sum(x**2))\nprint(\"Best solution:\", best_solution)\nprint(\"History:\", history)", "name": "Enhanced_Metaheuristic_Optimizer", "description": "Enhanced Metaheuristic Optimization Algorithm with Adaptive Frequency Modulation, Covariance Matrix Adaptation, and Simulated Annealing with Adaptive Temperature Cooling Schedule and Adaptive Step Size Learning, combined with Probabilistic Frequency Modulation and History Learning.", "configspace": "", "generation": 33, "fitness": 0.10972284882509442, "feedback": "The algorithm Enhanced_Metaheuristic_Optimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.11 with standard deviation 0.07.", "error": "", "parent_id": "62f0f28d-b1f3-482e-a59e-689769ae92f8", "metadata": {"aucs": [0.22894890373065435, 0.2201198322050617, 0.2260193189753551, 0.20768970585629554, 0.2159224777143831, 0.21379237506121207, 0.21650955685626028, 0.1991994640826621, 0.2096047004369581, 0.009166223806584495, 0.007403281722304977, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.05366551318766721, 0.0642083516454246, 0.053888982187720424, 0.0752819554611095, 0.05763593964041103, 0.05602409341473702, 0.07423763849681853, 0.0573712252121108, 0.057672344148493204, 0.04425380895149189, 0.06547284670462084, 0.04652632297896342, 0.06089365684580739, 0.06288041668585043, 0.04722668128368046, 0.05358975603131855, 0.07178513296281763, 0.05276224387103434, 0.12534004743092697, 0.1176893597522527, 0.09974632625771007, 0.1360224250664127, 0.10520221202622293, 0.12102461559548316, 0.13689611212365016, 0.11272414499843986, 0.15745392783450884, 0.08754512396279002, 0.12189581202386468, 0.092582877688129, 0.10360086944693425, 0.07809432514928905, 0.09898762623714796, 0.09547930842015906, 0.11669940608070084, 0.1103159969569848, 0.13512179542065927, 0.1807743865410848, 0.13371565557278753, 0.16717523962167202, 0.17145342842320366, 0.1833690157272625, 0.15627301801047433, 0.17106655899514756, 0.16677855287719745, 0.058731098031053586, 0.0578646317434528, 0.07100003887461936, 0.12151492239559614, 0.029012311047564077, 0.06006726744507873, 0.09440803107479334, 0.062163882523001535, 0.09383545954006278, 0.12626158195275694, 0.10143074498141746, 0.11608050610359244, 0.07987636329238668, 0.11448908153590676, 0.10363011500591679, 0.11098555905605012, 0.09706725091672275, 0.11168368424786679, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.06120872955534473, 0.10715979603130288, 0.07721418499158117, 0.050000650533789104, 0.055092883553983674, 0.024815030872303523, 0.09963127984809328, 0.084042998334802, 0.0705544292361917, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.0432168797666469, 0.024872461194505924, 0.022680132228471317, 0.03397287672011173, 0.03555481239571667, 0.046772507573341726, 0.02515603669560873, 0.041248585818516204, 0.037388832416412665, 0.24084439456715356, 0.226570804416813, 0.23785156788168482, 0.22217098589050266, 0.21784904357058177, 0.20450739327597167, 0.23737942914281418, 0.21902521001631126, 0.2335982396855133, 0.04800737467987859, 0.05904223215795423, 0.04935501364740735, 0.07375023122626712, 0.06622925744054942, 0.0661641060031789, 0.06452573649514981, 0.06741608176229452, 0.08577604119351234, 0.12951934043050328, 0.11827619164378356, 0.11842175776965747, 0.10678684419279072, 0.12174550683421059, 0.12703115265888076, 0.13394568321831124, 0.12080748518418782, 0.12209516501238227, 0.16328894294204488, 0.17643826727963952, 0.17160741987534622, 0.1855554007147442, 0.1832509464236718, 0.19643635421118877, 0.15802547959522473, 0.181297628544562, 0.15821233847567806, 0.12237428947569884, 0.1278826703900695, 0.12878669969791445, 0.12490211253168615, 0.12666812399532845, 0.1360551829704948, 0.12863283629245703, 0.1229333875882368, 0.10937515158434585, 0.1760946646456767, 0.17094789423921952, 0.1731868235916163, 0.18890076734615013, 0.17579200564423902, 0.16602506047545762, 0.1778638764492373, 0.16672557257631337, 0.1656422308741834, 0.15733700884359603, 0.1588329528385799, 0.15883476256279083, 0.15948488634028257, 0.1550779571791251, 0.16180506254424287, 0.15688859564462998, 0.16972273808601201, 0.1537086860693042, 0.16501677545809856, 0.16456728934881992, 0.16337326616754688, 0.14762276488395187, 0.17736822796375296, 0.225798242304321, 0.22780507194985777, 0.15935412075080235, 0.168280306739516, 0.20536897692392586, 0.2551191474148006, 0.18181823047854018, 0.21677428489241302, 0.13185835142245106, 0.13034605571099633, 0.11390149877432054, 0.1704357002883926, 0.14447845878174537, 0.16284857471608138, 0.16053100660652564, 0.1848393874321369, 0.1664406262340692, 0.1567700514908119, 0.16608913599159847, 0.18608483356158023, 0.1631022202559803, 0.1702689709926255, 0.05934090410736348, 0.0652996561515714, 0.0635126204883314, 0.058649687951666474, 0.0623315368715488, 0.08224606441492666, 0.05909190409464682, 0.06072998359981863, 0.06341780653415752]}, "mutation_prompt": null}
{"id": "e3a7dfdb-4a40-4132-8ac7-cfbf498f9641", "solution": "import numpy as np\nimport random\n\nclass Probabilistic_Frequency_Modulation_Covariance_Matrix_Adaptation:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.F = 0.5  # initial frequency\n        self.CR = 0.5  # initial crossover rate\n        self.pop_size = 50  # initial population size\n        self.population = np.random.uniform(-5.0, 5.0, (self.pop_size, self.dim))\n        self.best_solution = np.inf\n        self.probability = 0.3  # increased probability of probabilistic exploration\n        self.temperature = 100  # initial temperature for simulated annealing\n        self.covariance_matrix = np.eye(self.dim)  # initial covariance matrix\n        self.adaptive_step_size = 0.1  # initial adaptive step size for covariance matrix adaptation\n        self.step_size_learning_rate = 0.001  # learning rate for adaptive step size\n        self.history = np.zeros((self.budget, self.dim))  # history of best solutions\n\n    def evaluate(self, x):\n        return self.func(x)\n\n    def func(self, x):\n        # BBOB test suite of 24 noiseless functions\n        # Here we use the first function f1(x) = sum(x_i^2)\n        return np.sum(x**2)\n\n    def frequency_modulation(self, x):\n        # frequency modulation function\n        return self.F * np.sin(2 * np.pi * self.F * x)\n\n    def adaptive_frequency_modulation(self):\n        # adaptive frequency modulation\n        self.F *= 1.1  # increase frequency\n        if random.random() < self.probability:\n            self.F *= 0.9  # decrease frequency with probability\n        return self.F\n\n    def crossover(self, x1, x2):\n        # crossover operation\n        r = np.random.rand(self.dim)\n        return x1 + r * (x2 - x1)\n\n    def selection(self, x1, x2):\n        # selection operation\n        if self.evaluate(x2) < self.evaluate(x1):\n            return x2\n        else:\n            return x1\n\n    def simulated_annealing(self, x):\n        # simulated annealing\n        new_x = x + np.random.normal(0, 1, self.dim)\n        new_x = np.clip(new_x, -5.0, 5.0)\n        if self.evaluate(new_x) < self.evaluate(x):\n            return new_x\n        else:\n            probability = np.exp(-(self.evaluate(new_x) - self.evaluate(x)) / self.temperature)\n            if random.random() < probability:\n                return new_x\n            else:\n                return x\n        self.temperature *= 0.99  # decrease temperature with a higher rate\n\n    def covariance_matrix_adaptation(self):\n        # covariance matrix adaptation\n        self.covariance_matrix = (1 - self.adaptive_step_size) * self.covariance_matrix + self.adaptive_step_size * np.eye(self.dim)\n        return self.covariance_matrix\n\n    def adaptive_step_size_learning(self):\n        # adaptive step size learning\n        self.adaptive_step_size += self.step_size_learning_rate * (self.evaluate(self.population[0]) - self.evaluate(self.population[-1]))\n        return self.adaptive_step_size\n\n    def history_learning(self):\n        # history learning\n        self.history = np.vstack((self.history, self.population[0]))\n\n    def optimize(self, func):\n        for i in range(self.budget):\n            # evaluate population\n            fitness = [self.evaluate(x) for x in self.population]\n            # get best solution\n            self.best_solution = min(fitness)\n            best_index = fitness.index(self.best_solution)\n            # adaptive frequency modulation\n            self.F = self.adaptive_frequency_modulation()\n            # differential evolution\n            for j in range(self.pop_size):\n                # generate trial vector\n                trial = self.crossover(self.population[j], self.population[random.randint(0, self.pop_size - 1)])\n                # evaluate trial vector\n                trial_fitness = self.evaluate(trial)\n                # selection\n                self.population[j] = self.selection(self.population[j], trial)\n                # update best solution\n                if trial_fitness < self.evaluate(self.population[j]):\n                    self.population[j] = trial\n            # simulated annealing\n            for j in range(self.pop_size):\n                self.population[j] = self.simulated_annealing(self.population[j])\n            # covariance matrix adaptation\n            self.covariance_matrix = self.covariance_matrix_adaptation()\n            # adaptive step size learning\n            self.adaptive_step_size = self.adaptive_step_size_learning()\n            # history learning\n            self.history_learning()\n        return self.best_solution, self.history\n\n    def __call__(self, func):\n        self.func = func\n        return self.optimize(func)\n\n# example usage\nbudget = 1000\ndim = 10\noptimizer = Probabilistic_Frequency_Modulation_Covariance_Matrix_Adaptation(budget, dim)\nbest_solution, history = optimizer(lambda x: np.sum(x**2))\nprint(\"Best solution:\", best_solution)\nprint(\"History:\", history)", "name": "Probabilistic_Frequency_Modulation_Covariance_Matrix_Adaptation", "description": "This novel heuristic algorithm combines a probabilistic frequency modulation mechanism with a covariance matrix adaptation strategy, adaptive simulated annealing, and history learning to optimize black box functions on the BBOB test suite.", "configspace": "", "generation": 34, "fitness": 0.11267673539793055, "feedback": "The algorithm Probabilistic_Frequency_Modulation_Covariance_Matrix_Adaptation got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.11 with standard deviation 0.08.", "error": "", "parent_id": "62f0f28d-b1f3-482e-a59e-689769ae92f8", "metadata": {"aucs": [0.2541348002987345, 0.23917755244727845, 0.22892242185777223, 0.20946474228183276, 0.20474128215046083, 0.1806227996184685, 0.22215016668310072, 0.21020979912486548, 0.22941187702625843, 9.999999999998899e-05, 0.02746567029116831, 9.999999999998899e-05, 9.999999999998899e-05, 0.016763546840909638, 0.0314804240441392, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.04607501157575333, 0.07354400230645675, 0.06621941798432218, 0.06232166150575302, 0.057546507281275305, 0.06581110484511554, 0.07158816551192992, 0.088082171848888, 0.06731506378365026, 0.04526523915342551, 0.04753459239017632, 0.053130085734987564, 0.052465371234177605, 0.047291494858669125, 0.04641751085854662, 0.061114603595357364, 0.0621456161003513, 0.04261636368592858, 0.0969506841886919, 0.13962932537659056, 0.08726083700896903, 0.529455864458699, 0.1069497466227296, 0.1991836394530916, 0.11854920998188667, 0.10699879023309278, 0.157597378539875, 0.10634484437552782, 0.11717995592327735, 0.07092045488459575, 0.07993060807390451, 0.06883623101261693, 0.08034272739099813, 0.12982295161053847, 0.10234348840772567, 0.09995873251114085, 0.15268578628172413, 0.16675820010176723, 0.17582952322274126, 0.21662459873351814, 0.1748868783904559, 0.1591817097161483, 0.13489378924821482, 0.17821260360117974, 0.14485541569749583, 0.07609134308520604, 0.052831192170119334, 0.07967495914755895, 0.0790571080679432, 0.021153375849807365, 0.07005898763698948, 0.07137338452433939, 0.07576313268446011, 0.11069908398458628, 0.09671220724501706, 0.09165828751538863, 0.07887500274385362, 0.1036646010622253, 0.09621936626709215, 0.11476644040868178, 0.10378905076650513, 0.10254981180792888, 0.1327532520101955, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.006683269251440804, 9.999999999998899e-05, 9.999999999998899e-05, 0.07378448881561739, 0.08290870425626018, 0.07550297139608497, 0.061054199929309605, 0.07981112229336884, 0.05352944416738481, 0.06926133922169486, 0.059136279785253154, 0.05484932963474387, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.025761163026274736, 0.029704917915271634, 0.010249443397063795, 0.02654597101084888, 0.04633240621661461, 0.032328141846569625, 0.031591808767135965, 0.02247385809944591, 0.03717622082385785, 0.2359705900871667, 0.21715343852900637, 0.2328188174129453, 0.2134017864751928, 0.2196637443150795, 0.22755301466787703, 0.23849306621985988, 0.23720747572873513, 0.2692353963058314, 0.04215662412364252, 0.07604258315424195, 0.05123216117124507, 0.0583688060978097, 0.061477983078767284, 0.06905754915496198, 0.06648626433707594, 0.05814066575347099, 0.06248056711011196, 0.12242875440236645, 0.11199369519301305, 0.1374041366718769, 0.12340125882191111, 0.1172394685504371, 0.12197045586657318, 0.10982121760908992, 0.11196356813574315, 0.11732129835160432, 0.16070836305037917, 0.1711005128323384, 0.18132957450220188, 0.17179664228308267, 0.1829954863034141, 0.1919689172091854, 0.1818444788753414, 0.1848355223119671, 0.16608647222059625, 0.11722795142208486, 0.13657161035716525, 0.11410487335937669, 0.1166673789405509, 0.13925599740678862, 0.12822167562412723, 0.11578650688808734, 0.13699437458443753, 0.12405544437175864, 0.1677558852411548, 0.1797905360434372, 0.19317473480010372, 0.17027467515883632, 0.16668012569766966, 0.18126848083772318, 0.16512873764762004, 0.19117763960962375, 0.16306794535039681, 0.1622297824351825, 0.17769269903665463, 0.16315959047418005, 0.15030292507694942, 0.16129128418236416, 0.15137166100891775, 0.16174693089074021, 0.15444801656382967, 0.15947785881955245, 0.15516259123508902, 0.16267000924236907, 0.15588921865990413, 0.15095914833870538, 0.17067020564842006, 0.17850757470382195, 0.208143117624903, 0.22048981056589523, 0.19523044413093338, 0.2716570386901942, 0.23495840766814824, 0.20633083345699865, 0.18839753975009976, 0.1444250515028429, 0.19029510890929335, 0.14878573924083305, 0.16568726175277348, 0.14162749189099, 0.1819141355131012, 0.16703652175967165, 0.16722667427267845, 0.16967130741507264, 0.18295212069502487, 0.16766093112503644, 0.1825988747477305, 0.17307468277288696, 0.16135541184371704, 0.0694464447141181, 0.07169832715436386, 0.06227838516034612, 0.05098312475745548, 0.05983348311491632, 0.057888874233708365, 0.07610132086335375, 0.08052717840301649, 0.062076066852039924]}, "mutation_prompt": null}
{"id": "94765694-674b-4d95-b811-d3a8b3255c46", "solution": "import numpy as np\nimport random\n\nclass Adaptive_Harmony_Search:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.HM = 0.1  # harmony memory size\n        self.PA = 0.5  # pitch adjusting rate\n        self.CR = 0.5  # initial crossover rate\n        self.pop_size = int(self.HM * self.dim)  # initial population size\n        self.population = np.random.uniform(-5.0, 5.0, (self.pop_size, self.dim))\n        self.best_solution = np.inf\n        self.probability = 0.2  # increased probability of probabilistic exploration\n        self.temperature = 100  # initial temperature for simulated annealing\n        self.covariance_matrix = np.eye(self.dim)  # initial covariance matrix\n        self.adaptive_step_size = 0.1  # initial adaptive step size for covariance matrix adaptation\n        self.step_size_learning_rate = 0.001  # learning rate for adaptive step size\n        self.history = np.zeros((self.budget, self.dim))  # history of best solutions\n\n    def evaluate(self, x):\n        return self.func(x)\n\n    def func(self, x):\n        # BBOB test suite of 24 noiseless functions\n        # Here we use the first function f1(x) = sum(x_i^2)\n        return np.sum(x**2)\n\n    def pitch_adjusting(self, x):\n        # pitch adjusting function\n        return self.PA * np.sin(2 * np.pi * self.PA * x)\n\n    def adaptive_pitch_adjusting(self):\n        # adaptive pitch adjusting\n        self.PA *= 1.1  # increase pitch\n        if random.random() < self.probability:\n            self.PA *= 0.9  # decrease pitch with probability\n        return self.PA\n\n    def crossover(self, x1, x2):\n        # crossover operation\n        r = np.random.rand(self.dim)\n        return x1 + r * (x2 - x1)\n\n    def selection(self, x1, x2):\n        # selection operation\n        if self.evaluate(x2) < self.evaluate(x1):\n            return x2\n        else:\n            return x1\n\n    def cuckoo_search(self, x):\n        # cuckoo search\n        new_x = x + np.random.normal(0, 1, self.dim)\n        new_x = np.clip(new_x, -5.0, 5.0)\n        if self.evaluate(new_x) < self.evaluate(x):\n            return new_x\n        else:\n            return x\n\n    def simulated_annealing(self, x):\n        # simulated annealing\n        new_x = x + np.random.normal(0, 1, self.dim)\n        new_x = np.clip(new_x, -5.0, 5.0)\n        if self.evaluate(new_x) < self.evaluate(x):\n            return new_x\n        else:\n            probability = np.exp(-(self.evaluate(new_x) - self.evaluate(x)) / self.temperature)\n            if random.random() < probability:\n                return new_x\n            else:\n                return x\n        self.temperature *= 0.99  # decrease temperature with a higher rate\n\n    def covariance_matrix_adaptation(self):\n        # covariance matrix adaptation\n        self.covariance_matrix = (1 - self.adaptive_step_size) * self.covariance_matrix + self.adaptive_step_size * np.eye(self.dim)\n        return self.covariance_matrix\n\n    def adaptive_step_size_learning(self):\n        # adaptive step size learning\n        self.adaptive_step_size += self.step_size_learning_rate * (self.evaluate(self.population[0]) - self.evaluate(self.population[-1]))\n        return self.adaptive_step_size\n\n    def history_learning(self):\n        # history learning\n        self.history = np.vstack((self.history, self.population[0]))\n\n    def optimize(self, func):\n        for i in range(self.budget):\n            # evaluate population\n            fitness = [self.evaluate(x) for x in self.population]\n            # get best solution\n            self.best_solution = min(fitness)\n            best_index = fitness.index(self.best_solution)\n            # adaptive pitch adjusting\n            self.PA = self.adaptive_pitch_adjusting()\n            # harmony search\n            for j in range(self.pop_size):\n                # generate trial vector\n                trial = self.crossover(self.population[j], self.population[random.randint(0, self.pop_size - 1)])\n                # evaluate trial vector\n                trial_fitness = self.evaluate(trial)\n                # selection\n                self.population[j] = self.selection(self.population[j], trial)\n                # update best solution\n                if trial_fitness < self.evaluate(self.population[j]):\n                    self.population[j] = trial\n            # cuckoo search\n            for j in range(self.pop_size):\n                self.population[j] = self.cuckoo_search(self.population[j])\n            # simulated annealing\n            for j in range(self.pop_size):\n                self.population[j] = self.simulated_annealing(self.population[j])\n            # covariance matrix adaptation\n            self.covariance_matrix = self.covariance_matrix_adaptation()\n            # adaptive step size learning\n            self.adaptive_step_size = self.adaptive_step_size_learning()\n            # history learning\n            self.history_learning()\n        return self.best_solution, self.history\n\n    def __call__(self, func):\n        self.func = func\n        return self.optimize(func)\n\n# example usage\nbudget = 1000\ndim = 10\noptimizer = Adaptive_Harmony_Search(budget, dim)\nbest_solution, history = optimizer(lambda x: np.sum(x**2))\nprint(\"Best solution:\", best_solution)\nprint(\"History:\", history)", "name": "Adaptive_Harmony_Search", "description": "Adaptive Harmony Search with Cuckoo Search and Simulated Annealing Inspired Covariance Matrix Adaptation and Adaptive Frequency Modulation for Black Box Optimization", "configspace": "", "generation": 35, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('min() arg is an empty sequence').", "error": "ValueError('min() arg is an empty sequence')", "parent_id": "62f0f28d-b1f3-482e-a59e-689769ae92f8", "metadata": {}, "mutation_prompt": null}
{"id": "9aa2ef24-e18b-4332-a3a1-e94472604b8f", "solution": "import numpy as np\nimport random\n\nclass Enhanced_Metaheuristic_Algorithm:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.F = 0.5  # initial frequency\n        self.CR = 0.5  # initial crossover rate\n        self.pop_size = 50  # initial population size\n        self.population = np.random.uniform(-5.0, 5.0, (self.pop_size, self.dim))\n        self.best_solution = np.inf\n        self.probability = 0.2  # increased probability of probabilistic exploration\n        self.temperature = 100  # initial temperature for simulated annealing\n        self.covariance_matrix = np.eye(self.dim)  # initial covariance matrix\n        self.adaptive_step_size = 0.1  # initial adaptive step size for covariance matrix adaptation\n        self.step_size_learning_rate = 0.001  # learning rate for adaptive step size\n        self.history = np.zeros((self.budget, self.dim))  # history of best solutions\n        self.history_index = 0  # index for history learning\n\n    def evaluate(self, x):\n        return self.func(x)\n\n    def func(self, x):\n        # BBOB test suite of 24 noiseless functions\n        # Here we use the first function f1(x) = sum(x_i^2)\n        return np.sum(x**2)\n\n    def frequency_modulation(self, x):\n        # frequency modulation function\n        return self.F * np.sin(2 * np.pi * self.F * x)\n\n    def adaptive_frequency_modulation(self):\n        # adaptive frequency modulation\n        self.F *= 1.1  # increase frequency\n        if random.random() < self.probability:\n            self.F *= 0.9  # decrease frequency with probability\n        return self.F\n\n    def crossover(self, x1, x2):\n        # crossover operation\n        r = np.random.rand(self.dim)\n        return x1 + r * (x2 - x1)\n\n    def selection(self, x1, x2):\n        # selection operation\n        if self.evaluate(x2) < self.evaluate(x1):\n            return x2\n        else:\n            return x1\n\n    def simulated_annealing(self, x):\n        # simulated annealing\n        new_x = x + np.random.normal(0, 1, self.dim)\n        new_x = np.clip(new_x, -5.0, 5.0)\n        if self.evaluate(new_x) < self.evaluate(x):\n            return new_x\n        else:\n            probability = np.exp(-(self.evaluate(new_x) - self.evaluate(x)) / self.temperature)\n            if random.random() < probability:\n                return new_x\n            else:\n                return x\n        self.temperature *= 0.99  # decrease temperature with a higher rate\n\n    def covariance_matrix_adaptation(self):\n        # covariance matrix adaptation\n        self.covariance_matrix = (1 - self.adaptive_step_size) * self.covariance_matrix + self.adaptive_step_size * np.eye(self.dim)\n        return self.covariance_matrix\n\n    def adaptive_step_size_learning(self):\n        # adaptive step size learning\n        self.adaptive_step_size += self.step_size_learning_rate * (self.evaluate(self.population[0]) - self.evaluate(self.population[-1]))\n        return self.adaptive_step_size\n\n    def history_learning(self):\n        # history learning\n        self.history[self.history_index] = self.population[0]\n        self.history_index = (self.history_index + 1) % self.budget\n\n    def optimize(self, func):\n        for i in range(self.budget):\n            # evaluate population\n            fitness = [self.evaluate(x) for x in self.population]\n            # get best solution\n            self.best_solution = min(fitness)\n            best_index = fitness.index(self.best_solution)\n            # adaptive frequency modulation\n            self.F = self.adaptive_frequency_modulation()\n            # differential evolution\n            for j in range(self.pop_size):\n                # generate trial vector\n                trial = self.crossover(self.population[j], self.population[random.randint(0, self.pop_size - 1)])\n                # evaluate trial vector\n                trial_fitness = self.evaluate(trial)\n                # selection\n                self.population[j] = self.selection(self.population[j], trial)\n                # update best solution\n                if trial_fitness < self.evaluate(self.population[j]):\n                    self.population[j] = trial\n            # simulated annealing\n            for j in range(self.pop_size):\n                self.population[j] = self.simulated_annealing(self.population[j])\n            # covariance matrix adaptation\n            self.covariance_matrix = self.covariance_matrix_adaptation()\n            # adaptive step size learning\n            self.adaptive_step_size = self.adaptive_step_size_learning()\n            # history learning\n            self.history_learning()\n        return self.best_solution, self.history\n\n    def __call__(self, func):\n        self.func = func\n        return self.optimize(func)\n\n    def update_strategy(self):\n        # update strategy based on history learning\n        self.history_index = 0  # reset history index\n        self.history = np.zeros((self.budget, self.dim))  # reset history\n        for i in range(self.budget):\n            # evaluate population\n            fitness = [self.evaluate(x) for x in self.population]\n            # get best solution\n            self.best_solution = min(fitness)\n            best_index = fitness.index(self.best_solution)\n            # adaptive frequency modulation\n            self.F = self.adaptive_frequency_modulation()\n            # differential evolution\n            for j in range(self.pop_size):\n                # generate trial vector\n                trial = self.crossover(self.population[j], self.population[random.randint(0, self.pop_size - 1)])\n                # evaluate trial vector\n                trial_fitness = self.evaluate(trial)\n                # selection\n                self.population[j] = self.selection(self.population[j], trial)\n                # update best solution\n                if trial_fitness < self.evaluate(self.population[j]):\n                    self.population[j] = trial\n            # simulated annealing\n            for j in range(self.pop_size):\n                self.population[j] = self.simulated_annealing(self.population[j])\n            # covariance matrix adaptation\n            self.covariance_matrix = self.covariance_matrix_adaptation()\n            # adaptive step size learning\n            self.adaptive_step_size = self.adaptive_step_size_learning()\n            # history learning\n            self.history_learning()\n        # update strategy based on history\n        self.probability = 0.2 + 0.1 * np.mean(self.history[:, 0])  # update probability based on history\n        self.temperature = 100 + 50 * np.mean(self.history[:, 0])  # update temperature based on history\n        self.adaptive_step_size = 0.1 + 0.05 * np.mean(self.history[:, 0])  # update adaptive step size based on history\n\n# example usage\nbudget = 1000\ndim = 10\noptimizer = Enhanced_Metaheuristic_Algorithm(budget, dim)\nbest_solution, history = optimizer(lambda x: np.sum(x**2))\nprint(\"Best solution:\", best_solution)\nprint(\"History:\", history)\noptimizer.update_strategy()\nprint(\"Updated strategy:\")\nprint(\"Probability:\", optimizer.probability)\nprint(\"Temperature:\", optimizer.temperature)\nprint(\"Adaptive step size:\", optimizer.adaptive_step_size)", "name": "Enhanced_Metaheuristic_Algorithm", "description": "Enhanced Metaheuristic Algorithm with Frequency Modulation, Adaptive Step Size Learning, and Covariance Matrix Adaptation with Simulated Annealing and History Learning for Black Box Optimization.", "configspace": "", "generation": 36, "fitness": 0.10965160587293382, "feedback": "The algorithm Enhanced_Metaheuristic_Algorithm got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.11 with standard deviation 0.07.", "error": "", "parent_id": "62f0f28d-b1f3-482e-a59e-689769ae92f8", "metadata": {"aucs": [0.2211092091723983, 0.22924800107778842, 0.23545379923639698, 0.20065499655386898, 0.20192377413817675, 0.21931240816777353, 0.20878494100712053, 0.19560877017999312, 0.20435320170678273, 0.0010589370893200334, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.0019762183775732245, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.06181997879395784, 0.06294986817603787, 0.049751210082918274, 0.06335122809257698, 0.06388750683109778, 0.06165329108437689, 0.0606157233730783, 0.06467482034621863, 0.08047056095923, 0.040530352880086884, 0.0473084807274714, 0.0352890157896909, 0.050061573388933134, 0.047838826082745256, 0.044210136483404106, 0.06030088160037361, 0.06304880859110817, 0.07102307618279247, 0.11454990318922831, 0.11469883483522403, 0.09387877460699878, 0.22429321064722385, 0.11609524221478063, 0.1161340813392675, 0.18003830469476856, 0.11835204748831152, 0.1272402818575603, 0.10039037857950883, 0.10472647626304588, 0.07030057231357312, 0.07053310085519693, 0.09702010177207399, 0.07530243443182316, 0.12872482628090642, 0.10299584732537803, 0.08655798583600449, 0.13518193033664283, 0.14990889711853594, 0.1481580280316679, 0.17301870687315823, 0.16532645864872952, 0.1698320537407355, 0.15293799848873024, 0.16305807073579126, 0.15703643678906976, 0.055101528324922544, 0.11382039138291733, 0.0694501783438809, 0.06425236163404646, 0.03619602855891102, 0.04511301369390763, 0.10322927694954653, 0.06981727296377827, 0.07617306549663794, 0.10610359584707807, 0.07798980974315639, 0.08898131553816069, 0.0900918453966284, 0.10755123385046117, 0.11325588789859597, 0.13588508760842744, 0.1021494790948001, 0.09872841146621814, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.06802390739445874, 0.07757082911826041, 0.10151459263949714, 0.044114098357137976, 0.049730582433507475, 0.028732517920763145, 0.044126442892807205, 0.04849685048174812, 0.04563159083867707, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.04038624469114116, 0.027909179434106424, 0.03009807146403387, 0.01709935788866712, 0.04022830002148581, 0.03894116971884232, 0.03927661581245334, 0.033311390879619096, 0.03449906226317312, 0.24027045854473927, 0.2587257544691962, 0.2618840732127329, 0.24569580235605248, 0.20202600129837134, 0.20391788403613242, 0.22771446667619932, 0.2636139658763508, 0.23893349535299835, 0.059279920602190006, 0.058245873833994266, 0.05785638611548838, 0.06856479494373635, 0.06134418775194195, 0.0659818907747749, 0.063310270751604, 0.06406539461520677, 0.059988100904476815, 0.12485830108846674, 0.11634554790161256, 0.12401628089056882, 0.12072754956579157, 0.12369205895848812, 0.12623573070202032, 0.1141204480081024, 0.11323909836612145, 0.13335285754721138, 0.1666490343525172, 0.17603770786944173, 0.1973215957821346, 0.1984222167025137, 0.17972357812467876, 0.18864753891933828, 0.15875031975634868, 0.17652819127357666, 0.16257574335421743, 0.10580788516955797, 0.10065662777393458, 0.11347487597096606, 0.1384712671420666, 0.12863057641644826, 0.1318346185020557, 0.11764163494086044, 0.1264590789419895, 0.11658418862843412, 0.1744692026522935, 0.17196286714799858, 0.19032380955396566, 0.1724691569428296, 0.17002008738695729, 0.2264222880346234, 0.1768849006648383, 0.17570456716112948, 0.18240025457373, 0.15917997775861736, 0.16968554563614158, 0.16473104327484578, 0.16869722306396107, 0.15969779167459996, 0.1591558595194188, 0.15562204213607833, 0.15256914658417065, 0.15077281616385374, 0.17007130534172232, 0.15438882335193738, 0.153598425552252, 0.21537687035703446, 0.1971663843197663, 0.1996820746730542, 0.2340428617531275, 0.15347058055827145, 0.1755149205771389, 0.13202148858445895, 0.23786372859315674, 0.16695462875378075, 0.13556420929831003, 0.17766195784831906, 0.21418244199750303, 0.2102588314352809, 0.18278249125254875, 0.12058444734964491, 0.17436704140801662, 0.168967647868266, 0.16845264546309313, 0.1578478200675325, 0.1738456139175132, 0.16548976185292397, 0.16880464780326132, 0.1808673856410301, 0.16774947529246242, 0.06342136480468707, 0.0749678390558367, 0.05053763087402763, 0.07193146303269204, 0.06139200860295857, 0.06009203684950404, 0.06850154429950184, 0.05855220880155354, 0.08413923711615301]}, "mutation_prompt": null}
{"id": "f794feef-e10f-4377-b64b-5118ecf3e53d", "solution": "import numpy as np\nimport random\n\nclass Adaptive_Frequency_Modulation:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.F = 0.5  # initial frequency\n        self.CR = 0.5  # initial crossover rate\n        self.pop_size = 50  # initial population size\n        self.population = np.random.uniform(-5.0, 5.0, (self.pop_size, self.dim))\n        self.best_solution = np.inf\n        self.probability = 0.2  # increased probability of probabilistic exploration\n        self.temperature = 100  # initial temperature for simulated annealing\n        self.covariance_matrix = np.eye(self.dim)  # initial covariance matrix\n        self.adaptive_step_size = 0.1  # initial adaptive step size for covariance matrix adaptation\n        self.step_size_learning_rate = 0.001  # learning rate for adaptive step size\n        self.history = np.zeros((self.budget, self.dim))  # history of best solutions\n        self.frequency_hopping = 0  # initial frequency hopping counter\n\n    def evaluate(self, x):\n        return self.func(x)\n\n    def func(self, x):\n        # BBOB test suite of 24 noiseless functions\n        # Here we use the first function f1(x) = sum(x_i^2)\n        return np.sum(x**2)\n\n    def frequency_modulation(self, x):\n        # frequency modulation function\n        return self.F * np.sin(2 * np.pi * self.F * x)\n\n    def adaptive_frequency_modulation(self):\n        # adaptive frequency modulation\n        self.F *= 1.1  # increase frequency\n        if random.random() < self.probability:\n            self.F *= 0.9  # decrease frequency with probability\n        return self.F\n\n    def frequency_hopping(self):\n        # frequency hopping\n        self.F = random.uniform(0.1, 10.0)  # random frequency\n        self.frequency_hopping += 1  # increment frequency hopping counter\n        if self.frequency_hopping % 10 == 0:  # hop frequency every 10 iterations\n            self.probability = random.uniform(0.1, 0.5)  # update probability of probabilistic exploration\n        return self.F\n\n    def crossover(self, x1, x2):\n        # crossover operation\n        r = np.random.rand(self.dim)\n        return x1 + r * (x2 - x1)\n\n    def selection(self, x1, x2):\n        # selection operation\n        if self.evaluate(x2) < self.evaluate(x1):\n            return x2\n        else:\n            return x1\n\n    def simulated_annealing(self, x):\n        # simulated annealing\n        new_x = x + np.random.normal(0, 1, self.dim)\n        new_x = np.clip(new_x, -5.0, 5.0)\n        if self.evaluate(new_x) < self.evaluate(x):\n            return new_x\n        else:\n            probability = np.exp(-(self.evaluate(new_x) - self.evaluate(x)) / self.temperature)\n            if random.random() < probability:\n                return new_x\n            else:\n                return x\n        self.temperature *= 0.99  # decrease temperature with a higher rate\n\n    def covariance_matrix_adaptation(self):\n        # covariance matrix adaptation\n        self.covariance_matrix = (1 - self.adaptive_step_size) * self.covariance_matrix + self.adaptive_step_size * np.eye(self.dim)\n        return self.covariance_matrix\n\n    def adaptive_step_size_learning(self):\n        # adaptive step size learning\n        self.adaptive_step_size += self.step_size_learning_rate * (self.evaluate(self.population[0]) - self.evaluate(self.population[-1]))\n        return self.adaptive_step_size\n\n    def history_learning(self):\n        # history learning\n        self.history = np.vstack((self.history, self.population[0]))\n\n    def optimize(self, func):\n        for i in range(self.budget):\n            # evaluate population\n            fitness = [self.evaluate(x) for x in self.population]\n            # get best solution\n            self.best_solution = min(fitness)\n            best_index = fitness.index(self.best_solution)\n            # adaptive frequency modulation\n            self.F = self.frequency_hopping()\n            # differential evolution\n            for j in range(self.pop_size):\n                # generate trial vector\n                trial = self.crossover(self.population[j], self.population[random.randint(0, self.pop_size - 1)])\n                # evaluate trial vector\n                trial_fitness = self.evaluate(trial)\n                # selection\n                self.population[j] = self.selection(self.population[j], trial)\n                # update best solution\n                if trial_fitness < self.evaluate(self.population[j]):\n                    self.population[j] = trial\n            # simulated annealing\n            for j in range(self.pop_size):\n                self.population[j] = self.simulated_annealing(self.population[j])\n            # covariance matrix adaptation\n            self.covariance_matrix = self.covariance_matrix_adaptation()\n            # adaptive step size learning\n            self.adaptive_step_size = self.adaptive_step_size_learning()\n            # history learning\n            self.history_learning()\n        return self.best_solution, self.history\n\n    def __call__(self, func):\n        self.func = func\n        return self.optimize(func)\n\n# example usage\nbudget = 1000\ndim = 10\noptimizer = Adaptive_Frequency_Modulation(budget, dim)\nbest_solution, history = optimizer(lambda x: np.sum(x**2))\nprint(\"Best solution:\", best_solution)\nprint(\"History:\", history)", "name": "Adaptive_Frequency_Modulation", "description": "Adaptive Frequency Modulation with Probabilistic Frequency Hopping and Covariance Matrix Adaptation with Adaptive Step Size Learning and History Learning", "configspace": "", "generation": 37, "fitness": -Infinity, "feedback": "An exception occurred: TypeError(\"'int' object is not callable\").", "error": "TypeError(\"'int' object is not callable\")", "parent_id": "62f0f28d-b1f3-482e-a59e-689769ae92f8", "metadata": {}, "mutation_prompt": null}
{"id": "e5b9c405-12d7-47d6-b544-5c54c4eb0380", "solution": "import numpy as np\nimport random\n\nclass Enhanced_Adaptive_Differential_Evolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.F = 0.5  # initial frequency\n        self.CR = 0.5  # initial crossover rate\n        self.pop_size = 50  # initial population size\n        self.population = np.random.uniform(-5.0, 5.0, (self.pop_size, self.dim))\n        self.best_solution = np.inf\n        self.probability = 0.2  # increased probability of probabilistic exploration\n        self.temperature = 100  # initial temperature for simulated annealing\n        self.covariance_matrix = np.eye(self.dim)  # initial covariance matrix\n        self.adaptive_step_size = 0.1  # initial adaptive step size for covariance matrix adaptation\n        self.step_size_learning_rate = 0.001  # learning rate for adaptive step size\n        self.history = np.zeros((self.budget, self.dim))  # history of best solutions\n\n    def evaluate(self, x):\n        return self.func(x)\n\n    def func(self, x):\n        # BBOB test suite of 24 noiseless functions\n        # Here we use the first function f1(x) = sum(x_i^2)\n        return np.sum(x**2)\n\n    def frequency_modulation(self, x):\n        # frequency modulation function\n        return self.F * np.sin(2 * np.pi * self.F * x)\n\n    def adaptive_frequency_modulation(self):\n        # adaptive frequency modulation\n        self.F *= 1.1  # increase frequency\n        if random.random() < self.probability:\n            self.F *= 0.9  # decrease frequency with probability\n        return self.F\n\n    def crossover(self, x1, x2):\n        # crossover operation\n        r = np.random.rand(self.dim)\n        return x1 + r * (x2 - x1)\n\n    def selection(self, x1, x2):\n        # selection operation\n        if self.evaluate(x2) < self.evaluate(x1):\n            return x2\n        else:\n            return x1\n\n    def simulated_annealing(self, x):\n        # simulated annealing\n        new_x = x + np.random.normal(0, 1, self.dim)\n        new_x = np.clip(new_x, -5.0, 5.0)\n        if self.evaluate(new_x) < self.evaluate(x):\n            return new_x\n        else:\n            probability = np.exp(-(self.evaluate(new_x) - self.evaluate(x)) / self.temperature)\n            if random.random() < probability:\n                return new_x\n            else:\n                return x\n        self.temperature *= 0.99  # decrease temperature with a higher rate\n\n    def covariance_matrix_adaptation(self):\n        # covariance matrix adaptation\n        self.covariance_matrix = (1 - self.adaptive_step_size) * self.covariance_matrix + self.adaptive_step_size * np.eye(self.dim)\n        return self.covariance_matrix\n\n    def adaptive_step_size_learning(self):\n        # adaptive step size learning\n        self.adaptive_step_size += self.step_size_learning_rate * (self.evaluate(self.population[0]) - self.evaluate(self.population[-1]))\n        return self.adaptive_step_size\n\n    def history_learning(self):\n        # history learning\n        self.history = np.vstack((self.history, self.population[0]))\n\n    def optimize(self, func):\n        for i in range(self.budget):\n            # evaluate population\n            fitness = [self.evaluate(x) for x in self.population]\n            # get best solution\n            self.best_solution = min(fitness)\n            best_index = fitness.index(self.best_solution)\n            # adaptive frequency modulation\n            self.F = self.adaptive_frequency_modulation()\n            # differential evolution\n            for j in range(self.pop_size):\n                # generate trial vector\n                trial = self.crossover(self.population[j], self.population[random.randint(0, self.pop_size - 1)])\n                # evaluate trial vector\n                trial_fitness = self.evaluate(trial)\n                # selection\n                self.population[j] = self.selection(self.population[j], trial)\n                # update best solution\n                if trial_fitness < self.evaluate(self.population[j]):\n                    self.population[j] = trial\n            # simulated annealing\n            for j in range(self.pop_size):\n                self.population[j] = self.simulated_annealing(self.population[j])\n            # covariance matrix adaptation\n            self.covariance_matrix = self.covariance_matrix_adaptation()\n            # adaptive step size learning\n            self.adaptive_step_size = self.adaptive_step_size_learning()\n            # history learning\n            self.history_learning()\n        return self.best_solution, self.history\n\n    def __call__(self, func):\n        self.func = func\n        return self.optimize(func)\n\n# example usage\nbudget = 1000\ndim = 10\noptimizer = Enhanced_Adaptive_Differential_Evolution(budget, dim)\nbest_solution, history = optimizer(lambda x: np.sum(x**2))\nprint(\"Best solution:\", best_solution)\nprint(\"History:\", history)", "name": "Enhanced_Adaptive_Differential_Evolution", "description": "Enhanced Adaptive Differential Evolution with Probabilistic Frequency Modulation, Covariance Matrix Adaptation, and Simulated Annealing with Adaptive Temperature Cooling Schedule and Adaptive Step Size Learning.", "configspace": "", "generation": 26, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "62f0f28d-b1f3-482e-a59e-689769ae92f8", "metadata": {"aucs": [0.23769731172917852, 0.23720457413485296, 0.23398524340821303, 0.20549460010352794, 0.20555116867582168, 0.20130651801061838, 0.2111034242676071, 0.2007516942743227, 0.2116305929248098, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.006392611173892893, 9.999999999998899e-05, 0.059900294776183416, 0.07012785618662798, 0.05419817263252702, 0.0795014722763715, 0.06935901025400615, 0.07305402795587912, 0.07197872740953881, 0.06459557626532919, 0.06996007361196876, 0.053111208791695175, 0.053566490408007095, 0.045790153649368226, 0.08521993086965163, 0.0694819433532079, 0.04350696073483884, 0.060257967750235175, 0.05189063106084635, 0.04649729526248825, 0.14662710504050225, 0.12405344652255246, 0.10395021756261902, 0.5379579998710504, 0.16193852016424048, 0.11791330384339704, 0.18518522693319261, 0.10037714751656313, 0.4283067989696846, 0.09280746828909836, 0.08837625468069787, 0.07937852657802957, 0.07854238359383503, 0.08252944843027832, 0.11767926985163069, 0.1205915686158825, 0.0852113029185213, 0.09530169292310442, 0.17330123385194474, 0.1802252971353444, 0.1525284743121933, 0.1555795762620088, 0.17867323713658245, 0.17229723458815238, 0.16781672295818117, 0.15990674428772478, 0.16337899010881085, 0.08900179420138443, 0.03865389075710812, 0.08160113257791912, 0.03927597651265646, 0.026075257529084794, 0.054926825543406665, 0.09841040259303824, 0.06333280723678636, 0.07418265885714515, 0.09796359396919452, 0.09649198930530256, 0.1100308792533109, 0.10331574585163705, 0.10284233020551181, 0.09918302484193209, 0.10784997913383398, 0.10545554298550341, 0.08743879185238912, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.06531471720729043, 0.06322776317563727, 0.10764868144200157, 0.04234868722028973, 0.06891894424860334, 0.11389557244681159, 0.08902233063245635, 0.04959990216821064, 0.06503624669867603, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.019302053195070723, 0.02569447570168837, 0.02218694043020275, 0.030389077976176515, 0.05011639472944973, 0.026064793258674057, 0.03964242745950375, 0.03523032723265518, 0.04990912829843397, 0.26045759541142244, 0.22792046210319727, 0.24789883372762955, 0.22782461580683844, 0.20323172004362933, 0.21785474159582874, 0.24977361138643406, 0.22257910459292318, 0.23487337912939432, 0.047406160946790954, 0.06913972508922595, 0.04628131465091223, 0.06565901983375388, 0.055748382497863735, 0.0633631736000938, 0.06015180734300485, 0.0755272335023841, 0.06646283240006434, 0.12119223920814659, 0.12470214300559834, 0.14030068848721566, 0.11557895408650731, 0.1328869344681134, 0.12954047545064928, 0.12548124776483527, 0.11082883532147259, 0.11613807783079211, 0.1646706089612735, 0.17402029364751725, 0.169957030806709, 0.17721735438905684, 0.18977506642230046, 0.18552711273919076, 0.16008964310468943, 0.18975185260984473, 0.16153221359268788, 0.11588258721205402, 0.12741740017735936, 0.13192378680608652, 0.1228000957149119, 0.12838293347600316, 0.1273759751841037, 0.10303660034216178, 0.12869446988955147, 0.11553907992403178, 0.1591644210830242, 0.190149261923249, 0.16711753718862765, 0.1942395294204997, 0.2132047975084318, 0.15491365322932638, 0.1620218121660999, 0.16842459346413563, 0.19539101481697096, 0.1574308748762071, 0.15527046019467394, 0.16117056355370862, 0.1507179142739733, 0.16571259637988223, 0.1572215990897441, 0.1537357474934712, 0.15304501824019534, 0.17085057300372697, 0.14647342296828503, 0.16458182587567938, 0.1475369256476301, 0.24951655367433845, 0.1706906960414144, 0.22975424671538114, 0.1468282263746179, 0.18998475289801375, 0.19872740316007598, 0.17967694151965607, 0.2095248766304756, 0.16531236969298302, 0.17673728075527995, 0.1573563880111336, 0.16079461976257647, 0.13630563294661702, 0.18943475494288797, 0.15794340341591473, 0.1694964781221654, 0.17584746439259435, 0.1675693088119269, 0.16675380106618054, 0.16310744584612002, 0.17812108928443993, 0.16876649757309747, 0.16601458261862279, 0.18324856573202208, 0.0679542418940462, 0.06107766964719452, 0.0634038621543137, 0.0589167546534245, 0.06826269804045038, 0.051810492248762996, 0.06457290160761553, 0.0671718679707437, 0.06687823879460175]}, "mutation_prompt": null}
{"id": "3dacf61c-199d-4e3b-b317-719905031982", "solution": "import numpy as np\nimport random\n\nclass Adaptive_Frequency_Modulation_Enhanced_Adaptive_Differential_Evolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.F = 0.5  # initial frequency\n        self.CR = 0.5  # initial crossover rate\n        self.pop_size = 50  # initial population size\n        self.population = np.random.uniform(-5.0, 5.0, (self.pop_size, self.dim))\n        self.best_solution = np.inf\n        self.probability = 0.9  # probability to decrease frequency modulation factor\n        self.temperature = 100  # initial temperature for simulated annealing\n        self.covariance_matrix = np.eye(self.dim)  # initial covariance matrix\n        self.adaptive_step_size = 0.1  # initial adaptive step size for covariance matrix adaptation\n        self.step_size_learning_rate = 0.001  # learning rate for adaptive step size\n        self.history = np.zeros((self.budget, self.dim))  # history of best solutions\n\n    def evaluate(self, x):\n        return self.func(x)\n\n    def func(self, x):\n        # BBOB test suite of 24 noiseless functions\n        # Here we use the first function f1(x) = sum(x_i^2)\n        return np.sum(x**2)\n\n    def frequency_modulation(self, x):\n        # frequency modulation function\n        return self.F * np.sin(2 * np.pi * self.F * x)\n\n    def adaptive_frequency_modulation(self):\n        # adaptive frequency modulation\n        self.F *= 1.1  # increase frequency\n        if random.random() < self.probability:\n            self.F *= 0.9  # decrease frequency with probability\n        return self.F\n\n    def crossover(self, x1, x2):\n        # crossover operation\n        r = np.random.rand(self.dim)\n        return x1 + r * (x2 - x1)\n\n    def selection(self, x1, x2):\n        # selection operation\n        if self.evaluate(x2) < self.evaluate(x1):\n            return x2\n        else:\n            return x1\n\n    def simulated_annealing(self, x):\n        # simulated annealing\n        new_x = x + np.random.normal(0, 1, self.dim)\n        new_x = np.clip(new_x, -5.0, 5.0)\n        if self.evaluate(new_x) < self.evaluate(x):\n            return new_x\n        else:\n            probability = np.exp(-(self.evaluate(new_x) - self.evaluate(x)) / self.temperature)\n            if random.random() < probability:\n                return new_x\n            else:\n                return x\n        self.temperature *= 0.99  # decrease temperature with a higher rate\n\n    def covariance_matrix_adaptation(self):\n        # covariance matrix adaptation\n        self.covariance_matrix = (1 - self.adaptive_step_size) * self.covariance_matrix + self.adaptive_step_size * np.eye(self.dim)\n        return self.covariance_matrix\n\n    def adaptive_step_size_learning(self):\n        # adaptive step size learning\n        self.adaptive_step_size += self.step_size_learning_rate * (self.evaluate(self.population[0]) - self.evaluate(self.population[-1]))\n        return self.adaptive_step_size\n\n    def history_learning(self):\n        # history learning\n        self.history = np.vstack((self.history, self.population[0]))\n\n    def optimize(self, func):\n        for i in range(self.budget):\n            # evaluate population\n            fitness = [self.evaluate(x) for x in self.population]\n            # get best solution\n            self.best_solution = min(fitness)\n            best_index = fitness.index(self.best_solution)\n            # adaptive frequency modulation\n            self.F = self.adaptive_frequency_modulation()\n            # differential evolution\n            for j in range(self.pop_size):\n                # generate trial vector\n                trial = self.crossover(self.population[j], self.population[random.randint(0, self.pop_size - 1)])\n                # evaluate trial vector\n                trial_fitness = self.evaluate(trial)\n                # selection\n                self.population[j] = self.selection(self.population[j], trial)\n                # update best solution\n                if trial_fitness < self.evaluate(self.population[j]):\n                    self.population[j] = trial\n            # simulated annealing\n            for j in range(self.pop_size):\n                self.population[j] = self.simulated_annealing(self.population[j])\n            # covariance matrix adaptation\n            self.covariance_matrix = self.covariance_matrix_adaptation()\n            # adaptive step size learning\n            self.adaptive_step_size = self.adaptive_step_size_learning()\n            # history learning\n            self.history_learning()\n        return self.best_solution, self.history\n\n    def __call__(self, func):\n        self.func = func\n        return self.optimize(func)\n\n# example usage\nbudget = 1000\ndim = 10\noptimizer = Adaptive_Frequency_Modulation_Enhanced_Adaptive_Differential_Evolution(budget, dim)\nbest_solution, history = optimizer(lambda x: np.sum(x**2))\nprint(\"Best solution:\", best_solution)\nprint(\"History:\", history)", "name": "Adaptive_Frequency_Modulation_Enhanced_Adaptive_Differential_Evolution", "description": "Adaptive Frequency Modulation Enhanced Adaptive Differential Evolution with Probabilistic Frequency Modulation, Covariance Matrix Adaptation, and Simulated Annealing with Adaptive Temperature Cooling Schedule and Adaptive Step Size Learning, with a probability of 0.9 to decrease the frequency modulation factor.", "configspace": "", "generation": 39, "fitness": 0.10783784422836676, "feedback": "The algorithm Adaptive_Frequency_Modulation_Enhanced_Adaptive_Differential_Evolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.11 with standard deviation 0.07.", "error": "", "parent_id": "62f0f28d-b1f3-482e-a59e-689769ae92f8", "metadata": {"aucs": [0.2754349550099693, 0.21900525293174522, 0.25526381379128515, 0.21596537889748657, 0.2018070431488096, 0.21151503657532678, 0.22059925999139163, 0.2111638257372861, 0.22129806199615087, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.05352248022369299, 0.054355441781095526, 0.064512262517057, 0.06475642485579625, 0.06121131274448399, 0.06153183419702524, 0.0588480654157012, 0.06549600576133563, 0.06525294104019852, 0.05584715144469066, 0.0463992961916998, 0.03737018165062289, 0.05710157846568997, 0.04987319635815768, 0.0510565775627434, 0.06256146841019128, 0.058204525228577, 0.04338356611208616, 0.12604563906955923, 0.11203123757905109, 0.0901751001299348, 0.16273642494516094, 0.1170034744063676, 0.11745950912809022, 0.18458249275112826, 0.12607447032453512, 0.11886016626101426, 0.09453398157721171, 0.11332121032688591, 0.08457227594018801, 0.07536783170687156, 0.0693108524910947, 0.09046173718318573, 0.10870316003740121, 0.0877670322672639, 0.09975751270858413, 0.14535694294614365, 0.1555944915919505, 0.1431324678969429, 0.16684512357659897, 0.18379091590953744, 0.1968514193600256, 0.13906559510182837, 0.15538812640761812, 0.12705999064922258, 0.056022589498665565, 0.04548816957389723, 0.031832251682224366, 0.025208320716733623, 0.049936466547398806, 0.03366150764175413, 0.10213541673589477, 0.06101794734861066, 0.0779977546430286, 0.1071674306156889, 0.10844969373338642, 0.09486411676278006, 0.0835322311554666, 0.10905596718123145, 0.102155911674084, 0.12266720033921097, 0.09576826283320339, 0.11156277230937173, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.08674239492653746, 0.0855093961855049, 0.08977878801883199, 0.05638498116340085, 0.02807681698986686, 0.03412515780674352, 0.06806349423017832, 0.06576802551501293, 0.07989640394554876, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.024022369360672036, 0.028823288165680805, 0.01623209718930796, 0.0368112108250922, 0.04940650097969612, 0.0484487241825462, 0.048714428738092286, 0.022932156066332587, 0.037386127438934325, 0.22570670991038144, 0.2784442832250845, 0.2354376190590045, 0.21410652499372085, 0.21403120779912155, 0.23170771782772903, 0.2582104186876706, 0.2174722365081142, 0.24446947222222082, 0.04554988798163395, 0.05196486296055969, 0.04894640617809387, 0.07422937210437464, 0.05843307289887467, 0.05515094601391102, 0.061357838329584746, 0.0654503659498249, 0.06619483157648798, 0.12416980976357717, 0.11988800099933195, 0.11854711644304172, 0.13319649195275052, 0.11019697409864315, 0.12467225340905497, 0.12211787083534786, 0.10762649910792499, 0.12907183724066873, 0.15916297316276562, 0.17471734457124144, 0.16448526829158583, 0.19431902200323037, 0.18503091012019124, 0.17940182298194574, 0.17072602497342304, 0.17401311243450313, 0.16805389875768562, 0.11487303834952223, 0.10017564392281797, 0.11344331810041586, 0.1210552041158216, 0.1287356163654586, 0.1411900972696576, 0.12867337671264234, 0.11703656181795397, 0.09959441025687099, 0.17927608510133974, 0.17208172584735515, 0.1831369308725025, 0.1618592537390976, 0.1660329922312478, 0.19837905270571965, 0.21494243155417558, 0.17813531385702697, 0.16631858456992288, 0.1645917270840761, 0.16113161551479172, 0.1581837812900474, 0.1599847193747519, 0.15606425992651707, 0.15023814238855537, 0.16074549153020512, 0.16255165772301716, 0.15296194644658279, 0.14221056672280008, 0.1920009968264691, 0.16013129356948297, 0.21440730410841402, 0.1763450197216676, 0.14716859550893935, 0.23338282231018048, 0.14320793862470327, 0.1551833841617366, 0.25616916045877947, 0.15871091574415985, 0.1043407267007539, 0.15958456370936724, 0.21081894259966671, 0.15363595875701208, 0.1334400870635305, 0.1685742760639105, 0.11459262117316027, 0.17175695904324817, 0.15849986398334526, 0.16224485212044182, 0.16658380245381, 0.1703158592423284, 0.1715675128902514, 0.1805295004768337, 0.16956027902408277, 0.18066296395672032, 0.05646171182846771, 0.05925823314374101, 0.06338005598265939, 0.06806204341071476, 0.061289143096852405, 0.0667787080079224, 0.06351819637222544, 0.0534367984956331, 0.06397621889750493]}, "mutation_prompt": null}
{"id": "c8d53060-23d1-471d-9a30-15592552e960", "solution": "import numpy as np\nimport random\n\nclass Enhanced_Adaptive_Differential_Evolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.F = 0.5  # initial frequency\n        self.CR = 0.5  # initial crossover rate\n        self.pop_size = 50  # initial population size\n        self.population = np.random.uniform(-5.0, 5.0, (self.pop_size, self.dim))\n        self.best_solution = np.inf\n        self.probability = 0.4  # increased probability of probabilistic exploration\n        self.temperature = 100  # initial temperature for simulated annealing\n        self.covariance_matrix = np.eye(self.dim)  # initial covariance matrix\n        self.adaptive_step_size = 0.2  # increased initial adaptive step size for covariance matrix adaptation\n        self.step_size_learning_rate = 0.002  # increased learning rate for adaptive step size\n        self.history = np.zeros((self.budget, self.dim))  # history of best solutions\n\n    def evaluate(self, x):\n        return self.func(x)\n\n    def func(self, x):\n        # BBOB test suite of 24 noiseless functions\n        # Here we use the first function f1(x) = sum(x_i^2)\n        return np.sum(x**2)\n\n    def frequency_modulation(self, x):\n        # frequency modulation function\n        return self.F * np.sin(2 * np.pi * self.F * x)\n\n    def adaptive_frequency_modulation(self):\n        # adaptive frequency modulation\n        self.F *= 1.1  # increase frequency\n        if random.random() < self.probability:\n            self.F *= 0.9  # decrease frequency with probability\n        return self.F\n\n    def crossover(self, x1, x2):\n        # crossover operation\n        r = np.random.rand(self.dim)\n        return x1 + r * (x2 - x1)\n\n    def selection(self, x1, x2):\n        # selection operation\n        if self.evaluate(x2) < self.evaluate(x1):\n            return x2\n        else:\n            return x1\n\n    def simulated_annealing(self, x):\n        # simulated annealing\n        new_x = x + np.random.normal(0, 1, self.dim)\n        new_x = np.clip(new_x, -5.0, 5.0)\n        if self.evaluate(new_x) < self.evaluate(x):\n            return new_x\n        else:\n            probability = np.exp(-(self.evaluate(new_x) - self.evaluate(x)) / self.temperature)\n            if random.random() < probability:\n                return new_x\n            else:\n                return x\n        self.temperature *= 0.98  # decreased temperature with a higher rate\n\n    def covariance_matrix_adaptation(self):\n        # covariance matrix adaptation\n        self.covariance_matrix = (1 - self.adaptive_step_size) * self.covariance_matrix + self.adaptive_step_size * np.eye(self.dim)\n        return self.covariance_matrix\n\n    def adaptive_step_size_learning(self):\n        # adaptive step size learning\n        self.adaptive_step_size += self.step_size_learning_rate * (self.evaluate(self.population[0]) - self.evaluate(self.population[-1]))\n        return self.adaptive_step_size\n\n    def history_learning(self):\n        # history learning\n        self.history = np.vstack((self.history, self.population[0]))\n\n    def optimize(self, func):\n        for i in range(self.budget):\n            # evaluate population\n            fitness = [self.evaluate(x) for x in self.population]\n            # get best solution\n            self.best_solution = min(fitness)\n            best_index = fitness.index(self.best_solution)\n            # adaptive frequency modulation\n            self.F = self.adaptive_frequency_modulation()\n            # differential evolution\n            for j in range(self.pop_size):\n                # generate trial vector\n                trial = self.crossover(self.population[j], self.population[random.randint(0, self.pop_size - 1)])\n                # evaluate trial vector\n                trial_fitness = self.evaluate(trial)\n                # selection\n                self.population[j] = self.selection(self.population[j], trial)\n                # update best solution\n                if trial_fitness < self.evaluate(self.population[j]):\n                    self.population[j] = trial\n            # simulated annealing\n            for j in range(self.pop_size):\n                self.population[j] = self.simulated_annealing(self.population[j])\n            # covariance matrix adaptation\n            self.covariance_matrix = self.covariance_matrix_adaptation()\n            # adaptive step size learning\n            self.adaptive_step_size = self.adaptive_step_size_learning()\n            # history learning\n            self.history_learning()\n        return self.best_solution, self.history\n\n    def __call__(self, func):\n        self.func = func\n        return self.optimize(func)\n\n# example usage\nbudget = 1000\ndim = 10\noptimizer = Enhanced_Adaptive_Differential_Evolution(budget, dim)\nbest_solution, history = optimizer(lambda x: np.sum(x**2))\nprint(\"Best solution:\", best_solution)\nprint(\"History:\", history)", "name": "Enhanced_Adaptive_Differential_Evolution", "description": "Enhanced Adaptive Frequency Modulated Differential Evolution with Covariance Matrix Adaptation, Simulated Annealing, and Adaptive Temperature Cooling Schedule, where the frequency modulation is increased with a higher probability and the adaptive step size learning rate is increased by 0.1.", "configspace": "", "generation": 40, "fitness": 0.11118552759404877, "feedback": "The algorithm Enhanced_Adaptive_Differential_Evolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.11 with standard deviation 0.07.", "error": "", "parent_id": "62f0f28d-b1f3-482e-a59e-689769ae92f8", "metadata": {"aucs": [0.2519713827872918, 0.27839207378878095, 0.24737511178268734, 0.24899392366614737, 0.18963397156773532, 0.2192147656155582, 0.21639065353949316, 0.19464242571832957, 0.21905734178281466, 0.011139740361941763, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.0008730538575982827, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.07360390025398633, 0.07161820166474286, 0.053956992667260284, 0.07459627572348204, 0.06925718334904563, 0.0658934595768107, 0.06423363565329787, 0.06111057134093112, 0.0863279663362797, 0.04721057899291359, 0.054977662187795207, 0.05024741186049719, 0.05533055448872093, 0.039299085465238415, 0.04598237553825435, 0.0578920532594428, 0.06866341040149304, 0.04924860436959022, 0.1692271568072562, 0.11212832090976477, 0.10833806022807835, 0.14217658322122717, 0.10923831588445299, 0.11436789284368665, 0.18460428479018953, 0.11315047683116752, 0.12892212837443418, 0.10916055877193209, 0.08795267469136725, 0.09407074475713484, 0.10342232341598812, 0.08283974477451261, 0.08924380471151672, 0.10818786844720596, 0.09498351734339305, 0.09606767543991901, 0.1550033160260964, 0.16160961714738065, 0.14271151472381627, 0.17468213682240907, 0.16873127822714484, 0.1458227364025817, 0.14271334835253513, 0.1443409515937788, 0.14298683856693828, 0.05676664786947416, 0.016081497628975594, 0.065221616795338, 0.0878503654180589, 0.05878731084394473, 0.03902097982249275, 0.08006563863368088, 0.09976694305419098, 0.09383088346597035, 0.1052449649429067, 0.11908810318561569, 0.09913928465274435, 0.10876605829587283, 0.12365263411251748, 0.12178469891814314, 0.10289223758899024, 0.11135210141599294, 0.09267767464050503, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.0825200558405198, 0.049385564651809766, 0.0968314961358856, 0.04437526945185, 0.0664920151985583, 0.01747855282037014, 0.08420121943202519, 0.0758822516022235, 0.08826109447610675, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.021201649885417506, 0.02447408491410208, 0.026426081310001015, 0.030737172386845146, 0.03630604644814883, 0.05125426565657909, 0.03842874422313036, 0.04826155717845626, 0.03354302121189867, 0.2279363106476643, 0.2431422775223807, 0.23845709525900083, 0.23563601710249849, 0.21559806486876565, 0.25276430966943986, 0.22940420027436026, 0.22877219739906907, 0.2426193084071504, 0.050887323690072095, 0.05260932808807661, 0.06304811149497458, 0.06611674980586124, 0.055894627964138244, 0.06880576212155753, 0.0662414743238724, 0.0715053771852927, 0.06995805894225726, 0.12767404980431896, 0.14276848518941876, 0.12753833932697944, 0.13507347388469448, 0.13836949304435764, 0.12788948887365081, 0.13937289955154775, 0.12734372818674544, 0.13155039349624242, 0.1701898977941877, 0.17432914499983543, 0.17062766693929265, 0.18331255285555026, 0.18966148768135505, 0.1801811456046858, 0.15225270857034434, 0.18116384244250083, 0.18611255859008402, 0.12329869364282964, 0.11704116934990139, 0.1302674272190889, 0.12941373734764805, 0.1363763594041404, 0.12624237986207698, 0.11232159161567701, 0.13439407034848827, 0.13473740450134009, 0.2312707290627658, 0.16804309781316518, 0.19593916217729013, 0.1727722729993948, 0.1775492620946214, 0.17933850989222444, 0.1717230653511309, 0.17548746422344152, 0.1890945370381768, 0.16309094503076926, 0.15254551532219862, 0.163747744615806, 0.170579624652886, 0.16122973257567974, 0.16312706824043943, 0.1540754621598015, 0.16535704230869464, 0.16154864772404776, 0.15729254281911986, 0.1631354562072409, 0.14558566938382045, 0.3127617321468058, 0.19280378342504056, 0.16582084779484685, 0.17985932171558616, 0.1823215415494972, 0.22516432421304666, 0.21330565379753497, 0.1892542842965712, 0.1250466275006189, 0.13314082813845307, 0.1594506943570323, 0.13628148230785464, 0.14618180012892845, 0.17256997845018196, 0.11184437212888354, 0.15519348399211264, 0.16797626436249302, 0.17331648991613569, 0.16468799389509914, 0.16848533093714124, 0.1684524422591126, 0.1814748815613232, 0.18101317683727225, 0.1729212075117761, 0.06610859648737077, 0.06535984786971372, 0.05423007669026447, 0.06375755959123375, 0.06031116165700501, 0.0604100018424899, 0.0641090917824747, 0.057533536402695096, 0.059123186368399194]}, "mutation_prompt": null}
{"id": "59787571-37ab-4cd7-a503-7e17a0b26c8c", "solution": "import numpy as np\nimport random\n\nclass Probabilistic_Frequency_Modulation_Enhanced_Adaptive_Differential_Evolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.F = 0.5  # initial frequency\n        self.CR = 0.5  # initial crossover rate\n        self.pop_size = 50  # initial population size\n        self.population = np.random.uniform(-5.0, 5.0, (self.pop_size, self.dim))\n        self.best_solution = np.inf\n        self.probability = 0.4  # increased probability of probabilistic exploration\n        self.temperature = 100  # initial temperature for simulated annealing\n        self.covariance_matrix = np.eye(self.dim)  # initial covariance matrix\n        self.adaptive_step_size = 0.1  # initial adaptive step size for covariance matrix adaptation\n        self.step_size_learning_rate = 0.001  # learning rate for adaptive step size\n        self.history = np.zeros((self.budget, self.dim))  # history of best solutions\n\n    def evaluate(self, x):\n        return self.func(x)\n\n    def func(self, x):\n        # BBOB test suite of 24 noiseless functions\n        # Here we use the first function f1(x) = sum(x_i^2)\n        return np.sum(x**2)\n\n    def frequency_modulation(self, x):\n        # frequency modulation function\n        return self.F * np.sin(2 * np.pi * self.F * x)\n\n    def adaptive_frequency_modulation(self):\n        # adaptive frequency modulation\n        self.F *= 1.1  # increase frequency\n        if random.random() < self.probability:\n            self.F *= 0.9  # decrease frequency with probability\n        return self.F\n\n    def crossover(self, x1, x2):\n        # crossover operation\n        r = np.random.rand(self.dim)\n        return x1 + r * (x2 - x1)\n\n    def selection(self, x1, x2):\n        # selection operation\n        if self.evaluate(x2) < self.evaluate(x1):\n            return x2\n        else:\n            return x1\n\n    def simulated_annealing(self, x):\n        # simulated annealing\n        new_x = x + np.random.normal(0, 1, self.dim)\n        new_x = np.clip(new_x, -5.0, 5.0)\n        if self.evaluate(new_x) < self.evaluate(x):\n            return new_x\n        else:\n            probability = np.exp(-(self.evaluate(new_x) - self.evaluate(x)) / self.temperature)\n            if random.random() < probability:\n                return new_x\n            else:\n                return x\n        self.temperature *= 0.99  # decrease temperature with a higher rate\n\n    def covariance_matrix_adaptation(self):\n        # covariance matrix adaptation\n        self.covariance_matrix = (1 - self.adaptive_step_size) * self.covariance_matrix + self.adaptive_step_size * np.eye(self.dim)\n        return self.covariance_matrix\n\n    def adaptive_step_size_learning(self):\n        # adaptive step size learning\n        self.adaptive_step_size += self.step_size_learning_rate * (self.evaluate(self.population[0]) - self.evaluate(self.population[-1]))\n        return self.adaptive_step_size\n\n    def history_learning(self):\n        # history learning\n        self.history = np.vstack((self.history, self.population[0]))\n\n    def enhanced_exploration(self):\n        # enhanced exploration\n        for j in range(self.pop_size):\n            if random.random() < self.probability:\n                self.population[j] = self.population[j] + np.random.normal(0, 1, self.dim)\n\n    def optimize(self, func):\n        for i in range(self.budget):\n            # evaluate population\n            fitness = [self.evaluate(x) for x in self.population]\n            # get best solution\n            self.best_solution = min(fitness)\n            best_index = fitness.index(self.best_solution)\n            # adaptive frequency modulation\n            self.F = self.adaptive_frequency_modulation()\n            # differential evolution\n            for j in range(self.pop_size):\n                # generate trial vector\n                trial = self.crossover(self.population[j], self.population[random.randint(0, self.pop_size - 1)])\n                # evaluate trial vector\n                trial_fitness = self.evaluate(trial)\n                # selection\n                self.population[j] = self.selection(self.population[j], trial)\n                # update best solution\n                if trial_fitness < self.evaluate(self.population[j]):\n                    self.population[j] = trial\n            # simulated annealing\n            for j in range(self.pop_size):\n                self.population[j] = self.simulated_annealing(self.population[j])\n            # covariance matrix adaptation\n            self.covariance_matrix = self.covariance_matrix_adaptation()\n            # adaptive step size learning\n            self.adaptive_step_size = self.adaptive_step_size_learning()\n            # history learning\n            self.history_learning()\n            # enhanced exploration\n            self.enhanced_exploration()\n        return self.best_solution, self.history\n\n    def __call__(self, func):\n        self.func = func\n        return self.optimize(func)\n\n# example usage\nbudget = 1000\ndim = 10\noptimizer = Probabilistic_Frequency_Modulation_Enhanced_Adaptive_Differential_Evolution(budget, dim)\nbest_solution, history = optimizer(lambda x: np.sum(x**2))\nprint(\"Best solution:\", best_solution)\nprint(\"History:\", history)", "name": "Probabilistic_Frequency_Modulation_Enhanced_Adaptive_Differential_Evolution", "description": "Probabilistic Frequency Modulation Enhanced Adaptive Differential Evolution with Covariance Matrix Adaptation and Simulated Annealing with Adaptive Temperature Cooling Schedule and Adaptive Step Size Learning with History Learning and Enhanced Exploration using Increased Probability of Probabilistic Exploration.", "configspace": "", "generation": 41, "fitness": 0.10640179806999263, "feedback": "The algorithm Probabilistic_Frequency_Modulation_Enhanced_Adaptive_Differential_Evolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.11 with standard deviation 0.07.", "error": "", "parent_id": "62f0f28d-b1f3-482e-a59e-689769ae92f8", "metadata": {"aucs": [0.2516793038734617, 0.2358641041634124, 0.2250047003978214, 0.1941039900521988, 0.1854517215834358, 0.22896273540520273, 0.18720718973454176, 0.2061690588963062, 0.20592118379014845, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.059380675616663936, 0.06256943072801047, 0.05188771013714688, 0.06061843564532943, 0.0726583238641535, 0.0753758863888212, 0.05814294944197751, 0.06165168692197709, 0.05399715210559575, 0.05721480250310884, 0.05156040967127595, 0.03991209097420012, 0.04968843132937051, 0.052040074702035755, 0.04560001930510116, 0.04160201859134671, 0.06990240927418168, 0.038467237091330375, 0.14785575335450796, 0.11501197784633166, 0.08851109235165955, 0.13145753315360087, 0.1462497793848443, 0.17431100892383655, 0.1328078380451947, 0.10439344581190546, 0.12153664059169333, 0.10215633674868563, 0.09137092576347583, 0.08412856624674936, 0.07125302824403568, 0.09388494232140077, 0.07822536881448283, 0.13050637454026837, 0.09263221790207588, 0.09296293528228072, 0.15790067674265718, 0.1682374147614566, 0.1456802260801081, 0.1891824372171954, 0.19499749633668828, 0.16117116184856473, 0.12507912385478026, 0.15467816173875326, 0.14706563500201442, 0.04065964697459201, 0.06334497590022237, 0.04980558857482409, 0.04871438891300239, 0.030719857917777316, 0.037015484709302826, 0.05285474740894458, 0.07036937518448194, 0.067913069857226, 0.08567554043508319, 0.08530158563789103, 0.08819554562338638, 0.09816196834188218, 0.07687195732097629, 0.08807187389300197, 0.08427836132970623, 0.11080253444725419, 0.07130744201652273, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.09378283609985938, 0.06115400725535636, 0.05354101241654319, 0.04980498336993622, 0.07829877640535943, 0.04332628670078997, 0.06504632817659473, 0.06867020665937384, 0.0540094145883675, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.015784625888814463, 0.01906907734544261, 0.01449445079366174, 0.03306889464734164, 0.021451581322793567, 0.024441054648818183, 0.032418833086596766, 0.011291017345835153, 0.02205523043882951, 0.22451555602845408, 0.23358403156072338, 0.23031979945579273, 0.20984402742243524, 0.21152052979634717, 0.21322457011375295, 0.2333330532591662, 0.2321462404532777, 0.22498311869810683, 0.046163953096917454, 0.06267744641174977, 0.05049320543783575, 0.07305568227224923, 0.050866425155145945, 0.060254486696859066, 0.06486948987288066, 0.06166725745346502, 0.06242769049987584, 0.11894331678491177, 0.12929842247167378, 0.1435822213050849, 0.1291787151957966, 0.11641740666928968, 0.11406826288865435, 0.1309299057585489, 0.11546370129734118, 0.12426466121848301, 0.16531092786777513, 0.16112277623421634, 0.18664243123709123, 0.1709786867740346, 0.18202287225185665, 0.19242014934508345, 0.1667942191888826, 0.1877423573525181, 0.1809193967037499, 0.109890206859377, 0.12086138389299617, 0.12248996067382067, 0.1326586967789024, 0.12429740276177015, 0.13657016332631866, 0.10853924518099489, 0.11738008403720268, 0.10938673086200346, 0.17652168008271096, 0.20549525875859975, 0.17526425345704189, 0.18198741447311018, 0.1649422920833633, 0.1938593526753597, 0.17056505526116394, 0.19504913499983212, 0.18615339923417706, 0.15144870445119418, 0.1609480115984252, 0.1562588623766782, 0.16965340549801855, 0.15652625985083757, 0.16105042415206883, 0.15236391062489618, 0.15355031557689947, 0.16855721135535828, 0.1877614769937742, 0.15670399907345767, 0.14957569939334625, 0.16167556169850827, 0.17067020564842006, 0.1532101874659516, 0.21491513252346595, 0.16324413702550133, 0.19540333435545298, 0.23411733729267348, 0.14604065000755218, 0.15119001257317544, 0.15682491573133162, 0.1579674838995636, 0.15359631370299753, 0.1582967184271079, 0.1650864126124869, 0.13139040848709937, 0.2044802340512072, 0.1978520035552609, 0.18902278875639045, 0.17496772128832394, 0.1685708972221046, 0.1662275880558678, 0.16509141365562796, 0.16254097191762007, 0.1705630798453912, 0.06028964235979073, 0.057918683527544035, 0.07064295186850211, 0.05953018355033224, 0.06102279041624248, 0.06792528286689814, 0.06014171374137478, 0.06767352400608129, 0.0659867176350366]}, "mutation_prompt": null}
{"id": "3f230648-8234-4542-9a99-e08031076e8a", "solution": "import numpy as np\nimport random\n\nclass Enhanced_Adaptive_Differential_Evolution_Refined:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.F = 0.5  # initial frequency\n        self.CR = 0.5  # initial crossover rate\n        self.pop_size = 50  # initial population size\n        self.population = np.random.uniform(-5.0, 5.0, (self.pop_size, self.dim))\n        self.best_solution = np.inf\n        self.probability = 0.25  # increased probability of probabilistic exploration\n        self.temperature = 100  # initial temperature for simulated annealing\n        self.covariance_matrix = np.eye(self.dim)  # initial covariance matrix\n        self.adaptive_step_size = 0.1  # initial adaptive step size for covariance matrix adaptation\n        self.step_size_learning_rate = 0.001  # learning rate for adaptive step size\n        self.history = np.zeros((self.budget, self.dim))  # history of best solutions\n        self.frequency_modulation_history = np.zeros(self.budget)  # history of frequency modulation\n\n    def evaluate(self, x):\n        return self.func(x)\n\n    def func(self, x):\n        # BBOB test suite of 24 noiseless functions\n        # Here we use the first function f1(x) = sum(x_i^2)\n        return np.sum(x**2)\n\n    def frequency_modulation(self, x):\n        # frequency modulation function\n        return self.F * np.sin(2 * np.pi * self.F * x)\n\n    def adaptive_frequency_modulation(self):\n        # adaptive frequency modulation\n        self.F *= 1.1  # increase frequency\n        if random.random() < self.probability:\n            self.F *= 0.9  # decrease frequency with probability\n        self.frequency_modulation_history[i] = self.F\n        return self.F\n\n    def crossover(self, x1, x2):\n        # crossover operation\n        r = np.random.rand(self.dim)\n        return x1 + r * (x2 - x1)\n\n    def selection(self, x1, x2):\n        # selection operation\n        if self.evaluate(x2) < self.evaluate(x1):\n            return x2\n        else:\n            return x1\n\n    def simulated_annealing(self, x):\n        # simulated annealing\n        new_x = x + np.random.normal(0, 1, self.dim)\n        new_x = np.clip(new_x, -5.0, 5.0)\n        if self.evaluate(new_x) < self.evaluate(x):\n            return new_x\n        else:\n            probability = np.exp(-(self.evaluate(new_x) - self.evaluate(x)) / self.temperature)\n            if random.random() < probability:\n                return new_x\n            else:\n                return x\n        self.temperature *= 0.995  # decrease temperature with a higher rate\n\n    def covariance_matrix_adaptation(self):\n        # covariance matrix adaptation\n        self.covariance_matrix = (1 - self.adaptive_step_size) * self.covariance_matrix + self.adaptive_step_size * np.eye(self.dim)\n        return self.covariance_matrix\n\n    def adaptive_step_size_learning(self):\n        # adaptive step size learning\n        self.adaptive_step_size += self.step_size_learning_rate * (self.evaluate(self.population[0]) - self.evaluate(self.population[-1]))\n        return self.adaptive_step_size\n\n    def history_learning(self):\n        # history learning\n        self.history = np.vstack((self.history, self.population[0]))\n\n    def optimize(self, func):\n        for i in range(self.budget):\n            # evaluate population\n            fitness = [self.evaluate(x) for x in self.population]\n            # get best solution\n            self.best_solution = min(fitness)\n            best_index = fitness.index(self.best_solution)\n            # adaptive frequency modulation\n            self.F = self.adaptive_frequency_modulation()\n            # differential evolution\n            for j in range(self.pop_size):\n                # generate trial vector\n                trial = self.crossover(self.population[j], self.population[random.randint(0, self.pop_size - 1)])\n                # evaluate trial vector\n                trial_fitness = self.evaluate(trial)\n                # selection\n                self.population[j] = self.selection(self.population[j], trial)\n                # update best solution\n                if trial_fitness < self.evaluate(self.population[j]):\n                    self.population[j] = trial\n            # simulated annealing\n            for j in range(self.pop_size):\n                self.population[j] = self.simulated_annealing(self.population[j])\n            # covariance matrix adaptation\n            self.covariance_matrix = self.covariance_matrix_adaptation()\n            # adaptive step size learning\n            self.adaptive_step_size = self.adaptive_step_size_learning()\n            # history learning\n            self.history_learning()\n        return self.best_solution, self.history, self.frequency_modulation_history\n\n    def __call__(self, func):\n        self.func = func\n        return self.optimize(func)\n\n# example usage\nbudget = 1000\ndim = 10\noptimizer = Enhanced_Adaptive_Differential_Evolution_Refined(budget, dim)\nbest_solution, history, frequency_modulation_history = optimizer(lambda x: np.sum(x**2))\nprint(\"Best solution:\", best_solution)\nprint(\"History:\", history)\nprint(\"Frequency modulation history:\", frequency_modulation_history)", "name": "Enhanced_Adaptive_Differential_Evolution_Refined", "description": "Enhanced Adaptive Frequency Modulation Differential Evolution with Covariance Matrix Adaptation and Simulated Annealing with Adaptive Temperature Cooling Schedule and Adaptive Step Size Learning Refinement.", "configspace": "", "generation": 42, "fitness": -Infinity, "feedback": "An exception occurred: NameError(\"name 'i' is not defined\").", "error": "NameError(\"name 'i' is not defined\")", "parent_id": "62f0f28d-b1f3-482e-a59e-689769ae92f8", "metadata": {}, "mutation_prompt": null}
{"id": "23f750b2-1a8a-4a29-b100-6c04ac3dd423", "solution": "import numpy as np\nimport random\n\nclass Adaptive_Frequency_Modulated_Differential_Evolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.F = 0.5  # initial frequency\n        self.CR = 0.5  # initial crossover rate\n        self.pop_size = 50  # initial population size\n        self.population = np.random.uniform(-5.0, 5.0, (self.pop_size, self.dim))\n        self.best_solution = np.inf\n        self.probability = 0.2  # increased probability of probabilistic exploration\n        self.temperature = 100  # initial temperature for simulated annealing\n        self.covariance_matrix = np.eye(self.dim)  # initial covariance matrix\n        self.adaptive_step_size = 0.1  # initial adaptive step size for covariance matrix adaptation\n        self.step_size_learning_rate = 0.001  # learning rate for adaptive step size\n        self.history = np.zeros((self.budget, self.dim))  # history of best solutions\n\n    def evaluate(self, x):\n        return self.func(x)\n\n    def func(self, x):\n        # BBOB test suite of 24 noiseless functions\n        # Here we use the first function f1(x) = sum(x_i^2)\n        return np.sum(x**2)\n\n    def frequency_modulation(self, x):\n        # frequency modulation function\n        return self.F * np.sin(2 * np.pi * self.F * x)\n\n    def adaptive_frequency_modulation(self):\n        # adaptive frequency modulation\n        self.F *= 1.1  # increase frequency\n        if random.random() < self.probability:\n            self.F *= 0.9  # decrease frequency with probability\n        return self.F\n\n    def crossover(self, x1, x2):\n        # crossover operation\n        r = np.random.rand(self.dim)\n        return x1 + r * (x2 - x1)\n\n    def selection(self, x1, x2):\n        # selection operation\n        if self.evaluate(x2) < self.evaluate(x1):\n            return x2\n        else:\n            return x1\n\n    def simulated_annealing(self, x):\n        # simulated annealing\n        new_x = x + np.random.normal(0, 1, self.dim)\n        new_x = np.clip(new_x, -5.0, 5.0)\n        if self.evaluate(new_x) < self.evaluate(x):\n            return new_x\n        else:\n            probability = np.exp(-(self.evaluate(new_x) - self.evaluate(x)) / self.temperature)\n            if random.random() < probability:\n                return new_x\n            else:\n                return x\n        self.temperature *= 0.99  # decrease temperature with a higher rate\n\n    def covariance_matrix_adaptation(self):\n        # covariance matrix adaptation\n        self.covariance_matrix = (1 - self.adaptive_step_size) * self.covariance_matrix + self.adaptive_step_size * np.eye(self.dim)\n        return self.covariance_matrix\n\n    def adaptive_step_size_learning(self):\n        # adaptive step size learning\n        self.adaptive_step_size += self.step_size_learning_rate * (self.evaluate(self.population[0]) - self.evaluate(self.population[-1]))\n        return self.adaptive_step_size\n\n    def history_learning(self):\n        # history learning\n        self.history = np.vstack((self.history, self.population[0]))\n\n    def optimize(self, func):\n        for i in range(self.budget):\n            # evaluate population\n            fitness = [self.evaluate(x) for x in self.population]\n            # get best solution\n            self.best_solution = min(fitness)\n            best_index = fitness.index(self.best_solution)\n            # adaptive frequency modulation\n            self.F = self.adaptive_frequency_modulation()\n            # differential evolution\n            for j in range(self.pop_size):\n                # generate trial vector\n                trial = self.crossover(self.population[j], self.population[random.randint(0, self.pop_size - 1)])\n                # evaluate trial vector\n                trial_fitness = self.evaluate(trial)\n                # selection\n                self.population[j] = self.selection(self.population[j], trial)\n                # update best solution\n                if trial_fitness < self.evaluate(self.population[j]):\n                    self.population[j] = trial\n            # simulated annealing\n            for j in range(self.pop_size):\n                self.population[j] = self.simulated_annealing(self.population[j])\n            # covariance matrix adaptation\n            self.covariance_matrix = self.covariance_matrix_adaptation()\n            # adaptive step size learning\n            self.adaptive_step_size = self.adaptive_step_size_learning()\n            # history learning\n            self.history_learning()\n        return self.best_solution, self.history\n\n    def __call__(self, func):\n        self.func = func\n        return self.optimize(func)\n\n# example usage\nbudget = 1000\ndim = 10\noptimizer = Adaptive_Frequency_Modulated_Differential_Evolution(budget, dim)\nbest_solution, history = optimizer(lambda x: np.sum(x**2))\nprint(\"Best solution:\", best_solution)\nprint(\"History:\", history)", "name": "Adaptive_Frequency_Modulated_Differential_Evolution", "description": "A novel metaheuristic algorithm called Adaptive Frequency Modulated Differential Evolution with Covariance Matrix Adaptation and Simulated Annealing with Adaptive Temperature Cooling Schedule and Adaptive Step Size Learning, which enhances the selected solution by incorporating frequency modulation, covariance matrix adaptation, and simulated annealing with adaptive temperature and step size learning.", "configspace": "", "generation": 43, "fitness": 0.11078626830200443, "feedback": "The algorithm Adaptive_Frequency_Modulated_Differential_Evolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.11 with standard deviation 0.07.", "error": "", "parent_id": "62f0f28d-b1f3-482e-a59e-689769ae92f8", "metadata": {"aucs": [0.22006824575915784, 0.2187595108845719, 0.23442220875974795, 0.20327609122346946, 0.19789256233441166, 0.1998054626917568, 0.21172558417441123, 0.1856017857844614, 0.18950266892419998, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.0644317433135444, 0.06465751107303275, 0.056118890697064105, 0.0711216800786495, 0.06513022809804714, 0.06258628968923474, 0.06338377822511299, 0.06547881937367417, 0.05878038393258245, 0.04791371112272591, 0.05267388601200673, 0.045373292239693885, 0.0561280986220134, 0.05710599118348858, 0.044621978622473635, 0.0546660679990848, 0.07161929107437537, 0.0468007777170375, 0.16532263971801164, 0.11168702876963565, 0.10047925261253698, 0.1730466562909676, 0.11151388541398977, 0.12780490564547675, 0.1748761731123304, 0.4215726421644206, 0.3513609497181739, 0.11571147500640455, 0.0877845694264544, 0.08050678796770117, 0.09004801106869031, 0.08336589782631476, 0.09722367131240661, 0.0992842156258743, 0.08513853320095444, 0.07287589812018336, 0.1422211116836214, 0.18132386666855171, 0.15716348112751566, 0.15768000183559105, 0.161214180048586, 0.14326469472826897, 0.14662010769490208, 0.18101164144776394, 0.14553272788782, 0.06269185314278747, 0.061718072728688234, 0.08171422068557765, 0.019804826771028683, 0.04928035665605257, 0.055809219756631, 0.08741564918134603, 0.0657276482745709, 0.0747175689872458, 0.10937898651829658, 0.0805955752590638, 0.10422321958802971, 0.09074906399469984, 0.10869213537878764, 0.09786334914159867, 0.10181937259071261, 0.11352323579228307, 0.08274701797043638, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.05877172607311132, 0.07304231259283955, 0.07186207683587587, 0.029486472724959967, 0.050301846181320053, 0.03214751358867085, 0.0701931106367989, 0.07823261244958635, 0.09240118836743516, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.032514607515148564, 0.02339037875790495, 0.027071154654848772, 0.02527726224380422, 0.05121785876618179, 0.04067721448486994, 0.04692980274569947, 0.027195512292938706, 0.03673440004445727, 0.2367951364878239, 0.23857472405873148, 0.23350736792684101, 0.22380400675277046, 0.20161307567837072, 0.22649494903163758, 0.24115727457973712, 0.23908511450982228, 0.2189896367428944, 0.04318463800407846, 0.06113333182291625, 0.04626983568127596, 0.06751303673100428, 0.058444570748965474, 0.0641325204858223, 0.06329986099088514, 0.06648484837046686, 0.0582458905622425, 0.12318153960382783, 0.14103101063508716, 0.1327127023796678, 0.1190823800321823, 0.14683920019550656, 0.1198201169448253, 0.12844938248690374, 0.120877381326294, 0.12786833465491365, 0.16709553052175286, 0.17643324926425208, 0.1800229240644281, 0.1865854061574025, 0.1861406371209271, 0.19081317235839756, 0.1530235844626111, 0.17488393411861125, 0.1572219295663867, 0.11277549221751215, 0.11751463348478353, 0.12088510356769422, 0.1301607689626062, 0.1258526667170341, 0.13289606834197765, 0.10569413448479603, 0.1309501863068152, 0.14760432772057286, 0.1759613896068165, 0.19268265648697191, 0.18113603727315453, 0.1716500238191735, 0.17942412442330258, 0.17534934348672337, 0.17643312196180172, 0.15656131303388066, 0.2004949293610171, 0.1540406783865872, 0.1548730567586234, 0.16605214407294533, 0.15902132664223023, 0.15339836419938402, 0.15676812935416096, 0.16057079637354277, 0.15990641772956005, 0.1612808093979896, 0.18993159059046127, 0.15125946481200192, 0.14869337740174837, 0.23596700514653202, 0.19970710704212524, 0.1628280231041399, 0.16387007825048183, 0.17797866304090837, 0.2684434827152593, 0.1697717266932246, 0.19585735240297786, 0.15664874767614456, 0.15039345236440183, 0.12734197975982064, 0.13500940695276897, 0.13586903229194103, 0.19504419906786974, 0.12438124218539781, 0.17794971074145727, 0.16779912854432077, 0.18927656536170478, 0.18240333529638453, 0.16578967789907217, 0.1684590520435093, 0.16511793833471744, 0.20404810369658555, 0.17161065203885395, 0.0639149336599747, 0.06599004793665064, 0.06489710721299047, 0.05473294553776564, 0.06595205099556622, 0.054590241900259184, 0.06174159758498432, 0.061548672642374735, 0.0803660119681936]}, "mutation_prompt": null}
{"id": "8bee1764-7422-4586-908a-c411e2b82823", "solution": "import numpy as np\nimport random\n\nclass Adaptive_Frequency_Modulation_Enhanced_Differential_Evolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.F = 0.5  # initial frequency\n        self.CR = 0.5  # initial crossover rate\n        self.pop_size = 50  # initial population size\n        self.population = np.random.uniform(-5.0, 5.0, (self.pop_size, self.dim))\n        self.best_solution = np.inf\n        self.probability = 0.25  # increased probability of probabilistic exploration\n        self.temperature = 100  # initial temperature for simulated annealing\n        self.covariance_matrix = np.eye(self.dim)  # initial covariance matrix\n        self.adaptive_step_size = 0.05  # initial adaptive step size for covariance matrix adaptation\n        self.step_size_learning_rate = 0.001  # learning rate for adaptive step size\n        self.history = np.zeros((self.budget, self.dim))  # history of best solutions\n        self.history_learning_rate = 0.1  # learning rate for history learning\n\n    def evaluate(self, x):\n        return self.func(x)\n\n    def func(self, x):\n        # BBOB test suite of 24 noiseless functions\n        # Here we use the first function f1(x) = sum(x_i^2)\n        return np.sum(x**2)\n\n    def frequency_modulation(self, x):\n        # frequency modulation function\n        return self.F * np.sin(2 * np.pi * self.F * x)\n\n    def adaptive_frequency_modulation(self):\n        # adaptive frequency modulation\n        self.F *= 1.05  # increase frequency\n        if random.random() < self.probability:\n            self.F *= 0.9  # decrease frequency with probability\n        return self.F\n\n    def crossover(self, x1, x2):\n        # crossover operation\n        r = np.random.rand(self.dim)\n        return x1 + r * (x2 - x1)\n\n    def selection(self, x1, x2):\n        # selection operation\n        if self.evaluate(x2) < self.evaluate(x1):\n            return x2\n        else:\n            return x1\n\n    def simulated_annealing(self, x):\n        # simulated annealing\n        new_x = x + np.random.normal(0, 1, self.dim)\n        new_x = np.clip(new_x, -5.0, 5.0)\n        if self.evaluate(new_x) < self.evaluate(x):\n            return new_x\n        else:\n            probability = np.exp(-(self.evaluate(new_x) - self.evaluate(x)) / self.temperature)\n            if random.random() < probability:\n                return new_x\n            else:\n                return x\n        self.temperature *= 0.995  # decrease temperature with a higher rate\n\n    def covariance_matrix_adaptation(self):\n        # covariance matrix adaptation\n        self.covariance_matrix = (1 - self.adaptive_step_size) * self.covariance_matrix + self.adaptive_step_size * np.eye(self.dim)\n        return self.covariance_matrix\n\n    def adaptive_step_size_learning(self):\n        # adaptive step size learning\n        self.adaptive_step_size += self.step_size_learning_rate * (self.evaluate(self.population[0]) - self.evaluate(self.population[-1]))\n        return self.adaptive_step_size\n\n    def history_learning(self):\n        # history learning\n        self.history += self.history_learning_rate * (self.population[0] - self.history[-1])\n        return self.history\n\n    def optimize(self, func):\n        for i in range(self.budget):\n            # evaluate population\n            fitness = [self.evaluate(x) for x in self.population]\n            # get best solution\n            self.best_solution = min(fitness)\n            best_index = fitness.index(self.best_solution)\n            # adaptive frequency modulation\n            self.F = self.adaptive_frequency_modulation()\n            # differential evolution\n            for j in range(self.pop_size):\n                # generate trial vector\n                trial = self.crossover(self.population[j], self.population[random.randint(0, self.pop_size - 1)])\n                # evaluate trial vector\n                trial_fitness = self.evaluate(trial)\n                # selection\n                self.population[j] = self.selection(self.population[j], trial)\n                # update best solution\n                if trial_fitness < self.evaluate(self.population[j]):\n                    self.population[j] = trial\n            # simulated annealing\n            for j in range(self.pop_size):\n                self.population[j] = self.simulated_annealing(self.population[j])\n            # covariance matrix adaptation\n            self.covariance_matrix = self.covariance_matrix_adaptation()\n            # adaptive step size learning\n            self.adaptive_step_size = self.adaptive_step_size_learning()\n            # history learning\n            self.history = self.history_learning()\n        return self.best_solution, self.history\n\n    def __call__(self, func):\n        self.func = func\n        return self.optimize(func)\n\n# example usage\nbudget = 1000\ndim = 10\noptimizer = Adaptive_Frequency_Modulation_Enhanced_Differential_Evolution(budget, dim)\nbest_solution, history = optimizer(lambda x: np.sum(x**2))\nprint(\"Best solution:\", best_solution)\nprint(\"History:\", history)", "name": "Adaptive_Frequency_Modulation_Enhanced_Differential_Evolution", "description": "Adaptive Frequency Modulation Enhanced Differential Evolution with Covariance Matrix Adaptation and Simulated Annealing with Adaptive Temperature Cooling Schedule and Adaptive Step Size Learning", "configspace": "", "generation": 44, "fitness": 0.11160962317725201, "feedback": "The algorithm Adaptive_Frequency_Modulation_Enhanced_Differential_Evolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.11 with standard deviation 0.07.", "error": "", "parent_id": "62f0f28d-b1f3-482e-a59e-689769ae92f8", "metadata": {"aucs": [0.24372520966055122, 0.23991455505299386, 0.25093395949790076, 0.2041150132227938, 0.19719739076414167, 0.2060065165190046, 0.22930697626505758, 0.18974816280160411, 0.2042881003019873, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.02634030426257783, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.06183525605551421, 0.05465017586918908, 0.051897177446468934, 0.07019378473494964, 0.07196849795766991, 0.05480127994236339, 0.061769621151371745, 0.07458598571599628, 0.08446629548683926, 0.04131709158930841, 0.05282215643717658, 0.04499930352448844, 0.05764364611616224, 0.05404931947849834, 0.05129501374846146, 0.060134718982498114, 0.058432223534545136, 0.05448435910292715, 0.10622263550381283, 0.10739098249762169, 0.08796446863536744, 0.13676888822804834, 0.12929094177385902, 0.12130121750359102, 0.18358149113392774, 0.13449502827349957, 0.42077766195691346, 0.10418290153778031, 0.09760654667923285, 0.06961417563283612, 0.09027484105877903, 0.08095486247738659, 0.08266415882737166, 0.0881111140242683, 0.09730416685673227, 0.09821584579850406, 0.2031452363009988, 0.15511808760181267, 0.16744167620665762, 0.1689269661064311, 0.1935248097012645, 0.15894614306464228, 0.14643232944586548, 0.1579377302067636, 0.1958980850038643, 0.04731596636644497, 0.08004389716505833, 0.06656996999940312, 0.08444730240325538, 0.06003797949555956, 0.06921618442276245, 0.10181602524475752, 0.07112114472624764, 0.08883124449719448, 0.0974099648929121, 0.1250593268666791, 0.10416742419790337, 0.10322083369209778, 0.10647969326449058, 0.07378540924790089, 0.11217367394039046, 0.10051109002588332, 0.06676954894477949, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.053402616793933544, 0.07071481655179979, 0.1240565146884286, 0.06631026642658111, 0.03853181317391319, 0.045315984169552403, 0.05619695718628481, 0.07873525566732897, 0.04118767168922077, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.026545600053359486, 0.035506234339453324, 0.03744942642007687, 0.02409793210194544, 0.03896880703433203, 0.035323884917384585, 0.03669434089711776, 0.03997608118181328, 0.029402166652224504, 0.22913139207759292, 0.24649223193836456, 0.24218613070110662, 0.2281002688080468, 0.2149072292225398, 0.2349612670863226, 0.2447130530707763, 0.2264974063250309, 0.24193320408818653, 0.042372916212349954, 0.06308632526076563, 0.05940429697313243, 0.06951306339169372, 0.05814474561280425, 0.06367028822447862, 0.0701059467035513, 0.05634306295979663, 0.057363732759424746, 0.15073700570268178, 0.12503159321753154, 0.13399526452380084, 0.1047149423871937, 0.1273490401764833, 0.12399367211617696, 0.12861156157841103, 0.1250800000395158, 0.1157432980748746, 0.15511383729444728, 0.17553031331145685, 0.17006161854505641, 0.1916907419492898, 0.1879255814789339, 0.19238974291598399, 0.14822108533634037, 0.17523157566170067, 0.17076846594995043, 0.11414295271558494, 0.11632556423829532, 0.13744414448796782, 0.1376757385944175, 0.12861276318452497, 0.12534158124600003, 0.13025616743189294, 0.12003978808704541, 0.12824429543716198, 0.17733524203946194, 0.16688286117949136, 0.18601118859348997, 0.17578866456759734, 0.16801128058555026, 0.17768291833138772, 0.18247308704244936, 0.19161869466557901, 0.1943732180161173, 0.15953021967639347, 0.18824266656284483, 0.15456644440530853, 0.16490156997226424, 0.172743117600327, 0.1492937853379368, 0.16579974826811084, 0.15440661258199584, 0.16184135607363825, 0.14934373676610013, 0.17171687051026097, 0.16013360635592, 0.24494165867517992, 0.2473321162668316, 0.133157580363787, 0.22510583842211662, 0.15433208594335734, 0.19145898884460544, 0.19989742905098995, 0.16370434761901886, 0.13574631226317146, 0.15742711209889548, 0.1817496700354374, 0.20087643673319833, 0.11475473116768631, 0.16075933560069333, 0.14323205755525337, 0.17447903099178952, 0.1986372927354514, 0.18380321472492234, 0.1792068319954616, 0.16805992634091804, 0.1682991287396015, 0.17115468648637067, 0.15911462307963464, 0.16395735943569267, 0.06606937375453359, 0.0649511019427722, 0.056029967836902284, 0.058733255688433905, 0.057813815619411124, 0.054300650685607965, 0.06576419687229207, 0.06418467074346967, 0.05973985514269531]}, "mutation_prompt": null}
{"id": "e166ced3-91ad-48b9-85c5-7341688bed47", "solution": "import numpy as np\nimport random\n\nclass Adaptive_Frequency_Modulation_Covariance_Matrix_Adaptation:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.F = 0.5  # initial frequency\n        self.CR = 0.5  # initial crossover rate\n        self.pop_size = 50  # initial population size\n        self.population = np.random.uniform(-5.0, 5.0, (self.pop_size, self.dim))\n        self.best_solution = np.inf\n        self.probability = 0.2  # increased probability of probabilistic exploration\n        self.temperature = 100  # initial temperature for simulated annealing\n        self.covariance_matrix = np.eye(self.dim)  # initial covariance matrix\n        self.adaptive_step_size = 0.1  # initial adaptive step size for covariance matrix adaptation\n        self.step_size_learning_rate = 0.001  # learning rate for adaptive step size\n        self.history = np.zeros((self.budget, self.dim))  # history of best solutions\n\n    def evaluate(self, x):\n        return self.func(x)\n\n    def func(self, x):\n        # BBOB test suite of 24 noiseless functions\n        # Here we use the first function f1(x) = sum(x_i^2)\n        return np.sum(x**2)\n\n    def frequency_modulation(self, x):\n        # frequency modulation function\n        return self.F * np.sin(2 * np.pi * self.F * x)\n\n    def adaptive_frequency_modulation(self):\n        # adaptive frequency modulation\n        self.F *= 1.1  # increase frequency\n        if random.random() < self.probability:\n            self.F *= 0.9  # decrease frequency with probability\n        return self.F\n\n    def crossover(self, x1, x2):\n        # crossover operation\n        r = np.random.rand(self.dim)\n        return x1 + r * (x2 - x1)\n\n    def selection(self, x1, x2):\n        # selection operation\n        if self.evaluate(x2) < self.evaluate(x1):\n            return x2\n        else:\n            return x1\n\n    def simulated_annealing(self, x):\n        # simulated annealing\n        new_x = x + np.random.normal(0, 1, self.dim)\n        new_x = np.clip(new_x, -5.0, 5.0)\n        if self.evaluate(new_x) < self.evaluate(x):\n            return new_x\n        else:\n            probability = np.exp(-(self.evaluate(new_x) - self.evaluate(x)) / self.temperature)\n            if random.random() < probability:\n                return new_x\n            else:\n                return x\n        self.temperature *= 0.99  # decrease temperature with a higher rate\n\n    def covariance_matrix_adaptation(self):\n        # covariance matrix adaptation\n        self.covariance_matrix = (1 - self.adaptive_step_size) * self.covariance_matrix + self.adaptive_step_size * np.eye(self.dim)\n        return self.covariance_matrix\n\n    def adaptive_step_size_learning(self):\n        # adaptive step size learning\n        self.adaptive_step_size += self.step_size_learning_rate * (self.evaluate(self.population[0]) - self.evaluate(self.population[-1]))\n        return self.adaptive_step_size\n\n    def history_learning(self):\n        # history learning\n        self.history = np.vstack((self.history, self.population[0]))\n\n    def optimize(self, func):\n        for i in range(self.budget):\n            # evaluate population\n            fitness = [self.evaluate(x) for x in self.population]\n            # get best solution\n            self.best_solution = min(fitness)\n            best_index = fitness.index(self.best_solution)\n            # adaptive frequency modulation\n            self.F = self.adaptive_frequency_modulation()\n            # differential evolution\n            for j in range(self.pop_size):\n                # generate trial vector\n                trial = self.crossover(self.population[j], self.population[random.randint(0, self.pop_size - 1)])\n                # evaluate trial vector\n                trial_fitness = self.evaluate(trial)\n                # selection\n                self.population[j] = self.selection(self.population[j], trial)\n                # update best solution\n                if trial_fitness < self.evaluate(self.population[j]):\n                    self.population[j] = trial\n            # simulated annealing\n            for j in range(self.pop_size):\n                self.population[j] = self.simulated_annealing(self.population[j])\n            # covariance matrix adaptation\n            self.covariance_matrix = self.covariance_matrix_adaptation()\n            # adaptive step size learning\n            self.adaptive_step_size = self.adaptive_step_size_learning()\n            # history learning\n            self.history_learning()\n            # update population with probability 1\n            self.population = np.random.uniform(-5.0, 5.0, (self.pop_size, self.dim))\n        return self.best_solution, self.history\n\n    def __call__(self, func):\n        self.func = func\n        return self.optimize(func)\n\n# example usage\nbudget = 1000\ndim = 10\noptimizer = Adaptive_Frequency_Modulation_Covariance_Matrix_Adaptation(budget, dim)\nbest_solution, history = optimizer(lambda x: np.sum(x**2))\nprint(\"Best solution:\", best_solution)\nprint(\"History:\", history)", "name": "Adaptive_Frequency_Modulation_Covariance_Matrix_Adaptation", "description": "Adaptive Frequency Modulation and Covariance Matrix Adaptation Enhanced Adaptive Differential Evolution with Simulated Annealing and Adaptive Step Size Learning.", "configspace": "", "generation": 45, "fitness": 0.08692000909235144, "feedback": "The algorithm Adaptive_Frequency_Modulation_Covariance_Matrix_Adaptation got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09 with standard deviation 0.07.", "error": "", "parent_id": "62f0f28d-b1f3-482e-a59e-689769ae92f8", "metadata": {"aucs": [0.17715078672818674, 0.17427712405494145, 0.18120843240659523, 0.16041207384864598, 0.16588512769717778, 0.14612695258347175, 0.1330539371896573, 0.13835997502262232, 0.1538538680151038, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.03655789186099356, 0.07059884460872623, 0.04460568481211158, 0.043268257689073275, 0.03892924071001014, 0.041115540556860064, 0.04995678939302328, 0.043945600126867435, 0.03950977308030856, 0.0428961118138208, 0.030831854152107874, 0.029687375107376512, 0.04487865391434531, 0.0678987358312324, 0.029195734481943014, 0.03249908025393666, 0.05438555995798111, 0.04739141322824836, 0.07513200376912466, 0.0714172658248099, 0.07525837229584209, 0.06796248002318206, 0.08437301390509888, 0.08470112366077043, 0.08222331888464307, 0.08045121897974439, 0.0686524393473239, 0.06418862973723172, 0.061058820199894615, 0.04832875050057095, 0.05678709408058624, 0.0658430231875804, 0.046764764572285844, 0.053909308545991186, 0.07214910776639849, 0.05458000169198107, 0.115353397810953, 0.12390392862525856, 0.12279828839187357, 0.08501553798985151, 0.14028157014394205, 0.1226808694432493, 0.1027246160140518, 0.11166364912275817, 0.14461415250786924, 0.013583824022436408, 9.999999999998899e-05, 0.0020135216044362414, 9.999999999998899e-05, 9.999999999998899e-05, 0.042504524773570695, 0.013936451412757789, 0.039303038040560345, 0.010769977611500114, 0.004848648914576148, 0.01688400062286599, 0.009401971439771839, 0.0040713153543394665, 0.010922013930685037, 0.002597592370705004, 0.019091000454542484, 0.01548279457142654, 0.010405145794253667, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.07080509252020584, 0.03241655185847958, 0.07161438490246452, 0.043580703486350125, 0.04895275795659659, 0.017028914695561226, 0.05251873093482029, 0.07805731808676541, 0.013265915863290179, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.004538182297411875, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.2437097221747112, 0.1911382212443159, 0.19722807537749687, 0.20688586171578793, 0.1865334910460058, 0.1891488013238498, 0.1877347753779064, 0.18430559632332744, 0.19306593080140244, 0.03583303692186712, 0.044547884453894016, 0.05046977404204822, 0.03714497308695974, 0.03474230852608995, 0.04009419391465885, 0.04066166363803314, 0.049365397881466766, 0.041694071825121304, 0.1325246246734607, 0.13950950585045907, 0.12913577514008578, 0.1186283624134894, 0.12241075100192811, 0.12654453492136486, 0.12929596235841323, 0.1541078408209734, 0.11589527414620115, 0.15606460003054579, 0.15888556809011156, 0.1613932299901245, 0.15807174827752057, 0.21038516565458598, 0.16745104111456166, 0.1601495718439344, 0.17845328412287675, 0.14663766770284803, 0.10121138926597162, 0.10211072648520547, 0.0939938477632839, 0.12954345027117797, 0.14398968280483038, 0.11177691247739463, 0.12130652779636819, 0.12965460994638722, 0.09987012373178072, 0.1712909100196266, 0.16911139420500365, 0.16901504490306796, 0.15420097615351624, 0.17290297960797374, 0.1553111846619406, 0.16018330907298384, 0.15160685894304515, 0.14317531900308245, 0.14684800521068153, 0.14747046498619054, 0.14106397571400142, 0.15203214035139556, 0.14292285725531761, 0.15297946982348043, 0.1394037827032405, 0.14715944654237323, 0.145887366593064, 0.13272918824908664, 0.16379515204541328, 0.21477628331313448, 0.1492964897285045, 0.17079823157714658, 0.1551938143037983, 0.1719744506007843, 0.1705140225816536, 0.21742877533459382, 0.1301794307613574, 0.17096517809055622, 0.12022889053392083, 0.16437363101036184, 0.13361217641329282, 0.13082315660009503, 0.1777403923327049, 0.15989820758837936, 0.1567613472935936, 0.17251747069996948, 0.18763422934933338, 0.18539116006535117, 0.1884067941779386, 0.16897060301985034, 0.17409210281520804, 0.1745866792810522, 0.17102918990802807, 0.18345088502192142, 0.06363547240738265, 0.04417550657433744, 0.05247262316619439, 0.06664083512480867, 0.05858393324218214, 0.05447592070871177, 0.05751859996984987, 0.05470090126783711, 0.05578765500015437]}, "mutation_prompt": null}
{"id": "8c22a929-14b8-4717-ad7b-1ff8e1d6d7cf", "solution": "import numpy as np\nimport random\n\nclass Enhanced_Adaptive_Differential_Evolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.F = 0.5  # initial frequency\n        self.CR = 0.5  # initial crossover rate\n        self.pop_size = 50  # initial population size\n        self.population = np.random.uniform(-5.0, 5.0, (self.pop_size, self.dim))\n        self.best_solution = np.inf\n        self.probability = 0.2  # increased probability of probabilistic exploration\n        self.temperature = 100  # initial temperature for simulated annealing\n        self.covariance_matrix = np.eye(self.dim)  # initial covariance matrix\n        self.adaptive_step_size = 0.1  # initial adaptive step size for covariance matrix adaptation\n        self.step_size_learning_rate = 0.001  # learning rate for adaptive step size\n        self.history = np.zeros((self.budget, self.dim))  # history of best solutions\n        self.probability_increase_rate = 0.0005  # increased probability of probabilistic exploration\n\n    def evaluate(self, x):\n        return self.func(x)\n\n    def func(self, x):\n        # BBOB test suite of 24 noiseless functions\n        # Here we use the first function f1(x) = sum(x_i^2)\n        return np.sum(x**2)\n\n    def frequency_modulation(self, x):\n        # frequency modulation function\n        return self.F * np.sin(2 * np.pi * self.F * x)\n\n    def adaptive_frequency_modulation(self):\n        # adaptive frequency modulation\n        self.F *= 1.1  # increase frequency\n        self.probability += self.probability_increase_rate  # increase probability of probabilistic exploration\n        if random.random() < self.probability:\n            self.F *= 0.9  # decrease frequency with probability\n        return self.F\n\n    def crossover(self, x1, x2):\n        # crossover operation\n        r = np.random.rand(self.dim)\n        return x1 + r * (x2 - x1)\n\n    def selection(self, x1, x2):\n        # selection operation\n        if self.evaluate(x2) < self.evaluate(x1):\n            return x2\n        else:\n            return x1\n\n    def simulated_annealing(self, x):\n        # simulated annealing\n        new_x = x + np.random.normal(0, 1, self.dim)\n        new_x = np.clip(new_x, -5.0, 5.0)\n        if self.evaluate(new_x) < self.evaluate(x):\n            return new_x\n        else:\n            probability = np.exp(-(self.evaluate(new_x) - self.evaluate(x)) / self.temperature)\n            if random.random() < probability:\n                return new_x\n            else:\n                return x\n        self.temperature *= 0.99  # decrease temperature with a higher rate\n\n    def covariance_matrix_adaptation(self):\n        # covariance matrix adaptation\n        self.covariance_matrix = (1 - self.adaptive_step_size) * self.covariance_matrix + self.adaptive_step_size * np.eye(self.dim)\n        return self.covariance_matrix\n\n    def adaptive_step_size_learning(self):\n        # adaptive step size learning\n        self.adaptive_step_size += self.step_size_learning_rate * (self.evaluate(self.population[0]) - self.evaluate(self.population[-1]))\n        return self.adaptive_step_size\n\n    def history_learning(self):\n        # history learning\n        self.history = np.vstack((self.history, self.population[0]))\n\n    def optimize(self, func):\n        for i in range(self.budget):\n            # evaluate population\n            fitness = [self.evaluate(x) for x in self.population]\n            # get best solution\n            self.best_solution = min(fitness)\n            best_index = fitness.index(self.best_solution)\n            # adaptive frequency modulation\n            self.F = self.adaptive_frequency_modulation()\n            # differential evolution\n            for j in range(self.pop_size):\n                # generate trial vector\n                trial = self.crossover(self.population[j], self.population[random.randint(0, self.pop_size - 1)])\n                # evaluate trial vector\n                trial_fitness = self.evaluate(trial)\n                # selection\n                self.population[j] = self.selection(self.population[j], trial)\n                # update best solution\n                if trial_fitness < self.evaluate(self.population[j]):\n                    self.population[j] = trial\n            # simulated annealing\n            for j in range(self.pop_size):\n                self.population[j] = self.simulated_annealing(self.population[j])\n            # covariance matrix adaptation\n            self.covariance_matrix = self.covariance_matrix_adaptation()\n            # adaptive step size learning\n            self.adaptive_step_size = self.adaptive_step_size_learning()\n            # history learning\n            self.history_learning()\n        return self.best_solution, self.history\n\n    def __call__(self, func):\n        self.func = func\n        return self.optimize(func)\n\n# example usage\nbudget = 1000\ndim = 10\noptimizer = Enhanced_Adaptive_Differential_Evolution(budget, dim)\nbest_solution, history = optimizer(lambda x: np.sum(x**2))\nprint(\"Best solution:\", best_solution)\nprint(\"History:\", history)", "name": "Enhanced_Adaptive_Differential_Evolution", "description": "Enhanced Adaptive Frequency Modulation Differential Evolution with Simulated Annealing and Covariance Matrix Adaptation with Adaptive Step Size Learning and History Learning.", "configspace": "", "generation": 46, "fitness": 0.11001710249735687, "feedback": "The algorithm Enhanced_Adaptive_Differential_Evolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.11 with standard deviation 0.07.", "error": "", "parent_id": "62f0f28d-b1f3-482e-a59e-689769ae92f8", "metadata": {"aucs": [0.23173886470366478, 0.21550708011431985, 0.228893988875989, 0.2324342906312502, 0.20605087302681568, 0.21872530563230919, 0.21689449409303774, 0.18917888402741723, 0.18970334318219162, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.03303712504047929, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.055680148297501564, 0.07087811973382718, 0.06200440984110933, 0.07121940185939024, 0.06681975454157163, 0.06028223506968022, 0.0696590635824268, 0.06004009991263426, 0.06051961519985816, 0.04801458308542694, 0.04405585672912615, 0.043127096255705655, 0.06174362098958741, 0.049985315904284344, 0.04961311849678662, 0.05629216040221763, 0.046924066124710784, 0.05406520597450204, 0.16200111144871787, 0.11028223881632915, 0.10380100567035788, 0.17577064359153116, 0.1295690814559668, 0.12185685088167297, 0.17132033989169781, 0.12728614167283125, 0.1781728617489331, 0.1278764122038316, 0.10235169250169096, 0.08627972926644978, 0.0783280328835887, 0.0874434635012099, 0.06289011413749468, 0.09156865125197422, 0.08257539905548072, 0.09216314060769348, 0.13604185537643187, 0.18053457114977467, 0.15145211414629156, 0.18984574113988706, 0.16886478463823462, 0.18012308586616543, 0.1505531381251396, 0.13985372203793944, 0.15525375922845874, 0.039045417715707686, 0.02997609920211275, 0.04084246867631913, 0.05006285789375997, 0.021817815276971686, 0.05859166282281514, 0.10877088180741068, 0.05968930460098565, 0.08104598616448455, 0.09713762458103337, 0.10760888498627375, 0.10278519852149326, 0.12483311648062934, 0.10872041079478045, 0.10982688168595323, 0.12190872730146185, 0.12333822397957739, 0.09168062400755805, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.002391418071656548, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.08258677673752024, 0.09422774471782813, 0.1548097467228836, 0.037705713565112986, 0.02013948380025432, 0.04115559199379171, 0.0810821537063443, 0.07057269995352089, 0.06663930576070287, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.029919285619130265, 0.018380764709095443, 0.03378805505165383, 0.02807505317129566, 0.04122371396263769, 0.03754248235114732, 0.03543285391543194, 0.04179163743202974, 0.042912728785105325, 0.23667828458689077, 0.23890136379222227, 0.24624677290470742, 0.22873834132938453, 0.2223280636506808, 0.20809726553487573, 0.23698488695559428, 0.23035237529436403, 0.25705724620355297, 0.04438194039575505, 0.050221376226521564, 0.04873014461304015, 0.06532713849421612, 0.0760658661282867, 0.07376489143851306, 0.05864703973682717, 0.05983592224145884, 0.07001886588012873, 0.11095078850704809, 0.13042636970650734, 0.1588107760743831, 0.11399682125400723, 0.11600937265380529, 0.11427340722009816, 0.12289366508282396, 0.14792204499657713, 0.11275550247600674, 0.16943449296325108, 0.17618374446921647, 0.16564544940659365, 0.177761027008687, 0.18743259643069698, 0.1897655549722862, 0.15742357477018398, 0.18953528744901182, 0.17343607730848176, 0.1155444886219067, 0.14801983434523647, 0.11468478642101365, 0.1271993052594953, 0.1428936305934394, 0.1349245331037917, 0.12807983785694088, 0.1327585536890885, 0.11417360477893812, 0.20392513000417423, 0.17519661604304382, 0.1825253317760387, 0.1678774532253854, 0.15744831471408016, 0.17335923195017655, 0.21505691681303307, 0.19006681005507176, 0.15577742485031487, 0.16060970547236064, 0.17691409836304517, 0.16214917003740503, 0.1561586580739982, 0.1563513102112316, 0.1528239698250411, 0.15177436633687358, 0.15232722108516084, 0.166644507933163, 0.13743129317096625, 0.1585215926422907, 0.1517999943449433, 0.2363269653821669, 0.18866961594583642, 0.15773083812599964, 0.20756849338828587, 0.20770977388541256, 0.16504924658502373, 0.1464548640697254, 0.14898138122077653, 0.1359001099894045, 0.12477576811469715, 0.1607735755811982, 0.1894680395461702, 0.14005781854327504, 0.2467308412771092, 0.13742755632097847, 0.21312244747548859, 0.17662418784761014, 0.16080416577083545, 0.16900951357391203, 0.19204491293512127, 0.16924002007568617, 0.17492602743726327, 0.15444701590310683, 0.17574381671418648, 0.055936717140546044, 0.056305281517464945, 0.06004313633305103, 0.0662433845958822, 0.06988021681515288, 0.05688096071439008, 0.06939769119121975, 0.07140294957856264, 0.07025757592914272]}, "mutation_prompt": null}
{"id": "d02d8cce-6cf3-4fee-a42d-eb575b361a24", "solution": "import numpy as np\nimport random\n\nclass Novel_Adaptive_Frequency_Modulation:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.F = 0.5  # initial frequency\n        self.CR = 0.5  # initial crossover rate\n        self.pop_size = 50  # initial population size\n        self.population = np.random.uniform(-5.0, 5.0, (self.pop_size, self.dim))\n        self.best_solution = np.inf\n        self.probability = 0.2  # increased probability of probabilistic exploration\n        self.temperature = 100  # initial temperature for simulated annealing\n        self.covariance_matrix = np.eye(self.dim)  # initial covariance matrix\n        self.adaptive_step_size = 0.1  # initial adaptive step size for covariance matrix adaptation\n        self.step_size_learning_rate = 0.001  # learning rate for adaptive step size\n        self.history = np.zeros((self.budget, self.dim))  # history of best solutions\n        self.history_index = 0  # index for history learning\n\n    def evaluate(self, x):\n        return self.func(x)\n\n    def func(self, x):\n        # BBOB test suite of 24 noiseless functions\n        # Here we use the first function f1(x) = sum(x_i^2)\n        return np.sum(x**2)\n\n    def frequency_modulation(self, x):\n        # frequency modulation function\n        return self.F * np.sin(2 * np.pi * self.F * x)\n\n    def adaptive_frequency_modulation(self):\n        # adaptive frequency modulation\n        self.F *= 1.1  # increase frequency\n        if random.random() < self.probability:\n            self.F *= 0.9  # decrease frequency with probability\n        return self.F\n\n    def crossover(self, x1, x2):\n        # crossover operation\n        r = np.random.rand(self.dim)\n        return x1 + r * (x2 - x1)\n\n    def selection(self, x1, x2):\n        # selection operation\n        if self.evaluate(x2) < self.evaluate(x1):\n            return x2\n        else:\n            return x1\n\n    def simulated_annealing(self, x):\n        # simulated annealing\n        new_x = x + np.random.normal(0, 1, self.dim)\n        new_x = np.clip(new_x, -5.0, 5.0)\n        if self.evaluate(new_x) < self.evaluate(x):\n            return new_x\n        else:\n            probability = np.exp(-(self.evaluate(new_x) - self.evaluate(x)) / self.temperature)\n            if random.random() < probability:\n                return new_x\n            else:\n                return x\n        self.temperature *= 0.99  # decrease temperature with a higher rate\n\n    def covariance_matrix_adaptation(self):\n        # covariance matrix adaptation\n        self.covariance_matrix = (1 - self.adaptive_step_size) * self.covariance_matrix + self.adaptive_step_size * np.eye(self.dim)\n        return self.covariance_matrix\n\n    def adaptive_step_size_learning(self):\n        # adaptive step size learning\n        self.adaptive_step_size += self.step_size_learning_rate * (self.evaluate(self.population[0]) - self.evaluate(self.population[-1]))\n        return self.adaptive_step_size\n\n    def history_learning(self):\n        # history learning\n        self.history[self.history_index] = self.population[0]\n        self.history_index += 1\n        if self.history_index >= self.budget:\n            self.history_index = 0\n\n    def optimize(self, func):\n        for i in range(self.budget):\n            # evaluate population\n            fitness = [self.evaluate(x) for x in self.population]\n            # get best solution\n            self.best_solution = min(fitness)\n            best_index = fitness.index(self.best_solution)\n            # adaptive frequency modulation\n            self.F = self.adaptive_frequency_modulation()\n            # differential evolution\n            for j in range(self.pop_size):\n                # generate trial vector\n                trial = self.crossover(self.population[j], self.population[random.randint(0, self.pop_size - 1)])\n                # evaluate trial vector\n                trial_fitness = self.evaluate(trial)\n                # selection\n                self.population[j] = self.selection(self.population[j], trial)\n                # update best solution\n                if trial_fitness < self.evaluate(self.population[j]):\n                    self.population[j] = trial\n            # simulated annealing\n            for j in range(self.pop_size):\n                self.population[j] = self.simulated_annealing(self.population[j])\n            # covariance matrix adaptation\n            self.covariance_matrix = self.covariance_matrix_adaptation()\n            # adaptive step size learning\n            self.adaptive_step_size = self.adaptive_step_size_learning()\n            # history learning\n            self.history_learning()\n        return self.best_solution, self.history\n\n    def __call__(self, func):\n        self.func = func\n        return self.optimize(func)\n\n# example usage\nbudget = 1000\ndim = 10\noptimizer = Novel_Adaptive_Frequency_Modulation(budget, dim)\nbest_solution, history = optimizer(lambda x: np.sum(x**2))\nprint(\"Best solution:\", best_solution)\nprint(\"History:\", history)", "name": "Novel_Adaptive_Frequency_Modulation", "description": "Novel Adaptive Frequency Modulation and Covariance Matrix Adaptation with History Learning and Simulated Annealing with Adaptive Temperature Cooling Schedule and Adaptive Step Size Learning.", "configspace": "", "generation": 47, "fitness": 0.1103687626685967, "feedback": "The algorithm Novel_Adaptive_Frequency_Modulation got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.11 with standard deviation 0.07.", "error": "", "parent_id": "62f0f28d-b1f3-482e-a59e-689769ae92f8", "metadata": {"aucs": [0.21689241493889688, 0.21977747715362506, 0.24303443551267956, 0.20981655605333882, 0.19541908521217166, 0.20368044154962206, 0.2296737577304805, 0.20724505064079868, 0.21701681865098, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.006587421031342466, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.06347873038163454, 0.06821520569953643, 0.06555164703257121, 0.06637880078163894, 0.06152494831105726, 0.06969286789586093, 0.07273788842557183, 0.059213907138292776, 0.05164912038567615, 0.05327623023432282, 0.06265321700339865, 0.052279555748090734, 0.06458372204986595, 0.05707361734171823, 0.05082123984479092, 0.06190509740611794, 0.0485384580659427, 0.04924876538969969, 0.11531848514949561, 0.13024308927960826, 0.09078860019921808, 0.12993668080536558, 0.15775310953518096, 0.13130636628770354, 0.17619800597895852, 0.12432249087033487, 0.4163419279099092, 0.09679241816041573, 0.10158363456288622, 0.06820007864803102, 0.08694016418941486, 0.07568626801355716, 0.06808634750719522, 0.10484941124533542, 0.09885790521680704, 0.10555620519703002, 0.16728898447754492, 0.18918431826587245, 0.13784465977493432, 0.172123621426848, 0.17860553604945084, 0.1845653481848586, 0.1539730672176931, 0.1339325682939383, 0.1534891190220118, 0.03906559114613484, 0.04184848332761071, 0.09470009746425645, 0.03968198754002372, 0.06328040572213633, 0.03675987039661488, 0.07395135762521987, 0.05025401244181815, 0.08528333068841731, 0.08961430422329542, 0.09917012432421601, 0.10749899328542889, 0.10089607607897655, 0.10240526203943157, 0.09793484311845624, 0.11098558599614428, 0.08432596041752782, 0.10541987036622413, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.06687963311828993, 0.07592278185193224, 0.08434533747471773, 0.03280467366830797, 0.03234414249666007, 0.05511107554758565, 0.08696633102262608, 0.0337890759684597, 0.09214271137960062, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.02465904880031866, 0.02730697938243698, 0.02830757737988876, 0.03909404737447364, 0.03939931427073862, 0.04018880184092399, 0.04013393938401244, 0.033071773179419006, 0.03506825488856424, 0.23270789092677524, 0.23048740192845418, 0.21828152068937223, 0.21783054525366075, 0.20302036659318834, 0.2116932727311841, 0.2305643364561193, 0.23993803782014145, 0.262392189869196, 0.049804804052661855, 0.060707922085374566, 0.0492053323336229, 0.07320446959792493, 0.057042112570878434, 0.07809514683053276, 0.07415700761901589, 0.05076544818051876, 0.06504478348862197, 0.1265387756902837, 0.11496420656727391, 0.14092693620688423, 0.12113013343128365, 0.1321438870802225, 0.14541030380084286, 0.10526430964058764, 0.12451141465554261, 0.1299884136560986, 0.1531378288264179, 0.18466378052033572, 0.17078512915527067, 0.1848831164383633, 0.1796295715519729, 0.19149167498091546, 0.1731302532240192, 0.1838037979968563, 0.17439230842283926, 0.11768391940937073, 0.13489635274657685, 0.11140394015840827, 0.14426541690846384, 0.12806739020563018, 0.13933213258922483, 0.10849532999093203, 0.11669975754655026, 0.10834663349305995, 0.18267232453784865, 0.16209230555197174, 0.19606222081710167, 0.16879937425247427, 0.17172484438897084, 0.17655061342058975, 0.1760966855307592, 0.16554383584915067, 0.17955165499611248, 0.1537937000301316, 0.164368872050896, 0.16356179628003553, 0.16000631157968204, 0.1627392307836497, 0.15432308866122968, 0.1696349496395536, 0.1576654232389315, 0.15934311158175174, 0.1673584041109425, 0.15914665783506932, 0.15767915424323242, 0.2373563633555782, 0.1989503471327001, 0.24351223230953178, 0.18722998857181194, 0.18245156509541782, 0.1442437927091521, 0.1597072559103826, 0.21141962246002255, 0.1529038433888671, 0.20344309911801384, 0.16280444751831347, 0.1285507343734249, 0.14114393879184084, 0.18905233225548534, 0.14407743386542315, 0.17162083874086043, 0.19520993920036056, 0.16721035895097414, 0.16848668253141763, 0.16552612248772025, 0.1850422371281345, 0.16953407608702287, 0.16545910531471986, 0.18783745721089695, 0.06184354827533878, 0.05569046600961336, 0.06140126076483843, 0.06171807484454217, 0.05522440855382793, 0.06339875453037125, 0.060129837477791104, 0.054243432558900295, 0.07263873128182352]}, "mutation_prompt": null}
{"id": "829387a4-aebb-41ae-ad9c-d2b28f631fbb", "solution": "import numpy as np\nimport random\n\nclass Adaptive_Frequency_Modulation_and_Covariance_Matrix_Adaptation:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.F = 0.5  # initial frequency\n        self.CR = 0.5  # initial crossover rate\n        self.pop_size = 50  # initial population size\n        self.population = np.random.uniform(-5.0, 5.0, (self.pop_size, self.dim))\n        self.best_solution = np.inf\n        self.probability = 0.2  # increased probability of probabilistic exploration\n        self.temperature = 100  # initial temperature for simulated annealing\n        self.covariance_matrix = np.eye(self.dim)  # initial covariance matrix\n        self.adaptive_step_size = 0.1  # initial adaptive step size for covariance matrix adaptation\n        self.step_size_learning_rate = 0.001  # learning rate for adaptive step size\n        self.history = np.zeros((self.budget, self.dim))  # history of best solutions\n        self.hard_coded_solution = np.array([1.0, -1.0, -1.0, -1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0])\n\n    def evaluate(self, x):\n        return self.func(x)\n\n    def func(self, x):\n        # BBOB test suite of 24 noiseless functions\n        # Here we use the first function f1(x) = sum(x_i^2)\n        return np.sum(x**2)\n\n    def frequency_modulation(self, x):\n        # frequency modulation function\n        return self.F * np.sin(2 * np.pi * self.F * x)\n\n    def adaptive_frequency_modulation(self):\n        # adaptive frequency modulation\n        self.F *= 1.1  # increase frequency\n        if random.random() < self.probability:\n            self.F *= 0.9  # decrease frequency with probability\n        return self.F\n\n    def crossover(self, x1, x2):\n        # crossover operation\n        r = np.random.rand(self.dim)\n        return x1 + r * (x2 - x1)\n\n    def selection(self, x1, x2):\n        # selection operation\n        if self.evaluate(x2) < self.evaluate(x1):\n            return x2\n        else:\n            return x1\n\n    def hybrid_simulated_annealing(self, x):\n        # hybrid simulated annealing\n        new_x = self.simulated_annealing(x)\n        if np.random.rand() < 0.1:\n            new_x = self.crossover(x, self.hard_coded_solution)\n        return new_x\n\n    def simulated_annealing(self, x):\n        # simulated annealing\n        new_x = x + np.random.normal(0, 1, self.dim)\n        new_x = np.clip(new_x, -5.0, 5.0)\n        if self.evaluate(new_x) < self.evaluate(x):\n            return new_x\n        else:\n            probability = np.exp(-(self.evaluate(new_x) - self.evaluate(x)) / self.temperature)\n            if random.random() < probability:\n                return new_x\n            else:\n                return x\n        self.temperature *= 0.99  # decrease temperature with a higher rate\n\n    def covariance_matrix_adaptation(self):\n        # covariance matrix adaptation\n        self.covariance_matrix = (1 - self.adaptive_step_size) * self.covariance_matrix + self.adaptive_step_size * np.eye(self.dim)\n        return self.covariance_matrix\n\n    def adaptive_step_size_learning(self):\n        # adaptive step size learning\n        self.adaptive_step_size += self.step_size_learning_rate * (self.evaluate(self.population[0]) - self.evaluate(self.population[-1]))\n        return self.adaptive_step_size\n\n    def history_learning(self):\n        # history learning\n        self.history = np.vstack((self.history, self.population[0]))\n\n    def optimize(self, func):\n        for i in range(self.budget):\n            # evaluate population\n            fitness = [self.evaluate(x) for x in self.population]\n            # get best solution\n            self.best_solution = min(fitness)\n            best_index = fitness.index(self.best_solution)\n            # adaptive frequency modulation\n            self.F = self.adaptive_frequency_modulation()\n            # differential evolution\n            for j in range(self.pop_size):\n                # generate trial vector\n                trial = self.crossover(self.population[j], self.population[random.randint(0, self.pop_size - 1)])\n                # evaluate trial vector\n                trial_fitness = self.evaluate(trial)\n                # selection\n                self.population[j] = self.selection(self.population[j], trial)\n                # update best solution\n                if trial_fitness < self.evaluate(self.population[j]):\n                    self.population[j] = trial\n            # hybrid simulated annealing\n            for j in range(self.pop_size):\n                self.population[j] = self.hybrid_simulated_annealing(self.population[j])\n            # covariance matrix adaptation\n            self.covariance_matrix = self.covariance_matrix_adaptation()\n            # adaptive step size learning\n            self.adaptive_step_size = self.adaptive_step_size_learning()\n            # history learning\n            self.history_learning()\n        return self.best_solution, self.history\n\n    def __call__(self, func):\n        self.func = func\n        return self.optimize(func)\n\n# example usage\nbudget = 1000\ndim = 10\noptimizer = Adaptive_Frequency_Modulation_and_Covariance_Matrix_Adaptation(budget, dim)\nbest_solution, history = optimizer(lambda x: np.sum(x**2))\nprint(\"Best solution:\", best_solution)\nprint(\"History:\", history)", "name": "Adaptive_Frequency_Modulation_and_Covariance_Matrix_Adaptation", "description": "Adaptive Frequency Modulation and Covariance Matrix Adaptation with Hybrid Simulated Annealing and Differential Evolution for Black Box Optimization.", "configspace": "", "generation": 48, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('operands could not be broadcast together with shapes (10,) (5,) ').", "error": "ValueError('operands could not be broadcast together with shapes (10,) (5,) ')", "parent_id": "62f0f28d-b1f3-482e-a59e-689769ae92f8", "metadata": {}, "mutation_prompt": null}
{"id": "c8448788-5b4f-4d5f-a944-1241f893cdd9", "solution": "import numpy as np\nimport random\n\nclass Enhanced_Adaptive_Differential_Evolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.F = 0.5  # initial frequency\n        self.CR = 0.5  # initial crossover rate\n        self.pop_size = 50  # initial population size\n        self.population = np.random.uniform(-5.0, 5.0, (self.pop_size, self.dim))\n        self.best_solution = np.inf\n        self.probability = 0.3  # increased probability of probabilistic exploration\n        self.temperature = 100  # initial temperature for simulated annealing\n        self.covariance_matrix = np.eye(self.dim)  # initial covariance matrix\n        self.adaptive_step_size = 0.1  # initial adaptive step size for covariance matrix adaptation\n        self.step_size_learning_rate = 0.001  # learning rate for adaptive step size\n        self.history = np.zeros((self.budget, self.dim))  # history of best solutions\n\n    def evaluate(self, x):\n        return self.func(x)\n\n    def func(self, x):\n        # BBOB test suite of 24 noiseless functions\n        # Here we use the first function f1(x) = sum(x_i^2)\n        return np.sum(x**2)\n\n    def frequency_modulation(self, x):\n        # frequency modulation function\n        return self.F * np.sin(2 * np.pi * self.F * x)\n\n    def adaptive_frequency_modulation(self):\n        # adaptive frequency modulation\n        self.F *= 1.1  # increase frequency\n        if random.random() < self.probability:\n            self.F *= 0.9  # decrease frequency with probability\n        return self.F\n\n    def crossover(self, x1, x2):\n        # crossover operation\n        r = np.random.rand(self.dim)\n        return x1 + r * (x2 - x1)\n\n    def selection(self, x1, x2):\n        # selection operation\n        if self.evaluate(x2) < self.evaluate(x1):\n            return x2\n        else:\n            return x1\n\n    def simulated_annealing(self, x):\n        # simulated annealing\n        new_x = x + np.random.normal(0, 1, self.dim)\n        new_x = np.clip(new_x, -5.0, 5.0)\n        if self.evaluate(new_x) < self.evaluate(x):\n            return new_x\n        else:\n            probability = np.exp(-(self.evaluate(new_x) - self.evaluate(x)) / self.temperature)\n            if random.random() < probability:\n                return new_x\n            else:\n                return x\n        self.temperature *= 0.99  # decrease temperature with a higher rate\n\n    def covariance_matrix_adaptation(self):\n        # covariance matrix adaptation\n        self.covariance_matrix = (1 - self.adaptive_step_size) * self.covariance_matrix + self.adaptive_step_size * np.eye(self.dim)\n        return self.covariance_matrix\n\n    def adaptive_step_size_learning(self):\n        # adaptive step size learning\n        self.adaptive_step_size += self.step_size_learning_rate * (self.evaluate(self.population[0]) - self.evaluate(self.population[-1]))\n        return self.adaptive_step_size\n\n    def history_learning(self):\n        # history learning\n        self.history = np.vstack((self.history, self.population[0]))\n\n    def optimize(self, func):\n        for i in range(self.budget):\n            # evaluate population\n            fitness = [self.evaluate(x) for x in self.population]\n            # get best solution\n            self.best_solution = min(fitness)\n            best_index = fitness.index(self.best_solution)\n            # adaptive frequency modulation\n            self.F = self.adaptive_frequency_modulation()\n            # differential evolution\n            for j in range(self.pop_size):\n                # generate trial vector\n                trial = self.crossover(self.population[j], self.population[random.randint(0, self.pop_size - 1)])\n                # evaluate trial vector\n                trial_fitness = self.evaluate(trial)\n                # selection\n                self.population[j] = self.selection(self.population[j], trial)\n                # update best solution\n                if trial_fitness < self.evaluate(self.population[j]):\n                    self.population[j] = trial\n            # simulated annealing\n            for j in range(self.pop_size):\n                self.population[j] = self.simulated_annealing(self.population[j])\n            # covariance matrix adaptation\n            self.covariance_matrix = self.covariance_matrix_adaptation()\n            # adaptive step size learning\n            self.adaptive_step_size = self.adaptive_step_size_learning()\n            # history learning\n            self.history_learning()\n        return self.best_solution, self.history\n\n    def __call__(self, func):\n        self.func = func\n        return self.optimize(func)\n\n# example usage\nbudget = 1000\ndim = 10\noptimizer = Enhanced_Adaptive_Differential_Evolution(budget, dim)\nbest_solution, history = optimizer(lambda x: np.sum(x**2))\nprint(\"Best solution:\", best_solution)\nprint(\"History:\", history)", "name": "Enhanced_Adaptive_Differential_Evolution", "description": "Enhanced Adaptive Differential Evolution with Probabilistic Frequency Modulation, Covariance Matrix Adaptation, and Simulated Annealing with Adaptive Temperature Cooling Schedule and Adaptive Step Size Learning, and the probability of changing the frequency modulation is increased to 0.3.", "configspace": "", "generation": 49, "fitness": 0.10910075591407031, "feedback": "The algorithm Enhanced_Adaptive_Differential_Evolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.11 with standard deviation 0.07.", "error": "", "parent_id": "62f0f28d-b1f3-482e-a59e-689769ae92f8", "metadata": {"aucs": [0.24371662099370117, 0.2264000021985938, 0.25208576511888825, 0.2076840595182865, 0.21877302643419794, 0.19397505783869284, 0.21575255558886797, 0.22011047812843143, 0.1898345261645208, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.01121975791584262, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.05741964321663251, 0.06459827736471857, 0.05682053496043604, 0.0738210188777011, 0.06855222724171339, 0.07142262631741181, 0.07675596545837782, 0.06482737012690176, 0.06808227564284719, 0.04917026207408304, 0.05577124977159309, 0.04488277231654725, 0.05440811769666831, 0.050167046010722194, 0.045512572080675495, 0.050930532321565636, 0.06061371621274103, 0.047460579090558896, 0.2287350690632851, 0.12496635529750222, 0.09737201061925493, 0.22795526005270683, 0.12901962783524668, 0.12864601459113711, 0.16970485402627422, 0.12401836243329145, 0.13277184524919994, 0.11114517410216995, 0.09334730553391224, 0.08345166045074692, 0.1012615071898304, 0.1084117001499737, 0.0906215311694143, 0.10927168311950042, 0.12943711265289803, 0.0958538402829614, 0.17134060111350247, 0.168040048660705, 0.1780245852816772, 0.16142432055710998, 0.17319581943038354, 0.16814672156733557, 0.12410764620353276, 0.1590728496257141, 0.15361410382129803, 0.04616924674423373, 0.03679888969754863, 0.08044996220598855, 0.0373857741428385, 0.05019611375890376, 0.0437856590433221, 0.08568788144961159, 0.049217984862368946, 0.06625400722068131, 0.09474513676732588, 0.11112342414120358, 0.0916947145974416, 0.11725972638836613, 0.11096605317159136, 0.09352114164857683, 0.10331191942765217, 0.10167948332912802, 0.10332465142756175, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.06520590121070746, 0.08289452070273984, 0.06608090606523398, 0.029129742940381176, 0.04791177302444871, 0.04134769841639463, 0.06168915365181182, 0.09704699808332629, 0.05953054124690116, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.023211416840701493, 0.033083890190980125, 0.029519859092763334, 0.02039436354815516, 0.04222187694121726, 0.04551362050564334, 0.030794832713540776, 0.029577606681917445, 0.04087109960568047, 0.23846170529741617, 0.24144429744161966, 0.21890059646655013, 0.22722947535854776, 0.20054352530771158, 0.21694879110019316, 0.26082306058627625, 0.22817036542282898, 0.22099718358098064, 0.05230106508175636, 0.0574004666743807, 0.05092621886916637, 0.07941831188749371, 0.05670247715986387, 0.06831746790000148, 0.06834184271030552, 0.05862866451730875, 0.055619804604575385, 0.11108794136465061, 0.11250442710427289, 0.12169964597380278, 0.1690210282727318, 0.11612246328586284, 0.11098804602720369, 0.12672953050997826, 0.20617082910723428, 0.12160379242078401, 0.15524624330903225, 0.18876309729403096, 0.1731025627871945, 0.17695598917513655, 0.19205180234622188, 0.18080021686880032, 0.17017339417123944, 0.19380555024047452, 0.16145310841358718, 0.11713170350574931, 0.1241936867895147, 0.10636060635442413, 0.11833414078967253, 0.1274168733281078, 0.12368630976777917, 0.11023892845274608, 0.11480809999990704, 0.11696761766727504, 0.20342187421506064, 0.169408762249887, 0.18038123249153026, 0.18391033766213527, 0.16904661038463908, 0.16647881515342655, 0.1962587767338524, 0.1822230292016066, 0.1757500160223564, 0.16657080252324363, 0.1616294983935188, 0.1614104222910846, 0.15365922586863745, 0.16312457733020602, 0.16680798624846283, 0.15708166918040056, 0.15643069568517243, 0.16061969448835767, 0.16291268372897905, 0.17603481990222225, 0.15792136748192787, 0.1816991003238666, 0.22250344983615855, 0.15639330757460757, 0.2153941087571145, 0.14436574331881769, 0.15304594718475295, 0.1478241366260672, 0.19450663683521585, 0.1489568527529822, 0.15279461073060863, 0.127064473200174, 0.1518071860521999, 0.11634757342339674, 0.12714581290282456, 0.13713319388544554, 0.1588655861238667, 0.1624744838538169, 0.16440350924550529, 0.16392045729229832, 0.17040617844748263, 0.1963632049709939, 0.1777383488130737, 0.16833639157030156, 0.18793054024727784, 0.06316168711058912, 0.0658629576026587, 0.05819968212118942, 0.05949994128376079, 0.06018135938893521, 0.06669117167782002, 0.05805522030351662, 0.07145758631931909, 0.05963316688207998]}, "mutation_prompt": null}
{"id": "d3a8f2a3-9057-41cf-b3c0-2d4bacf40ba3", "solution": "import numpy as np\nimport random\n\nclass Adaptive_Frequency_Modulation:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.F = 0.5  # initial frequency\n        self.CR = 0.5  # initial crossover rate\n        self.pop_size = 50  # initial population size\n        self.population = np.random.uniform(-5.0, 5.0, (self.pop_size, self.dim))\n        self.best_solution = np.inf\n        self.temperature = 100  # initial temperature for simulated annealing\n        self.covariance_matrix = np.eye(self.dim)  # initial covariance matrix\n        self.adaptive_step_size = 0.1  # initial adaptive step size for covariance matrix adaptation\n        self.step_size_learning_rate = 0.001  # learning rate for adaptive step size\n        self.history = np.zeros((self.budget, self.dim))  # history of best solutions\n\n    def evaluate(self, x):\n        return self.func(x)\n\n    def func(self, x):\n        # BBOB test suite of 24 noiseless functions\n        # Here we use the first function f1(x) = sum(x_i^2)\n        return np.sum(x**2)\n\n    def frequency_modulation(self, x):\n        # frequency modulation function\n        return self.F * np.sin(2 * np.pi * self.F * x)\n\n    def adaptive_frequency_modulation(self):\n        # adaptive frequency modulation\n        self.F *= 1.1  # increase frequency\n        self.F = np.clip(self.F, 0.1, 10)  # clip frequency\n        return self.F\n\n    def crossover(self, x1, x2):\n        # crossover operation\n        r = np.random.rand(self.dim)\n        return x1 + r * (x2 - x1)\n\n    def selection(self, x1, x2):\n        # selection operation\n        if self.evaluate(x2) < self.evaluate(x1):\n            return x2\n        else:\n            return x1\n\n    def simulated_annealing(self, x):\n        # simulated annealing\n        new_x = x + np.random.normal(0, 1, self.dim)\n        new_x = np.clip(new_x, -5.0, 5.0)\n        if self.evaluate(new_x) < self.evaluate(x):\n            return new_x\n        else:\n            probability = np.exp(-(self.evaluate(new_x) - self.evaluate(x)) / self.temperature)\n            if random.random() < probability:\n                return new_x\n            else:\n                return x\n        self.temperature *= 0.99  # decrease temperature with a higher rate\n\n    def covariance_matrix_adaptation(self):\n        # covariance matrix adaptation\n        self.covariance_matrix = (1 - self.adaptive_step_size) * self.covariance_matrix + self.adaptive_step_size * np.eye(self.dim)\n        return self.covariance_matrix\n\n    def adaptive_step_size_learning(self):\n        # adaptive step size learning\n        self.adaptive_step_size += self.step_size_learning_rate * (self.evaluate(self.population[0]) - self.evaluate(self.population[-1]))\n        return self.adaptive_step_size\n\n    def history_learning(self):\n        # history learning\n        self.history = np.vstack((self.history, self.population[0]))\n\n    def optimize(self, func):\n        for i in range(self.budget):\n            # evaluate population\n            fitness = [self.evaluate(x) for x in self.population]\n            # get best solution\n            self.best_solution = min(fitness)\n            best_index = fitness.index(self.best_solution)\n            # adaptive frequency modulation\n            self.F = self.adaptive_frequency_modulation()\n            # differential evolution\n            for j in range(self.pop_size):\n                # generate trial vector\n                trial = self.crossover(self.population[j], self.population[random.randint(0, self.pop_size - 1)])\n                # evaluate trial vector\n                trial_fitness = self.evaluate(trial)\n                # selection\n                self.population[j] = self.selection(self.population[j], trial)\n                # update best solution\n                if trial_fitness < self.evaluate(self.population[j]):\n                    self.population[j] = trial\n            # simulated annealing\n            for j in range(self.pop_size):\n                self.population[j] = self.simulated_annealing(self.population[j])\n            # covariance matrix adaptation\n            self.covariance_matrix = self.covariance_matrix_adaptation()\n            # adaptive step size learning\n            self.adaptive_step_size = self.adaptive_step_size_learning()\n            # history learning\n            self.history_learning()\n        return self.best_solution, self.history\n\n    def __call__(self, func):\n        self.func = func\n        return self.optimize(func)\n\n# example usage\nbudget = 1000\ndim = 10\noptimizer = Adaptive_Frequency_Modulation(budget, dim)\nbest_solution, history = optimizer(lambda x: np.sum(x**2))\nprint(\"Best solution:\", best_solution)\nprint(\"History:\", history)", "name": "Adaptive_Frequency_Modulation", "description": "Adaptive Frequency Modulation with Covariance Matrix Adaptation and Simulated Annealing with Adaptive Temperature Cooling Schedule and Adaptive Step Size Learning, and History Learning with Probability 1.", "configspace": "", "generation": 50, "fitness": 0.10881619305733395, "feedback": "The algorithm Adaptive_Frequency_Modulation got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.11 with standard deviation 0.07.", "error": "", "parent_id": "62f0f28d-b1f3-482e-a59e-689769ae92f8", "metadata": {"aucs": [0.22302498684353167, 0.2533504410528339, 0.22462499991286922, 0.2129013124462742, 0.20417365413261757, 0.2341983751090062, 0.20248333596348755, 0.19390950330966605, 0.20831156655903915, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.062354037314273025, 0.07987229708459176, 0.052416896473441055, 0.08296873857228892, 0.05626249084611967, 0.07059592779201596, 0.05864154518096765, 0.06909886283635003, 0.0616186122308342, 0.050637474405626715, 0.05050424024749789, 0.04685310184939895, 0.054874797183626645, 0.05539979937426376, 0.06065706761246403, 0.046550679976327225, 0.05043430356715961, 0.047839378794968246, 0.20059844439622065, 0.1280890963779322, 0.10337824075903079, 0.15900151653845418, 0.11170640740574622, 0.13348919598567643, 0.18863566550322997, 0.11125990299434418, 0.13857242478221632, 0.11641958067691072, 0.09668795239017747, 0.0856292380877468, 0.08131557021774793, 0.08239758834326416, 0.08520917386239457, 0.094298173106526, 0.08915668852293479, 0.09426358434588344, 0.16875383470587224, 0.1575670804625956, 0.14569733562049392, 0.1782096202442437, 0.15920807168945084, 0.1480199727143594, 0.13785805659398043, 0.151538734145361, 0.1318950732185804, 0.05994162801517866, 0.043246452110631606, 0.04476825481061786, 0.07564999676870632, 0.030195515703621112, 0.03181029556259307, 0.07188195908300243, 0.07978388366055555, 0.09821035323103211, 0.07890320639747328, 0.12285686199470491, 0.09724170448191582, 0.09912165789224525, 0.1073073857824044, 0.11215115077879623, 0.12634401023695896, 0.09529301371209242, 0.09127286694871728, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.09745751235839051, 0.13184099199998123, 0.0676977311068595, 0.059804092007583076, 0.031083902106156525, 0.03373838956107522, 0.07265460749002883, 0.05872503623489078, 0.06491612282297843, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.02711233320401374, 0.024875437624444552, 0.02602289214376141, 0.042443270783152576, 0.04657919233286778, 0.04651071848003008, 0.023544022366144612, 0.02563406806821844, 0.04367447457111173, 0.20830385025287734, 0.23595869566813055, 0.23437184499343722, 0.21267528752559073, 0.21299134516157592, 0.21471335687626225, 0.21467511530194894, 0.22028352737745616, 0.22276933875806515, 0.05737465315095125, 0.055648211573279927, 0.043913799282971566, 0.06346629529947478, 0.053468158095380525, 0.07686220575587654, 0.06801361371953885, 0.0664136159261276, 0.07385137258345931, 0.1258332809235937, 0.14009894049793115, 0.12416198069126572, 0.11406989294015935, 0.12368121628847517, 0.13678048275495058, 0.12186940582829808, 0.12854239744823337, 0.12980737166856537, 0.18184505305263032, 0.16591145852129252, 0.16265387112473473, 0.17357161830548173, 0.18759466153529314, 0.17887099853327215, 0.1577951520283124, 0.18132813779107793, 0.16838258079604151, 0.12436191564352517, 0.1184814054610237, 0.11709919745240915, 0.1237871795948895, 0.1315322658476662, 0.1412272246701286, 0.11728563352649346, 0.12379235762394725, 0.116636227167773, 0.1792659449550118, 0.19169943022532465, 0.17288281003144035, 0.1731644495355189, 0.18290895218028969, 0.16899192068017543, 0.18486678265500234, 0.17583056266243524, 0.18630965170152647, 0.1585729830126984, 0.15683205736692252, 0.17220722947246525, 0.15951591062613402, 0.15868165138546686, 0.16169693402815666, 0.16186581223679586, 0.162891934685939, 0.16987936095107692, 0.15845672195595162, 0.1529589794194004, 0.15293612429258685, 0.2300195600843764, 0.18488154596505568, 0.1818462745649918, 0.15075886023001062, 0.1731379629328489, 0.16039280020683233, 0.20750306275402208, 0.16715374588522713, 0.1779372502713903, 0.16218974206769, 0.14464014705024653, 0.15731868804285531, 0.2100297471704795, 0.17194110405793728, 0.1418773490554044, 0.17552536916787198, 0.166442317142785, 0.1827363507735248, 0.16447899511807695, 0.1706630495205793, 0.16067208546961176, 0.17756499815882232, 0.1620844392431472, 0.17528401933012883, 0.06335628076446498, 0.06730163584965598, 0.067891742739141, 0.06313342632555963, 0.056316304331931355, 0.0574190404653937, 0.054490919622103284, 0.06305288871054815, 0.07267395543537947]}, "mutation_prompt": null}
{"id": "69a303ac-b16e-41ef-b543-8a92e3db7005", "solution": "import numpy as np\nimport random\n\nclass Probabilistic_Frequency_Modulation_Covariance_Matrix_Adaptation:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.F = 0.5  # initial frequency\n        self.CR = 0.5  # initial crossover rate\n        self.pop_size = 50  # initial population size\n        self.population = np.random.uniform(-5.0, 5.0, (self.pop_size, self.dim))\n        self.best_solution = np.inf\n        self.probability = 0.2  # increased probability of probabilistic exploration\n        self.temperature = 100  # initial temperature for simulated annealing\n        self.covariance_matrix = np.eye(self.dim)  # initial covariance matrix\n        self.adaptive_step_size = 0.1  # initial adaptive step size for covariance matrix adaptation\n        self.step_size_learning_rate = 0.001  # learning rate for adaptive step size\n        self.history = np.zeros((self.budget, self.dim))  # history of best solutions\n        self.adaptive_probability = 0.01  # initial adaptive probability for simulated annealing\n\n    def evaluate(self, x):\n        return self.func(x)\n\n    def func(self, x):\n        # BBOB test suite of 24 noiseless functions\n        # Here we use the first function f1(x) = sum(x_i^2)\n        return np.sum(x**2)\n\n    def frequency_modulation(self, x):\n        # frequency modulation function\n        return self.F * np.sin(2 * np.pi * self.F * x)\n\n    def adaptive_frequency_modulation(self):\n        # adaptive frequency modulation\n        self.F *= 1.1  # increase frequency\n        if random.random() < self.probability:\n            self.F *= 0.9  # decrease frequency with probability\n        return self.F\n\n    def adaptive_probability_modulation(self):\n        # adaptive probability modulation\n        self.probability += self.adaptive_probability\n        if self.probability > 0.5:\n            self.probability = 0.5\n        return self.probability\n\n    def crossover(self, x1, x2):\n        # crossover operation\n        r = np.random.rand(self.dim)\n        return x1 + r * (x2 - x1)\n\n    def selection(self, x1, x2):\n        # selection operation\n        if self.evaluate(x2) < self.evaluate(x1):\n            return x2\n        else:\n            return x1\n\n    def simulated_annealing(self, x):\n        # simulated annealing\n        new_x = x + np.random.normal(0, 1, self.dim)\n        new_x = np.clip(new_x, -5.0, 5.0)\n        if self.evaluate(new_x) < self.evaluate(x):\n            return new_x\n        else:\n            probability = np.exp(-(self.evaluate(new_x) - self.evaluate(x)) / self.temperature)\n            if random.random() < probability:\n                return new_x\n            else:\n                return x\n        self.temperature *= 0.99  # decrease temperature with a higher rate\n        self.probability = self.adaptive_probability_modulation()\n\n    def covariance_matrix_adaptation(self):\n        # covariance matrix adaptation\n        self.covariance_matrix = (1 - self.adaptive_step_size) * self.covariance_matrix + self.adaptive_step_size * np.eye(self.dim)\n        return self.covariance_matrix\n\n    def adaptive_step_size_learning(self):\n        # adaptive step size learning\n        self.adaptive_step_size += self.step_size_learning_rate * (self.evaluate(self.population[0]) - self.evaluate(self.population[-1]))\n        return self.adaptive_step_size\n\n    def history_learning(self):\n        # history learning\n        self.history = np.vstack((self.history, self.population[0]))\n\n    def optimize(self, func):\n        for i in range(self.budget):\n            # evaluate population\n            fitness = [self.evaluate(x) for x in self.population]\n            # get best solution\n            self.best_solution = min(fitness)\n            best_index = fitness.index(self.best_solution)\n            # adaptive frequency modulation\n            self.F = self.adaptive_frequency_modulation()\n            # differential evolution\n            for j in range(self.pop_size):\n                # generate trial vector\n                trial = self.crossover(self.population[j], self.population[random.randint(0, self.pop_size - 1)])\n                # evaluate trial vector\n                trial_fitness = self.evaluate(trial)\n                # selection\n                self.population[j] = self.selection(self.population[j], trial)\n                # update best solution\n                if trial_fitness < self.evaluate(self.population[j]):\n                    self.population[j] = trial\n            # simulated annealing\n            for j in range(self.pop_size):\n                self.population[j] = self.simulated_annealing(self.population[j])\n            # covariance matrix adaptation\n            self.covariance_matrix = self.covariance_matrix_adaptation()\n            # adaptive step size learning\n            self.adaptive_step_size = self.adaptive_step_size_learning()\n            # history learning\n            self.history_learning()\n        return self.best_solution, self.history\n\n    def __call__(self, func):\n        self.func = func\n        return self.optimize(func)\n\n# example usage\nbudget = 1000\ndim = 10\noptimizer = Probabilistic_Frequency_Modulation_Covariance_Matrix_Adaptation(budget, dim)\nbest_solution, history = optimizer(lambda x: np.sum(x**2))\nprint(\"Best solution:\", best_solution)\nprint(\"History:\", history)", "name": "Probabilistic_Frequency_Modulation_Covariance_Matrix_Adaptation", "description": "Probabilistic Frequency Modulation and Covariance Matrix Adaptation with Adaptive Step Size Learning and Simulated Annealing with Adaptive Temperature Cooling Schedule and Adaptive Probability Adjustment", "configspace": "", "generation": 51, "fitness": 0.11016688429603298, "feedback": "The algorithm Probabilistic_Frequency_Modulation_Covariance_Matrix_Adaptation got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.11 with standard deviation 0.07.", "error": "", "parent_id": "62f0f28d-b1f3-482e-a59e-689769ae92f8", "metadata": {"aucs": [0.23254126992869573, 0.23387494331995018, 0.216965413342599, 0.20777528055910843, 0.21674757020031166, 0.19728808596425684, 0.20793097882395206, 0.19341257441734971, 0.1940688299482729, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.06446608776410045, 0.05446151872419558, 0.050703459159309405, 0.07085082004192578, 0.06197724544895955, 0.07205350369898533, 0.07142429162533759, 0.06923104696915783, 0.06909281479944207, 0.04649615702772825, 0.05782740543625764, 0.04730354715111884, 0.06574950085401965, 0.05354783799895435, 0.05041144163807143, 0.04662150148020994, 0.048548921022781855, 0.046475683733578244, 0.15458129357892014, 0.14720683109446075, 0.09417900784999578, 0.22845198123408395, 0.13126984852228618, 0.13719791268400183, 0.15957403233112877, 0.14498583964183542, 0.4182377776764127, 0.09712438513899468, 0.10317697245625457, 0.06467843833342102, 0.08835711058896578, 0.06967561715244397, 0.08211718307236748, 0.12357479051755815, 0.09385140902334987, 0.08873962289532156, 0.16179607846889865, 0.1601719738198435, 0.19151432083766973, 0.16924889738759197, 0.18291442677588798, 0.15998138482615665, 0.15151968876104716, 0.15030355034173204, 0.16814886110303573, 0.069986414434972, 0.05351656156340068, 0.07952406590971972, 0.02206936105771129, 0.0841680644174867, 0.06109357431770479, 0.06602030546803583, 0.11458140653220084, 0.07901928117618351, 0.12622409027755077, 0.09367168081724209, 0.0743108991274174, 0.10893659282340873, 0.09725538626738339, 0.11839779503313985, 0.0990864731763309, 0.09654503456865171, 0.09118790529642917, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.002188038215010568, 0.04945706058727051, 0.048656074818462236, 0.11292686880616076, 0.03494880729959693, 0.03590234285838978, 0.04639994211957632, 0.05525223751880459, 0.08371044853435983, 0.04578965136872415, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.020449670586748514, 0.021630583512094237, 0.025245412394266697, 0.03748418135076137, 0.03978978314552972, 0.04334867885870752, 0.03660981285935927, 0.029128058747867724, 0.03613417682299225, 0.2250258211619186, 0.23211269443369276, 0.22304889619978752, 0.21070204999363362, 0.20637148382089898, 0.21347240682684676, 0.260851502810008, 0.23872073070516775, 0.22557060091195158, 0.05179196390094798, 0.0499846206218546, 0.05112484734176592, 0.07002283434725765, 0.06164467096273185, 0.061328034571905166, 0.07272530342819139, 0.06719203957521036, 0.07049717902948138, 0.12088372410913695, 0.12846731223943386, 0.12796516558269289, 0.11960392825987387, 0.12936449484169066, 0.11655694635467262, 0.12957217499138463, 0.1356573982627598, 0.12817438434403539, 0.17545504549162083, 0.19256230381770145, 0.16262704477298995, 0.18169496601479262, 0.18356784011555882, 0.1863635810449933, 0.1662683955562918, 0.18317191780520115, 0.1587382770117599, 0.12533240535723011, 0.11656079108991158, 0.15408047364725885, 0.1553054848083706, 0.12967769255093897, 0.13067676856589294, 0.12830909412130642, 0.13055545717731853, 0.1057878467482879, 0.19534348215148079, 0.17501180444863462, 0.19905197395437657, 0.18186488459733963, 0.17114222280236824, 0.1760227097815419, 0.19277815403356802, 0.1638696827522943, 0.1762381719982784, 0.15669099990398028, 0.1523210303235757, 0.1618867474803537, 0.15884467234314492, 0.15650943484006452, 0.16230735975943367, 0.1599593939277989, 0.1570443500934665, 0.1535303650777078, 0.15416842130027197, 0.16092478208656358, 0.14534603270067115, 0.18880342789175142, 0.2318731976568249, 0.19163785518962007, 0.17237138362798032, 0.15920658285621825, 0.15844930484842223, 0.20071431614934698, 0.15657112627255176, 0.14799959343993763, 0.202727824507566, 0.14058954329808204, 0.1456443224255336, 0.1254161599335868, 0.1436205468542987, 0.11861955370327104, 0.1812756264796963, 0.15220709349046047, 0.17066100375585425, 0.16906439049235522, 0.1602392672241274, 0.18536691318018084, 0.17546730203618066, 0.17735783462119037, 0.15983982760535131, 0.06027647530238245, 0.06041767940345322, 0.06211248559648597, 0.05509289382738025, 0.06900330918670083, 0.06047182508712978, 0.05685929166582959, 0.07284139255530031, 0.06354869623285253]}, "mutation_prompt": null}
{"id": "a7b7e4b8-148c-4f9c-afd8-c1c83e002263", "solution": "import numpy as np\nimport random\n\nclass Adaptive_Frequency_Modulation_Enhanced_Differential_Evolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.F = 0.5  # initial frequency\n        self.CR = 0.5  # initial crossover rate\n        self.pop_size = 50  # initial population size\n        self.population = np.random.uniform(-5.0, 5.0, (self.pop_size, self.dim))\n        self.best_solution = np.inf\n        self.probability = 0.4  # increased probability of probabilistic exploration\n        self.temperature = 100  # initial temperature for simulated annealing\n        self.covariance_matrix = np.eye(self.dim)  # initial covariance matrix\n        self.adaptive_step_size = 0.05  # initial adaptive step size for covariance matrix adaptation\n        self.step_size_learning_rate = 0.005  # learning rate for adaptive step size\n        self.history = np.zeros((self.budget, self.dim))  # history of best solutions\n        self.exploration_rate = 0.2  # exploration rate for probability-based exploration\n\n    def evaluate(self, x):\n        return self.func(x)\n\n    def func(self, x):\n        # BBOB test suite of 24 noiseless functions\n        # Here we use the first function f1(x) = sum(x_i^2)\n        return np.sum(x**2)\n\n    def frequency_modulation(self, x):\n        # frequency modulation function\n        return self.F * np.sin(2 * np.pi * self.F * x)\n\n    def adaptive_frequency_modulation(self):\n        # adaptive frequency modulation\n        self.F *= 1.1  # increase frequency\n        if random.random() < self.probability:\n            self.F *= 0.9  # decrease frequency with probability\n        return self.F\n\n    def crossover(self, x1, x2):\n        # crossover operation\n        r = np.random.rand(self.dim)\n        return x1 + r * (x2 - x1)\n\n    def selection(self, x1, x2):\n        # selection operation\n        if self.evaluate(x2) < self.evaluate(x1):\n            return x2\n        else:\n            return x1\n\n    def simulated_annealing(self, x):\n        # simulated annealing\n        new_x = x + np.random.normal(0, 1, self.dim)\n        new_x = np.clip(new_x, -5.0, 5.0)\n        if self.evaluate(new_x) < self.evaluate(x):\n            return new_x\n        else:\n            probability = np.exp(-(self.evaluate(new_x) - self.evaluate(x)) / self.temperature)\n            if random.random() < probability:\n                return new_x\n            else:\n                return x\n        self.temperature *= 0.99  # decrease temperature with a higher rate\n\n    def covariance_matrix_adaptation(self):\n        # covariance matrix adaptation\n        self.covariance_matrix = (1 - self.adaptive_step_size) * self.covariance_matrix + self.adaptive_step_size * np.eye(self.dim)\n        return self.covariance_matrix\n\n    def adaptive_step_size_learning(self):\n        # adaptive step size learning\n        self.adaptive_step_size += self.step_size_learning_rate * (self.evaluate(self.population[0]) - self.evaluate(self.population[-1]))\n        return self.adaptive_step_size\n\n    def probability_based_exploration(self):\n        # probability-based exploration\n        if random.random() < self.exploration_rate:\n            self.probability = 0.6  # increase probability of probabilistic exploration\n        else:\n            self.probability = 0.2  # decrease probability of probabilistic exploration\n        return self.probability\n\n    def history_learning(self):\n        # history learning\n        self.history = np.vstack((self.history, self.population[0]))\n\n    def optimize(self, func):\n        for i in range(self.budget):\n            # evaluate population\n            fitness = [self.evaluate(x) for x in self.population]\n            # get best solution\n            self.best_solution = min(fitness)\n            best_index = fitness.index(self.best_solution)\n            # adaptive frequency modulation\n            self.F = self.adaptive_frequency_modulation()\n            # differential evolution\n            for j in range(self.pop_size):\n                # generate trial vector\n                trial = self.crossover(self.population[j], self.population[random.randint(0, self.pop_size - 1)])\n                # evaluate trial vector\n                trial_fitness = self.evaluate(trial)\n                # selection\n                self.population[j] = self.selection(self.population[j], trial)\n                # update best solution\n                if trial_fitness < self.evaluate(self.population[j]):\n                    self.population[j] = trial\n            # simulated annealing\n            for j in range(self.pop_size):\n                self.population[j] = self.simulated_annealing(self.population[j])\n            # covariance matrix adaptation\n            self.covariance_matrix = self.covariance_matrix_adaptation()\n            # adaptive step size learning\n            self.adaptive_step_size = self.adaptive_step_size_learning()\n            # probability-based exploration\n            self.probability = self.probability_based_exploration()\n            # history learning\n            self.history_learning()\n        return self.best_solution, self.history\n\n    def __call__(self, func):\n        self.func = func\n        return self.optimize(func)\n\n# example usage\nbudget = 1000\ndim = 10\noptimizer = Adaptive_Frequency_Modulation_Enhanced_Differential_Evolution(budget, dim)\nbest_solution, history = optimizer(lambda x: np.sum(x**2))\nprint(\"Best solution:\", best_solution)\nprint(\"History:\", history)", "name": "Adaptive_Frequency_Modulation_Enhanced_Differential_Evolution", "description": "Adaptive Frequency Modulation Enhanced Differential Evolution with Covariance Matrix Adaptation, Simulated Annealing, and Adaptive Step Size Learning, Probability-Based Exploration, and History-Based Learning.", "configspace": "", "generation": 52, "fitness": 0.10830658342309617, "feedback": "The algorithm Adaptive_Frequency_Modulation_Enhanced_Differential_Evolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.11 with standard deviation 0.07.", "error": "", "parent_id": "62f0f28d-b1f3-482e-a59e-689769ae92f8", "metadata": {"aucs": [0.23815057022092678, 0.21564506038935172, 0.23307474368619363, 0.20620939084973267, 0.2126831120188606, 0.21435647270350733, 0.21389031491767296, 0.18668394146776146, 0.20226282907085946, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.049685669480814765, 0.056020370349437854, 0.060271605451919785, 0.06018939483181296, 0.061691615079981554, 0.05823786900684047, 0.05457626909542301, 0.05443002691376497, 0.06599412612935807, 0.06497020677386844, 0.05512916325305428, 0.04249612149919779, 0.05827049735613432, 0.06643459716280631, 0.04690845659464771, 0.060769321506521146, 0.06885059024512152, 0.04388171839741484, 0.17627507075971116, 0.12819274953298132, 0.07935682746621064, 0.12656312683234705, 0.12929560504452997, 0.1359871458973705, 0.16201740756390504, 0.10365878802422701, 0.16715642234191785, 0.09690129881084486, 0.11806892694793658, 0.07138924643361211, 0.0950327130328873, 0.06909336077696027, 0.07605773516192471, 0.12808147580403562, 0.08447938499937502, 0.1158211467335657, 0.18771925798303868, 0.18044713895166953, 0.12825111773597853, 0.16734952856087093, 0.1631765918735315, 0.16145651429323538, 0.1752579007823588, 0.14575453675823447, 0.14129681549692252, 0.08076737511614096, 0.046577601014641035, 0.0645828178362724, 0.07641052409505678, 0.07311573992894504, 0.05560784325629198, 0.08659409085601621, 0.09817572374465577, 0.08236107204149357, 0.08931246169350016, 0.09066312580179003, 0.08747575078844638, 0.09780108305435098, 0.09426614451329263, 0.09939279759419861, 0.11192653337643932, 0.08780234098133621, 0.12230153995107607, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.09843226692849316, 0.08484268577418064, 0.08341620556935636, 0.032646729543685415, 0.05285517118984029, 0.0341755310130718, 0.06917880733342086, 0.0676473138933883, 0.053161420686762884, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.023992713959398282, 0.011807593701368413, 0.025309846041329354, 0.022130267919783675, 0.036921056715367384, 0.04606764085383497, 0.024505799244122106, 0.04200649650553134, 0.04637217733322774, 0.2296554906582452, 0.22008584702314382, 0.22640060042568733, 0.2213039078283371, 0.1945522685499642, 0.22363940855576803, 0.23112219099316356, 0.21894637677944795, 0.24451799051821976, 0.04301591874829813, 0.05098014286797825, 0.05267050245978244, 0.06036651224930023, 0.05815146081312017, 0.06401537682567582, 0.052767241624150696, 0.06339817998280284, 0.07335177243656876, 0.1133199709514271, 0.11922808006785912, 0.12483911526463021, 0.1368946554097309, 0.1614692810070235, 0.12877957867876466, 0.13086500653765776, 0.11199524874955191, 0.11313167438361638, 0.1588834946156381, 0.18918439783979846, 0.17853031744603887, 0.19041685066592484, 0.18822937042036625, 0.19059722245007926, 0.15176397548128284, 0.17803336207199516, 0.17422035799264268, 0.1080330517279916, 0.11836590179257378, 0.11691567073299447, 0.12140362305065189, 0.14356030236334194, 0.13350953226019702, 0.1109761614652034, 0.15208852707600595, 0.10583760660144581, 0.18817584819763933, 0.18218190627996722, 0.17380480904928386, 0.176766447991486, 0.17092979178667045, 0.1682038357596135, 0.18253744063160926, 0.1785658077761142, 0.187822935663583, 0.14929917206476384, 0.154608407964684, 0.16647938640221183, 0.15836616604773412, 0.15696815354258964, 0.1568024112432831, 0.1576949179625553, 0.16587435793274485, 0.15190588485486256, 0.1670878237228557, 0.1575214579900157, 0.15125701205505615, 0.15945475086315308, 0.2173583860242535, 0.19761144627743, 0.21788101018954387, 0.15483397210693806, 0.1951963097213233, 0.14968096131059394, 0.15079674829606282, 0.15641451237529658, 0.20238985547262456, 0.14498312860664542, 0.14485306318300895, 0.19361170015402818, 0.1758422929355289, 0.14243506569754794, 0.16170095097056125, 0.17355196661468286, 0.15963789109085325, 0.16405552230978993, 0.17816816350387632, 0.15637800982734684, 0.17700972677494997, 0.1607114207928425, 0.16130704287604547, 0.058167051550916726, 0.06528009699796478, 0.05680272066196701, 0.06160321064803975, 0.05413042867064266, 0.05524303135528186, 0.0680368772401162, 0.06372310976504958, 0.06329791425717579]}, "mutation_prompt": null}
{"id": "2a339ca0-47b1-441e-bb54-2ea9257d7a59", "solution": "import numpy as np\nimport random\n\nclass Enhanced_Adaptive_Differential_Evolution_Probability_1:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.F = 0.5  # initial frequency\n        self.CR = 0.5  # initial crossover rate\n        self.pop_size = 50  # initial population size\n        self.population = np.random.uniform(-5.0, 5.0, (self.pop_size, self.dim))\n        self.best_solution = np.inf\n        self.temperature = 100  # initial temperature for simulated annealing\n        self.covariance_matrix = np.eye(self.dim)  # initial covariance matrix\n        self.adaptive_step_size = 0.1  # initial adaptive step size for covariance matrix adaptation\n        self.step_size_learning_rate = 0.001  # learning rate for adaptive step size\n        self.history = np.zeros((self.budget, self.dim))  # history of best solutions\n\n    def evaluate(self, x):\n        return self.func(x)\n\n    def func(self, x):\n        # BBOB test suite of 24 noiseless functions\n        # Here we use the first function f1(x) = sum(x_i^2)\n        return np.sum(x**2)\n\n    def frequency_modulation(self, x):\n        # frequency modulation function\n        return self.F * np.sin(2 * np.pi * self.F * x)\n\n    def adaptive_frequency_modulation(self):\n        # adaptive frequency modulation\n        self.F *= 1.1  # increase frequency\n        self.F *= 1.1  # increase frequency with probability 1\n        return self.F\n\n    def crossover(self, x1, x2):\n        # crossover operation\n        r = np.random.rand(self.dim)\n        return x1 + r * (x2 - x1)\n\n    def selection(self, x1, x2):\n        # selection operation\n        if self.evaluate(x2) < self.evaluate(x1):\n            return x2\n        else:\n            return x1\n\n    def simulated_annealing(self, x):\n        # simulated annealing\n        new_x = x + np.random.normal(0, 1, self.dim)\n        new_x = np.clip(new_x, -5.0, 5.0)\n        if self.evaluate(new_x) < self.evaluate(x):\n            return new_x\n        else:\n            probability = np.exp(-(self.evaluate(new_x) - self.evaluate(x)) / self.temperature)\n            if random.random() < probability:\n                return new_x\n            else:\n                return x\n        self.temperature *= 0.99  # decrease temperature with a higher rate\n\n    def covariance_matrix_adaptation(self):\n        # covariance matrix adaptation\n        self.covariance_matrix = (1 - self.adaptive_step_size) * self.covariance_matrix + self.adaptive_step_size * np.eye(self.dim)\n        return self.covariance_matrix\n\n    def adaptive_step_size_learning(self):\n        # adaptive step size learning\n        self.adaptive_step_size += self.step_size_learning_rate * (self.evaluate(self.population[0]) - self.evaluate(self.population[-1]))\n        return self.adaptive_step_size\n\n    def history_learning(self):\n        # history learning\n        self.history = np.vstack((self.history, self.population[0]))\n\n    def optimize(self, func):\n        for i in range(self.budget):\n            # evaluate population\n            fitness = [self.evaluate(x) for x in self.population]\n            # get best solution\n            self.best_solution = min(fitness)\n            best_index = fitness.index(self.best_solution)\n            # adaptive frequency modulation\n            self.F = self.adaptive_frequency_modulation()\n            # differential evolution\n            for j in range(self.pop_size):\n                # generate trial vector\n                trial = self.crossover(self.population[j], self.population[random.randint(0, self.pop_size - 1)])\n                # evaluate trial vector\n                trial_fitness = self.evaluate(trial)\n                # selection\n                self.population[j] = self.selection(self.population[j], trial)\n                # update best solution\n                if trial_fitness < self.evaluate(self.population[j]):\n                    self.population[j] = trial\n            # simulated annealing\n            for j in range(self.pop_size):\n                self.population[j] = self.simulated_annealing(self.population[j])\n            # covariance matrix adaptation\n            self.covariance_matrix = self.covariance_matrix_adaptation()\n            # adaptive step size learning\n            self.adaptive_step_size = self.adaptive_step_size_learning()\n            # history learning\n            self.history_learning()\n        return self.best_solution, self.history\n\n    def __call__(self, func):\n        self.func = func\n        return self.optimize(func)\n\n# example usage\nbudget = 1000\ndim = 10\noptimizer = Enhanced_Adaptive_Differential_Evolution_Probability_1(budget, dim)\nbest_solution, history = optimizer(lambda x: np.sum(x**2))\nprint(\"Best solution:\", best_solution)\nprint(\"History:\", history)", "name": "Enhanced_Adaptive_Differential_Evolution_Probability_1", "description": "Enhanced Adaptive Frequency Modulation and Covariance Matrix Adaptation with Simulated Annealing and Adaptive Step Size Learning and History Learning with Probability 1", "configspace": "", "generation": 53, "fitness": 0.11067438978030007, "feedback": "The algorithm Enhanced_Adaptive_Differential_Evolution_Probability_1 got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.11 with standard deviation 0.07.", "error": "", "parent_id": "62f0f28d-b1f3-482e-a59e-689769ae92f8", "metadata": {"aucs": [0.2367032524536492, 0.20865588682299074, 0.23408309364645918, 0.20944546829837307, 0.20483527221806364, 0.2497240637706587, 0.20210029994873202, 0.21345435880837338, 0.20647028088761266, 9.999999999998899e-05, 9.999999999998899e-05, 0.0024284539548798945, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.0007202436854466532, 9.999999999998899e-05, 0.06146066898616975, 0.055693844346582755, 0.054753332649876585, 0.06837939604913568, 0.07031475400979947, 0.07109428275713015, 0.06812136420239001, 0.06649644989238368, 0.05580791926552042, 0.049835935671955656, 0.046338531934727145, 0.051102429220025836, 0.060886447545903555, 0.0508772876385416, 0.047388582121023815, 0.058681126223091296, 0.047407829222091635, 0.04668036330916403, 0.1083742703826629, 0.12778580263732253, 0.10091234020197548, 0.16714755641575085, 0.11979969430288162, 0.1173952817401912, 0.12016878337096581, 0.11263095852932092, 0.4285411441300869, 0.09780376641017374, 0.10832053463932878, 0.09012299552879688, 0.09423493923513215, 0.07974387069224509, 0.10889774232873028, 0.11496796642161822, 0.10802735627329707, 0.07161164354752358, 0.16654165698157608, 0.1542591628572898, 0.14968154557598468, 0.19871334517506856, 0.18033235262445502, 0.18004057550390407, 0.13189822274453378, 0.14579479688978392, 0.17919380142603591, 0.066671444990827, 0.05937760955044302, 0.07320030252224319, 0.06717067430931711, 0.04912686530078969, 0.030931520537349377, 0.08274040210101608, 0.05294739446683283, 0.09435396590743006, 0.1110961278179835, 0.10820132828429141, 0.09662414923922513, 0.10818798031249077, 0.11277670679780927, 0.09310218903949319, 0.10809157884949405, 0.07644183807810279, 0.07816009522227507, 9.999999999998899e-05, 0.0006200499337001286, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.08178288860990468, 0.0622492873642837, 0.08492800342850382, 0.02176808656865492, 0.08641586846485494, 0.018052353058649384, 0.05715471786563908, 0.08115985600479692, 0.10337971254981881, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.011085097651340914, 0.029171845184226886, 0.026016894542662317, 0.03774795431680811, 0.03853249881981591, 0.04144727475267551, 0.0384920565429171, 0.034269970894496904, 0.03834727764628165, 0.2182551495394911, 0.23482387541506067, 0.27815092070606995, 0.23457318772132996, 0.20699607313622004, 0.21940307108517343, 0.24994737213491958, 0.2729176924488119, 0.2654395321432561, 0.04421437901278602, 0.04680773339540867, 0.05739977289583986, 0.0645131601478548, 0.06460984735354991, 0.066237866277152, 0.06237277784814965, 0.06247203352772435, 0.06936121098730019, 0.13368345916875368, 0.1338979307350121, 0.11842175776965747, 0.10383345327043458, 0.11616035082019605, 0.13324016495402935, 0.13080091540807848, 0.11530742494369639, 0.128730710490914, 0.1533345410039023, 0.18421137164057544, 0.1831855167506944, 0.1708249216784672, 0.18726498147500703, 0.19040798226217903, 0.16280138873741357, 0.17603428530980325, 0.17798192601292717, 0.13471636872496295, 0.13719151544022046, 0.11325724065465459, 0.11880904720064045, 0.13896411000767017, 0.13031731094005938, 0.11156284768221081, 0.12175931207153701, 0.11007105776272996, 0.16314267147331774, 0.18812389914492111, 0.1726281424548044, 0.17080271294342797, 0.17741855411474694, 0.18182415370371763, 0.18447312193574905, 0.17710767365178348, 0.1819644270068249, 0.15436536884416474, 0.14932504468366115, 0.15400231887078075, 0.15361188474721588, 0.16319950482839818, 0.1573023838888934, 0.15424725022947583, 0.15209892208069264, 0.15591131009841952, 0.1566396582171128, 0.150198848844587, 0.16694118139134173, 0.23950479092922594, 0.2501537689051665, 0.14344408265431374, 0.1445921872209034, 0.1716885284530707, 0.16777133581154724, 0.25004938871537596, 0.19347941852407613, 0.2033483575325652, 0.19951207482750732, 0.12251050921481821, 0.1559319022445852, 0.14249028240553585, 0.17336539835389264, 0.13941676453002705, 0.19720587539910783, 0.17533465984754315, 0.18740615342734912, 0.1610236818303845, 0.1649154738320816, 0.16036334596099955, 0.16128030686662043, 0.17455020555669376, 0.16623263916121667, 0.05463080674037357, 0.0606598175787707, 0.06200621836575759, 0.06821748029751307, 0.060728249386490796, 0.0541394074774082, 0.059452085705456015, 0.046139403982749516, 0.06641981532429397]}, "mutation_prompt": null}
{"id": "63613e2a-60be-44d0-bc19-1b8547fd6175", "solution": "import numpy as np\nimport random\n\nclass Adaptive_Frequency_Modulation_Enhanced_Differential_Evolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.F = 0.5  # initial frequency\n        self.CR = 0.5  # initial crossover rate\n        self.pop_size = 50  # initial population size\n        self.population = np.random.uniform(-5.0, 5.0, (self.pop_size, self.dim))\n        self.best_solution = np.inf\n        self.probability = 0.3  # increased probability of probabilistic exploration\n        self.temperature = 100  # initial temperature for simulated annealing\n        self.covariance_matrix = np.eye(self.dim)  # initial covariance matrix\n        self.adaptive_step_size = 0.1  # initial adaptive step size for covariance matrix adaptation\n        self.step_size_learning_rate = 0.001  # learning rate for adaptive step size\n        self.history = np.zeros((self.budget, self.dim))  # history of best solutions\n        self.probabilistic_exploration = 0.2  # probability of probabilistic exploration\n\n    def evaluate(self, x):\n        return self.func(x)\n\n    def func(self, x):\n        # BBOB test suite of 24 noiseless functions\n        # Here we use the first function f1(x) = sum(x_i^2)\n        return np.sum(x**2)\n\n    def frequency_modulation(self, x):\n        # frequency modulation function\n        return self.F * np.sin(2 * np.pi * self.F * x)\n\n    def adaptive_frequency_modulation(self):\n        # adaptive frequency modulation\n        self.F *= 1.1  # increase frequency\n        if random.random() < self.probability:\n            self.F *= 0.9  # decrease frequency with probability\n        return self.F\n\n    def crossover(self, x1, x2):\n        # crossover operation\n        r = np.random.rand(self.dim)\n        return x1 + r * (x2 - x1)\n\n    def selection(self, x1, x2):\n        # selection operation\n        if self.evaluate(x2) < self.evaluate(x1):\n            return x2\n        else:\n            return x1\n\n    def simulated_annealing(self, x):\n        # simulated annealing\n        new_x = x + np.random.normal(0, 1, self.dim)\n        new_x = np.clip(new_x, -5.0, 5.0)\n        if self.evaluate(new_x) < self.evaluate(x):\n            return new_x\n        else:\n            probability = np.exp(-(self.evaluate(new_x) - self.evaluate(x)) / self.temperature)\n            if random.random() < probability:\n                return new_x\n            else:\n                return x\n        self.temperature *= 0.99  # decrease temperature with a higher rate\n\n    def covariance_matrix_adaptation(self):\n        # covariance matrix adaptation\n        self.covariance_matrix = (1 - self.adaptive_step_size) * self.covariance_matrix + self.adaptive_step_size * np.eye(self.dim)\n        return self.covariance_matrix\n\n    def adaptive_step_size_learning(self):\n        # adaptive step size learning\n        self.adaptive_step_size += self.step_size_learning_rate * (self.evaluate(self.population[0]) - self.evaluate(self.population[-1]))\n        return self.adaptive_step_size\n\n    def history_learning(self):\n        # history learning\n        self.history = np.vstack((self.history, self.population[0]))\n\n    def probabilistic_exploration(self):\n        # probabilistic exploration\n        if random.random() < self.probabilistic_exploration:\n            self.population = np.random.uniform(-5.0, 5.0, (self.pop_size, self.dim))\n        return self.population\n\n    def optimize(self, func):\n        for i in range(self.budget):\n            # evaluate population\n            fitness = [self.evaluate(x) for x in self.population]\n            # get best solution\n            self.best_solution = min(fitness)\n            best_index = fitness.index(self.best_solution)\n            # adaptive frequency modulation\n            self.F = self.adaptive_frequency_modulation()\n            # differential evolution\n            for j in range(self.pop_size):\n                # generate trial vector\n                trial = self.crossover(self.population[j], self.population[random.randint(0, self.pop_size - 1)])\n                # evaluate trial vector\n                trial_fitness = self.evaluate(trial)\n                # selection\n                self.population[j] = self.selection(self.population[j], trial)\n                # update best solution\n                if trial_fitness < self.evaluate(self.population[j]):\n                    self.population[j] = trial\n            # simulated annealing\n            for j in range(self.pop_size):\n                self.population[j] = self.simulated_annealing(self.population[j])\n            # covariance matrix adaptation\n            self.covariance_matrix = self.covariance_matrix_adaptation()\n            # adaptive step size learning\n            self.adaptive_step_size = self.adaptive_step_size_learning()\n            # history learning\n            self.history_learning()\n            # probabilistic exploration\n            self.population = self.probabilistic_exploration()\n        return self.best_solution, self.history\n\n    def __call__(self, func):\n        self.func = func\n        return self.optimize(func)\n\n# example usage\nbudget = 1000\ndim = 10\noptimizer = Adaptive_Frequency_Modulation_Enhanced_Differential_Evolution(budget, dim)\nbest_solution, history = optimizer(lambda x: np.sum(x**2))\nprint(\"Best solution:\", best_solution)\nprint(\"History:\", history)", "name": "Adaptive_Frequency_Modulation_Enhanced_Differential_Evolution", "description": "Adaptive Frequency Modulation Enhanced Differential Evolution with Simulated Annealing and Covariance Matrix Adaptation, which combines the strengths of adaptive frequency modulation, differential evolution, simulated annealing, and covariance matrix adaptation to refine the search process and improve convergence speed and accuracy.", "configspace": "", "generation": 54, "fitness": -Infinity, "feedback": "An exception occurred: TypeError(\"'float' object is not callable\").", "error": "TypeError(\"'float' object is not callable\")", "parent_id": "62f0f28d-b1f3-482e-a59e-689769ae92f8", "metadata": {}, "mutation_prompt": null}
{"id": "f0a9e688-4526-40b2-bcfb-d7012d15cad3", "solution": "import numpy as np\nimport random\n\nclass Enhanced_Adaptive_Differential_Evolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.F = 0.5  # initial frequency\n        self.CR = 0.5  # initial crossover rate\n        self.pop_size = 50  # initial population size\n        self.population = np.random.uniform(-5.0, 5.0, (self.pop_size, self.dim))\n        self.best_solution = np.inf\n        self.probability = 0.2  # increased probability of probabilistic exploration\n        self.temperature = 100  # initial temperature for simulated annealing\n        self.covariance_matrix = np.eye(self.dim)  # initial covariance matrix\n        self.adaptive_step_size = 0.1  # initial adaptive step size for covariance matrix adaptation\n        self.step_size_learning_rate = 0.001  # learning rate for adaptive step size\n        self.history = np.zeros((self.budget, self.dim))  # history of best solutions\n\n    def evaluate(self, x):\n        return self.func(x)\n\n    def func(self, x):\n        # BBOB test suite of 24 noiseless functions\n        # Here we use the first function f1(x) = sum(x_i^2)\n        return np.sum(x**2)\n\n    def frequency_modulation(self, x):\n        # frequency modulation function\n        return self.F * np.sin(2 * np.pi * self.F * x)\n\n    def adaptive_frequency_modulation(self):\n        # adaptive frequency modulation\n        self.F *= 1.1  # increase frequency\n        if random.random() < self.probability:\n            self.F *= 0.9  # decrease frequency with probability\n        return self.F\n\n    def crossover(self, x1, x2):\n        # crossover operation\n        r = np.random.rand(self.dim)\n        return x1 + r * (x2 - x1)\n\n    def selection(self, x1, x2):\n        # selection operation\n        if self.evaluate(x2) < self.evaluate(x1):\n            return x2\n        else:\n            return x1\n\n    def simulated_annealing(self, x):\n        # simulated annealing\n        new_x = x + np.random.normal(0, 1, self.dim)\n        new_x = np.clip(new_x, -5.0, 5.0)\n        if self.evaluate(new_x) < self.evaluate(x):\n            return new_x\n        else:\n            probability = np.exp(-(self.evaluate(new_x) - self.evaluate(x)) / self.temperature)\n            if random.random() < probability:\n                return new_x\n            else:\n                return x\n        self.temperature *= 0.99  # decrease temperature with a higher rate\n\n    def covariance_matrix_adaptation(self):\n        # covariance matrix adaptation\n        self.covariance_matrix = (1 - self.adaptive_step_size) * self.covariance_matrix + self.adaptive_step_size * np.eye(self.dim)\n        return self.covariance_matrix\n\n    def adaptive_step_size_learning(self):\n        # adaptive step size learning\n        self.adaptive_step_size += self.step_size_learning_rate * (self.evaluate(self.population[0]) - self.evaluate(self.population[-1]))\n        return self.adaptive_step_size\n\n    def history_learning(self):\n        # history learning\n        self.history = np.vstack((self.history, self.population[0]))\n\n    def adaptive_crossover(self, x1, x2):\n        # adaptive crossover operation\n        r = np.random.rand(self.dim)\n        r = np.where(r < self.CR, r, 0.5)  # adaptive crossover rate\n        return x1 + r * (x2 - x1)\n\n    def adaptive_selection(self, x1, x2):\n        # adaptive selection operation\n        if random.random() < 0.5:\n            return x1\n        else:\n            return x2\n\n    def optimize(self, func):\n        for i in range(self.budget):\n            # evaluate population\n            fitness = [self.evaluate(x) for x in self.population]\n            # get best solution\n            self.best_solution = min(fitness)\n            best_index = fitness.index(self.best_solution)\n            # adaptive frequency modulation\n            self.F = self.adaptive_frequency_modulation()\n            # differential evolution\n            for j in range(self.pop_size):\n                # generate trial vector\n                trial = self.adaptive_crossover(self.population[j], self.population[random.randint(0, self.pop_size - 1)])\n                # evaluate trial vector\n                trial_fitness = self.evaluate(trial)\n                # selection\n                self.population[j] = self.adaptive_selection(self.population[j], trial)\n                # update best solution\n                if trial_fitness < self.evaluate(self.population[j]):\n                    self.population[j] = trial\n            # simulated annealing\n            for j in range(self.pop_size):\n                self.population[j] = self.simulated_annealing(self.population[j])\n            # covariance matrix adaptation\n            self.covariance_matrix = self.covariance_matrix_adaptation()\n            # adaptive step size learning\n            self.adaptive_step_size = self.adaptive_step_size_learning()\n            # history learning\n            self.history_learning()\n        return self.best_solution, self.history\n\n    def __call__(self, func):\n        self.func = func\n        return self.optimize(func)\n\n# example usage\nbudget = 1000\ndim = 10\noptimizer = Enhanced_Adaptive_Differential_Evolution(budget, dim)\nbest_solution, history = optimizer(lambda x: np.sum(x**2))\nprint(\"Best solution:\", best_solution)\nprint(\"History:\", history)", "name": "Enhanced_Adaptive_Differential_Evolution", "description": "Enhanced Adaptive Frequency Modulation with Covariance Matrix Adaptation and Simulated Annealing with Adaptive Temperature Cooling Schedule and Adaptive Step Size Learning, with a novel addition of a probabilistic frequency modulation mechanism that adjusts the frequency of the modulation based on a probability of 0.2, allowing the algorithm to explore the search space more effectively.", "configspace": "", "generation": 55, "fitness": 0.10368501006290846, "feedback": "The algorithm Enhanced_Adaptive_Differential_Evolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.10 with standard deviation 0.07.", "error": "", "parent_id": "62f0f28d-b1f3-482e-a59e-689769ae92f8", "metadata": {"aucs": [0.22721845137968566, 0.20836022464681636, 0.22359967297729733, 0.16083759923431662, 0.17772409577706927, 0.192497494981813, 0.16024951985901592, 0.16329179053427711, 0.16040256301412337, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.04314523512283919, 0.06231958435048568, 0.04001562608605436, 0.062408617772198305, 0.06870618274984863, 0.052673087286155384, 0.07156212465503597, 0.06561693054992901, 0.05939476631380891, 0.043200213044419455, 0.04530943813333377, 0.06521901482844328, 0.04681594314096349, 0.04171254602146457, 0.039284494499815414, 0.05572155103299703, 0.05071572113203637, 0.031348560738133746, 0.07786448588217465, 0.09399422566560223, 0.07574350499539051, 0.08669295663440402, 0.07391986327326572, 0.08878969182106344, 0.11218113532298701, 0.07031418867567263, 0.09338795890916918, 0.09961001268146896, 0.07607676732524793, 0.06379101247824215, 0.07238272801664503, 0.06868250870796944, 0.08755579988071072, 0.1007842845497866, 0.10404402633962839, 0.07208948474811461, 0.1546279622145066, 0.16099639928413834, 0.1386380782135439, 0.16747203060566007, 0.17480537310128386, 0.1336382287550716, 0.15174485579363062, 0.11760395223626163, 0.1278538006819243, 0.04752214292180201, 0.05415185295542413, 0.04148479966255236, 0.02600821169268952, 0.03337952617837969, 0.03514598233531707, 0.05281041351309723, 0.08587365933971358, 0.06887723265309564, 0.060500856545361636, 0.09295968281737677, 0.09663926225780706, 0.07626851573120874, 0.1105577365448156, 0.09108010046387083, 0.1328478355514413, 0.12372033562233464, 0.07523964310306386, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.09287796135098214, 0.07942256838924477, 0.12147242573684003, 0.04024470723364992, 0.06033154644444494, 0.05018264536266104, 0.07813755524783739, 0.07689332533117521, 0.09154617990495972, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.01538202043782333, 0.010175456142675121, 0.02232683371104427, 0.022740423222956663, 0.012152741554231739, 0.04118057321032631, 0.024847065850945205, 0.016529638952009762, 0.016725835624404883, 0.21920384675297322, 0.23896535733190594, 0.2351580026681911, 0.24176875053967928, 0.1786861204189656, 0.19645955004000215, 0.2194754226592439, 0.2093123653700858, 0.26345641479143533, 0.03145092835868524, 0.04543216860635124, 0.03799162126725297, 0.053511607692011576, 0.05983625890020228, 0.06736013206301028, 0.08012431647437246, 0.05302039399449421, 0.06582435352095384, 0.13557768829529326, 0.1196571046649928, 0.14137479581426282, 0.1019777787032553, 0.12379898908761788, 0.13162177824555854, 0.16438114658302416, 0.12247164230155672, 0.11696244931708699, 0.14656801195358926, 0.16203139620321105, 0.1613932299901245, 0.1879138448134473, 0.17831484666021458, 0.17259475194724816, 0.14441639707199005, 0.17854998732217642, 0.14135648657922262, 0.10140645289328498, 0.11926319954123177, 0.09406375525493682, 0.1117530091088682, 0.14866963574121095, 0.13314698174197714, 0.0936026485455761, 0.11629214182887027, 0.10802930978256631, 0.18077329724019764, 0.17556858456797508, 0.17206652611927042, 0.17155213615435694, 0.17820708149368225, 0.19429182407644685, 0.1793354067761629, 0.18987992637957007, 0.20257952565938664, 0.16045998366021508, 0.16267806727940792, 0.1590425177840179, 0.16035897892542894, 0.1615882004677195, 0.17222941359386912, 0.163655842833196, 0.15673160636452865, 0.1616381376785524, 0.1640432099934267, 0.16174554584428336, 0.15815826353249862, 0.2064803109078408, 0.20723752305473764, 0.19564702457683558, 0.15692550607440447, 0.2515318642655333, 0.2109414532947066, 0.17678635807842824, 0.30929337853498096, 0.1838284241816771, 0.1294447866764563, 0.11834819726775858, 0.1609029022639128, 0.23070717468755775, 0.1457308480854257, 0.13522407977339312, 0.18020858887807756, 0.16162186756647612, 0.18178995180034807, 0.18343773439034183, 0.1727174392417885, 0.17167591061866083, 0.17252982704485076, 0.1793030145623965, 0.20100967401909697, 0.07294900009992289, 0.06354406415153391, 0.051722753310634806, 0.07859441198765815, 0.06043630188405591, 0.07638474134419682, 0.06428212133833, 0.06165251150934814, 0.05668761999694982]}, "mutation_prompt": null}
{"id": "3a51adba-f946-48d1-9c05-9fee822d94bb", "solution": "import numpy as np\nimport random\n\nclass Enhanced_Adaptive_Differential_Evolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.F = 0.5  # initial frequency\n        self.CR = 0.5  # initial crossover rate\n        self.pop_size = 50  # initial population size\n        self.population = np.random.uniform(-5.0, 5.0, (self.pop_size, self.dim))\n        self.best_solution = np.inf\n        self.probability = 0.2  # increased probability of probabilistic exploration\n        self.temperature = 100  # initial temperature for simulated annealing\n        self.covariance_matrix = np.eye(self.dim)  # initial covariance matrix\n        self.adaptive_step_size = 0.1  # initial adaptive step size for covariance matrix adaptation\n        self.step_size_learning_rate = 0.001  # learning rate for adaptive step size\n        self.history = np.zeros((self.budget, self.dim))  # history of best solutions\n\n    def evaluate(self, x):\n        return self.func(x)\n\n    def func(self, x):\n        # BBOB test suite of 24 noiseless functions\n        # Here we use the first function f1(x) = sum(x_i^2)\n        return np.sum(x**2)\n\n    def frequency_modulation(self, x):\n        # frequency modulation function\n        return self.F * np.sin(2 * np.pi * self.F * x)\n\n    def adaptive_frequency_modulation(self):\n        # adaptive frequency modulation\n        self.F *= 1.1  # increase frequency\n        if random.random() < self.probability:\n            self.F *= 0.9  # decrease frequency with probability\n        return self.F\n\n    def crossover(self, x1, x2):\n        # crossover operation\n        r = np.random.rand(self.dim)\n        return x1 + r * (x2 - x1)\n\n    def selection(self, x1, x2):\n        # selection operation\n        if self.evaluate(x2) < self.evaluate(x1):\n            return x2\n        else:\n            return x1\n\n    def simulated_annealing(self, x):\n        # simulated annealing\n        new_x = x + np.random.normal(0, 1, self.dim)\n        new_x = np.clip(new_x, -5.0, 5.0)\n        if self.evaluate(new_x) < self.evaluate(x):\n            return new_x\n        else:\n            probability = np.exp(-(self.evaluate(new_x) - self.evaluate(x)) / self.temperature)\n            if random.random() < probability:\n                return new_x\n            else:\n                return x\n        self.temperature *= 0.99  # decrease temperature with a higher rate\n\n    def covariance_matrix_adaptation(self):\n        # covariance matrix adaptation\n        self.covariance_matrix = (1 - self.adaptive_step_size) * self.covariance_matrix + self.adaptive_step_size * np.eye(self.dim)\n        return self.covariance_matrix\n\n    def adaptive_step_size_learning(self):\n        # adaptive step size learning\n        self.adaptive_step_size += self.step_size_learning_rate * (self.evaluate(self.population[0]) - self.evaluate(self.population[-1]))\n        return self.adaptive_step_size\n\n    def history_learning(self):\n        # history learning\n        self.history = np.vstack((self.history, self.population[0]))\n\n    def optimize(self, func):\n        for i in range(self.budget):\n            # evaluate population\n            fitness = [self.evaluate(x) for x in self.population]\n            # get best solution\n            self.best_solution = min(fitness)\n            best_index = fitness.index(self.best_solution)\n            # adaptive frequency modulation\n            self.F = self.adaptive_frequency_modulation()\n            # differential evolution\n            for j in range(self.pop_size):\n                # generate trial vector\n                trial = self.crossover(self.population[j], self.population[random.randint(0, self.pop_size - 1)])\n                # evaluate trial vector\n                trial_fitness = self.evaluate(trial)\n                # selection\n                self.population[j] = self.selection(self.population[j], trial)\n                # update best solution\n                if trial_fitness < self.evaluate(self.population[j]):\n                    self.population[j] = trial\n            # simulated annealing\n            for j in range(self.pop_size):\n                self.population[j] = self.simulated_annealing(self.population[j])\n            # covariance matrix adaptation\n            self.covariance_matrix = self.covariance_matrix_adaptation()\n            # adaptive step size learning\n            self.adaptive_step_size = self.adaptive_step_size_learning()\n            # history learning\n            self.history_learning()\n        return self.best_solution, self.history\n\n    def __call__(self, func):\n        self.func = func\n        return self.optimize(func)\n\n# example usage\nbudget = 1000\ndim = 10\noptimizer = Enhanced_Adaptive_Differential_Evolution(budget, dim)\nbest_solution, history = optimizer(lambda x: np.sum(x**2))\nprint(\"Best solution:\", best_solution)\nprint(\"History:\", history)", "name": "Enhanced_Adaptive_Differential_Evolution", "description": "Enhanced Adaptive Differential Evolution with Probabilistic Frequency Modulation, Covariance Matrix Adaptation, and Simulated Annealing with Adaptive Temperature Cooling Schedule and Adaptive Step Size Learning.", "configspace": "", "generation": 26, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "62f0f28d-b1f3-482e-a59e-689769ae92f8", "metadata": {"aucs": [0.23769731172917852, 0.23720457413485296, 0.23398524340821303, 0.20549460010352794, 0.20555116867582168, 0.20130651801061838, 0.2111034242676071, 0.2007516942743227, 0.2116305929248098, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.006392611173892893, 9.999999999998899e-05, 0.059900294776183416, 0.07012785618662798, 0.05419817263252702, 0.0795014722763715, 0.06935901025400615, 0.07305402795587912, 0.07197872740953881, 0.06459557626532919, 0.06996007361196876, 0.053111208791695175, 0.053566490408007095, 0.045790153649368226, 0.08521993086965163, 0.0694819433532079, 0.04350696073483884, 0.060257967750235175, 0.05189063106084635, 0.04649729526248825, 0.14662710504050225, 0.12405344652255246, 0.10395021756261902, 0.5379579998710504, 0.16193852016424048, 0.11791330384339704, 0.18518522693319261, 0.10037714751656313, 0.4283067989696846, 0.09280746828909836, 0.08837625468069787, 0.07937852657802957, 0.07854238359383503, 0.08252944843027832, 0.11767926985163069, 0.1205915686158825, 0.0852113029185213, 0.09530169292310442, 0.17330123385194474, 0.1802252971353444, 0.1525284743121933, 0.1555795762620088, 0.17867323713658245, 0.17229723458815238, 0.16781672295818117, 0.15990674428772478, 0.16337899010881085, 0.08900179420138443, 0.03865389075710812, 0.08160113257791912, 0.03927597651265646, 0.026075257529084794, 0.054926825543406665, 0.09841040259303824, 0.06333280723678636, 0.07418265885714515, 0.09796359396919452, 0.09649198930530256, 0.1100308792533109, 0.10331574585163705, 0.10284233020551181, 0.09918302484193209, 0.10784997913383398, 0.10545554298550341, 0.08743879185238912, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.06531471720729043, 0.06322776317563727, 0.10764868144200157, 0.04234868722028973, 0.06891894424860334, 0.11389557244681159, 0.08902233063245635, 0.04959990216821064, 0.06503624669867603, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.019302053195070723, 0.02569447570168837, 0.02218694043020275, 0.030389077976176515, 0.05011639472944973, 0.026064793258674057, 0.03964242745950375, 0.03523032723265518, 0.04990912829843397, 0.26045759541142244, 0.22792046210319727, 0.24789883372762955, 0.22782461580683844, 0.20323172004362933, 0.21785474159582874, 0.24977361138643406, 0.22257910459292318, 0.23487337912939432, 0.047406160946790954, 0.06913972508922595, 0.04628131465091223, 0.06565901983375388, 0.055748382497863735, 0.0633631736000938, 0.06015180734300485, 0.0755272335023841, 0.06646283240006434, 0.12119223920814659, 0.12470214300559834, 0.14030068848721566, 0.11557895408650731, 0.1328869344681134, 0.12954047545064928, 0.12548124776483527, 0.11082883532147259, 0.11613807783079211, 0.1646706089612735, 0.17402029364751725, 0.169957030806709, 0.17721735438905684, 0.18977506642230046, 0.18552711273919076, 0.16008964310468943, 0.18975185260984473, 0.16153221359268788, 0.11588258721205402, 0.12741740017735936, 0.13192378680608652, 0.1228000957149119, 0.12838293347600316, 0.1273759751841037, 0.10303660034216178, 0.12869446988955147, 0.11553907992403178, 0.1591644210830242, 0.190149261923249, 0.16711753718862765, 0.1942395294204997, 0.2132047975084318, 0.15491365322932638, 0.1620218121660999, 0.16842459346413563, 0.19539101481697096, 0.1574308748762071, 0.15527046019467394, 0.16117056355370862, 0.1507179142739733, 0.16571259637988223, 0.1572215990897441, 0.1537357474934712, 0.15304501824019534, 0.17085057300372697, 0.14647342296828503, 0.16458182587567938, 0.1475369256476301, 0.24951655367433845, 0.1706906960414144, 0.22975424671538114, 0.1468282263746179, 0.18998475289801375, 0.19872740316007598, 0.17967694151965607, 0.2095248766304756, 0.16531236969298302, 0.17673728075527995, 0.1573563880111336, 0.16079461976257647, 0.13630563294661702, 0.18943475494288797, 0.15794340341591473, 0.1694964781221654, 0.17584746439259435, 0.1675693088119269, 0.16675380106618054, 0.16310744584612002, 0.17812108928443993, 0.16876649757309747, 0.16601458261862279, 0.18324856573202208, 0.0679542418940462, 0.06107766964719452, 0.0634038621543137, 0.0589167546534245, 0.06826269804045038, 0.051810492248762996, 0.06457290160761553, 0.0671718679707437, 0.06687823879460175]}, "mutation_prompt": null}
{"id": "99282fa8-64c7-48fc-b51e-8578c400d940", "solution": "import numpy as np\nimport random\n\nclass DAFM_CMA_SA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.F = 0.5  # initial frequency\n        self.CR = 0.5  # initial crossover rate\n        self.pop_size = 50  # initial population size\n        self.population = np.random.uniform(-5.0, 5.0, (self.pop_size, self.dim))\n        self.best_solution = np.inf\n        self.probability = 0.2  # increased probability of probabilistic exploration\n        self.temperature = 100  # initial temperature for simulated annealing\n        self.covariance_matrix = np.eye(self.dim)  # initial covariance matrix\n        self.adaptive_step_size = 0.1  # initial adaptive step size for covariance matrix adaptation\n        self.step_size_learning_rate = 0.001  # learning rate for adaptive step size\n        self.history = np.zeros((self.budget, self.dim))  # history of best solutions\n        self.dynamic_frequency_modulation = False  # flag for dynamic frequency modulation\n\n    def evaluate(self, x):\n        return self.func(x)\n\n    def func(self, x):\n        # BBOB test suite of 24 noiseless functions\n        # Here we use the first function f1(x) = sum(x_i^2)\n        return np.sum(x**2)\n\n    def frequency_modulation(self, x):\n        # frequency modulation function\n        return self.F * np.sin(2 * np.pi * self.F * x)\n\n    def adaptive_frequency_modulation(self):\n        # adaptive frequency modulation\n        self.F *= 1.1  # increase frequency\n        if random.random() < self.probability:\n            self.F *= 0.9  # decrease frequency with probability\n        return self.F\n\n    def dynamic_adaptive_frequency_modulation(self):\n        # dynamic adaptive frequency modulation\n        self.F = np.random.uniform(0.1, 10)  # randomly initialize frequency\n        if self.dynamic_frequency_modulation:\n            self.F *= 1.1  # increase frequency\n            if random.random() < self.probability:\n                self.F *= 0.9  # decrease frequency with probability\n        return self.F\n\n    def crossover(self, x1, x2):\n        # crossover operation\n        r = np.random.rand(self.dim)\n        return x1 + r * (x2 - x1)\n\n    def selection(self, x1, x2):\n        # selection operation\n        if self.evaluate(x2) < self.evaluate(x1):\n            return x2\n        else:\n            return x1\n\n    def simulated_annealing(self, x):\n        # simulated annealing\n        new_x = x + np.random.normal(0, 1, self.dim)\n        new_x = np.clip(new_x, -5.0, 5.0)\n        if self.evaluate(new_x) < self.evaluate(x):\n            return new_x\n        else:\n            probability = np.exp(-(self.evaluate(new_x) - self.evaluate(x)) / self.temperature)\n            if random.random() < probability:\n                return new_x\n            else:\n                return x\n        self.temperature *= 0.99  # decrease temperature with a higher rate\n\n    def covariance_matrix_adaptation(self):\n        # covariance matrix adaptation\n        self.covariance_matrix = (1 - self.adaptive_step_size) * self.covariance_matrix + self.adaptive_step_size * np.eye(self.dim)\n        return self.covariance_matrix\n\n    def adaptive_step_size_learning(self):\n        # adaptive step size learning\n        self.adaptive_step_size += self.step_size_learning_rate * (self.evaluate(self.population[0]) - self.evaluate(self.population[-1]))\n        return self.adaptive_step_size\n\n    def history_learning(self):\n        # history learning\n        self.history = np.vstack((self.history, self.population[0]))\n\n    def optimize(self, func):\n        for i in range(self.budget):\n            # evaluate population\n            fitness = [self.evaluate(x) for x in self.population]\n            # get best solution\n            self.best_solution = min(fitness)\n            best_index = fitness.index(self.best_solution)\n            # dynamic adaptive frequency modulation\n            if i % 10 == 0:\n                self.dynamic_frequency_modulation = True\n            else:\n                self.dynamic_frequency_modulation = False\n            self.F = self.dynamic_adaptive_frequency_modulation()\n            # differential evolution\n            for j in range(self.pop_size):\n                # generate trial vector\n                trial = self.crossover(self.population[j], self.population[random.randint(0, self.pop_size - 1)])\n                # evaluate trial vector\n                trial_fitness = self.evaluate(trial)\n                # selection\n                self.population[j] = self.selection(self.population[j], trial)\n                # update best solution\n                if trial_fitness < self.evaluate(self.population[j]):\n                    self.population[j] = trial\n            # simulated annealing\n            for j in range(self.pop_size):\n                self.population[j] = self.simulated_annealing(self.population[j])\n            # covariance matrix adaptation\n            self.covariance_matrix = self.covariance_matrix_adaptation()\n            # adaptive step size learning\n            self.adaptive_step_size = self.adaptive_step_size_learning()\n            # history learning\n            self.history_learning()\n        return self.best_solution, self.history\n\n    def __call__(self, func):\n        self.func = func\n        return self.optimize(func)\n\n# example usage\nbudget = 1000\ndim = 10\noptimizer = DAFM_CMA_SA(budget, dim)\nbest_solution, history = optimizer(lambda x: np.sum(x**2))\nprint(\"Best solution:\", best_solution)\nprint(\"History:\", history)", "name": "DAFM_CMA_SA", "description": "A novel metaheuristic algorithm called \"Dynamically Adaptive Frequency Modulation with Covariance Matrix Adaptation and Simulated Annealing\" (DAFM-CMA-SA), which combines adaptive frequency modulation with differential evolution, covariance matrix adaptation, and simulated annealing to optimize black box functions.", "configspace": "", "generation": 57, "fitness": 0.11012836149854988, "feedback": "The algorithm DAFM_CMA_SA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.11 with standard deviation 0.07.", "error": "", "parent_id": "62f0f28d-b1f3-482e-a59e-689769ae92f8", "metadata": {"aucs": [0.23832109509243826, 0.2241905876141611, 0.22793260604155685, 0.20930141876145247, 0.22300238093827962, 0.20664993627302708, 0.21698346967909155, 0.22150171460031465, 0.2288002666326, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.024447696740599212, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.056767327601446094, 0.06407069759717654, 0.06074821977450651, 0.07557260768365992, 0.06437053858021713, 0.06726649117907824, 0.062383636528574926, 0.06163327565034382, 0.05755939976353441, 0.05149685763773504, 0.05400146221639601, 0.06020677775495731, 0.06762289027922652, 0.06794966911422129, 0.05706855690236967, 0.05907784210849765, 0.061236296977817295, 0.05227278788996592, 0.08740362122590595, 0.10052305853371435, 0.10463976123598373, 0.11892563640119791, 0.1336820071038941, 0.1205709087865725, 0.1074064891606964, 0.11764929308876049, 0.1372215199635748, 0.10828346565943625, 0.09613094523780963, 0.0593968496420052, 0.0826367682492517, 0.08069785031922871, 0.1159965124145077, 0.14590659684436014, 0.09419425901084288, 0.0883884667980176, 0.15964697301429132, 0.17137965984149173, 0.12969664763031796, 0.21127219319273782, 0.17437543418589452, 0.15482069842804735, 0.1258035528368231, 0.13826419442823112, 0.15694304730251696, 0.11583204390884716, 0.07347545210532969, 0.05551550008931483, 0.045213777851980774, 0.02842115329007211, 0.03393103556074839, 0.09682255370847037, 0.08748394157500661, 0.06136709671607676, 0.11332702234693037, 0.10156960217726829, 0.06571548512012815, 0.10099935192181964, 0.1031611712396503, 0.11597397317252922, 0.11616071494416391, 0.11101784128737813, 0.10175070305724587, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.08271980002830392, 0.08112909794140633, 0.07128848417579414, 0.04468725840882748, 0.0355827025026656, 0.019292629588293142, 0.05395339777872632, 0.13010297709142105, 0.09829226495247712, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.03361710002734053, 0.01680109018931797, 0.01943204426433187, 0.023648735622370798, 0.033226393578745905, 0.03936622543416701, 0.043702275331348184, 0.0395618073596925, 0.03995319277978793, 0.2481136421389991, 0.2423351042672658, 0.23870552656334032, 0.21591917033623875, 0.20927000844788513, 0.23505807448722482, 0.2581368155116279, 0.21954512654781488, 0.23455550813014148, 0.042681209859939395, 0.06256783827967649, 0.05340067893551481, 0.05824907073623942, 0.05064522513498482, 0.08104306393471938, 0.06643879077457937, 0.07069917877795273, 0.07403597877385559, 0.12109905117469677, 0.10464365686929156, 0.11842175776965747, 0.12498392726468077, 0.13875764824631065, 0.1213006339147863, 0.11214003315831944, 0.1314248903857146, 0.11931488343689778, 0.17519516895005216, 0.1749101634739333, 0.18233763796179814, 0.18386482582929176, 0.17734242070773065, 0.18844050243909993, 0.17059559370163002, 0.18029646059471038, 0.19164585720461258, 0.1247594616447888, 0.1054661233848585, 0.1238253186280237, 0.13903657328694385, 0.14443588906742721, 0.12623308035007308, 0.10945883816525415, 0.1196260852459805, 0.12254953831590043, 0.16746937519425542, 0.19744033053672216, 0.22435448849512374, 0.17322551359465688, 0.21624565344662727, 0.18249395549981862, 0.17408948272189806, 0.18608300265867972, 0.17187548598938462, 0.15864006748207216, 0.1625429293568339, 0.15951170656809188, 0.15409150945610728, 0.1609737533632335, 0.17061361863281832, 0.16655472835232976, 0.1593847205670631, 0.1562510472971077, 0.15433342828741425, 0.1561516900643115, 0.23482419215904748, 0.16572597280521872, 0.1590535893463061, 0.22284931869824287, 0.17975852826737304, 0.21073229347763267, 0.12292099144010349, 0.21458432539047445, 0.23061539705228196, 0.16751994961098515, 0.1517539104210902, 0.12885047094853797, 0.15194032021073434, 0.16319425926180486, 0.16902974902431922, 0.13051865736363377, 0.160156233531637, 0.17017371720172436, 0.17139427249630568, 0.1630038435025959, 0.16788313261258614, 0.1859878990737479, 0.1831122177906579, 0.18673796404859677, 0.17365060727440362, 0.05505395426500692, 0.05673846082290601, 0.058900678832256914, 0.08263025319819917, 0.06415078313353928, 0.0748219740523638, 0.06558765566887048, 0.06837374587737799, 0.0686754291738616]}, "mutation_prompt": null}
{"id": "48e9f9fd-45d0-428d-94e9-59fbf7b8b053", "solution": "import numpy as np\nimport random\n\nclass Enhanced_Adaptive_Differential_Evolution_Levy:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.F = 0.5  # initial frequency\n        self.CR = 0.5  # initial crossover rate\n        self.pop_size = 50  # initial population size\n        self.population = np.random.uniform(-5.0, 5.0, (self.pop_size, self.dim))\n        self.best_solution = np.inf\n        self.probability = 0.2  # increased probability of probabilistic exploration\n        self.temperature = 100  # initial temperature for simulated annealing\n        self.covariance_matrix = np.eye(self.dim)  # initial covariance matrix\n        self.adaptive_step_size = 0.1  # initial adaptive step size for covariance matrix adaptation\n        self.step_size_learning_rate = 0.001  # learning rate for adaptive step size\n        self.history = np.zeros((self.budget, self.dim))  # history of best solutions\n        self.levy_flight_alpha = 1.5  # Levy flight parameter\n\n    def evaluate(self, x):\n        return self.func(x)\n\n    def func(self, x):\n        # BBOB test suite of 24 noiseless functions\n        # Here we use the first function f1(x) = sum(x_i^2)\n        return np.sum(x**2)\n\n    def frequency_modulation(self, x):\n        # frequency modulation function\n        return self.F * np.sin(2 * np.pi * self.F * x)\n\n    def adaptive_frequency_modulation(self):\n        # adaptive frequency modulation\n        self.F *= 1.1  # increase frequency\n        if random.random() < self.probability:\n            self.F *= 0.9  # decrease frequency with probability\n        return self.F\n\n    def crossover(self, x1, x2):\n        # crossover operation\n        r = np.random.rand(self.dim)\n        return x1 + r * (x2 - x1)\n\n    def selection(self, x1, x2):\n        # selection operation\n        if self.evaluate(x2) < self.evaluate(x1):\n            return x2\n        else:\n            return x1\n\n    def simulated_annealing(self, x):\n        # simulated annealing\n        new_x = x + np.random.normal(0, 1, self.dim)\n        new_x = np.clip(new_x, -5.0, 5.0)\n        if self.evaluate(new_x) < self.evaluate(x):\n            return new_x\n        else:\n            probability = np.exp(-(self.evaluate(new_x) - self.evaluate(x)) / self.temperature)\n            if random.random() < probability:\n                return new_x\n            else:\n                return x\n        self.temperature *= 0.99  # decrease temperature with a higher rate\n\n    def covariance_matrix_adaptation(self):\n        # covariance matrix adaptation\n        self.covariance_matrix = (1 - self.adaptive_step_size) * self.covariance_matrix + self.adaptive_step_size * np.eye(self.dim)\n        return self.covariance_matrix\n\n    def adaptive_step_size_learning(self):\n        # adaptive step size learning\n        self.adaptive_step_size += self.step_size_learning_rate * (self.evaluate(self.population[0]) - self.evaluate(self.population[-1]))\n        return self.adaptive_step_size\n\n    def history_learning(self):\n        # history learning\n        self.history = np.vstack((self.history, self.population[0]))\n\n    def levy_flight(self, x):\n        # Levy flight function\n        step_size = np.random levy(3, self.levy_flight_alpha, self.dim)\n        return x + step_size\n\n    def optimize(self, func):\n        for i in range(self.budget):\n            # evaluate population\n            fitness = [self.evaluate(x) for x in self.population]\n            # get best solution\n            self.best_solution = min(fitness)\n            best_index = fitness.index(self.best_solution)\n            # adaptive frequency modulation\n            self.F = self.adaptive_frequency_modulation()\n            # differential evolution\n            for j in range(self.pop_size):\n                # generate trial vector\n                trial = self.crossover(self.population[j], self.population[random.randint(0, self.pop_size - 1)])\n                # evaluate trial vector\n                trial_fitness = self.evaluate(trial)\n                # selection\n                self.population[j] = self.selection(self.population[j], trial)\n                # update best solution\n                if trial_fitness < self.evaluate(self.population[j]):\n                    self.population[j] = trial\n            # Levy flight exploration\n            for j in range(self.pop_size):\n                self.population[j] = self.levy_flight(self.population[j])\n            # simulated annealing\n            for j in range(self.pop_size):\n                self.population[j] = self.simulated_annealing(self.population[j])\n            # covariance matrix adaptation\n            self.covariance_matrix = self.covariance_matrix_adaptation()\n            # adaptive step size learning\n            self.adaptive_step_size = self.adaptive_step_size_learning()\n            # history learning\n            self.history_learning()\n        return self.best_solution, self.history\n\n    def __call__(self, func):\n        self.func = func\n        return self.optimize(func)\n\n# example usage\nbudget = 1000\ndim = 10\noptimizer = Enhanced_Adaptive_Differential_Evolution_Levy(budget, dim)\nbest_solution, history = optimizer(lambda x: np.sum(x**2))\nprint(\"Best solution:\", best_solution)\nprint(\"History:\", history)", "name": "Enhanced_Adaptive_Differential_Evolution_Levy", "description": "Enhanced Adaptive Differential Evolution with Probabilistic Frequency Modulation, Covariance Matrix Adaptation, and Simulated Annealing with Adaptive Temperature Cooling Schedule and Adaptive Step Size Learning is refined to use the concept of Levy Flights to explore the search space more effectively.", "configspace": "", "generation": 58, "fitness": -Infinity, "feedback": "An exception occurred: SyntaxError('invalid syntax', ('<string>', 82, 31, '        step_size = np.random levy(3, self.levy_flight_alpha, self.dim)\\n')).", "error": "SyntaxError('invalid syntax', ('<string>', 82, 31, '        step_size = np.random levy(3, self.levy_flight_alpha, self.dim)\\n'))", "parent_id": "62f0f28d-b1f3-482e-a59e-689769ae92f8", "metadata": {}, "mutation_prompt": null}
{"id": "eb33abfe-1628-442c-8759-2028e30c69a4", "solution": "import numpy as np\nimport random\n\nclass Adaptive_Frequency_Modulation_Covariance_Matrix_Adaptation:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.F = 0.5  # initial frequency\n        self.CR = 0.5  # initial crossover rate\n        self.pop_size = 50  # initial population size\n        self.population = np.random.uniform(-5.0, 5.0, (self.pop_size, self.dim))\n        self.best_solution = np.inf\n        self.probability = 0.25  # increased probability of probabilistic exploration\n        self.temperature = 100  # initial temperature for simulated annealing\n        self.covariance_matrix = np.eye(self.dim)  # initial covariance matrix\n        self.adaptive_step_size = 0.05  # initial adaptive step size for covariance matrix adaptation\n        self.step_size_learning_rate = 0.005  # learning rate for adaptive step size\n        self.history = np.zeros((self.budget, self.dim))  # history of best solutions\n\n    def evaluate(self, x):\n        return self.func(x)\n\n    def func(self, x):\n        # BBOB test suite of 24 noiseless functions\n        # Here we use the first function f1(x) = sum(x_i^2)\n        return np.sum(x**2)\n\n    def frequency_modulation(self, x):\n        # frequency modulation function\n        return self.F * np.sin(2 * np.pi * self.F * x)\n\n    def adaptive_frequency_modulation(self):\n        # adaptive frequency modulation\n        self.F *= 1.05  # increase frequency\n        if random.random() < self.probability:\n            self.F *= 0.95  # decrease frequency with probability\n        return self.F\n\n    def crossover(self, x1, x2):\n        # crossover operation\n        r = np.random.rand(self.dim)\n        return x1 + r * (x2 - x1)\n\n    def selection(self, x1, x2):\n        # selection operation\n        if self.evaluate(x2) < self.evaluate(x1):\n            return x2\n        else:\n            return x1\n\n    def simulated_annealing(self, x):\n        # simulated annealing\n        new_x = x + np.random.normal(0, 1, self.dim)\n        new_x = np.clip(new_x, -5.0, 5.0)\n        if self.evaluate(new_x) < self.evaluate(x):\n            return new_x\n        else:\n            probability = np.exp(-(self.evaluate(new_x) - self.evaluate(x)) / self.temperature)\n            if random.random() < probability:\n                return new_x\n            else:\n                return x\n        self.temperature *= 0.995  # decrease temperature with a higher rate\n\n    def covariance_matrix_adaptation(self):\n        # covariance matrix adaptation\n        self.covariance_matrix = (1 - self.adaptive_step_size) * self.covariance_matrix + self.adaptive_step_size * np.eye(self.dim)\n        return self.covariance_matrix\n\n    def adaptive_step_size_learning(self):\n        # adaptive step size learning\n        self.adaptive_step_size += self.step_size_learning_rate * (self.evaluate(self.population[0]) - self.evaluate(self.population[-1]))\n        return self.adaptive_step_size\n\n    def history_learning(self):\n        # history learning\n        self.history = np.vstack((self.history, self.population[0]))\n\n    def optimize(self, func):\n        for i in range(self.budget):\n            # evaluate population\n            fitness = [self.evaluate(x) for x in self.population]\n            # get best solution\n            self.best_solution = min(fitness)\n            best_index = fitness.index(self.best_solution)\n            # adaptive frequency modulation\n            self.F = self.adaptive_frequency_modulation()\n            # differential evolution\n            for j in range(self.pop_size):\n                # generate trial vector\n                trial = self.crossover(self.population[j], self.population[random.randint(0, self.pop_size - 1)])\n                # evaluate trial vector\n                trial_fitness = self.evaluate(trial)\n                # selection\n                self.population[j] = self.selection(self.population[j], trial)\n                # update best solution\n                if trial_fitness < self.evaluate(self.population[j]):\n                    self.population[j] = trial\n            # simulated annealing\n            for j in range(self.pop_size):\n                self.population[j] = self.simulated_annealing(self.population[j])\n            # covariance matrix adaptation\n            self.covariance_matrix = self.covariance_matrix_adaptation()\n            # adaptive step size learning\n            self.adaptive_step_size = self.adaptive_step_size_learning()\n            # history learning\n            self.history_learning()\n        return self.best_solution, self.history\n\n    def __call__(self, func):\n        self.func = func\n        return self.optimize(func)\n\n# example usage\nbudget = 1000\ndim = 10\noptimizer = Adaptive_Frequency_Modulation_Covariance_Matrix_Adaptation(budget, dim)\nbest_solution, history = optimizer(lambda x: np.sum(x**2))\nprint(\"Best solution:\", best_solution)\nprint(\"History:\", history)", "name": "Adaptive_Frequency_Modulation_Covariance_Matrix_Adaptation", "description": "Adaptive Frequency Modulation and Covariance Matrix Adaptation with Enhanced Differential Evolution and Simulated Annealing with Adaptive Temperature Cooling Schedule and Adaptive Step Size Learning.", "configspace": "", "generation": 59, "fitness": 0.10912196131727417, "feedback": "The algorithm Adaptive_Frequency_Modulation_Covariance_Matrix_Adaptation got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.11 with standard deviation 0.07.", "error": "", "parent_id": "62f0f28d-b1f3-482e-a59e-689769ae92f8", "metadata": {"aucs": [0.24770605223450726, 0.25640496196224705, 0.24290027850021412, 0.21613094606448302, 0.19035031804880986, 0.19678148731443623, 0.19651800936877994, 0.20053206837364423, 0.20719472009733486, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.010010872065327248, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.053613952217550276, 0.06045491733582131, 0.06399902826170978, 0.0658286203485774, 0.06131731164310916, 0.07482913827067228, 0.07554188173348397, 0.0590607721005636, 0.05984274602902062, 0.049510301135702095, 0.04455122371057063, 0.04213166996873552, 0.05141253026072223, 0.051903390986621556, 0.060330113984539246, 0.052174519470856495, 0.051248017496274056, 0.04950097318372915, 0.10337024731610678, 0.11106621489675705, 0.09113627171031857, 0.2091266923469357, 0.09830429913358962, 0.13283008153375375, 0.1809352109436817, 0.09946817666536023, 0.13758899953817694, 0.10341448692197353, 0.08771580883103047, 0.07436192320763313, 0.09423377440964653, 0.09658076106190161, 0.07519555195706251, 0.10899419606693317, 0.08063322612126411, 0.07372560227231917, 0.16080277207801952, 0.16996030083189906, 0.145542848838202, 0.17921865501357537, 0.15258268382270412, 0.16534307707150364, 0.14760045856234705, 0.16181653414233266, 0.14725219234723064, 0.067125879517547, 0.06047010860609148, 0.06313508613758512, 0.07260893692236181, 0.028782793614682722, 0.04259564209645195, 0.09860665045600914, 0.06495460385613028, 0.11089201860188225, 0.10565934567112323, 0.08512558656248004, 0.10900813422940614, 0.08887834051729637, 0.10195541130326669, 0.10561876883558585, 0.11347141466311472, 0.10888944693465707, 0.09751127936774795, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.12608148141828912, 0.07966664618523123, 0.07883038299509804, 0.047751827854428996, 0.05517861746543351, 0.044821911681186766, 0.09847301631727567, 0.06250829126264357, 0.06089754149602011, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.00013595033734947748, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.027453616442923212, 0.012921932332455643, 0.023827999978583647, 0.0339715862851836, 0.037914619830726415, 0.046805159280445485, 0.04048920786883137, 0.04195663907368563, 0.04125617384151359, 0.23291714692005716, 0.2810460628131668, 0.23376266128294776, 0.23281178207556674, 0.22062991730290116, 0.21797588050523053, 0.232170232784774, 0.2400787558266222, 0.23542471995470582, 0.03960182400093348, 0.05923674911210741, 0.04835158078890145, 0.06996203052944405, 0.07384867913841786, 0.059522668519717103, 0.055309737249156554, 0.06770735028499053, 0.08308206860538137, 0.12003011498633309, 0.1196789577441899, 0.13008574580669197, 0.12390870069895554, 0.1276810939513211, 0.12410498683953974, 0.11909271300809776, 0.13904067202331172, 0.10737903074909905, 0.16483203026124615, 0.16668648273604703, 0.18639166336412683, 0.17973019653125266, 0.17773642928943012, 0.1821366844675868, 0.15233551017111369, 0.19058710879365104, 0.18324784384546433, 0.11128594027652383, 0.11562197099395433, 0.12495637675274995, 0.12418688961355584, 0.12743865971500312, 0.14010598887639458, 0.11351809157689419, 0.11665534717337267, 0.10937898969247384, 0.1628874189232853, 0.17757395916918906, 0.20010690409090992, 0.17078689675848402, 0.18033962860295638, 0.16602937736023926, 0.18515859339976903, 0.16233970798564956, 0.17921782800357755, 0.1593475963139661, 0.15934539861832742, 0.1651608639150245, 0.17456450329308115, 0.1643063428220758, 0.1648681162202833, 0.15985728171636837, 0.16119298413367056, 0.1588648271474702, 0.17623269385943918, 0.16679433061418003, 0.15704327492988213, 0.17044649303955028, 0.22498230718835155, 0.15827537227825939, 0.18009732078086804, 0.2521239335719129, 0.16086420662433487, 0.1616583041206595, 0.17033953864625961, 0.1454394992899103, 0.12684174258484304, 0.16856739346751004, 0.13668783462373213, 0.17935383014920603, 0.19197520933210255, 0.1309677045188279, 0.16506100810917557, 0.1556499458409697, 0.16236701684409982, 0.17117554032389304, 0.171328759457114, 0.15726781220611485, 0.17639722515477774, 0.16610418515799064, 0.17910189960450773, 0.06977808053569956, 0.07460773436689105, 0.06722427974255041, 0.10203626792495268, 0.0694196770493698, 0.06032068145750258, 0.061499596963681125, 0.05847540309829313, 0.06130132531505039]}, "mutation_prompt": null}
{"id": "628fd00a-7eb7-45cd-81ac-be4bba47d544", "solution": "import numpy as np\nimport random\n\nclass Enhanced_Adaptive_Differential_Evolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.F = 0.5  # initial frequency\n        self.CR = 0.5  # initial crossover rate\n        self.pop_size = 50  # initial population size\n        self.population = np.random.uniform(-5.0, 5.0, (self.pop_size, self.dim))\n        self.best_solution = np.inf\n        self.probability = 1.0  # increased probability of probabilistic exploration\n        self.temperature = 100  # initial temperature for simulated annealing\n        self.covariance_matrix = np.eye(self.dim)  # initial covariance matrix\n        self.adaptive_step_size = 0.1  # initial adaptive step size for covariance matrix adaptation\n        self.step_size_learning_rate = 0.001  # learning rate for adaptive step size\n        self.history = np.zeros((self.budget, self.dim))  # history of best solutions\n\n    def evaluate(self, x):\n        return self.func(x)\n\n    def func(self, x):\n        # BBOB test suite of 24 noiseless functions\n        # Here we use the first function f1(x) = sum(x_i^2)\n        return np.sum(x**2)\n\n    def frequency_modulation(self, x):\n        # frequency modulation function\n        return self.F * np.sin(2 * np.pi * self.F * x)\n\n    def adaptive_frequency_modulation(self):\n        # adaptive frequency modulation\n        self.F *= 1.1  # increase frequency\n        self.F *= 0.9  # decrease frequency with probability\n        return self.F\n\n    def crossover(self, x1, x2):\n        # crossover operation\n        r = np.random.rand(self.dim)\n        return x1 + r * (x2 - x1)\n\n    def selection(self, x1, x2):\n        # selection operation\n        if self.evaluate(x2) < self.evaluate(x1):\n            return x2\n        else:\n            return x1\n\n    def simulated_annealing(self, x):\n        # simulated annealing\n        new_x = x + np.random.normal(0, 1, self.dim)\n        new_x = np.clip(new_x, -5.0, 5.0)\n        if self.evaluate(new_x) < self.evaluate(x):\n            return new_x\n        else:\n            probability = np.exp(-(self.evaluate(new_x) - self.evaluate(x)) / self.temperature)\n            if random.random() < probability:\n                return new_x\n            else:\n                return x\n        self.temperature *= 0.99  # decrease temperature with a higher rate\n\n    def covariance_matrix_adaptation(self):\n        # covariance matrix adaptation\n        self.covariance_matrix = (1 - self.adaptive_step_size) * self.covariance_matrix + self.adaptive_step_size * np.eye(self.dim)\n        return self.covariance_matrix\n\n    def adaptive_step_size_learning(self):\n        # adaptive step size learning\n        self.adaptive_step_size += self.step_size_learning_rate * (self.evaluate(self.population[0]) - self.evaluate(self.population[-1]))\n        return self.adaptive_step_size\n\n    def history_learning(self):\n        # history learning\n        self.history = np.vstack((self.history, self.population[0]))\n\n    def optimize(self, func):\n        for i in range(self.budget):\n            # evaluate population\n            fitness = [self.evaluate(x) for x in self.population]\n            # get best solution\n            self.best_solution = min(fitness)\n            best_index = fitness.index(self.best_solution)\n            # adaptive frequency modulation\n            self.F = self.adaptive_frequency_modulation()\n            # differential evolution\n            for j in range(self.pop_size):\n                # generate trial vector\n                trial = self.crossover(self.population[j], self.population[random.randint(0, self.pop_size - 1)])\n                # evaluate trial vector\n                trial_fitness = self.evaluate(trial)\n                # selection\n                self.population[j] = self.selection(self.population[j], trial)\n                # update best solution\n                if trial_fitness < self.evaluate(self.population[j]):\n                    self.population[j] = trial\n            # simulated annealing\n            for j in range(self.pop_size):\n                self.population[j] = self.simulated_annealing(self.population[j])\n            # covariance matrix adaptation\n            self.covariance_matrix = self.covariance_matrix_adaptation()\n            # adaptive step size learning\n            self.adaptive_step_size = self.adaptive_step_size_learning()\n            # history learning\n            self.history_learning()\n        return self.best_solution, self.history\n\n    def __call__(self, func):\n        self.func = func\n        return self.optimize(func)\n\n# example usage\nbudget = 1000\ndim = 10\noptimizer = Enhanced_Adaptive_Differential_Evolution(budget, dim)\nbest_solution, history = optimizer(lambda x: np.sum(x**2))\nprint(\"Best solution:\", best_solution)\nprint(\"History:\", history)", "name": "Enhanced_Adaptive_Differential_Evolution", "description": "Enhanced Adaptive Frequency Modulation Differential Evolution with Covariance Matrix Adaptation and Simulated Annealing with Adaptive Temperature Cooling Schedule and Adaptive Step Size Learning, but with Probability 1 to Change the Individual Lines of the Selected Solution.", "configspace": "", "generation": 60, "fitness": 0.11108683450469589, "feedback": "The algorithm Enhanced_Adaptive_Differential_Evolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.11 with standard deviation 0.07.", "error": "", "parent_id": "62f0f28d-b1f3-482e-a59e-689769ae92f8", "metadata": {"aucs": [0.2411829193217756, 0.21718480211193703, 0.21761250995127734, 0.1847047679640249, 0.2268779756903656, 0.22375010735484702, 0.21883295770057742, 0.2017149728040547, 0.19707131533693678, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.037941860036565744, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.06666199431125497, 0.059504617047782093, 0.0790399621982637, 0.06123548894024311, 0.06033061488338265, 0.07276497356210387, 0.07786355416557666, 0.0756070138478897, 0.05863487238008491, 0.06095284427420611, 0.050729567932547015, 0.054473300942399105, 0.05838131934659996, 0.049140100523333285, 0.04751991861419724, 0.04960182683996861, 0.05272641442261883, 0.0593787812360832, 0.13803465631034129, 0.11954530310601619, 0.0825422369269121, 0.14787893307358213, 0.11263586833642003, 0.11331548042490269, 0.18719668163874725, 0.11400319151096372, 0.4358409498410084, 0.09283456761118869, 0.1004314809235819, 0.0806186728987417, 0.0994224947194905, 0.07342041862524606, 0.0913246908919757, 0.10886060948781018, 0.08648695485439384, 0.08664791481523082, 0.1454017097528365, 0.18479485718663635, 0.1431251599295601, 0.20579603349280706, 0.1859517087425805, 0.13521969874654738, 0.12515372308500772, 0.1669537862809215, 0.15837460265552505, 0.036749357949188766, 0.031041283143187548, 0.057318355406456245, 0.04725809307894413, 0.10279367956039098, 0.037398631924176695, 0.09615221879889957, 0.07594197859293728, 0.0909756807311407, 0.11241212103082476, 0.11156784701004852, 0.09186230869076639, 0.10381325744722258, 0.11747846076417867, 0.0979902479107343, 0.09701071500478686, 0.11440698655174308, 0.1061665931400958, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.08704672115992307, 0.12516304626407349, 0.06677054839521313, 0.043128304045371824, 0.02887314329289581, 0.041875546406723196, 0.08296552211029573, 0.06163904265603637, 0.07672865869945888, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.021059286851053183, 0.017983267363567923, 0.018997877752991332, 0.026273823243323946, 0.04186247754090289, 0.02730144868705775, 0.03787039876255749, 0.03087168085855152, 0.046118147366012496, 0.22735425604451365, 0.2354055489685748, 0.2564460507031189, 0.21131769112123433, 0.21527766940480364, 0.24296257821695266, 0.2451179234163089, 0.23391523572232142, 0.22339827075215823, 0.04660076546765235, 0.061717919580981606, 0.04900179760606338, 0.04960831248172837, 0.05765061756479428, 0.07057909657190686, 0.06517317969509373, 0.06151597252302776, 0.059677015536765965, 0.12085076894192737, 0.10462749202409505, 0.12560523149878788, 0.11560145109146613, 0.13241413087162168, 0.12814700199385565, 0.1189314107181706, 0.12637609518901705, 0.11598979757400352, 0.15959490372971508, 0.17383028700594927, 0.17044417204973816, 0.1841884224295547, 0.18632995005769482, 0.1818161996915434, 0.15746716829185914, 0.1946268450177303, 0.16107653860278537, 0.11035064026183916, 0.13054093564909286, 0.120654161181039, 0.12957836049769866, 0.13801274254155993, 0.13676052476880396, 0.1116840876401296, 0.128188563192131, 0.11282199387733349, 0.1618425106743121, 0.1678749128660435, 0.18708106339109598, 0.18516843840437935, 0.16840158512893266, 0.1926421540640837, 0.19457257427756713, 0.1713573761841095, 0.17111256485979798, 0.1550429198575748, 0.15702155527831418, 0.1596825091058539, 0.1642710214361245, 0.16192990230906767, 0.16674593410347427, 0.16164983694108903, 0.15972928487857074, 0.15966978798182174, 0.1528674846194994, 0.19090364788270764, 0.15302651513706844, 0.25114076097211757, 0.20809784748479565, 0.24265504856943576, 0.18925495369634904, 0.16846676603275124, 0.19954461887088548, 0.2081664129378431, 0.19696749074052022, 0.21981893347167125, 0.15551932606933838, 0.1627088708652551, 0.1385621221278599, 0.11035336447061561, 0.1385983182052981, 0.1380155212903863, 0.19117926586929157, 0.17925944758936563, 0.16014028249450096, 0.1643842281176474, 0.18030511308565744, 0.16352369674060863, 0.16238500116229004, 0.17710849631260683, 0.18902760122958417, 0.06470604121329215, 0.06924553788298815, 0.06010635626937799, 0.04975076466961159, 0.06629265792507522, 0.06011664548475948, 0.06623698465761385, 0.05969814578578059, 0.07489577576889983]}, "mutation_prompt": null}
{"id": "fea2ad9b-4afc-460f-85d5-87a7f9733791", "solution": "import numpy as np\nimport random\n\nclass Adaptive_Frequency_Modulation_Enhanced_Differential_Evolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.F = 0.5  # initial frequency\n        self.CR = 0.5  # initial crossover rate\n        self.pop_size = 50  # initial population size\n        self.population = np.random.uniform(-5.0, 5.0, (self.pop_size, self.dim))\n        self.best_solution = np.inf\n        self.probability = 0.2  # increased probability of probabilistic exploration\n        self.temperature = 100  # initial temperature for simulated annealing\n        self.covariance_matrix = np.eye(self.dim)  # initial covariance matrix\n        self.adaptive_step_size = 0.1  # initial adaptive step size for covariance matrix adaptation\n        self.step_size_learning_rate = 0.0015  # increased learning rate for adaptive step size\n        self.history = np.zeros((self.budget, self.dim))  # history of best solutions\n\n    def evaluate(self, x):\n        return self.func(x)\n\n    def func(self, x):\n        # BBOB test suite of 24 noiseless functions\n        # Here we use the first function f1(x) = sum(x_i^2)\n        return np.sum(x**2)\n\n    def frequency_modulation(self, x):\n        # frequency modulation function\n        return self.F * np.sin(2 * np.pi * self.F * x)\n\n    def adaptive_frequency_modulation(self):\n        # adaptive frequency modulation\n        self.F *= 1.1  # increase frequency\n        if random.random() < self.probability:\n            self.F *= 0.9  # decrease frequency with probability\n        return self.F\n\n    def crossover(self, x1, x2):\n        # crossover operation\n        r = np.random.rand(self.dim)\n        return x1 + r * (x2 - x1)\n\n    def selection(self, x1, x2):\n        # selection operation\n        if self.evaluate(x2) < self.evaluate(x1):\n            return x2\n        else:\n            return x1\n\n    def simulated_annealing(self, x):\n        # simulated annealing\n        new_x = x + np.random.normal(0, 1, self.dim)\n        new_x = np.clip(new_x, -5.0, 5.0)\n        if self.evaluate(new_x) < self.evaluate(x):\n            return new_x\n        else:\n            probability = np.exp(-(self.evaluate(new_x) - self.evaluate(x)) / self.temperature)\n            if random.random() < probability:\n                return new_x\n            else:\n                return x\n        self.temperature *= 0.99  # decrease temperature with a higher rate\n\n    def covariance_matrix_adaptation(self):\n        # covariance matrix adaptation\n        self.covariance_matrix = (1 - self.adaptive_step_size) * self.covariance_matrix + self.adaptive_step_size * np.eye(self.dim)\n        return self.covariance_matrix\n\n    def adaptive_step_size_learning(self):\n        # adaptive step size learning\n        self.adaptive_step_size += self.step_size_learning_rate * (self.evaluate(self.population[0]) - self.evaluate(self.population[-1]))\n        return self.adaptive_step_size\n\n    def history_learning(self):\n        # history learning\n        self.history = np.vstack((self.history, self.population[0]))\n\n    def optimize(self, func):\n        for i in range(self.budget):\n            # evaluate population\n            fitness = [self.evaluate(x) for x in self.population]\n            # get best solution\n            self.best_solution = min(fitness)\n            best_index = fitness.index(self.best_solution)\n            # adaptive frequency modulation\n            self.F = self.adaptive_frequency_modulation()\n            # differential evolution\n            for j in range(self.pop_size):\n                # generate trial vector\n                trial = self.crossover(self.population[j], self.population[random.randint(0, self.pop_size - 1)])\n                # evaluate trial vector\n                trial_fitness = self.evaluate(trial)\n                # selection\n                self.population[j] = self.selection(self.population[j], trial)\n                # update best solution\n                if trial_fitness < self.evaluate(self.population[j]):\n                    self.population[j] = trial\n            # simulated annealing\n            for j in range(self.pop_size):\n                self.population[j] = self.simulated_annealing(self.population[j])\n            # covariance matrix adaptation\n            self.covariance_matrix = self.covariance_matrix_adaptation()\n            # adaptive step size learning\n            self.adaptive_step_size = self.adaptive_step_size_learning()\n            # history learning\n            self.history_learning()\n        return self.best_solution, self.history\n\n    def __call__(self, func):\n        self.func = func\n        return self.optimize(func)\n\n# example usage\nbudget = 1000\ndim = 10\noptimizer = Adaptive_Frequency_Modulation_Enhanced_Differential_Evolution(budget, dim)\nbest_solution, history = optimizer(lambda x: np.sum(x**2))\nprint(\"Best solution:\", best_solution)\nprint(\"History:\", history)", "name": "Adaptive_Frequency_Modulation_Enhanced_Differential_Evolution", "description": "Adaptive Frequency Modulation Enhanced Differential Evolution with Covariance Matrix Adaptation and Simulated Annealing with Adaptive Temperature Cooling Schedule and Adaptive Step Size Learning, where the frequency modulation is changed with a 20% probability and the adaptive step size learning rate is increased by 50%.", "configspace": "", "generation": 61, "fitness": 0.11165527401097416, "feedback": "The algorithm Adaptive_Frequency_Modulation_Enhanced_Differential_Evolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.11 with standard deviation 0.08.", "error": "", "parent_id": "62f0f28d-b1f3-482e-a59e-689769ae92f8", "metadata": {"aucs": [0.22980928305980453, 0.2718149632429321, 0.24267388701084724, 0.20264793070538567, 0.21043283915012045, 0.21168302797189176, 0.20606565367316143, 0.18483205530530544, 0.22671050025090433, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.05970058601694328, 0.06279144686272653, 0.05899509203081055, 0.05875871158600021, 0.06030517100019828, 0.07561774946470634, 0.06797649418907181, 0.05801646193132348, 0.060194339475824976, 0.04922163623965137, 0.053135119712213186, 0.04180558636561671, 0.05809430454154174, 0.0435996591450003, 0.0668308382772006, 0.045725348640030083, 0.05788482909038162, 0.05351259242397155, 0.12021099234426291, 0.12686150051620537, 0.08996968532791894, 0.17076173583781384, 0.5174381196051783, 0.1316667754430717, 0.19893486206366162, 0.11896983822514196, 0.419366735928256, 0.10440712307680222, 0.0955624662160659, 0.1332001909100894, 0.07974225452586325, 0.07865012631899049, 0.0782180537728232, 0.11965628634834469, 0.08633950505003962, 0.14253859793471324, 0.16832103062787152, 0.1561631147947552, 0.14300971291071785, 0.1818075364208257, 0.18309373597291656, 0.1708919615981418, 0.16491461501705018, 0.14440850723346865, 0.1650691979599025, 0.08095870867137478, 0.04140784457759705, 0.0763596311439424, 0.04539614411550319, 0.03586217765364086, 0.07186315110174035, 0.08756700254123839, 0.07678365903452067, 0.08011907111198502, 0.04873394186612845, 0.09732427435453317, 0.11651460289280002, 0.11673005069972187, 0.09947408401531321, 0.08578178760299959, 0.09709743078090705, 0.10713920327223792, 0.09731829238283174, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.07585971755347742, 0.06716649873158165, 0.07923763053671351, 0.044432973197609305, 0.04795887286941036, 0.046130713445610816, 0.05180078952579259, 0.07387038753345843, 0.04832323882154155, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.025402302823774647, 0.026982792955362656, 0.01683333749545124, 0.03891625811485078, 0.041165655191849115, 0.0364906704355622, 0.040555566546364163, 0.051511542310853975, 0.03967060583015736, 0.26380893571126574, 0.23529230838191117, 0.226352576739411, 0.22326063400127838, 0.21457148504035117, 0.21945261628244328, 0.21915229896377897, 0.21904118053678578, 0.2249475207419821, 0.052984149002632486, 0.06510724503205734, 0.04150295922414371, 0.061672222464206006, 0.06328592935881572, 0.07300101573936257, 0.07393643004568329, 0.05657266088508017, 0.06150928424626767, 0.12760712705323018, 0.15862160338181053, 0.12379840896715288, 0.13329139712914462, 0.12782160231491435, 0.12161287559716694, 0.12211787083534786, 0.10018775943965408, 0.10570957161092431, 0.1637984633605316, 0.1679840953347752, 0.17695913323087786, 0.17560158262555092, 0.184862206242012, 0.18795014933397114, 0.15452806641499706, 0.1795998105677974, 0.16454666393728312, 0.1154608990185354, 0.12977078453737523, 0.11199457654069755, 0.12826225426495608, 0.13036541415554348, 0.12738067240301132, 0.12280858330669775, 0.11769577318324431, 0.13004909953775945, 0.1675714364829055, 0.18271072241008146, 0.17727976288173808, 0.17110389383212332, 0.1871241629953584, 0.1809853894119804, 0.19327950730249455, 0.16704607905672542, 0.20677599220401777, 0.1562002262474207, 0.15195872887111128, 0.16128901851046917, 0.163528378024552, 0.17139982382272412, 0.16150975693182135, 0.1574578753106387, 0.16140520505592582, 0.16175575516459262, 0.15339108616155295, 0.15033790031255334, 0.14953191450298853, 0.28154352232911883, 0.18201464384076627, 0.15867927426936024, 0.17065054912074717, 0.14734513899676316, 0.15714670761233918, 0.17284232776455866, 0.163869690262609, 0.156622057126943, 0.1502238054349586, 0.17330179150070213, 0.13461210913639465, 0.10928560514013574, 0.14878952745865026, 0.1539040985236514, 0.17413391621859187, 0.17155686682961746, 0.186914856683275, 0.1651294131257961, 0.16988075661812363, 0.18104651705733998, 0.1713583685985801, 0.16874790857956645, 0.1538620968371035, 0.05058389033985622, 0.07068952000973472, 0.05909570238418693, 0.06099485230106638, 0.0797469352385013, 0.06593972789597924, 0.06354937145955086, 0.052368220817722966, 0.06517792355903906]}, "mutation_prompt": null}
{"id": "3a508c4c-c990-4f54-806a-b715857d6f2c", "solution": "import numpy as np\nimport random\n\nclass Enhanced_Adaptive_Differential_Evolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.F = 0.5  # initial frequency\n        self.CR = 0.5  # initial crossover rate\n        self.pop_size = 50  # initial population size\n        self.population = np.random.uniform(-5.0, 5.0, (self.pop_size, self.dim))\n        self.best_solution = np.inf\n        self.probability = 0.3  # increased probability of probabilistic exploration\n        self.temperature = 100  # initial temperature for simulated annealing\n        self.covariance_matrix = np.eye(self.dim)  # initial covariance matrix\n        self.adaptive_step_size = 0.1  # initial adaptive step size for covariance matrix adaptation\n        self.step_size_learning_rate = 0.001  # learning rate for adaptive step size\n        self.history = np.zeros((self.budget, self.dim))  # history of best solutions\n\n    def evaluate(self, x):\n        return self.func(x)\n\n    def func(self, x):\n        # BBOB test suite of 24 noiseless functions\n        # Here we use the first function f1(x) = sum(x_i^2)\n        return np.sum(x**2)\n\n    def frequency_modulation(self, x):\n        # frequency modulation function\n        return self.F * np.sin(2 * np.pi * self.F * x)\n\n    def adaptive_frequency_modulation(self):\n        # adaptive frequency modulation\n        self.F *= 1.1  # increase frequency\n        if random.random() < self.probability:\n            self.F *= 0.9  # decrease frequency with probability\n        return self.F\n\n    def crossover(self, x1, x2):\n        # crossover operation\n        r = np.random.rand(self.dim)\n        return x1 + r * (x2 - x1)\n\n    def selection(self, x1, x2):\n        # selection operation\n        if self.evaluate(x2) < self.evaluate(x1):\n            return x2\n        else:\n            return x1\n\n    def simulated_annealing(self, x):\n        # simulated annealing\n        new_x = x + np.random.normal(0, 1, self.dim)\n        new_x = np.clip(new_x, -5.0, 5.0)\n        if self.evaluate(new_x) < self.evaluate(x):\n            return new_x\n        else:\n            probability = np.exp(-(self.evaluate(new_x) - self.evaluate(x)) / self.temperature)\n            if random.random() < probability:\n                return new_x\n            else:\n                return x\n        self.temperature *= 0.99  # decrease temperature with a higher rate\n\n    def covariance_matrix_adaptation(self):\n        # covariance matrix adaptation\n        self.covariance_matrix = (1 - self.adaptive_step_size) * self.covariance_matrix + self.adaptive_step_size * np.eye(self.dim)\n        return self.covariance_matrix\n\n    def adaptive_step_size_learning(self):\n        # adaptive step size learning\n        self.adaptive_step_size += self.step_size_learning_rate * (self.evaluate(self.population[0]) - self.evaluate(self.population[-1]))\n        return self.adaptive_step_size\n\n    def history_learning(self):\n        # history learning\n        self.history = np.vstack((self.history, self.population[0]))\n\n    def optimize(self, func):\n        for i in range(self.budget):\n            # evaluate population\n            fitness = [self.evaluate(x) for x in self.population]\n            # get best solution\n            self.best_solution = min(fitness)\n            best_index = fitness.index(self.best_solution)\n            # adaptive frequency modulation\n            self.F = self.adaptive_frequency_modulation()\n            # differential evolution\n            for j in range(self.pop_size):\n                # generate trial vector\n                trial = self.crossover(self.population[j], self.population[random.randint(0, self.pop_size - 1)])\n                # evaluate trial vector\n                trial_fitness = self.evaluate(trial)\n                # selection\n                self.population[j] = self.selection(self.population[j], trial)\n                # update best solution\n                if trial_fitness < self.evaluate(self.population[j]):\n                    self.population[j] = trial\n            # simulated annealing\n            for j in range(self.pop_size):\n                self.population[j] = self.simulated_annealing(self.population[j])\n            # covariance matrix adaptation\n            self.covariance_matrix = self.covariance_matrix_adaptation()\n            # adaptive step size learning\n            self.adaptive_step_size = self.adaptive_step_size_learning()\n            # history learning\n            self.history_learning()\n        return self.best_solution, self.history\n\n    def __call__(self, func):\n        self.func = func\n        return self.optimize(func)\n\n# example usage\nbudget = 1000\ndim = 10\noptimizer = Enhanced_Adaptive_Differential_Evolution(budget, dim)\nbest_solution, history = optimizer(lambda x: np.sum(x**2))\nprint(\"Best solution:\", best_solution)\nprint(\"History:\", history)", "name": "Enhanced_Adaptive_Differential_Evolution", "description": "Enhanced Adaptive Frequency Modulation with Covariance Matrix Adaptation and Simulated Annealing with Adaptive Temperature Cooling Schedule and Adaptive Step Size Learning, with a probability of 0.3 to increase the frequency and a probability of 0.2 to decrease the frequency, and a probability of 0.4 to accept a new solution in simulated annealing.", "configspace": "", "generation": 62, "fitness": 0.11066998709786978, "feedback": "The algorithm Enhanced_Adaptive_Differential_Evolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.11 with standard deviation 0.07.", "error": "", "parent_id": "62f0f28d-b1f3-482e-a59e-689769ae92f8", "metadata": {"aucs": [0.24326436759474368, 0.26001462541651366, 0.21928957726621923, 0.217498726762286, 0.20756702984871533, 0.23818168588007382, 0.21078373830115626, 0.2068876500289627, 0.18916323346519193, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.00014352438625120367, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.07643112825717313, 0.07320430008328227, 0.06916765203173725, 0.06735348053220092, 0.06131862689293466, 0.07136272271045274, 0.061979752528044996, 0.07945800864304109, 0.06731858983092043, 0.04400164577380494, 0.05810785366615423, 0.049585992704243465, 0.05090356813530139, 0.05773335014760417, 0.04516067283719549, 0.04601539209591887, 0.06102880097902563, 0.059897052358767855, 0.10716454545132514, 0.14877044286848828, 0.09648288501319913, 0.12763119652799726, 0.13377516990369964, 0.11963443014196329, 0.17292591445627625, 0.11902272124099378, 0.3585844731308552, 0.11084655981846336, 0.10454333346824118, 0.08589303589636055, 0.07131895162332802, 0.08499545621425786, 0.08892220410628271, 0.10382352245846327, 0.07902479751083069, 0.08306466653798183, 0.18758352523949973, 0.15942189815264574, 0.14379250810826816, 0.16433813562718458, 0.15977453330782632, 0.1785279230552984, 0.14581820228176168, 0.15093470767255335, 0.15289396250791032, 0.08521232029945092, 0.06893742205949605, 0.09060041750470971, 0.02403338121663856, 0.05577644460429565, 0.059908392164484736, 0.09597371207976435, 0.06310406455340889, 0.07407629667699733, 0.08260712323250718, 0.08214734588342076, 0.10851053589818604, 0.10529251166217712, 0.11428028715664129, 0.10009299653907344, 0.10338673061599712, 0.09963787476123276, 0.12343392925489893, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.08205399067594077, 0.08699653310566213, 0.06810469324631163, 0.030445171269614213, 0.03763998374522304, 0.027431989683061486, 0.0682624617070885, 0.09124046649358986, 0.057190913938146015, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.025601066914351267, 0.027487019901777376, 0.028204954594426845, 0.02849543080272343, 0.03673113992786725, 0.04148166560807187, 0.03239486754627374, 0.03430016471566544, 0.028499131755020946, 0.2338302194647952, 0.2334375416474158, 0.2228567496667464, 0.20440239601425336, 0.2438142909885832, 0.22714879290647494, 0.23313565954237503, 0.23480498638572478, 0.2639863984108026, 0.04834442549322482, 0.05241437310783492, 0.05725241182625629, 0.07310175223255433, 0.05835574070205207, 0.06620731127246726, 0.05529960747077567, 0.08389732761932156, 0.057968824239644, 0.12215310222141329, 0.11859371125996698, 0.14362822495308158, 0.10755426706726934, 0.12082579262857329, 0.15344276750840946, 0.12785149831172948, 0.10766851329623472, 0.11686366875785292, 0.17885135472070524, 0.16645975318063433, 0.18627974346666554, 0.17765358219213612, 0.19040281378619095, 0.1866337167218618, 0.1705890384883234, 0.1860144686003783, 0.16645406395238438, 0.10719915899810606, 0.12543043332796044, 0.1142953648583247, 0.12780918095788052, 0.1198893892208136, 0.12710598733253387, 0.10530502966083877, 0.12758888940148716, 0.1192221288568619, 0.17436243641566995, 0.15622828560465718, 0.17718936316402667, 0.1772350060555764, 0.1673740285174481, 0.1774066714967818, 0.17044607969299863, 0.18895560798191635, 0.16888726129904152, 0.1575972417878223, 0.17013314857096118, 0.16040110962324305, 0.16922692985045928, 0.14947076400867276, 0.1620115351033371, 0.1714921907148098, 0.16018945507839166, 0.15740116451791053, 0.15102737738793748, 0.16484915630798658, 0.14993250720228024, 0.25309383720750955, 0.22459394743432926, 0.15360260767069633, 0.22917020356379203, 0.18682593783552703, 0.12904441048691784, 0.2849454446085168, 0.17021066210516278, 0.15317252154854177, 0.20849285397882955, 0.1691243471332562, 0.1547484787788801, 0.16901429470874918, 0.17127866795625557, 0.12045220835363657, 0.18077058041952754, 0.18146539657908756, 0.1706752864079223, 0.16447201018513513, 0.1626982820531726, 0.16116371045307387, 0.16213281343966168, 0.1661986479320201, 0.18170188777644114, 0.06068977144713261, 0.057426849978975736, 0.06268857374120385, 0.06887327658435838, 0.05889943000200182, 0.05704846370176442, 0.06369025831961805, 0.059855261255200465, 0.06814863311705499]}, "mutation_prompt": null}
{"id": "743da4ec-e9fe-4593-8532-d7c8142a898e", "solution": "import numpy as np\nimport random\n\nclass Adaptive_Frequency_Modulation_Enhanced_Adaptive_Differential_Evolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.F = 0.5  # initial frequency\n        self.CR = 0.5  # initial crossover rate\n        self.pop_size = 50  # initial population size\n        self.population = np.random.uniform(-5.0, 5.0, (self.pop_size, self.dim))\n        self.best_solution = np.inf\n        self.probability = 0.3  # increased probability of probabilistic exploration\n        self.temperature = 100  # initial temperature for simulated annealing\n        self.covariance_matrix = np.eye(self.dim)  # initial covariance matrix\n        self.adaptive_step_size = 0.1  # initial adaptive step size for covariance matrix adaptation\n        self.step_size_learning_rate = 0.001  # learning rate for adaptive step size\n        self.history = np.zeros((self.budget, self.dim))  # history of best solutions\n        self.probability_decrease_rate = 0.001  # decrease probability of probabilistic exploration\n\n    def evaluate(self, x):\n        return self.func(x)\n\n    def func(self, x):\n        # BBOB test suite of 24 noiseless functions\n        # Here we use the first function f1(x) = sum(x_i^2)\n        return np.sum(x**2)\n\n    def frequency_modulation(self, x):\n        # frequency modulation function\n        return self.F * np.sin(2 * np.pi * self.F * x)\n\n    def adaptive_frequency_modulation(self):\n        # adaptive frequency modulation\n        self.F *= 1.1  # increase frequency\n        if random.random() < self.probability:\n            self.F *= 0.9  # decrease frequency with probability\n            self.probability -= self.probability_decrease_rate  # decrease probability of probabilistic exploration\n            if self.probability < 0.1:\n                self.probability = 0.1  # minimum probability\n        return self.F\n\n    def crossover(self, x1, x2):\n        # crossover operation\n        r = np.random.rand(self.dim)\n        return x1 + r * (x2 - x1)\n\n    def selection(self, x1, x2):\n        # selection operation\n        if self.evaluate(x2) < self.evaluate(x1):\n            return x2\n        else:\n            return x1\n\n    def simulated_annealing(self, x):\n        # simulated annealing\n        new_x = x + np.random.normal(0, 1, self.dim)\n        new_x = np.clip(new_x, -5.0, 5.0)\n        if self.evaluate(new_x) < self.evaluate(x):\n            return new_x\n        else:\n            probability = np.exp(-(self.evaluate(new_x) - self.evaluate(x)) / self.temperature)\n            if random.random() < probability:\n                return new_x\n            else:\n                return x\n        self.temperature *= 0.99  # decrease temperature with a higher rate\n\n    def covariance_matrix_adaptation(self):\n        # covariance matrix adaptation\n        self.covariance_matrix = (1 - self.adaptive_step_size) * self.covariance_matrix + self.adaptive_step_size * np.eye(self.dim)\n        return self.covariance_matrix\n\n    def adaptive_step_size_learning(self):\n        # adaptive step size learning\n        self.adaptive_step_size += self.step_size_learning_rate * (self.evaluate(self.population[0]) - self.evaluate(self.population[-1]))\n        return self.adaptive_step_size\n\n    def history_learning(self):\n        # history learning\n        self.history = np.vstack((self.history, self.population[0]))\n\n    def optimize(self, func):\n        for i in range(self.budget):\n            # evaluate population\n            fitness = [self.evaluate(x) for x in self.population]\n            # get best solution\n            self.best_solution = min(fitness)\n            best_index = fitness.index(self.best_solution)\n            # adaptive frequency modulation\n            self.F = self.adaptive_frequency_modulation()\n            # differential evolution\n            for j in range(self.pop_size):\n                # generate trial vector\n                trial = self.crossover(self.population[j], self.population[random.randint(0, self.pop_size - 1)])\n                # evaluate trial vector\n                trial_fitness = self.evaluate(trial)\n                # selection\n                self.population[j] = self.selection(self.population[j], trial)\n                # update best solution\n                if trial_fitness < self.evaluate(self.population[j]):\n                    self.population[j] = trial\n            # simulated annealing\n            for j in range(self.pop_size):\n                self.population[j] = self.simulated_annealing(self.population[j])\n            # covariance matrix adaptation\n            self.covariance_matrix = self.covariance_matrix_adaptation()\n            # adaptive step size learning\n            self.adaptive_step_size = self.adaptive_step_size_learning()\n            # history learning\n            self.history_learning()\n        return self.best_solution, self.history\n\n    def __call__(self, func):\n        self.func = func\n        return self.optimize(func)\n\n# example usage\nbudget = 1000\ndim = 10\noptimizer = Adaptive_Frequency_Modulation_Enhanced_Adaptive_Differential_Evolution(budget, dim)\nbest_solution, history = optimizer(lambda x: np.sum(x**2))\nprint(\"Best solution:\", best_solution)\nprint(\"History:\", history)", "name": "Adaptive_Frequency_Modulation_Enhanced_Adaptive_Differential_Evolution", "description": "Adaptive Frequency Modulation Enhanced Adaptive Differential Evolution with Probabilistic Exploration and Covariance Matrix Adaptation, and Simulated Annealing with Adaptive Temperature Cooling Schedule and Adaptive Step Size Learning with History Learning and Probabilistic Frequency Modulation.", "configspace": "", "generation": 63, "fitness": 0.10879231659622383, "feedback": "The algorithm Adaptive_Frequency_Modulation_Enhanced_Adaptive_Differential_Evolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.11 with standard deviation 0.07.", "error": "", "parent_id": "62f0f28d-b1f3-482e-a59e-689769ae92f8", "metadata": {"aucs": [0.25263999411576876, 0.22320120358240136, 0.23053926077498277, 0.21796336004968597, 0.20176355279578195, 0.22544885958608962, 0.2026964129443617, 0.2104295476989404, 0.21930347296424457, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.004830088081412609, 9.999999999998899e-05, 0.05034558905182274, 0.0700481784319611, 0.05466985559385573, 0.06065944726161232, 0.059262430689196344, 0.06788476361672768, 0.05484070268722552, 0.07673398335173498, 0.05497697759462916, 0.04335983997082238, 0.05094469900254517, 0.047802859565079014, 0.05670845165482441, 0.058643157286366265, 0.04743069897260144, 0.05660177996691995, 0.05500667107436619, 0.04330263351627561, 0.12178757766386428, 0.11765400839740503, 0.09854418084228489, 0.16988018356716783, 0.12728144235241112, 0.12454043294253425, 0.12339025886913191, 0.12664412782916934, 0.1392193027560361, 0.12670606959121355, 0.09462516756860873, 0.09155548537641933, 0.14277585049295083, 0.08290825400626489, 0.07936954583211331, 0.10619093444284977, 0.08164627198067698, 0.12606281067724234, 0.16388033637336574, 0.15666985132692968, 0.13344675290996133, 0.1775412552654949, 0.17410245089972143, 0.13763011704834027, 0.15774658082513637, 0.13071310013489634, 0.145283946618741, 0.10961573912036804, 0.02278884148216953, 0.0802945642649856, 0.03373152488415909, 0.03624283760412639, 0.07743942860942732, 0.07532078088672478, 0.05282998532825045, 0.09330978662844625, 0.09996404730638009, 0.06267749119336419, 0.10485401748534062, 0.10576044545016139, 0.09787864658683554, 0.10575664107694305, 0.09142571154173018, 0.09862870975139482, 0.10725502859111968, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.049533024551729476, 0.05396931849253361, 0.07178553339368521, 0.05653593026271919, 0.05557087371347802, 0.029761817386177447, 0.07490530373325344, 0.06404305575461056, 0.059541699684509175, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.023419745143362536, 0.022362820397985872, 0.02013502462579586, 0.03775914668926761, 0.022753073056716477, 0.035738076704859845, 0.03186332108234802, 0.0276839021842612, 0.03765149423983116, 0.2277561097006755, 0.23730296884482838, 0.24096994754588064, 0.21101041391719422, 0.22208401733229732, 0.20772082262974334, 0.22755818671528627, 0.21406602863330437, 0.22741898500815583, 0.043198087166891, 0.061083896803368565, 0.04707545085020515, 0.07427339316712755, 0.05527797169011428, 0.06750272680743763, 0.08788474178542727, 0.060639989747130474, 0.056630734950894324, 0.12051355656848539, 0.13158274003884252, 0.1416040898200912, 0.10266004965666087, 0.1477117633164482, 0.17368084695048203, 0.12463134281443944, 0.11181630960716082, 0.10686752647510767, 0.15630532553558585, 0.18048193373755517, 0.1652759473482307, 0.17356267772392875, 0.19437055304139728, 0.1886656672426027, 0.16260926038285584, 0.18043138533652614, 0.16695702685120772, 0.10972374853282474, 0.12135294215583414, 0.11222962340047582, 0.13582729446118014, 0.12985136247928786, 0.13971892326086, 0.12295619143214132, 0.11494302166388337, 0.11642952847012544, 0.18532540043695933, 0.19720518338004256, 0.18707768671698055, 0.1613839757978116, 0.15738453811316344, 0.17963111590901404, 0.23871456259260648, 0.1559236986420257, 0.18739723572987055, 0.16007553139200337, 0.1660566314125137, 0.16340611357911516, 0.16378733796450506, 0.1615617502009934, 0.15958294951986263, 0.15340773889216675, 0.15994182182289662, 0.16125137782124288, 0.15253390009640244, 0.15763713055113182, 0.1469056882179811, 0.263501194575553, 0.17126581099473803, 0.18686008436604895, 0.22779305713106346, 0.16359428838843482, 0.16918246334891296, 0.175248556678963, 0.20529356856655523, 0.13866930682945933, 0.17409322113515513, 0.1371037442680072, 0.1423227295672057, 0.18956449136750675, 0.1766603718318578, 0.16058515946012375, 0.21315846812746408, 0.21910073188153334, 0.17389086298705336, 0.161157812517692, 0.16561862819144246, 0.16490277628775651, 0.17349780035866968, 0.17982892697699848, 0.18205426907763345, 0.07266290783887708, 0.058134894210511434, 0.06770414512824663, 0.05067446905410866, 0.05315319696631082, 0.057324896844191287, 0.05470217222046847, 0.06261767864176893, 0.059589061212343974]}, "mutation_prompt": null}
{"id": "2e1179e5-3a37-4be8-a9e2-0288cf085983", "solution": "import numpy as np\nimport random\n\nclass Enhanced_Adaptive_Differential_Evolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.F = 0.5  # initial frequency\n        self.CR = 0.5  # initial crossover rate\n        self.pop_size = 50  # initial population size\n        self.population = np.random.uniform(-5.0, 5.0, (self.pop_size, self.dim))\n        self.best_solution = np.inf\n        self.probability = 0.2  # increased probability of probabilistic exploration\n        self.temperature = 100  # initial temperature for simulated annealing\n        self.covariance_matrix = np.eye(self.dim)  # initial covariance matrix\n        self.adaptive_step_size = 0.1  # initial adaptive step size for covariance matrix adaptation\n        self.step_size_learning_rate = 0.001  # learning rate for adaptive step size\n        self.history = np.zeros((self.budget, self.dim))  # history of best solutions\n\n    def evaluate(self, x):\n        return self.func(x)\n\n    def func(self, x):\n        # BBOB test suite of 24 noiseless functions\n        # Here we use the first function f1(x) = sum(x_i^2)\n        return np.sum(x**2)\n\n    def frequency_modulation(self, x):\n        # frequency modulation function\n        return self.F * np.sin(2 * np.pi * self.F * x)\n\n    def adaptive_frequency_modulation(self):\n        # adaptive frequency modulation\n        self.F *= 1.1  # increase frequency\n        if random.random() < self.probability:\n            self.F *= 0.9  # decrease frequency with probability\n        return self.F\n\n    def crossover(self, x1, x2):\n        # crossover operation\n        r = np.random.rand(self.dim)\n        return x1 + r * (x2 - x1)\n\n    def selection(self, x1, x2):\n        # selection operation\n        if self.evaluate(x2) < self.evaluate(x1):\n            return x2\n        else:\n            return x1\n\n    def simulated_annealing(self, x):\n        # simulated annealing\n        new_x = x + np.random.normal(0, 1, self.dim)\n        new_x = np.clip(new_x, -5.0, 5.0)\n        if self.evaluate(new_x) < self.evaluate(x):\n            return new_x\n        else:\n            probability = np.exp(-(self.evaluate(new_x) - self.evaluate(x)) / self.temperature)\n            if random.random() < probability:\n                return new_x\n            else:\n                return x\n        self.temperature *= 0.99  # decrease temperature with a higher rate\n\n    def covariance_matrix_adaptation(self):\n        # covariance matrix adaptation\n        self.covariance_matrix = (1 - self.adaptive_step_size) * self.covariance_matrix + self.adaptive_step_size * np.eye(self.dim)\n        return self.covariance_matrix\n\n    def adaptive_step_size_learning(self):\n        # adaptive step size learning\n        self.adaptive_step_size += self.step_size_learning_rate * (self.evaluate(self.population[0]) - self.evaluate(self.population[-1]))\n        return self.adaptive_step_size\n\n    def history_learning(self):\n        # history learning\n        self.history = np.vstack((self.history, self.population[0]))\n\n    def optimize(self, func):\n        for i in range(self.budget):\n            # evaluate population\n            fitness = [self.evaluate(x) for x in self.population]\n            # get best solution\n            self.best_solution = min(fitness)\n            best_index = fitness.index(self.best_solution)\n            # adaptive frequency modulation\n            self.F = self.adaptive_frequency_modulation()\n            # differential evolution\n            for j in range(self.pop_size):\n                # generate trial vector\n                trial = self.crossover(self.population[j], self.population[random.randint(0, self.pop_size - 1)])\n                # evaluate trial vector\n                trial_fitness = self.evaluate(trial)\n                # selection\n                self.population[j] = self.selection(self.population[j], trial)\n                # update best solution\n                if trial_fitness < self.evaluate(self.population[j]):\n                    self.population[j] = trial\n            # simulated annealing\n            for j in range(self.pop_size):\n                self.population[j] = self.simulated_annealing(self.population[j])\n            # covariance matrix adaptation\n            self.covariance_matrix = self.covariance_matrix_adaptation()\n            # adaptive step size learning\n            self.adaptive_step_size = self.adaptive_step_size_learning()\n            # history learning\n            self.history_learning()\n        return self.best_solution, self.history\n\n    def __call__(self, func):\n        self.func = func\n        return self.optimize(func)\n\n# example usage\nbudget = 1000\ndim = 10\noptimizer = Enhanced_Adaptive_Differential_Evolution(budget, dim)\nbest_solution, history = optimizer(lambda x: np.sum(x**2))\nprint(\"Best solution:\", best_solution)\nprint(\"History:\", history)", "name": "Enhanced_Adaptive_Differential_Evolution", "description": "Enhanced Adaptive Differential Evolution with Probabilistic Frequency Modulation, Covariance Matrix Adaptation, and Simulated Annealing with Adaptive Temperature Cooling Schedule and Adaptive Step Size Learning.", "configspace": "", "generation": 26, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "62f0f28d-b1f3-482e-a59e-689769ae92f8", "metadata": {"aucs": [0.23769731172917852, 0.23720457413485296, 0.23398524340821303, 0.20549460010352794, 0.20555116867582168, 0.20130651801061838, 0.2111034242676071, 0.2007516942743227, 0.2116305929248098, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.006392611173892893, 9.999999999998899e-05, 0.059900294776183416, 0.07012785618662798, 0.05419817263252702, 0.0795014722763715, 0.06935901025400615, 0.07305402795587912, 0.07197872740953881, 0.06459557626532919, 0.06996007361196876, 0.053111208791695175, 0.053566490408007095, 0.045790153649368226, 0.08521993086965163, 0.0694819433532079, 0.04350696073483884, 0.060257967750235175, 0.05189063106084635, 0.04649729526248825, 0.14662710504050225, 0.12405344652255246, 0.10395021756261902, 0.5379579998710504, 0.16193852016424048, 0.11791330384339704, 0.18518522693319261, 0.10037714751656313, 0.4283067989696846, 0.09280746828909836, 0.08837625468069787, 0.07937852657802957, 0.07854238359383503, 0.08252944843027832, 0.11767926985163069, 0.1205915686158825, 0.0852113029185213, 0.09530169292310442, 0.17330123385194474, 0.1802252971353444, 0.1525284743121933, 0.1555795762620088, 0.17867323713658245, 0.17229723458815238, 0.16781672295818117, 0.15990674428772478, 0.16337899010881085, 0.08900179420138443, 0.03865389075710812, 0.08160113257791912, 0.03927597651265646, 0.026075257529084794, 0.054926825543406665, 0.09841040259303824, 0.06333280723678636, 0.07418265885714515, 0.09796359396919452, 0.09649198930530256, 0.1100308792533109, 0.10331574585163705, 0.10284233020551181, 0.09918302484193209, 0.10784997913383398, 0.10545554298550341, 0.08743879185238912, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.06531471720729043, 0.06322776317563727, 0.10764868144200157, 0.04234868722028973, 0.06891894424860334, 0.11389557244681159, 0.08902233063245635, 0.04959990216821064, 0.06503624669867603, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.019302053195070723, 0.02569447570168837, 0.02218694043020275, 0.030389077976176515, 0.05011639472944973, 0.026064793258674057, 0.03964242745950375, 0.03523032723265518, 0.04990912829843397, 0.26045759541142244, 0.22792046210319727, 0.24789883372762955, 0.22782461580683844, 0.20323172004362933, 0.21785474159582874, 0.24977361138643406, 0.22257910459292318, 0.23487337912939432, 0.047406160946790954, 0.06913972508922595, 0.04628131465091223, 0.06565901983375388, 0.055748382497863735, 0.0633631736000938, 0.06015180734300485, 0.0755272335023841, 0.06646283240006434, 0.12119223920814659, 0.12470214300559834, 0.14030068848721566, 0.11557895408650731, 0.1328869344681134, 0.12954047545064928, 0.12548124776483527, 0.11082883532147259, 0.11613807783079211, 0.1646706089612735, 0.17402029364751725, 0.169957030806709, 0.17721735438905684, 0.18977506642230046, 0.18552711273919076, 0.16008964310468943, 0.18975185260984473, 0.16153221359268788, 0.11588258721205402, 0.12741740017735936, 0.13192378680608652, 0.1228000957149119, 0.12838293347600316, 0.1273759751841037, 0.10303660034216178, 0.12869446988955147, 0.11553907992403178, 0.1591644210830242, 0.190149261923249, 0.16711753718862765, 0.1942395294204997, 0.2132047975084318, 0.15491365322932638, 0.1620218121660999, 0.16842459346413563, 0.19539101481697096, 0.1574308748762071, 0.15527046019467394, 0.16117056355370862, 0.1507179142739733, 0.16571259637988223, 0.1572215990897441, 0.1537357474934712, 0.15304501824019534, 0.17085057300372697, 0.14647342296828503, 0.16458182587567938, 0.1475369256476301, 0.24951655367433845, 0.1706906960414144, 0.22975424671538114, 0.1468282263746179, 0.18998475289801375, 0.19872740316007598, 0.17967694151965607, 0.2095248766304756, 0.16531236969298302, 0.17673728075527995, 0.1573563880111336, 0.16079461976257647, 0.13630563294661702, 0.18943475494288797, 0.15794340341591473, 0.1694964781221654, 0.17584746439259435, 0.1675693088119269, 0.16675380106618054, 0.16310744584612002, 0.17812108928443993, 0.16876649757309747, 0.16601458261862279, 0.18324856573202208, 0.0679542418940462, 0.06107766964719452, 0.0634038621543137, 0.0589167546534245, 0.06826269804045038, 0.051810492248762996, 0.06457290160761553, 0.0671718679707437, 0.06687823879460175]}, "mutation_prompt": null}
{"id": "de3e2b8c-eeed-49dc-9326-c1ae4de2584b", "solution": "import numpy as np\nimport random\n\nclass Enhanced_Adaptive_Differential_Evolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.F = 0.5  # initial frequency\n        self.CR = 0.5  # initial crossover rate\n        self.pop_size = 50  # initial population size\n        self.population = np.random.uniform(-5.0, 5.0, (self.pop_size, self.dim))\n        self.best_solution = np.inf\n        self.probability = 0.4  # increased probability of probabilistic exploration\n        self.temperature = 100  # initial temperature for simulated annealing\n        self.covariance_matrix = np.eye(self.dim)  # initial covariance matrix\n        self.adaptive_step_size = 0.1  # initial adaptive step size for covariance matrix adaptation\n        self.step_size_learning_rate = 0.001  # learning rate for adaptive step size\n        self.history = np.zeros((self.budget, self.dim))  # history of best solutions\n\n    def evaluate(self, x):\n        return self.func(x)\n\n    def func(self, x):\n        # BBOB test suite of 24 noiseless functions\n        # Here we use the first function f1(x) = sum(x_i^2)\n        return np.sum(x**2)\n\n    def frequency_modulation(self, x):\n        # frequency modulation function\n        return self.F * np.sin(2 * np.pi * self.F * x)\n\n    def adaptive_frequency_modulation(self):\n        # adaptive frequency modulation\n        self.F *= 1.1  # increase frequency\n        if random.random() < self.probability:\n            self.F *= 0.9  # decrease frequency with probability\n        return self.F\n\n    def crossover(self, x1, x2):\n        # crossover operation\n        r = np.random.rand(self.dim)\n        return x1 + r * (x2 - x1)\n\n    def selection(self, x1, x2):\n        # selection operation\n        if self.evaluate(x2) < self.evaluate(x1):\n            return x2\n        else:\n            return x1\n\n    def simulated_annealing(self, x):\n        # simulated annealing\n        new_x = x + np.random.normal(0, 1, self.dim)\n        new_x = np.clip(new_x, -5.0, 5.0)\n        if self.evaluate(new_x) < self.evaluate(x):\n            return new_x\n        else:\n            probability = np.exp(-(self.evaluate(new_x) - self.evaluate(x)) / self.temperature)\n            if random.random() < probability:\n                return new_x\n            else:\n                return x\n        self.temperature *= 0.99  # decrease temperature with a higher rate\n\n    def covariance_matrix_adaptation(self):\n        # covariance matrix adaptation\n        self.covariance_matrix = (1 - self.adaptive_step_size) * self.covariance_matrix + self.adaptive_step_size * np.eye(self.dim)\n        return self.covariance_matrix\n\n    def adaptive_step_size_learning(self):\n        # adaptive step size learning\n        self.adaptive_step_size += self.step_size_learning_rate * (self.evaluate(self.population[0]) - self.evaluate(self.population[-1]))\n        return self.adaptive_step_size\n\n    def history_learning(self):\n        # history learning\n        self.history = np.vstack((self.history, self.population[0]))\n\n    def optimize(self, func):\n        for i in range(self.budget):\n            # evaluate population\n            fitness = [self.evaluate(x) for x in self.population]\n            # get best solution\n            self.best_solution = min(fitness)\n            best_index = fitness.index(self.best_solution)\n            # adaptive frequency modulation\n            self.F = self.adaptive_frequency_modulation()\n            # differential evolution\n            for j in range(self.pop_size):\n                # generate trial vector\n                trial = self.crossover(self.population[j], self.population[random.randint(0, self.pop_size - 1)])\n                # evaluate trial vector\n                trial_fitness = self.evaluate(trial)\n                # selection\n                self.population[j] = self.selection(self.population[j], trial)\n                # update best solution\n                if trial_fitness < self.evaluate(self.population[j]):\n                    self.population[j] = trial\n            # simulated annealing\n            for j in range(self.pop_size):\n                self.population[j] = self.simulated_annealing(self.population[j])\n            # covariance matrix adaptation\n            self.covariance_matrix = self.covariance_matrix_adaptation()\n            # adaptive step size learning\n            self.adaptive_step_size = self.adaptive_step_size_learning()\n            # history learning\n            self.history_learning()\n        return self.best_solution, self.history\n\n    def __call__(self, func):\n        self.func = func\n        return self.optimize(func)\n\n# example usage\nbudget = 1000\ndim = 10\noptimizer = Enhanced_Adaptive_Differential_Evolution(budget, dim)\nbest_solution, history = optimizer(lambda x: np.sum(x**2))\nprint(\"Best solution:\", best_solution)\nprint(\"History:\", history)", "name": "Enhanced_Adaptive_Differential_Evolution", "description": "Enhanced Adaptive Frequency Modulation with Covariance Matrix Adaptation, Simulated Annealing, and Adaptive Step Size Learning with Probabilistic Frequency Modulation and Adaptive Temperature Cooling Schedule.", "configspace": "", "generation": 65, "fitness": 0.11026585565694563, "feedback": "The algorithm Enhanced_Adaptive_Differential_Evolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.11 with standard deviation 0.07.", "error": "", "parent_id": "62f0f28d-b1f3-482e-a59e-689769ae92f8", "metadata": {"aucs": [0.2218887608537342, 0.2882245510116205, 0.21325876537559763, 0.19589021066875179, 0.20510527373742615, 0.21611897293915694, 0.22449475322196744, 0.21013490932187762, 0.2006945560277602, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.006088870701117632, 9.999999999998899e-05, 0.05803335548697419, 0.0657099313179682, 0.06157754384357994, 0.09256112598779687, 0.062383460500070376, 0.0750584892865862, 0.07720090096635979, 0.06865640881266222, 0.06589816094552148, 0.04929625354909006, 0.057557513565008045, 0.04752960381877602, 0.05693462505550595, 0.05612836951440059, 0.054642947480957105, 0.05508752289684493, 0.07023688129669226, 0.05027299696200083, 0.12276030437170427, 0.14263265276763482, 0.10742820877343329, 0.1780107735057429, 0.14334341362468894, 0.15070020653163574, 0.1767785317106363, 0.10582156281945643, 0.13395002551609358, 0.1044121190105256, 0.08681625801655479, 0.0752761491584335, 0.07368614162959874, 0.0765110893350609, 0.09853209896032256, 0.1235853967415631, 0.11025653842670669, 0.08849782761283476, 0.15972425292967907, 0.15384322569435083, 0.13855402647828718, 0.16880817895525602, 0.17031053318598477, 0.1421171164784244, 0.14528571991908268, 0.20903409352256108, 0.16865280101392077, 0.03421699621870267, 0.043464128794526546, 0.09206275566792921, 0.0720599715543121, 0.03514457590801778, 0.07029010990329065, 0.0895647338149429, 0.07070175899283826, 0.10257686875934469, 0.09017924919031706, 0.08299524608077369, 0.0861602191116756, 0.10388473161753187, 0.08784788259334886, 0.11079233712531511, 0.1053035801210046, 0.09808194343272258, 0.11373156685293906, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.0844904411108871, 0.10688907420114413, 0.0695178254192228, 0.04424280281142878, 0.04035501502975858, 0.01439657098052849, 0.05871913014177521, 0.0627250082379247, 0.0786056235110194, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.022768642390614824, 0.014566301798032644, 0.030911073843516768, 0.031120307938390046, 0.03045084046533575, 0.03887326268530411, 0.03061364717137438, 0.03030977760289333, 0.03992803179203108, 0.23744653259511872, 0.23211679429061882, 0.2449705512086966, 0.2139018796249066, 0.1936839568475447, 0.2312424646087735, 0.230036371920179, 0.24016358492439982, 0.24773995707125174, 0.049768113996658925, 0.05699281904512021, 0.07204799968002007, 0.0770524329600436, 0.05603587491971851, 0.06485465347102348, 0.06487951173228357, 0.06368780784807837, 0.07037331931857937, 0.1249031393420863, 0.12469846358386172, 0.12560523149878788, 0.1159943463770956, 0.1098744984195118, 0.125196540172821, 0.12026955656125615, 0.11432296137571407, 0.12189258495622324, 0.16119863905834553, 0.16743679954010005, 0.17304450434555074, 0.1871879413849573, 0.17784093965478243, 0.18983801668968647, 0.1617609363508684, 0.17564712009975159, 0.17659526491388755, 0.11872363573708844, 0.1159188235616373, 0.11677401617522087, 0.139798795223946, 0.14724821361081575, 0.1372410337883313, 0.10439589948154537, 0.13524115359680888, 0.1073562979548024, 0.18233365033055537, 0.19409929348827115, 0.17260237899460373, 0.16233121774961567, 0.18336092906577017, 0.17972323535545798, 0.19557153798295224, 0.17261602428163714, 0.18259121017748803, 0.16033046392767725, 0.16105573911268112, 0.16378766243609166, 0.1538142387070186, 0.16446659637190097, 0.1585169388575297, 0.16005796135984673, 0.15877325563941724, 0.1597112820848201, 0.15776215908257385, 0.1616679119755723, 0.19203107778287887, 0.21169327644581604, 0.18735210952342762, 0.19930519863729612, 0.1903031267392118, 0.19116789384266708, 0.20350577306240347, 0.20227497094604185, 0.15227574864439208, 0.20187805073136578, 0.1958308295410096, 0.15664102185368667, 0.16564161047601123, 0.1693436464789515, 0.16559812419016762, 0.14724959086480993, 0.1754552908536925, 0.16903437626136797, 0.16364665651809407, 0.15783119004030266, 0.16301399005313777, 0.1865397135903747, 0.1714633140773616, 0.18138016732302875, 0.18136706078706688, 0.056909002988970836, 0.06104990230801577, 0.06423714609189357, 0.06214971925788526, 0.05882706617625977, 0.060789332193059264, 0.056309977013780044, 0.05274973157477558, 0.07126847382437973]}, "mutation_prompt": null}
{"id": "3aee03b3-4e5c-4c51-8979-82df4dcc288a", "solution": "import numpy as np\nimport random\n\nclass Enhanced_Adaptive_Frequency_Modulation:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.F = 0.5  # initial frequency\n        self.CR = 0.5  # initial crossover rate\n        self.pop_size = 50  # initial population size\n        self.population = np.random.uniform(-5.0, 5.0, (self.pop_size, self.dim))\n        self.best_solution = np.inf\n        self.probability = 0.3  # increased probability of probabilistic exploration\n        self.temperature = 100  # initial temperature for simulated annealing\n        self.covariance_matrix = np.eye(self.dim)  # initial covariance matrix\n        self.adaptive_step_size = 0.1  # initial adaptive step size for covariance matrix adaptation\n        self.step_size_learning_rate = 0.001  # learning rate for adaptive step size\n        self.history = np.zeros((self.budget, self.dim))  # history of best solutions\n\n    def evaluate(self, x):\n        return self.func(x)\n\n    def func(self, x):\n        # BBOB test suite of 24 noiseless functions\n        # Here we use the first function f1(x) = sum(x_i^2)\n        return np.sum(x**2)\n\n    def frequency_modulation(self, x):\n        # frequency modulation function\n        return self.F * np.sin(2 * np.pi * self.F * x)\n\n    def adaptive_frequency_modulation(self):\n        # adaptive frequency modulation\n        self.F *= 1.05  # increase frequency\n        if random.random() < self.probability:\n            self.F *= 0.95  # decrease frequency with probability\n        return self.F\n\n    def crossover(self, x1, x2):\n        # crossover operation\n        r = np.random.rand(self.dim)\n        return x1 + r * (x2 - x1)\n\n    def selection(self, x1, x2):\n        # selection operation\n        if self.evaluate(x2) < self.evaluate(x1):\n            return x2\n        else:\n            return x1\n\n    def simulated_annealing(self, x):\n        # simulated annealing\n        new_x = x + np.random.normal(0, 1, self.dim)\n        new_x = np.clip(new_x, -5.0, 5.0)\n        if self.evaluate(new_x) < self.evaluate(x):\n            return new_x\n        else:\n            probability = np.exp(-(self.evaluate(new_x) - self.evaluate(x)) / self.temperature)\n            if random.random() < probability:\n                return new_x\n            else:\n                return x\n        self.temperature *= 0.995  # decrease temperature with a higher rate\n\n    def covariance_matrix_adaptation(self):\n        # covariance matrix adaptation\n        self.covariance_matrix = (1 - self.adaptive_step_size) * self.covariance_matrix + self.adaptive_step_size * np.eye(self.dim)\n        return self.covariance_matrix\n\n    def adaptive_step_size_learning(self):\n        # adaptive step size learning\n        self.adaptive_step_size += self.step_size_learning_rate * (self.evaluate(self.population[0]) - self.evaluate(self.population[-1]))\n        return self.adaptive_step_size\n\n    def history_learning(self):\n        # history learning\n        self.history = np.vstack((self.history, self.population[0]))\n\n    def optimize(self, func):\n        for i in range(self.budget):\n            # evaluate population\n            fitness = [self.evaluate(x) for x in self.population]\n            # get best solution\n            self.best_solution = min(fitness)\n            best_index = fitness.index(self.best_solution)\n            # adaptive frequency modulation\n            self.F = self.adaptive_frequency_modulation()\n            # differential evolution\n            for j in range(self.pop_size):\n                # generate trial vector\n                trial = self.crossover(self.population[j], self.population[random.randint(0, self.pop_size - 1)])\n                # evaluate trial vector\n                trial_fitness = self.evaluate(trial)\n                # selection\n                self.population[j] = self.selection(self.population[j], trial)\n                # update best solution\n                if trial_fitness < self.evaluate(self.population[j]):\n                    self.population[j] = trial\n            # simulated annealing\n            for j in range(self.pop_size):\n                self.population[j] = self.simulated_annealing(self.population[j])\n            # covariance matrix adaptation\n            self.covariance_matrix = self.covariance_matrix_adaptation()\n            # adaptive step size learning\n            self.adaptive_step_size = self.adaptive_step_size_learning()\n            # history learning\n            self.history_learning()\n        return self.best_solution, self.history\n\n    def __call__(self, func):\n        self.func = func\n        return self.optimize(func)\n\n# example usage\nbudget = 1000\ndim = 10\noptimizer = Enhanced_Adaptive_Frequency_Modulation(budget, dim)\nbest_solution, history = optimizer(lambda x: np.sum(x**2))\nprint(\"Best solution:\", best_solution)\nprint(\"History:\", history)", "name": "Enhanced_Adaptive_Frequency_Modulation", "description": "Enhanced Adaptive Frequency Modulation-based Hybridization of Simulated Annealing and Covariance Matrix Adaptation Evolution Strategy with Adaptive Frequency Modulation and Adaptive Step Size Learning.", "configspace": "", "generation": 66, "fitness": 0.11128027268298256, "feedback": "The algorithm Enhanced_Adaptive_Frequency_Modulation got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.11 with standard deviation 0.07.", "error": "", "parent_id": "62f0f28d-b1f3-482e-a59e-689769ae92f8", "metadata": {"aucs": [0.23689849278997477, 0.2485528498584355, 0.23251066436142886, 0.20409331825490873, 0.19777139220257967, 0.19832028796146728, 0.22651948810506894, 0.20240580538433273, 0.20642258779481937, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.05421615532750779, 0.06067077856606862, 0.055106429318082295, 0.0753276097137281, 0.06863909811360558, 0.05706887376663905, 0.06382585808719743, 0.06234323046251089, 0.06150461393495499, 0.05754485669912679, 0.058258042470066385, 0.042798452145462185, 0.061437571993596785, 0.06364452109995178, 0.044207465542319135, 0.05392932847975973, 0.058224956648533, 0.05835667863934746, 0.11976204259752243, 0.12443625706451311, 0.09825370332730543, 0.24904816715413425, 0.10391218503704491, 0.11911615646380924, 0.1677324586798522, 0.10315356809743681, 0.42159691789766573, 0.10932541954874431, 0.10236526900012977, 0.0703642140995222, 0.08811681880171651, 0.09645401578726942, 0.08784579686334493, 0.11848520447087363, 0.09110644325434392, 0.06874991737914882, 0.14581940351248546, 0.16857025932138536, 0.13349244652790893, 0.1501165376809095, 0.18525893705520546, 0.1509271045422672, 0.14706460750192252, 0.1268652524680408, 0.12577894500216535, 0.06544995897253558, 0.06433862587900008, 0.06832135212392332, 0.07714831973830827, 0.040727857313440086, 0.03218231810794836, 0.08420431484287982, 0.08302514803599026, 0.0792779015011511, 0.09381805100826046, 0.10557747842670162, 0.08635010442652313, 0.09968379184237908, 0.08919602758532186, 0.08543880947783855, 0.12014580156525989, 0.10357922521158003, 0.0894778390133264, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.07825931159343003, 0.049056090335884206, 0.11468006490069749, 0.044460004977355694, 0.025774823206765496, 0.03631606828141576, 0.05550188644551113, 0.0641727077834019, 0.04258586587750446, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.029870241738367054, 0.028042829373225886, 0.02485818128897177, 0.022222983204968627, 0.056617225687364825, 0.048532962720970296, 0.044845812761383774, 0.04351536244358445, 0.043127522383054395, 0.2215560125156104, 0.23461808882657376, 0.22955235020436748, 0.2269557083335021, 0.21985542492582555, 0.21682909558218177, 0.2282905226657158, 0.2271136236468082, 0.24287846451963546, 0.06295704246279676, 0.05505762830909389, 0.048623609066278384, 0.06303648404934425, 0.07297099780779792, 0.05697580067508934, 0.06559796557599074, 0.0604068555339069, 0.06451430248058476, 0.11191511276355526, 0.13571848693044253, 0.1283292526476083, 0.1261532319860984, 0.17062739888477085, 0.12417255652074122, 0.14095254596590712, 0.12127564118305778, 0.11438126646186175, 0.1630056653686912, 0.17128167855249243, 0.198445209976868, 0.16296696444734082, 0.18090625470474808, 0.17785641552860432, 0.1704784593794857, 0.1882424893370962, 0.16989680108732497, 0.1255723167133176, 0.14106000887635506, 0.13056087303562158, 0.12815349639603446, 0.13158493501769042, 0.12492602319125723, 0.10943781328533464, 0.14200965187729564, 0.10426539257262213, 0.24417514891094005, 0.16991561612743467, 0.16624143631603694, 0.18680659618858075, 0.16612568432052444, 0.1870679652803836, 0.17698455175074268, 0.197695244581369, 0.21954451181271384, 0.16427846251681877, 0.16044861973444002, 0.15358516404320943, 0.1661365917343548, 0.17559972058682505, 0.1593265000014259, 0.15674370792332648, 0.1595714665762591, 0.15500253394214947, 0.22958925187601065, 0.1566157280290732, 0.15945835118980045, 0.2056177540414048, 0.2609622227876828, 0.1789383592745165, 0.14771633849609334, 0.19097061740444576, 0.16491207407295172, 0.21981923135847814, 0.14227882298402272, 0.13402985797550482, 0.14899693948215953, 0.1859989092069001, 0.141677583285316, 0.15975573068438298, 0.1858215685479987, 0.21641040775884868, 0.19372826961878087, 0.17019795029936247, 0.16952526216412556, 0.17693325573287333, 0.16799126052039703, 0.16978448621474618, 0.1650162324691422, 0.16417218369097974, 0.17935276888881124, 0.0754139806582027, 0.05910629542439583, 0.05500475622514478, 0.06554707369088597, 0.06392339889813192, 0.07074186601839738, 0.0591906909669242, 0.06669131567206243, 0.07003233476004678]}, "mutation_prompt": null}
{"id": "4e7a09de-d622-4f2f-a474-de555815cb8e", "solution": "import numpy as np\nimport random\n\nclass Novel_Adaptive_Frequency_Modulation_Enhanced_Differential_Evolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.F = 0.5  # initial frequency\n        self.CR = 0.5  # initial crossover rate\n        self.pop_size = 50  # initial population size\n        self.population = np.random.uniform(-5.0, 5.0, (self.pop_size, self.dim))\n        self.best_solution = np.inf\n        self.probability = 0.4  # increased probability of probabilistic exploration\n        self.temperature = 100  # initial temperature for simulated annealing\n        self.covariance_matrix = np.eye(self.dim)  # initial covariance matrix\n        self.adaptive_step_size = 0.1  # initial adaptive step size for covariance matrix adaptation\n        self.step_size_learning_rate = 0.001  # learning rate for adaptive step size\n        self.history = np.zeros((self.budget, self.dim))  # history of best solutions\n        self.history_learning_rate = 0.05  # learning rate for history learning\n\n    def evaluate(self, x):\n        return self.func(x)\n\n    def func(self, x):\n        # BBOB test suite of 24 noiseless functions\n        # Here we use the first function f1(x) = sum(x_i^2)\n        return np.sum(x**2)\n\n    def frequency_modulation(self, x):\n        # frequency modulation function\n        return self.F * np.sin(2 * np.pi * self.F * x)\n\n    def adaptive_frequency_modulation(self):\n        # adaptive frequency modulation\n        self.F *= 1.1  # increase frequency\n        if random.random() < self.probability:\n            self.F *= 0.9  # decrease frequency with probability\n        return self.F\n\n    def crossover(self, x1, x2):\n        # crossover operation\n        r = np.random.rand(self.dim)\n        return x1 + r * (x2 - x1)\n\n    def selection(self, x1, x2):\n        # selection operation\n        if self.evaluate(x2) < self.evaluate(x1):\n            return x2\n        else:\n            return x1\n\n    def simulated_annealing(self, x):\n        # simulated annealing\n        new_x = x + np.random.normal(0, 1, self.dim)\n        new_x = np.clip(new_x, -5.0, 5.0)\n        if self.evaluate(new_x) < self.evaluate(x):\n            return new_x\n        else:\n            probability = np.exp(-(self.evaluate(new_x) - self.evaluate(x)) / self.temperature)\n            if random.random() < probability:\n                return new_x\n            else:\n                return x\n        self.temperature *= 0.99  # decrease temperature with a higher rate\n\n    def covariance_matrix_adaptation(self):\n        # covariance matrix adaptation\n        self.covariance_matrix = (1 - self.adaptive_step_size) * self.covariance_matrix + self.adaptive_step_size * np.eye(self.dim)\n        return self.covariance_matrix\n\n    def adaptive_step_size_learning(self):\n        # adaptive step size learning\n        self.adaptive_step_size += self.step_size_learning_rate * (self.evaluate(self.population[0]) - self.evaluate(self.population[-1]))\n        return self.adaptive_step_size\n\n    def history_learning(self):\n        # history learning\n        self.history += self.history_learning_rate * (self.evaluate(self.population[0]) - self.history[-1])\n        return self.history\n\n    def optimize(self, func):\n        for i in range(self.budget):\n            # evaluate population\n            fitness = [self.evaluate(x) for x in self.population]\n            # get best solution\n            self.best_solution = min(fitness)\n            best_index = fitness.index(self.best_solution)\n            # adaptive frequency modulation\n            self.F = self.adaptive_frequency_modulation()\n            # differential evolution\n            for j in range(self.pop_size):\n                # generate trial vector\n                trial = self.crossover(self.population[j], self.population[random.randint(0, self.pop_size - 1)])\n                # evaluate trial vector\n                trial_fitness = self.evaluate(trial)\n                # selection\n                self.population[j] = self.selection(self.population[j], trial)\n                # update best solution\n                if trial_fitness < self.evaluate(self.population[j]):\n                    self.population[j] = trial\n            # simulated annealing\n            for j in range(self.pop_size):\n                self.population[j] = self.simulated_annealing(self.population[j])\n            # covariance matrix adaptation\n            self.covariance_matrix = self.covariance_matrix_adaptation()\n            # adaptive step size learning\n            self.adaptive_step_size = self.adaptive_step_size_learning()\n            # history learning\n            self.history = self.history_learning()\n        return self.best_solution, self.history\n\n    def __call__(self, func):\n        self.func = func\n        return self.optimize(func)\n\n# example usage\nbudget = 1000\ndim = 10\noptimizer = Novel_Adaptive_Frequency_Modulation_Enhanced_Differential_Evolution(budget, dim)\nbest_solution, history = optimizer(lambda x: np.sum(x**2))\nprint(\"Best solution:\", best_solution)\nprint(\"History:\", history)", "name": "Novel_Adaptive_Frequency_Modulation_Enhanced_Differential_Evolution", "description": "Novel Adaptive Frequency Modulation Enhanced Differential Evolution with Adaptive Covariance Matrix and Simulated Annealing with Probability-Driven Exploration.", "configspace": "", "generation": 67, "fitness": 0.10827138556953832, "feedback": "The algorithm Novel_Adaptive_Frequency_Modulation_Enhanced_Differential_Evolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.11 with standard deviation 0.07.", "error": "", "parent_id": "62f0f28d-b1f3-482e-a59e-689769ae92f8", "metadata": {"aucs": [0.22614883092667892, 0.2227526993688923, 0.2214557226649162, 0.22994443871665826, 0.1893504760492849, 0.21752651640349763, 0.22102390262944038, 0.20298906053990595, 0.21318852619242978, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.05458566659745334, 0.06545779511992755, 0.05667320034679246, 0.06597736117759578, 0.08474125720012271, 0.05992609409064076, 0.06950884064531215, 0.06450799935950313, 0.06351421715745309, 0.0386898891982046, 0.05237546985128205, 0.04814407599584192, 0.05696218716291501, 0.05525475601616803, 0.048806644188124304, 0.05527143393748246, 0.05488528212123289, 0.04622728464908055, 0.11407323635942779, 0.1103563501581899, 0.0893776517342274, 0.13089561025969365, 0.12439281450133566, 0.1282225405082672, 0.17229686376227749, 0.10028651276631395, 0.14596494732600895, 0.11600769669540134, 0.09835217290460474, 0.07080533272284506, 0.10852445683222123, 0.06971791506355784, 0.07127166615520641, 0.11472028809847179, 0.0821351174849928, 0.10658139632305608, 0.1619623731084936, 0.16679936511814053, 0.11367242293880331, 0.1901017777755739, 0.20166573395922882, 0.15615259172684748, 0.13954108442434499, 0.17472027470218032, 0.13780521030834114, 0.056509340480298986, 0.07991384820976544, 0.05293443278313026, 0.06557030461291691, 0.01736658953993997, 0.03475475414936957, 0.0700003075779061, 0.11849501571625753, 0.0814734735444268, 0.11690346293352338, 0.10754622920597101, 0.08869596008887415, 0.10423829989787958, 0.10096523470468599, 0.10106147855491565, 0.11008952382779558, 0.10239986027285608, 0.09565394995148946, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.06283832302879322, 0.06781720597907881, 0.08925906996680555, 0.019879574691395674, 0.04484307832552126, 0.03605082981698082, 0.08079331510466359, 0.08235936724275439, 0.10203678208151146, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.025608222038228767, 0.032181185402525125, 0.03078804231204979, 0.040198293449540734, 0.036136731306364744, 0.031109565291033814, 0.03210479855587656, 0.045072947798806884, 0.04726298441844701, 0.23424403319141163, 0.22652499166692674, 0.2592633666254298, 0.19456671356488353, 0.22008743659766372, 0.21624942174064832, 0.22295254578110335, 0.23999011518512292, 0.24980091604200105, 0.05043258935269512, 0.052060483524309364, 0.04234280938318502, 0.05535234725311433, 0.05613883805425035, 0.06923514215970139, 0.05831564442984871, 0.056639232771258374, 0.06454617945960472, 0.12089310216534888, 0.1223615244352142, 0.11842175776965747, 0.1365455296025717, 0.13021582032546053, 0.12351314382422063, 0.1454865840802675, 0.10865800188243668, 0.12975111526938565, 0.17879698829248225, 0.18202827096859986, 0.16798121338801097, 0.20192698349440452, 0.18516410198107613, 0.1865633587631117, 0.16337124209061915, 0.18299690862282314, 0.20056606246418096, 0.10791076364162078, 0.11242568856437951, 0.13433769941183904, 0.12443095914176372, 0.1486089066217119, 0.13350339997384675, 0.10615783218961328, 0.14094957502639593, 0.10436489872866894, 0.17208069643914592, 0.1648507355304093, 0.16678005259636497, 0.16753091202696502, 0.1734404092273567, 0.1958721457217144, 0.16811617949407465, 0.15556435194570017, 0.16941232781499316, 0.15290680527946987, 0.15765033916428872, 0.17090427818277587, 0.157477725255051, 0.1515888638859817, 0.15112052599268622, 0.15648922832319023, 0.15825804116228215, 0.16101700413499198, 0.15561451111938118, 0.15634793462679408, 0.1536094132161414, 0.2190896393652383, 0.2718078664965393, 0.1358124033467668, 0.14157303599228288, 0.13258984755718084, 0.18540950393504896, 0.1838944202298327, 0.20104273360710667, 0.18014140680524215, 0.13939774707518704, 0.2076491525932752, 0.11750140506493811, 0.14670674996601119, 0.17061308568008993, 0.128747609111866, 0.18985850089763656, 0.17231836906590958, 0.17651897908566883, 0.16934552466915553, 0.1660760606581132, 0.16484598805393147, 0.18640111846350915, 0.17727827866101475, 0.19045786728561676, 0.06004119785344897, 0.07751821414873938, 0.06619037276726325, 0.062388011409529875, 0.05848287966408994, 0.054246926494161185, 0.055080603166014375, 0.05871639466621492, 0.05966525365375419]}, "mutation_prompt": null}
{"id": "c7d155ee-7b90-443a-82cf-cd7b9213ad5b", "solution": "import numpy as np\nimport random\n\nclass Enhanced_Adaptive_Frequency_Modulated_Simulated_Annealing:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.F = 0.5  # initial frequency\n        self.CR = 0.5  # initial crossover rate\n        self.pop_size = 50  # initial population size\n        self.population = np.random.uniform(-5.0, 5.0, (self.pop_size, self.dim))\n        self.best_solution = np.inf\n        self.probability = 0.2  # increased probability of probabilistic exploration\n        self.temperature = 100  # initial temperature for simulated annealing\n        self.covariance_matrix = np.eye(self.dim)  # initial covariance matrix\n        self.adaptive_step_size = 0.1  # initial adaptive step size for covariance matrix adaptation\n        self.step_size_learning_rate = 0.001  # learning rate for adaptive step size\n        self.history = np.zeros((self.budget, self.dim))  # history of best solutions\n        self.temperature_cooling_rate = 0.99  # temperature cooling rate\n\n    def evaluate(self, x):\n        return self.func(x)\n\n    def func(self, x):\n        # BBOB test suite of 24 noiseless functions\n        # Here we use the first function f1(x) = sum(x_i^2)\n        return np.sum(x**2)\n\n    def frequency_modulation(self, x):\n        # frequency modulation function\n        return self.F * np.sin(2 * np.pi * self.F * x)\n\n    def adaptive_frequency_modulation(self):\n        # adaptive frequency modulation\n        self.F *= 1.1  # increase frequency\n        if random.random() < self.probability:\n            self.F *= 0.9  # decrease frequency with probability\n        return self.F\n\n    def crossover(self, x1, x2):\n        # crossover operation\n        r = np.random.rand(self.dim)\n        return x1 + r * (x2 - x1)\n\n    def selection(self, x1, x2):\n        # selection operation\n        if self.evaluate(x2) < self.evaluate(x1):\n            return x2\n        else:\n            return x1\n\n    def simulated_annealing(self, x):\n        # simulated annealing\n        new_x = x + np.random.normal(0, 1, self.dim)\n        new_x = np.clip(new_x, -5.0, 5.0)\n        if self.evaluate(new_x) < self.evaluate(x):\n            return new_x\n        else:\n            probability = np.exp(-(self.evaluate(new_x) - self.evaluate(x)) / self.temperature)\n            if random.random() < probability:\n                return new_x\n            else:\n                return x\n        self.temperature *= self.temperature_cooling_rate  # decrease temperature with a higher rate\n\n    def covariance_matrix_adaptation(self):\n        # covariance matrix adaptation\n        self.covariance_matrix = (1 - self.adaptive_step_size) * self.covariance_matrix + self.adaptive_step_size * np.eye(self.dim)\n        return self.covariance_matrix\n\n    def adaptive_step_size_learning(self):\n        # adaptive step size learning\n        self.adaptive_step_size += self.step_size_learning_rate * (self.evaluate(self.population[0]) - self.evaluate(self.population[-1]))\n        return self.adaptive_step_size\n\n    def history_learning(self):\n        # history learning\n        self.history = np.vstack((self.history, self.population[0]))\n\n    def optimize(self, func):\n        for i in range(self.budget):\n            # evaluate population\n            fitness = [self.evaluate(x) for x in self.population]\n            # get best solution\n            self.best_solution = min(fitness)\n            best_index = fitness.index(self.best_solution)\n            # adaptive frequency modulation\n            self.F = self.adaptive_frequency_modulation()\n            # differential evolution\n            for j in range(self.pop_size):\n                # generate trial vector\n                trial = self.crossover(self.population[j], self.population[random.randint(0, self.pop_size - 1)])\n                # evaluate trial vector\n                trial_fitness = self.evaluate(trial)\n                # selection\n                self.population[j] = self.selection(self.population[j], trial)\n                # update best solution\n                if trial_fitness < self.evaluate(self.population[j]):\n                    self.population[j] = trial\n            # simulated annealing\n            for j in range(self.pop_size):\n                self.population[j] = self.simulated_annealing(self.population[j])\n            # covariance matrix adaptation\n            self.covariance_matrix = self.covariance_matrix_adaptation()\n            # adaptive step size learning\n            self.adaptive_step_size = self.adaptive_step_size_learning()\n            # history learning\n            self.history_learning()\n        return self.best_solution, self.history\n\n    def __call__(self, func):\n        self.func = func\n        return self.optimize(func)\n\n# example usage\nbudget = 1000\ndim = 10\noptimizer = Enhanced_Adaptive_Frequency_Modulated_Simulated_Annealing(budget, dim)\nbest_solution, history = optimizer(lambda x: np.sum(x**2))\nprint(\"Best solution:\", best_solution)\nprint(\"History:\", history)", "name": "Enhanced_Adaptive_Frequency_Modulated_Simulated_Annealing", "description": "Enhanced Adaptive Frequency Modulated Simulated Annealing with Covariance Matrix Adaptation and Adaptive Step Size Learning with History Learning and Adaptive Temperature Cooling Schedule.", "configspace": "", "generation": 68, "fitness": 0.11279592846506167, "feedback": "The algorithm Enhanced_Adaptive_Frequency_Modulated_Simulated_Annealing got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.11 with standard deviation 0.07.", "error": "", "parent_id": "62f0f28d-b1f3-482e-a59e-689769ae92f8", "metadata": {"aucs": [0.25450049499229666, 0.218699241097982, 0.23631707109736877, 0.2467658541227823, 0.21918372255644303, 0.20085714104770058, 0.2174310250710858, 0.20384308981794497, 0.19730870691024593, 0.0014996308828399973, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.00011379984862436299, 9.999999999998899e-05, 0.0642228504138308, 0.06194036619902221, 0.052994029634572004, 0.08423385256581828, 0.056389985035102286, 0.06480187786068403, 0.06088725359235936, 0.07283244988397908, 0.06046266541103884, 0.04446072735699158, 0.054480903857202945, 0.04403840938131309, 0.06434046556361495, 0.04650005489663911, 0.05025056701752495, 0.056609439816777574, 0.06838630623086683, 0.05396580040372212, 0.11719877045787308, 0.11728556202635354, 0.10877341510800764, 0.13737687214969574, 0.11491941825752983, 0.12321020475750588, 0.17579323159248939, 0.1167080765116254, 0.4223083421767979, 0.10923886834534702, 0.11460489027457221, 0.07707524844543545, 0.08090492238986713, 0.07538316982343873, 0.10773123440853183, 0.12134337892638847, 0.09505653338666176, 0.10902249657558516, 0.16228838110557753, 0.16477600899297362, 0.16024665546818995, 0.1976623359549129, 0.19221712386954903, 0.19180879154816466, 0.13700971782055948, 0.2035371608679276, 0.15431689360233392, 0.0679617590485252, 0.048244017138464046, 0.06429049614045379, 0.043122957912617, 0.027064175154754788, 0.058329326620743305, 0.09286251366668208, 0.08898066589634634, 0.07818078192535938, 0.1100445331563471, 0.09450330575011978, 0.14563867417797427, 0.0952524254694781, 0.10672824223116673, 0.10371731265976813, 0.10343075699104554, 0.07894266225832158, 0.10228294689429418, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.14624841595363014, 0.0748129040140193, 0.11999742401947355, 0.05023071565237491, 0.039912894918371467, 0.0513566374163027, 0.10187277943709194, 0.059607058753598174, 0.06286108585592609, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.028136425406301346, 0.0319796219285019, 0.014861379135003183, 0.026980772416960064, 0.030086017413344046, 0.04476145243342522, 0.04406235089789168, 0.031310629937472134, 0.0460707520278113, 0.23146488681508914, 0.2289956036427926, 0.2285327795640888, 0.22349837929575744, 0.23759985892881708, 0.23671188247266917, 0.23602558440459032, 0.2750052616221369, 0.2465052034385279, 0.05565448689250063, 0.05731809248082642, 0.04496277154980288, 0.06903415749927755, 0.05477211640463209, 0.0759879654399298, 0.06622139348703326, 0.06367268083629751, 0.06789085141451634, 0.12199663174315989, 0.11354735894134826, 0.1268905682394439, 0.14819526836312902, 0.12586078356866093, 0.11284826382528113, 0.14412068462858751, 0.11025797277233629, 0.1165902552198278, 0.1700652433748885, 0.17011656661406105, 0.16712920844718504, 0.18518029070303732, 0.1828664848526057, 0.18829298825168173, 0.16274197955147718, 0.1818420310192208, 0.17914147410576786, 0.12783469728239338, 0.14586742388371898, 0.12918729912296567, 0.13552492108829317, 0.1347604160060648, 0.14542196580720534, 0.12011657195964032, 0.11821764028279047, 0.11557487740023786, 0.1739479771788166, 0.16976530187269, 0.19764432414696187, 0.2003659214818213, 0.21725549136899125, 0.17515609264463128, 0.17173154312953887, 0.17344428380982813, 0.1910210573509823, 0.165936436078174, 0.16029127585699066, 0.15899175507485852, 0.15505118906518378, 0.15866467921385063, 0.16404139940823992, 0.15200406058278038, 0.16720557889005871, 0.15082631615488595, 0.15345185316065568, 0.1625453168333243, 0.14887119523813852, 0.20880288552164705, 0.24861224376301483, 0.22493245389807992, 0.20492017722028344, 0.18133801453006637, 0.17345796712169392, 0.12370906439443863, 0.2544643358417745, 0.11438988428632169, 0.13624433668308356, 0.1309152616831707, 0.15710937906719535, 0.15966780654900126, 0.14746251135011768, 0.15197161720751196, 0.17132792820735043, 0.16142327872691709, 0.20240387443935182, 0.17763099971982987, 0.1700879650088014, 0.1664359294748834, 0.17308887258085048, 0.1911039606265036, 0.17029759568907243, 0.06563843472384523, 0.06443002231767747, 0.05280583949063, 0.05944136350165685, 0.06104731732815216, 0.06997580802177139, 0.053843464953002185, 0.056005271025003855, 0.06733545359469095]}, "mutation_prompt": null}
{"id": "e0583d3b-3ce1-45dc-b2e5-bfea05297ed1", "solution": "import numpy as np\nimport random\n\nclass Novel_Adaptive_Hybrid_Differential_Evolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.F = 0.5  # initial frequency\n        self.CR = 0.5  # initial crossover rate\n        self.pop_size = 50  # initial population size\n        self.population = np.random.uniform(-5.0, 5.0, (self.pop_size, self.dim))\n        self.best_solution = np.inf\n        self.probability = 0.2  # increased probability of probabilistic exploration\n        self.temperature = 100  # initial temperature for simulated annealing\n        self.covariance_matrix = np.eye(self.dim)  # initial covariance matrix\n        self.adaptive_step_size = 0.1  # initial adaptive step size for covariance matrix adaptation\n        self.step_size_learning_rate = 0.001  # learning rate for adaptive step size\n        self.history = np.zeros((self.budget, self.dim))  # history of best solutions\n        self.multi_modal_exploration = 0.5  # initial probability of multi-modal exploration\n\n    def evaluate(self, x):\n        return self.func(x)\n\n    def func(self, x):\n        # BBOB test suite of 24 noiseless functions\n        # Here we use the first function f1(x) = sum(x_i^2)\n        return np.sum(x**2)\n\n    def frequency_modulation(self, x):\n        # frequency modulation function\n        return self.F * np.sin(2 * np.pi * self.F * x)\n\n    def adaptive_frequency_modulation(self):\n        # adaptive frequency modulation\n        self.F *= 1.1  # increase frequency\n        if random.random() < self.probability:\n            self.F *= 0.9  # decrease frequency with probability\n        return self.F\n\n    def crossover(self, x1, x2):\n        # crossover operation\n        r = np.random.rand(self.dim)\n        return x1 + r * (x2 - x1)\n\n    def selection(self, x1, x2):\n        # selection operation\n        if self.evaluate(x2) < self.evaluate(x1):\n            return x2\n        else:\n            return x1\n\n    def simulated_annealing(self, x):\n        # simulated annealing\n        new_x = x + np.random.normal(0, 1, self.dim)\n        new_x = np.clip(new_x, -5.0, 5.0)\n        if self.evaluate(new_x) < self.evaluate(x):\n            return new_x\n        else:\n            probability = np.exp(-(self.evaluate(new_x) - self.evaluate(x)) / self.temperature)\n            if random.random() < probability:\n                return new_x\n            else:\n                return x\n        self.temperature *= 0.99  # decrease temperature with a higher rate\n\n    def covariance_matrix_adaptation(self):\n        # covariance matrix adaptation\n        self.covariance_matrix = (1 - self.adaptive_step_size) * self.covariance_matrix + self.adaptive_step_size * np.eye(self.dim)\n        return self.covariance_matrix\n\n    def adaptive_step_size_learning(self):\n        # adaptive step size learning\n        self.adaptive_step_size += self.step_size_learning_rate * (self.evaluate(self.population[0]) - self.evaluate(self.population[-1]))\n        return self.adaptive_step_size\n\n    def history_learning(self):\n        # history learning\n        self.history = np.vstack((self.history, self.population[0]))\n\n    def multi_modal_exploration(self):\n        # multi-modal exploration\n        if random.random() < self.multi_modal_exploration:\n            self.population = np.random.uniform(-5.0, 5.0, (self.pop_size, self.dim))\n            self.multi_modal_exploration *= 0.9  # decrease probability of multi-modal exploration\n\n    def optimize(self, func):\n        for i in range(self.budget):\n            # evaluate population\n            fitness = [self.evaluate(x) for x in self.population]\n            # get best solution\n            self.best_solution = min(fitness)\n            best_index = fitness.index(self.best_solution)\n            # adaptive frequency modulation\n            self.F = self.adaptive_frequency_modulation()\n            # differential evolution\n            for j in range(self.pop_size):\n                # generate trial vector\n                trial = self.crossover(self.population[j], self.population[random.randint(0, self.pop_size - 1)])\n                # evaluate trial vector\n                trial_fitness = self.evaluate(trial)\n                # selection\n                self.population[j] = self.selection(self.population[j], trial)\n                # update best solution\n                if trial_fitness < self.evaluate(self.population[j]):\n                    self.population[j] = trial\n            # simulated annealing\n            for j in range(self.pop_size):\n                self.population[j] = self.simulated_annealing(self.population[j])\n            # covariance matrix adaptation\n            self.covariance_matrix = self.covariance_matrix_adaptation()\n            # adaptive step size learning\n            self.adaptive_step_size = self.adaptive_step_size_learning()\n            # history learning\n            self.history_learning()\n            # multi-modal exploration\n            self.multi_modal_exploration()\n        return self.best_solution, self.history\n\n    def __call__(self, func):\n        self.func = func\n        return self.optimize(func)\n\n# example usage\nbudget = 1000\ndim = 10\noptimizer = Novel_Adaptive_Hybrid_Differential_Evolution(budget, dim)\nbest_solution, history = optimizer(lambda x: np.sum(x**2))\nprint(\"Best solution:\", best_solution)\nprint(\"History:\", history)", "name": "Novel_Adaptive_Hybrid_Differential_Evolution", "description": "Novel Adaptive Hybrid Differential Evolution with Multi-Modal Exploration and Covariance Matrix Adaptation for Black Box Optimization.", "configspace": "", "generation": 69, "fitness": -Infinity, "feedback": "An exception occurred: TypeError(\"'float' object is not callable\").", "error": "TypeError(\"'float' object is not callable\")", "parent_id": "62f0f28d-b1f3-482e-a59e-689769ae92f8", "metadata": {}, "mutation_prompt": null}
{"id": "f16e047a-aa35-4da6-8146-b0842e6528ab", "solution": "import numpy as np\nimport random\n\nclass Adaptive_Frequency_Modulation_Enhanced_Differential_Evolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.F = 0.5  # initial frequency\n        self.CR = 0.5  # initial crossover rate\n        self.pop_size = 50  # initial population size\n        self.population = np.random.uniform(-5.0, 5.0, (self.pop_size, self.dim))\n        self.best_solution = np.inf\n        self.probability = 0.25  # increased probability of probabilistic exploration\n        self.temperature = 100  # initial temperature for simulated annealing\n        self.covariance_matrix = np.eye(self.dim)  # initial covariance matrix\n        self.adaptive_step_size = 0.1  # initial adaptive step size for covariance matrix adaptation\n        self.step_size_learning_rate = 0.001  # learning rate for adaptive step size\n        self.history = np.zeros((self.budget, self.dim))  # history of best solutions\n        self.history_index = 0  # index for history learning\n\n    def evaluate(self, x):\n        return self.func(x)\n\n    def func(self, x):\n        # BBOB test suite of 24 noiseless functions\n        # Here we use the first function f1(x) = sum(x_i^2)\n        return np.sum(x**2)\n\n    def frequency_modulation(self, x):\n        # frequency modulation function\n        return self.F * np.sin(2 * np.pi * self.F * x)\n\n    def adaptive_frequency_modulation(self):\n        # adaptive frequency modulation\n        self.F *= 1.1  # increase frequency\n        if random.random() < self.probability:\n            self.F *= 0.9  # decrease frequency with probability\n        return self.F\n\n    def crossover(self, x1, x2):\n        # crossover operation\n        r = np.random.rand(self.dim)\n        return x1 + r * (x2 - x1)\n\n    def selection(self, x1, x2):\n        # selection operation\n        if self.evaluate(x2) < self.evaluate(x1):\n            return x2\n        else:\n            return x1\n\n    def simulated_annealing(self, x):\n        # simulated annealing\n        new_x = x + np.random.normal(0, 1, self.dim)\n        new_x = np.clip(new_x, -5.0, 5.0)\n        if self.evaluate(new_x) < self.evaluate(x):\n            return new_x\n        else:\n            probability = np.exp(-(self.evaluate(new_x) - self.evaluate(x)) / self.temperature)\n            if random.random() < probability:\n                return new_x\n            else:\n                return x\n        self.temperature *= 0.995  # decrease temperature with a higher rate\n\n    def covariance_matrix_adaptation(self):\n        # covariance matrix adaptation\n        self.covariance_matrix = (1 - self.adaptive_step_size) * self.covariance_matrix + self.adaptive_step_size * np.eye(self.dim)\n        return self.covariance_matrix\n\n    def adaptive_step_size_learning(self):\n        # adaptive step size learning\n        self.adaptive_step_size += self.step_size_learning_rate * (self.evaluate(self.population[0]) - self.evaluate(self.population[-1]))\n        return self.adaptive_step_size\n\n    def history_learning(self):\n        # history learning\n        if self.history_index < self.budget:\n            self.history[self.history_index] = self.population[0]\n            self.history_index += 1\n\n    def optimize(self, func):\n        for i in range(self.budget):\n            # evaluate population\n            fitness = [self.evaluate(x) for x in self.population]\n            # get best solution\n            self.best_solution = min(fitness)\n            best_index = fitness.index(self.best_solution)\n            # adaptive frequency modulation\n            self.F = self.adaptive_frequency_modulation()\n            # differential evolution\n            for j in range(self.pop_size):\n                # generate trial vector\n                trial = self.crossover(self.population[j], self.population[random.randint(0, self.pop_size - 1)])\n                # evaluate trial vector\n                trial_fitness = self.evaluate(trial)\n                # selection\n                self.population[j] = self.selection(self.population[j], trial)\n                # update best solution\n                if trial_fitness < self.evaluate(self.population[j]):\n                    self.population[j] = trial\n            # simulated annealing\n            for j in range(self.pop_size):\n                self.population[j] = self.simulated_annealing(self.population[j])\n            # covariance matrix adaptation\n            self.covariance_matrix = self.covariance_matrix_adaptation()\n            # adaptive step size learning\n            self.adaptive_step_size = self.adaptive_step_size_learning()\n            # history learning\n            self.history_learning()\n        return self.best_solution, self.history\n\n    def __call__(self, func):\n        self.func = func\n        return self.optimize(func)\n\n# example usage\nbudget = 1000\ndim = 10\noptimizer = Adaptive_Frequency_Modification_Enhanced_Differential_Evolution(budget, dim)\nbest_solution, history = optimizer(lambda x: np.sum(x**2))\nprint(\"Best solution:\", best_solution)\nprint(\"History:\", history)", "name": "Adaptive_Frequency_Modulation_Enhanced_Differential_Evolution", "description": "Adaptive Frequency Modulation Enhanced Differential Evolution with Covariance Matrix Adaptation and Simulated Annealing with Adaptive Temperature Cooling Schedule and Adaptive Step Size Learning using History Learning and Frequency Modulation.", "configspace": "", "generation": 70, "fitness": -Infinity, "feedback": "An exception occurred: NameError(\"name 'Adaptive_Frequency_Modification_Enhanced_Differential_Evolution' is not defined\").", "error": "NameError(\"name 'Adaptive_Frequency_Modification_Enhanced_Differential_Evolution' is not defined\")", "parent_id": "62f0f28d-b1f3-482e-a59e-689769ae92f8", "metadata": {}, "mutation_prompt": null}
{"id": "84b6a1c3-2951-405a-9d95-29fe89dbcca3", "solution": "import numpy as np\nimport random\n\nclass Dynamically_Adaptive_Frequency_Modulation:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.F = 0.5  # initial frequency\n        self.CR = 0.5  # initial crossover rate\n        self.pop_size = 50  # initial population size\n        self.population = np.random.uniform(-5.0, 5.0, (self.pop_size, self.dim))\n        self.best_solution = np.inf\n        self.probability = 0.2  # increased probability of probabilistic exploration\n        self.temperature = 100  # initial temperature for simulated annealing\n        self.covariance_matrix = np.eye(self.dim)  # initial covariance matrix\n        self.adaptive_step_size = 0.1  # initial adaptive step size for covariance matrix adaptation\n        self.step_size_learning_rate = 0.001  # learning rate for adaptive step size\n        self.history = np.zeros((self.budget, self.dim))  # history of best solutions\n        self.adaptive_cooling_rate = 0.99  # adaptive cooling rate for simulated annealing\n\n    def evaluate(self, x):\n        return self.func(x)\n\n    def func(self, x):\n        # BBOB test suite of 24 noiseless functions\n        # Here we use the first function f1(x) = sum(x_i^2)\n        return np.sum(x**2)\n\n    def frequency_modulation(self, x):\n        # frequency modulation function\n        return self.F * np.sin(2 * np.pi * self.F * x)\n\n    def adaptive_frequency_modulation(self):\n        # adaptive frequency modulation\n        self.F *= 1.1  # increase frequency\n        if random.random() < self.probability:\n            self.F *= 0.9  # decrease frequency with probability\n        return self.F\n\n    def crossover(self, x1, x2):\n        # crossover operation\n        r = np.random.rand(self.dim)\n        return x1 + r * (x2 - x1)\n\n    def selection(self, x1, x2):\n        # selection operation\n        if self.evaluate(x2) < self.evaluate(x1):\n            return x2\n        else:\n            return x1\n\n    def simulated_annealing(self, x):\n        # simulated annealing\n        new_x = x + np.random.normal(0, 1, self.dim)\n        new_x = np.clip(new_x, -5.0, 5.0)\n        if self.evaluate(new_x) < self.evaluate(x):\n            return new_x\n        else:\n            probability = np.exp(-(self.evaluate(new_x) - self.evaluate(x)) / self.temperature)\n            if random.random() < probability:\n                return new_x\n            else:\n                return x\n        self.temperature *= self.adaptive_cooling_rate  # decrease temperature with adaptive cooling rate\n\n    def covariance_matrix_adaptation(self):\n        # covariance matrix adaptation\n        self.covariance_matrix = (1 - self.adaptive_step_size) * self.covariance_matrix + self.adaptive_step_size * np.eye(self.dim)\n        return self.covariance_matrix\n\n    def adaptive_step_size_learning(self):\n        # adaptive step size learning\n        self.adaptive_step_size += self.step_size_learning_rate * (self.evaluate(self.population[0]) - self.evaluate(self.population[-1]))\n        return self.adaptive_step_size\n\n    def history_learning(self):\n        # history learning\n        self.history = np.vstack((self.history, self.population[0]))\n\n    def optimize(self, func):\n        for i in range(self.budget):\n            # evaluate population\n            fitness = [self.evaluate(x) for x in self.population]\n            # get best solution\n            self.best_solution = min(fitness)\n            best_index = fitness.index(self.best_solution)\n            # adaptive frequency modulation\n            self.F = self.adaptive_frequency_modulation()\n            # differential evolution\n            for j in range(self.pop_size):\n                # generate trial vector\n                trial = self.crossover(self.population[j], self.population[random.randint(0, self.pop_size - 1)])\n                # evaluate trial vector\n                trial_fitness = self.evaluate(trial)\n                # selection\n                self.population[j] = self.selection(self.population[j], trial)\n                # update best solution\n                if trial_fitness < self.evaluate(self.population[j]):\n                    self.population[j] = trial\n            # simulated annealing\n            for j in range(self.pop_size):\n                self.population[j] = self.simulated_annealing(self.population[j])\n            # covariance matrix adaptation\n            self.covariance_matrix = self.covariance_matrix_adaptation()\n            # adaptive step size learning\n            self.adaptive_step_size = self.adaptive_step_size_learning()\n            # history learning\n            self.history_learning()\n        return self.best_solution, self.history\n\n    def __call__(self, func):\n        self.func = func\n        return self.optimize(func)\n\n# example usage\nbudget = 1000\ndim = 10\noptimizer = Dynamically_Adaptive_Frequency_Modulation(budget, dim)\nbest_solution, history = optimizer(lambda x: np.sum(x**2))\nprint(\"Best solution:\", best_solution)\nprint(\"History:\", history)", "name": "Dynamically_Adaptive_Frequency_Modulation", "description": "A Novel Metaheuristic Algorithm, called \"Dynamically Adaptive Frequency Modulation with Covariance Matrix Adaptation and Simulated Annealing with Adaptive Cooling Schedule and History-Based Learning\" which combines adaptive frequency modulation, covariance matrix adaptation, and simulated annealing with adaptive cooling schedule and history-based learning to efficiently explore the search space and adapt to the problem characteristics.", "configspace": "", "generation": 71, "fitness": 0.10960325179076741, "feedback": "The algorithm Dynamically_Adaptive_Frequency_Modulation got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.11 with standard deviation 0.07.", "error": "", "parent_id": "62f0f28d-b1f3-482e-a59e-689769ae92f8", "metadata": {"aucs": [0.23118413179707753, 0.2152984458241416, 0.22167908371153533, 0.20869347128560634, 0.2083740579303509, 0.20515828938258096, 0.24074332312504443, 0.21274790223632922, 0.1928772636060978, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.0001071441296673914, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.05696435985803128, 0.06734214921854476, 0.058183415931188365, 0.07060497457170334, 0.07112055681503915, 0.06417032644480647, 0.07396354433884611, 0.06118600249749795, 0.06442949069944826, 0.04954216599775696, 0.058509253961430696, 0.04884328307830477, 0.0517003769719766, 0.05268531359884254, 0.039424142355697245, 0.06171026558476567, 0.0513997327071436, 0.04513070449708756, 0.22169771840412478, 0.11747102855063618, 0.08421031573325233, 0.15891557904868314, 0.10649900911762322, 0.10943383577849197, 0.17491712974514573, 0.11208873320880142, 0.16439870095380316, 0.08238458557999584, 0.08923144142788975, 0.0935027241738009, 0.07591125392340414, 0.06445035437683311, 0.08965920263611404, 0.10322594967331811, 0.08816186405426729, 0.09747913914659223, 0.16659504336675668, 0.16148704415259452, 0.1417982115746892, 0.16827731295445159, 0.19461290616357996, 0.1594782844710937, 0.1727918463201551, 0.15299335081087195, 0.14456030780137263, 0.07035889216226243, 0.06524034441160587, 0.0764737458907111, 0.07996588582930197, 0.057365024331040515, 0.047062349156883676, 0.08362839515132925, 0.09188457909028125, 0.11273112758604908, 0.0909000015608441, 0.11815228058040994, 0.09270540290977569, 0.09078913094112917, 0.10246398415111035, 0.12424965771308216, 0.11032967049183084, 0.09453768204007007, 0.10122994605774438, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.08755811419667825, 0.07030717394606223, 0.08476857496489376, 0.033348867224395895, 0.06000215038640877, 0.036682046452525374, 0.07026740756942051, 0.13176879158618127, 0.031007451798460206, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.014406229917954594, 0.01737035425969502, 0.013828296655677308, 0.018323517140406698, 0.020898206021148114, 0.031172326029837327, 0.038832870530711605, 0.037819722401317035, 0.04500148717366992, 0.21626429292708582, 0.23029959181297477, 0.21842082101297688, 0.2637896786646551, 0.2266031073426762, 0.22577442774760337, 0.22784819906748033, 0.1860451279058266, 0.27234298360257014, 0.05996517343043051, 0.05711769535118305, 0.05655885845498865, 0.06219907014955206, 0.0551947738338342, 0.06355561714562974, 0.0681362549986545, 0.060813340834872776, 0.06630885240554163, 0.1380780870330509, 0.10290000143689482, 0.1281308626836637, 0.12587655624115945, 0.12415409992004256, 0.12158394245602355, 0.10400436716953954, 0.1166027166801954, 0.10587422812836556, 0.17257374004721804, 0.17309835235228532, 0.18270380797298025, 0.185655329525371, 0.19672776461958585, 0.18607391962121578, 0.16020063771891002, 0.1742027414513595, 0.16789447603178254, 0.13103078798650936, 0.11052304107517852, 0.11742377337737431, 0.12853019841814262, 0.13353181688395366, 0.1331914229524065, 0.10733872907865749, 0.11992627740513306, 0.11551175731132457, 0.2196699756620103, 0.18373375823621185, 0.19484260404678166, 0.1763253907314225, 0.19499261690016179, 0.168923277625586, 0.16833843914297086, 0.167815852619438, 0.18560868280090426, 0.1721706367914878, 0.16214627542360927, 0.16521114068345322, 0.15709874154971182, 0.161052636603375, 0.15565247315614883, 0.15809485897713416, 0.16021658031608987, 0.16071027300108964, 0.2073477989828012, 0.16390502685869124, 0.15552965208231928, 0.17666780530745163, 0.20905771393535866, 0.20413269981549487, 0.15274833150354483, 0.20499429774790912, 0.20414439740483226, 0.14884947742138288, 0.1893489472034593, 0.15746831658295868, 0.21032760168697096, 0.14894518994724681, 0.1359100268351776, 0.1431688428876723, 0.15173457093798715, 0.16496435965973832, 0.17613860143765558, 0.16612290708935196, 0.1630982420726066, 0.17603841691142785, 0.16877164598387828, 0.1557205902388048, 0.16427324772643548, 0.1791601673410731, 0.1675378911913642, 0.06122495626462643, 0.06623153115546176, 0.06668295734900365, 0.06803192935034952, 0.0671931321491589, 0.05560901247765415, 0.07022799314245165, 0.07353019990737775, 0.07024065806584256]}, "mutation_prompt": null}
{"id": "25c7d282-3688-45f6-930e-8106ead44207", "solution": "import numpy as np\nimport random\n\nclass Enhanced_Adaptive_Differential_Evolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.F = 0.5  # initial frequency\n        self.CR = 0.5  # initial crossover rate\n        self.pop_size = 50  # initial population size\n        self.population = np.random.uniform(-5.0, 5.0, (self.pop_size, self.dim))\n        self.best_solution = np.inf\n        self.probability = 0.2  # increased probability of probabilistic exploration\n        self.temperature = 100  # initial temperature for simulated annealing\n        self.covariance_matrix = np.eye(self.dim)  # initial covariance matrix\n        self.adaptive_step_size = 0.1  # initial adaptive step size for covariance matrix adaptation\n        self.step_size_learning_rate = 0.001  # learning rate for adaptive step size\n        self.history = np.zeros((self.budget, self.dim))  # history of best solutions\n        self.pollination_rate = 0.1  # pollination rate for flower pollination algorithm\n        self.pollination_directions = np.random.uniform(-5.0, 5.0, (self.pop_size, self.dim))  # initial pollination directions\n\n    def evaluate(self, x):\n        return self.func(x)\n\n    def func(self, x):\n        # BBOB test suite of 24 noiseless functions\n        # Here we use the first function f1(x) = sum(x_i^2)\n        return np.sum(x**2)\n\n    def frequency_modulation(self, x):\n        # frequency modulation function\n        return self.F * np.sin(2 * np.pi * self.F * x)\n\n    def adaptive_frequency_modulation(self):\n        # adaptive frequency modulation\n        self.F *= 1.1  # increase frequency\n        if random.random() < self.probability:\n            self.F *= 0.9  # decrease frequency with probability\n        return self.F\n\n    def crossover(self, x1, x2):\n        # crossover operation\n        r = np.random.rand(self.dim)\n        return x1 + r * (x2 - x1)\n\n    def selection(self, x1, x2):\n        # selection operation\n        if self.evaluate(x2) < self.evaluate(x1):\n            return x2\n        else:\n            return x1\n\n    def simulated_annealing(self, x):\n        # simulated annealing\n        new_x = x + np.random.normal(0, 1, self.dim)\n        new_x = np.clip(new_x, -5.0, 5.0)\n        if self.evaluate(new_x) < self.evaluate(x):\n            return new_x\n        else:\n            probability = np.exp(-(self.evaluate(new_x) - self.evaluate(x)) / self.temperature)\n            if random.random() < probability:\n                return new_x\n            else:\n                return x\n        self.temperature *= 0.99  # decrease temperature with a higher rate\n\n    def covariance_matrix_adaptation(self):\n        # covariance matrix adaptation\n        self.covariance_matrix = (1 - self.adaptive_step_size) * self.covariance_matrix + self.adaptive_step_size * np.eye(self.dim)\n        return self.covariance_matrix\n\n    def adaptive_step_size_learning(self):\n        # adaptive step size learning\n        self.adaptive_step_size += self.step_size_learning_rate * (self.evaluate(self.population[0]) - self.evaluate(self.population[-1]))\n        return self.adaptive_step_size\n\n    def history_learning(self):\n        # history learning\n        self.history = np.vstack((self.history, self.population[0]))\n\n    def pollination(self, x):\n        # pollination operation\n        new_x = x + self.pollination_directions[random.randint(0, self.pop_size - 1)]\n        new_x = np.clip(new_x, -5.0, 5.0)\n        return new_x\n\n    def optimize(self, func):\n        for i in range(self.budget):\n            # evaluate population\n            fitness = [self.evaluate(x) for x in self.population]\n            # get best solution\n            self.best_solution = min(fitness)\n            best_index = fitness.index(self.best_solution)\n            # adaptive frequency modulation\n            self.F = self.adaptive_frequency_modulation()\n            # differential evolution\n            for j in range(self.pop_size):\n                # generate trial vector\n                trial = self.crossover(self.population[j], self.population[random.randint(0, self.pop_size - 1)])\n                # evaluate trial vector\n                trial_fitness = self.evaluate(trial)\n                # selection\n                self.population[j] = self.selection(self.population[j], trial)\n                # update best solution\n                if trial_fitness < self.evaluate(self.population[j]):\n                    self.population[j] = trial\n            # simulated annealing\n            for j in range(self.pop_size):\n                self.population[j] = self.simulated_annealing(self.population[j])\n            # covariance matrix adaptation\n            self.covariance_matrix = self.covariance_matrix_adaptation()\n            # adaptive step size learning\n            self.adaptive_step_size = self.adaptive_step_size_learning()\n            # history learning\n            self.history_learning()\n            # flower pollination algorithm\n            for j in range(int(self.pollination_rate * self.pop_size)):\n                self.population[j] = self.pollination(self.population[j])\n        return self.best_solution, self.history\n\n    def __call__(self, func):\n        self.func = func\n        return self.optimize(func)\n\n# example usage\nbudget = 1000\ndim = 10\noptimizer = Enhanced_Adaptive_Differential_Evolution(budget, dim)\nbest_solution, history = optimizer(lambda x: np.sum(x**2))\nprint(\"Best solution:\", best_solution)\nprint(\"History:\", history)", "name": "Enhanced_Adaptive_Differential_Evolution", "description": "Enhanced Adaptive Differential Evolution with Self-Adaptive Frequency Modulation, Covariance Matrix Adaptation, and Simulated Annealing with Adaptive Temperature Cooling Schedule and Adaptive Step Size Learning, and a Novel Hybridization with the Flower Pollination Algorithm.", "configspace": "", "generation": 72, "fitness": 0.11220375719243113, "feedback": "The algorithm Enhanced_Adaptive_Differential_Evolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.11 with standard deviation 0.08.", "error": "", "parent_id": "62f0f28d-b1f3-482e-a59e-689769ae92f8", "metadata": {"aucs": [0.21428711192994743, 0.23157807600912894, 0.29538536031066487, 0.20357828679062928, 0.21184279800298567, 0.2061401591599452, 0.2281555076643884, 0.20417177699994749, 0.22003871358740756, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.012595408384641127, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.05876440464210719, 0.06181717595063674, 0.05334229647076061, 0.06456251814638403, 0.06061632901137837, 0.0581089100522828, 0.0694600300239725, 0.05908023693871256, 0.061509882049159414, 0.047373600762434354, 0.05005323457951216, 0.053812909027310174, 0.05996924879812815, 0.05950438089089838, 0.05675184301510705, 0.050496374458590565, 0.047887363239131786, 0.044896021539325925, 0.10285146187673722, 0.24059174115056114, 0.08874476726049785, 0.11766840012118451, 0.12211436713558654, 0.1406511064828897, 0.1409655341371162, 0.7397955146457715, 0.2022544176076866, 0.0808949025253769, 0.07643846695141598, 0.08728294445078422, 0.0833467145860124, 0.10298786040353025, 0.06899694288396874, 0.1028661188241391, 0.08560074269682028, 0.1105043195241382, 0.16468833801052896, 0.18477865023749795, 0.16448697187281558, 0.15798035184279235, 0.1866414944024326, 0.15132805457596754, 0.14103892310660926, 0.1514909750278628, 0.16247404917225228, 0.06237848979486793, 0.06211071748279484, 0.06104150211691939, 0.06359058589805122, 0.042960491823049396, 0.04652745717357043, 0.07714981020153389, 0.06663137284875797, 0.07161416422121669, 0.09210772497041764, 0.08583949874285413, 0.10341382148422851, 0.1006755190606875, 0.10247055646163994, 0.0848307986063419, 0.13777671935027425, 0.09315657887053441, 0.08069448447806338, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.07630329674730874, 0.04841832870553209, 0.07968043660931867, 0.0538517978802775, 0.05260525434845109, 0.04370472044903073, 0.06046298299845754, 0.06053887115277956, 0.07060306641957281, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.07803372349118565, 0.024122502172319593, 0.03489523185952126, 0.041629312222368986, 0.03294135462473191, 0.051459299838760075, 0.03102505411392542, 0.04908441123484297, 0.026566782306172043, 0.23452569993193917, 0.23721594661727097, 0.2516579114856179, 0.21089426477272122, 0.21663549903748458, 0.2725797186784824, 0.2235821023412472, 0.22656609625587565, 0.22548725562194016, 0.04358607695439476, 0.047864903694833316, 0.05508111170896757, 0.06774809629238732, 0.05416792598316911, 0.06191638436946345, 0.06668098219382645, 0.058849152728888465, 0.055843885647681835, 0.12257148290646058, 0.1302407254807213, 0.11842175776965747, 0.15931299847655755, 0.12986090313549725, 0.14004614135186144, 0.11736605350982776, 0.1067123929618714, 0.11518820503419125, 0.1673666128585104, 0.18215904018655882, 0.17827577781971704, 0.20134226808240419, 0.19536199843896407, 0.19109102694406233, 0.17611054224475398, 0.18372007824827152, 0.1769201492072936, 0.13046723186598486, 0.1124962069528892, 0.1324076003814052, 0.14466310283182826, 0.141264563249906, 0.13608280768156256, 0.13515190068104566, 0.14089748927932544, 0.12048963238432375, 0.17778366176914584, 0.18209925015666462, 0.17633284114878245, 0.1699704022849211, 0.16671911850446575, 0.20287394969353278, 0.18072464115590448, 0.17035063799900185, 0.16964398301117634, 0.1657421516401093, 0.15356921647170219, 0.15780915461302758, 0.15590886208604737, 0.15822573398619055, 0.16708863110842576, 0.15579790153133322, 0.16614354807227227, 0.15458446311196716, 0.16241475130028793, 0.147718595062307, 0.15025288103170498, 0.21241328467153076, 0.1483383049140442, 0.14730727031896362, 0.17944983939834258, 0.1495201019218384, 0.19487539084755046, 0.21676640065525266, 0.14966786379512464, 0.16178399446099556, 0.16007457443484363, 0.1568369219053427, 0.13542199166538504, 0.15094189774765, 0.1634022318456858, 0.1259342841690574, 0.17520886637396582, 0.17008500377269442, 0.17656871202366653, 0.2205041562290525, 0.16878060033477926, 0.175920781758251, 0.16459876603447754, 0.16337763557792406, 0.17471160791540497, 0.06083504572910403, 0.06598091734445422, 0.05686087619474911, 0.05986932650899157, 0.06567799272260577, 0.05764821880843607, 0.0656455677324268, 0.06096638910421204, 0.0526698459188476]}, "mutation_prompt": null}
{"id": "d2679303-58b4-4c59-a6e1-2c011d97f65a", "solution": "import numpy as np\nimport random\n\nclass Enhanced_Adaptive_Differential_Evolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.F = 0.5  # initial frequency\n        self.CR = 0.5  # initial crossover rate\n        self.pop_size = 50  # initial population size\n        self.population = np.random.uniform(-5.0, 5.0, (self.pop_size, self.dim))\n        self.best_solution = np.inf\n        self.probability = 0.4  # increased probability of probabilistic exploration\n        self.temperature = 100  # initial temperature for simulated annealing\n        self.covariance_matrix = np.eye(self.dim)  # initial covariance matrix\n        self.adaptive_step_size = 0.1  # initial adaptive step size for covariance matrix adaptation\n        self.step_size_learning_rate = 0.001  # learning rate for adaptive step size\n        self.history = np.zeros((self.budget, self.dim))  # history of best solutions\n        self.history_index = 0\n\n    def evaluate(self, x):\n        return self.func(x)\n\n    def func(self, x):\n        # BBOB test suite of 24 noiseless functions\n        # Here we use the first function f1(x) = sum(x_i^2)\n        return np.sum(x**2)\n\n    def frequency_modulation(self, x):\n        # frequency modulation function\n        return self.F * np.sin(2 * np.pi * self.F * x)\n\n    def adaptive_frequency_modulation(self):\n        # adaptive frequency modulation\n        self.F *= 1.1  # increase frequency\n        if random.random() < self.probability:\n            self.F *= 0.9  # decrease frequency with probability\n        return self.F\n\n    def crossover(self, x1, x2):\n        # crossover operation\n        r = np.random.rand(self.dim)\n        return x1 + r * (x2 - x1)\n\n    def selection(self, x1, x2):\n        # selection operation\n        if self.evaluate(x2) < self.evaluate(x1):\n            return x2\n        else:\n            return x1\n\n    def simulated_annealing(self, x):\n        # simulated annealing\n        new_x = x + np.random.normal(0, 1, self.dim)\n        new_x = np.clip(new_x, -5.0, 5.0)\n        if self.evaluate(new_x) < self.evaluate(x):\n            return new_x\n        else:\n            probability = np.exp(-(self.evaluate(new_x) - self.evaluate(x)) / self.temperature)\n            if random.random() < probability:\n                return new_x\n            else:\n                return x\n        self.temperature *= 0.99  # decrease temperature with a higher rate\n\n    def covariance_matrix_adaptation(self):\n        # covariance matrix adaptation\n        self.covariance_matrix = (1 - self.adaptive_step_size) * self.covariance_matrix + self.adaptive_step_size * np.eye(self.dim)\n        return self.covariance_matrix\n\n    def adaptive_step_size_learning(self):\n        # adaptive step size learning\n        self.adaptive_step_size += self.step_size_learning_rate * (self.evaluate(self.population[0]) - self.evaluate(self.population[-1]))\n        return self.adaptive_step_size\n\n    def history_learning(self):\n        # history learning\n        self.history[self.history_index] = self.population[0]\n        self.history_index = (self.history_index + 1) % self.budget\n\n    def optimize(self, func):\n        for i in range(self.budget):\n            # evaluate population\n            fitness = [self.evaluate(x) for x in self.population]\n            # get best solution\n            self.best_solution = min(fitness)\n            best_index = fitness.index(self.best_solution)\n            # adaptive frequency modulation\n            self.F = self.adaptive_frequency_modulation()\n            # differential evolution\n            for j in range(self.pop_size):\n                # generate trial vector\n                trial = self.crossover(self.population[j], self.population[random.randint(0, self.pop_size - 1)])\n                # evaluate trial vector\n                trial_fitness = self.evaluate(trial)\n                # selection\n                self.population[j] = self.selection(self.population[j], trial)\n                # update best solution\n                if trial_fitness < self.evaluate(self.population[j]):\n                    self.population[j] = trial\n            # simulated annealing\n            for j in range(self.pop_size):\n                self.population[j] = self.simulated_annealing(self.population[j])\n            # covariance matrix adaptation\n            self.covariance_matrix = self.covariance_matrix_adaptation()\n            # adaptive step size learning\n            self.adaptive_step_size = self.adaptive_step_size_learning()\n            # history learning\n            self.history_learning()\n        return self.best_solution, self.history\n\n    def __call__(self, func):\n        self.func = func\n        return self.optimize(func)\n\n# example usage\nbudget = 1000\ndim = 10\noptimizer = Enhanced_Adaptive_Differential_Evolution(budget, dim)\nbest_solution, history = optimizer(lambda x: np.sum(x**2))\nprint(\"Best solution:\", best_solution)\nprint(\"History:\", history)", "name": "Enhanced_Adaptive_Differential_Evolution", "description": "Enhanced Adaptive Frequency Modulation with Adaptive Step Size Learning and Covariance Matrix Adaptation with Simulated Annealing and History Learning.", "configspace": "", "generation": 73, "fitness": 0.11118187660834453, "feedback": "The algorithm Enhanced_Adaptive_Differential_Evolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.11 with standard deviation 0.07.", "error": "", "parent_id": "62f0f28d-b1f3-482e-a59e-689769ae92f8", "metadata": {"aucs": [0.21567173671298234, 0.22082071485087185, 0.23310892428336372, 0.2260655750326721, 0.2162731375508654, 0.2226523713127465, 0.22739580751942834, 0.2053590553589555, 0.19731435732971891, 0.0009157106394613868, 0.00014731125587963412, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.059998023358009744, 0.06726422044149316, 0.06487626068198227, 0.06601912472867111, 0.07655078174126506, 0.06770178585687514, 0.05807642760291798, 0.07170626641785127, 0.06127130266390035, 0.04911695637684044, 0.06766952397753523, 0.04341078101417939, 0.06291562319793598, 0.06986588710372799, 0.044475462905776975, 0.05049267079895381, 0.0512671035023734, 0.05973754340448667, 0.10492827893093748, 0.12215318457153446, 0.0974384581662664, 0.15851786255495026, 0.11326661770012147, 0.12522260416666664, 0.16619143631140232, 0.10368291561373266, 0.4154793764540866, 0.10269161645804448, 0.10708459639423529, 0.09047090284352366, 0.09849300472374345, 0.06937326189175586, 0.10722481399771955, 0.1021104399526277, 0.09791959398011407, 0.08843354731142428, 0.17536063496451448, 0.1845080089857809, 0.14832381985948728, 0.16152410387203364, 0.21952456729666836, 0.17127488947917568, 0.14098551378587776, 0.19855017738603697, 0.13925145643651582, 0.08040095168685801, 0.061138894737880034, 0.06960927181675292, 0.04026506696100174, 0.028754663080335363, 0.05429985782638269, 0.08850568219644306, 0.0797693762863777, 0.10641847453069297, 0.0965452107361181, 0.1025494647821219, 0.07284890204926542, 0.09980757938431706, 0.09022376828196665, 0.11260029184298237, 0.10922403248976442, 0.09974559112432724, 0.09269150144137606, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.09541614190000658, 0.06820154551724589, 0.11621056035339039, 0.05048133030488755, 0.03027103672185405, 0.030203653098154204, 0.06855797259035967, 0.10072875812500237, 0.055939018603274104, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.01760970766450587, 0.021167194490718466, 0.041424711615171694, 0.031761275216320106, 0.0368486963782525, 0.033264774843990375, 0.03142022897826724, 0.036016759005187526, 0.03504554751181399, 0.24257523501160116, 0.24351028676812625, 0.23137445843359894, 0.20637484754634072, 0.20432089305690493, 0.21086340137881, 0.21786226938224285, 0.2272124691626486, 0.241304179357396, 0.04331694755596982, 0.05420636653690891, 0.04883744435906723, 0.056745229663995866, 0.06890955473331584, 0.07038629589471002, 0.06639040678625674, 0.062243802273302395, 0.07047077975555205, 0.12571189838299357, 0.1292337505139809, 0.1325592633686996, 0.1317862898581278, 0.11554955210002515, 0.11252343232879225, 0.10908983560306884, 0.10061368946266647, 0.1170015417567487, 0.15564710513146163, 0.17131530612344648, 0.1703865515357189, 0.18027961016448557, 0.19416281614249464, 0.18671036654995055, 0.1819350931819126, 0.18246850161791395, 0.1650398673484489, 0.10832154829389706, 0.12102366853317004, 0.1260866583299819, 0.13278230744553476, 0.1325772042222052, 0.1340653501990211, 0.1191160466628125, 0.11846342175281566, 0.11548377189569525, 0.192173030032736, 0.1895542049734953, 0.19067851582718176, 0.18804699496346544, 0.1684716548843469, 0.17131879708151776, 0.17001938410899098, 0.18487215543786562, 0.16992093244818074, 0.16460942831909287, 0.1542355708515588, 0.15919290116039775, 0.16371620664912512, 0.16033569471093723, 0.15975206117323903, 0.16455718461834035, 0.15505521829101965, 0.1575535768614632, 0.15887112958991, 0.16437269185676773, 0.16843299054666605, 0.16988825522323625, 0.19109190210777904, 0.17614515837379163, 0.20574760081382615, 0.17861279640189964, 0.16741077560559137, 0.22443351676544465, 0.2587558074985763, 0.15399573674498135, 0.1707052964688628, 0.16324207084157916, 0.1624553814694707, 0.2522002919466936, 0.16650051014361889, 0.1338399444234123, 0.1626801387989757, 0.16070606831866374, 0.17568002325451593, 0.16850979697376223, 0.1810533583899402, 0.1958334806245886, 0.1612441834534073, 0.1810491311023924, 0.16482399885992527, 0.06798641997628585, 0.059520671148196924, 0.06491827752272039, 0.05297512468635324, 0.057461445563388724, 0.05385309888550438, 0.05956216947460491, 0.055761140144515386, 0.06746850896273193]}, "mutation_prompt": null}
{"id": "11e1f01d-df5f-40dc-9ac2-8cd236da1dcc", "solution": "import numpy as np\nimport random\n\nclass Adaptive_Hybrid_Metaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.F = 0.5  # initial frequency\n        self.CR = 0.5  # initial crossover rate\n        self.pop_size = 50  # initial population size\n        self.population = np.random.uniform(-5.0, 5.0, (self.pop_size, self.dim))\n        self.best_solution = np.inf\n        self.probability = 0.2  # increased probability of probabilistic exploration\n        self.temperature = 100  # initial temperature for simulated annealing\n        self.covariance_matrix = np.eye(self.dim)  # initial covariance matrix\n        self.adaptive_step_size = 0.1  # initial adaptive step size for covariance matrix adaptation\n        self.step_size_learning_rate = 0.001  # learning rate for adaptive step size\n        self.history = np.zeros((self.budget, self.dim))  # history of best solutions\n        self.history_index = 0  # index for history learning\n\n    def evaluate(self, x):\n        return self.func(x)\n\n    def func(self, x):\n        # BBOB test suite of 24 noiseless functions\n        # Here we use the first function f1(x) = sum(x_i^2)\n        return np.sum(x**2)\n\n    def frequency_modulation(self, x):\n        # frequency modulation function\n        return self.F * np.sin(2 * np.pi * self.F * x)\n\n    def adaptive_frequency_modulation(self):\n        # adaptive frequency modulation\n        self.F *= 1.1  # increase frequency\n        if random.random() < self.probability:\n            self.F *= 0.9  # decrease frequency with probability\n        return self.F\n\n    def crossover(self, x1, x2):\n        # crossover operation\n        r = np.random.rand(self.dim)\n        return x1 + r * (x2 - x1)\n\n    def selection(self, x1, x2):\n        # selection operation\n        if self.evaluate(x2) < self.evaluate(x1):\n            return x2\n        else:\n            return x1\n\n    def simulated_annealing(self, x):\n        # simulated annealing\n        new_x = x + np.random.normal(0, 1, self.dim)\n        new_x = np.clip(new_x, -5.0, 5.0)\n        if self.evaluate(new_x) < self.evaluate(x):\n            return new_x\n        else:\n            probability = np.exp(-(self.evaluate(new_x) - self.evaluate(x)) / self.temperature)\n            if random.random() < probability:\n                return new_x\n            else:\n                return x\n        self.temperature *= 0.99  # decrease temperature with a higher rate\n\n    def covariance_matrix_adaptation(self):\n        # covariance matrix adaptation\n        self.covariance_matrix = (1 - self.adaptive_step_size) * self.covariance_matrix + self.adaptive_step_size * np.eye(self.dim)\n        return self.covariance_matrix\n\n    def adaptive_step_size_learning(self):\n        # adaptive step size learning\n        self.adaptive_step_size += self.step_size_learning_rate * (self.evaluate(self.population[0]) - self.evaluate(self.population[-1]))\n        return self.adaptive_step_size\n\n    def history_learning(self):\n        # history learning\n        self.history[self.history_index] = self.population[0]\n        self.history_index = (self.history_index + 1) % self.budget\n\n    def optimize(self, func):\n        for i in range(self.budget):\n            # evaluate population\n            fitness = [self.evaluate(x) for x in self.population]\n            # get best solution\n            self.best_solution = min(fitness)\n            best_index = fitness.index(self.best_solution)\n            # adaptive frequency modulation\n            self.F = self.adaptive_frequency_modulation()\n            # differential evolution\n            for j in range(self.pop_size):\n                # generate trial vector\n                trial = self.crossover(self.population[j], self.population[random.randint(0, self.pop_size - 1)])\n                # evaluate trial vector\n                trial_fitness = self.evaluate(trial)\n                # selection\n                self.population[j] = self.selection(self.population[j], trial)\n                # update best solution\n                if trial_fitness < self.evaluate(self.population[j]):\n                    self.population[j] = trial\n            # simulated annealing\n            for j in range(self.pop_size):\n                self.population[j] = self.simulated_annealing(self.population[j])\n            # covariance matrix adaptation\n            self.covariance_matrix = self.covariance_matrix_adaptation()\n            # adaptive step size learning\n            self.adaptive_step_size = self.adaptive_step_size_learning()\n            # history learning\n            self.history_learning()\n        return self.best_solution, self.history\n\n    def __call__(self, func):\n        self.func = func\n        return self.optimize(func)\n\n# example usage\nbudget = 1000\ndim = 10\noptimizer = Adaptive_Hybrid_Metaheuristic(budget, dim)\nbest_solution, history = optimizer(lambda x: np.sum(x**2))\nprint(\"Best solution:\", best_solution)\nprint(\"History:\", history)", "name": "Adaptive_Hybrid_Metaheuristic", "description": "Adaptive Hybrid Metaheuristic Algorithm with Probabilistic Frequency Modulation, Covariance Matrix Adaptation, Simulated Annealing, and History Learning.", "configspace": "", "generation": 74, "fitness": 0.10927134357876044, "feedback": "The algorithm Adaptive_Hybrid_Metaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.11 with standard deviation 0.07.", "error": "", "parent_id": "62f0f28d-b1f3-482e-a59e-689769ae92f8", "metadata": {"aucs": [0.21422101355509438, 0.23416219234532698, 0.21755192530856637, 0.20006003413615114, 0.21431842711093518, 0.21263602467120424, 0.21573538648822665, 0.18409923952650253, 0.198871085426937, 0.00567648976002999, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.07465801151176021, 0.072796242938718, 0.06224412641727084, 0.08913334107353854, 0.08298846067579202, 0.061481690330886574, 0.06723058335463283, 0.0568431005982285, 0.06500629852584427, 0.044309684604574295, 0.050174270851872316, 0.04660330831569004, 0.051186152252291284, 0.04377158720047625, 0.04952302999318503, 0.05215907044656165, 0.05519215991780979, 0.04735318906978303, 0.12841348128936447, 0.10765629904955132, 0.08709878464091758, 0.1380716933776719, 0.1280693861772212, 0.13608412359323885, 0.1772509083620053, 0.11193836360552012, 0.13880077605449737, 0.12644552668706655, 0.11312424662485898, 0.07584181547635238, 0.16352578618614677, 0.0808142407697724, 0.07853090325232603, 0.10926099552694579, 0.08752240291002655, 0.08332889929780796, 0.15219263941820993, 0.15123071457633086, 0.16368880688179432, 0.1691954292572735, 0.18914746640292146, 0.15644847244584414, 0.1340286463152497, 0.1478065231074981, 0.1550625217634064, 0.07337956703830695, 0.07355604530386084, 0.07581131105536931, 0.04077192562663967, 0.061439667735146086, 0.02550246621795338, 0.12931427324636602, 0.08106405146934215, 0.09346828037630805, 0.08373155129164134, 0.1160596274867346, 0.08627313602361542, 0.10764891882545158, 0.10006234295534755, 0.09192418992870366, 0.10752524346025816, 0.11458079464031201, 0.08122450720976482, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.073766107199979, 0.0731306545852376, 0.07624044669989183, 0.05699171011467019, 0.0610390128326147, 0.023063127100623415, 0.08381330224158678, 0.0875346335428927, 0.039919053372279856, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.020706272451729624, 0.020912785640358034, 0.04372233168994011, 0.03268623697811912, 0.058009674810874046, 0.03605616141712942, 0.030794303867037742, 0.028064302387267315, 0.03998252386006007, 0.2316820343958187, 0.2386837434387551, 0.23658054637433523, 0.23012460353527953, 0.22436564978732165, 0.22214866535775424, 0.23789071422471553, 0.2266022692034243, 0.23166641078266204, 0.04607095205757983, 0.055320791489565035, 0.052369727418979495, 0.08849015591382081, 0.06097362141006635, 0.07246028166751672, 0.06282675603755683, 0.06591135619933608, 0.0567639585637717, 0.11988834935075687, 0.14755307114791272, 0.1339645231699973, 0.1273780851543057, 0.11324418932263236, 0.1252181139027707, 0.14539355963656453, 0.10860709650101452, 0.11572474770165897, 0.16351224678521792, 0.18697665428251498, 0.17344443903851026, 0.1986135991328528, 0.18123276938322985, 0.19210004543302484, 0.1643280004124006, 0.18294329082246863, 0.1732147823036646, 0.1253226478215218, 0.1303620222896259, 0.10551711514546769, 0.1311426869799288, 0.1235602650020281, 0.12498855940627729, 0.12289998024636428, 0.11563654898106257, 0.11650589544153866, 0.18516717066583777, 0.18723433047403837, 0.18042999500032275, 0.16891477075001127, 0.16355750182670492, 0.20407510365180692, 0.18793512039205584, 0.18537483925217102, 0.17993789355316636, 0.15807129725256086, 0.15997238742886932, 0.16881962477626344, 0.15855484150875232, 0.15570658640405, 0.15986390672648665, 0.16204964813820621, 0.16039301145206342, 0.16360218864991116, 0.16856042155704087, 0.16943330748259178, 0.16208706901732783, 0.2472686195013568, 0.17289531859581786, 0.18634685562491926, 0.1657377632715854, 0.15732664163860266, 0.15771545163512402, 0.1739933652556379, 0.21983745755624162, 0.22906691562094295, 0.15780178448125193, 0.14418762705856336, 0.1425188863857727, 0.12287208740172995, 0.15628731249763272, 0.11477693339184136, 0.15526391823204544, 0.17700320300835015, 0.1714411728456522, 0.16385488663626524, 0.15491280256502338, 0.18693483883596307, 0.17047385970481932, 0.16269767529844792, 0.17994822195542182, 0.0659335783409557, 0.06871382098600665, 0.05085751759371793, 0.061628463373207154, 0.07228929700634945, 0.05711645534170562, 0.057391802134551306, 0.06219814928805678, 0.062434471193757]}, "mutation_prompt": null}
{"id": "a1e09db0-3d46-4862-b61f-e18c47bde8e4", "solution": "import numpy as np\nimport random\n\nclass Adaptive_Multi_Objective_Differential_Evolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.F = 0.5  # initial frequency\n        self.CR = 0.5  # initial crossover rate\n        self.pop_size = 50  # initial population size\n        self.population = np.random.uniform(-5.0, 5.0, (self.pop_size, self.dim))\n        self.best_solution = np.inf\n        self.probability = 0.2  # increased probability of probabilistic exploration\n        self.temperature = 100  # initial temperature for simulated annealing\n        self.covariance_matrix = np.eye(self.dim)  # initial covariance matrix\n        self.adaptive_step_size = 0.1  # initial adaptive step size for covariance matrix adaptation\n        self.step_size_learning_rate = 0.001  # learning rate for adaptive step size\n        self.history = np.zeros((self.budget, self.dim))  # history of best solutions\n        self.multi_objective = 0.5  # initial multi-objective weight\n\n    def evaluate(self, x):\n        return self.func(x)\n\n    def func(self, x):\n        # BBOB test suite of 24 noiseless functions\n        # Here we use the first function f1(x) = sum(x_i^2)\n        return np.sum(x**2)\n\n    def frequency_modulation(self, x):\n        # frequency modulation function\n        return self.F * np.sin(2 * np.pi * self.F * x)\n\n    def adaptive_frequency_modulation(self):\n        # adaptive frequency modulation\n        self.F *= 1.1  # increase frequency\n        if random.random() < self.probability:\n            self.F *= 0.9  # decrease frequency with probability\n        return self.F\n\n    def crossover(self, x1, x2):\n        # crossover operation\n        r = np.random.rand(self.dim)\n        return x1 + r * (x2 - x1)\n\n    def selection(self, x1, x2):\n        # selection operation\n        if self.evaluate(x2) < self.evaluate(x1):\n            return x2\n        else:\n            return x1\n\n    def simulated_annealing(self, x):\n        # simulated annealing\n        new_x = x + np.random.normal(0, 1, self.dim)\n        new_x = np.clip(new_x, -5.0, 5.0)\n        if self.evaluate(new_x) < self.evaluate(x):\n            return new_x\n        else:\n            probability = np.exp(-(self.evaluate(new_x) - self.evaluate(x)) / self.temperature)\n            if random.random() < probability:\n                return new_x\n            else:\n                return x\n        self.temperature *= 0.99  # decrease temperature with a higher rate\n\n    def covariance_matrix_adaptation(self):\n        # covariance matrix adaptation\n        self.covariance_matrix = (1 - self.adaptive_step_size) * self.covariance_matrix + self.adaptive_step_size * np.eye(self.dim)\n        return self.covariance_matrix\n\n    def adaptive_step_size_learning(self):\n        # adaptive step size learning\n        self.adaptive_step_size += self.step_size_learning_rate * (self.evaluate(self.population[0]) - self.evaluate(self.population[-1]))\n        return self.adaptive_step_size\n\n    def history_learning(self):\n        # history learning\n        self.history = np.vstack((self.history, self.population[0]))\n\n    def multi_objective_weight_learning(self):\n        # multi-objective weight learning\n        self.multi_objective += 0.001 * (self.evaluate(self.population[0]) - self.evaluate(self.population[-1]))\n        return self.multi_objective\n\n    def optimize(self, func):\n        for i in range(self.budget):\n            # evaluate population\n            fitness = [self.evaluate(x) for x in self.population]\n            # get best solution\n            self.best_solution = min(fitness)\n            best_index = fitness.index(self.best_solution)\n            # adaptive frequency modulation\n            self.F = self.adaptive_frequency_modulation()\n            # differential evolution\n            for j in range(self.pop_size):\n                # generate trial vector\n                trial = self.crossover(self.population[j], self.population[random.randint(0, self.pop_size - 1)])\n                # evaluate trial vector\n                trial_fitness = self.evaluate(trial)\n                # selection\n                self.population[j] = self.selection(self.population[j], trial)\n                # update best solution\n                if trial_fitness < self.evaluate(self.population[j]):\n                    self.population[j] = trial\n            # simulated annealing\n            for j in range(self.pop_size):\n                self.population[j] = self.simulated_annealing(self.population[j])\n            # covariance matrix adaptation\n            self.covariance_matrix = self.covariance_matrix_adaptation()\n            # adaptive step size learning\n            self.adaptive_step_size = self.adaptive_step_size_learning()\n            # multi-objective weight learning\n            self.multi_objective = self.multi_objective_weight_learning()\n            # history learning\n            self.history_learning()\n        return self.best_solution, self.history\n\n    def __call__(self, func):\n        self.func = func\n        return self.optimize(func)\n\n# example usage\nbudget = 1000\ndim = 10\noptimizer = Adaptive_Multi_Objective_Differential_Evolution(budget, dim)\nbest_solution, history = optimizer(lambda x: np.sum(x**2))\nprint(\"Best solution:\", best_solution)\nprint(\"History:\", history)", "name": "Adaptive_Multi_Objective_Differential_Evolution", "description": "Adaptive Multi-Objective Differential Evolution with Frequency Modulation and Covariance Matrix Adaptation, Simulated Annealing with Adaptive Temperature Cooling Schedule and Adaptive Step Size Learning, and History Learning with Probabilistic Exploration.", "configspace": "", "generation": 75, "fitness": 0.10904768668189721, "feedback": "The algorithm Adaptive_Multi_Objective_Differential_Evolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.11 with standard deviation 0.07.", "error": "", "parent_id": "62f0f28d-b1f3-482e-a59e-689769ae92f8", "metadata": {"aucs": [0.24528227773200917, 0.25474427970995217, 0.22204976664970455, 0.2094869299869918, 0.19539122870477788, 0.22037290308693647, 0.21954970685437347, 0.19898863457391847, 0.2088525769055649, 0.010500674407409538, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.06967400625504327, 0.07243264318129916, 0.06765358138380961, 0.06206846592581916, 0.05950315433998454, 0.07166848060014885, 0.08684385045483523, 0.07517136155314164, 0.07499993935294702, 0.048968786592841984, 0.059621093794673063, 0.05253641968280953, 0.06052636267415923, 0.058264385725410484, 0.04932076380592321, 0.05223313783861139, 0.05740705554150749, 0.050715161284487165, 0.11192921094815733, 0.11225674898979632, 0.10371304586130214, 0.12086908562282062, 0.12117642449536448, 0.11376880342959905, 0.12071397455106014, 0.11348297253642337, 0.13312605461436855, 0.10729345396700374, 0.09621423282429165, 0.0742069793283806, 0.09490934229538595, 0.09955910329023265, 0.08566502176010127, 0.10251023923701164, 0.08981033453436849, 0.08298704046326588, 0.17837273622017435, 0.15222306507338346, 0.13606916897769972, 0.20262009905080391, 0.16855346968736173, 0.16683561277475834, 0.12427660751804703, 0.15752853844095172, 0.1402211861948608, 0.05416477370123596, 0.03647095471066264, 0.10291070038097094, 0.033740469618566915, 0.03348717213197383, 0.07109626358557852, 0.09694397369781294, 0.05262903555469933, 0.09047372521291586, 0.09354728314619276, 0.08051367949797406, 0.10502305209633267, 0.12096543454108777, 0.09353104707991866, 0.10138900948494534, 0.11874060375216067, 0.10331849496644008, 0.10087541834485347, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.08244874723304363, 0.08876280593609476, 0.0898951852078721, 0.04683115800801185, 0.04613602316755716, 0.047367539828662264, 0.08569359427014456, 0.06278519190336085, 0.056775076102850486, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.015490367979918429, 0.02330588717452542, 0.030361684345161843, 0.024456392974976038, 0.04573465182323966, 0.04330705836403248, 0.04385692405230224, 0.05039208277050111, 0.03378348844628676, 0.23040325102718717, 0.24191835119381533, 0.25400518225467317, 0.2163046089581765, 0.20349173882824867, 0.2288110911107727, 0.24900465549714002, 0.217219103103935, 0.22681443906535137, 0.05217193929867281, 0.05973254412537865, 0.04537582335433055, 0.06663053937393904, 0.06984343585092212, 0.06303747007108373, 0.058461343389929254, 0.06151750343982043, 0.07361142902689843, 0.13518484263018238, 0.10592400100134458, 0.11842175776965747, 0.13363209132421394, 0.12111756318295885, 0.11114243132375701, 0.11747185059231724, 0.12434555348874077, 0.11331746095698914, 0.15409792030477898, 0.1586476069736107, 0.16812239426293674, 0.1798240905684868, 0.19803757010515766, 0.176471672545374, 0.17458592581425014, 0.1823118574349042, 0.16260700757824775, 0.11915758164373869, 0.12710800850226, 0.11145807699359755, 0.13105383858085518, 0.12039837910798246, 0.12558901236790154, 0.10737333761856771, 0.13400873221215603, 0.12224625560395375, 0.18375184329241878, 0.17156531381446616, 0.19861482935866548, 0.17451278888708788, 0.20042624142565568, 0.18063656574178422, 0.17063513221853643, 0.17364798311720875, 0.2022401747889344, 0.15613068920282025, 0.15828454772206169, 0.15697257242502638, 0.15657821723074938, 0.16058590557167896, 0.15273391137703962, 0.15712756604717448, 0.1614774157463278, 0.1549805499173037, 0.1518939322599484, 0.14932324225855576, 0.16136077019855788, 0.24470340010797786, 0.2157327695595146, 0.15774474392438864, 0.2274714510779129, 0.2479348785700055, 0.16939033619724686, 0.22429250485694585, 0.18593067734933943, 0.14786655266195192, 0.16209186226834205, 0.11646234597477101, 0.16588394568220755, 0.12350174139152603, 0.16351135593475719, 0.14341615629045568, 0.21322003805985035, 0.16774332647623946, 0.16583969757350847, 0.1614886183963865, 0.1779707341207637, 0.1550919933565258, 0.16719931199683669, 0.16263351285484073, 0.1829418853188487, 0.05919314654013175, 0.0773675449243143, 0.06582899736877224, 0.06538592111494712, 0.06168988103955908, 0.05897791928116736, 0.06060404900962146, 0.07014846724602886, 0.0640239623239861]}, "mutation_prompt": null}
{"id": "a314f313-9e1c-47c4-8e3e-1119b65086c5", "solution": "import numpy as np\nimport random\n\nclass Adaptive_Frequency_Modulation_Enhanced_Differential_Evolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.F = 0.5  # initial frequency\n        self.CR = 0.5  # initial crossover rate\n        self.pop_size = 50  # initial population size\n        self.population = np.random.uniform(-5.0, 5.0, (self.pop_size, self.dim))\n        self.best_solution = np.inf\n        self.probability = 0.22  # increased probability of probabilistic exploration\n        self.temperature = 100  # initial temperature for simulated annealing\n        self.covariance_matrix = np.eye(self.dim)  # initial covariance matrix\n        self.adaptive_step_size = 0.11  # increased initial adaptive step size for covariance matrix adaptation\n        self.step_size_learning_rate = 0.0015  # increased learning rate for adaptive step size\n        self.history = np.zeros((self.budget, self.dim))  # history of best solutions\n\n    def evaluate(self, x):\n        return self.func(x)\n\n    def func(self, x):\n        # BBOB test suite of 24 noiseless functions\n        # Here we use the first function f1(x) = sum(x_i^2)\n        return np.sum(x**2)\n\n    def frequency_modulation(self, x):\n        # frequency modulation function\n        return self.F * np.sin(2 * np.pi * self.F * x)\n\n    def adaptive_frequency_modulation(self):\n        # adaptive frequency modulation\n        self.F *= 1.1  # increase frequency\n        if random.random() < self.probability:\n            self.F *= 0.9  # decrease frequency with probability\n        return self.F\n\n    def crossover(self, x1, x2):\n        # crossover operation\n        r = np.random.rand(self.dim)\n        return x1 + r * (x2 - x1)\n\n    def selection(self, x1, x2):\n        # selection operation\n        if self.evaluate(x2) < self.evaluate(x1):\n            return x2\n        else:\n            return x1\n\n    def simulated_annealing(self, x):\n        # simulated annealing\n        new_x = x + np.random.normal(0, 1, self.dim)\n        new_x = np.clip(new_x, -5.0, 5.0)\n        if self.evaluate(new_x) < self.evaluate(x):\n            return new_x\n        else:\n            probability = np.exp(-(self.evaluate(new_x) - self.evaluate(x)) / self.temperature)\n            if random.random() < probability:\n                return new_x\n            else:\n                return x\n        self.temperature *= 0.995  # decrease temperature with a higher rate\n\n    def covariance_matrix_adaptation(self):\n        # covariance matrix adaptation\n        self.covariance_matrix = (1 - self.adaptive_step_size) * self.covariance_matrix + self.adaptive_step_size * np.eye(self.dim)\n        return self.covariance_matrix\n\n    def adaptive_step_size_learning(self):\n        # adaptive step size learning\n        self.adaptive_step_size += self.step_size_learning_rate * (self.evaluate(self.population[0]) - self.evaluate(self.population[-1]))\n        return self.adaptive_step_size\n\n    def history_learning(self):\n        # history learning\n        self.history = np.vstack((self.history, self.population[0]))\n\n    def optimize(self, func):\n        for i in range(self.budget):\n            # evaluate population\n            fitness = [self.evaluate(x) for x in self.population]\n            # get best solution\n            self.best_solution = min(fitness)\n            best_index = fitness.index(self.best_solution)\n            # adaptive frequency modulation\n            self.F = self.adaptive_frequency_modulation()\n            # differential evolution\n            for j in range(self.pop_size):\n                # generate trial vector\n                trial = self.crossover(self.population[j], self.population[random.randint(0, self.pop_size - 1)])\n                # evaluate trial vector\n                trial_fitness = self.evaluate(trial)\n                # selection\n                self.population[j] = self.selection(self.population[j], trial)\n                # update best solution\n                if trial_fitness < self.evaluate(self.population[j]):\n                    self.population[j] = trial\n            # simulated annealing\n            for j in range(self.pop_size):\n                self.population[j] = self.simulated_annealing(self.population[j])\n            # covariance matrix adaptation\n            self.covariance_matrix = self.covariance_matrix_adaptation()\n            # adaptive step size learning\n            self.adaptive_step_size = self.adaptive_step_size_learning()\n            # history learning\n            self.history_learning()\n        return self.best_solution, self.history\n\n    def __call__(self, func):\n        self.func = func\n        return self.optimize(func)\n\n# example usage\nbudget = 1000\ndim = 10\noptimizer = Adaptive_Frequency_Modulation_Enhanced_Differential_Evolution(budget, dim)\nbest_solution, history = optimizer(lambda x: np.sum(x**2))\nprint(\"Best solution:\", best_solution)\nprint(\"History:\", history)", "name": "Adaptive_Frequency_Modulation_Enhanced_Differential_Evolution", "description": "Adaptive Frequency Modulation Enhanced Differential Evolution with Covariance Matrix Adaptation and Simulated Annealing with Adaptive Temperature Cooling Schedule and Adaptive Step Size Learning with History Learning and Frequency Modulation.", "configspace": "", "generation": 76, "fitness": 0.11259790859824863, "feedback": "The algorithm Adaptive_Frequency_Modulation_Enhanced_Differential_Evolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.11 with standard deviation 0.07.", "error": "", "parent_id": "62f0f28d-b1f3-482e-a59e-689769ae92f8", "metadata": {"aucs": [0.22346289703857325, 0.24484474945265167, 0.22517411687502287, 0.1973670812564592, 0.2171201844190428, 0.2325697885192951, 0.2087343838035044, 0.20199827876560528, 0.21460956369151085, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.05127854713056823, 0.0788718820535238, 0.06346618797126613, 0.06567391845722381, 0.06332783178546675, 0.06838310687244553, 0.06917072834450211, 0.06263423856879147, 0.07701192902124254, 0.04551832131023381, 0.05354228859779997, 0.048493490573776166, 0.060390260209938096, 0.051077211681331725, 0.04548409064272996, 0.04986342113718234, 0.05799645660419117, 0.05274277176475939, 0.09702175311559746, 0.11326960814973441, 0.0891004013357759, 0.16275782227498092, 0.1505573646808973, 0.11729495619205588, 0.1657862484153647, 0.12067281845082656, 0.4170191601136044, 0.09266517632527116, 0.09342215998498593, 0.08808910273992099, 0.09122515214126192, 0.0853175116010021, 0.1038894357993615, 0.11645075938969918, 0.12801611899118892, 0.13627291447805634, 0.15909745321413682, 0.1692237363200425, 0.16352652837026949, 0.17720065256924478, 0.15765462711190426, 0.1628489574321027, 0.13987421236199937, 0.15287299380808494, 0.15773560738207848, 0.0634679801160275, 0.050475676392887414, 0.06177476059365106, 0.028038825000359813, 0.07036598740196875, 0.07402497172216904, 0.09282030787991913, 0.10504714765938139, 0.11945138057278859, 0.09452045932318942, 0.11075486001230606, 0.0583870515162731, 0.11261915783504772, 0.1017370865710574, 0.10716211217516536, 0.08568602346724352, 0.10190117433284507, 0.10215533681803679, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.12863136305276235, 0.0926846649184705, 0.09213495089328294, 0.03807526868308542, 0.04072432204039378, 0.04831568620271953, 0.07890840378539166, 0.12162272108839822, 0.0857670832075933, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.030152246669208238, 0.017472057138229302, 0.025567717401906664, 0.03766611504557826, 0.02473216430869085, 0.0416845551270435, 0.03334210733647347, 0.03069063426269969, 0.05151304270967538, 0.2178722782400414, 0.24143563922040834, 0.22688042253245555, 0.23176980791858537, 0.207208028946418, 0.2253258858759013, 0.23183786538342133, 0.2276481487385409, 0.23529498047233766, 0.048207841999933554, 0.0627402195385739, 0.05231119309108567, 0.06448282386943105, 0.0502065871500188, 0.06388272642074988, 0.06133838851931617, 0.06910807537741526, 0.06547858611442425, 0.11942325259716724, 0.11906001326334681, 0.1251405963341944, 0.11132692984726322, 0.13343776919715478, 0.13180342491146735, 0.11870971049077339, 0.1431820268595575, 0.11606747236278336, 0.1794503768762794, 0.1734834814440377, 0.17329459397760494, 0.1760729442804142, 0.17917643124265648, 0.19824795184198452, 0.16246287909319201, 0.18423264504315162, 0.17553067859345173, 0.09545259512916471, 0.11887208236796487, 0.10907269632394367, 0.13209945856075234, 0.13741532872358297, 0.1401434567465809, 0.11372178840954017, 0.14039564297430207, 0.11988393767819938, 0.16688504493749756, 0.19290174610189803, 0.17551062355059988, 0.1740884011454038, 0.1891311288982679, 0.22532696917141382, 0.17394043057597097, 0.17828129522596015, 0.1598125223947292, 0.16189743211126695, 0.15563083821561774, 0.15842138693948982, 0.16439382001150438, 0.16657763572208262, 0.1582869619634928, 0.16474449090283927, 0.14785198820658862, 0.1547983229719463, 0.14989055910828375, 0.15811179717247015, 0.18083189705138125, 0.2606871500462844, 0.21975283390560985, 0.24170693794517628, 0.17089525454911658, 0.15712906621177303, 0.18434659908591233, 0.19789877486025065, 0.153844424315268, 0.19392323037861392, 0.16017716534588833, 0.1843375548140006, 0.25762595438264857, 0.11061287536321929, 0.15463146848844422, 0.14487411624509638, 0.1879132828416129, 0.16959461635166406, 0.1913745803619502, 0.18087895886872707, 0.17022715830558222, 0.18402151319763582, 0.1853409688342037, 0.15913552747627413, 0.1681042399348771, 0.060594484645462354, 0.06363425098141506, 0.057490840406370425, 0.06377247297132616, 0.060830972836734665, 0.06818030055917568, 0.05880207948030236, 0.06642766747448581, 0.06242961623831922]}, "mutation_prompt": null}
{"id": "c0f9277b-8df2-448a-b3ee-bbfc86da9374", "solution": "import numpy as np\nimport random\n\nclass Hybridized_Adaptive_Frequency_Modulated_Differential_Evolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.F = 0.5  # initial frequency\n        self.CR = 0.5  # initial crossover rate\n        self.pop_size = 50  # initial population size\n        self.population = np.random.uniform(-5.0, 5.0, (self.pop_size, self.dim))\n        self.best_solution = np.inf\n        self.probability = 0.25  # increased probability of probabilistic exploration\n        self.temperature = 100  # initial temperature for simulated annealing\n        self.covariance_matrix = np.eye(self.dim)  # initial covariance matrix\n        self.adaptive_step_size = 0.05  # initial adaptive step size for covariance matrix adaptation\n        self.step_size_learning_rate = 0.005  # learning rate for adaptive step size\n        self.history = np.zeros((self.budget, self.dim))  # history of best solutions\n        self.history_learning_rate = 0.1  # learning rate for history learning\n\n    def evaluate(self, x):\n        return self.func(x)\n\n    def func(self, x):\n        # BBOB test suite of 24 noiseless functions\n        # Here we use the first function f1(x) = sum(x_i^2)\n        return np.sum(x**2)\n\n    def frequency_modulation(self, x):\n        # frequency modulation function\n        return self.F * np.sin(2 * np.pi * self.F * x)\n\n    def adaptive_frequency_modulation(self):\n        # adaptive frequency modulation\n        self.F *= 1.1  # increase frequency\n        if random.random() < self.probability:\n            self.F *= 0.9  # decrease frequency with probability\n        return self.F\n\n    def crossover(self, x1, x2):\n        # crossover operation\n        r = np.random.rand(self.dim)\n        return x1 + r * (x2 - x1)\n\n    def selection(self, x1, x2):\n        # selection operation\n        if self.evaluate(x2) < self.evaluate(x1):\n            return x2\n        else:\n            return x1\n\n    def simulated_annealing(self, x):\n        # simulated annealing\n        new_x = x + np.random.normal(0, 1, self.dim)\n        new_x = np.clip(new_x, -5.0, 5.0)\n        if self.evaluate(new_x) < self.evaluate(x):\n            return new_x\n        else:\n            probability = np.exp(-(self.evaluate(new_x) - self.evaluate(x)) / self.temperature)\n            if random.random() < probability:\n                return new_x\n            else:\n                return x\n        self.temperature *= 0.995  # decrease temperature with a higher rate\n\n    def covariance_matrix_adaptation(self):\n        # covariance matrix adaptation\n        self.covariance_matrix = (1 - self.adaptive_step_size) * self.covariance_matrix + self.adaptive_step_size * np.eye(self.dim)\n        return self.covariance_matrix\n\n    def adaptive_step_size_learning(self):\n        # adaptive step size learning\n        self.adaptive_step_size += self.step_size_learning_rate * (self.evaluate(self.population[0]) - self.evaluate(self.population[-1]))\n        return self.adaptive_step_size\n\n    def history_learning(self):\n        # history learning\n        self.history = self.history_learning_rate * self.history + (1 - self.history_learning_rate) * np.vstack((self.history, self.population[0]))\n\n    def optimize(self, func):\n        for i in range(self.budget):\n            # evaluate population\n            fitness = [self.evaluate(x) for x in self.population]\n            # get best solution\n            self.best_solution = min(fitness)\n            best_index = fitness.index(self.best_solution)\n            # adaptive frequency modulation\n            self.F = self.adaptive_frequency_modulation()\n            # differential evolution\n            for j in range(self.pop_size):\n                # generate trial vector\n                trial = self.crossover(self.population[j], self.population[random.randint(0, self.pop_size - 1)])\n                # evaluate trial vector\n                trial_fitness = self.evaluate(trial)\n                # selection\n                self.population[j] = self.selection(self.population[j], trial)\n                # update best solution\n                if trial_fitness < self.evaluate(self.population[j]):\n                    self.population[j] = trial\n            # simulated annealing\n            for j in range(self.pop_size):\n                self.population[j] = self.simulated_annealing(self.population[j])\n            # covariance matrix adaptation\n            self.covariance_matrix = self.covariance_matrix_adaptation()\n            # adaptive step size learning\n            self.adaptive_step_size = self.adaptive_step_size_learning()\n            # history learning\n            self.history_learning()\n        return self.best_solution, self.history\n\n    def __call__(self, func):\n        self.func = func\n        return self.optimize(func)\n\n# example usage\nbudget = 1000\ndim = 10\noptimizer = Hybridized_Adaptive_Frequency_Modulated_Differential_Evolution(budget, dim)\nbest_solution, history = optimizer(lambda x: np.sum(x**2))\nprint(\"Best solution:\", best_solution)\nprint(\"History:\", history)", "name": "Hybridized_Adaptive_Frequency_Modulated_Differential_Evolution", "description": "Hybridized Adaptive Frequency Modulated Differential Evolution with Covariance Matrix Adaptation, Simulated Annealing, and History-Based Learning for Black Box Optimization.", "configspace": "", "generation": 77, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('operands could not be broadcast together with shapes (1000,10) (1001,10) ').", "error": "ValueError('operands could not be broadcast together with shapes (1000,10) (1001,10) ')", "parent_id": "62f0f28d-b1f3-482e-a59e-689769ae92f8", "metadata": {}, "mutation_prompt": null}
{"id": "eba5c703-cf3c-4b49-8a6c-7915b96c1e53", "solution": "import numpy as np\nimport random\n\nclass Enhanced_Adaptive_Differential_Evolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.F = 0.5  # initial frequency\n        self.CR = 0.5  # initial crossover rate\n        self.pop_size = 50  # initial population size\n        self.population = np.random.uniform(-5.0, 5.0, (self.pop_size, self.dim))\n        self.best_solution = np.inf\n        self.probability = 0.2  # increased probability of probabilistic exploration\n        self.temperature = 100  # initial temperature for simulated annealing\n        self.covariance_matrix = np.eye(self.dim)  # initial covariance matrix\n        self.adaptive_step_size = 0.1  # initial adaptive step size for covariance matrix adaptation\n        self.step_size_learning_rate = 0.001  # learning rate for adaptive step size\n        self.history = np.zeros((self.budget, self.dim))  # history of best solutions\n\n    def evaluate(self, x):\n        return self.func(x)\n\n    def func(self, x):\n        # BBOB test suite of 24 noiseless functions\n        # Here we use the first function f1(x) = sum(x_i^2)\n        return np.sum(x**2)\n\n    def frequency_modulation(self, x):\n        # frequency modulation function\n        return self.F * np.sin(2 * np.pi * self.F * x)\n\n    def adaptive_frequency_modulation(self):\n        # adaptive frequency modulation\n        self.F *= 1.1  # increase frequency\n        if random.random() < self.probability:\n            self.F *= 0.9  # decrease frequency with probability\n        return self.F\n\n    def crossover(self, x1, x2):\n        # crossover operation\n        r = np.random.rand(self.dim)\n        return x1 + r * (x2 - x1)\n\n    def selection(self, x1, x2):\n        # selection operation\n        if self.evaluate(x2) < self.evaluate(x1):\n            return x2\n        else:\n            return x1\n\n    def simulated_annealing(self, x):\n        # simulated annealing\n        new_x = x + np.random.normal(0, 1, self.dim)\n        new_x = np.clip(new_x, -5.0, 5.0)\n        if self.evaluate(new_x) < self.evaluate(x):\n            return new_x\n        else:\n            probability = np.exp(-(self.evaluate(new_x) - self.evaluate(x)) / self.temperature)\n            if random.random() < probability:\n                return new_x\n            else:\n                return x\n        self.temperature *= 0.99  # decrease temperature with a higher rate\n\n    def covariance_matrix_adaptation(self):\n        # covariance matrix adaptation\n        self.covariance_matrix = (1 - self.adaptive_step_size) * self.covariance_matrix + self.adaptive_step_size * np.eye(self.dim)\n        return self.covariance_matrix\n\n    def adaptive_step_size_learning(self):\n        # adaptive step size learning\n        self.adaptive_step_size += self.step_size_learning_rate * (self.evaluate(self.population[0]) - self.evaluate(self.population[-1]))\n        return self.adaptive_step_size\n\n    def history_learning(self):\n        # history learning\n        self.history = np.vstack((self.history, self.population[0]))\n\n    def optimize(self, func):\n        for i in range(self.budget):\n            # evaluate population\n            fitness = [self.evaluate(x) for x in self.population]\n            # get best solution\n            self.best_solution = min(fitness)\n            best_index = fitness.index(self.best_solution)\n            # adaptive frequency modulation\n            self.F = self.adaptive_frequency_modulation()\n            # differential evolution\n            for j in range(self.pop_size):\n                # generate trial vector\n                trial = self.crossover(self.population[j], self.population[random.randint(0, self.pop_size - 1)])\n                # evaluate trial vector\n                trial_fitness = self.evaluate(trial)\n                # selection\n                self.population[j] = self.selection(self.population[j], trial)\n                # update best solution\n                if trial_fitness < self.evaluate(self.population[j]):\n                    self.population[j] = trial\n            # simulated annealing\n            for j in range(self.pop_size):\n                self.population[j] = self.simulated_annealing(self.population[j])\n            # covariance matrix adaptation\n            self.covariance_matrix = self.covariance_matrix_adaptation()\n            # adaptive step size learning\n            self.adaptive_step_size = self.adaptive_step_size_learning()\n            # history learning\n            self.history_learning()\n        return self.best_solution, self.history\n\n    def __call__(self, func):\n        self.func = func\n        return self.optimize(func)\n\n# example usage\nbudget = 1000\ndim = 10\noptimizer = Enhanced_Adaptive_Differential_Evolution(budget, dim)\nbest_solution, history = optimizer(lambda x: np.sum(x**2))\nprint(\"Best solution:\", best_solution)\nprint(\"History:\", history)", "name": "Enhanced_Adaptive_Differential_Evolution", "description": "Enhanced Adaptive Differential Evolution with Probabilistic Frequency Modulation, Covariance Matrix Adaptation, and Simulated Annealing with Adaptive Temperature Cooling Schedule and Adaptive Step Size Learning.", "configspace": "", "generation": 26, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "62f0f28d-b1f3-482e-a59e-689769ae92f8", "metadata": {"aucs": [0.23769731172917852, 0.23720457413485296, 0.23398524340821303, 0.20549460010352794, 0.20555116867582168, 0.20130651801061838, 0.2111034242676071, 0.2007516942743227, 0.2116305929248098, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.006392611173892893, 9.999999999998899e-05, 0.059900294776183416, 0.07012785618662798, 0.05419817263252702, 0.0795014722763715, 0.06935901025400615, 0.07305402795587912, 0.07197872740953881, 0.06459557626532919, 0.06996007361196876, 0.053111208791695175, 0.053566490408007095, 0.045790153649368226, 0.08521993086965163, 0.0694819433532079, 0.04350696073483884, 0.060257967750235175, 0.05189063106084635, 0.04649729526248825, 0.14662710504050225, 0.12405344652255246, 0.10395021756261902, 0.5379579998710504, 0.16193852016424048, 0.11791330384339704, 0.18518522693319261, 0.10037714751656313, 0.4283067989696846, 0.09280746828909836, 0.08837625468069787, 0.07937852657802957, 0.07854238359383503, 0.08252944843027832, 0.11767926985163069, 0.1205915686158825, 0.0852113029185213, 0.09530169292310442, 0.17330123385194474, 0.1802252971353444, 0.1525284743121933, 0.1555795762620088, 0.17867323713658245, 0.17229723458815238, 0.16781672295818117, 0.15990674428772478, 0.16337899010881085, 0.08900179420138443, 0.03865389075710812, 0.08160113257791912, 0.03927597651265646, 0.026075257529084794, 0.054926825543406665, 0.09841040259303824, 0.06333280723678636, 0.07418265885714515, 0.09796359396919452, 0.09649198930530256, 0.1100308792533109, 0.10331574585163705, 0.10284233020551181, 0.09918302484193209, 0.10784997913383398, 0.10545554298550341, 0.08743879185238912, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.06531471720729043, 0.06322776317563727, 0.10764868144200157, 0.04234868722028973, 0.06891894424860334, 0.11389557244681159, 0.08902233063245635, 0.04959990216821064, 0.06503624669867603, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.019302053195070723, 0.02569447570168837, 0.02218694043020275, 0.030389077976176515, 0.05011639472944973, 0.026064793258674057, 0.03964242745950375, 0.03523032723265518, 0.04990912829843397, 0.26045759541142244, 0.22792046210319727, 0.24789883372762955, 0.22782461580683844, 0.20323172004362933, 0.21785474159582874, 0.24977361138643406, 0.22257910459292318, 0.23487337912939432, 0.047406160946790954, 0.06913972508922595, 0.04628131465091223, 0.06565901983375388, 0.055748382497863735, 0.0633631736000938, 0.06015180734300485, 0.0755272335023841, 0.06646283240006434, 0.12119223920814659, 0.12470214300559834, 0.14030068848721566, 0.11557895408650731, 0.1328869344681134, 0.12954047545064928, 0.12548124776483527, 0.11082883532147259, 0.11613807783079211, 0.1646706089612735, 0.17402029364751725, 0.169957030806709, 0.17721735438905684, 0.18977506642230046, 0.18552711273919076, 0.16008964310468943, 0.18975185260984473, 0.16153221359268788, 0.11588258721205402, 0.12741740017735936, 0.13192378680608652, 0.1228000957149119, 0.12838293347600316, 0.1273759751841037, 0.10303660034216178, 0.12869446988955147, 0.11553907992403178, 0.1591644210830242, 0.190149261923249, 0.16711753718862765, 0.1942395294204997, 0.2132047975084318, 0.15491365322932638, 0.1620218121660999, 0.16842459346413563, 0.19539101481697096, 0.1574308748762071, 0.15527046019467394, 0.16117056355370862, 0.1507179142739733, 0.16571259637988223, 0.1572215990897441, 0.1537357474934712, 0.15304501824019534, 0.17085057300372697, 0.14647342296828503, 0.16458182587567938, 0.1475369256476301, 0.24951655367433845, 0.1706906960414144, 0.22975424671538114, 0.1468282263746179, 0.18998475289801375, 0.19872740316007598, 0.17967694151965607, 0.2095248766304756, 0.16531236969298302, 0.17673728075527995, 0.1573563880111336, 0.16079461976257647, 0.13630563294661702, 0.18943475494288797, 0.15794340341591473, 0.1694964781221654, 0.17584746439259435, 0.1675693088119269, 0.16675380106618054, 0.16310744584612002, 0.17812108928443993, 0.16876649757309747, 0.16601458261862279, 0.18324856573202208, 0.0679542418940462, 0.06107766964719452, 0.0634038621543137, 0.0589167546534245, 0.06826269804045038, 0.051810492248762996, 0.06457290160761553, 0.0671718679707437, 0.06687823879460175]}, "mutation_prompt": null}
{"id": "c094a064-c7be-4521-a851-4c92a3f2aa0c", "solution": "import numpy as np\nimport random\n\nclass Adaptive_Frequency_Modulation_Enhanced_Adaptive_Differential_Evolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.F = 0.5  # initial frequency\n        self.CR = 0.5  # initial crossover rate\n        self.pop_size = 50  # initial population size\n        self.population = np.random.uniform(-5.0, 5.0, (self.pop_size, self.dim))\n        self.best_solution = np.inf\n        self.probability = 0.2  # increased probability of probabilistic exploration\n        self.temperature = 100  # initial temperature for simulated annealing\n        self.covariance_matrix = np.eye(self.dim)  # initial covariance matrix\n        self.adaptive_step_size = 0.1  # initial adaptive step size for covariance matrix adaptation\n        self.step_size_learning_rate = 0.001  # learning rate for adaptive step size\n        self.history = np.zeros((self.budget, self.dim))  # history of best solutions\n        self.frequency_change_probability = 0.8  # probability to change frequency modulation\n\n    def evaluate(self, x):\n        return self.func(x)\n\n    def func(self, x):\n        # BBOB test suite of 24 noiseless functions\n        # Here we use the first function f1(x) = sum(x_i^2)\n        return np.sum(x**2)\n\n    def frequency_modulation(self, x):\n        # frequency modulation function\n        return self.F * np.sin(2 * np.pi * self.F * x)\n\n    def adaptive_frequency_modulation(self):\n        # adaptive frequency modulation\n        if random.random() < self.frequency_change_probability:\n            self.F *= 1.1  # increase frequency\n            if random.random() < self.probability:\n                self.F *= 0.9  # decrease frequency with probability\n        return self.F\n\n    def crossover(self, x1, x2):\n        # crossover operation\n        r = np.random.rand(self.dim)\n        return x1 + r * (x2 - x1)\n\n    def selection(self, x1, x2):\n        # selection operation\n        if self.evaluate(x2) < self.evaluate(x1):\n            return x2\n        else:\n            return x1\n\n    def simulated_annealing(self, x):\n        # simulated annealing\n        new_x = x + np.random.normal(0, 1, self.dim)\n        new_x = np.clip(new_x, -5.0, 5.0)\n        if self.evaluate(new_x) < self.evaluate(x):\n            return new_x\n        else:\n            probability = np.exp(-(self.evaluate(new_x) - self.evaluate(x)) / self.temperature)\n            if random.random() < probability:\n                return new_x\n            else:\n                return x\n        self.temperature *= 0.99  # decrease temperature with a higher rate\n\n    def covariance_matrix_adaptation(self):\n        # covariance matrix adaptation\n        self.covariance_matrix = (1 - self.adaptive_step_size) * self.covariance_matrix + self.adaptive_step_size * np.eye(self.dim)\n        return self.covariance_matrix\n\n    def adaptive_step_size_learning(self):\n        # adaptive step size learning\n        self.adaptive_step_size += self.step_size_learning_rate * (self.evaluate(self.population[0]) - self.evaluate(self.population[-1]))\n        return self.adaptive_step_size\n\n    def history_learning(self):\n        # history learning\n        self.history = np.vstack((self.history, self.population[0]))\n\n    def optimize(self, func):\n        for i in range(self.budget):\n            # evaluate population\n            fitness = [self.evaluate(x) for x in self.population]\n            # get best solution\n            self.best_solution = min(fitness)\n            best_index = fitness.index(self.best_solution)\n            # adaptive frequency modulation\n            self.F = self.adaptive_frequency_modulation()\n            # differential evolution\n            for j in range(self.pop_size):\n                # generate trial vector\n                trial = self.crossover(self.population[j], self.population[random.randint(0, self.pop_size - 1)])\n                # evaluate trial vector\n                trial_fitness = self.evaluate(trial)\n                # selection\n                self.population[j] = self.selection(self.population[j], trial)\n                # update best solution\n                if trial_fitness < self.evaluate(self.population[j]):\n                    self.population[j] = trial\n            # simulated annealing\n            for j in range(self.pop_size):\n                self.population[j] = self.simulated_annealing(self.population[j])\n            # covariance matrix adaptation\n            self.covariance_matrix = self.covariance_matrix_adaptation()\n            # adaptive step size learning\n            self.adaptive_step_size = self.adaptive_step_size_learning()\n            # history learning\n            self.history_learning()\n        return self.best_solution, self.history\n\n    def __call__(self, func):\n        self.func = func\n        return self.optimize(func)\n\n# example usage\nbudget = 1000\ndim = 10\noptimizer = Adaptive_Frequency_Modulation_Enhanced_Adaptive_Differential_Evolution(budget, dim)\nbest_solution, history = optimizer(lambda x: np.sum(x**2))\nprint(\"Best solution:\", best_solution)\nprint(\"History:\", history)", "name": "Adaptive_Frequency_Modulation_Enhanced_Adaptive_Differential_Evolution", "description": "Adaptive Frequency Modulation Enhanced Adaptive Differential Evolution with Probabilistic Frequency Modulation, Covariance Matrix Adaptation, and Simulated Annealing with Adaptive Temperature Cooling Schedule and Adaptive Step Size Learning, but with a probability 4 times higher to change the frequency modulation.", "configspace": "", "generation": 79, "fitness": 0.11154761449064628, "feedback": "The algorithm Adaptive_Frequency_Modulation_Enhanced_Adaptive_Differential_Evolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.11 with standard deviation 0.07.", "error": "", "parent_id": "62f0f28d-b1f3-482e-a59e-689769ae92f8", "metadata": {"aucs": [0.23848334490236944, 0.2268772374090624, 0.23506953543710296, 0.21167186392568937, 0.2240022371362692, 0.21204659999701603, 0.20204624936929305, 0.19686427608992996, 0.20760265076884266, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.01751170584970574, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.05608693614802285, 0.0651666036257198, 0.046952640557067604, 0.06372316869216355, 0.06256323093737115, 0.0749865441209111, 0.06938432498480074, 0.06850380547468737, 0.06872043270150008, 0.052066188988499906, 0.049374757308957906, 0.040134246187297884, 0.0518196845590152, 0.0536123390868144, 0.0504754322063935, 0.06351548765001624, 0.05010309769524046, 0.07426325486033836, 0.10886195354758421, 0.11746508471671713, 0.0906120155250505, 0.11767415583446073, 0.5126539148780727, 0.13797602024371114, 0.1712122302058069, 0.10371584883876117, 0.12571096859331043, 0.11369901540922112, 0.10187827063842392, 0.07397860828714742, 0.0868374003764093, 0.07822706630645937, 0.07194866215869844, 0.1066367282416657, 0.11093664739618181, 0.09727274805109842, 0.15702594705746709, 0.1717004548440909, 0.14328188034988543, 0.17329892004154468, 0.18536963406458273, 0.15585443303843405, 0.14232363412406612, 0.18315679623227066, 0.14917083281533483, 0.06161694336304957, 0.0661160126964554, 0.07070845786898894, 0.049281254920635176, 0.045617504258506414, 0.03267272769903706, 0.0897155531098055, 0.05569455596994033, 0.1252641140713111, 0.1141557256993323, 0.11422322663745488, 0.09483674718202162, 0.12341027585285591, 0.09569272263584172, 0.09138190596181772, 0.08756927121307834, 0.08959724245766998, 0.11009182039397203, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.10008679911943019, 0.05163445299428282, 0.0989799708982757, 0.0876991547044641, 0.05824494546131376, 0.025750327104794968, 0.07126745326593531, 0.06879902995475806, 0.09165368166009702, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.044548470286029485, 0.019488627771598876, 0.020187830553843766, 0.024704008399471733, 0.02847606758777088, 0.029227715741982463, 0.04571238705723757, 0.03340028199900735, 0.04006046350918646, 0.2318888896710436, 0.2253601473765774, 0.2671442577769506, 0.21070155099840782, 0.2075072183663056, 0.21879946770826364, 0.23810796443386917, 0.23285610451262018, 0.22186885406452372, 0.05598960910973583, 0.06624282717369501, 0.051325403366249356, 0.06288017942182356, 0.053280970203635425, 0.05607620392096124, 0.06491323448404929, 0.06442123120739629, 0.06729652783380313, 0.12960126006917905, 0.10972871261863693, 0.12473791983052329, 0.1260931659971275, 0.14596718114798535, 0.12038551968122246, 0.13397222267725628, 0.11931753523581323, 0.1173616536439811, 0.1584997788914334, 0.17934768413167912, 0.1660840415231789, 0.18765928302200396, 0.17972375778416683, 0.18752054869652934, 0.16569103988164713, 0.17634852800664613, 0.15322348677435926, 0.11622020228788188, 0.13398812056032516, 0.10928526114594805, 0.12977064956910234, 0.12398008381952152, 0.14163009924432446, 0.10030588688701414, 0.1388559555156944, 0.12420224120335155, 0.17457503672253083, 0.19334321727020387, 0.17952710514547887, 0.18497378215793592, 0.17328708437547335, 0.1939178075032857, 0.17698038962099494, 0.16293150651707589, 0.17265110566177022, 0.15666745721352504, 0.17121743353472307, 0.16162406364210258, 0.1595314771522438, 0.15452558672770889, 0.15248899887793654, 0.15046756929861493, 0.15819990600595424, 0.15847225428510758, 0.14643591611316875, 0.15068862393159288, 0.15382745817813015, 0.28407342676633685, 0.21844534268304128, 0.23758297172031106, 0.18494437529326346, 0.14910270206147036, 0.17110945647041276, 0.20706314950822113, 0.21652179870340682, 0.19285839514568948, 0.196099417486037, 0.1238871862641836, 0.18503863575966595, 0.1604352579794901, 0.1396611610518539, 0.10539189225405665, 0.17145664215833678, 0.17791101851814362, 0.16818435229574513, 0.20146772595082263, 0.16011581155068977, 0.17889309401175468, 0.16887173447684423, 0.16127291973640845, 0.19729685567044075, 0.06092765644983078, 0.07255001255848659, 0.06694914430045651, 0.06065447980046723, 0.06317542005960819, 0.06484116270722406, 0.05972508034416679, 0.05786150028358039, 0.06514322999933808]}, "mutation_prompt": null}
{"id": "d5da104c-4aaa-402b-a922-dc6fd61c1f39", "solution": "import numpy as np\nimport random\n\nclass Novel_Metaheuristic_Algorithm:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.F = 0.5  # initial frequency\n        self.CR = 0.5  # initial crossover rate\n        self.pop_size = 50  # initial population size\n        self.population = np.random.uniform(-5.0, 5.0, (self.pop_size, self.dim))\n        self.best_solution = np.inf\n        self.probability = 0.2  # increased probability of probabilistic exploration\n        self.temperature = 100  # initial temperature for simulated annealing\n        self.covariance_matrix = np.eye(self.dim)  # initial covariance matrix\n        self.adaptive_step_size = 0.1  # initial adaptive step size for covariance matrix adaptation\n        self.step_size_learning_rate = 0.001  # learning rate for adaptive step size\n        self.levy_flight_probability = 0.1  # probability of levy flight\n        self.levy_flight_step_size = 0.1  # step size for levy flight\n        self.history = np.zeros((self.budget, self.dim))  # history of best solutions\n\n    def evaluate(self, x):\n        return self.func(x)\n\n    def func(self, x):\n        # BBOB test suite of 24 noiseless functions\n        # Here we use the first function f1(x) = sum(x_i^2)\n        return np.sum(x**2)\n\n    def frequency_modulation(self, x):\n        # frequency modulation function\n        return self.F * np.sin(2 * np.pi * self.F * x)\n\n    def adaptive_frequency_modulation(self):\n        # adaptive frequency modulation\n        self.F *= 1.1  # increase frequency\n        if random.random() < self.probability:\n            self.F *= 0.9  # decrease frequency with probability\n        return self.F\n\n    def levy_flight(self, x):\n        # levy flight function\n        step = np.random.normal(0, self.levy_flight_step_size, self.dim)\n        return x + step / np.abs(step)\n\n    def crossover(self, x1, x2):\n        # crossover operation\n        r = np.random.rand(self.dim)\n        return x1 + r * (x2 - x1)\n\n    def selection(self, x1, x2):\n        # selection operation\n        if self.evaluate(x2) < self.evaluate(x1):\n            return x2\n        else:\n            return x1\n\n    def information_based_selection(self, x1, x2):\n        # information-based selection\n        if self.evaluate(x2) < self.evaluate(x1):\n            return x2\n        else:\n            if random.random() < 0.5:\n                return x2\n            else:\n                return x1\n\n    def simulated_annealing(self, x):\n        # simulated annealing\n        new_x = x + np.random.normal(0, 1, self.dim)\n        new_x = np.clip(new_x, -5.0, 5.0)\n        if self.evaluate(new_x) < self.evaluate(x):\n            return new_x\n        else:\n            probability = np.exp(-(self.evaluate(new_x) - self.evaluate(x)) / self.temperature)\n            if random.random() < probability:\n                return new_x\n            else:\n                return x\n        self.temperature *= 0.99  # decrease temperature with a higher rate\n\n    def covariance_matrix_adaptation(self):\n        # covariance matrix adaptation\n        self.covariance_matrix = (1 - self.adaptive_step_size) * self.covariance_matrix + self.adaptive_step_size * np.eye(self.dim)\n        return self.covariance_matrix\n\n    def adaptive_step_size_learning(self):\n        # adaptive step size learning\n        self.adaptive_step_size += self.step_size_learning_rate * (self.evaluate(self.population[0]) - self.evaluate(self.population[-1]))\n        return self.adaptive_step_size\n\n    def history_learning(self):\n        # history learning\n        self.history = np.vstack((self.history, self.population[0]))\n\n    def optimize(self, func):\n        for i in range(self.budget):\n            # evaluate population\n            fitness = [self.evaluate(x) for x in self.population]\n            # get best solution\n            self.best_solution = min(fitness)\n            best_index = fitness.index(self.best_solution)\n            # adaptive frequency modulation\n            self.F = self.adaptive_frequency_modulation()\n            # differential evolution\n            for j in range(self.pop_size):\n                # generate trial vector\n                trial = self.crossover(self.population[j], self.population[random.randint(0, self.pop_size - 1)])\n                # evaluate trial vector\n                trial_fitness = self.evaluate(trial)\n                # selection\n                self.population[j] = self.selection(trial, self.population[j])\n                # update best solution\n                if trial_fitness < self.evaluate(self.population[j]):\n                    self.population[j] = trial\n            # levy flight\n            for j in range(self.pop_size):\n                if random.random() < self.levy_flight_probability:\n                    self.population[j] = self.levy_flight(self.population[j])\n            # simulated annealing\n            for j in range(self.pop_size):\n                self.population[j] = self.simulated_annealing(self.population[j])\n            # information-based selection\n            for j in range(self.pop_size):\n                self.population[j] = self.information_based_selection(self.population[j], self.population[random.randint(0, self.pop_size - 1)])\n            # covariance matrix adaptation\n            self.covariance_matrix = self.covariance_matrix_adaptation()\n            # adaptive step size learning\n            self.adaptive_step_size = self.adaptive_step_size_learning()\n            # history learning\n            self.history_learning()\n        return self.best_solution, self.history\n\n    def __call__(self, func):\n        self.func = func\n        return self.optimize(func)\n\n# example usage\nbudget = 1000\ndim = 10\noptimizer = Novel_Metaheuristic_Algorithm(budget, dim)\nbest_solution, history = optimizer(lambda x: np.sum(x**2))\nprint(\"Best solution:\", best_solution)\nprint(\"History:\", history)", "name": "Novel_Metaheuristic_Algorithm", "description": "Novel Metaheuristic Algorithm with Adaptive Frequency Modulation, Covariance Matrix Adaptation, and Simulated Annealing with Adaptive Temperature Cooling Schedule and Adaptive Step Size Learning with an added feature of Levy Flight and Information-based Selection.", "configspace": "", "generation": 80, "fitness": 0.11737257883093302, "feedback": "The algorithm Novel_Metaheuristic_Algorithm got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.12 with standard deviation 0.08.", "error": "", "parent_id": "62f0f28d-b1f3-482e-a59e-689769ae92f8", "metadata": {"aucs": [0.26456469516385195, 0.2254952241464735, 0.2269076791311373, 0.24705228977120608, 0.20431502241003607, 0.23488064865503322, 0.2579520815833398, 0.22014044009370615, 0.22487911509872593, 0.00018101599222941633, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.06159177000438887, 0.09029745220442187, 0.07529805566935555, 0.07227431107185478, 0.05540678950731104, 0.06459996901570175, 0.07161488043253539, 0.07705313266238012, 0.06523610902921095, 0.048756699061230924, 0.06094297425803419, 0.053417251123325515, 0.06512708680692814, 0.05641200146833003, 0.04415602205838387, 0.06256916239647725, 0.05235020642839738, 0.05865787594630256, 0.408962337738047, 0.10639940019651872, 0.12912795533576105, 0.18192996727977617, 0.11512701237347756, 0.11486462426683808, 0.44563868667437323, 0.19308269459372596, 0.13447313667326, 0.10758098019855933, 0.06738876324312149, 0.10105482986860426, 0.11735140141870526, 0.07745772281203567, 0.1104669265825391, 0.10039194679321484, 0.10324068167424139, 0.13096400416761445, 0.19412667271355888, 0.17777137556251943, 0.1350334644014627, 0.177016365140506, 0.17091842022899983, 0.14703130823805755, 0.16440343140074798, 0.193209285111119, 0.20427495981246002, 0.11553859473644656, 0.17171987088797946, 0.02722642692131927, 0.06665214702780176, 0.12066640452414434, 0.032084277777160275, 0.08483914838621909, 0.08644149063452644, 0.09347046501233403, 0.108785913251919, 0.10621463733106196, 0.11716515991830545, 0.09422814135296553, 0.12236996881871798, 0.12130725486868632, 0.09756742434549193, 0.0826836467662646, 0.10317771063098458, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.06639746712235883, 0.05310875713887164, 0.04232680583629622, 0.036986761787959566, 0.00030652477346848617, 0.03190939696183137, 0.0526244596213441, 0.06979424738870144, 0.06865620554910945, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.0251273807599256, 0.0009427622369366784, 0.025698033827724842, 0.07676748293253899, 0.03007668625512705, 0.07836631857683174, 0.021674316943880556, 0.06994886391496913, 0.026599821866860607, 0.25019111094918967, 0.2494461719103529, 0.23290208696009107, 0.26457572145450126, 0.21655155801099157, 0.2294583998126798, 0.2451298264749866, 0.23841337420409203, 0.25591541669591966, 0.056337232809303917, 0.06287743793427292, 0.061365040792005665, 0.0636326967654588, 0.06546437513260384, 0.06457880315921116, 0.07653459235720916, 0.0648147918831592, 0.06700829231895078, 0.12010250373614817, 0.12056000798999944, 0.1293447644667991, 0.1281841295881868, 0.13003092987316234, 0.14039449691587358, 0.12022052525616933, 0.11104340036649352, 0.10279253637985042, 0.19472359319476473, 0.21802555163541004, 0.1830223872012866, 0.19874036483972346, 0.2046823266325829, 0.1958098940598636, 0.17087682020726702, 0.19141519691817654, 0.198355083889302, 0.14487527545502366, 0.12993741685520377, 0.12841078202877687, 0.14500199924113244, 0.13627805493760348, 0.15910952147331248, 0.1344791432629716, 0.13320092875514478, 0.12852015216139334, 0.1708086418145962, 0.17076057893253782, 0.2022964148057116, 0.18325644204438096, 0.1837552680150869, 0.1869056322470517, 0.16964139404503187, 0.1862060747423151, 0.17377296211368065, 0.15761641213739497, 0.1642911113761356, 0.15447366221943615, 0.16305510509745613, 0.17424998049767615, 0.16535211517107273, 0.1635473193951762, 0.15062902020258784, 0.17077866368673622, 0.16210773914830645, 0.14083508880987283, 0.1691006696293058, 0.28201280648219385, 0.1546077855727337, 0.21023435019087733, 0.255714466643434, 0.21606294434125162, 0.26459312997817586, 0.14230382890996718, 0.13631722584562178, 0.11913226380361652, 0.14083697764971403, 0.14653907062834837, 0.15600820750312872, 0.14209538729184334, 0.1260486536763974, 0.1400816345071184, 0.17718497133632227, 0.17854910052626272, 0.18333213977810114, 0.17333993719814367, 0.19075551213414343, 0.20207141055444966, 0.18092720017885966, 0.16657149628236, 0.1889372120916465, 0.06514460498195407, 0.05360323807700951, 0.059687058353244726, 0.062323878991750514, 0.06567246273277849, 0.06903388888795103, 0.05901856755208812, 0.07628430249671547, 0.05720700684796298]}, "mutation_prompt": null}
{"id": "88969cc7-3c67-4dfe-9d52-bcbaa0eb3640", "solution": "import numpy as np\nimport random\n\nclass Novel_Metaheuristic_Algorithm:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.F = 0.5  # initial frequency\n        self.CR = 0.5  # initial crossover rate\n        self.pop_size = 50  # initial population size\n        self.population = np.random.uniform(-5.0, 5.0, (self.pop_size, self.dim))\n        self.best_solution = np.inf\n        self.probability = 0.4  # increased probability of probabilistic exploration\n        self.temperature = 100  # initial temperature for simulated annealing\n        self.covariance_matrix = np.eye(self.dim)  # initial covariance matrix\n        self.adaptive_step_size = 0.1  # initial adaptive step size for covariance matrix adaptation\n        self.step_size_learning_rate = 0.001  # learning rate for adaptive step size\n        self.levy_flight_probability = 0.2  # probability of levy flight\n        self.levy_flight_step_size = 0.1  # step size for levy flight\n        self.history = np.zeros((self.budget, self.dim))  # history of best solutions\n        self.cooling_rate = 0.999  # higher rate of temperature cooling\n\n    def evaluate(self, x):\n        return self.func(x)\n\n    def func(self, x):\n        # BBOB test suite of 24 noiseless functions\n        # Here we use the first function f1(x) = sum(x_i^2)\n        return np.sum(x**2)\n\n    def frequency_modulation(self, x):\n        # frequency modulation function\n        return self.F * np.sin(2 * np.pi * self.F * x)\n\n    def adaptive_frequency_modulation(self):\n        # adaptive frequency modulation\n        self.F *= 1.1  # increase frequency\n        if random.random() < self.probability:\n            self.F *= 0.9  # decrease frequency with probability\n        return self.F\n\n    def levy_flight(self, x):\n        # levy flight function\n        step = np.random.normal(0, self.levy_flight_step_size, self.dim)\n        return x + step / np.abs(step)\n\n    def crossover(self, x1, x2):\n        # crossover operation\n        r = np.random.rand(self.dim)\n        return x1 + r * (x2 - x1)\n\n    def selection(self, x1, x2):\n        # selection operation\n        if self.evaluate(x2) < self.evaluate(x1):\n            return x2\n        else:\n            return x1\n\n    def information_based_selection(self, x1, x2):\n        # information-based selection\n        if self.evaluate(x2) < self.evaluate(x1):\n            return x2\n        else:\n            if random.random() < 0.5:\n                return x2\n            else:\n                return x1\n\n    def simulated_annealing(self, x):\n        # simulated annealing\n        new_x = x + np.random.normal(0, 1, self.dim)\n        new_x = np.clip(new_x, -5.0, 5.0)\n        if self.evaluate(new_x) < self.evaluate(x):\n            return new_x\n        else:\n            probability = np.exp(-(self.evaluate(new_x) - self.evaluate(x)) / self.temperature)\n            if random.random() < probability:\n                return new_x\n            else:\n                return x\n        self.temperature *= self.cooling_rate  # decrease temperature with a higher rate\n\n    def covariance_matrix_adaptation(self):\n        # covariance matrix adaptation\n        self.covariance_matrix = (1 - self.adaptive_step_size) * self.covariance_matrix + self.adaptive_step_size * np.eye(self.dim)\n        return self.covariance_matrix\n\n    def adaptive_step_size_learning(self):\n        # adaptive step size learning\n        self.adaptive_step_size += self.step_size_learning_rate * (self.evaluate(self.population[0]) - self.evaluate(self.population[-1]))\n        return self.adaptive_step_size\n\n    def history_learning(self):\n        # history learning\n        self.history = np.vstack((self.history, self.population[0]))\n\n    def optimize(self, func):\n        for i in range(self.budget):\n            # evaluate population\n            fitness = [self.evaluate(x) for x in self.population]\n            # get best solution\n            self.best_solution = min(fitness)\n            best_index = fitness.index(self.best_solution)\n            # adaptive frequency modulation\n            self.F = self.adaptive_frequency_modulation()\n            # differential evolution\n            for j in range(self.pop_size):\n                # generate trial vector\n                trial = self.crossover(self.population[j], self.population[random.randint(0, self.pop_size - 1)])\n                # evaluate trial vector\n                trial_fitness = self.evaluate(trial)\n                # selection\n                self.population[j] = self.selection(trial, self.population[j])\n                # update best solution\n                if trial_fitness < self.evaluate(self.population[j]):\n                    self.population[j] = trial\n            # levy flight\n            for j in range(self.pop_size):\n                if random.random() < self.levy_flight_probability:\n                    self.population[j] = self.levy_flight(self.population[j])\n            # simulated annealing\n            for j in range(self.pop_size):\n                self.population[j] = self.simulated_annealing(self.population[j])\n            # information-based selection\n            for j in range(self.pop_size):\n                self.population[j] = self.information_based_selection(self.population[j], self.population[random.randint(0, self.pop_size - 1)])\n            # covariance matrix adaptation\n            self.covariance_matrix = self.covariance_matrix_adaptation()\n            # adaptive step size learning\n            self.adaptive_step_size = self.adaptive_step_size_learning()\n            # history learning\n            self.history_learning()\n        return self.best_solution, self.history\n\n    def __call__(self, func):\n        self.func = func\n        return self.optimize(func)\n\n# example usage\nbudget = 1000\ndim = 10\noptimizer = Novel_Metaheuristic_Algorithm(budget, dim)\nbest_solution, history = optimizer(lambda x: np.sum(x**2))\nprint(\"Best solution:\", best_solution)\nprint(\"History:\", history)", "name": "Novel_Metaheuristic_Algorithm", "description": "Novel Metaheuristic Algorithm with Adaptive Frequency Modulation, Covariance Matrix Adaptation, and Simulated Annealing with Adaptive Temperature Cooling Schedule and Adaptive Step Size Learning with an added feature of Levy Flight and Information-based Selection, refined to increase the probability of probabilistic exploration and to use a higher rate of temperature cooling.", "configspace": "", "generation": 81, "fitness": 0.1190684458248614, "feedback": "The algorithm Novel_Metaheuristic_Algorithm got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.12 with standard deviation 0.08.", "error": "", "parent_id": "d5da104c-4aaa-402b-a922-dc6fd61c1f39", "metadata": {"aucs": [0.25089624389388854, 0.2217261639438356, 0.2126031555527842, 0.2493327874692448, 0.24026173674172402, 0.2520731908404129, 0.2344801580191428, 0.21763354216172937, 0.20659539282582184, 0.0037620560788709234, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.06372799130071993, 0.060276049938563614, 0.0746552968247286, 0.07877439728272473, 0.07613547432861267, 0.06342117057085839, 0.08946797492099978, 0.07934696936808083, 0.07213886018343041, 0.04123151268594416, 0.04904330421134817, 0.047559497724399935, 0.05669855107860278, 0.053676277694207575, 0.057190223643244775, 0.056542056296729815, 0.0639113767335523, 0.06811495813000734, 0.4610377444581354, 0.1624565625590021, 0.11497497740812701, 0.2819286282133284, 0.11378321847323236, 0.2354877228649429, 0.47147015562604133, 0.21828798631708968, 0.3806597491153947, 0.1457782857014459, 0.0995959429714045, 0.10333500036427978, 0.13697445392735297, 0.09823648179033495, 0.10767180910151797, 0.11236013540713352, 0.08923223191274698, 0.13130627652573346, 0.13095462520879264, 0.1968684518054089, 0.15897179688575613, 0.16889852133107142, 0.18069006380732877, 0.19512833275490393, 0.17434907023977853, 0.15692438437286205, 0.18185203114187698, 0.0813891154353118, 0.08027098682535583, 0.08127663161091059, 0.07074815587531247, 0.09977292667835647, 0.0491530211874075, 0.09949357842827578, 0.08333482126360447, 0.10380110999651493, 0.07117241637206517, 0.08267223223909481, 0.08193032447179971, 0.06299207550095198, 0.09233964128220362, 0.10401396928362938, 0.0880915171205846, 0.10502010362609404, 0.09621448138540922, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.07921038026259808, 0.0509805727602517, 0.0803970549965275, 0.030148798520977294, 0.03318401363794454, 0.03501062865945803, 0.08183191445280413, 0.06664166768201163, 0.06334404031366381, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.025066104031163605, 0.02358585592413487, 0.023305140645161426, 0.04972698772281836, 0.026668016854255572, 0.03221354927637332, 0.027356614219504083, 0.028392100526013042, 0.03882671475173394, 0.2685189573150022, 0.26851453344668097, 0.26864199846511805, 0.24411925578718807, 0.2405089020650666, 0.23510047296127878, 0.2341580890314926, 0.2414242455730914, 0.2823407710660658, 0.05213297256557303, 0.05752238323336023, 0.056965367784221566, 0.07143066157367939, 0.08224496879628673, 0.055150319125936376, 0.0770566692787008, 0.050871157066497696, 0.06333659572660877, 0.11236828975104496, 0.11026714280663841, 0.12647081169468588, 0.1260982960842606, 0.12139680072262526, 0.13266563367604067, 0.1150645091435808, 0.12516161205199805, 0.11179200131772649, 0.1776280956688524, 0.16365096477741425, 0.18957325609308895, 0.17804248845504367, 0.19955868477945116, 0.18572778369791987, 0.17219913482424376, 0.1959097611980618, 0.18212828012630555, 0.1323954977288111, 0.13554229461982326, 0.1505872847852674, 0.12881070323580956, 0.14378105386965634, 0.13931751675522575, 0.11212694624237174, 0.12475656493436071, 0.14495378094289013, 0.18351575574942858, 0.20297179621041506, 0.19611604304473174, 0.16604250506198737, 0.18287667006737784, 0.1751144888645545, 0.1663666149843842, 0.1797021605183401, 0.19925070484022078, 0.16119326692163793, 0.16300042450715924, 0.15638024203333356, 0.1671181321488321, 0.16631433041436394, 0.15131009559718078, 0.15590963111674605, 0.1711561280476117, 0.154754534394969, 0.16427662021342915, 0.15021062503884064, 0.232162107301768, 0.2917975787484669, 0.19829179186743273, 0.1970324289828801, 0.1433244834101124, 0.15497238365091648, 0.15656296254943813, 0.20084081279043153, 0.17229349972400743, 0.1575389965176377, 0.20834157381178287, 0.15400130067351092, 0.1557977865627267, 0.13070078628810256, 0.13824681854544818, 0.10018725449017696, 0.15805956167734914, 0.16995966383704542, 0.17297824051290267, 0.17586351801343625, 0.1772185108455233, 0.19146286718669936, 0.16426853905246663, 0.15880805902891715, 0.1865421286262524, 0.05854354410059637, 0.07274622457603486, 0.07745137017132453, 0.06027918147482869, 0.0669619337131222, 0.05790844406452966, 0.06721882868436369, 0.06543120166149596, 0.060727359858228414]}, "mutation_prompt": null}
{"id": "19338725-212f-4c79-a8cb-466087e7d532", "solution": "import numpy as np\nimport random\n\nclass Novel_Metaheuristic_Algorithm:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.F = 0.5  # initial frequency\n        self.CR = 0.5  # initial crossover rate\n        self.pop_size = 50  # initial population size\n        self.population = np.random.uniform(-5.0, 5.0, (self.pop_size, self.dim))\n        self.best_solution = np.inf\n        self.probability = 0.4  # increased probability of probabilistic exploration\n        self.temperature = 100  # initial temperature for simulated annealing\n        self.covariance_matrix = np.eye(self.dim)  # initial covariance matrix\n        self.adaptive_step_size = 0.1  # initial adaptive step size for covariance matrix adaptation\n        self.step_size_learning_rate = 0.001  # learning rate for adaptive step size\n        self.levy_flight_probability = 0.2  # probability of levy flight\n        self.levy_flight_step_size = 0.1  # step size for levy flight\n        self.history = np.zeros((self.budget, self.dim))  # history of best solutions\n        self.cooling_rate = 0.999  # higher rate of temperature cooling\n        self.pso_velocity = np.zeros((self.pop_size, self.dim))  # initial velocity for particle swarm optimization\n        self.pso_position = np.copy(self.population)  # initial position for particle swarm optimization\n        self.pso_best_solution = np.copy(self.population)  # initial best solution for particle swarm optimization\n        self.pso_best_fitness = np.inf  # initial best fitness for particle swarm optimization\n\n    def evaluate(self, x):\n        return self.func(x)\n\n    def func(self, x):\n        # BBOB test suite of 24 noiseless functions\n        # Here we use the first function f1(x) = sum(x_i^2)\n        return np.sum(x**2)\n\n    def frequency_modulation(self, x):\n        # frequency modulation function\n        return self.F * np.sin(2 * np.pi * self.F * x)\n\n    def adaptive_frequency_modulation(self):\n        # adaptive frequency modulation\n        self.F *= 1.1  # increase frequency\n        if random.random() < self.probability:\n            self.F *= 0.9  # decrease frequency with probability\n        return self.F\n\n    def levy_flight(self, x):\n        # levy flight function\n        step = np.random.normal(0, self.levy_flight_step_size, self.dim)\n        return x + step / np.abs(step)\n\n    def crossover(self, x1, x2):\n        # crossover operation\n        r = np.random.rand(self.dim)\n        return x1 + r * (x2 - x1)\n\n    def selection(self, x1, x2):\n        # selection operation\n        if self.evaluate(x2) < self.evaluate(x1):\n            return x2\n        else:\n            return x1\n\n    def information_based_selection(self, x1, x2):\n        # information-based selection\n        if self.evaluate(x2) < self.evaluate(x1):\n            return x2\n        else:\n            if random.random() < 0.5:\n                return x2\n            else:\n                return x1\n\n    def simulated_annealing(self, x):\n        # simulated annealing\n        new_x = x + np.random.normal(0, 1, self.dim)\n        new_x = np.clip(new_x, -5.0, 5.0)\n        if self.evaluate(new_x) < self.evaluate(x):\n            return new_x\n        else:\n            probability = np.exp(-(self.evaluate(new_x) - self.evaluate(x)) / self.temperature)\n            if random.random() < probability:\n                return new_x\n            else:\n                return x\n        self.temperature *= self.cooling_rate  # decrease temperature with a higher rate\n\n    def covariance_matrix_adaptation(self):\n        # covariance matrix adaptation\n        self.covariance_matrix = (1 - self.adaptive_step_size) * self.covariance_matrix + self.adaptive_step_size * np.eye(self.dim)\n        return self.covariance_matrix\n\n    def adaptive_step_size_learning(self):\n        # adaptive step size learning\n        self.adaptive_step_size += self.step_size_learning_rate * (self.evaluate(self.population[0]) - self.evaluate(self.population[-1]))\n        return self.adaptive_step_size\n\n    def history_learning(self):\n        # history learning\n        self.history = np.vstack((self.history, self.population[0]))\n\n    def pso_velocity_update(self):\n        # update velocity for particle swarm optimization\n        for i in range(self.pop_size):\n            r1 = random.random()\n            r2 = random.random()\n            self.pso_velocity[i] = self.pso_velocity[i] + self.pso_best_solution[i] - self.pso_position[i] + r1 * (self.pso_best_solution[i] - self.pso_position[i]) + r2 * (self.pso_best_solution[i] - self.pso_best_solution[i])\n\n    def pso_position_update(self):\n        # update position for particle swarm optimization\n        for i in range(self.pop_size):\n            self.pso_position[i] = self.pso_position[i] + self.pso_velocity[i]\n\n    def pso_selection(self):\n        # selection for particle swarm optimization\n        for i in range(self.pop_size):\n            if self.evaluate(self.pso_position[i]) < self.evaluate(self.pso_best_solution[i]):\n                self.pso_best_solution[i] = self.pso_position[i]\n\n    def pso_optimization(self):\n        # particle swarm optimization\n        for i in range(self.budget):\n            self.pso_velocity_update()\n            self.pso_position_update()\n            self.pso_selection()\n\n    def optimize(self, func):\n        for i in range(self.budget):\n            # evaluate population\n            fitness = [self.evaluate(x) for x in self.population]\n            # get best solution\n            self.best_solution = min(fitness)\n            best_index = fitness.index(self.best_solution)\n            # adaptive frequency modulation\n            self.F = self.adaptive_frequency_modulation()\n            # differential evolution\n            for j in range(self.pop_size):\n                # generate trial vector\n                trial = self.crossover(self.population[j], self.population[random.randint(0, self.pop_size - 1)])\n                # evaluate trial vector\n                trial_fitness = self.evaluate(trial)\n                # selection\n                self.population[j] = self.selection(trial, self.population[j])\n                # update best solution\n                if trial_fitness < self.evaluate(self.population[j]):\n                    self.population[j] = trial\n            # levy flight\n            for j in range(self.pop_size):\n                if random.random() < self.levy_flight_probability:\n                    self.population[j] = self.levy_flight(self.population[j])\n            # simulated annealing\n            for j in range(self.pop_size):\n                self.population[j] = self.simulated_annealing(self.population[j])\n            # information-based selection\n            for j in range(self.pop_size):\n                self.population[j] = self.information_based_selection(self.population[j], self.population[random.randint(0, self.pop_size - 1)])\n            # covariance matrix adaptation\n            self.covariance_matrix = self.covariance_matrix_adaptation()\n            # adaptive step size learning\n            self.adaptive_step_size = self.adaptive_step_size_learning()\n            # history learning\n            self.history_learning()\n            # particle swarm optimization\n            self.pso_optimization()\n        return self.best_solution, self.history\n\n    def __call__(self, func):\n        self.func = func\n        return self.optimize(func)\n\n# example usage\nbudget = 1000\ndim = 10\noptimizer = Novel_Metaheuristic_Algorithm(budget, dim)\nbest_solution, history = optimizer(lambda x: np.sum(x**2))\nprint(\"Best solution:\", best_solution)\nprint(\"History:\", history)", "name": "Novel_Metaheuristic_Algorithm", "description": "Novel Metaheuristic Algorithm with Adaptive Frequency Modulation, Covariance Matrix Adaptation, and Simulated Annealing with Adaptive Temperature Cooling Schedule and Adaptive Step Size Learning with an added feature of Levy Flight and Information-based Selection, refined to increase the probability of probabilistic exploration and to use a higher rate of temperature cooling, and to incorporate a novel hybridization of the selected solution with a particle swarm optimization (PSO) algorithm.", "configspace": "", "generation": 82, "fitness": 0.05924787349719777, "feedback": "The algorithm Novel_Metaheuristic_Algorithm got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06 with standard deviation 0.06.", "error": "", "parent_id": "88969cc7-3c67-4dfe-9d52-bcbaa0eb3640", "metadata": {"aucs": [0.12387886523122271, 0.1265149430757756, 0.12776446495069305, 0.09431231635004056, 0.10750000009520266, 0.13567253683705316, 0.12601439434303796, 0.10213402325986787, 0.08246013140426456, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.016665067719909166, 0.030879286322488042, 0.016134027515144256, 0.01960701790433117, 0.007079015665313659, 0.018662024466130744, 0.01947931185655538, 0.02537886634938913, 0.03638136491896771, 9.999999999998899e-05, 0.0042613094926138295, 0.047965595907319636, 0.02309510326727815, 0.0056662589244868, 0.012145786100181022, 0.027878269235220077, 0.004993710987267863, 0.01623281472698712, 0.055639867530497544, 0.05216503489053337, 0.04287777107098589, 0.053340261193588256, 0.04687545855643349, 0.06334375926191316, 0.0638860334912611, 0.048986754550083944, 0.056921363509118805, 0.06274794416174423, 9.999999999998899e-05, 0.03248495403717788, 0.009512744848940602, 0.018984221147313685, 0.08987329928960408, 9.999999999998899e-05, 0.0349212794753867, 9.999999999998899e-05, 0.10027683525177622, 0.0873096802268617, 0.07744534177503049, 0.09676152833554574, 0.07285119553694963, 9.999999999998899e-05, 0.08751961772345818, 0.05931056463008866, 0.1527317889550348, 9.999999999998899e-05, 9.999999999998899e-05, 0.03228442301781653, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.016802333515727463, 0.033249968069706703, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.03809586569963663, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.1478388955353257, 0.1637630563047946, 0.14904795559477102, 0.15013780986775205, 0.13824553260967454, 0.13781718781917696, 0.16381432563234155, 0.17419049746155146, 0.17618706526224848, 0.007596029757526823, 9.999999999998899e-05, 0.02198412468535549, 0.03266886175302153, 9.999999999998899e-05, 0.032968589820406624, 0.013398138776357649, 0.005861018192106937, 0.029425882074826992, 0.10070790586982115, 0.0915233911006782, 0.11842175776965747, 0.09855305120349966, 0.10852972522114013, 0.10798713552628325, 0.12131151821544572, 0.08099219012337167, 0.07726393575557644, 0.16170547477874986, 0.12879753890484935, 0.1613932299901245, 0.13800672315110074, 0.18068627258821635, 0.1358755256259001, 0.1252361419136372, 0.17377774300845283, 0.13439595589376607, 0.05985564337097804, 0.08512818855650583, 0.08398582550377798, 0.07857447070953127, 0.08201643736074371, 0.10722782946421872, 0.0837317855149382, 0.0997711199450243, 0.08406789557255323, 0.13974531398501822, 0.13857560331198493, 0.13345086981233123, 0.12263875275420355, 0.13807514749026173, 0.12183045238652779, 0.14374528941892617, 0.13475874372368357, 0.16401828516874917, 0.13709239775446036, 0.12736794858390876, 0.12561694916985566, 9.999999999998899e-05, 9.999999999998899e-05, 0.04043095754110926, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.10957712000126607, 0.10896105484452401, 0.0906024729726943, 0.09774532418207582, 0.1207914665021157, 0.15875716875462242, 0.1070176662487543, 0.10511484148144368, 0.10238816301814335, 0.2553542604189909, 0.10267165755511809, 0.07700846029464614, 0.09748215259061688, 0.07789903723331004, 0.05903250263678039, 0.06825906531926718, 0.06781950697813732, 0.07735531845231014, 0.14538824856825283, 0.15328129024805526, 0.15444172263999012, 0.14331663586812482, 0.15187848150109107, 0.21320446615525912, 0.16809252674941677, 0.13187179031699015, 0.14689007985489466, 0.04278400552629569, 0.04017032127361142, 0.031393493129599714, 0.037178252784482946, 0.03425093938292756, 0.03936747688924258, 0.03451725423057728, 0.0410111276965337, 0.02979283154879686]}, "mutation_prompt": null}
{"id": "6d530bc1-dc60-4c8e-87e9-caf1456699e3", "solution": "import numpy as np\nimport random\n\nclass Novel_Metaheuristic_Algorithm:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.F = 0.5  # initial frequency\n        self.CR = 0.5  # initial crossover rate\n        self.pop_size = 50  # initial population size\n        self.population = np.random.uniform(-5.0, 5.0, (self.pop_size, self.dim))\n        self.best_solution = np.inf\n        self.probability = 0.5  # increased probability of probabilistic exploration\n        self.temperature = 100  # initial temperature for simulated annealing\n        self.covariance_matrix = np.eye(self.dim)  # initial covariance matrix\n        self.adaptive_step_size = 0.1  # initial adaptive step size for covariance matrix adaptation\n        self.step_size_learning_rate = 0.001  # learning rate for adaptive step size\n        self.levy_flight_probability = 0.3  # probability of levy flight\n        self.levy_flight_step_size = 0.1  # step size for levy flight\n        self.history = np.zeros((self.budget, self.dim))  # history of best solutions\n        self.cooling_rate = 0.99  # higher rate of temperature cooling\n\n    def evaluate(self, x):\n        return self.func(x)\n\n    def func(self, x):\n        # BBOB test suite of 24 noiseless functions\n        # Here we use the first function f1(x) = sum(x_i^2)\n        return np.sum(x**2)\n\n    def frequency_modulation(self, x):\n        # frequency modulation function\n        return self.F * np.sin(2 * np.pi * self.F * x)\n\n    def adaptive_frequency_modulation(self):\n        # adaptive frequency modulation\n        self.F *= 1.1  # increase frequency\n        if random.random() < self.probability:\n            self.F *= 0.9  # decrease frequency with probability\n        return self.F\n\n    def levy_flight(self, x):\n        # levy flight function\n        step = np.random.normal(0, self.levy_flight_step_size, self.dim)\n        return x + step / np.abs(step)\n\n    def crossover(self, x1, x2):\n        # crossover operation\n        r = np.random.rand(self.dim)\n        return x1 + r * (x2 - x1)\n\n    def selection(self, x1, x2):\n        # selection operation\n        if self.evaluate(x2) < self.evaluate(x1):\n            return x2\n        else:\n            return x1\n\n    def information_based_selection(self, x1, x2):\n        # information-based selection\n        if self.evaluate(x2) < self.evaluate(x1):\n            return x2\n        else:\n            if random.random() < 0.5:\n                return x2\n            else:\n                return x1\n\n    def simulated_annealing(self, x):\n        # simulated annealing\n        new_x = x + np.random.normal(0, 1, self.dim)\n        new_x = np.clip(new_x, -5.0, 5.0)\n        if self.evaluate(new_x) < self.evaluate(x):\n            return new_x\n        else:\n            probability = np.exp(-(self.evaluate(new_x) - self.evaluate(x)) / self.temperature)\n            if random.random() < probability:\n                return new_x\n            else:\n                return x\n        self.temperature *= self.cooling_rate  # decrease temperature with a higher rate\n\n    def covariance_matrix_adaptation(self):\n        # covariance matrix adaptation\n        self.covariance_matrix = (1 - self.adaptive_step_size) * self.covariance_matrix + self.adaptive_step_size * np.eye(self.dim)\n        return self.covariance_matrix\n\n    def adaptive_step_size_learning(self):\n        # adaptive step size learning\n        self.adaptive_step_size += self.step_size_learning_rate * (self.evaluate(self.population[0]) - self.evaluate(self.population[-1]))\n        return self.adaptive_step_size\n\n    def history_learning(self):\n        # history learning\n        self.history = np.vstack((self.history, self.population[0]))\n\n    def optimize(self, func):\n        for i in range(self.budget):\n            # evaluate population\n            fitness = [self.evaluate(x) for x in self.population]\n            # get best solution\n            self.best_solution = min(fitness)\n            best_index = fitness.index(self.best_solution)\n            # adaptive frequency modulation\n            self.F = self.adaptive_frequency_modulation()\n            # differential evolution\n            for j in range(self.pop_size):\n                # generate trial vector\n                trial = self.crossover(self.population[j], self.population[random.randint(0, self.pop_size - 1)])\n                # evaluate trial vector\n                trial_fitness = self.evaluate(trial)\n                # selection\n                self.population[j] = self.selection(trial, self.population[j])\n                # update best solution\n                if trial_fitness < self.evaluate(self.population[j]):\n                    self.population[j] = trial\n            # levy flight\n            for j in range(self.pop_size):\n                if random.random() < self.levy_flight_probability:\n                    self.population[j] = self.levy_flight(self.population[j])\n            # simulated annealing\n            for j in range(self.pop_size):\n                self.population[j] = self.simulated_annealing(self.population[j])\n            # information-based selection\n            for j in range(self.pop_size):\n                self.population[j] = self.information_based_selection(self.population[j], self.population[random.randint(0, self.pop_size - 1)])\n            # covariance matrix adaptation\n            self.covariance_matrix = self.covariance_matrix_adaptation()\n            # adaptive step size learning\n            self.adaptive_step_size = self.adaptive_step_size_learning()\n            # history learning\n            self.history_learning()\n        return self.best_solution, self.history\n\n    def __call__(self, func):\n        self.func = func\n        return self.optimize(func)\n\n# example usage\nbudget = 1000\ndim = 10\noptimizer = Novel_Metaheuristic_Algorithm(budget, dim)\nbest_solution, history = optimizer(lambda x: np.sum(x**2))\nprint(\"Best solution:\", best_solution)\nprint(\"History:\", history)", "name": "Novel_Metaheuristic_Algorithm", "description": "Novel Metaheuristic Algorithm with Adaptive Frequency Modulation, Covariance Matrix Adaptation, Simulated Annealing with Adaptive Temperature Cooling Schedule, Adaptive Step Size Learning, Levy Flight, and Information-based Selection, further refined with increased probabilistic exploration and higher temperature cooling rate.", "configspace": "", "generation": 83, "fitness": 0.11686132827611813, "feedback": "The algorithm Novel_Metaheuristic_Algorithm got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.12 with standard deviation 0.09.", "error": "", "parent_id": "88969cc7-3c67-4dfe-9d52-bcbaa0eb3640", "metadata": {"aucs": [0.24508534083882483, 0.2469760163894793, 0.2351820311316627, 0.2213715742337501, 0.22205675915658607, 0.23104510373775533, 0.22297599049799166, 0.25796396093083873, 0.22890662599151335, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.0013182052205984451, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.05705007280932228, 0.07600019746379238, 0.06136145118123881, 0.07396992986349193, 0.0629277937317192, 0.06034480636655348, 0.07639011571499377, 0.06900380665185235, 0.06905199467992329, 0.05625289456445404, 0.06600113679956066, 0.05551818456041291, 0.06002081636408563, 0.06527779457449723, 0.07061812197504891, 0.05665359962969674, 0.05798581458457697, 0.05998110936968426, 0.10746972456685633, 0.2560466728940254, 0.2719776517919892, 0.17002319632817686, 0.43087060613404893, 0.14526392195120574, 0.7360346053967919, 0.298629723055739, 0.15403417863531088, 0.11251358627416852, 0.10896751475078381, 0.1016310173869398, 0.08677899059494854, 0.0732268474966924, 0.10905448340316048, 0.11383038559205139, 0.11541276352547503, 0.13899017794184243, 0.1524628570605373, 0.183897015659795, 0.1489065553273089, 0.19474943090595942, 0.19006093821423276, 0.15702610790691018, 0.11876761838626937, 0.1925464140846015, 0.15673545337729333, 0.07938969427665887, 0.07666645349370416, 0.047995312470294094, 0.068054633143805, 0.044217215165850754, 0.08165267444878122, 0.10603216915821045, 0.06312393514919346, 0.06324710496575103, 0.0696360997846357, 0.0950469952368217, 0.08815408133212455, 0.09971845203343765, 0.09668141367640148, 0.09748552282123568, 0.11825488369173742, 0.11689359643768638, 0.07890549457583806, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.0811277608315697, 0.040214432400350275, 0.08996593279671217, 0.01926054964987911, 0.035655263644460145, 0.009692804176228864, 0.06477403384970426, 0.06955713846829881, 0.035536939945268076, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.04563624410049616, 0.01775240818395596, 0.023712547874108925, 0.0144666766586804, 0.02493549820659724, 0.03817101247180421, 0.027130889005031356, 0.04148984611915707, 0.04038107022922521, 0.23317531153611604, 0.240189022891093, 0.23873908757499818, 0.2518217951410471, 0.22490082608457773, 0.2442481850146313, 0.24526017886352525, 0.23966350777357115, 0.23026142077796896, 0.07637704773705534, 0.05975949678177861, 0.06714151062601592, 0.05715623249154056, 0.06257919203060447, 0.07255589754059133, 0.07155916295687492, 0.05207761560266022, 0.07160538946194916, 0.1372993972712253, 0.12459422426406996, 0.12893580236721913, 0.10858611038261046, 0.11969435852234944, 0.18080419976442041, 0.13522998242295292, 0.12265198840463798, 0.10918739618220263, 0.18041918247584854, 0.1667023898072455, 0.1989238120227843, 0.18447304162306877, 0.18458229553011252, 0.1843776803858692, 0.18389400129242162, 0.19108500540290174, 0.19223336151725867, 0.13513062423270839, 0.13024466915139232, 0.12750543764054934, 0.13932677072115007, 0.12933214362801881, 0.1362267473491704, 0.12499404432477057, 0.1242489965751431, 0.12977445180882619, 0.18821694744727724, 0.17184532485752757, 0.17381579225751198, 0.1893931329363453, 0.1632081632062451, 0.19434067328327387, 0.17335096851419507, 0.1680648287990596, 0.17778905853720883, 0.15783528441061334, 0.15582056935694, 0.1557785636631115, 0.15892480965853883, 0.15833997149702594, 0.16904692643512076, 0.17317345677369844, 0.15857019361333602, 0.14785182951480835, 0.16627592412002234, 0.14277131534227316, 0.15866922140123008, 0.2841336704706503, 0.16329582177904034, 0.15782605550977902, 0.18548720304649347, 0.19244304756604247, 0.1703189805513553, 0.14691259653480426, 0.13995247708076275, 0.08508422651717262, 0.16089730095703203, 0.1442708033929162, 0.1602448960201458, 0.1384970461080347, 0.11102391542073109, 0.11487571964202448, 0.18003593878669977, 0.16928751566781453, 0.1659153527000513, 0.1671567449854241, 0.16771692047181608, 0.16330700296996947, 0.17681254071556474, 0.16286212539040656, 0.1823366562132741, 0.06742675603944503, 0.06945028094304306, 0.0668020959538298, 0.05862328698195285, 0.07155827948320315, 0.06391946624855305, 0.07831468077212433, 0.058883709550109575, 0.05734537345764479]}, "mutation_prompt": null}
{"id": "77484b1a-b9fa-494b-a72e-916b20e57c9b", "solution": "import numpy as np\nimport random\n\nclass Novel_Metaheuristic_Algorithm:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.F = 0.5  # initial frequency\n        self.CR = 0.5  # initial crossover rate\n        self.pop_size = 50  # initial population size\n        self.population = np.random.uniform(-5.0, 5.0, (self.pop_size, self.dim))\n        self.best_solution = np.inf\n        self.probability = 0.4  # increased probability of probabilistic exploration\n        self.temperature = 100  # initial temperature for simulated annealing\n        self.covariance_matrix = np.eye(self.dim)  # initial covariance matrix\n        self.adaptive_step_size = 0.1  # initial adaptive step size for covariance matrix adaptation\n        self.step_size_learning_rate = 0.001  # learning rate for adaptive step size\n        self.levy_flight_probability = 0.2  # probability of levy flight\n        self.levy_flight_step_size = 0.1  # step size for levy flight\n        self.history = np.zeros((self.budget, self.dim))  # history of best solutions\n        self.cooling_rate = 0.999  # higher rate of temperature cooling\n\n    def evaluate(self, x):\n        return self.func(x)\n\n    def func(self, x):\n        # BBOB test suite of 24 noiseless functions\n        # Here we use the first function f1(x) = sum(x_i^2)\n        return np.sum(x**2)\n\n    def frequency_modulation(self, x):\n        # frequency modulation function\n        return self.F * np.sin(2 * np.pi * self.F * x)\n\n    def adaptive_frequency_modulation(self):\n        # adaptive frequency modulation\n        self.F *= 1.1  # increase frequency\n        if random.random() < self.probability:\n            self.F *= 0.9  # decrease frequency with probability\n        return self.F\n\n    def levy_flight(self, x):\n        # levy flight function\n        step = np.random.normal(0, self.levy_flight_step_size, self.dim)\n        return x + step / np.abs(step)\n\n    def crossover(self, x1, x2):\n        # crossover operation\n        r = np.random.rand(self.dim)\n        return x1 + r * (x2 - x1)\n\n    def selection(self, x1, x2):\n        # selection operation\n        if self.evaluate(x2) < self.evaluate(x1):\n            return x2\n        else:\n            return x1\n\n    def information_based_selection(self, x1, x2):\n        # information-based selection\n        if self.evaluate(x2) < self.evaluate(x1):\n            return x2\n        else:\n            if random.random() < 0.5:\n                return x2\n            else:\n                return x1\n\n    def simulated_annealing(self, x):\n        # simulated annealing\n        new_x = x + np.random.normal(0, 1, self.dim)\n        new_x = np.clip(new_x, -5.0, 5.0)\n        if self.evaluate(new_x) < self.evaluate(x):\n            return new_x\n        else:\n            probability = np.exp(-(self.evaluate(new_x) - self.evaluate(x)) / self.temperature)\n            if random.random() < probability:\n                return new_x\n            else:\n                return x\n        self.temperature *= self.cooling_rate  # decrease temperature with a higher rate\n\n    def covariance_matrix_adaptation(self):\n        # covariance matrix adaptation\n        self.covariance_matrix = (1 - self.adaptive_step_size) * self.covariance_matrix + self.adaptive_step_size * np.eye(self.dim)\n        return self.covariance_matrix\n\n    def adaptive_step_size_learning(self):\n        # adaptive step size learning\n        self.adaptive_step_size += self.step_size_learning_rate * (self.evaluate(self.population[0]) - self.evaluate(self.population[-1]))\n        return self.adaptive_step_size\n\n    def history_learning(self):\n        # history learning\n        self.history = np.vstack((self.history, self.population[0]))\n\n    def optimize(self, func):\n        for i in range(self.budget):\n            # evaluate population\n            fitness = [self.evaluate(x) for x in self.population]\n            # get best solution\n            self.best_solution = min(fitness)\n            best_index = fitness.index(self.best_solution)\n            # adaptive frequency modulation\n            self.F = self.adaptive_frequency_modulation()\n            # differential evolution\n            for j in range(self.pop_size):\n                # generate trial vector\n                trial = self.crossover(self.population[j], self.population[random.randint(0, self.pop_size - 1)])\n                # evaluate trial vector\n                trial_fitness = self.evaluate(trial)\n                # selection\n                self.population[j] = self.selection(trial, self.population[j])\n                # update best solution\n                if trial_fitness < self.evaluate(self.population[j]):\n                    self.population[j] = trial\n            # levy flight\n            for j in range(self.pop_size):\n                if random.random() < self.levy_flight_probability:\n                    self.population[j] = self.levy_flight(self.population[j])\n            # simulated annealing\n            for j in range(self.pop_size):\n                self.population[j] = self.simulated_annealing(self.population[j])\n            # information-based selection\n            for j in range(self.pop_size):\n                self.population[j] = self.information_based_selection(self.population[j], self.population[random.randint(0, self.pop_size - 1)])\n            # covariance matrix adaptation\n            self.covariance_matrix = self.covariance_matrix_adaptation()\n            # adaptive step size learning\n            self.adaptive_step_size = self.adaptive_step_size_learning()\n            # history learning\n            self.history_learning()\n            # probability 4 to change the individual lines of the selected solution\n            if i % 4 == 0:\n                self.population[best_index] = self.population[random.randint(0, self.pop_size - 1)]\n        return self.best_solution, self.history\n\n    def __call__(self, func):\n        self.func = func\n        return self.optimize(func)\n\n# example usage\nbudget = 1000\ndim = 10\noptimizer = Novel_Metaheuristic_Algorithm(budget, dim)\nbest_solution, history = optimizer(lambda x: np.sum(x**2))\nprint(\"Best solution:\", best_solution)\nprint(\"History:\", history)", "name": "Novel_Metaheuristic_Algorithm", "description": "Novel Metaheuristic Algorithm with Adaptive Frequency Modulation, Covariance Matrix Adaptation, and Simulated Annealing with Adaptive Temperature Cooling Schedule and Adaptive Step Size Learning with an added feature of Levy Flight and Information-based Selection, refined to increase the probability of probabilistic exploration and to use a higher rate of temperature cooling, with probability 4 to change the individual lines of the selected solution to refine its strategy.", "configspace": "", "generation": 84, "fitness": 0.1152244766762777, "feedback": "The algorithm Novel_Metaheuristic_Algorithm got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.12 with standard deviation 0.07.", "error": "", "parent_id": "88969cc7-3c67-4dfe-9d52-bcbaa0eb3640", "metadata": {"aucs": [0.243531259807654, 0.21703453348433566, 0.24889290216677695, 0.21654885310001704, 0.20739296494485338, 0.2224897003847376, 0.23134657651086443, 0.23530461638710853, 0.23658885178207933, 9.999999999998899e-05, 0.042323565575897004, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.023232286949802394, 9.999999999998899e-05, 0.06502331991370192, 0.07789264379078775, 0.06757624413201102, 0.06814270704205139, 0.08367431946415504, 0.05824772126362043, 0.06530028579286662, 0.07753077357622384, 0.06608858209741597, 0.06356600739171026, 0.05386569266824537, 0.05371699581612721, 0.08674596045219007, 0.061856822027346436, 0.05143629417551643, 0.06976272111837512, 0.06323397280665666, 0.040750664077581544, 0.12471892340645352, 0.14390050813299604, 0.12252114798795732, 0.2554047493036795, 0.1344730402869242, 0.1420251951040714, 0.1695993157505411, 0.16570866811573737, 0.14391107774577505, 0.1004312490212631, 0.09147035144211235, 0.10643877118428513, 0.13416523828460036, 0.08710160386928167, 0.10305293109732305, 0.10462968763566838, 0.11508354514364472, 0.11428403651197094, 0.17379018529881973, 0.18023735819874875, 0.13530476944856218, 0.1884124795057669, 0.21950910694076298, 0.18509236076293423, 0.1496802548250451, 0.15797263985483168, 0.18318254239754028, 0.07510258426969363, 0.04697062560030896, 0.06378058946541565, 0.01813984163049731, 0.06505162027337685, 0.08260727218804076, 0.10753142220546763, 0.08539576157953144, 0.06598274424936468, 0.10476869130965483, 0.06784639786937441, 0.0919780332064134, 0.0944874926337147, 0.06058036414292023, 0.1309351334115494, 0.1262205267311236, 0.09993468300933805, 0.11921821440664282, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.06638086401401122, 0.017823231994556044, 0.08301678684142877, 0.04347918793816119, 0.06571408880746399, 0.052983906756246646, 0.07743163958914234, 0.12480000300712368, 0.02278008765216577, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.03825964635265011, 0.042912973817750166, 0.024487589931205367, 0.040736628008440356, 0.032398753839495864, 0.047564686045502236, 0.025961257976575336, 0.052631581242138625, 0.03698658729515791, 0.2442571781420133, 0.25207768795857133, 0.2522992153134168, 0.23531257818465878, 0.2095909567130787, 0.22137160369903652, 0.28910130706182213, 0.22813006063609842, 0.24568600324733347, 0.06246868054828558, 0.05799532838477517, 0.056401181947330015, 0.06480395938743533, 0.06520722462253759, 0.0717329776452863, 0.05544083248783849, 0.069490520528106, 0.07426569984807918, 0.11040473461459277, 0.16278733023634684, 0.15282118322154836, 0.13067810966750926, 0.13545193884010642, 0.10798713552628325, 0.10998338405828068, 0.11601334810539266, 0.1138855410589853, 0.1779180550861751, 0.17385418024611443, 0.20198044036118867, 0.1899043326567159, 0.20658116122670833, 0.1919681512234368, 0.18546888032520714, 0.19771754215544024, 0.1884266241144168, 0.10729124103734644, 0.1195573442363137, 0.13672627787392955, 0.13861058560140205, 0.15923002484775317, 0.134696684684526, 0.11867542568771883, 0.17270839149770256, 0.1273294722269004, 0.16735566494222054, 0.16608828301829415, 0.16901996558770893, 0.17581708916578587, 0.1660922476832084, 0.18623865572868858, 0.18282319778989908, 0.18380438578677827, 0.17336514643883694, 0.1580881987816528, 0.16024670079917336, 0.16864984336547872, 0.154814937121726, 0.14958044590616648, 0.167139315510073, 0.16062291763437075, 0.16108373240542784, 0.16763782384642745, 0.16951370074783356, 0.16796779846890908, 0.22368552562977662, 0.21372001504552085, 0.13691250396084909, 0.17153787225857853, 0.21386743808432251, 0.20205701012498734, 0.13664213506089984, 0.22288312221902074, 0.16564126216219466, 0.2693608168285577, 0.14879111716858462, 0.17394408736000366, 0.17537143290288026, 0.16667921252556006, 0.15783868324620398, 0.13434215195979793, 0.16732243890605192, 0.16705289390057843, 0.17778199034256126, 0.1652490917235222, 0.17865081454184606, 0.18516802718823855, 0.18092252875898218, 0.18399294112258036, 0.17230553553346306, 0.0787694279100638, 0.06429939679071628, 0.055566742827461324, 0.06067998826733234, 0.058064170290054284, 0.06738508909255214, 0.05978478501358353, 0.06926990254347587, 0.06612516803312607]}, "mutation_prompt": null}
{"id": "9e0a25c6-d361-4f9f-88d3-ca523102c6c3", "solution": "import numpy as np\nimport random\n\nclass Novel_Metaheuristic_Algorithm:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.F = 0.5  # initial frequency\n        self.CR = 0.5  # initial crossover rate\n        self.pop_size = 50  # initial population size\n        self.population = np.random.uniform(-5.0, 5.0, (self.pop_size, self.dim))\n        self.best_solution = np.inf\n        self.probability = 0.4  # increased probability of probabilistic exploration\n        self.temperature = 100  # initial temperature for simulated annealing\n        self.covariance_matrix = np.eye(self.dim)  # initial covariance matrix\n        self.adaptive_step_size = 0.1  # initial adaptive step size for covariance matrix adaptation\n        self.step_size_learning_rate = 0.001  # learning rate for adaptive step size\n        self.levy_flight_probability = 0.2  # probability of levy flight\n        self.levy_flight_step_size = 0.1  # step size for levy flight\n        self.history = np.zeros((self.budget, self.dim))  # history of best solutions\n        self.cooling_rate = 0.999  # higher rate of temperature cooling\n        self.learning_rate = 0.01  # learning rate for history learning\n\n    def evaluate(self, x):\n        return self.func(x)\n\n    def func(self, x):\n        # BBOB test suite of 24 noiseless functions\n        # Here we use the first function f1(x) = sum(x_i^2)\n        return np.sum(x**2)\n\n    def frequency_modulation(self, x):\n        # frequency modulation function\n        return self.F * np.sin(2 * np.pi * self.F * x)\n\n    def adaptive_frequency_modulation(self):\n        # adaptive frequency modulation\n        self.F *= 1.1  # increase frequency\n        if random.random() < self.probability:\n            self.F *= 0.9  # decrease frequency with probability\n        return self.F\n\n    def levy_flight(self, x):\n        # levy flight function\n        step = np.random.normal(0, self.levy_flight_step_size, self.dim)\n        return x + step / np.abs(step)\n\n    def crossover(self, x1, x2):\n        # crossover operation\n        r = np.random.rand(self.dim)\n        return x1 + r * (x2 - x1)\n\n    def selection(self, x1, x2):\n        # selection operation\n        if self.evaluate(x2) < self.evaluate(x1):\n            return x2\n        else:\n            return x1\n\n    def information_based_selection(self, x1, x2):\n        # information-based selection\n        if self.evaluate(x2) < self.evaluate(x1):\n            return x2\n        else:\n            if random.random() < 0.5:\n                return x2\n            else:\n                return x1\n\n    def simulated_annealing(self, x):\n        # simulated annealing\n        new_x = x + np.random.normal(0, 1, self.dim)\n        new_x = np.clip(new_x, -5.0, 5.0)\n        if self.evaluate(new_x) < self.evaluate(x):\n            return new_x\n        else:\n            probability = np.exp(-(self.evaluate(new_x) - self.evaluate(x)) / self.temperature)\n            if random.random() < probability:\n                return new_x\n            else:\n                return x\n        self.temperature *= self.cooling_rate  # decrease temperature with a higher rate\n\n    def covariance_matrix_adaptation(self):\n        # covariance matrix adaptation\n        self.covariance_matrix = (1 - self.adaptive_step_size) * self.covariance_matrix + self.adaptive_step_size * np.eye(self.dim)\n        return self.covariance_matrix\n\n    def adaptive_step_size_learning(self):\n        # adaptive step size learning\n        self.adaptive_step_size += self.step_size_learning_rate * (self.evaluate(self.population[0]) - self.evaluate(self.population[-1]))\n        return self.adaptive_step_size\n\n    def history_learning(self):\n        # history learning\n        self.history = np.vstack((self.history, self.population[0]))\n        self.learning_rate *= 0.99  # decrease learning rate over time\n        self.adaptive_step_size += self.learning_rate * (self.evaluate(self.population[0]) - self.evaluate(self.population[-1]))\n\n    def optimize(self, func):\n        for i in range(self.budget):\n            # evaluate population\n            fitness = [self.evaluate(x) for x in self.population]\n            # get best solution\n            self.best_solution = min(fitness)\n            best_index = fitness.index(self.best_solution)\n            # adaptive frequency modulation\n            self.F = self.adaptive_frequency_modulation()\n            # differential evolution\n            for j in range(self.pop_size):\n                # generate trial vector\n                trial = self.crossover(self.population[j], self.population[random.randint(0, self.pop_size - 1)])\n                # evaluate trial vector\n                trial_fitness = self.evaluate(trial)\n                # selection\n                self.population[j] = self.selection(trial, self.population[j])\n                # update best solution\n                if trial_fitness < self.evaluate(self.population[j]):\n                    self.population[j] = trial\n            # levy flight\n            for j in range(self.pop_size):\n                if random.random() < self.levy_flight_probability:\n                    self.population[j] = self.levy_flight(self.population[j])\n            # simulated annealing\n            for j in range(self.pop_size):\n                self.population[j] = self.simulated_annealing(self.population[j])\n            # information-based selection\n            for j in range(self.pop_size):\n                self.population[j] = self.information_based_selection(self.population[j], self.population[random.randint(0, self.pop_size - 1)])\n            # covariance matrix adaptation\n            self.covariance_matrix = self.covariance_matrix_adaptation()\n            # adaptive step size learning\n            self.adaptive_step_size = self.adaptive_step_size_learning()\n            # history learning\n            self.history_learning()\n        return self.best_solution, self.history\n\n    def __call__(self, func):\n        self.func = func\n        return self.optimize(func)\n\n# example usage\nbudget = 1000\ndim = 10\noptimizer = Novel_Metaheuristic_Algorithm(budget, dim)\nbest_solution, history = optimizer(lambda x: np.sum(x**2))\nprint(\"Best solution:\", best_solution)\nprint(\"History:\", history)", "name": "Novel_Metaheuristic_Algorithm", "description": "Novel Metaheuristic Algorithm with Adaptive Frequency Modulation, Covariance Matrix Adaptation, Simulated Annealing, Levy Flight, and Information-based Selection with a higher rate of temperature cooling and increased probability of probabilistic exploration, and Adaptive Step Size Learning with an added feature of history learning.", "configspace": "", "generation": 85, "fitness": 0.11769261232192117, "feedback": "The algorithm Novel_Metaheuristic_Algorithm got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.12 with standard deviation 0.08.", "error": "", "parent_id": "88969cc7-3c67-4dfe-9d52-bcbaa0eb3640", "metadata": {"aucs": [0.24379847215994188, 0.2514280515081493, 0.25128795122565684, 0.22960872931568188, 0.2692205248330283, 0.21556747239903107, 0.22885128140805377, 0.22051667346912196, 0.23114508619069185, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.0023802932526978404, 0.0037935059789637737, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.07234104813022968, 0.06280196485767575, 0.0677793457101834, 0.0839503516506166, 0.08141584319648787, 0.061747464760831594, 0.08072751717533033, 0.06508275625050686, 0.07006673666490948, 0.07320408781848653, 0.060575235971936814, 0.056682636778554674, 0.052666506071691765, 0.05717330864251258, 0.06351187456263341, 0.049683897219908846, 0.055494678017603105, 0.05743783268266722, 0.28530026974537637, 0.14509965615025233, 0.1361973460106396, 0.14104632105172732, 0.5628898550469953, 0.27629920433551247, 0.24830323280492916, 0.12933642222075337, 0.39521830302511884, 0.11016648400251938, 0.10333368750541794, 0.08789487105175342, 0.11590660846277112, 0.11863048441998258, 0.08126240199287404, 0.10641668524759462, 0.06082389721723325, 0.09178368958069094, 0.17218617279742188, 0.144206488163669, 0.12441135870222786, 0.18953012885026055, 0.20309379327473698, 0.19092812679636761, 0.14227784844534963, 0.1730498638714194, 0.1654672356685707, 0.05114322822008055, 0.047098451248204554, 0.08511195466903976, 0.11471828751230717, 0.07754679791066155, 0.06903769114726488, 0.12398356763158214, 0.14310520668169358, 0.12923652567802923, 0.10378614202670855, 0.09023210949707516, 0.09541961816046574, 0.09260919879027574, 0.09061669691398233, 0.08847731092201172, 0.07150665071434048, 0.10390266401407677, 0.10463067623783173, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.03585674350497081, 0.07433905829514409, 0.06967470642137996, 0.010885917480557894, 0.01816168803003415, 0.08225481490715103, 0.0319156925988211, 0.03867743968632198, 0.051213480591636995, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.023108329220447033, 0.05620601824932181, 0.012599774563189614, 0.03667540988908746, 0.05494214985151924, 0.027708296030556245, 0.040248644511913345, 0.025446134431067913, 0.03241123254847467, 0.26456646237517456, 0.24224306899379577, 0.22483290318094862, 0.2226884895619341, 0.2580150116092216, 0.24867904927748974, 0.26959685628051533, 0.25623224585296833, 0.23495908826336165, 0.05942587837100488, 0.06519251204009635, 0.06328399668658424, 0.05985490042345576, 0.09033520871787182, 0.08250095308587568, 0.06536925764644075, 0.07374260614118644, 0.06187075322907243, 0.16706487134589076, 0.12276216419608976, 0.11842175776965747, 0.1112336565704416, 0.11533414135003717, 0.12047461505173229, 0.135795564875417, 0.12001501635097889, 0.15858427211994708, 0.17483987178130922, 0.15977342567370156, 0.18575239334798577, 0.18466038165155274, 0.18669480669480887, 0.19364840192872845, 0.17625112666729748, 0.20013138387033502, 0.17788189736323945, 0.12108931604505813, 0.14073074639911887, 0.13300929611595325, 0.1273005059569643, 0.1406764748041156, 0.13944295219181946, 0.12795275449976617, 0.1422432907844564, 0.12817553845121454, 0.17600045817915966, 0.18901277865768362, 0.1813000665727933, 0.18984059783837814, 0.16969473531715762, 0.18780244159588755, 0.17226176430159557, 0.1776556548964885, 0.20269294202341703, 0.16116680109692794, 0.1632215324963745, 0.166046075959715, 0.15717117303227224, 0.16507087735436254, 0.15738646710429616, 0.1578637433524953, 0.1621469845829373, 0.15652721708129935, 0.17428757745121493, 0.15845116897884548, 0.16001527374865598, 0.24630128688387554, 0.19288020002219908, 0.19819118366264887, 0.11275264204658997, 0.11745766147526249, 0.13009859157327486, 0.20587765266821745, 0.20572151824751972, 0.16884046403999997, 0.1359180550092346, 0.1588101362174994, 0.15873512465714956, 0.11376862689429723, 0.1674691761882826, 0.1277706471953517, 0.1658432734487586, 0.17758289743522582, 0.188482563943141, 0.16426237223359352, 0.16820175510116608, 0.14989583681105634, 0.17476498599615842, 0.17056842846541387, 0.16804272467314363, 0.07213158882182324, 0.07089441322857326, 0.06534783483769568, 0.06300179735191391, 0.06569878088650383, 0.059699354426092754, 0.06383551915859587, 0.06632146319172516, 0.059499634393240575]}, "mutation_prompt": null}
{"id": "2a173169-09ae-4edb-b71e-816070d5693b", "solution": "import numpy as np\nimport random\n\nclass Novel_Metaheuristic_Algorithm:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.F = 0.5  # initial frequency\n        self.CR = 0.5  # initial crossover rate\n        self.pop_size = 50  # initial population size\n        self.population = np.random.uniform(-5.0, 5.0, (self.pop_size, self.dim))\n        self.best_solution = np.inf\n        self.probability = 0.4  # increased probability of probabilistic exploration\n        self.temperature = 100  # initial temperature for simulated annealing\n        self.covariance_matrix = np.eye(self.dim)  # initial covariance matrix\n        self.adaptive_step_size = 0.1  # initial adaptive step size for covariance matrix adaptation\n        self.step_size_learning_rate = 0.001  # learning rate for adaptive step size\n        self.levy_flight_probability = 0.2  # probability of levy flight\n        self.levy_flight_step_size = 0.1  # step size for levy flight\n        self.history = np.zeros((self.budget, self.dim))  # history of best solutions\n        self.cooling_rate = 0.999  # higher rate of temperature cooling\n        self.mutation_rate = 0.1  # mutation rate for simulated annealing\n\n    def evaluate(self, x):\n        return self.func(x)\n\n    def func(self, x):\n        # BBOB test suite of 24 noiseless functions\n        # Here we use the first function f1(x) = sum(x_i^2)\n        return np.sum(x**2)\n\n    def frequency_modulation(self, x):\n        # frequency modulation function\n        return self.F * np.sin(2 * np.pi * self.F * x)\n\n    def adaptive_frequency_modulation(self):\n        # adaptive frequency modulation\n        self.F *= 1.1  # increase frequency\n        if random.random() < self.probability:\n            self.F *= 0.9  # decrease frequency with probability\n        return self.F\n\n    def levy_flight(self, x):\n        # levy flight function\n        step = np.random.normal(0, self.levy_flight_step_size, self.dim)\n        return x + step / np.abs(step)\n\n    def crossover(self, x1, x2):\n        # crossover operation\n        r = np.random.rand(self.dim)\n        return x1 + r * (x2 - x1)\n\n    def selection(self, x1, x2):\n        # selection operation\n        if self.evaluate(x2) < self.evaluate(x1):\n            return x2\n        else:\n            return x1\n\n    def information_based_selection(self, x1, x2):\n        # information-based selection\n        if self.evaluate(x2) < self.evaluate(x1):\n            return x2\n        else:\n            if random.random() < 0.5:\n                return x2\n            else:\n                return x1\n\n    def simulated_annealing(self, x):\n        # simulated annealing\n        new_x = x + np.random.normal(0, 1, self.dim)\n        new_x = np.clip(new_x, -5.0, 5.0)\n        if self.evaluate(new_x) < self.evaluate(x):\n            return new_x\n        else:\n            probability = np.exp(-(self.evaluate(new_x) - self.evaluate(x)) / self.temperature)\n            if random.random() < probability:\n                return new_x\n            else:\n                return x\n        self.temperature *= self.cooling_rate  # decrease temperature with a higher rate\n        if random.random() < self.mutation_rate:\n            new_x = self.levy_flight(x)\n            if self.evaluate(new_x) < self.evaluate(x):\n                return new_x\n            else:\n                return x\n\n    def covariance_matrix_adaptation(self):\n        # covariance matrix adaptation\n        self.covariance_matrix = (1 - self.adaptive_step_size) * self.covariance_matrix + self.adaptive_step_size * np.eye(self.dim)\n        return self.covariance_matrix\n\n    def adaptive_step_size_learning(self):\n        # adaptive step size learning\n        self.adaptive_step_size += self.step_size_learning_rate * (self.evaluate(self.population[0]) - self.evaluate(self.population[-1]))\n        return self.adaptive_step_size\n\n    def history_learning(self):\n        # history learning\n        self.history = np.vstack((self.history, self.population[0]))\n\n    def optimize(self, func):\n        for i in range(self.budget):\n            # evaluate population\n            fitness = [self.evaluate(x) for x in self.population]\n            # get best solution\n            self.best_solution = min(fitness)\n            best_index = fitness.index(self.best_solution)\n            # adaptive frequency modulation\n            self.F = self.adaptive_frequency_modulation()\n            # differential evolution\n            for j in range(self.pop_size):\n                # generate trial vector\n                trial = self.crossover(self.population[j], self.population[random.randint(0, self.pop_size - 1)])\n                # evaluate trial vector\n                trial_fitness = self.evaluate(trial)\n                # selection\n                self.population[j] = self.selection(trial, self.population[j])\n                # update best solution\n                if trial_fitness < self.evaluate(self.population[j]):\n                    self.population[j] = trial\n            # levy flight\n            for j in range(self.pop_size):\n                if random.random() < self.levy_flight_probability:\n                    self.population[j] = self.levy_flight(self.population[j])\n            # simulated annealing\n            for j in range(self.pop_size):\n                self.population[j] = self.simulated_annealing(self.population[j])\n            # information-based selection\n            for j in range(self.pop_size):\n                self.population[j] = self.information_based_selection(self.population[j], self.population[random.randint(0, self.pop_size - 1)])\n            # covariance matrix adaptation\n            self.covariance_matrix = self.covariance_matrix_adaptation()\n            # adaptive step size learning\n            self.adaptive_step_size = self.adaptive_step_size_learning()\n            # history learning\n            self.history_learning()\n        return self.best_solution, self.history\n\n    def __call__(self, func):\n        self.func = func\n        return self.optimize(func)\n\n# example usage\nbudget = 1000\ndim = 10\noptimizer = Novel_Metaheuristic_Algorithm(budget, dim)\nbest_solution, history = optimizer(lambda x: np.sum(x**2))\nprint(\"Best solution:\", best_solution)\nprint(\"History:\", history)", "name": "Novel_Metaheuristic_Algorithm", "description": "Novel Metaheuristic Algorithm with Adaptive Frequency Modulation, Covariance Matrix Adaptation, Simulated Annealing, and Levy Flight with an added feature of Adaptive Temperature Cooling Schedule, Adaptive Step Size Learning, and Information-based Selection, refined to increase the probability of probabilistic exploration and to use a higher rate of temperature cooling.", "configspace": "", "generation": 86, "fitness": 0.11572680988500286, "feedback": "The algorithm Novel_Metaheuristic_Algorithm got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.12 with standard deviation 0.08.", "error": "", "parent_id": "88969cc7-3c67-4dfe-9d52-bcbaa0eb3640", "metadata": {"aucs": [0.26483834587652544, 0.22663447197593556, 0.24158003209614443, 0.21668188971676405, 0.23201180529571097, 0.22967195415551045, 0.23906354320363465, 0.2426855753615711, 0.2242097061257119, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.05484589901959536, 0.07926039725676381, 0.0653228947988087, 0.0941641949890325, 0.06299313217958424, 0.0739108441248798, 0.06929763712133608, 0.0626471674793102, 0.0817271414459263, 0.04879652372856835, 0.0507129949841395, 0.05649473963959517, 0.06607969948009285, 0.052328863580785034, 0.058036191545708604, 0.061474707273073026, 0.06548437762957815, 0.05700434755122441, 0.2985336663204473, 0.0990183614129021, 0.15307076676414166, 0.19251279870146776, 0.21739558234115208, 0.1513788963037478, 0.3481760101775244, 0.13224062222272626, 0.3444713520210224, 0.1155632552797049, 0.09094916861560054, 0.10301079225616538, 0.10615423778982658, 0.0895867071871993, 0.09826258960663437, 0.1687867032638224, 0.08196838772273041, 0.09821710228575053, 0.17037274940660552, 0.16418310973933514, 0.17128899239931994, 0.1665221983558839, 0.1676496613129761, 0.17527750940201825, 0.14217793387087319, 0.12985685428603067, 0.17087588589177882, 0.03441171821613065, 0.09048187151104592, 0.09984420067335009, 0.0503811380030591, 0.06495802088122982, 0.08587488125189324, 0.07464676150344718, 0.09550141750501573, 0.09047017969972293, 0.05384157942886669, 0.09865880428952856, 0.10759069008788213, 0.11690656158717139, 0.0961384275728604, 0.12438291791304601, 0.0901460148886355, 0.10009664122951667, 0.10565404388335287, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.003589080585697735, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.08867186803528515, 0.054683912767364395, 0.05121046378143179, 0.01609775620428122, 0.036854068152847086, 0.029646045953276023, 0.045362096268367846, 0.029920905301906386, 0.02895342262595446, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.020380108820432508, 0.03446170753468247, 0.057995695635788436, 0.042672268345416486, 0.05155334930715871, 0.042529722864715236, 0.04102036286649546, 0.03924020587847221, 0.05156519190856035, 0.2526662695499631, 0.23192292912114765, 0.2539693770328255, 0.22893384179074838, 0.22145558904184204, 0.25336462806821203, 0.2469104634894861, 0.22951111639992527, 0.28928047958739345, 0.05476150951023517, 0.07066635394602439, 0.05793381664546282, 0.0687761502680383, 0.08606948092569855, 0.0689851800083221, 0.06032752812373654, 0.07688364465555497, 0.06927311647750434, 0.10279501651727874, 0.1395691186307726, 0.11842175776965747, 0.11514578478323001, 0.1342730203312178, 0.11896337645310706, 0.1279448431991601, 0.16366818973334574, 0.11794308918139362, 0.1820091658998264, 0.20617757220972366, 0.1849238215379876, 0.18662388607298774, 0.1868385808562315, 0.19029797182315988, 0.18815435386180457, 0.19667998709431544, 0.18892021023371997, 0.12215514614990441, 0.13208520863117934, 0.12040602269597234, 0.1305513008784821, 0.15161692795536674, 0.14589173272738876, 0.1227484559298877, 0.14137732793966895, 0.13000377449242007, 0.1797019817834955, 0.16878077962997362, 0.1782870261201056, 0.17109389567019495, 0.1754204855567496, 0.18926529578562412, 0.17342395004535316, 0.18652595617835643, 0.16452763908424728, 0.16485571037207558, 0.16554896244140926, 0.15498160012195406, 0.15810009224745025, 0.16393874605849812, 0.16926219916555751, 0.16736877095061553, 0.1697473160571199, 0.16663525792214495, 0.15944609881143712, 0.16706679577354622, 0.16468390627731988, 0.23984165323740791, 0.15739886115205393, 0.20854343744887438, 0.21254282363550803, 0.14342505645855874, 0.13129852774336737, 0.15539903239759711, 0.16120415353819284, 0.21487662032668042, 0.13977809952103448, 0.14558478002726094, 0.1528257099854996, 0.10190309574361311, 0.12666568783440346, 0.17009803748456598, 0.17377579907730778, 0.1704872420859307, 0.172456313115127, 0.16070005795446773, 0.16026137592330036, 0.1739224910881385, 0.17072669444130628, 0.1804778488561577, 0.1616614862723763, 0.059258667427004585, 0.07133578827921339, 0.06281090242757581, 0.07522052334248619, 0.06053504836420509, 0.0666432130345247, 0.06788739163613378, 0.06243498178657503, 0.08401487369284244]}, "mutation_prompt": null}
{"id": "61fc0601-f2dd-492a-86cb-f0579f42e3ac", "solution": "import numpy as np\nimport random\n\nclass Novel_Metaheuristic_Algorithm:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.F = 0.5  # initial frequency\n        self.CR = 0.5  # initial crossover rate\n        self.pop_size = 50  # initial population size\n        self.population = np.random.uniform(-5.0, 5.0, (self.pop_size, self.dim))\n        self.best_solution = np.inf\n        self.probability = 0.46  # increased probability of probabilistic exploration\n        self.temperature = 100  # initial temperature for simulated annealing\n        self.covariance_matrix = np.eye(self.dim)  # initial covariance matrix\n        self.adaptive_step_size = 0.1  # initial adaptive step size for covariance matrix adaptation\n        self.step_size_learning_rate = 0.001  # learning rate for adaptive step size\n        self.levy_flight_probability = 0.2  # probability of levy flight\n        self.levy_flight_step_size = 0.1  # step size for levy flight\n        self.history = np.zeros((self.budget, self.dim))  # history of best solutions\n        self.cooling_rate = 0.999  # higher rate of temperature cooling\n\n    def evaluate(self, x):\n        return self.func(x)\n\n    def func(self, x):\n        # BBOB test suite of 24 noiseless functions\n        # Here we use the first function f1(x) = sum(x_i^2)\n        return np.sum(x**2)\n\n    def frequency_modulation(self, x):\n        # frequency modulation function\n        return self.F * np.sin(2 * np.pi * self.F * x)\n\n    def adaptive_frequency_modulation(self):\n        # adaptive frequency modulation\n        self.F *= 1.1  # increase frequency\n        if random.random() < self.probability:\n            self.F *= 0.9  # decrease frequency with probability\n        return self.F\n\n    def levy_flight(self, x):\n        # levy flight function\n        step = np.random.normal(0, self.levy_flight_step_size, self.dim)\n        return x + step / np.abs(step)\n\n    def crossover(self, x1, x2):\n        # crossover operation\n        r = np.random.rand(self.dim)\n        return x1 + r * (x2 - x1)\n\n    def selection(self, x1, x2):\n        # selection operation\n        if self.evaluate(x2) < self.evaluate(x1):\n            return x2\n        else:\n            return x1\n\n    def information_based_selection(self, x1, x2):\n        # information-based selection\n        if self.evaluate(x2) < self.evaluate(x1):\n            return x2\n        else:\n            if random.random() < 0.5:\n                return x2\n            else:\n                return x1\n\n    def simulated_annealing(self, x):\n        # simulated annealing\n        new_x = x + np.random.normal(0, 1, self.dim)\n        new_x = np.clip(new_x, -5.0, 5.0)\n        if self.evaluate(new_x) < self.evaluate(x):\n            return new_x\n        else:\n            probability = np.exp(-(self.evaluate(new_x) - self.evaluate(x)) / self.temperature)\n            if random.random() < probability:\n                return new_x\n            else:\n                return x\n        self.temperature *= self.cooling_rate  # decrease temperature with a higher rate\n\n    def covariance_matrix_adaptation(self):\n        # covariance matrix adaptation\n        self.covariance_matrix = (1 - self.adaptive_step_size) * self.covariance_matrix + self.adaptive_step_size * np.eye(self.dim)\n        return self.covariance_matrix\n\n    def adaptive_step_size_learning(self):\n        # adaptive step size learning\n        self.adaptive_step_size += self.step_size_learning_rate * (self.evaluate(self.population[0]) - self.evaluate(self.population[-1]))\n        return self.adaptive_step_size\n\n    def history_learning(self):\n        # history learning\n        self.history = np.vstack((self.history, self.population[0]))\n\n    def optimize(self, func):\n        for i in range(self.budget):\n            # evaluate population\n            fitness = [self.evaluate(x) for x in self.population]\n            # get best solution\n            self.best_solution = min(fitness)\n            best_index = fitness.index(self.best_solution)\n            # adaptive frequency modulation\n            self.F = self.adaptive_frequency_modulation()\n            # differential evolution\n            for j in range(self.pop_size):\n                # generate trial vector\n                trial = self.crossover(self.population[j], self.population[random.randint(0, self.pop_size - 1)])\n                # evaluate trial vector\n                trial_fitness = self.evaluate(trial)\n                # selection\n                self.population[j] = self.selection(trial, self.population[j])\n                # update best solution\n                if trial_fitness < self.evaluate(self.population[j]):\n                    self.population[j] = trial\n            # levy flight\n            for j in range(self.pop_size):\n                if random.random() < self.levy_flight_probability:\n                    self.population[j] = self.levy_flight(self.population[j])\n            # simulated annealing\n            for j in range(self.pop_size):\n                self.population[j] = self.simulated_annealing(self.population[j])\n            # information-based selection\n            for j in range(self.pop_size):\n                self.population[j] = self.information_based_selection(self.population[j], self.population[random.randint(0, self.pop_size - 1)])\n            # covariance matrix adaptation\n            self.covariance_matrix = self.covariance_matrix_adaptation()\n            # adaptive step size learning\n            self.adaptive_step_size = self.adaptive_step_size_learning()\n            # history learning\n            self.history_learning()\n        return self.best_solution, self.history\n\n    def __call__(self, func):\n        self.func = func\n        return self.optimize(func)\n\n# example usage\nbudget = 1000\ndim = 10\noptimizer = Novel_Metaheuristic_Algorithm(budget, dim)\nbest_solution, history = optimizer(lambda x: np.sum(x**2))\nprint(\"Best solution:\", best_solution)\nprint(\"History:\", history)", "name": "Novel_Metaheuristic_Algorithm", "description": "Novel Metaheuristic Algorithm with Adaptive Frequency Modulation, Covariance Matrix Adaptation, Simulated Annealing, and Levy Flight, refined to use a higher rate of temperature cooling and increased probability of probabilistic exploration with Information-based Selection and Adaptive Step Size Learning.", "configspace": "", "generation": 87, "fitness": 0.12211778214782026, "feedback": "The algorithm Novel_Metaheuristic_Algorithm got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.12 with standard deviation 0.09.", "error": "", "parent_id": "88969cc7-3c67-4dfe-9d52-bcbaa0eb3640", "metadata": {"aucs": [0.2611975896917712, 0.2409543998003264, 0.24122250346241159, 0.24255987279200486, 0.22368505397416327, 0.22035100184712508, 0.26472053084706726, 0.24940649660633074, 0.21609119557843992, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.026723770042792028, 9.999999999998899e-05, 0.05604226798504208, 0.06436464344552972, 0.06130360448585348, 0.06782397293140474, 0.0705197832496488, 0.05994419500653403, 0.08132375236984024, 0.06268555931380904, 0.061498768905314694, 0.04673985746826015, 0.0628226672297989, 0.05131893533822951, 0.05933212720442094, 0.052131558132236244, 0.05134153293806287, 0.07349150465632615, 0.05784618121636409, 0.06661479656638636, 0.33205500304625235, 0.243653108445119, 0.12314931332955803, 0.2376881922542322, 0.2266417892543332, 0.5539374202381453, 0.44988356341651603, 0.14443207973612793, 0.7008575046121612, 0.1056057360970224, 0.11073417327144619, 0.07389829145674776, 0.10281846690159069, 0.07181241637436797, 0.09433917073099563, 0.10789895997520305, 0.08936949458233867, 0.10855615156881182, 0.1457291544174557, 0.14777936819780024, 0.19014175414710832, 0.1865597571127131, 0.1661379436582764, 0.15030854207280941, 0.2007534115887487, 0.13895423827163011, 0.2178320793286328, 0.08668162576067462, 0.09772753595964645, 0.08615146131786988, 0.04773690415293386, 0.1316132118663791, 0.007865616498309969, 0.09953213744628797, 0.09457262736887695, 0.1107869266963395, 0.08317726379191426, 0.10715524199940141, 0.08520500876845705, 0.05317085807151867, 0.0944005353091949, 0.10093260681941196, 0.1116579107589123, 0.18393305451479613, 0.11798797560192875, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.05142499161992842, 0.000670504378120107, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.06127134315568861, 0.053246732656796136, 0.0892261893301407, 0.026850684561230964, 0.028428606045924787, 0.018901761422703056, 0.06753066868497104, 0.06554595738457869, 0.04006719658110047, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.02795363092598402, 0.014858096249458552, 0.06187968629145402, 0.052381797473331826, 0.04986500839565522, 0.02868408802880351, 0.021092562330686793, 0.027963074600834248, 0.046908915895129355, 0.24616937450483223, 0.2396625490634574, 0.23163540843951713, 0.26038506935307926, 0.23302593346470246, 0.22684476139585663, 0.252527868685554, 0.31517457646498126, 0.2399984776342159, 0.050561114248133476, 0.053954128777733645, 0.05541478732389582, 0.07020433672783999, 0.05505875654836778, 0.06541242953087423, 0.07210212239561076, 0.05753499651050242, 0.07024881981751441, 0.10327998871748001, 0.1189443160117819, 0.11842175776965747, 0.1041668394065568, 0.12478733730389935, 0.1267594604483384, 0.11694136874436012, 0.117972629074941, 0.13162209736588992, 0.18688191466865234, 0.19865041604755218, 0.18342523688341716, 0.18694548176278392, 0.19499573955111438, 0.202277754008242, 0.17985151918622067, 0.19288644073108063, 0.1883711738759779, 0.11901209386549128, 0.11676294940815846, 0.1322720947029613, 0.1294286137081443, 0.13556403336721667, 0.16820007474861287, 0.11147952890226187, 0.1319935578386232, 0.13346398840011497, 0.17582258167059295, 0.18472158727550447, 0.20782439527597596, 0.18893163644101862, 0.192944855882208, 0.18419019824330685, 0.17635210996679285, 0.19976224451300573, 0.16201098436978845, 0.1700373184771804, 0.16489708287673244, 0.16445749696026557, 0.16053808642112266, 0.15758343061708602, 0.15441197859960942, 0.16823869350297105, 0.15997018221709491, 0.15671162417453666, 0.2034464836327361, 0.17154758126846392, 0.21719506504727393, 0.20737627606132714, 0.17567348233492008, 0.16989195302806315, 0.13996851856957615, 0.14880029272752415, 0.20734740652488237, 0.12341745436422258, 0.15686996484474824, 0.17943220975463448, 0.11518103731110263, 0.19596958389889807, 0.154144957367769, 0.12548600068435067, 0.2608352227731793, 0.18509750268354253, 0.18497112627698598, 0.16206108625868743, 0.16380260563050575, 0.18204243193434433, 0.16624960454740212, 0.1556515560368713, 0.15722962274250418, 0.1760813542158245, 0.16934665073029065, 0.07207852180486718, 0.07156157445280242, 0.07215564527076768, 0.07253714232942143, 0.06450390541930284, 0.06016317518087344, 0.05198373484439911, 0.05301688720329201, 0.0706177154148121]}, "mutation_prompt": null}
{"id": "a2488e5c-9818-4f66-898e-eee300e81899", "solution": "import numpy as np\nimport random\n\nclass Novel_Metaheuristic_Algorithm:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.F = 0.5  # initial frequency\n        self.CR = 0.5  # initial crossover rate\n        self.pop_size = 50  # initial population size\n        self.population = np.random.uniform(-5.0, 5.0, (self.pop_size, self.dim))\n        self.best_solution = np.inf\n        self.probability = 0.46  # increased probability of probabilistic exploration\n        self.temperature = 100  # initial temperature for simulated annealing\n        self.covariance_matrix = np.eye(self.dim)  # initial covariance matrix\n        self.adaptive_step_size = 0.1  # initial adaptive step size for covariance matrix adaptation\n        self.step_size_learning_rate = 0.001  # learning rate for adaptive step size\n        self.levy_flight_probability = 0.2  # probability of levy flight\n        self.levy_flight_step_size = 0.1  # step size for levy flight\n        self.history = np.zeros((self.budget, self.dim))  # history of best solutions\n        self.cooling_rate = 0.999  # higher rate of temperature cooling\n        self.change_probability = 0.46  # probability of changing individual lines of the selected solution\n\n    def evaluate(self, x):\n        return self.func(x)\n\n    def func(self, x):\n        # BBOB test suite of 24 noiseless functions\n        # Here we use the first function f1(x) = sum(x_i^2)\n        return np.sum(x**2)\n\n    def frequency_modulation(self, x):\n        # frequency modulation function\n        return self.F * np.sin(2 * np.pi * self.F * x)\n\n    def adaptive_frequency_modulation(self):\n        # adaptive frequency modulation\n        self.F *= 1.1  # increase frequency\n        if random.random() < self.probability:\n            self.F *= 0.9  # decrease frequency with probability\n        return self.F\n\n    def levy_flight(self, x):\n        # levy flight function\n        step = np.random.normal(0, self.levy_flight_step_size, self.dim)\n        return x + step / np.abs(step)\n\n    def crossover(self, x1, x2):\n        # crossover operation\n        r = np.random.rand(self.dim)\n        return x1 + r * (x2 - x1)\n\n    def selection(self, x1, x2):\n        # selection operation\n        if self.evaluate(x2) < self.evaluate(x1):\n            return x2\n        else:\n            return x1\n\n    def information_based_selection(self, x1, x2):\n        # information-based selection\n        if self.evaluate(x2) < self.evaluate(x1):\n            return x2\n        else:\n            if random.random() < 0.5:\n                return x2\n            else:\n                return x1\n\n    def simulated_annealing(self, x):\n        # simulated annealing\n        new_x = x + np.random.normal(0, 1, self.dim)\n        new_x = np.clip(new_x, -5.0, 5.0)\n        if self.evaluate(new_x) < self.evaluate(x):\n            return new_x\n        else:\n            probability = np.exp(-(self.evaluate(new_x) - self.evaluate(x)) / self.temperature)\n            if random.random() < probability:\n                return new_x\n            else:\n                return x\n        self.temperature *= self.cooling_rate  # decrease temperature with a higher rate\n\n    def covariance_matrix_adaptation(self):\n        # covariance matrix adaptation\n        self.covariance_matrix = (1 - self.adaptive_step_size) * self.covariance_matrix + self.adaptive_step_size * np.eye(self.dim)\n        return self.covariance_matrix\n\n    def adaptive_step_size_learning(self):\n        # adaptive step size learning\n        self.adaptive_step_size += self.step_size_learning_rate * (self.evaluate(self.population[0]) - self.evaluate(self.population[-1]))\n        return self.adaptive_step_size\n\n    def history_learning(self):\n        # history learning\n        self.history = np.vstack((self.history, self.population[0]))\n\n    def change_individual_lines(self):\n        # change individual lines of the selected solution\n        for i in range(self.pop_size):\n            if random.random() < self.change_probability:\n                self.population[i] = self.population[random.randint(0, self.pop_size - 1)]\n\n    def optimize(self, func):\n        for i in range(self.budget):\n            # evaluate population\n            fitness = [self.evaluate(x) for x in self.population]\n            # get best solution\n            self.best_solution = min(fitness)\n            best_index = fitness.index(self.best_solution)\n            # adaptive frequency modulation\n            self.F = self.adaptive_frequency_modulation()\n            # differential evolution\n            for j in range(self.pop_size):\n                # generate trial vector\n                trial = self.crossover(self.population[j], self.population[random.randint(0, self.pop_size - 1)])\n                # evaluate trial vector\n                trial_fitness = self.evaluate(trial)\n                # selection\n                self.population[j] = self.selection(trial, self.population[j])\n                # update best solution\n                if trial_fitness < self.evaluate(self.population[j]):\n                    self.population[j] = trial\n            # levy flight\n            for j in range(self.pop_size):\n                if random.random() < self.levy_flight_probability:\n                    self.population[j] = self.levy_flight(self.population[j])\n            # simulated annealing\n            for j in range(self.pop_size):\n                self.population[j] = self.simulated_annealing(self.population[j])\n            # information-based selection\n            for j in range(self.pop_size):\n                self.population[j] = self.information_based_selection(self.population[j], self.population[random.randint(0, self.pop_size - 1)])\n            # change individual lines of the selected solution\n            self.change_individual_lines()\n            # covariance matrix adaptation\n            self.covariance_matrix = self.covariance_matrix_adaptation()\n            # adaptive step size learning\n            self.adaptive_step_size = self.adaptive_step_size_learning()\n            # history learning\n            self.history_learning()\n        return self.best_solution, self.history\n\n    def __call__(self, func):\n        self.func = func\n        return self.optimize(func)\n\n# example usage\nbudget = 1000\ndim = 10\noptimizer = Novel_Metaheuristic_Algorithm(budget, dim)\nbest_solution, history = optimizer(lambda x: np.sum(x**2))\nprint(\"Best solution:\", best_solution)\nprint(\"History:\", history)", "name": "Novel_Metaheuristic_Algorithm", "description": "A novel metaheuristic algorithm with Adaptive Frequency Modulation, Covariance Matrix Adaptation, Simulated Annealing, and Levy Flight, refined to use a higher rate of temperature cooling and increased probability of probabilistic exploration with Information-based Selection and Adaptive Step Size Learning, and further refined to use a probability of 0.46 to change the individual lines of the selected solution to refine its strategy.", "configspace": "", "generation": 88, "fitness": 0.1158115022941947, "feedback": "The algorithm Novel_Metaheuristic_Algorithm got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.12 with standard deviation 0.08.", "error": "", "parent_id": "61fc0601-f2dd-492a-86cb-f0579f42e3ac", "metadata": {"aucs": [0.23510864549628652, 0.24709872968506885, 0.24402082664945068, 0.22803005262818266, 0.20172350890507051, 0.23643608817772876, 0.24000983911589058, 0.22542516091647535, 0.22617774292081216, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.07720904997209221, 0.06028849647725165, 0.07286969436582746, 0.07025473600035803, 0.060741289114586094, 0.07786052723684245, 0.0665314501933737, 0.06773815762372792, 0.05817001537766786, 0.0542270871183157, 0.053790262458913274, 0.04808948988580597, 0.07383020852884825, 0.047449985084833, 0.04990023531781973, 0.06647806895936559, 0.06104833615662231, 0.0523698695264706, 0.13275687620297105, 0.22593288583160143, 0.17610157806137616, 0.481777763171228, 0.19733674113819866, 0.224858279957314, 0.5354923639949964, 0.13980733469723494, 0.43356491492525473, 0.11071911953634916, 0.10667835349978083, 0.08923310661323136, 0.0907315338467406, 0.09621159101345678, 0.1104396111444157, 0.0842312154685173, 0.11652068116709247, 0.07330641344790834, 0.13028425388587006, 0.14824097292981642, 0.17891688333889122, 0.16548326205094277, 0.16774581389238197, 0.14587619927390916, 0.1353443268674258, 0.1632570956402748, 0.1270023837085733, 0.11625153236796137, 0.025104613361993655, 0.02874029079864049, 0.0732909374101619, 0.08754499875543864, 0.046788863664787406, 0.08908678056026309, 0.08689403953630803, 0.10335409171462506, 0.04344946643736347, 0.08956181033071209, 0.06306746185856105, 0.12045976530150915, 0.08950625051915018, 0.0721332111917602, 0.09533362467414319, 0.07420739476151628, 0.0941617746560891, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.002591704887072388, 0.08241440870280825, 0.06901546815715576, 0.06265963642296468, 0.020627135550565057, 0.03490777180463611, 0.03516667637314708, 0.029756953832393296, 0.1067288667315841, 0.07433083211624947, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.00268967351952909, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.026187678164406814, 0.023186639873955417, 0.007793004021809269, 0.03490345494125391, 0.05577546592648319, 0.026028716969035304, 0.028497446528272352, 0.034604718407927226, 0.0264767439007092, 0.24460804746301712, 0.23609139949230273, 0.2511154754803223, 0.22710297375881683, 0.2170035753885512, 0.22476355482475263, 0.22713090176534967, 0.24056864700066227, 0.2478833210271837, 0.07953759023326878, 0.052712348186888636, 0.062170255893182924, 0.06107570616998459, 0.072689233136907, 0.07183415040443286, 0.0614202720746827, 0.0549349133125816, 0.06930711578592685, 0.10483514838817753, 0.15720362879867666, 0.11908644770030463, 0.13697287067944164, 0.15100227757207008, 0.11348480803346805, 0.11656120658199598, 0.12441981001453606, 0.11552390736490192, 0.18787009971378488, 0.1814523601489344, 0.21732398625371996, 0.19542875688624461, 0.2038646937762112, 0.17965377972608243, 0.19788664652722732, 0.17591643279008773, 0.1945945477719344, 0.11046184490858346, 0.12052229339140086, 0.14330423062234454, 0.1462259018000266, 0.1244340738310239, 0.12061549590664467, 0.13695028639682316, 0.11811748978199155, 0.13962100832985214, 0.17491497986688687, 0.20072871166586403, 0.18428207913593997, 0.19140842584685602, 0.170329080809261, 0.20815015860845265, 0.21769773807429593, 0.17526670339170014, 0.1822321345335004, 0.15474542700176463, 0.15680736546120055, 0.15916258954189966, 0.16529115406183315, 0.16725393071100592, 0.15952889253600755, 0.16126230454707258, 0.16366122109833703, 0.15689850114498727, 0.15809525071393238, 0.16553493042678202, 0.1456094369530767, 0.20006384230131746, 0.12341933103159808, 0.15263494389890198, 0.11952967400438685, 0.14238806187228603, 0.16693047703828745, 0.15268574400036083, 0.15017653763184913, 0.22466873561245326, 0.1556764646200418, 0.13927935321279405, 0.1439232185950745, 0.13469121452838484, 0.15418532273541918, 0.17095470796337864, 0.17469252768469068, 0.1817426776562172, 0.188482563943141, 0.16336292344853887, 0.17080674545635555, 0.20204445794581039, 0.16109076596472804, 0.16605353181238203, 0.17577081059909838, 0.05608277983304033, 0.059008143066679675, 0.06969390118404195, 0.05659628008923068, 0.058133698628514474, 0.07293716669412986, 0.06727865738939787, 0.0581244286704794, 0.07152126127469671]}, "mutation_prompt": null}
{"id": "4e593bb8-e5d3-4e7c-964b-5fbfd31bef80", "solution": "import numpy as np\nimport random\n\nclass Novel_Metaheuristic_Algorithm:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.F = 0.5  # initial frequency\n        self.CR = 0.5  # initial crossover rate\n        self.pop_size = 50  # initial population size\n        self.population = np.random.uniform(-5.0, 5.0, (self.pop_size, self.dim))\n        self.best_solution = np.inf\n        self.probability = 0.46  # increased probability of probabilistic exploration\n        self.temperature = 100  # initial temperature for simulated annealing\n        self.covariance_matrix = np.eye(self.dim)  # initial covariance matrix\n        self.adaptive_step_size = 0.1  # initial adaptive step size for covariance matrix adaptation\n        self.step_size_learning_rate = 0.001  # learning rate for adaptive step size\n        self.levy_flight_probability = 0.2  # probability of levy flight\n        self.levy_flight_step_size = 0.1  # step size for levy flight\n        self.history = np.zeros((self.budget, self.dim))  # history of best solutions\n        self.cooling_rate = 0.999  # higher rate of temperature cooling\n        self.mutation_probability = 0.3  # initial mutation probability\n        self.dynamic_mutation = True  # dynamic mutation\n\n    def evaluate(self, x):\n        return self.func(x)\n\n    def func(self, x):\n        # BBOB test suite of 24 noiseless functions\n        # Here we use the first function f1(x) = sum(x_i^2)\n        return np.sum(x**2)\n\n    def frequency_modulation(self, x):\n        # frequency modulation function\n        return self.F * np.sin(2 * np.pi * self.F * x)\n\n    def adaptive_frequency_modulation(self):\n        # adaptive frequency modulation\n        self.F *= 1.1  # increase frequency\n        if random.random() < self.probability:\n            self.F *= 0.9  # decrease frequency with probability\n        return self.F\n\n    def levy_flight(self, x):\n        # levy flight function\n        step = np.random.normal(0, self.levy_flight_step_size, self.dim)\n        return x + step / np.abs(step)\n\n    def crossover(self, x1, x2):\n        # crossover operation\n        r = np.random.rand(self.dim)\n        return x1 + r * (x2 - x1)\n\n    def selection(self, x1, x2):\n        # selection operation\n        if self.evaluate(x2) < self.evaluate(x1):\n            return x2\n        else:\n            return x1\n\n    def information_based_selection(self, x1, x2):\n        # information-based selection\n        if self.evaluate(x2) < self.evaluate(x1):\n            return x2\n        else:\n            if random.random() < 0.5:\n                return x2\n            else:\n                return x1\n\n    def simulated_annealing(self, x):\n        # simulated annealing\n        new_x = x + np.random.normal(0, 1, self.dim)\n        new_x = np.clip(new_x, -5.0, 5.0)\n        if self.evaluate(new_x) < self.evaluate(x):\n            return new_x\n        else:\n            probability = np.exp(-(self.evaluate(new_x) - self.evaluate(x)) / self.temperature)\n            if random.random() < probability:\n                return new_x\n            else:\n                return x\n        self.temperature *= self.cooling_rate  # decrease temperature with a higher rate\n\n    def covariance_matrix_adaptation(self):\n        # covariance matrix adaptation\n        self.covariance_matrix = (1 - self.adaptive_step_size) * self.covariance_matrix + self.adaptive_step_size * np.eye(self.dim)\n        return self.covariance_matrix\n\n    def adaptive_step_size_learning(self):\n        # adaptive step size learning\n        self.adaptive_step_size += self.step_size_learning_rate * (self.evaluate(self.population[0]) - self.evaluate(self.population[-1]))\n        return self.adaptive_step_size\n\n    def history_learning(self):\n        # history learning\n        self.history = np.vstack((self.history, self.population[0]))\n\n    def adaptive_differential_evolution(self):\n        # adaptive differential evolution\n        for j in range(self.pop_size):\n            # generate trial vector\n            trial = self.crossover(self.population[j], self.population[random.randint(0, self.pop_size - 1)])\n            # apply mutation\n            if random.random() < self.mutation_probability:\n                trial += np.random.normal(0, 1, self.dim)\n            # evaluate trial vector\n            trial_fitness = self.evaluate(trial)\n            # selection\n            self.population[j] = self.selection(trial, self.population[j])\n            # update mutation probability\n            if trial_fitness < self.evaluate(self.population[j]):\n                self.mutation_probability *= 1.1\n            else:\n                self.mutation_probability *= 0.9\n            # dynamic mutation\n            if self.dynamic_mutation:\n                self.mutation_probability = np.random.uniform(0, 1)\n\n    def optimize(self, func):\n        for i in range(self.budget):\n            # evaluate population\n            fitness = [self.evaluate(x) for x in self.population]\n            # get best solution\n            self.best_solution = min(fitness)\n            best_index = fitness.index(self.best_solution)\n            # adaptive frequency modulation\n            self.F = self.adaptive_frequency_modulation()\n            # differential evolution\n            for j in range(self.pop_size):\n                # generate trial vector\n                trial = self.crossover(self.population[j], self.population[random.randint(0, self.pop_size - 1)])\n                # evaluate trial vector\n                trial_fitness = self.evaluate(trial)\n                # selection\n                self.population[j] = self.selection(trial, self.population[j])\n                # update best solution\n                if trial_fitness < self.evaluate(self.population[j]):\n                    self.population[j] = trial\n            # levy flight\n            for j in range(self.pop_size):\n                if random.random() < self.levy_flight_probability:\n                    self.population[j] = self.levy_flight(self.population[j])\n            # simulated annealing\n            for j in range(self.pop_size):\n                self.population[j] = self.simulated_annealing(self.population[j])\n            # information-based selection\n            for j in range(self.pop_size):\n                self.population[j] = self.information_based_selection(self.population[j], self.population[random.randint(0, self.pop_size - 1)])\n            # adaptive differential evolution\n            self.adaptive_differential_evolution()\n            # covariance matrix adaptation\n            self.covariance_matrix = self.covariance_matrix_adaptation()\n            # adaptive step size learning\n            self.adaptive_step_size = self.adaptive_step_size_learning()\n            # history learning\n            self.history_learning()\n        return self.best_solution, self.history\n\n    def __call__(self, func):\n        self.func = func\n        return self.optimize(func)\n\n# example usage\nbudget = 1000\ndim = 10\noptimizer = Novel_Metaheuristic_Algorithm(budget, dim)\nbest_solution, history = optimizer(lambda x: np.sum(x**2))\nprint(\"Best solution:\", best_solution)\nprint(\"History:\", history)", "name": "Novel_Metaheuristic_Algorithm", "description": "Novel Metaheuristic Algorithm with Adaptive Frequency Modulation, Covariance Matrix Adaptation, Simulated Annealing, and Levy Flight, refined to use a higher rate of temperature cooling and increased probability of probabilistic exploration with Information-based Selection and Adaptive Step Size Learning, and also adding a new component of Adaptive Differential Evolution with Dynamic Mutation Probability.", "configspace": "", "generation": 89, "fitness": 0.12195826416947436, "feedback": "The algorithm Novel_Metaheuristic_Algorithm got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.12 with standard deviation 0.09.", "error": "", "parent_id": "61fc0601-f2dd-492a-86cb-f0579f42e3ac", "metadata": {"aucs": [0.25678034354241963, 0.2642435313851176, 0.25613517709990763, 0.25089547514576127, 0.23764050642263124, 0.24442728134120306, 0.2318708780899701, 0.22535753093065047, 0.22953760300258075, 0.0018357616515182196, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.06138510864614599, 0.07328183078911665, 0.08492115463564309, 0.07222147472246254, 0.08279108280888636, 0.07136783619442144, 0.08331427072747033, 0.08193357786141242, 0.07324023820763437, 0.05034352162939393, 0.07327685313759102, 0.054417574744955344, 0.06421800768090657, 0.05906287644617192, 0.061534927820154905, 0.06694776703603389, 0.054404664731895536, 0.048672377540956435, 0.27332152801036014, 0.21809055747997352, 0.11622637536787972, 0.24785264679025032, 0.5210459923602279, 0.11190577750692954, 0.41702447317005453, 0.3236078728048969, 0.5861134180079466, 0.12311599320698319, 0.0908209439937191, 0.10088921671455553, 0.10584727481651768, 0.10260506078252507, 0.10084840329337952, 0.13912679797917082, 0.13171508724687753, 0.09542999172927846, 0.1576357805676749, 0.18323707261671518, 0.1646034268159654, 0.17118050466705204, 0.17325731735458083, 0.18350410527686567, 0.12996720997315947, 0.18944651748022556, 0.1465049775856846, 0.030387805371322796, 0.06639374130697795, 0.07995637315508397, 0.07994135527918078, 0.04750514377209869, 0.08798375326216334, 0.07321058501162336, 0.08624169807514936, 0.08066375698849149, 0.09714495832762382, 0.09823074377885355, 0.0504530720203169, 0.10169233914715115, 0.12760347895387225, 0.10104465526903372, 0.10889144029169306, 0.10559799009416615, 0.09285142534728386, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.004791234884190088, 9.999999999998899e-05, 0.0006680046299619713, 9.999999999998899e-05, 0.003972509541998237, 0.07741458741133689, 0.09726601244044208, 0.034554689185454124, 0.048421932872296014, 0.022935495413086593, 0.027901156340137234, 0.07600186531355357, 0.09080610002971357, 0.07246316539091135, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.013771401379659487, 0.04729582532223853, 0.029140071634048748, 0.02527380941939994, 0.05684467088757561, 0.02220724604387947, 0.028896057728865165, 0.04298112043617952, 0.04503780619598985, 0.24484344972513028, 0.263153781605846, 0.25029776412643834, 0.24327480372012877, 0.22791920946783784, 0.25225001098170974, 0.27514808218345077, 0.2420729980601789, 0.2445117884363882, 0.06036707765970384, 0.06156473163818632, 0.06270614080214165, 0.07703489370421868, 0.046524560260518544, 0.06717731821442996, 0.07004263058461324, 0.07068292018920963, 0.06594662665855044, 0.11476125069223886, 0.11488109669644964, 0.12268496817046615, 0.1170098925251496, 0.11288939202485493, 0.10950781735234583, 0.12630755416204142, 0.11442584571144765, 0.15259236682719857, 0.1931791901229667, 0.18085459014623118, 0.1856162777610173, 0.20610558726527406, 0.1893206453199976, 0.20068617216401108, 0.18119068606050037, 0.19899662796775763, 0.18671407671551565, 0.11940143908220602, 0.1513067662418356, 0.13254309129512332, 0.1564859797967163, 0.14935197156760083, 0.1608280139786109, 0.12745241059120438, 0.14047087810182257, 0.1326346175101758, 0.16526341594733984, 0.1725491762230409, 0.1746751485048722, 0.16432785954743667, 0.17455323422523128, 0.18191345149008042, 0.19904279567770233, 0.1734709299495818, 0.17524748952506908, 0.15772331621796765, 0.16401273753703716, 0.1601174759519559, 0.16562128609379145, 0.1638737859559607, 0.1618655230939099, 0.15677575027793134, 0.15810980499172, 0.15647671682684072, 0.18580163212726208, 0.15939734927567084, 0.1543802354713547, 0.22350339276385622, 0.14226095427935048, 0.20159695314391257, 0.2114336166706261, 0.1450557472699343, 0.13043614705196327, 0.2285904674443513, 0.12834227461449677, 0.21771578380405399, 0.20677473761526755, 0.21181729779034708, 0.15555614595955702, 0.1841413880507149, 0.12881528478359727, 0.18926560811295234, 0.16871343478321577, 0.15429189949825106, 0.18849094906971053, 0.15551741625130922, 0.178498509161226, 0.15833226835570346, 0.1641359805080861, 0.1589873917096094, 0.1721676063211478, 0.07585030255801772, 0.05944915282795715, 0.08287482545364855, 0.06135250043213891, 0.05862239424240867, 0.0689828494199255, 0.06665620325733379, 0.06298590737673626, 0.06512992571838916]}, "mutation_prompt": null}
{"id": "3d21d674-c591-41dd-a221-471fe385c890", "solution": "import numpy as np\nimport random\n\nclass Novel_Metaheuristic_Algorithm:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.F = 0.5  # initial frequency\n        self.CR = 0.5  # initial crossover rate\n        self.pop_size = 50  # initial population size\n        self.population = np.random.uniform(-5.0, 5.0, (self.pop_size, self.dim))\n        self.best_solution = np.inf\n        self.probability = 0.46  # increased probability of probabilistic exploration\n        self.temperature = 100  # initial temperature for simulated annealing\n        self.covariance_matrix = np.eye(self.dim)  # initial covariance matrix\n        self.adaptive_step_size = 0.1  # initial adaptive step size for covariance matrix adaptation\n        self.step_size_learning_rate = 0.001  # learning rate for adaptive step size\n        self.levy_flight_probability = 0.2  # probability of levy flight\n        self.levy_flight_step_size = 0.1  # step size for levy flight\n        self.history = np.zeros((self.budget, self.dim))  # history of best solutions\n        self.cooling_rate = 0.999  # higher rate of temperature cooling\n        self.adaptive_learning_rate = 0.01  # adaptive learning rate for covariance matrix adaptation\n\n    def evaluate(self, x):\n        return self.func(x)\n\n    def func(self, x):\n        # BBOB test suite of 24 noiseless functions\n        # Here we use the first function f1(x) = sum(x_i^2)\n        return np.sum(x**2)\n\n    def frequency_modulation(self, x):\n        # frequency modulation function\n        return self.F * np.sin(2 * np.pi * self.F * x)\n\n    def adaptive_frequency_modulation(self):\n        # adaptive frequency modulation\n        self.F *= 1.1  # increase frequency\n        if random.random() < self.probability:\n            self.F *= 0.9  # decrease frequency with probability\n        return self.F\n\n    def levy_flight(self, x):\n        # levy flight function\n        step = np.random.normal(0, self.levy_flight_step_size, self.dim)\n        return x + step / np.abs(step)\n\n    def crossover(self, x1, x2):\n        # crossover operation\n        r = np.random.rand(self.dim)\n        return x1 + r * (x2 - x1)\n\n    def selection(self, x1, x2):\n        # selection operation\n        if self.evaluate(x2) < self.evaluate(x1):\n            return x2\n        else:\n            return x1\n\n    def information_based_selection(self, x1, x2):\n        # information-based selection\n        if self.evaluate(x2) < self.evaluate(x1):\n            return x2\n        else:\n            if random.random() < 0.5:\n                return x2\n            else:\n                return x1\n\n    def simulated_annealing(self, x):\n        # simulated annealing\n        new_x = x + np.random.normal(0, 1, self.dim)\n        new_x = np.clip(new_x, -5.0, 5.0)\n        if self.evaluate(new_x) < self.evaluate(x):\n            return new_x\n        else:\n            probability = np.exp(-(self.evaluate(new_x) - self.evaluate(x)) / self.temperature)\n            if random.random() < probability:\n                return new_x\n            else:\n                return x\n        self.temperature *= self.cooling_rate  # decrease temperature with a higher rate\n\n    def covariance_matrix_adaptation(self):\n        # covariance matrix adaptation\n        self.covariance_matrix = (1 - self.adaptive_step_size) * self.covariance_matrix + self.adaptive_step_size * np.eye(self.dim)\n        self.adaptive_step_size += self.adaptive_learning_rate * (self.evaluate(self.population[0]) - self.evaluate(self.population[-1]))\n        return self.covariance_matrix\n\n    def history_learning(self):\n        # history learning\n        self.history = np.vstack((self.history, self.population[0]))\n\n    def optimize(self, func):\n        for i in range(self.budget):\n            # evaluate population\n            fitness = [self.evaluate(x) for x in self.population]\n            # get best solution\n            self.best_solution = min(fitness)\n            best_index = fitness.index(self.best_solution)\n            # adaptive frequency modulation\n            self.F = self.adaptive_frequency_modulation()\n            # differential evolution\n            for j in range(self.pop_size):\n                # generate trial vector\n                trial = self.crossover(self.population[j], self.population[random.randint(0, self.pop_size - 1)])\n                # evaluate trial vector\n                trial_fitness = self.evaluate(trial)\n                # selection\n                self.population[j] = self.selection(trial, self.population[j])\n                # update best solution\n                if trial_fitness < self.evaluate(self.population[j]):\n                    self.population[j] = trial\n            # levy flight\n            for j in range(self.pop_size):\n                if random.random() < self.levy_flight_probability:\n                    self.population[j] = self.levy_flight(self.population[j])\n            # simulated annealing\n            for j in range(self.pop_size):\n                self.population[j] = self.simulated_annealing(self.population[j])\n            # information-based selection\n            for j in range(self.pop_size):\n                self.population[j] = self.information_based_selection(self.population[j], self.population[random.randint(0, self.pop_size - 1)])\n            # covariance matrix adaptation\n            self.covariance_matrix = self.covariance_matrix_adaptation()\n            # history learning\n            self.history_learning()\n        return self.best_solution, self.history\n\n    def __call__(self, func):\n        self.func = func\n        return self.optimize(func)\n\n# example usage\nbudget = 1000\ndim = 10\noptimizer = Novel_Metaheuristic_Algorithm(budget, dim)\nbest_solution, history = optimizer(lambda x: np.sum(x**2))\nprint(\"Best solution:\", best_solution)\nprint(\"History:\", history)", "name": "Novel_Metaheuristic_Algorithm", "description": "Novel Metaheuristic Algorithm with Adaptive Frequency Modulation, Covariance Matrix Adaptation, Simulated Annealing, and Levy Flight, refined to use a higher rate of temperature cooling, increased probability of probabilistic exploration, and Information-based Selection with Adaptive Step Size Learning and Covariance Matrix Adaptation with adaptive learning rate.", "configspace": "", "generation": 90, "fitness": 0.12019441219614789, "feedback": "The algorithm Novel_Metaheuristic_Algorithm got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.12 with standard deviation 0.09.", "error": "", "parent_id": "61fc0601-f2dd-492a-86cb-f0579f42e3ac", "metadata": {"aucs": [0.2553876170931437, 0.2146978769562845, 0.30862674024815806, 0.22494034674670438, 0.22591195587053514, 0.24153969475073667, 0.23898819053036446, 0.23955041756683615, 0.20382784812741894, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.07688486434594255, 0.06976126519040904, 0.06643104675535305, 0.056778295621530694, 0.06574144284071648, 0.05065078937168088, 0.06060748195678822, 0.060462253249020215, 0.07662372032002185, 0.04319359532994238, 0.04926233071536712, 0.05870246547947078, 0.06162395259786113, 0.04938325039113356, 0.03886329119320964, 0.055362923717583556, 0.07059106823359484, 0.047431814881120804, 0.12734557437075678, 0.1431461938352866, 0.16606136449500686, 0.17970732673753487, 0.5618022380422032, 0.497438079547859, 0.11549249778699322, 0.4677316039916455, 0.6199902413723424, 0.1180289672524808, 0.09373868660837803, 0.0933359940097005, 0.09881236955083272, 0.07716359599458755, 0.11443772931021101, 0.09364651223536935, 0.07145192338941875, 0.07255867223681212, 0.16288289444704862, 0.17771960600798464, 0.20289152021404944, 0.16080782805051275, 0.19006779698429832, 0.161437991443656, 0.15502233782968666, 0.1617690216065535, 0.13945882504766305, 0.08375319669895487, 0.1404415796257894, 0.04781780977572092, 0.11713284703141758, 0.06334154728344565, 0.02909043722861482, 0.09702308753311484, 0.07698243129846416, 0.07514560902160172, 0.08487426800393127, 0.07407281724171721, 0.09836583348624939, 0.11112409766546316, 0.096535557331338, 0.07328688843039488, 0.07647646756641358, 0.09432870492240764, 0.1060938608450166, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.0006891796381113258, 0.06215685617793276, 0.05910181173056572, 0.019108980604965686, 0.05213424329087635, 0.04800539785092439, 0.016621098750160268, 0.03974023379540126, 0.054667291801311135, 0.04132963897701525, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.033210910727145326, 0.02570715656329703, 0.031631901377495164, 0.022931902811813232, 0.03651664833280688, 0.0393196879273221, 0.03020961504619224, 0.04877631350878486, 0.047438733067990735, 0.2470020411083621, 0.26820932743092396, 0.27096504136246125, 0.22328937456764186, 0.2352859951596139, 0.22307708316348818, 0.2622909828144552, 0.23493805237406074, 0.26301994555318586, 0.0642597908642003, 0.04836585850425146, 0.04946873443684274, 0.06645944204071708, 0.05884975427421146, 0.06639235374463737, 0.06552301509695313, 0.061751151609421906, 0.06303929288542165, 0.12980582085228598, 0.11933438794139983, 0.14851811497596767, 0.13520758181187864, 0.11486733681555739, 0.11354473380061958, 0.11605253242774871, 0.12173315123917161, 0.1420935076793014, 0.20508431663175375, 0.18510668576160094, 0.19692033763850858, 0.18162925852828826, 0.19179544833112616, 0.1986141311645615, 0.16182506906200866, 0.1990433168499447, 0.1824022631236113, 0.14884744398473815, 0.13793019754759717, 0.14171357009825591, 0.13465501239228217, 0.14863364806357782, 0.18448547291737027, 0.12020169064861075, 0.1319561545159852, 0.15684342556841802, 0.18135669727999526, 0.1722263212300219, 0.17774524522035406, 0.17084509018439298, 0.16768585130667302, 0.17197035862775434, 0.1841847805818937, 0.17209956558463746, 0.18199579581984981, 0.1630563623215645, 0.1593024772589895, 0.15888403573020116, 0.16636755298920014, 0.1590289032999046, 0.1584973189019394, 0.16531994318991128, 0.1591360374828169, 0.15823527593574693, 0.16167931622588982, 0.15632484290882043, 0.15079423170493844, 0.2530572966858847, 0.16301908202521775, 0.22884781143085797, 0.18540198268016217, 0.1617165206850939, 0.1617340008807121, 0.23169858585917957, 0.15278892028192403, 0.19340485096296223, 0.23956855949586964, 0.25055046314545404, 0.16825407377506996, 0.1349482084129855, 0.1431735571443885, 0.10907307541094469, 0.1605808088291758, 0.18702373090334523, 0.1812149365983583, 0.1601646737533594, 0.17118859726704216, 0.162987012884238, 0.1604719663626405, 0.1677809674262717, 0.17539529999824022, 0.06123425235198432, 0.05810345929625471, 0.058716193878224465, 0.06002240533232417, 0.06284801019375119, 0.0577934899339867, 0.066920637528734, 0.06461689703996798, 0.05636627931485605]}, "mutation_prompt": null}
{"id": "e88d99b5-dc9d-4059-957a-48bca64d81c0", "solution": "import numpy as np\nimport random\n\nclass Novel_Metaheuristic_Algorithm:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.F = 0.5  # initial frequency\n        self.CR = 0.5  # initial crossover rate\n        self.pop_size = 50  # initial population size\n        self.population = np.random.uniform(-5.0, 5.0, (self.pop_size, self.dim))\n        self.best_solution = np.inf\n        self.probability = 0.46  # increased probability of probabilistic exploration\n        self.temperature = 100  # initial temperature for simulated annealing\n        self.covariance_matrix = np.eye(self.dim)  # initial covariance matrix\n        self.adaptive_step_size = 0.1  # initial adaptive step size for covariance matrix adaptation\n        self.step_size_learning_rate = 0.001  # learning rate for adaptive step size\n        self.levy_flight_probability = 0.2  # probability of levy flight\n        self.levy_flight_step_size = 0.1  # step size for levy flight\n        self.history = np.zeros((self.budget, self.dim))  # history of best solutions\n        self.cooling_rate = 0.999  # higher rate of temperature cooling\n\n    def evaluate(self, x):\n        return self.func(x)\n\n    def func(self, x):\n        # BBOB test suite of 24 noiseless functions\n        # Here we use the first function f1(x) = sum(x_i^2)\n        return np.sum(x**2)\n\n    def frequency_modulation(self, x):\n        # frequency modulation function\n        return self.F * np.sin(2 * np.pi * self.F * x)\n\n    def adaptive_frequency_modulation(self):\n        # adaptive frequency modulation\n        self.F *= 1.1  # increase frequency\n        if random.random() < self.probability:\n            self.F *= 0.9  # decrease frequency with probability\n        return self.F\n\n    def levy_flight(self, x):\n        # levy flight function\n        step = np.random.normal(0, self.levy_flight_step_size, self.dim)\n        return x + step / np.abs(step)\n\n    def frequency_modulated_levy_flight(self, x):\n        # frequency-modulated levy flight function\n        self.F = self.adaptive_frequency_modulation()\n        step = np.random.normal(0, self.levy_flight_step_size, self.dim)\n        return x + self.F * np.sin(2 * np.pi * self.F * x) * step / np.abs(step)\n\n    def crossover(self, x1, x2):\n        # crossover operation\n        r = np.random.rand(self.dim)\n        return x1 + r * (x2 - x1)\n\n    def selection(self, x1, x2):\n        # selection operation\n        if self.evaluate(x2) < self.evaluate(x1):\n            return x2\n        else:\n            return x1\n\n    def information_based_selection(self, x1, x2):\n        # information-based selection\n        if self.evaluate(x2) < self.evaluate(x1):\n            return x2\n        else:\n            if random.random() < 0.5:\n                return x2\n            else:\n                return x1\n\n    def simulated_annealing(self, x):\n        # simulated annealing\n        new_x = x + np.random.normal(0, 1, self.dim)\n        new_x = np.clip(new_x, -5.0, 5.0)\n        if self.evaluate(new_x) < self.evaluate(x):\n            return new_x\n        else:\n            probability = np.exp(-(self.evaluate(new_x) - self.evaluate(x)) / self.temperature)\n            if random.random() < probability:\n                return new_x\n            else:\n                return x\n        self.temperature *= self.cooling_rate  # decrease temperature with a higher rate\n\n    def covariance_matrix_adaptation(self):\n        # covariance matrix adaptation\n        self.covariance_matrix = (1 - self.adaptive_step_size) * self.covariance_matrix + self.adaptive_step_size * np.eye(self.dim)\n        return self.covariance_matrix\n\n    def adaptive_step_size_learning(self):\n        # adaptive step size learning\n        self.adaptive_step_size += self.step_size_learning_rate * (self.evaluate(self.population[0]) - self.evaluate(self.population[-1]))\n        return self.adaptive_step_size\n\n    def history_learning(self):\n        # history learning\n        self.history = np.vstack((self.history, self.population[0]))\n\n    def optimize(self, func):\n        for i in range(self.budget):\n            # evaluate population\n            fitness = [self.evaluate(x) for x in self.population]\n            # get best solution\n            self.best_solution = min(fitness)\n            best_index = fitness.index(self.best_solution)\n            # adaptive frequency modulation\n            self.F = self.adaptive_frequency_modulation()\n            # differential evolution\n            for j in range(self.pop_size):\n                # generate trial vector\n                trial = self.crossover(self.population[j], self.population[random.randint(0, self.pop_size - 1)])\n                # evaluate trial vector\n                trial_fitness = self.evaluate(trial)\n                # selection\n                self.population[j] = self.selection(trial, self.population[j])\n                # update best solution\n                if trial_fitness < self.evaluate(self.population[j]):\n                    self.population[j] = trial\n            # frequency-modulated levy flight\n            for j in range(self.pop_size):\n                if random.random() < self.levy_flight_probability:\n                    self.population[j] = self.frequency_modulated_levy_flight(self.population[j])\n            # simulated annealing\n            for j in range(self.pop_size):\n                self.population[j] = self.simulated_annealing(self.population[j])\n            # information-based selection\n            for j in range(self.pop_size):\n                self.population[j] = self.information_based_selection(self.population[j], self.population[random.randint(0, self.pop_size - 1)])\n            # covariance matrix adaptation\n            self.covariance_matrix = self.covariance_matrix_adaptation()\n            # adaptive step size learning\n            self.adaptive_step_size = self.adaptive_step_size_learning()\n            # history learning\n            self.history_learning()\n        return self.best_solution, self.history\n\n    def __call__(self, func):\n        self.func = func\n        return self.optimize(func)\n\n# example usage\nbudget = 1000\ndim = 10\noptimizer = Novel_Metaheuristic_Algorithm(budget, dim)\nbest_solution, history = optimizer(lambda x: np.sum(x**2))\nprint(\"Best solution:\", best_solution)\nprint(\"History:\", history)", "name": "Novel_Metaheuristic_Algorithm", "description": "Novel Metaheuristic Algorithm with Adaptive Frequency Modulation, Covariance Matrix Adaptation, Simulated Annealing, and Levy Flight, refined to use a higher rate of temperature cooling and increased probability of probabilistic exploration with Information-based Selection and Adaptive Step Size Learning, and the addition of a novel \"Frequency-Modulated Levy Flight\" operator that combines frequency modulation with levy flight to improve exploration and exploitation.", "configspace": "", "generation": 91, "fitness": 0.12956525311596045, "feedback": "The algorithm Novel_Metaheuristic_Algorithm got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.13 with standard deviation 0.14.", "error": "", "parent_id": "61fc0601-f2dd-492a-86cb-f0579f42e3ac", "metadata": {"aucs": [0.23515213697331294, 0.21837939525793348, 0.2306437993826157, 0.2027474533007778, 0.22279512922984757, 0.23550625755401522, 0.2227589239307084, 0.22128925473572847, 0.20802098711273154, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.05223085616113643, 0.08874301130767615, 0.06422997752127901, 0.06548637743676322, 0.0684923647474911, 0.06757487315307953, 0.05775667938048357, 0.057965871896490495, 0.05988237078494363, 0.06323826753622341, 0.05138344728767208, 0.057102688878647334, 0.05928274992881144, 0.04747728448367672, 0.059892844770615405, 0.06297139539269503, 0.06411363779269563, 0.047740024829620586, 0.5743801130390298, 0.6994824111104574, 0.7302739913614389, 0.838615206693847, 0.6480245257295898, 0.7828464141821567, 0.5619329359010077, 0.6929083142739176, 0.6018642932777847, 0.08375981959561574, 0.0725076997169597, 0.0822608503094503, 0.09974927659281474, 0.07203510574713856, 0.07289989723068269, 0.10392781941277995, 0.12181414905283894, 0.0925995133679911, 0.19165094402797944, 0.19358705193651427, 0.1261409364668652, 0.16471860278902206, 0.22856706606702704, 0.16005083155205446, 0.15583003188853595, 0.14637425315285868, 0.17701376261369983, 0.03569145658966355, 0.02827633361704196, 0.030855311953362663, 0.0810681676746412, 0.07550758741194152, 0.023876418382673514, 0.09053889276670779, 0.0573020079588108, 0.07297060899506558, 0.08932755708604256, 0.0746693965873626, 0.0847679710280903, 0.07743199520039235, 0.10346348469362565, 0.1264607487638998, 0.08420761018183365, 0.09173077556632447, 0.08147285078220434, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.051485070125743215, 0.05918951386493665, 0.05269270701778328, 0.05794520652122792, 0.016360847277333446, 0.014337939484356355, 0.0839397919434377, 0.03146815812025017, 0.03218670642440136, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.035767488748650655, 0.01620720589153901, 0.02010699011824324, 0.04396274129060862, 0.03332289330323224, 0.05974122677959259, 0.035745255895198724, 0.06150986718385343, 0.04094707201242631, 0.2291826094792122, 0.24747287897464543, 0.24025003269804357, 0.22811549743549864, 0.2550950228947795, 0.21052364470823615, 0.21175723550321746, 0.2353128402718555, 0.24878594381730135, 0.04523333279011055, 0.04896365258859092, 0.046232222364656406, 0.067086960926105, 0.06439589639391619, 0.07116944349430232, 0.05839858976856116, 0.06085919560200659, 0.06503311908020182, 0.11668490616105154, 0.1253654963680123, 0.12434251803721053, 0.11301251676570467, 0.12368121628847517, 0.10876066176073218, 0.11603400436157207, 0.11256274034400415, 0.11840301256798358, 0.17763298694089957, 0.16213970158699975, 0.18560005010151914, 0.20166214399600013, 0.20194277843185304, 0.19010210429334873, 0.16193917431626415, 0.17847463247348627, 0.15562580159558947, 0.12227946485807095, 0.1276963828290183, 0.12798676511438045, 0.1377031660613942, 0.1289449949307122, 0.13302048759479967, 0.11592691496129648, 0.12747442190186797, 0.13817818683341554, 0.16501184878255937, 0.164735558907538, 0.19141284113061574, 0.16452254427222923, 0.16899907785551227, 0.1688717312988245, 0.19421119979357948, 0.1576122650710522, 0.17674708543875617, 0.16818394196928899, 0.1593751385791351, 0.17060794330254625, 0.1532651885498637, 0.1592988419043202, 0.16504846412482388, 0.15446319338962566, 0.15480471589941314, 0.1556558057709675, 0.15339908491798904, 0.16196720769735162, 0.15857990056416837, 0.13694710732349813, 0.13260978468687368, 0.15335673669973948, 0.21387175619601317, 0.13683666045288878, 0.20522464332358437, 0.15607811733487564, 0.17499788554448836, 0.19600769677519925, 0.15544809213630129, 0.14649512592477076, 0.11936471230858092, 0.10303520185905468, 0.1977613283098012, 0.12421866872501508, 0.16277979192245107, 0.16151842819101492, 0.1746722095749641, 0.17012660780389322, 0.16114881211769438, 0.17065618078820122, 0.1931565874840102, 0.17722144254832362, 0.16097225085751798, 0.060359057427541885, 0.052756919897631316, 0.05948242263072212, 0.055310423029861555, 0.05310547599217008, 0.060207580237411684, 0.0726571042152887, 0.059218177299343755, 0.06676301884143943]}, "mutation_prompt": null}
{"id": "a862369d-3fc9-4ada-bf4a-31d6e6742c6b", "solution": "import numpy as np\nimport random\n\nclass Novel_Metaheuristic_Algorithm:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.F = 0.7  # increased initial frequency\n        self.CR = 0.5  # initial crossover rate\n        self.pop_size = 50  # initial population size\n        self.population = np.random.uniform(-5.0, 5.0, (self.pop_size, self.dim))\n        self.best_solution = np.inf\n        self.probability = 0.5  # increased probability of probabilistic exploration\n        self.temperature = 100  # initial temperature for simulated annealing\n        self.covariance_matrix = np.eye(self.dim)  # initial covariance matrix\n        self.adaptive_step_size = 0.1  # initial adaptive step size for covariance matrix adaptation\n        self.step_size_learning_rate = 0.001  # learning rate for adaptive step size\n        self.levy_flight_probability = 0.4  # increased probability of levy flight\n        self.levy_flight_step_size = 0.1  # step size for levy flight\n        self.history = np.zeros((self.budget, self.dim))  # history of best solutions\n        self.cooling_rate = 0.999  # higher rate of temperature cooling\n        self.adaptive_frequency_modulation_rate = 1.2  # higher rate of adaptive frequency modulation\n\n    def evaluate(self, x):\n        return self.func(x)\n\n    def func(self, x):\n        # BBOB test suite of 24 noiseless functions\n        # Here we use the first function f1(x) = sum(x_i^2)\n        return np.sum(x**2)\n\n    def frequency_modulation(self, x):\n        # frequency modulation function\n        return self.F * np.sin(2 * np.pi * self.F * x)\n\n    def adaptive_frequency_modulation(self):\n        # adaptive frequency modulation\n        self.F *= self.adaptive_frequency_modulation_rate  # increase frequency\n        if random.random() < self.probability:\n            self.F *= 0.9  # decrease frequency with probability\n        return self.F\n\n    def levy_flight(self, x):\n        # levy flight function\n        step = np.random.normal(0, self.levy_flight_step_size, self.dim)\n        return x + step / np.abs(step)\n\n    def frequency_modulated_levy_flight(self, x):\n        # frequency-modulated levy flight function\n        self.F = self.adaptive_frequency_modulation()\n        step = np.random.normal(0, self.levy_flight_step_size, self.dim)\n        return x + self.F * np.sin(2 * np.pi * self.F * x) * step / np.abs(step)\n\n    def crossover(self, x1, x2):\n        # crossover operation\n        r = np.random.rand(self.dim)\n        return x1 + r * (x2 - x1)\n\n    def selection(self, x1, x2):\n        # selection operation\n        if self.evaluate(x2) < self.evaluate(x1):\n            return x2\n        else:\n            return x1\n\n    def information_based_selection(self, x1, x2):\n        # information-based selection\n        if self.evaluate(x2) < self.evaluate(x1):\n            return x2\n        else:\n            if random.random() < 0.5:\n                return x2\n            else:\n                return x1\n\n    def simulated_annealing(self, x):\n        # simulated annealing\n        new_x = x + np.random.normal(0, 1, self.dim)\n        new_x = np.clip(new_x, -5.0, 5.0)\n        if self.evaluate(new_x) < self.evaluate(x):\n            return new_x\n        else:\n            probability = np.exp(-(self.evaluate(new_x) - self.evaluate(x)) / self.temperature)\n            if random.random() < probability:\n                return new_x\n            else:\n                return x\n        self.temperature *= self.cooling_rate  # decrease temperature with a higher rate\n\n    def covariance_matrix_adaptation(self):\n        # covariance matrix adaptation\n        self.covariance_matrix = (1 - self.adaptive_step_size) * self.covariance_matrix + self.adaptive_step_size * np.eye(self.dim)\n        return self.covariance_matrix\n\n    def adaptive_step_size_learning(self):\n        # adaptive step size learning\n        self.adaptive_step_size += self.step_size_learning_rate * (self.evaluate(self.population[0]) - self.evaluate(self.population[-1]))\n        return self.adaptive_step_size\n\n    def history_learning(self):\n        # history learning\n        self.history = np.vstack((self.history, self.population[0]))\n\n    def optimize(self, func):\n        for i in range(self.budget):\n            # evaluate population\n            fitness = [self.evaluate(x) for x in self.population]\n            # get best solution\n            self.best_solution = min(fitness)\n            best_index = fitness.index(self.best_solution)\n            # adaptive frequency modulation\n            self.F = self.adaptive_frequency_modulation()\n            # differential evolution\n            for j in range(self.pop_size):\n                # generate trial vector\n                trial = self.crossover(self.population[j], self.population[random.randint(0, self.pop_size - 1)])\n                # evaluate trial vector\n                trial_fitness = self.evaluate(trial)\n                # selection\n                self.population[j] = self.selection(trial, self.population[j])\n                # update best solution\n                if trial_fitness < self.evaluate(self.population[j]):\n                    self.population[j] = trial\n            # frequency-modulated levy flight\n            for j in range(self.pop_size):\n                if random.random() < self.levy_flight_probability:\n                    self.population[j] = self.frequency_modulated_levy_flight(self.population[j])\n            # simulated annealing\n            for j in range(self.pop_size):\n                self.population[j] = self.simulated_annealing(self.population[j])\n            # information-based selection\n            for j in range(self.pop_size):\n                self.population[j] = self.information_based_selection(self.population[j], self.population[random.randint(0, self.pop_size - 1)])\n            # covariance matrix adaptation\n            self.covariance_matrix = self.covariance_matrix_adaptation()\n            # adaptive step size learning\n            self.adaptive_step_size = self.adaptive_step_size_learning()\n            # history learning\n            self.history_learning()\n        return self.best_solution, self.history\n\n    def __call__(self, func):\n        self.func = func\n        return self.optimize(func)\n\n# example usage\nbudget = 1000\ndim = 10\noptimizer = Novel_Metaheuristic_Algorithm(budget, dim)\nbest_solution, history = optimizer(lambda x: np.sum(x**2))\nprint(\"Best solution:\", best_solution)\nprint(\"History:\", history)", "name": "Novel_Metaheuristic_Algorithm", "description": "Novel Metaheuristic Algorithm with Adaptive Frequency Modulation, Covariance Matrix Adaptation, Simulated Annealing, and Levy Flight, refined to use a higher rate of temperature cooling and increased probability of probabilistic exploration with Information-based Selection and Adaptive Step Size Learning, and the addition of a novel \"Frequency-Modulated Levy Flight\" operator that combines frequency modulation with levy flight to improve exploration and exploitation, and a higher rate of adaptive frequency modulation and increased probability of levy flight.", "configspace": "", "generation": 92, "fitness": 0.12613980428006502, "feedback": "The algorithm Novel_Metaheuristic_Algorithm got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.13 with standard deviation 0.17.", "error": "", "parent_id": "e88d99b5-dc9d-4059-957a-48bca64d81c0", "metadata": {"aucs": [0.20720888396922643, 0.19025176398067756, 0.16923134215579538, 0.2067140231883703, 0.203434753632329, 0.21410123642159018, 0.17803911845472686, 0.21352193589080115, 0.1912100457586543, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.046548494697980614, 0.06379487726899913, 0.04010621690683425, 0.05849787475827306, 0.046395282592118336, 0.06441312534765786, 0.04828197413989366, 0.06009062858316372, 0.051202591894879745, 0.0284771533725241, 0.030277402491962535, 0.04533023846417328, 0.04217310013991371, 0.035553347610418506, 0.022299365145648897, 0.04110833911571099, 0.04027561158998905, 0.05434672508238558, 0.8846938688932751, 0.8391714968356943, 0.8711059603941915, 0.7925421604437358, 0.9649631920667033, 0.8374695659823848, 0.7886142921300903, 0.7903934712377012, 0.8478351917511104, 0.0774384977834901, 0.10069741924998576, 0.04993901852269378, 0.08468326499699108, 0.08135316564753525, 0.12552433037131028, 0.08943698771160868, 0.06393513722088906, 0.091650169654795, 0.12857545505284074, 0.1257999493346157, 0.09949029069440785, 0.1438935957738433, 0.15846081777753984, 0.14028440055777014, 0.13395263679050973, 0.14022399139217945, 0.13889260638154177, 0.06479975135334393, 0.05141820567973887, 9.999999999998899e-05, 0.06232666549058563, 0.04872143675688767, 0.01991593772417477, 0.0441908185582377, 0.04997420796187202, 0.07114018499480679, 0.07802461093561597, 0.0917562480541686, 0.08565768729287737, 0.03294644857149853, 0.04545707585888137, 0.07491744513521481, 0.0073791393262574445, 0.07564400384662617, 0.03343105661053847, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.05717715969826809, 0.03929994739842768, 0.02343381826747082, 0.02625999583389005, 0.025700015082583016, 0.035528697745279336, 0.033990306155839645, 0.05767454570912267, 0.03968642310168269, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.012462718206288304, 0.006586864692299632, 0.020428120897668323, 0.011668411500769471, 0.04739577997829181, 0.026060073867753997, 0.003629470018938319, 0.011169224501944353, 0.013992881796884071, 0.2135618608583576, 0.24579787247089957, 0.21593181662445715, 0.1738999421946028, 0.19007875513911454, 0.2053147612859868, 0.22063832464692945, 0.21479964775209248, 0.22917653982906816, 0.02768224236199679, 0.0453122413699637, 0.05979085369112347, 0.04672737089921608, 0.035227466811682095, 0.03509374654031594, 0.04695994486416655, 0.04227407660302018, 0.04657723669733638, 0.1180638924158588, 0.12186401358094157, 0.18086789303271955, 0.12437882996061089, 0.1114610465108764, 0.11929662632175919, 0.11200903826857045, 0.12746115795834467, 0.1415628639250197, 0.16106938048410224, 0.16622831761120704, 0.1613932299901245, 0.14760371597271038, 0.18400961518676573, 0.1803064778435084, 0.1526146415164814, 0.17377774300845283, 0.17046170983555298, 0.10495207896118386, 0.07642890689197979, 0.08967218765296914, 0.1013902546337867, 0.12452020199533109, 0.11842883147500982, 0.0837317855149382, 0.14102543570575932, 0.10053428096757544, 0.18015938562152423, 0.15611698503693294, 0.14944538960557985, 0.16703758973701643, 0.17260786728092625, 0.22661875975869128, 0.15225352227296496, 0.15458140018267352, 0.174893506317925, 0.14263627949287316, 0.1545444718167488, 0.16365426820097195, 0.1526893129232516, 0.16113209483446678, 0.1564165192768633, 0.1584032213511104, 0.1521722508361778, 0.154394777434947, 0.14486791091565976, 0.13933439503118683, 0.14240665950433196, 0.14242822476852524, 0.15269561716143287, 0.28902034494690765, 0.18579709706407344, 0.13681279908531718, 0.1540796127002586, 0.15118455493507632, 0.11765214730906304, 0.13254570845969915, 0.10519069068105658, 0.13926709169890417, 0.12886907703652672, 0.13807039832047396, 0.2113710490621724, 0.1517088019439814, 0.1556330549632683, 0.16453905638992383, 0.1715906833185945, 0.18141060178381985, 0.17147049688108018, 0.1616624580584034, 0.18937012983911183, 0.15803200620715396, 0.16376039318585756, 0.062022055235814166, 0.05757841913624251, 0.06258287521845818, 0.05448627975974085, 0.05700721279094345, 0.06935426181322091, 0.06197262595956199, 0.05185626608410587, 0.05229844149111962]}, "mutation_prompt": null}
{"id": "125e5e24-938d-4db4-b6c5-b9f075ddd988", "solution": "import numpy as np\nimport random\n\nclass Novel_Metaheuristic_Algorithm:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.F = 0.5  # initial frequency\n        self.CR = 0.5  # initial crossover rate\n        self.pop_size = 50  # initial population size\n        self.population = np.random.uniform(-5.0, 5.0, (self.pop_size, self.dim))\n        self.best_solution = np.inf\n        self.probability = 0.46  # increased probability of probabilistic exploration\n        self.temperature = 100  # initial temperature for simulated annealing\n        self.covariance_matrix = np.eye(self.dim)  # initial covariance matrix\n        self.adaptive_step_size = 0.1  # initial adaptive step size for covariance matrix adaptation\n        self.step_size_learning_rate = 0.002  # learning rate for adaptive step size\n        self.levy_flight_probability = 0.8  # probability of levy flight\n        self.levy_flight_step_size = 0.05  # step size for levy flight\n        self.history = np.zeros((self.budget, self.dim))  # history of best solutions\n        self.cooling_rate = 0.999  # higher rate of temperature cooling\n\n    def evaluate(self, x):\n        return self.func(x)\n\n    def func(self, x):\n        # BBOB test suite of 24 noiseless functions\n        # Here we use the first function f1(x) = sum(x_i^2)\n        return np.sum(x**2)\n\n    def frequency_modulation(self, x):\n        # frequency modulation function\n        return self.F * np.sin(2 * np.pi * self.F * x)\n\n    def adaptive_frequency_modulation(self):\n        # adaptive frequency modulation\n        self.F *= 1.1  # increase frequency\n        if random.random() < self.probability:\n            self.F *= 0.9  # decrease frequency with probability\n        return self.F\n\n    def levy_flight(self, x):\n        # levy flight function\n        step = np.random.normal(0, self.levy_flight_step_size, self.dim)\n        return x + step / np.abs(step)\n\n    def frequency_modulated_levy_flight(self, x):\n        # frequency-modulated levy flight function\n        self.F = self.adaptive_frequency_modulation()\n        step = np.random.normal(0, self.levy_flight_step_size, self.dim)\n        return x + self.F * np.sin(2 * np.pi * self.F * x) * step / np.abs(step)\n\n    def crossover(self, x1, x2):\n        # crossover operation\n        r = np.random.rand(self.dim)\n        return x1 + r * (x2 - x1)\n\n    def selection(self, x1, x2):\n        # selection operation\n        if self.evaluate(x2) < self.evaluate(x1):\n            return x2\n        else:\n            return x1\n\n    def information_based_selection(self, x1, x2):\n        # information-based selection\n        if self.evaluate(x2) < self.evaluate(x1):\n            return x2\n        else:\n            if random.random() < 0.5:\n                return x2\n            else:\n                return x1\n\n    def simulated_annealing(self, x):\n        # simulated annealing\n        new_x = x + np.random.normal(0, 1, self.dim)\n        new_x = np.clip(new_x, -5.0, 5.0)\n        if self.evaluate(new_x) < self.evaluate(x):\n            return new_x\n        else:\n            probability = np.exp(-(self.evaluate(new_x) - self.evaluate(x)) / self.temperature)\n            if random.random() < probability:\n                return new_x\n            else:\n                return x\n        self.temperature *= self.cooling_rate  # decrease temperature with a higher rate\n\n    def covariance_matrix_adaptation(self):\n        # covariance matrix adaptation\n        self.covariance_matrix = (1 - self.adaptive_step_size) * self.covariance_matrix + self.adaptive_step_size * np.eye(self.dim)\n        return self.covariance_matrix\n\n    def adaptive_step_size_learning(self):\n        # adaptive step size learning\n        self.adaptive_step_size += self.step_size_learning_rate * (self.evaluate(self.population[0]) - self.evaluate(self.population[-1]))\n        return self.adaptive_step_size\n\n    def history_learning(self):\n        # history learning\n        self.history = np.vstack((self.history, self.population[0]))\n\n    def optimize(self, func):\n        for i in range(self.budget):\n            # evaluate population\n            fitness = [self.evaluate(x) for x in self.population]\n            # get best solution\n            self.best_solution = min(fitness)\n            best_index = fitness.index(self.best_solution)\n            # adaptive frequency modulation\n            self.F = self.adaptive_frequency_modulation()\n            # differential evolution\n            for j in range(self.pop_size):\n                # generate trial vector\n                trial = self.crossover(self.population[j], self.population[random.randint(0, self.pop_size - 1)])\n                # evaluate trial vector\n                trial_fitness = self.evaluate(trial)\n                # selection\n                self.population[j] = self.selection(trial, self.population[j])\n                # update best solution\n                if trial_fitness < self.evaluate(self.population[j]):\n                    self.population[j] = trial\n            # frequency-modulated levy flight\n            for j in range(self.pop_size):\n                if random.random() < self.levy_flight_probability:\n                    self.population[j] = self.frequency_modulated_levy_flight(self.population[j])\n            # simulated annealing\n            for j in range(self.pop_size):\n                self.population[j] = self.simulated_annealing(self.population[j])\n            # information-based selection\n            for j in range(self.pop_size):\n                self.population[j] = self.information_based_selection(self.population[j], self.population[random.randint(0, self.pop_size - 1)])\n            # covariance matrix adaptation\n            self.covariance_matrix = self.covariance_matrix_adaptation()\n            # adaptive step size learning\n            self.adaptive_step_size = self.adaptive_step_size_learning()\n            # history learning\n            self.history_learning()\n        return self.best_solution, self.history\n\n    def __call__(self, func):\n        self.func = func\n        return self.optimize(func)\n\n# example usage\nbudget = 1000\ndim = 10\noptimizer = Novel_Metaheuristic_Algorithm(budget, dim)\nbest_solution, history = optimizer(lambda x: np.sum(x**2))\nprint(\"Best solution:\", best_solution)\nprint(\"History:\", history)", "name": "Novel_Metaheuristic_Algorithm", "description": "Novel Metaheuristic Algorithm with Adaptive Frequency Modulation, Covariance Matrix Adaptation, Simulated Annealing, and Levy Flight, refined to use a higher rate of temperature cooling, increased probability of probabilistic exploration, and a novel \"Frequency-Modulated Levy Flight\" operator with probability 0.8, adaptive step size learning rate 0.002, and levy flight step size 0.05.", "configspace": "", "generation": 93, "fitness": 0.11102488833238411, "feedback": "The algorithm Novel_Metaheuristic_Algorithm got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.11 with standard deviation 0.17.", "error": "", "parent_id": "e88d99b5-dc9d-4059-957a-48bca64d81c0", "metadata": {"aucs": [0.21887772384223936, 0.15391754955163128, 0.17556957459938172, 0.15543703665393427, 0.14799800438327315, 0.18112295124952071, 0.1455373160507457, 0.15539926435574558, 0.1521344389424828, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.0316228343430619, 0.033019504788742826, 0.03734550032526296, 0.0704454971926829, 0.030596612940222423, 0.04687633820923165, 0.029211469579404015, 0.03477287995123557, 0.06494151590376274, 0.009854321153656587, 0.0083770522093497, 0.04559588332878428, 0.027720081592018664, 0.006899068711262779, 0.038975785321739176, 0.01837844892490359, 0.011807380468210393, 0.03142561453650994, 0.8775900227954135, 0.9183640608657476, 0.8380446112980179, 0.8468634119945161, 0.9215685346864984, 0.91842995168775, 0.921742138909637, 0.9242689742583711, 0.8093550658777944, 0.06274794416174423, 0.058778107644013144, 0.0397884956783815, 0.05901905970651444, 0.08668575238868848, 0.08742443713676151, 0.0483264830121225, 0.06827275580235914, 0.056595419539956504, 0.12110122713446148, 0.09701063093873619, 0.0744287201297944, 0.10675043793426453, 0.08092170846345703, 0.11593690591581396, 0.08214887219145117, 0.09925502074533998, 0.10975638121751818, 9.999999999998899e-05, 9.999999999998899e-05, 0.005010509380365291, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.0104222530645105, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.008891727397257632, 0.014673166119442471, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.0063746551637891, 0.03606409991413073, 0.082478941712395, 0.022143818687357042, 0.03013236015124543, 0.05374064828035097, 0.0559050592450725, 0.0008735954993913664, 0.0008512846827670728, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.19790565947466587, 0.1888551377465426, 0.20426272980137195, 0.17237996160668112, 0.16744490737803486, 0.1709177754442831, 0.16112956940342849, 0.20912138854919748, 0.18436822520469165, 0.03628360898487348, 0.03155688919301225, 0.03637738214645547, 0.0349126628866101, 0.03932347723166929, 0.03256109998973644, 0.02654747441553862, 0.02514106796369342, 0.03992142469599658, 0.11750501102480371, 0.13688285044556692, 0.12249841878010326, 0.1250299236134289, 0.11286929410499147, 0.1361613197852385, 0.1124290805625533, 0.10276695965070726, 0.10122546521551634, 0.14028892247577696, 0.15225407719879203, 0.17005098252849982, 0.17567262520043492, 0.14872781974782734, 0.16679550695476897, 0.13979593231229992, 0.17377774300845283, 0.14196580598110453, 0.08688606892835926, 0.08985391697396095, 0.10617576836274623, 0.09110142101179897, 0.12456830104723682, 0.1174872763879401, 0.0837317855149382, 0.12399134245902821, 0.0742738103407311, 0.13606998598852138, 0.14449751442484737, 0.144675536783904, 0.1460334454153268, 0.12734385633499445, 0.14356757785942753, 0.1387367959617979, 0.15121868209273548, 0.14225672583337434, 0.16652003558013673, 0.1285149950395309, 0.14514256750807564, 0.1304517831345927, 0.1548140350841679, 0.15797390357476582, 0.14316176825731364, 0.13982476861681337, 0.13941947203743676, 0.14234404421253144, 0.13604962333479176, 0.1224154679893783, 0.1182350325265501, 0.16680071192203927, 0.12223637352742311, 0.13462154153358263, 0.11976741811433078, 0.16097047743874393, 0.15020837565119238, 0.12179651650750911, 0.11608521326334831, 0.09970280269154741, 0.11235055738155753, 0.16181488193337645, 0.11996242077317898, 0.14395249961645817, 0.08980535215520324, 0.16452833556079371, 0.15714706571654968, 0.163320165908863, 0.1632997697209967, 0.16499694440907642, 0.16852189087072744, 0.1586568882326692, 0.17535073495713727, 0.1661963779127451, 0.045311572695782676, 0.05434737143046764, 0.0406118020177102, 0.0514703171462243, 0.04261486249562296, 0.0473927743644692, 0.05320430115088037, 0.048638524124663296, 0.035048648722797004]}, "mutation_prompt": null}
{"id": "28bde7b9-fd6a-4f87-8483-f420df97a80d", "solution": "import numpy as np\nimport random\n\nclass Novel_Metaheuristic_Algorithm:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.F = 0.5  # initial frequency\n        self.CR = 0.5  # initial crossover rate\n        self.pop_size = 50  # initial population size\n        self.population = np.random.uniform(-5.0, 5.0, (self.pop_size, self.dim))\n        self.best_solution = np.inf\n        self.probability = 0.46  # increased probability of probabilistic exploration\n        self.temperature = 100  # initial temperature for simulated annealing\n        self.covariance_matrix = np.eye(self.dim)  # initial covariance matrix\n        self.adaptive_step_size = 0.1  # initial adaptive step size for covariance matrix adaptation\n        self.step_size_learning_rate = 0.001  # learning rate for adaptive step size\n        self.levy_flight_probability = 0.2  # probability of levy flight\n        self.levy_flight_step_size = 0.1  # step size for levy flight\n        self.history = np.zeros((self.budget, self.dim))  # history of best solutions\n        self.cooling_rate = 0.999  # higher rate of temperature cooling\n        self.probability_of_change = 0.5  # probability of changing individual lines of the selected solution\n\n    def evaluate(self, x):\n        return self.func(x)\n\n    def func(self, x):\n        # BBOB test suite of 24 noiseless functions\n        # Here we use the first function f1(x) = sum(x_i^2)\n        return np.sum(x**2)\n\n    def frequency_modulation(self, x):\n        # frequency modulation function\n        return self.F * np.sin(2 * np.pi * self.F * x)\n\n    def adaptive_frequency_modulation(self):\n        # adaptive frequency modulation\n        self.F *= 1.1  # increase frequency\n        if random.random() < self.probability:\n            self.F *= 0.9  # decrease frequency with probability\n        return self.F\n\n    def levy_flight(self, x):\n        # levy flight function\n        step = np.random.normal(0, self.levy_flight_step_size, self.dim)\n        return x + step / np.abs(step)\n\n    def frequency_modulated_levy_flight(self, x):\n        # frequency-modulated levy flight function\n        self.F = self.adaptive_frequency_modulation()\n        step = np.random.normal(0, self.levy_flight_step_size, self.dim)\n        return x + self.F * np.sin(2 * np.pi * self.F * x) * step / np.abs(step)\n\n    def crossover(self, x1, x2):\n        # crossover operation\n        r = np.random.rand(self.dim)\n        return x1 + r * (x2 - x1)\n\n    def selection(self, x1, x2):\n        # selection operation\n        if self.evaluate(x2) < self.evaluate(x1):\n            return x2\n        else:\n            return x1\n\n    def information_based_selection(self, x1, x2):\n        # information-based selection\n        if self.evaluate(x2) < self.evaluate(x1):\n            return x2\n        else:\n            if random.random() < 0.5:\n                return x2\n            else:\n                return x1\n\n    def simulated_annealing(self, x):\n        # simulated annealing\n        new_x = x + np.random.normal(0, 1, self.dim)\n        new_x = np.clip(new_x, -5.0, 5.0)\n        if self.evaluate(new_x) < self.evaluate(x):\n            return new_x\n        else:\n            probability = np.exp(-(self.evaluate(new_x) - self.evaluate(x)) / self.temperature)\n            if random.random() < probability:\n                return new_x\n            else:\n                return x\n        self.temperature *= self.cooling_rate  # decrease temperature with a higher rate\n\n    def covariance_matrix_adaptation(self):\n        # covariance matrix adaptation\n        self.covariance_matrix = (1 - self.adaptive_step_size) * self.covariance_matrix + self.adaptive_step_size * np.eye(self.dim)\n        return self.covariance_matrix\n\n    def adaptive_step_size_learning(self):\n        # adaptive step size learning\n        self.adaptive_step_size += self.step_size_learning_rate * (self.evaluate(self.population[0]) - self.evaluate(self.population[-1]))\n        return self.adaptive_step_size\n\n    def history_learning(self):\n        # history learning\n        self.history = np.vstack((self.history, self.population[0]))\n\n    def change_individual_lines(self):\n        # change individual lines of the selected solution\n        if random.random() < self.probability_of_change:\n            for i in range(self.pop_size):\n                self.population[i] = self.frequency_modulated_levy_flight(self.population[i])\n            for i in range(self.pop_size):\n                self.population[i] = self.simulated_annealing(self.population[i])\n            for i in range(self.pop_size):\n                self.population[i] = self.information_based_selection(self.population[i], self.population[random.randint(0, self.pop_size - 1)])\n\n    def optimize(self, func):\n        for i in range(self.budget):\n            # evaluate population\n            fitness = [self.evaluate(x) for x in self.population]\n            # get best solution\n            self.best_solution = min(fitness)\n            best_index = fitness.index(self.best_solution)\n            # adaptive frequency modulation\n            self.F = self.adaptive_frequency_modulation()\n            # differential evolution\n            for j in range(self.pop_size):\n                # generate trial vector\n                trial = self.crossover(self.population[j], self.population[random.randint(0, self.pop_size - 1)])\n                # evaluate trial vector\n                trial_fitness = self.evaluate(trial)\n                # selection\n                self.population[j] = self.selection(trial, self.population[j])\n                # update best solution\n                if trial_fitness < self.evaluate(self.population[j]):\n                    self.population[j] = trial\n            # change individual lines of the selected solution\n            self.change_individual_lines()\n            # frequency-modulated levy flight\n            for j in range(self.pop_size):\n                if random.random() < self.levy_flight_probability:\n                    self.population[j] = self.frequency_modulated_levy_flight(self.population[j])\n            # simulated annealing\n            for j in range(self.pop_size):\n                self.population[j] = self.simulated_annealing(self.population[j])\n            # information-based selection\n            for j in range(self.pop_size):\n                self.population[j] = self.information_based_selection(self.population[j], self.population[random.randint(0, self.pop_size - 1)])\n            # covariance matrix adaptation\n            self.covariance_matrix = self.covariance_matrix_adaptation()\n            # adaptive step size learning\n            self.adaptive_step_size = self.adaptive_step_size_learning()\n            # history learning\n            self.history_learning()\n        return self.best_solution, self.history\n\n    def __call__(self, func):\n        self.func = func\n        return self.optimize(func)\n\n# example usage\nbudget = 1000\ndim = 10\noptimizer = Novel_Metaheuristic_Algorithm(budget, dim)\nbest_solution, history = optimizer(lambda x: np.sum(x**2))\nprint(\"Best solution:\", best_solution)\nprint(\"History:\", history)", "name": "Novel_Metaheuristic_Algorithm", "description": "Novel Metaheuristic Algorithm with Adaptive Frequency Modulation, Covariance Matrix Adaptation, Simulated Annealing, and Levy Flight, refined to use a higher rate of temperature cooling and increased probability of probabilistic exploration with Information-based Selection and Adaptive Step Size Learning, and the addition of a novel \"Frequency-Modulated Levy Flight\" operator that combines frequency modulation with levy flight to improve exploration and exploitation, and a probability of 0.5 to change the individual lines of the selected solution to refine its strategy.", "configspace": "", "generation": 94, "fitness": 0.11732631591995898, "feedback": "The algorithm Novel_Metaheuristic_Algorithm got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.12 with standard deviation 0.16.", "error": "", "parent_id": "e88d99b5-dc9d-4059-957a-48bca64d81c0", "metadata": {"aucs": [0.16086115140878787, 0.2040910813002571, 0.16586396152929261, 0.13983981103498788, 0.1615280085467904, 0.18313177937038871, 0.1502090247204807, 0.22522083734893472, 0.14813086273210907, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.04431682772325407, 0.04744117798701497, 0.043592349610471315, 0.059687267140490774, 0.03534541401018254, 0.05536777986070285, 0.05913409836313044, 0.05363266572008252, 0.04493058199810673, 0.013485273878512949, 0.01948745556536835, 0.047936266463142196, 0.026015260123481543, 0.029570955857318948, 0.006012306036196957, 0.033912773628511195, 0.006979558023583743, 0.020060090978141876, 0.8626637667343011, 0.8267485281610623, 0.8982260119413902, 0.7895659694226127, 0.8607136888658843, 0.7643916350825868, 0.8138825939583814, 0.84847131657746, 0.817097084221255, 0.0669990350859514, 0.058034405384129295, 0.0837084306462832, 0.043306597064018515, 0.07424957820144118, 0.08498513782055306, 0.08478490228874325, 0.07171234396190118, 0.05680675230003984, 0.14000729683479018, 0.11320231237167411, 0.14990231391424091, 0.12403564046277826, 0.09681155433979916, 0.12871513332310547, 0.10679322707648731, 0.1055987754323705, 0.08919620670134842, 9.999999999998899e-05, 9.999999999998899e-05, 0.020235017881904294, 0.004569147046577515, 0.06843431233814024, 9.999999999998899e-05, 0.008875700048080026, 0.016898749585758432, 9.999999999998899e-05, 0.060007764598725255, 0.026106604998781813, 0.054437018285711014, 0.03907822226316182, 0.0962632915098538, 0.023385206354313404, 0.024323654952850715, 9.999999999998899e-05, 0.03576727127954049, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.010461692488867302, 0.03509979319681622, 0.05879750586742871, 0.025910813915750786, 0.030196347111265842, 0.057161750839293535, 0.03166481040320013, 0.04913963708401914, 0.012630437237632242, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.002979762797847618, 9.999999999998899e-05, 0.023210152969500464, 9.999999999998899e-05, 9.999999999998899e-05, 0.012731229426731994, 0.004519511837630441, 0.004500386567242498, 9.999999999998899e-05, 0.19143813167257284, 0.19255694877603335, 0.1868083343567707, 0.19654165992213168, 0.19663864482529048, 0.15344552269632827, 0.22586099785630387, 0.18269048117825992, 0.23489163513139344, 0.05520612791899382, 0.04244549240406503, 0.045332592026994134, 0.034441803276561545, 0.03808603217601958, 0.035269676800980365, 0.03912597205108903, 0.03308667972651691, 0.03807099349800935, 0.10852981832653863, 0.10935350134102517, 0.13060412945564115, 0.1306142761269139, 0.129353279991819, 0.1380984727528075, 0.14077177871232904, 0.1142832668978041, 0.11867367709611842, 0.12273617656721181, 0.14015381659312975, 0.1613932299901245, 0.1396418893726128, 0.15199528441427823, 0.16856341364867222, 0.15825944432958927, 0.17377774300845283, 0.16550932944387753, 0.0711115964401382, 0.09200512800209004, 0.1049645956115215, 0.11426640641852359, 0.09278042309219747, 0.1321435489653885, 0.09887089827177986, 0.1299388886129591, 0.11840512605173248, 0.1580449883908972, 0.13912965565862145, 0.15221222445115745, 0.15547172949427512, 0.13951214833580017, 0.17901760651139487, 0.17824366611307474, 0.1901778233801622, 0.1598011143247593, 0.13714409359734914, 0.13587578177245518, 0.15578294459407693, 0.15847848746445847, 0.15207185068059093, 0.14529101719923032, 0.15562006430421638, 0.14980379984080716, 0.16029280196345097, 0.13564798143775192, 0.265641515798258, 0.12791503918772473, 0.11756951356871104, 0.16105270450264286, 0.16274740266110044, 0.15048868736441745, 0.1502755419772156, 0.2128936030929498, 0.20569743908561555, 0.1523044584945099, 0.1276455936919605, 0.11702545088936389, 0.10783353986778621, 0.14250142122039067, 0.16132139047772676, 0.1572380583456865, 0.14249078605450616, 0.16589852406715289, 0.15965615040374959, 0.173586964998811, 0.1513872071503185, 0.17116385893599273, 0.18783612887310097, 0.16174133359597398, 0.1628045595037112, 0.1535836731572242, 0.07356344958311789, 0.05674084355373343, 0.040595897471765285, 0.04759035417127999, 0.05643308267010139, 0.0716695319336037, 0.040827879548608115, 0.049772177766026826, 0.05125317768124682]}, "mutation_prompt": null}
{"id": "9a321ade-6ddb-49a3-8b55-8b6a9e0156da", "solution": "import numpy as np\nimport random\n\nclass Novel_Metaheuristic_Algorithm:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.F = 0.5  # initial frequency\n        self.CR = 0.5  # initial crossover rate\n        self.pop_size = 50  # initial population size\n        self.population = np.random.uniform(-5.0, 5.0, (self.pop_size, self.dim))\n        self.best_solution = np.inf\n        self.probability = 0.46  # increased probability of probabilistic exploration\n        self.temperature = 100  # initial temperature for simulated annealing\n        self.covariance_matrix = np.eye(self.dim)  # initial covariance matrix\n        self.adaptive_step_size = 0.1  # initial adaptive step size for covariance matrix adaptation\n        self.step_size_learning_rate = 0.001  # learning rate for adaptive step size\n        self.levy_flight_probability = 0.2  # probability of levy flight\n        self.levy_flight_step_size = 0.1  # step size for levy flight\n        self.adaptive_mutation_rate = 0.1  # initial adaptive mutation rate\n        self.history = np.zeros((self.budget, self.dim))  # history of best solutions\n        self.cooling_rate = 0.999  # higher rate of temperature cooling\n\n    def evaluate(self, x):\n        return self.func(x)\n\n    def func(self, x):\n        # BBOB test suite of 24 noiseless functions\n        # Here we use the first function f1(x) = sum(x_i^2)\n        return np.sum(x**2)\n\n    def frequency_modulation(self, x):\n        # frequency modulation function\n        return self.F * np.sin(2 * np.pi * self.F * x)\n\n    def adaptive_frequency_modulation(self):\n        # adaptive frequency modulation\n        self.F *= 1.1  # increase frequency\n        if random.random() < self.probability:\n            self.F *= 0.9  # decrease frequency with probability\n        return self.F\n\n    def levy_flight(self, x):\n        # levy flight function\n        step = np.random.normal(0, self.levy_flight_step_size, self.dim)\n        return x + step / np.abs(step)\n\n    def frequency_modulated_levy_flight(self, x):\n        # frequency-modulated levy flight function\n        self.F = self.adaptive_frequency_modulation()\n        step = np.random.normal(0, self.levy_flight_step_size, self.dim)\n        return x + self.F * np.sin(2 * np.pi * self.F * x) * step / np.abs(step)\n\n    def crossover(self, x1, x2):\n        # crossover operation\n        r = np.random.rand(self.dim)\n        return x1 + r * (x2 - x1)\n\n    def selection(self, x1, x2):\n        # selection operation\n        if self.evaluate(x2) < self.evaluate(x1):\n            return x2\n        else:\n            return x1\n\n    def information_based_selection(self, x1, x2):\n        # information-based selection\n        if self.evaluate(x2) < self.evaluate(x1):\n            return x2\n        else:\n            if random.random() < 0.5:\n                return x2\n            else:\n                return x1\n\n    def simulated_annealing(self, x):\n        # simulated annealing\n        new_x = x + np.random.normal(0, 1, self.dim)\n        new_x = np.clip(new_x, -5.0, 5.0)\n        if self.evaluate(new_x) < self.evaluate(x):\n            return new_x\n        else:\n            probability = np.exp(-(self.evaluate(new_x) - self.evaluate(x)) / self.temperature)\n            if random.random() < probability:\n                return new_x\n            else:\n                return x\n        self.temperature *= self.cooling_rate  # decrease temperature with a higher rate\n\n    def covariance_matrix_adaptation(self):\n        # covariance matrix adaptation\n        self.covariance_matrix = (1 - self.adaptive_step_size) * self.covariance_matrix + self.adaptive_step_size * np.eye(self.dim)\n        return self.covariance_matrix\n\n    def adaptive_step_size_learning(self):\n        # adaptive step size learning\n        self.adaptive_step_size += self.step_size_learning_rate * (self.evaluate(self.population[0]) - self.evaluate(self.population[-1]))\n        return self.adaptive_step_size\n\n    def adaptive_mutation_rate_learning(self):\n        # adaptive mutation rate learning\n        self.adaptive_mutation_rate += 0.01 * (self.evaluate(self.population[0]) - self.evaluate(self.population[-1]))\n        return self.adaptive_mutation_rate\n\n    def history_learning(self):\n        # history learning\n        self.history = np.vstack((self.history, self.population[0]))\n\n    def optimize(self, func):\n        for i in range(self.budget):\n            # evaluate population\n            fitness = [self.evaluate(x) for x in self.population]\n            # get best solution\n            self.best_solution = min(fitness)\n            best_index = fitness.index(self.best_solution)\n            # adaptive frequency modulation\n            self.F = self.adaptive_frequency_modulation()\n            # differential evolution\n            for j in range(self.pop_size):\n                # generate trial vector\n                trial = self.crossover(self.population[j], self.population[random.randint(0, self.pop_size - 1)])\n                # mutate trial vector with adaptive mutation rate\n                trial = trial + np.random.normal(0, self.adaptive_mutation_rate, self.dim)\n                trial = np.clip(trial, -5.0, 5.0)\n                # evaluate trial vector\n                trial_fitness = self.evaluate(trial)\n                # selection\n                self.population[j] = self.selection(trial, self.population[j])\n                # update best solution\n                if trial_fitness < self.evaluate(self.population[j]):\n                    self.population[j] = trial\n            # frequency-modulated levy flight\n            for j in range(self.pop_size):\n                if random.random() < self.levy_flight_probability:\n                    self.population[j] = self.frequency_modulated_levy_flight(self.population[j])\n            # simulated annealing\n            for j in range(self.pop_size):\n                self.population[j] = self.simulated_annealing(self.population[j])\n            # information-based selection\n            for j in range(self.pop_size):\n                self.population[j] = self.information_based_selection(self.population[j], self.population[random.randint(0, self.pop_size - 1)])\n            # covariance matrix adaptation\n            self.covariance_matrix = self.covariance_matrix_adaptation()\n            # adaptive step size learning\n            self.adaptive_step_size = self.adaptive_step_size_learning()\n            # adaptive mutation rate learning\n            self.adaptive_mutation_rate = self.adaptive_mutation_rate_learning()\n            # history learning\n            self.history_learning()\n        return self.best_solution, self.history\n\n    def __call__(self, func):\n        self.func = func\n        return self.optimize(func)\n\n# example usage\nbudget = 1000\ndim = 10\noptimizer = Novel_Metaheuristic_Algorithm(budget, dim)\nbest_solution, history = optimizer(lambda x: np.sum(x**2))\nprint(\"Best solution:\", best_solution)\nprint(\"History:\", history)", "name": "Novel_Metaheuristic_Algorithm", "description": "Novel Metaheuristic Algorithm with Adaptive Frequency Modulation, Covariance Matrix Adaptation, Simulated Annealing, and Levy Flight, refined to use a higher rate of temperature cooling and increased probability of probabilistic exploration with Information-based Selection and Adaptive Step Size Learning, and the addition of a novel \"Frequency-Modulated Levy Flight\" operator that combines frequency modulation with levy flight to improve exploration and exploitation, and also incorporating a new \"Adaptive Mutation Rate\" operator that adjusts the mutation rate based on the fitness of the population.", "configspace": "", "generation": 95, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('scale < 0').", "error": "ValueError('scale < 0')", "parent_id": "e88d99b5-dc9d-4059-957a-48bca64d81c0", "metadata": {}, "mutation_prompt": null}
{"id": "e4ede626-d460-4fc1-84d5-29ea98f656e9", "solution": "import numpy as np\nimport random\n\nclass Novel_Metaheuristic_Algorithm:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.F = 0.5  # initial frequency\n        self.CR = 0.5  # initial crossover rate\n        self.pop_size = 50  # initial population size\n        self.population = np.random.uniform(-5.0, 5.0, (self.pop_size, self.dim))\n        self.best_solution = np.inf\n        self.probability = 0.46  # increased probability of probabilistic exploration\n        self.temperature = 100  # initial temperature for simulated annealing\n        self.covariance_matrix = np.eye(self.dim)  # initial covariance matrix\n        self.adaptive_step_size = 0.1  # initial adaptive step size for covariance matrix adaptation\n        self.step_size_learning_rate = 0.001  # learning rate for adaptive step size\n        self.levy_flight_probability = 0.2  # probability of levy flight\n        self.levy_flight_step_size = 0.1  # step size for levy flight\n        self.history = np.zeros((self.budget, self.dim))  # history of best solutions\n        self.cooling_rate = 0.999  # higher rate of temperature cooling\n\n    def evaluate(self, x):\n        return self.func(x)\n\n    def func(self, x):\n        # BBOB test suite of 24 noiseless functions\n        # Here we use the first function f1(x) = sum(x_i^2)\n        return np.sum(x**2)\n\n    def frequency_modulation(self, x):\n        # frequency modulation function\n        return self.F * np.sin(2 * np.pi * self.F * x)\n\n    def adaptive_frequency_modulation(self):\n        # adaptive frequency modulation\n        self.F *= 1.1  # increase frequency\n        if random.random() < self.probability:\n            self.F *= 0.9  # decrease frequency with probability\n        return self.F\n\n    def levy_flight(self, x):\n        # levy flight function\n        step = np.random.normal(0, self.levy_flight_step_size, self.dim)\n        return x + step / np.abs(step)\n\n    def frequency_modulated_levy_flight(self, x):\n        # frequency-modulated levy flight function\n        self.F = self.adaptive_frequency_modulation()\n        step = np.random.normal(0, self.levy_flight_step_size, self.dim)\n        return x + self.F * np.sin(2 * np.pi * self.F * x) * step / np.abs(step)\n\n    def crossover(self, x1, x2):\n        # crossover operation\n        r = np.random.rand(self.dim)\n        return x1 + r * (x2 - x1)\n\n    def selection(self, x1, x2):\n        # selection operation\n        if self.evaluate(x2) < self.evaluate(x1):\n            return x2\n        else:\n            return x1\n\n    def information_based_selection(self, x1, x2):\n        # information-based selection\n        if self.evaluate(x2) < self.evaluate(x1):\n            return x2\n        else:\n            if random.random() < 0.5:\n                return x2\n            else:\n                return x1\n\n    def simulated_annealing(self, x):\n        # simulated annealing\n        new_x = x + np.random.normal(0, 1, self.dim)\n        new_x = np.clip(new_x, -5.0, 5.0)\n        if self.evaluate(new_x) < self.evaluate(x):\n            return new_x\n        else:\n            probability = np.exp(-(self.evaluate(new_x) - self.evaluate(x)) / self.temperature)\n            if random.random() < probability:\n                return new_x\n            else:\n                return x\n        self.temperature *= self.cooling_rate  # decrease temperature with a higher rate\n\n    def covariance_matrix_adaptation(self):\n        # covariance matrix adaptation\n        self.covariance_matrix = (1 - self.adaptive_step_size) * self.covariance_matrix + self.adaptive_step_size * np.eye(self.dim)\n        return self.covariance_matrix\n\n    def adaptive_step_size_learning(self):\n        # adaptive step size learning\n        self.adaptive_step_size += self.step_size_learning_rate * (self.evaluate(self.population[0]) - self.evaluate(self.population[-1]))\n        return self.adaptive_step_size\n\n    def history_learning(self):\n        # history learning\n        self.history = np.vstack((self.history, self.population[0]))\n\n    def optimize(self, func):\n        for i in range(self.budget):\n            # evaluate population\n            fitness = [self.evaluate(x) for x in self.population]\n            # get best solution\n            self.best_solution = min(fitness)\n            best_index = fitness.index(self.best_solution)\n            # adaptive frequency modulation\n            self.F = self.adaptive_frequency_modulation()\n            # differential evolution\n            for j in range(self.pop_size):\n                # generate trial vector\n                trial = self.crossover(self.population[j], self.population[random.randint(0, self.pop_size - 1)])\n                # evaluate trial vector\n                trial_fitness = self.evaluate(trial)\n                # selection\n                self.population[j] = self.selection(trial, self.population[j])\n                # update best solution\n                if trial_fitness < self.evaluate(self.population[j]):\n                    self.population[j] = trial\n            # frequency-modulated levy flight\n            for j in range(self.pop_size):\n                if random.random() < self.levy_flight_probability:\n                    self.population[j] = self.frequency_modulated_levy_flight(self.population[j])\n            # simulated annealing\n            for j in range(self.pop_size):\n                self.population[j] = self.simulated_annealing(self.population[j])\n            # information-based selection\n            for j in range(self.pop_size):\n                self.population[j] = self.information_based_selection(self.population[j], self.population[random.randint(0, self.pop_size - 1)])\n            # covariance matrix adaptation\n            self.covariance_matrix = self.covariance_matrix_adaptation()\n            # adaptive step size learning\n            self.adaptive_step_size = self.adaptive_step_size_learning()\n            # history learning\n            self.history_learning()\n        return self.best_solution, self.history\n\n    def __call__(self, func):\n        self.func = func\n        return self.optimize(func)\n\n# example usage\nbudget = 1000\ndim = 10\noptimizer = Novel_Metaheuristic_Algorithm(budget, dim)\nbest_solution, history = optimizer(lambda x: np.sum(x**2))\nprint(\"Best solution:\", best_solution)\nprint(\"History:\", history)", "name": "Novel_Metaheuristic_Algorithm", "description": "Novel Metaheuristic Algorithm with Adaptive Frequency Modulation, Covariance Matrix Adaptation, Simulated Annealing, and Levy Flight, refined to use a higher rate of temperature cooling and increased probability of probabilistic exploration with Information-based Selection and Adaptive Step Size Learning, and the addition of a novel \"Frequency-Modulated Levy Flight\" operator that combines frequency modulation with levy flight to improve exploration and exploitation.", "configspace": "", "generation": 92, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "e88d99b5-dc9d-4059-957a-48bca64d81c0", "metadata": {"aucs": [0.23515213697331294, 0.21837939525793348, 0.2306437993826157, 0.2027474533007778, 0.22279512922984757, 0.23550625755401522, 0.2227589239307084, 0.22128925473572847, 0.20802098711273154, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.05223085616113643, 0.08874301130767615, 0.06422997752127901, 0.06548637743676322, 0.0684923647474911, 0.06757487315307953, 0.05775667938048357, 0.057965871896490495, 0.05988237078494363, 0.06323826753622341, 0.05138344728767208, 0.057102688878647334, 0.05928274992881144, 0.04747728448367672, 0.059892844770615405, 0.06297139539269503, 0.06411363779269563, 0.047740024829620586, 0.5743801130390298, 0.6994824111104574, 0.7302739913614389, 0.838615206693847, 0.6480245257295898, 0.7828464141821567, 0.5619329359010077, 0.6929083142739176, 0.6018642932777847, 0.08375981959561574, 0.0725076997169597, 0.0822608503094503, 0.09974927659281474, 0.07203510574713856, 0.07289989723068269, 0.10392781941277995, 0.12181414905283894, 0.0925995133679911, 0.19165094402797944, 0.19358705193651427, 0.1261409364668652, 0.16471860278902206, 0.22856706606702704, 0.16005083155205446, 0.15583003188853595, 0.14637425315285868, 0.17701376261369983, 0.03569145658966355, 0.02827633361704196, 0.030855311953362663, 0.0810681676746412, 0.07550758741194152, 0.023876418382673514, 0.09053889276670779, 0.0573020079588108, 0.07297060899506558, 0.08932755708604256, 0.0746693965873626, 0.0847679710280903, 0.07743199520039235, 0.10346348469362565, 0.1264607487638998, 0.08420761018183365, 0.09173077556632447, 0.08147285078220434, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.051485070125743215, 0.05918951386493665, 0.05269270701778328, 0.05794520652122792, 0.016360847277333446, 0.014337939484356355, 0.0839397919434377, 0.03146815812025017, 0.03218670642440136, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.035767488748650655, 0.01620720589153901, 0.02010699011824324, 0.04396274129060862, 0.03332289330323224, 0.05974122677959259, 0.035745255895198724, 0.06150986718385343, 0.04094707201242631, 0.2291826094792122, 0.24747287897464543, 0.24025003269804357, 0.22811549743549864, 0.2550950228947795, 0.21052364470823615, 0.21175723550321746, 0.2353128402718555, 0.24878594381730135, 0.04523333279011055, 0.04896365258859092, 0.046232222364656406, 0.067086960926105, 0.06439589639391619, 0.07116944349430232, 0.05839858976856116, 0.06085919560200659, 0.06503311908020182, 0.11668490616105154, 0.1253654963680123, 0.12434251803721053, 0.11301251676570467, 0.12368121628847517, 0.10876066176073218, 0.11603400436157207, 0.11256274034400415, 0.11840301256798358, 0.17763298694089957, 0.16213970158699975, 0.18560005010151914, 0.20166214399600013, 0.20194277843185304, 0.19010210429334873, 0.16193917431626415, 0.17847463247348627, 0.15562580159558947, 0.12227946485807095, 0.1276963828290183, 0.12798676511438045, 0.1377031660613942, 0.1289449949307122, 0.13302048759479967, 0.11592691496129648, 0.12747442190186797, 0.13817818683341554, 0.16501184878255937, 0.164735558907538, 0.19141284113061574, 0.16452254427222923, 0.16899907785551227, 0.1688717312988245, 0.19421119979357948, 0.1576122650710522, 0.17674708543875617, 0.16818394196928899, 0.1593751385791351, 0.17060794330254625, 0.1532651885498637, 0.1592988419043202, 0.16504846412482388, 0.15446319338962566, 0.15480471589941314, 0.1556558057709675, 0.15339908491798904, 0.16196720769735162, 0.15857990056416837, 0.13694710732349813, 0.13260978468687368, 0.15335673669973948, 0.21387175619601317, 0.13683666045288878, 0.20522464332358437, 0.15607811733487564, 0.17499788554448836, 0.19600769677519925, 0.15544809213630129, 0.14649512592477076, 0.11936471230858092, 0.10303520185905468, 0.1977613283098012, 0.12421866872501508, 0.16277979192245107, 0.16151842819101492, 0.1746722095749641, 0.17012660780389322, 0.16114881211769438, 0.17065618078820122, 0.1931565874840102, 0.17722144254832362, 0.16097225085751798, 0.060359057427541885, 0.052756919897631316, 0.05948242263072212, 0.055310423029861555, 0.05310547599217008, 0.060207580237411684, 0.0726571042152887, 0.059218177299343755, 0.06676301884143943]}, "mutation_prompt": null}
{"id": "dbdceeeb-d701-47a0-912b-1f3b5ab26f43", "solution": "import numpy as np\nimport random\n\nclass Novel_Metaheuristic_Algorithm:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.F = 0.5  # initial frequency\n        self.CR = 0.5  # initial crossover rate\n        self.pop_size = 50  # initial population size\n        self.population = np.random.uniform(-5.0, 5.0, (self.pop_size, self.dim))\n        self.best_solution = np.inf\n        self.probability = 0.46  # increased probability of probabilistic exploration\n        self.temperature = 100  # initial temperature for simulated annealing\n        self.covariance_matrix = np.eye(self.dim)  # initial covariance matrix\n        self.adaptive_step_size = 0.1  # initial adaptive step size for covariance matrix adaptation\n        self.step_size_learning_rate = 0.001  # learning rate for adaptive step size\n        self.levy_flight_probability = 0.2  # probability of levy flight\n        self.levy_flight_step_size = 0.1  # step size for levy flight\n        self.history = np.zeros((self.budget, self.dim))  # history of best solutions\n        self.cooling_rate = 0.999  # higher rate of temperature cooling\n        self.probability_change = 1.1  # probability to change the individual lines of the selected solution\n\n    def evaluate(self, x):\n        return self.func(x)\n\n    def func(self, x):\n        # BBOB test suite of 24 noiseless functions\n        # Here we use the first function f1(x) = sum(x_i^2)\n        return np.sum(x**2)\n\n    def frequency_modulation(self, x):\n        # frequency modulation function\n        return self.F * np.sin(2 * np.pi * self.F * x)\n\n    def adaptive_frequency_modulation(self):\n        # adaptive frequency modulation\n        self.F *= 1.1  # increase frequency\n        if random.random() < self.probability:\n            self.F *= 0.9  # decrease frequency with probability\n        if random.random() < self.probability_change:\n            self.F = random.uniform(0, 10)  # change frequency\n        return self.F\n\n    def levy_flight(self, x):\n        # levy flight function\n        step = np.random.normal(0, self.levy_flight_step_size, self.dim)\n        return x + step / np.abs(step)\n\n    def frequency_modulated_levy_flight(self, x):\n        # frequency-modulated levy flight function\n        self.F = self.adaptive_frequency_modulation()\n        step = np.random.normal(0, self.levy_flight_step_size, self.dim)\n        return x + self.F * np.sin(2 * np.pi * self.F * x) * step / np.abs(step)\n\n    def crossover(self, x1, x2):\n        # crossover operation\n        r = np.random.rand(self.dim)\n        return x1 + r * (x2 - x1)\n\n    def selection(self, x1, x2):\n        # selection operation\n        if self.evaluate(x2) < self.evaluate(x1):\n            return x2\n        else:\n            return x1\n\n    def information_based_selection(self, x1, x2):\n        # information-based selection\n        if self.evaluate(x2) < self.evaluate(x1):\n            return x2\n        else:\n            if random.random() < 0.5:\n                return x2\n            else:\n                return x1\n\n    def simulated_annealing(self, x):\n        # simulated annealing\n        new_x = x + np.random.normal(0, 1, self.dim)\n        new_x = np.clip(new_x, -5.0, 5.0)\n        if self.evaluate(new_x) < self.evaluate(x):\n            return new_x\n        else:\n            probability = np.exp(-(self.evaluate(new_x) - self.evaluate(x)) / self.temperature)\n            if random.random() < probability:\n                return new_x\n            else:\n                return x\n        self.temperature *= self.cooling_rate  # decrease temperature with a higher rate\n\n    def covariance_matrix_adaptation(self):\n        # covariance matrix adaptation\n        self.covariance_matrix = (1 - self.adaptive_step_size) * self.covariance_matrix + self.adaptive_step_size * np.eye(self.dim)\n        return self.covariance_matrix\n\n    def adaptive_step_size_learning(self):\n        # adaptive step size learning\n        self.adaptive_step_size += self.step_size_learning_rate * (self.evaluate(self.population[0]) - self.evaluate(self.population[-1]))\n        return self.adaptive_step_size\n\n    def history_learning(self):\n        # history learning\n        self.history = np.vstack((self.history, self.population[0]))\n\n    def optimize(self, func):\n        for i in range(self.budget):\n            # evaluate population\n            fitness = [self.evaluate(x) for x in self.population]\n            # get best solution\n            self.best_solution = min(fitness)\n            best_index = fitness.index(self.best_solution)\n            # adaptive frequency modulation\n            self.F = self.adaptive_frequency_modulation()\n            # differential evolution\n            for j in range(self.pop_size):\n                # generate trial vector\n                trial = self.crossover(self.population[j], self.population[random.randint(0, self.pop_size - 1)])\n                # evaluate trial vector\n                trial_fitness = self.evaluate(trial)\n                # selection\n                self.population[j] = self.selection(trial, self.population[j])\n                # update best solution\n                if trial_fitness < self.evaluate(self.population[j]):\n                    self.population[j] = trial\n            # frequency-modulated levy flight\n            for j in range(self.pop_size):\n                if random.random() < self.levy_flight_probability:\n                    self.population[j] = self.frequency_modulated_levy_flight(self.population[j])\n            # simulated annealing\n            for j in range(self.pop_size):\n                self.population[j] = self.simulated_annealing(self.population[j])\n            # information-based selection\n            for j in range(self.pop_size):\n                self.population[j] = self.information_based_selection(self.population[j], self.population[random.randint(0, self.pop_size - 1)])\n            # covariance matrix adaptation\n            self.covariance_matrix = self.covariance_matrix_adaptation()\n            # adaptive step size learning\n            self.adaptive_step_size = self.adaptive_step_size_learning()\n            # history learning\n            self.history_learning()\n        return self.best_solution, self.history\n\n    def __call__(self, func):\n        self.func = func\n        return self.optimize(func)\n\n# example usage\nbudget = 1000\ndim = 10\noptimizer = Novel_Metaheuristic_Algorithm(budget, dim)\nbest_solution, history = optimizer(lambda x: np.sum(x**2))\nprint(\"Best solution:\", best_solution)\nprint(\"History:\", history)", "name": "Novel_Metaheuristic_Algorithm", "description": "Novel Metaheuristic Algorithm with Adaptive Frequency Modulation, Covariance Matrix Adaptation, Simulated Annealing, and Levy Flight, refined to use a higher rate of temperature cooling and increased probability of probabilistic exploration with Information-based Selection and Adaptive Step Size Learning, and the addition of a novel \"Frequency-Modulated Levy Flight\" operator that combines frequency modulation with levy flight to improve exploration and exploitation, and also with a 1.1 probability to change the individual lines of the selected solution to refine its strategy.", "configspace": "", "generation": 97, "fitness": 0.12807091469132653, "feedback": "The algorithm Novel_Metaheuristic_Algorithm got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.13 with standard deviation 0.12.", "error": "", "parent_id": "e88d99b5-dc9d-4059-957a-48bca64d81c0", "metadata": {"aucs": [0.2348016969105834, 0.2275034779397339, 0.2530094783225213, 0.2144861099308205, 0.23506224576192458, 0.2528144470587649, 0.2271301250390878, 0.19715594128336755, 0.261763874789687, 0.03814355321370677, 9.999999999998899e-05, 0.00023062399406859058, 0.030168157437646892, 9.999999999998899e-05, 0.02062953605726814, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.05430189822395037, 0.06131108252358752, 0.06478277513553676, 0.0833957890717707, 0.06084484176949334, 0.07301976311480773, 0.060414794207004574, 0.06222042914070558, 0.060052605525775515, 0.053246141092241905, 0.061196773497640744, 0.06004158690055794, 0.050350228917397044, 0.052355067790264886, 0.06323324574584022, 0.044868890907675096, 0.06995930412733598, 0.05867460582187323, 0.7043424323121523, 0.4658769676567789, 0.17473997113546413, 0.3975180728070219, 0.885313978094946, 0.14017286351868674, 0.6935714317026132, 0.68831649546766, 0.47875247694812306, 0.09376084597592238, 0.14093176193216472, 0.09819470104947647, 0.13177347114197657, 0.07880598795516702, 0.12064113114842989, 0.12136916882146631, 0.0882436232259104, 0.10563275884054024, 0.19499964044448115, 0.17837652158609651, 0.13073526131393953, 0.17101558202246725, 0.15755331290777175, 0.12581459345439017, 0.15856546267548932, 0.2016309085662008, 0.16911834056784247, 0.07446905864038511, 0.08944180811899516, 0.07153047687871494, 0.0715080806062588, 0.07366379911141596, 0.09202521677844722, 0.1080823031958591, 0.09537557982111833, 0.07568645736729629, 0.08570303372004562, 0.08534371533464113, 0.10700334331266803, 0.09622629745408817, 0.08235813656486768, 0.11525771653990347, 0.09615462039052558, 0.07215236791646307, 0.08459767573895771, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.0019963682879222677, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.000514365146216389, 0.07329980837022065, 0.12106691646913603, 0.08090424660510875, 0.07694963453512738, 0.043153912797239125, 0.027908576857528122, 0.04929600852398397, 0.023319863317559753, 0.040699701717760606, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.04111653677154503, 0.03733272446761782, 0.013062269598463128, 0.02410845610302026, 0.05711196753230119, 0.06403333457287697, 9.999999999998899e-05, 0.025688737883728163, 0.025486814924670975, 0.2346929738947544, 0.2253767208759434, 0.20805143866963605, 0.24634852106697258, 0.2393718763900895, 0.23886808947941496, 0.26405751501237096, 0.24783109432617123, 0.2175702482366083, 0.052185268903246484, 0.08101935363319901, 0.06112560647260523, 0.06216854338218458, 0.06319300584518373, 0.05096451452923845, 0.05454413892630383, 0.08485628908477716, 0.06928991482745972, 0.11679193044909297, 0.12918741103932896, 0.11842175776965747, 0.11757310052718839, 0.12976277286434323, 0.13044211365586977, 0.1036525939030194, 0.11662097153138506, 0.1325969413111021, 0.16672112126644845, 0.1863802394842029, 0.17638159890597105, 0.1744850157256158, 0.17026141549427642, 0.19307318513990845, 0.169207718877729, 0.19239976452313035, 0.17427677290865629, 0.1238411524681331, 0.1515316388620851, 0.14431544885630132, 0.11617607630278304, 0.11709932316715532, 0.12610764152945686, 0.12731244992608315, 0.1255653496475121, 0.11106456228769335, 0.16849262718536684, 0.2216974137774178, 0.18809499562081877, 0.187323569847206, 0.1813955607827289, 0.18048124166685298, 0.17901369539561074, 0.17191402717520832, 0.1719357218177513, 0.162318573943405, 0.15359042649191357, 0.16094538996698105, 0.1649670032398135, 0.16398057763077722, 0.17676941507571087, 0.15764152399890663, 0.156488025958179, 0.15376160664808847, 0.15443078721992753, 0.1562418512707533, 0.1372237638667031, 0.2865732881719093, 0.16694341884617425, 0.2772795921373027, 0.15412161078177955, 0.18393846891295296, 0.18148964616478291, 0.22301228546103113, 0.1472426474411883, 0.18744761703260326, 0.16672348964202266, 0.1719595300692328, 0.2080801693562484, 0.13914312312082455, 0.15482702503219703, 0.11165917921535184, 0.16252527223954405, 0.15999425035483383, 0.16419772837385604, 0.1722663066382667, 0.15723609500066027, 0.16987139417395591, 0.15119786862698825, 0.1730030724474666, 0.17278775654830947, 0.05677129887770993, 0.06481981240862822, 0.05208130118125531, 0.06396083529945551, 0.0551153033312769, 0.06087676105599382, 0.05023013366095874, 0.0778562854240854, 0.057845239238775825]}, "mutation_prompt": null}
{"id": "85803a9e-685a-4f7f-84bf-102799cc2068", "solution": "import numpy as np\nimport random\n\nclass Novel_Metaheuristic_Algorithm:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.F = 0.5  # initial frequency\n        self.CR = 0.5  # initial crossover rate\n        self.pop_size = 50  # initial population size\n        self.population = np.random.uniform(-5.0, 5.0, (self.pop_size, self.dim))\n        self.best_solution = np.inf\n        self.probability = 0.46  # increased probability of probabilistic exploration\n        self.temperature = 100  # initial temperature for simulated annealing\n        self.covariance_matrix = np.eye(self.dim)  # initial covariance matrix\n        self.adaptive_step_size = 0.1  # initial adaptive step size for covariance matrix adaptation\n        self.step_size_learning_rate = 0.001  # learning rate for adaptive step size\n        self.levy_flight_probability = 0.2  # probability of levy flight\n        self.levy_flight_step_size = 0.1  # step size for levy flight\n        self.history = np.zeros((self.budget, self.dim))  # history of best solutions\n        self.cooling_rate = 0.999  # higher rate of temperature cooling\n\n    def evaluate(self, x):\n        return self.func(x)\n\n    def func(self, x):\n        # BBOB test suite of 24 noiseless functions\n        # Here we use the first function f1(x) = sum(x_i^2)\n        return np.sum(x**2)\n\n    def frequency_modulation(self, x):\n        # frequency modulation function\n        return self.F * np.sin(2 * np.pi * self.F * x)\n\n    def adaptive_frequency_modulation(self):\n        # adaptive frequency modulation\n        self.F *= 1.1  # increase frequency\n        if random.random() < self.probability:\n            self.F *= 0.9  # decrease frequency with probability\n        return self.F\n\n    def levy_flight(self, x):\n        # levy flight function\n        step = np.random.normal(0, self.levy_flight_step_size, self.dim)\n        return x + step / np.abs(step)\n\n    def frequency_modulated_levy_flight(self, x):\n        # frequency-modulated levy flight function\n        self.F = self.adaptive_frequency_modulation()\n        step = np.random.normal(0, self.levy_flight_step_size, self.dim)\n        return x + self.F * np.sin(2 * np.pi * self.F * x) * step / np.abs(step)\n\n    def crossover(self, x1, x2):\n        # crossover operation\n        r = np.random.rand(self.dim)\n        return x1 + r * (x2 - x1)\n\n    def selection(self, x1, x2):\n        # selection operation\n        if self.evaluate(x2) < self.evaluate(x1):\n            return x2\n        else:\n            return x1\n\n    def information_based_selection(self, x1, x2):\n        # information-based selection\n        if self.evaluate(x2) < self.evaluate(x1):\n            return x2\n        else:\n            if random.random() < 0.5:\n                return x2\n            else:\n                return x1\n\n    def simulated_annealing(self, x):\n        # simulated annealing\n        new_x = x + np.random.normal(0, 1, self.dim)\n        new_x = np.clip(new_x, -5.0, 5.0)\n        if self.evaluate(new_x) < self.evaluate(x):\n            return new_x\n        else:\n            probability = np.exp(-(self.evaluate(new_x) - self.evaluate(x)) / self.temperature)\n            if random.random() < probability:\n                return new_x\n            else:\n                return x\n        self.temperature *= self.cooling_rate  # decrease temperature with a higher rate\n\n    def covariance_matrix_adaptation(self):\n        # covariance matrix adaptation\n        self.covariance_matrix = (1 - self.adaptive_step_size) * self.covariance_matrix + self.adaptive_step_size * np.eye(self.dim)\n        return self.covariance_matrix\n\n    def adaptive_step_size_learning(self):\n        # adaptive step size learning\n        self.adaptive_step_size += self.step_size_learning_rate * (self.evaluate(self.population[0]) - self.evaluate(self.population[-1]))\n        return self.adaptive_step_size\n\n    def history_learning(self):\n        # history learning\n        self.history = np.vstack((self.history, self.population[0]))\n\n    def modify_individual_lines(self):\n        # 5% chance of modifying individual lines of the selected solution\n        if random.random() < 0.05:\n            self.population[random.randint(0, self.pop_size - 1)] = np.random.uniform(-5.0, 5.0, self.dim)\n\n    def optimize(self, func):\n        for i in range(self.budget):\n            # evaluate population\n            fitness = [self.evaluate(x) for x in self.population]\n            # get best solution\n            self.best_solution = min(fitness)\n            best_index = fitness.index(self.best_solution)\n            # adaptive frequency modulation\n            self.F = self.adaptive_frequency_modulation()\n            # differential evolution\n            for j in range(self.pop_size):\n                # generate trial vector\n                trial = self.crossover(self.population[j], self.population[random.randint(0, self.pop_size - 1)])\n                # evaluate trial vector\n                trial_fitness = self.evaluate(trial)\n                # selection\n                self.population[j] = self.selection(trial, self.population[j])\n                # update best solution\n                if trial_fitness < self.evaluate(self.population[j]):\n                    self.population[j] = trial\n            # frequency-modulated levy flight\n            for j in range(self.pop_size):\n                if random.random() < self.levy_flight_probability:\n                    self.population[j] = self.frequency_modulated_levy_flight(self.population[j])\n            # simulated annealing\n            for j in range(self.pop_size):\n                self.population[j] = self.simulated_annealing(self.population[j])\n            # information-based selection\n            for j in range(self.pop_size):\n                self.population[j] = self.information_based_selection(self.population[j], self.population[random.randint(0, self.pop_size - 1)])\n            # covariance matrix adaptation\n            self.covariance_matrix = self.covariance_matrix_adaptation()\n            # adaptive step size learning\n            self.adaptive_step_size = self.adaptive_step_size_learning()\n            # history learning\n            self.history_learning()\n            # modify individual lines\n            self.modify_individual_lines()\n        return self.best_solution, self.history\n\n    def __call__(self, func):\n        self.func = func\n        return self.optimize(func)\n\n# example usage\nbudget = 1000\ndim = 10\noptimizer = Novel_Metaheuristic_Algorithm(budget, dim)\nbest_solution, history = optimizer(lambda x: np.sum(x**2))\nprint(\"Best solution:\", best_solution)\nprint(\"History:\", history)", "name": "Novel_Metaheuristic_Algorithm", "description": "Novel Metaheuristic Algorithm with Adaptive Frequency Modulation, Covariance Matrix Adaptation, Simulated Annealing, and Levy Flight, refined to use a higher rate of temperature cooling and increased probability of probabilistic exploration with Information-based Selection and Adaptive Step Size Learning, and the addition of a novel \"Frequency-Modulated Levy Flight\" operator that combines frequency modulation with levy flight to improve exploration and exploitation, with 5% chance of modifying individual lines of the selected solution.", "configspace": "", "generation": 98, "fitness": 0.12899762608719442, "feedback": "The algorithm Novel_Metaheuristic_Algorithm got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.13 with standard deviation 0.12.", "error": "", "parent_id": "e88d99b5-dc9d-4059-957a-48bca64d81c0", "metadata": {"aucs": [0.21658079735074975, 0.2555340726045443, 0.24997247949350399, 0.23066540584798823, 0.22337986163026413, 0.22543155180811014, 0.2608538176944578, 0.22255646113882477, 0.2184829609845439, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.06108511510674819, 0.062217822522978916, 0.051487688888322225, 0.06680351624406866, 0.06271278525027424, 0.06685674410784137, 0.06722105701177827, 0.06977703312703776, 0.09231944874640363, 0.05408807069137056, 0.043809939442870593, 0.06821923475575986, 0.04992846316793709, 0.04795964403085562, 0.039623767580101, 0.05976745888446788, 0.050684080065719894, 0.05492717349801801, 0.5192480368466568, 0.737398403848829, 0.6442101362344306, 0.6066263674723307, 0.3279449809092113, 0.7025120814310865, 0.4685896313139152, 0.6432323200909436, 0.5675969151606798, 0.08905670883460404, 0.09016385522826853, 0.07838804445836411, 0.09709162227704438, 0.08688517067371215, 0.09077659048964237, 0.10397566462713748, 0.12182345201176104, 0.12214962962839504, 0.14765577804077623, 0.17083809299828667, 0.21135250984450482, 0.2034659050301446, 0.17000863718984438, 0.2019681192720737, 0.12228175725805068, 0.14215826143863874, 0.15423175544148293, 0.08306684700604205, 0.00541810881354432, 0.03035650311558835, 0.050715021042256, 0.1136672561693487, 0.062343455889287136, 0.08253445166636686, 0.049272844127201676, 0.09427456145522795, 0.09312398074641492, 0.09569885540746259, 0.10159184313992664, 0.07675124625310181, 0.10283783425422277, 0.08778484092219618, 0.07633112481104087, 0.08441047203364527, 0.09721282285352806, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.12165802065053921, 0.06420594577494287, 0.04398957146698301, 0.05261257625560478, 0.06154238552754143, 0.042822219178016874, 0.06362616447353087, 0.036393957001731536, 0.03870073183336231, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.01592751415841487, 0.007224826324398026, 0.02001073300590961, 0.040047215807696324, 0.05728854187091714, 0.02820719659398707, 0.02764233763750057, 0.017568112345142106, 0.028114510335137677, 0.23724690772487267, 0.23897670263575455, 0.22889363177372024, 0.22056351806536678, 0.23663682302111944, 0.2446375383275401, 0.2279636437330399, 0.22791333411150894, 0.24555939216305356, 0.053648093497289495, 0.06718068794202114, 0.08156464642802308, 0.05828827119813318, 0.05037348156982824, 0.06230671828108447, 0.060880621487809705, 0.06141532650632686, 0.05353624675707347, 0.1256242694100549, 0.10812746320652955, 0.12986736164624157, 0.10923937348972101, 0.1345191435787657, 0.154099918588412, 0.1299035198540618, 0.11411210217049572, 0.13197928462461117, 0.1915457676680027, 0.17966180364570428, 0.18446799393622693, 0.19623181727422545, 0.1853050141752688, 0.17473538249769915, 0.17092284587162887, 0.19334488677254114, 0.15997325423329, 0.12167379995442262, 0.14521999414938092, 0.12394780080619161, 0.15645032444067297, 0.12006024308040797, 0.13406219889658444, 0.12727101206693647, 0.15096932776669492, 0.11629306177724974, 0.17122408254903698, 0.1743218911111999, 0.18260693730307231, 0.14951041075302696, 0.17556429725247558, 0.16269409893477715, 0.19012330927377374, 0.18787215095240073, 0.16088853460265995, 0.16152466173024171, 0.15613832611179046, 0.15180982807628274, 0.16485937652650795, 0.16392470429481087, 0.15848740102697756, 0.17402818709609014, 0.15533049169940827, 0.16302661050320144, 0.16590794205672277, 0.1587520614046547, 0.15964715560797782, 0.3425576104037181, 0.1956544432098889, 0.1283286862901163, 0.21163839881418367, 0.1372155491381889, 0.17944308982016755, 0.220898721744752, 0.18387845375859146, 0.16541465278503187, 0.1660969423595583, 0.16057015712786327, 0.21887965452983293, 0.14217010591205292, 0.14455343440695323, 0.1499691561922415, 0.16501771153385636, 0.17389639250562794, 0.16973876319934478, 0.16372206918934495, 0.17929300987046248, 0.16347724547879516, 0.1721175522122177, 0.1720671197802469, 0.1652950695768688, 0.0548268149872172, 0.06318709053140159, 0.05144319927850649, 0.05928775455061874, 0.057708132906433285, 0.06651704667026714, 0.06347393706381721, 0.055419520796886945, 0.05204122886214879]}, "mutation_prompt": null}
{"id": "c19cd2b5-ec66-4bcd-a7fe-9640d9c1aed5", "solution": "import numpy as np\nimport random\n\nclass Novel_Metaheuristic_Algorithm:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.F = 0.5  # initial frequency\n        self.CR = 0.5  # initial crossover rate\n        self.pop_size = 50  # initial population size\n        self.population = np.random.uniform(-5.0, 5.0, (self.pop_size, self.dim))\n        self.best_solution = np.inf\n        self.probability = 0.46  # increased probability of probabilistic exploration\n        self.temperature = 100  # initial temperature for simulated annealing\n        self.covariance_matrix = np.eye(self.dim)  # initial covariance matrix\n        self.adaptive_step_size = 0.1  # initial adaptive step size for covariance matrix adaptation\n        self.step_size_learning_rate = 0.001  # learning rate for adaptive step size\n        self.levy_flight_probability = 0.2  # probability of levy flight\n        self.levy_flight_step_size = 0.1  # step size for levy flight\n        self.history = np.zeros((self.budget, self.dim))  # history of best solutions\n        self.cooling_rate = 0.999  # higher rate of temperature cooling\n        self.leapfrog_probability = 0.2  # probability of leapfrog operator\n\n    def evaluate(self, x):\n        return self.func(x)\n\n    def func(self, x):\n        # BBOB test suite of 24 noiseless functions\n        # Here we use the first function f1(x) = sum(x_i^2)\n        return np.sum(x**2)\n\n    def frequency_modulation(self, x):\n        # frequency modulation function\n        return self.F * np.sin(2 * np.pi * self.F * x)\n\n    def adaptive_frequency_modulation(self):\n        # adaptive frequency modulation\n        self.F *= 1.1  # increase frequency\n        if random.random() < self.probability:\n            self.F *= 0.9  # decrease frequency with probability\n        return self.F\n\n    def levy_flight(self, x):\n        # levy flight function\n        step = np.random.normal(0, self.levy_flight_step_size, self.dim)\n        return x + step / np.abs(step)\n\n    def frequency_modulated_levy_flight(self, x):\n        # frequency-modulated levy flight function\n        self.F = self.adaptive_frequency_modulation()\n        step = np.random.normal(0, self.levy_flight_step_size, self.dim)\n        return x + self.F * np.sin(2 * np.pi * self.F * x) * step / np.abs(step)\n\n    def leapfrog(self, x):\n        # leapfrog operator\n        self.F = self.adaptive_frequency_modulation()\n        step = np.random.normal(0, self.levy_flight_step_size, self.dim)\n        return x + self.F * np.sin(2 * np.pi * self.F * x) * step / np.abs(step)\n\n    def crossover(self, x1, x2):\n        # crossover operation\n        r = np.random.rand(self.dim)\n        return x1 + r * (x2 - x1)\n\n    def selection(self, x1, x2):\n        # selection operation\n        if self.evaluate(x2) < self.evaluate(x1):\n            return x2\n        else:\n            return x1\n\n    def information_based_selection(self, x1, x2):\n        # information-based selection\n        if self.evaluate(x2) < self.evaluate(x1):\n            return x2\n        else:\n            if random.random() < 0.5:\n                return x2\n            else:\n                return x1\n\n    def simulated_annealing(self, x):\n        # simulated annealing\n        new_x = x + np.random.normal(0, 1, self.dim)\n        new_x = np.clip(new_x, -5.0, 5.0)\n        if self.evaluate(new_x) < self.evaluate(x):\n            return new_x\n        else:\n            probability = np.exp(-(self.evaluate(new_x) - self.evaluate(x)) / self.temperature)\n            if random.random() < probability:\n                return new_x\n            else:\n                return x\n        self.temperature *= self.cooling_rate  # decrease temperature with a higher rate\n\n    def covariance_matrix_adaptation(self):\n        # covariance matrix adaptation\n        self.covariance_matrix = (1 - self.adaptive_step_size) * self.covariance_matrix + self.adaptive_step_size * np.eye(self.dim)\n        return self.covariance_matrix\n\n    def adaptive_step_size_learning(self):\n        # adaptive step size learning\n        self.adaptive_step_size += self.step_size_learning_rate * (self.evaluate(self.population[0]) - self.evaluate(self.population[-1]))\n        return self.adaptive_step_size\n\n    def history_learning(self):\n        # history learning\n        self.history = np.vstack((self.history, self.population[0]))\n\n    def optimize(self, func):\n        for i in range(self.budget):\n            # evaluate population\n            fitness = [self.evaluate(x) for x in self.population]\n            # get best solution\n            self.best_solution = min(fitness)\n            best_index = fitness.index(self.best_solution)\n            # adaptive frequency modulation\n            self.F = self.adaptive_frequency_modulation()\n            # differential evolution\n            for j in range(self.pop_size):\n                # generate trial vector\n                trial = self.crossover(self.population[j], self.population[random.randint(0, self.pop_size - 1)])\n                # evaluate trial vector\n                trial_fitness = self.evaluate(trial)\n                # selection\n                self.population[j] = self.selection(trial, self.population[j])\n                # update best solution\n                if trial_fitness < self.evaluate(self.population[j]):\n                    self.population[j] = trial\n            # frequency-modulated levy flight\n            for j in range(self.pop_size):\n                if random.random() < self.levy_flight_probability:\n                    self.population[j] = self.frequency_modulated_levy_flight(self.population[j])\n            # leapfrog operator\n            for j in range(self.pop_size):\n                if random.random() < self.leapfrog_probability:\n                    self.population[j] = self.leapfrog(self.population[j])\n            # simulated annealing\n            for j in range(self.pop_size):\n                self.population[j] = self.simulated_annealing(self.population[j])\n            # information-based selection\n            for j in range(self.pop_size):\n                self.population[j] = self.information_based_selection(self.population[j], self.population[random.randint(0, self.pop_size - 1)])\n            # covariance matrix adaptation\n            self.covariance_matrix = self.covariance_matrix_adaptation()\n            # adaptive step size learning\n            self.adaptive_step_size = self.adaptive_step_size_learning()\n            # history learning\n            self.history_learning()\n        return self.best_solution, self.history\n\n    def __call__(self, func):\n        self.func = func\n        return self.optimize(func)\n\n# example usage\nbudget = 1000\ndim = 10\noptimizer = Novel_Metaheuristic_Algorithm(budget, dim)\nbest_solution, history = optimizer(lambda x: np.sum(x**2))\nprint(\"Best solution:\", best_solution)\nprint(\"History:\", history)", "name": "Novel_Metaheuristic_Algorithm", "description": "Novel Metaheuristic Algorithm with Adaptive Frequency Modulation, Covariance Matrix Adaptation, Simulated Annealing, Levy Flight, and Frequency-Modulated Levy Flight operators, refined with a higher rate of temperature cooling, increased probability of probabilistic exploration, and information-based selection.", "configspace": "", "generation": 99, "fitness": 0.1266752543512147, "feedback": "The algorithm Novel_Metaheuristic_Algorithm got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.13 with standard deviation 0.15.", "error": "", "parent_id": "e88d99b5-dc9d-4059-957a-48bca64d81c0", "metadata": {"aucs": [0.20982729597392447, 0.19626811688770318, 0.2178979083785031, 0.22828192264013902, 0.20366846454240617, 0.2373450409692608, 0.23659487629716514, 0.19295199420330977, 0.22827683333185023, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.00014954203004524658, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.056654672674949724, 0.050904436277432796, 0.048323037049119, 0.08043854735883926, 0.05397534418626759, 0.07611181726143257, 0.047080809879861185, 0.05819691587971432, 0.0832621456442032, 0.04571189496725825, 0.0483756995400727, 0.04691905699369159, 0.04425822547031133, 0.05957729091647146, 0.04182701541254119, 0.02504456484046691, 0.03335278843090961, 0.04483177459162069, 0.787550404595254, 0.7914060051726983, 0.46431866621266793, 0.7907500944673733, 0.8844028548624613, 0.7000388738701816, 0.5768672245495682, 0.7877311279666511, 0.8236849604694547, 0.07360143027731414, 0.09420358137128337, 0.06690060581608415, 0.08275757921469218, 0.07209520918406853, 0.06465515984881198, 0.10619002495985586, 0.06622627958817962, 0.07453493592367444, 0.13935667481252945, 0.1402924759686035, 0.10085107474048238, 0.15793244753582114, 0.16012581026741168, 0.1971853610554014, 0.13107304389301389, 0.14780109804488184, 0.12972162751748018, 0.05614731448257315, 0.0235885299907439, 0.06805607154482363, 0.030341870595040343, 0.009364922924937624, 0.07289677850553378, 0.06473977481743043, 0.0630437980158971, 0.082654545632593, 0.018029667412335804, 0.03685852116145327, 0.0626661775904942, 0.06683077953944194, 0.0749055064638382, 0.03647339851913478, 0.051852697780135903, 0.039642747070312345, 0.06385372786501031, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.09232613499689091, 0.05226572399421714, 0.04435691742443015, 0.0401032349982543, 0.0006398916026167933, 0.027795349232409672, 0.029193982570606347, 0.07338600271688633, 0.018403875852551987, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.0159305209024162, 0.02410368661242679, 0.06734280374027435, 0.023742330859316096, 0.009919507083055445, 0.03161816411374552, 0.01669558182000852, 0.02441078401831165, 0.03719635066782412, 0.2170376240160382, 0.2148210903604073, 0.24549656762310523, 0.1856067268139724, 0.1814774564988727, 0.20329548703046563, 0.20987095237636988, 0.21794656635626397, 0.22063515526362254, 0.0661179004021083, 0.06933765290601912, 0.06732867552277988, 0.06608263371085954, 0.04828078857582774, 0.05270522917022025, 0.06786748495408179, 0.048142304225416765, 0.06863082339734794, 0.11628566214750025, 0.10330790900033449, 0.12748605600430896, 0.11562565036691197, 0.11926874907852703, 0.1177626366905139, 0.10881860674598665, 0.10165587181958602, 0.10708919551630247, 0.18457742563606439, 0.15910631753336768, 0.1613932299901245, 0.16526126757513993, 0.18189580315589526, 0.1665787112181799, 0.15949458614512246, 0.19401750501627812, 0.17747672414853732, 0.11304099769867948, 0.09958737525247885, 0.15083899270057377, 0.12114999847008168, 0.1207427244824596, 0.12273581331864247, 0.12883799455213452, 0.1422449832600441, 0.12723477381593018, 0.16310930397059142, 0.15395300967766867, 0.18494265519146613, 0.14913657900801613, 0.15812633088159622, 0.17984841100465732, 0.16265694732428293, 0.16331765136216603, 0.1599233827044474, 0.1726743317106002, 0.1524955618503695, 0.17741789173507883, 0.15935946494790465, 0.16073143068855356, 0.16521804921909367, 0.1514617109810219, 0.15630493698857284, 0.16119894731843776, 0.16197366799986035, 0.1551595594668046, 0.18319624219425046, 0.2034891225114629, 0.18918175193888642, 0.12356431665860856, 0.34314195147286397, 0.20069406350594643, 0.20560941196654137, 0.1320848513661682, 0.24508446962643715, 0.1783131113054799, 0.11026068607766337, 0.13561957482566123, 0.1551448963635541, 0.15269546850444726, 0.1224535589275858, 0.12656904915373812, 0.15764489801519777, 0.16016208103007812, 0.17622522329358958, 0.21256027705191682, 0.15936266432199786, 0.17930812756973313, 0.16447132687766775, 0.18323213413479722, 0.19580391660603758, 0.05927872640667431, 0.052727686237953875, 0.0524972160945828, 0.05733414683021654, 0.056031869010982005, 0.06377560005028626, 0.05771048675227919, 0.05309708147849579, 0.057982109192574516]}, "mutation_prompt": null}
