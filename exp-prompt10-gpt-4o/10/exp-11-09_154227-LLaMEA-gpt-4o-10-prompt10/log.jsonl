{"id": "2778f81b-4fe3-49b5-ac9f-5e39c1e0f4f7", "solution": "import numpy as np\n\nclass HybridStochasticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20  # could be tuned\n        self.scale_factor = 0.8\n        self.crossover_prob = 0.7\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n    \n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:  # If not evaluated yet\n                self.fitness[i] = func(self.population[i])\n                self.evaluations += 1\n    \n    def select_parents(self, idx):\n        indices = [i for i in range(self.population_size) if i != idx]\n        return np.random.choice(indices, 3, replace=False)\n    \n    def differential_evolution_mutation(self, idx):\n        a, b, c = self.select_parents(idx)\n        mutant_vector = self.population[a] + self.scale_factor * (self.population[b] - self.population[c])\n        return np.clip(mutant_vector, self.lower_bound, self.upper_bound)\n    \n    def crossover(self, target_vector, mutant_vector):\n        crossover_mask = np.random.rand(self.dim) < self.crossover_prob\n        return np.where(crossover_mask, mutant_vector, target_vector)\n    \n    def local_search(self, vector):\n        perturbation = np.random.normal(0, 0.1, self.dim)  # Small Gaussian perturbations\n        candidate_vector = vector + perturbation\n        return np.clip(candidate_vector, self.lower_bound, self.upper_bound)\n    \n    def __call__(self, func):\n        self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n                \n                mutant_vector = self.differential_evolution_mutation(i)\n                trial_vector = self.crossover(self.population[i], mutant_vector)\n                trial_fitness = func(trial_vector)\n                self.evaluations += 1\n                \n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial_vector\n                    self.fitness[i] = trial_fitness\n                \n                # Adaptive Local Search\n                if np.random.rand() < 0.2:  # Probability of local search\n                    candidate_vector = self.local_search(self.population[i])\n                    candidate_fitness = func(candidate_vector)\n                    self.evaluations += 1\n                    if candidate_fitness < self.fitness[i]:\n                        self.population[i] = candidate_vector\n                        self.fitness[i] = candidate_fitness\n        \n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx]", "name": "HybridStochasticOptimizer", "description": "A novel hybrid stochastic optimization algorithm combining Differential Evolution with Adaptive Local Search for efficient exploration and exploitation in high-dimensional spaces.", "configspace": "", "generation": 0, "fitness": 0.35908017455813773, "feedback": "The algorithm HybridStochasticOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.36 with standard deviation 0.25.", "error": "", "parent_id": null, "metadata": {"aucs": [0.838550702247986, 0.8335387980441803, 0.8392883035317981, 0.8448220232438199, 0.8692280794153219, 0.86492167545159, 0.8423065034848981, 0.8684684289902511, 0.8603227149202752, 0.7334709241625984, 0.7334474051410083, 0.7264747402639247, 0.7344716495540463, 0.7333723373934493, 0.7400257469840834, 0.7326254850725233, 0.7347331739693774, 0.7361787942160427, 0.2900640312488526, 0.47241555583359396, 0.4068320718407943, 0.38522901052495495, 0.4980851072709567, 0.36017996570170985, 0.3855858146223492, 0.5301915371647596, 0.45367220095702276, 0.15700893469067034, 0.15086174717667622, 0.3606580261151082, 0.3820765391627686, 0.37857498203107787, 0.24358195595397236, 0.44006871328822617, 0.15619687169305285, 0.5431907915884118, 0.9514039641368535, 0.9599028786233561, 0.9703152109391081, 0.9689783778060999, 0.9634907937987585, 0.977053323860018, 0.9713025537743715, 0.9710944937497286, 0.9623817898603562, 0.4811632069641354, 0.4861494717101681, 0.5181873123716814, 0.4868754422257876, 0.5064595460782774, 0.5046714079907575, 0.48795856678445426, 0.5189360989349037, 0.4807572622639553, 0.5868371238845421, 0.6828198726555201, 0.6456891282317958, 0.5918970855007204, 0.6200859298334032, 0.7702265673351774, 0.6958432356063231, 0.7340731017036689, 0.7310117434316175, 0.2595202684690293, 0.2665440796289047, 0.3447806314120041, 0.2259147785649217, 0.27326843203813556, 0.32150330600063937, 0.28010562289764906, 0.29552641888490006, 0.32221432565738917, 0.12522923128088514, 0.21777955819361272, 0.14790043362950323, 0.27777879274959094, 0.2221290906104053, 0.2546698244616422, 0.28777697821894743, 0.25863731044319516, 0.20839151152332214, 0.08200045947449175, 0.1233998060896413, 0.09590295126948145, 0.11121731514540156, 0.08626097600496452, 0.10894404364206134, 0.08859205077834265, 0.08679856412127096, 0.07606669628336682, 0.21252808557164604, 0.22412982702821604, 0.20390446043916832, 0.22534486281182287, 0.16119227720619456, 0.17904454354061838, 0.21567655887183101, 0.3290280255186192, 0.2655490022975723, 0.00656682574070222, 0.030575438199327065, 0.03955418050983606, 0.02393376066661479, 0.05524215352879602, 0.07951077146365404, 0.03032444160772796, 0.08778729249895956, 0.07737778818842145, 0.19516566745890707, 0.1937347077828694, 0.19411490582830215, 0.21154639947615128, 0.20856128496804915, 0.20410401494498287, 0.18879954051159387, 0.188381415941566, 0.1647126038437029, 0.5421754637547482, 0.5695984805554837, 0.5600632409189952, 0.5552469615873223, 0.5724375125380385, 0.5565735237775709, 0.5734271428993263, 0.5412097018577038, 0.5590617205170056, 0.11818203894987733, 0.11594934899816578, 0.11548728812285303, 0.10910969597536346, 0.09324766404200946, 0.11297884701366567, 0.11728542093218253, 0.1358283894323311, 0.11510974598355384, 0.22540283983726417, 0.17103914371889317, 0.24935010026333593, 0.23139708991581187, 0.2116834840101205, 0.19135882098285617, 0.1705371299341023, 0.2126699572935572, 0.2209012418595554, 0.32685385154538005, 0.34314356484347186, 0.36988756800192024, 0.36952191550610336, 0.375443089514364, 0.2927272068359621, 0.41490454614332994, 0.4348893987389886, 0.3834987498697635, 0.23366535767951468, 0.25866189725809885, 0.2302530339437413, 0.22554822145020648, 0.24019648694644558, 0.21306175836868801, 0.23819888206020323, 0.27857543630058956, 0.25651404700229974, 0.18477259167128046, 0.19426795408013386, 0.19595086844004206, 0.20717051734986014, 0.19752325636984547, 0.22049037243377545, 0.19758769044907198, 0.2250685195540405, 0.1757806127460937, 0.2890409684568702, 0.2535622220515875, 0.40789869201342577, 0.36433021362110163, 0.43665556031431274, 0.20627895553091957, 0.2585961925724303, 0.43201072946856545, 0.21213576965002445, 0.7592369689589549, 0.2000334564012084, 0.19557859121926469, 0.7296203247163067, 0.18252932373457764, 0.6585988508625994, 0.6557618619990195, 0.17668310156736222, 0.4550001636102414, 0.1977316349668934, 0.20570403192958386, 0.6376280177947713, 0.5364671950603048, 0.5697194968937839, 0.6046270222808969, 0.19828487166059172, 0.20677602317736232, 0.20952700098802524, 0.17504352135657608, 0.1872149123248671, 0.18058125767784716, 0.18363327923768158, 0.1734550494379451, 0.1811900681761487, 0.19638412069592814, 0.1784610582440399, 0.1785635020273718, 0.08375222723055509, 0.08554103552116354, 0.07667496226636883, 0.08154009881601743, 0.07207178993966012, 0.06753014772800747, 0.09178354426194113, 0.08273888202466828, 0.08214048838785781]}, "mutation_prompt": null}
{"id": "c50c1eb1-0405-43c8-98ed-19fbba123c45", "solution": "import numpy as np\n\nclass HybridStochasticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20  # could be tuned\n        self.scale_factor = 0.8\n        self.crossover_prob = 0.7\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n    \n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:  # If not evaluated yet\n                self.fitness[i] = func(self.population[i])\n                self.evaluations += 1\n    \n    def select_parents(self, idx):\n        indices = [i for i in range(self.population_size) if i != idx]\n        return np.random.choice(indices, 3, replace=False)\n    \n    def differential_evolution_mutation(self, idx):\n        a, b, c = self.select_parents(idx)\n        mutant_vector = self.population[a] + self.scale_factor * (self.population[b] - self.population[c])\n        return np.clip(mutant_vector, self.lower_bound, self.upper_bound)\n    \n    def crossover(self, target_vector, mutant_vector):\n        crossover_mask = np.random.rand(self.dim) < self.crossover_prob\n        return np.where(crossover_mask, mutant_vector, target_vector)\n    \n    def local_search(self, vector):\n        perturbation = np.random.normal(0, 0.1, self.dim)  # Small Gaussian perturbations\n        candidate_vector = vector + perturbation\n        return np.clip(candidate_vector, self.lower_bound, self.upper_bound)\n    \n    def __call__(self, func):\n        self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n                \n                mutant_vector = self.differential_evolution_mutation(i)\n                trial_vector = self.crossover(self.population[i], mutant_vector)\n                trial_fitness = func(trial_vector)\n                self.evaluations += 1\n                \n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial_vector\n                    self.fitness[i] = trial_fitness\n                \n                # Adaptive Local Search\n                if np.random.rand() < 0.2:  # Probability of local search\n                    candidate_vector = self.local_search(self.population[i])\n                    candidate_fitness = func(candidate_vector)\n                    self.evaluations += 1\n                    if candidate_fitness < self.fitness[i]:\n                        self.population[i] = candidate_vector\n                        self.fitness[i] = candidate_fitness\n        \n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx]", "name": "HybridStochasticOptimizer", "description": "A novel hybrid stochastic optimization algorithm combining Differential Evolution with Adaptive Local Search for efficient exploration and exploitation in high-dimensional spaces.", "configspace": "", "generation": 1, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "2778f81b-4fe3-49b5-ac9f-5e39c1e0f4f7", "metadata": {"aucs": [0.838550702247986, 0.8335387980441803, 0.8392883035317981, 0.8448220232438199, 0.8692280794153219, 0.86492167545159, 0.8423065034848981, 0.8684684289902511, 0.8603227149202752, 0.7334709241625984, 0.7334474051410083, 0.7264747402639247, 0.7344716495540463, 0.7333723373934493, 0.7400257469840834, 0.7326254850725233, 0.7347331739693774, 0.7361787942160427, 0.2900640312488526, 0.47241555583359396, 0.4068320718407943, 0.38522901052495495, 0.4980851072709567, 0.36017996570170985, 0.3855858146223492, 0.5301915371647596, 0.45367220095702276, 0.15700893469067034, 0.15086174717667622, 0.3606580261151082, 0.3820765391627686, 0.37857498203107787, 0.24358195595397236, 0.44006871328822617, 0.15619687169305285, 0.5431907915884118, 0.9514039641368535, 0.9599028786233561, 0.9703152109391081, 0.9689783778060999, 0.9634907937987585, 0.977053323860018, 0.9713025537743715, 0.9710944937497286, 0.9623817898603562, 0.4811632069641354, 0.4861494717101681, 0.5181873123716814, 0.4868754422257876, 0.5064595460782774, 0.5046714079907575, 0.48795856678445426, 0.5189360989349037, 0.4807572622639553, 0.5868371238845421, 0.6828198726555201, 0.6456891282317958, 0.5918970855007204, 0.6200859298334032, 0.7702265673351774, 0.6958432356063231, 0.7340731017036689, 0.7310117434316175, 0.2595202684690293, 0.2665440796289047, 0.3447806314120041, 0.2259147785649217, 0.27326843203813556, 0.32150330600063937, 0.28010562289764906, 0.29552641888490006, 0.32221432565738917, 0.12522923128088514, 0.21777955819361272, 0.14790043362950323, 0.27777879274959094, 0.2221290906104053, 0.2546698244616422, 0.28777697821894743, 0.25863731044319516, 0.20839151152332214, 0.08200045947449175, 0.1233998060896413, 0.09590295126948145, 0.11121731514540156, 0.08626097600496452, 0.10894404364206134, 0.08859205077834265, 0.08679856412127096, 0.07606669628336682, 0.21252808557164604, 0.22412982702821604, 0.20390446043916832, 0.22534486281182287, 0.16119227720619456, 0.17904454354061838, 0.21567655887183101, 0.3290280255186192, 0.2655490022975723, 0.00656682574070222, 0.030575438199327065, 0.03955418050983606, 0.02393376066661479, 0.05524215352879602, 0.07951077146365404, 0.03032444160772796, 0.08778729249895956, 0.07737778818842145, 0.19516566745890707, 0.1937347077828694, 0.19411490582830215, 0.21154639947615128, 0.20856128496804915, 0.20410401494498287, 0.18879954051159387, 0.188381415941566, 0.1647126038437029, 0.5421754637547482, 0.5695984805554837, 0.5600632409189952, 0.5552469615873223, 0.5724375125380385, 0.5565735237775709, 0.5734271428993263, 0.5412097018577038, 0.5590617205170056, 0.11818203894987733, 0.11594934899816578, 0.11548728812285303, 0.10910969597536346, 0.09324766404200946, 0.11297884701366567, 0.11728542093218253, 0.1358283894323311, 0.11510974598355384, 0.22540283983726417, 0.17103914371889317, 0.24935010026333593, 0.23139708991581187, 0.2116834840101205, 0.19135882098285617, 0.1705371299341023, 0.2126699572935572, 0.2209012418595554, 0.32685385154538005, 0.34314356484347186, 0.36988756800192024, 0.36952191550610336, 0.375443089514364, 0.2927272068359621, 0.41490454614332994, 0.4348893987389886, 0.3834987498697635, 0.23366535767951468, 0.25866189725809885, 0.2302530339437413, 0.22554822145020648, 0.24019648694644558, 0.21306175836868801, 0.23819888206020323, 0.27857543630058956, 0.25651404700229974, 0.18477259167128046, 0.19426795408013386, 0.19595086844004206, 0.20717051734986014, 0.19752325636984547, 0.22049037243377545, 0.19758769044907198, 0.2250685195540405, 0.1757806127460937, 0.2890409684568702, 0.2535622220515875, 0.40789869201342577, 0.36433021362110163, 0.43665556031431274, 0.20627895553091957, 0.2585961925724303, 0.43201072946856545, 0.21213576965002445, 0.7592369689589549, 0.2000334564012084, 0.19557859121926469, 0.7296203247163067, 0.18252932373457764, 0.6585988508625994, 0.6557618619990195, 0.17668310156736222, 0.4550001636102414, 0.1977316349668934, 0.20570403192958386, 0.6376280177947713, 0.5364671950603048, 0.5697194968937839, 0.6046270222808969, 0.19828487166059172, 0.20677602317736232, 0.20952700098802524, 0.17504352135657608, 0.1872149123248671, 0.18058125767784716, 0.18363327923768158, 0.1734550494379451, 0.1811900681761487, 0.19638412069592814, 0.1784610582440399, 0.1785635020273718, 0.08375222723055509, 0.08554103552116354, 0.07667496226636883, 0.08154009881601743, 0.07207178993966012, 0.06753014772800747, 0.09178354426194113, 0.08273888202466828, 0.08214048838785781]}, "mutation_prompt": null}
{"id": "b06a9ef6-5387-4e00-83cf-06348f078bfe", "solution": "import numpy as np\n\nclass HybridStochasticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20  # could be tuned\n        self.scale_factor = 0.8\n        self.crossover_prob = 0.7\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n    \n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:  # If not evaluated yet\n                self.fitness[i] = func(self.population[i])\n                self.evaluations += 1\n    \n    def select_parents(self, idx):\n        indices = [i for i in range(self.population_size) if i != idx]\n        return np.random.choice(indices, 3, replace=False)\n    \n    def differential_evolution_mutation(self, idx):\n        a, b, c = self.select_parents(idx)\n        mutant_vector = self.population[a] + self.scale_factor * (self.population[b] - self.population[c])\n        return np.clip(mutant_vector, self.lower_bound, self.upper_bound)\n    \n    def crossover(self, target_vector, mutant_vector):\n        crossover_mask = np.random.rand(self.dim) < self.crossover_prob\n        return np.where(crossover_mask, mutant_vector, target_vector)\n    \n    def local_search(self, vector):\n        perturbation = np.random.normal(0, 0.1, self.dim)  # Small Gaussian perturbations\n        candidate_vector = vector + perturbation\n        return np.clip(candidate_vector, self.lower_bound, self.upper_bound)\n    \n    def __call__(self, func):\n        self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n                \n                mutant_vector = self.differential_evolution_mutation(i)\n                trial_vector = self.crossover(self.population[i], mutant_vector)\n                trial_fitness = func(trial_vector)\n                self.evaluations += 1\n                \n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial_vector\n                    self.fitness[i] = trial_fitness\n                \n                # Adaptive Local Search\n                if np.random.rand() < 0.2:  # Probability of local search\n                    candidate_vector = self.local_search(self.population[i])\n                    candidate_fitness = func(candidate_vector)\n                    self.evaluations += 1\n                    if candidate_fitness < self.fitness[i]:\n                        self.population[i] = candidate_vector\n                        self.fitness[i] = candidate_fitness\n        \n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx]", "name": "HybridStochasticOptimizer", "description": "A novel hybrid stochastic optimization algorithm combining Differential Evolution with Adaptive Local Search for efficient exploration and exploitation in high-dimensional spaces.", "configspace": "", "generation": 1, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "2778f81b-4fe3-49b5-ac9f-5e39c1e0f4f7", "metadata": {"aucs": [0.838550702247986, 0.8335387980441803, 0.8392883035317981, 0.8448220232438199, 0.8692280794153219, 0.86492167545159, 0.8423065034848981, 0.8684684289902511, 0.8603227149202752, 0.7334709241625984, 0.7334474051410083, 0.7264747402639247, 0.7344716495540463, 0.7333723373934493, 0.7400257469840834, 0.7326254850725233, 0.7347331739693774, 0.7361787942160427, 0.2900640312488526, 0.47241555583359396, 0.4068320718407943, 0.38522901052495495, 0.4980851072709567, 0.36017996570170985, 0.3855858146223492, 0.5301915371647596, 0.45367220095702276, 0.15700893469067034, 0.15086174717667622, 0.3606580261151082, 0.3820765391627686, 0.37857498203107787, 0.24358195595397236, 0.44006871328822617, 0.15619687169305285, 0.5431907915884118, 0.9514039641368535, 0.9599028786233561, 0.9703152109391081, 0.9689783778060999, 0.9634907937987585, 0.977053323860018, 0.9713025537743715, 0.9710944937497286, 0.9623817898603562, 0.4811632069641354, 0.4861494717101681, 0.5181873123716814, 0.4868754422257876, 0.5064595460782774, 0.5046714079907575, 0.48795856678445426, 0.5189360989349037, 0.4807572622639553, 0.5868371238845421, 0.6828198726555201, 0.6456891282317958, 0.5918970855007204, 0.6200859298334032, 0.7702265673351774, 0.6958432356063231, 0.7340731017036689, 0.7310117434316175, 0.2595202684690293, 0.2665440796289047, 0.3447806314120041, 0.2259147785649217, 0.27326843203813556, 0.32150330600063937, 0.28010562289764906, 0.29552641888490006, 0.32221432565738917, 0.12522923128088514, 0.21777955819361272, 0.14790043362950323, 0.27777879274959094, 0.2221290906104053, 0.2546698244616422, 0.28777697821894743, 0.25863731044319516, 0.20839151152332214, 0.08200045947449175, 0.1233998060896413, 0.09590295126948145, 0.11121731514540156, 0.08626097600496452, 0.10894404364206134, 0.08859205077834265, 0.08679856412127096, 0.07606669628336682, 0.21252808557164604, 0.22412982702821604, 0.20390446043916832, 0.22534486281182287, 0.16119227720619456, 0.17904454354061838, 0.21567655887183101, 0.3290280255186192, 0.2655490022975723, 0.00656682574070222, 0.030575438199327065, 0.03955418050983606, 0.02393376066661479, 0.05524215352879602, 0.07951077146365404, 0.03032444160772796, 0.08778729249895956, 0.07737778818842145, 0.19516566745890707, 0.1937347077828694, 0.19411490582830215, 0.21154639947615128, 0.20856128496804915, 0.20410401494498287, 0.18879954051159387, 0.188381415941566, 0.1647126038437029, 0.5421754637547482, 0.5695984805554837, 0.5600632409189952, 0.5552469615873223, 0.5724375125380385, 0.5565735237775709, 0.5734271428993263, 0.5412097018577038, 0.5590617205170056, 0.11818203894987733, 0.11594934899816578, 0.11548728812285303, 0.10910969597536346, 0.09324766404200946, 0.11297884701366567, 0.11728542093218253, 0.1358283894323311, 0.11510974598355384, 0.22540283983726417, 0.17103914371889317, 0.24935010026333593, 0.23139708991581187, 0.2116834840101205, 0.19135882098285617, 0.1705371299341023, 0.2126699572935572, 0.2209012418595554, 0.32685385154538005, 0.34314356484347186, 0.36988756800192024, 0.36952191550610336, 0.375443089514364, 0.2927272068359621, 0.41490454614332994, 0.4348893987389886, 0.3834987498697635, 0.23366535767951468, 0.25866189725809885, 0.2302530339437413, 0.22554822145020648, 0.24019648694644558, 0.21306175836868801, 0.23819888206020323, 0.27857543630058956, 0.25651404700229974, 0.18477259167128046, 0.19426795408013386, 0.19595086844004206, 0.20717051734986014, 0.19752325636984547, 0.22049037243377545, 0.19758769044907198, 0.2250685195540405, 0.1757806127460937, 0.2890409684568702, 0.2535622220515875, 0.40789869201342577, 0.36433021362110163, 0.43665556031431274, 0.20627895553091957, 0.2585961925724303, 0.43201072946856545, 0.21213576965002445, 0.7592369689589549, 0.2000334564012084, 0.19557859121926469, 0.7296203247163067, 0.18252932373457764, 0.6585988508625994, 0.6557618619990195, 0.17668310156736222, 0.4550001636102414, 0.1977316349668934, 0.20570403192958386, 0.6376280177947713, 0.5364671950603048, 0.5697194968937839, 0.6046270222808969, 0.19828487166059172, 0.20677602317736232, 0.20952700098802524, 0.17504352135657608, 0.1872149123248671, 0.18058125767784716, 0.18363327923768158, 0.1734550494379451, 0.1811900681761487, 0.19638412069592814, 0.1784610582440399, 0.1785635020273718, 0.08375222723055509, 0.08554103552116354, 0.07667496226636883, 0.08154009881601743, 0.07207178993966012, 0.06753014772800747, 0.09178354426194113, 0.08273888202466828, 0.08214048838785781]}, "mutation_prompt": null}
{"id": "354ae229-0ba1-4aae-8324-36db60d192b2", "solution": "import numpy as np\n\nclass HybridStochasticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20  # could be tuned\n        self.scale_factor = 0.8\n        self.crossover_prob = 0.7\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n    \n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:  # If not evaluated yet\n                self.fitness[i] = func(self.population[i])\n                self.evaluations += 1\n    \n    def select_parents(self, idx):\n        indices = [i for i in range(self.population_size) if i != idx]\n        return np.random.choice(indices, 3, replace=False)\n    \n    def differential_evolution_mutation(self, idx):\n        a, b, c = self.select_parents(idx)\n        mutant_vector = self.population[a] + self.scale_factor * (self.population[b] - self.population[c])\n        return np.clip(mutant_vector, self.lower_bound, self.upper_bound)\n    \n    def crossover(self, target_vector, mutant_vector):\n        crossover_mask = np.random.rand(self.dim) < self.crossover_prob\n        return np.where(crossover_mask, mutant_vector, target_vector)\n    \n    def local_search(self, vector):\n        perturbation = np.random.normal(0, 0.1, self.dim)  # Small Gaussian perturbations\n        candidate_vector = vector + perturbation\n        return np.clip(candidate_vector, self.lower_bound, self.upper_bound)\n    \n    def __call__(self, func):\n        self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n                \n                mutant_vector = self.differential_evolution_mutation(i)\n                trial_vector = self.crossover(self.population[i], mutant_vector)\n                trial_fitness = func(trial_vector)\n                self.evaluations += 1\n                \n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial_vector\n                    self.fitness[i] = trial_fitness\n                \n                # Adaptive Local Search\n                if np.random.rand() < 0.2:  # Probability of local search\n                    candidate_vector = self.local_search(self.population[i])\n                    candidate_fitness = func(candidate_vector)\n                    self.evaluations += 1\n                    if candidate_fitness < self.fitness[i]:\n                        self.population[i] = candidate_vector\n                        self.fitness[i] = candidate_fitness\n        \n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx]", "name": "HybridStochasticOptimizer", "description": "A novel hybrid stochastic optimization algorithm combining Differential Evolution with Adaptive Local Search for efficient exploration and exploitation in high-dimensional spaces.", "configspace": "", "generation": 1, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "2778f81b-4fe3-49b5-ac9f-5e39c1e0f4f7", "metadata": {"aucs": [0.838550702247986, 0.8335387980441803, 0.8392883035317981, 0.8448220232438199, 0.8692280794153219, 0.86492167545159, 0.8423065034848981, 0.8684684289902511, 0.8603227149202752, 0.7334709241625984, 0.7334474051410083, 0.7264747402639247, 0.7344716495540463, 0.7333723373934493, 0.7400257469840834, 0.7326254850725233, 0.7347331739693774, 0.7361787942160427, 0.2900640312488526, 0.47241555583359396, 0.4068320718407943, 0.38522901052495495, 0.4980851072709567, 0.36017996570170985, 0.3855858146223492, 0.5301915371647596, 0.45367220095702276, 0.15700893469067034, 0.15086174717667622, 0.3606580261151082, 0.3820765391627686, 0.37857498203107787, 0.24358195595397236, 0.44006871328822617, 0.15619687169305285, 0.5431907915884118, 0.9514039641368535, 0.9599028786233561, 0.9703152109391081, 0.9689783778060999, 0.9634907937987585, 0.977053323860018, 0.9713025537743715, 0.9710944937497286, 0.9623817898603562, 0.4811632069641354, 0.4861494717101681, 0.5181873123716814, 0.4868754422257876, 0.5064595460782774, 0.5046714079907575, 0.48795856678445426, 0.5189360989349037, 0.4807572622639553, 0.5868371238845421, 0.6828198726555201, 0.6456891282317958, 0.5918970855007204, 0.6200859298334032, 0.7702265673351774, 0.6958432356063231, 0.7340731017036689, 0.7310117434316175, 0.2595202684690293, 0.2665440796289047, 0.3447806314120041, 0.2259147785649217, 0.27326843203813556, 0.32150330600063937, 0.28010562289764906, 0.29552641888490006, 0.32221432565738917, 0.12522923128088514, 0.21777955819361272, 0.14790043362950323, 0.27777879274959094, 0.2221290906104053, 0.2546698244616422, 0.28777697821894743, 0.25863731044319516, 0.20839151152332214, 0.08200045947449175, 0.1233998060896413, 0.09590295126948145, 0.11121731514540156, 0.08626097600496452, 0.10894404364206134, 0.08859205077834265, 0.08679856412127096, 0.07606669628336682, 0.21252808557164604, 0.22412982702821604, 0.20390446043916832, 0.22534486281182287, 0.16119227720619456, 0.17904454354061838, 0.21567655887183101, 0.3290280255186192, 0.2655490022975723, 0.00656682574070222, 0.030575438199327065, 0.03955418050983606, 0.02393376066661479, 0.05524215352879602, 0.07951077146365404, 0.03032444160772796, 0.08778729249895956, 0.07737778818842145, 0.19516566745890707, 0.1937347077828694, 0.19411490582830215, 0.21154639947615128, 0.20856128496804915, 0.20410401494498287, 0.18879954051159387, 0.188381415941566, 0.1647126038437029, 0.5421754637547482, 0.5695984805554837, 0.5600632409189952, 0.5552469615873223, 0.5724375125380385, 0.5565735237775709, 0.5734271428993263, 0.5412097018577038, 0.5590617205170056, 0.11818203894987733, 0.11594934899816578, 0.11548728812285303, 0.10910969597536346, 0.09324766404200946, 0.11297884701366567, 0.11728542093218253, 0.1358283894323311, 0.11510974598355384, 0.22540283983726417, 0.17103914371889317, 0.24935010026333593, 0.23139708991581187, 0.2116834840101205, 0.19135882098285617, 0.1705371299341023, 0.2126699572935572, 0.2209012418595554, 0.32685385154538005, 0.34314356484347186, 0.36988756800192024, 0.36952191550610336, 0.375443089514364, 0.2927272068359621, 0.41490454614332994, 0.4348893987389886, 0.3834987498697635, 0.23366535767951468, 0.25866189725809885, 0.2302530339437413, 0.22554822145020648, 0.24019648694644558, 0.21306175836868801, 0.23819888206020323, 0.27857543630058956, 0.25651404700229974, 0.18477259167128046, 0.19426795408013386, 0.19595086844004206, 0.20717051734986014, 0.19752325636984547, 0.22049037243377545, 0.19758769044907198, 0.2250685195540405, 0.1757806127460937, 0.2890409684568702, 0.2535622220515875, 0.40789869201342577, 0.36433021362110163, 0.43665556031431274, 0.20627895553091957, 0.2585961925724303, 0.43201072946856545, 0.21213576965002445, 0.7592369689589549, 0.2000334564012084, 0.19557859121926469, 0.7296203247163067, 0.18252932373457764, 0.6585988508625994, 0.6557618619990195, 0.17668310156736222, 0.4550001636102414, 0.1977316349668934, 0.20570403192958386, 0.6376280177947713, 0.5364671950603048, 0.5697194968937839, 0.6046270222808969, 0.19828487166059172, 0.20677602317736232, 0.20952700098802524, 0.17504352135657608, 0.1872149123248671, 0.18058125767784716, 0.18363327923768158, 0.1734550494379451, 0.1811900681761487, 0.19638412069592814, 0.1784610582440399, 0.1785635020273718, 0.08375222723055509, 0.08554103552116354, 0.07667496226636883, 0.08154009881601743, 0.07207178993966012, 0.06753014772800747, 0.09178354426194113, 0.08273888202466828, 0.08214048838785781]}, "mutation_prompt": null}
{"id": "7360b8cd-928e-4666-83a9-4b71556435e2", "solution": "import numpy as np\n\nclass HybridStochasticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20  # could be tuned\n        self.scale_factor = 0.8\n        self.crossover_prob = 0.7\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n    \n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:  # If not evaluated yet\n                self.fitness[i] = func(self.population[i])\n                self.evaluations += 1\n    \n    def select_parents(self, idx):\n        indices = [i for i in range(self.population_size) if i != idx]\n        return np.random.choice(indices, 3, replace=False)\n    \n    def differential_evolution_mutation(self, idx):\n        a, b, c = self.select_parents(idx)\n        mutant_vector = self.population[a] + self.scale_factor * (self.population[b] - self.population[c])\n        return np.clip(mutant_vector, self.lower_bound, self.upper_bound)\n    \n    def crossover(self, target_vector, mutant_vector):\n        crossover_mask = np.random.rand(self.dim) < self.crossover_prob\n        return np.where(crossover_mask, mutant_vector, target_vector)\n    \n    def local_search(self, vector):\n        perturbation = np.random.normal(0, 0.1, self.dim)  # Small Gaussian perturbations\n        candidate_vector = vector + perturbation\n        return np.clip(candidate_vector, self.lower_bound, self.upper_bound)\n    \n    def __call__(self, func):\n        self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n                \n                mutant_vector = self.differential_evolution_mutation(i)\n                trial_vector = self.crossover(self.population[i], mutant_vector)\n                trial_fitness = func(trial_vector)\n                self.evaluations += 1\n                \n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial_vector\n                    self.fitness[i] = trial_fitness\n                \n                # Adaptive Local Search\n                if np.random.rand() < 0.2:  # Probability of local search\n                    candidate_vector = self.local_search(self.population[i])\n                    candidate_fitness = func(candidate_vector)\n                    self.evaluations += 1\n                    if candidate_fitness < self.fitness[i]:\n                        self.population[i] = candidate_vector\n                        self.fitness[i] = candidate_fitness\n        \n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx]", "name": "HybridStochasticOptimizer", "description": "A novel hybrid stochastic optimization algorithm combining Differential Evolution with Adaptive Local Search for efficient exploration and exploitation in high-dimensional spaces.", "configspace": "", "generation": 1, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "2778f81b-4fe3-49b5-ac9f-5e39c1e0f4f7", "metadata": {"aucs": [0.838550702247986, 0.8335387980441803, 0.8392883035317981, 0.8448220232438199, 0.8692280794153219, 0.86492167545159, 0.8423065034848981, 0.8684684289902511, 0.8603227149202752, 0.7334709241625984, 0.7334474051410083, 0.7264747402639247, 0.7344716495540463, 0.7333723373934493, 0.7400257469840834, 0.7326254850725233, 0.7347331739693774, 0.7361787942160427, 0.2900640312488526, 0.47241555583359396, 0.4068320718407943, 0.38522901052495495, 0.4980851072709567, 0.36017996570170985, 0.3855858146223492, 0.5301915371647596, 0.45367220095702276, 0.15700893469067034, 0.15086174717667622, 0.3606580261151082, 0.3820765391627686, 0.37857498203107787, 0.24358195595397236, 0.44006871328822617, 0.15619687169305285, 0.5431907915884118, 0.9514039641368535, 0.9599028786233561, 0.9703152109391081, 0.9689783778060999, 0.9634907937987585, 0.977053323860018, 0.9713025537743715, 0.9710944937497286, 0.9623817898603562, 0.4811632069641354, 0.4861494717101681, 0.5181873123716814, 0.4868754422257876, 0.5064595460782774, 0.5046714079907575, 0.48795856678445426, 0.5189360989349037, 0.4807572622639553, 0.5868371238845421, 0.6828198726555201, 0.6456891282317958, 0.5918970855007204, 0.6200859298334032, 0.7702265673351774, 0.6958432356063231, 0.7340731017036689, 0.7310117434316175, 0.2595202684690293, 0.2665440796289047, 0.3447806314120041, 0.2259147785649217, 0.27326843203813556, 0.32150330600063937, 0.28010562289764906, 0.29552641888490006, 0.32221432565738917, 0.12522923128088514, 0.21777955819361272, 0.14790043362950323, 0.27777879274959094, 0.2221290906104053, 0.2546698244616422, 0.28777697821894743, 0.25863731044319516, 0.20839151152332214, 0.08200045947449175, 0.1233998060896413, 0.09590295126948145, 0.11121731514540156, 0.08626097600496452, 0.10894404364206134, 0.08859205077834265, 0.08679856412127096, 0.07606669628336682, 0.21252808557164604, 0.22412982702821604, 0.20390446043916832, 0.22534486281182287, 0.16119227720619456, 0.17904454354061838, 0.21567655887183101, 0.3290280255186192, 0.2655490022975723, 0.00656682574070222, 0.030575438199327065, 0.03955418050983606, 0.02393376066661479, 0.05524215352879602, 0.07951077146365404, 0.03032444160772796, 0.08778729249895956, 0.07737778818842145, 0.19516566745890707, 0.1937347077828694, 0.19411490582830215, 0.21154639947615128, 0.20856128496804915, 0.20410401494498287, 0.18879954051159387, 0.188381415941566, 0.1647126038437029, 0.5421754637547482, 0.5695984805554837, 0.5600632409189952, 0.5552469615873223, 0.5724375125380385, 0.5565735237775709, 0.5734271428993263, 0.5412097018577038, 0.5590617205170056, 0.11818203894987733, 0.11594934899816578, 0.11548728812285303, 0.10910969597536346, 0.09324766404200946, 0.11297884701366567, 0.11728542093218253, 0.1358283894323311, 0.11510974598355384, 0.22540283983726417, 0.17103914371889317, 0.24935010026333593, 0.23139708991581187, 0.2116834840101205, 0.19135882098285617, 0.1705371299341023, 0.2126699572935572, 0.2209012418595554, 0.32685385154538005, 0.34314356484347186, 0.36988756800192024, 0.36952191550610336, 0.375443089514364, 0.2927272068359621, 0.41490454614332994, 0.4348893987389886, 0.3834987498697635, 0.23366535767951468, 0.25866189725809885, 0.2302530339437413, 0.22554822145020648, 0.24019648694644558, 0.21306175836868801, 0.23819888206020323, 0.27857543630058956, 0.25651404700229974, 0.18477259167128046, 0.19426795408013386, 0.19595086844004206, 0.20717051734986014, 0.19752325636984547, 0.22049037243377545, 0.19758769044907198, 0.2250685195540405, 0.1757806127460937, 0.2890409684568702, 0.2535622220515875, 0.40789869201342577, 0.36433021362110163, 0.43665556031431274, 0.20627895553091957, 0.2585961925724303, 0.43201072946856545, 0.21213576965002445, 0.7592369689589549, 0.2000334564012084, 0.19557859121926469, 0.7296203247163067, 0.18252932373457764, 0.6585988508625994, 0.6557618619990195, 0.17668310156736222, 0.4550001636102414, 0.1977316349668934, 0.20570403192958386, 0.6376280177947713, 0.5364671950603048, 0.5697194968937839, 0.6046270222808969, 0.19828487166059172, 0.20677602317736232, 0.20952700098802524, 0.17504352135657608, 0.1872149123248671, 0.18058125767784716, 0.18363327923768158, 0.1734550494379451, 0.1811900681761487, 0.19638412069592814, 0.1784610582440399, 0.1785635020273718, 0.08375222723055509, 0.08554103552116354, 0.07667496226636883, 0.08154009881601743, 0.07207178993966012, 0.06753014772800747, 0.09178354426194113, 0.08273888202466828, 0.08214048838785781]}, "mutation_prompt": null}
{"id": "20cd7b79-d4ea-49f7-ae85-97bd03265382", "solution": "import numpy as np\n\nclass HybridStochasticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20  # could be tuned\n        self.scale_factor = 0.8\n        self.crossover_prob = 0.7\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n    \n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:  # If not evaluated yet\n                self.fitness[i] = func(self.population[i])\n                self.evaluations += 1\n    \n    def select_parents(self, idx):\n        indices = [i for i in range(self.population_size) if i != idx]\n        return np.random.choice(indices, 3, replace=False)\n    \n    def differential_evolution_mutation(self, idx):\n        a, b, c = self.select_parents(idx)\n        mutant_vector = self.population[a] + self.scale_factor * (self.population[b] - self.population[c])\n        return np.clip(mutant_vector, self.lower_bound, self.upper_bound)\n    \n    def crossover(self, target_vector, mutant_vector):\n        crossover_mask = np.random.rand(self.dim) < self.crossover_prob\n        return np.where(crossover_mask, mutant_vector, target_vector)\n    \n    def local_search(self, vector):\n        perturbation = np.random.normal(0, 0.1, self.dim)  # Small Gaussian perturbations\n        candidate_vector = vector + perturbation\n        return np.clip(candidate_vector, self.lower_bound, self.upper_bound)\n    \n    def __call__(self, func):\n        self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n                \n                mutant_vector = self.differential_evolution_mutation(i)\n                trial_vector = self.crossover(self.population[i], mutant_vector)\n                trial_fitness = func(trial_vector)\n                self.evaluations += 1\n                \n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial_vector\n                    self.fitness[i] = trial_fitness\n                \n                # Adaptive Local Search\n                if np.random.rand() < 0.2:  # Probability of local search\n                    candidate_vector = self.local_search(self.population[i])\n                    candidate_fitness = func(candidate_vector)\n                    self.evaluations += 1\n                    if candidate_fitness < self.fitness[i]:\n                        self.population[i] = candidate_vector\n                        self.fitness[i] = candidate_fitness\n        \n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx]", "name": "HybridStochasticOptimizer", "description": "A novel hybrid stochastic optimization algorithm combining Differential Evolution with Adaptive Local Search for efficient exploration and exploitation in high-dimensional spaces.", "configspace": "", "generation": 1, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "2778f81b-4fe3-49b5-ac9f-5e39c1e0f4f7", "metadata": {"aucs": [0.838550702247986, 0.8335387980441803, 0.8392883035317981, 0.8448220232438199, 0.8692280794153219, 0.86492167545159, 0.8423065034848981, 0.8684684289902511, 0.8603227149202752, 0.7334709241625984, 0.7334474051410083, 0.7264747402639247, 0.7344716495540463, 0.7333723373934493, 0.7400257469840834, 0.7326254850725233, 0.7347331739693774, 0.7361787942160427, 0.2900640312488526, 0.47241555583359396, 0.4068320718407943, 0.38522901052495495, 0.4980851072709567, 0.36017996570170985, 0.3855858146223492, 0.5301915371647596, 0.45367220095702276, 0.15700893469067034, 0.15086174717667622, 0.3606580261151082, 0.3820765391627686, 0.37857498203107787, 0.24358195595397236, 0.44006871328822617, 0.15619687169305285, 0.5431907915884118, 0.9514039641368535, 0.9599028786233561, 0.9703152109391081, 0.9689783778060999, 0.9634907937987585, 0.977053323860018, 0.9713025537743715, 0.9710944937497286, 0.9623817898603562, 0.4811632069641354, 0.4861494717101681, 0.5181873123716814, 0.4868754422257876, 0.5064595460782774, 0.5046714079907575, 0.48795856678445426, 0.5189360989349037, 0.4807572622639553, 0.5868371238845421, 0.6828198726555201, 0.6456891282317958, 0.5918970855007204, 0.6200859298334032, 0.7702265673351774, 0.6958432356063231, 0.7340731017036689, 0.7310117434316175, 0.2595202684690293, 0.2665440796289047, 0.3447806314120041, 0.2259147785649217, 0.27326843203813556, 0.32150330600063937, 0.28010562289764906, 0.29552641888490006, 0.32221432565738917, 0.12522923128088514, 0.21777955819361272, 0.14790043362950323, 0.27777879274959094, 0.2221290906104053, 0.2546698244616422, 0.28777697821894743, 0.25863731044319516, 0.20839151152332214, 0.08200045947449175, 0.1233998060896413, 0.09590295126948145, 0.11121731514540156, 0.08626097600496452, 0.10894404364206134, 0.08859205077834265, 0.08679856412127096, 0.07606669628336682, 0.21252808557164604, 0.22412982702821604, 0.20390446043916832, 0.22534486281182287, 0.16119227720619456, 0.17904454354061838, 0.21567655887183101, 0.3290280255186192, 0.2655490022975723, 0.00656682574070222, 0.030575438199327065, 0.03955418050983606, 0.02393376066661479, 0.05524215352879602, 0.07951077146365404, 0.03032444160772796, 0.08778729249895956, 0.07737778818842145, 0.19516566745890707, 0.1937347077828694, 0.19411490582830215, 0.21154639947615128, 0.20856128496804915, 0.20410401494498287, 0.18879954051159387, 0.188381415941566, 0.1647126038437029, 0.5421754637547482, 0.5695984805554837, 0.5600632409189952, 0.5552469615873223, 0.5724375125380385, 0.5565735237775709, 0.5734271428993263, 0.5412097018577038, 0.5590617205170056, 0.11818203894987733, 0.11594934899816578, 0.11548728812285303, 0.10910969597536346, 0.09324766404200946, 0.11297884701366567, 0.11728542093218253, 0.1358283894323311, 0.11510974598355384, 0.22540283983726417, 0.17103914371889317, 0.24935010026333593, 0.23139708991581187, 0.2116834840101205, 0.19135882098285617, 0.1705371299341023, 0.2126699572935572, 0.2209012418595554, 0.32685385154538005, 0.34314356484347186, 0.36988756800192024, 0.36952191550610336, 0.375443089514364, 0.2927272068359621, 0.41490454614332994, 0.4348893987389886, 0.3834987498697635, 0.23366535767951468, 0.25866189725809885, 0.2302530339437413, 0.22554822145020648, 0.24019648694644558, 0.21306175836868801, 0.23819888206020323, 0.27857543630058956, 0.25651404700229974, 0.18477259167128046, 0.19426795408013386, 0.19595086844004206, 0.20717051734986014, 0.19752325636984547, 0.22049037243377545, 0.19758769044907198, 0.2250685195540405, 0.1757806127460937, 0.2890409684568702, 0.2535622220515875, 0.40789869201342577, 0.36433021362110163, 0.43665556031431274, 0.20627895553091957, 0.2585961925724303, 0.43201072946856545, 0.21213576965002445, 0.7592369689589549, 0.2000334564012084, 0.19557859121926469, 0.7296203247163067, 0.18252932373457764, 0.6585988508625994, 0.6557618619990195, 0.17668310156736222, 0.4550001636102414, 0.1977316349668934, 0.20570403192958386, 0.6376280177947713, 0.5364671950603048, 0.5697194968937839, 0.6046270222808969, 0.19828487166059172, 0.20677602317736232, 0.20952700098802524, 0.17504352135657608, 0.1872149123248671, 0.18058125767784716, 0.18363327923768158, 0.1734550494379451, 0.1811900681761487, 0.19638412069592814, 0.1784610582440399, 0.1785635020273718, 0.08375222723055509, 0.08554103552116354, 0.07667496226636883, 0.08154009881601743, 0.07207178993966012, 0.06753014772800747, 0.09178354426194113, 0.08273888202466828, 0.08214048838785781]}, "mutation_prompt": null}
{"id": "86383e83-3f19-48c2-943f-afde61e4b090", "solution": "import numpy as np\n\nclass HybridStochasticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20  # could be tuned\n        self.scale_factor = 0.8\n        self.crossover_prob = 0.7\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n    \n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:  # If not evaluated yet\n                self.fitness[i] = func(self.population[i])\n                self.evaluations += 1\n    \n    def select_parents(self, idx):\n        indices = [i for i in range(self.population_size) if i != idx]\n        return np.random.choice(indices, 3, replace=False)\n    \n    def differential_evolution_mutation(self, idx):\n        a, b, c = self.select_parents(idx)\n        mutant_vector = self.population[a] + self.scale_factor * (self.population[b] - self.population[c])\n        return np.clip(mutant_vector, self.lower_bound, self.upper_bound)\n    \n    def crossover(self, target_vector, mutant_vector):\n        crossover_mask = np.random.rand(self.dim) < self.crossover_prob\n        return np.where(crossover_mask, mutant_vector, target_vector)\n    \n    def local_search(self, vector):\n        perturbation = np.random.normal(0, 0.1, self.dim)  # Small Gaussian perturbations\n        candidate_vector = vector + perturbation\n        return np.clip(candidate_vector, self.lower_bound, self.upper_bound)\n    \n    def __call__(self, func):\n        self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n                \n                mutant_vector = self.differential_evolution_mutation(i)\n                trial_vector = self.crossover(self.population[i], mutant_vector)\n                trial_fitness = func(trial_vector)\n                self.evaluations += 1\n                \n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial_vector\n                    self.fitness[i] = trial_fitness\n                \n                # Adaptive Local Search\n                if np.random.rand() < 0.2:  # Probability of local search\n                    candidate_vector = self.local_search(self.population[i])\n                    candidate_fitness = func(candidate_vector)\n                    self.evaluations += 1\n                    if candidate_fitness < self.fitness[i]:\n                        self.population[i] = candidate_vector\n                        self.fitness[i] = candidate_fitness\n        \n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx]", "name": "HybridStochasticOptimizer", "description": "A novel hybrid stochastic optimization algorithm combining Differential Evolution with Adaptive Local Search for efficient exploration and exploitation in high-dimensional spaces.", "configspace": "", "generation": 1, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "2778f81b-4fe3-49b5-ac9f-5e39c1e0f4f7", "metadata": {"aucs": [0.838550702247986, 0.8335387980441803, 0.8392883035317981, 0.8448220232438199, 0.8692280794153219, 0.86492167545159, 0.8423065034848981, 0.8684684289902511, 0.8603227149202752, 0.7334709241625984, 0.7334474051410083, 0.7264747402639247, 0.7344716495540463, 0.7333723373934493, 0.7400257469840834, 0.7326254850725233, 0.7347331739693774, 0.7361787942160427, 0.2900640312488526, 0.47241555583359396, 0.4068320718407943, 0.38522901052495495, 0.4980851072709567, 0.36017996570170985, 0.3855858146223492, 0.5301915371647596, 0.45367220095702276, 0.15700893469067034, 0.15086174717667622, 0.3606580261151082, 0.3820765391627686, 0.37857498203107787, 0.24358195595397236, 0.44006871328822617, 0.15619687169305285, 0.5431907915884118, 0.9514039641368535, 0.9599028786233561, 0.9703152109391081, 0.9689783778060999, 0.9634907937987585, 0.977053323860018, 0.9713025537743715, 0.9710944937497286, 0.9623817898603562, 0.4811632069641354, 0.4861494717101681, 0.5181873123716814, 0.4868754422257876, 0.5064595460782774, 0.5046714079907575, 0.48795856678445426, 0.5189360989349037, 0.4807572622639553, 0.5868371238845421, 0.6828198726555201, 0.6456891282317958, 0.5918970855007204, 0.6200859298334032, 0.7702265673351774, 0.6958432356063231, 0.7340731017036689, 0.7310117434316175, 0.2595202684690293, 0.2665440796289047, 0.3447806314120041, 0.2259147785649217, 0.27326843203813556, 0.32150330600063937, 0.28010562289764906, 0.29552641888490006, 0.32221432565738917, 0.12522923128088514, 0.21777955819361272, 0.14790043362950323, 0.27777879274959094, 0.2221290906104053, 0.2546698244616422, 0.28777697821894743, 0.25863731044319516, 0.20839151152332214, 0.08200045947449175, 0.1233998060896413, 0.09590295126948145, 0.11121731514540156, 0.08626097600496452, 0.10894404364206134, 0.08859205077834265, 0.08679856412127096, 0.07606669628336682, 0.21252808557164604, 0.22412982702821604, 0.20390446043916832, 0.22534486281182287, 0.16119227720619456, 0.17904454354061838, 0.21567655887183101, 0.3290280255186192, 0.2655490022975723, 0.00656682574070222, 0.030575438199327065, 0.03955418050983606, 0.02393376066661479, 0.05524215352879602, 0.07951077146365404, 0.03032444160772796, 0.08778729249895956, 0.07737778818842145, 0.19516566745890707, 0.1937347077828694, 0.19411490582830215, 0.21154639947615128, 0.20856128496804915, 0.20410401494498287, 0.18879954051159387, 0.188381415941566, 0.1647126038437029, 0.5421754637547482, 0.5695984805554837, 0.5600632409189952, 0.5552469615873223, 0.5724375125380385, 0.5565735237775709, 0.5734271428993263, 0.5412097018577038, 0.5590617205170056, 0.11818203894987733, 0.11594934899816578, 0.11548728812285303, 0.10910969597536346, 0.09324766404200946, 0.11297884701366567, 0.11728542093218253, 0.1358283894323311, 0.11510974598355384, 0.22540283983726417, 0.17103914371889317, 0.24935010026333593, 0.23139708991581187, 0.2116834840101205, 0.19135882098285617, 0.1705371299341023, 0.2126699572935572, 0.2209012418595554, 0.32685385154538005, 0.34314356484347186, 0.36988756800192024, 0.36952191550610336, 0.375443089514364, 0.2927272068359621, 0.41490454614332994, 0.4348893987389886, 0.3834987498697635, 0.23366535767951468, 0.25866189725809885, 0.2302530339437413, 0.22554822145020648, 0.24019648694644558, 0.21306175836868801, 0.23819888206020323, 0.27857543630058956, 0.25651404700229974, 0.18477259167128046, 0.19426795408013386, 0.19595086844004206, 0.20717051734986014, 0.19752325636984547, 0.22049037243377545, 0.19758769044907198, 0.2250685195540405, 0.1757806127460937, 0.2890409684568702, 0.2535622220515875, 0.40789869201342577, 0.36433021362110163, 0.43665556031431274, 0.20627895553091957, 0.2585961925724303, 0.43201072946856545, 0.21213576965002445, 0.7592369689589549, 0.2000334564012084, 0.19557859121926469, 0.7296203247163067, 0.18252932373457764, 0.6585988508625994, 0.6557618619990195, 0.17668310156736222, 0.4550001636102414, 0.1977316349668934, 0.20570403192958386, 0.6376280177947713, 0.5364671950603048, 0.5697194968937839, 0.6046270222808969, 0.19828487166059172, 0.20677602317736232, 0.20952700098802524, 0.17504352135657608, 0.1872149123248671, 0.18058125767784716, 0.18363327923768158, 0.1734550494379451, 0.1811900681761487, 0.19638412069592814, 0.1784610582440399, 0.1785635020273718, 0.08375222723055509, 0.08554103552116354, 0.07667496226636883, 0.08154009881601743, 0.07207178993966012, 0.06753014772800747, 0.09178354426194113, 0.08273888202466828, 0.08214048838785781]}, "mutation_prompt": null}
{"id": "f0d6750c-4569-4a37-b9f1-c5eb999baedd", "solution": "import numpy as np\n\nclass HybridStochasticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20  # could be tuned\n        self.scale_factor = 0.8\n        self.crossover_prob = 0.7\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n    \n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:  # If not evaluated yet\n                self.fitness[i] = func(self.population[i])\n                self.evaluations += 1\n    \n    def select_parents(self, idx):\n        indices = [i for i in range(self.population_size) if i != idx]\n        return np.random.choice(indices, 3, replace=False)\n    \n    def differential_evolution_mutation(self, idx):\n        a, b, c = self.select_parents(idx)\n        mutant_vector = self.population[a] + self.scale_factor * (self.population[b] - self.population[c])\n        return np.clip(mutant_vector, self.lower_bound, self.upper_bound)\n    \n    def crossover(self, target_vector, mutant_vector):\n        crossover_mask = np.random.rand(self.dim) < self.crossover_prob\n        return np.where(crossover_mask, mutant_vector, target_vector)\n    \n    def local_search(self, vector):\n        perturbation = np.random.normal(0, 0.1, self.dim)  # Small Gaussian perturbations\n        candidate_vector = vector + perturbation\n        return np.clip(candidate_vector, self.lower_bound, self.upper_bound)\n    \n    def __call__(self, func):\n        self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n                \n                mutant_vector = self.differential_evolution_mutation(i)\n                trial_vector = self.crossover(self.population[i], mutant_vector)\n                trial_fitness = func(trial_vector)\n                self.evaluations += 1\n                \n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial_vector\n                    self.fitness[i] = trial_fitness\n                \n                # Adaptive Local Search\n                if np.random.rand() < 0.2:  # Probability of local search\n                    candidate_vector = self.local_search(self.population[i])\n                    candidate_fitness = func(candidate_vector)\n                    self.evaluations += 1\n                    if candidate_fitness < self.fitness[i]:\n                        self.population[i] = candidate_vector\n                        self.fitness[i] = candidate_fitness\n        \n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx]", "name": "HybridStochasticOptimizer", "description": "A novel hybrid stochastic optimization algorithm combining Differential Evolution with Adaptive Local Search for efficient exploration and exploitation in high-dimensional spaces.", "configspace": "", "generation": 1, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "2778f81b-4fe3-49b5-ac9f-5e39c1e0f4f7", "metadata": {"aucs": [0.838550702247986, 0.8335387980441803, 0.8392883035317981, 0.8448220232438199, 0.8692280794153219, 0.86492167545159, 0.8423065034848981, 0.8684684289902511, 0.8603227149202752, 0.7334709241625984, 0.7334474051410083, 0.7264747402639247, 0.7344716495540463, 0.7333723373934493, 0.7400257469840834, 0.7326254850725233, 0.7347331739693774, 0.7361787942160427, 0.2900640312488526, 0.47241555583359396, 0.4068320718407943, 0.38522901052495495, 0.4980851072709567, 0.36017996570170985, 0.3855858146223492, 0.5301915371647596, 0.45367220095702276, 0.15700893469067034, 0.15086174717667622, 0.3606580261151082, 0.3820765391627686, 0.37857498203107787, 0.24358195595397236, 0.44006871328822617, 0.15619687169305285, 0.5431907915884118, 0.9514039641368535, 0.9599028786233561, 0.9703152109391081, 0.9689783778060999, 0.9634907937987585, 0.977053323860018, 0.9713025537743715, 0.9710944937497286, 0.9623817898603562, 0.4811632069641354, 0.4861494717101681, 0.5181873123716814, 0.4868754422257876, 0.5064595460782774, 0.5046714079907575, 0.48795856678445426, 0.5189360989349037, 0.4807572622639553, 0.5868371238845421, 0.6828198726555201, 0.6456891282317958, 0.5918970855007204, 0.6200859298334032, 0.7702265673351774, 0.6958432356063231, 0.7340731017036689, 0.7310117434316175, 0.2595202684690293, 0.2665440796289047, 0.3447806314120041, 0.2259147785649217, 0.27326843203813556, 0.32150330600063937, 0.28010562289764906, 0.29552641888490006, 0.32221432565738917, 0.12522923128088514, 0.21777955819361272, 0.14790043362950323, 0.27777879274959094, 0.2221290906104053, 0.2546698244616422, 0.28777697821894743, 0.25863731044319516, 0.20839151152332214, 0.08200045947449175, 0.1233998060896413, 0.09590295126948145, 0.11121731514540156, 0.08626097600496452, 0.10894404364206134, 0.08859205077834265, 0.08679856412127096, 0.07606669628336682, 0.21252808557164604, 0.22412982702821604, 0.20390446043916832, 0.22534486281182287, 0.16119227720619456, 0.17904454354061838, 0.21567655887183101, 0.3290280255186192, 0.2655490022975723, 0.00656682574070222, 0.030575438199327065, 0.03955418050983606, 0.02393376066661479, 0.05524215352879602, 0.07951077146365404, 0.03032444160772796, 0.08778729249895956, 0.07737778818842145, 0.19516566745890707, 0.1937347077828694, 0.19411490582830215, 0.21154639947615128, 0.20856128496804915, 0.20410401494498287, 0.18879954051159387, 0.188381415941566, 0.1647126038437029, 0.5421754637547482, 0.5695984805554837, 0.5600632409189952, 0.5552469615873223, 0.5724375125380385, 0.5565735237775709, 0.5734271428993263, 0.5412097018577038, 0.5590617205170056, 0.11818203894987733, 0.11594934899816578, 0.11548728812285303, 0.10910969597536346, 0.09324766404200946, 0.11297884701366567, 0.11728542093218253, 0.1358283894323311, 0.11510974598355384, 0.22540283983726417, 0.17103914371889317, 0.24935010026333593, 0.23139708991581187, 0.2116834840101205, 0.19135882098285617, 0.1705371299341023, 0.2126699572935572, 0.2209012418595554, 0.32685385154538005, 0.34314356484347186, 0.36988756800192024, 0.36952191550610336, 0.375443089514364, 0.2927272068359621, 0.41490454614332994, 0.4348893987389886, 0.3834987498697635, 0.23366535767951468, 0.25866189725809885, 0.2302530339437413, 0.22554822145020648, 0.24019648694644558, 0.21306175836868801, 0.23819888206020323, 0.27857543630058956, 0.25651404700229974, 0.18477259167128046, 0.19426795408013386, 0.19595086844004206, 0.20717051734986014, 0.19752325636984547, 0.22049037243377545, 0.19758769044907198, 0.2250685195540405, 0.1757806127460937, 0.2890409684568702, 0.2535622220515875, 0.40789869201342577, 0.36433021362110163, 0.43665556031431274, 0.20627895553091957, 0.2585961925724303, 0.43201072946856545, 0.21213576965002445, 0.7592369689589549, 0.2000334564012084, 0.19557859121926469, 0.7296203247163067, 0.18252932373457764, 0.6585988508625994, 0.6557618619990195, 0.17668310156736222, 0.4550001636102414, 0.1977316349668934, 0.20570403192958386, 0.6376280177947713, 0.5364671950603048, 0.5697194968937839, 0.6046270222808969, 0.19828487166059172, 0.20677602317736232, 0.20952700098802524, 0.17504352135657608, 0.1872149123248671, 0.18058125767784716, 0.18363327923768158, 0.1734550494379451, 0.1811900681761487, 0.19638412069592814, 0.1784610582440399, 0.1785635020273718, 0.08375222723055509, 0.08554103552116354, 0.07667496226636883, 0.08154009881601743, 0.07207178993966012, 0.06753014772800747, 0.09178354426194113, 0.08273888202466828, 0.08214048838785781]}, "mutation_prompt": null}
{"id": "17ef32ee-c4f2-47b3-93fa-e197dd2a7e99", "solution": "import numpy as np\n\nclass HybridStochasticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20  # could be tuned\n        self.scale_factor = 0.8\n        self.crossover_prob = 0.7\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n    \n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:  # If not evaluated yet\n                self.fitness[i] = func(self.population[i])\n                self.evaluations += 1\n    \n    def select_parents(self, idx):\n        indices = [i for i in range(self.population_size) if i != idx]\n        return np.random.choice(indices, 3, replace=False)\n    \n    def differential_evolution_mutation(self, idx):\n        a, b, c = self.select_parents(idx)\n        mutant_vector = self.population[a] + self.scale_factor * (self.population[b] - self.population[c])\n        return np.clip(mutant_vector, self.lower_bound, self.upper_bound)\n    \n    def crossover(self, target_vector, mutant_vector):\n        crossover_mask = np.random.rand(self.dim) < self.crossover_prob\n        return np.where(crossover_mask, mutant_vector, target_vector)\n    \n    def local_search(self, vector):\n        perturbation = np.random.normal(0, 0.1, self.dim)  # Small Gaussian perturbations\n        candidate_vector = vector + perturbation\n        return np.clip(candidate_vector, self.lower_bound, self.upper_bound)\n    \n    def __call__(self, func):\n        self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n                \n                mutant_vector = self.differential_evolution_mutation(i)\n                trial_vector = self.crossover(self.population[i], mutant_vector)\n                trial_fitness = func(trial_vector)\n                self.evaluations += 1\n                \n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial_vector\n                    self.fitness[i] = trial_fitness\n                \n                # Adaptive Local Search\n                if np.random.rand() < 0.2:  # Probability of local search\n                    candidate_vector = self.local_search(self.population[i])\n                    candidate_fitness = func(candidate_vector)\n                    self.evaluations += 1\n                    if candidate_fitness < self.fitness[i]:\n                        self.population[i] = candidate_vector\n                        self.fitness[i] = candidate_fitness\n        \n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx]", "name": "HybridStochasticOptimizer", "description": "A novel hybrid stochastic optimization algorithm combining Differential Evolution with Adaptive Local Search for efficient exploration and exploitation in high-dimensional spaces.", "configspace": "", "generation": 1, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "2778f81b-4fe3-49b5-ac9f-5e39c1e0f4f7", "metadata": {"aucs": [0.838550702247986, 0.8335387980441803, 0.8392883035317981, 0.8448220232438199, 0.8692280794153219, 0.86492167545159, 0.8423065034848981, 0.8684684289902511, 0.8603227149202752, 0.7334709241625984, 0.7334474051410083, 0.7264747402639247, 0.7344716495540463, 0.7333723373934493, 0.7400257469840834, 0.7326254850725233, 0.7347331739693774, 0.7361787942160427, 0.2900640312488526, 0.47241555583359396, 0.4068320718407943, 0.38522901052495495, 0.4980851072709567, 0.36017996570170985, 0.3855858146223492, 0.5301915371647596, 0.45367220095702276, 0.15700893469067034, 0.15086174717667622, 0.3606580261151082, 0.3820765391627686, 0.37857498203107787, 0.24358195595397236, 0.44006871328822617, 0.15619687169305285, 0.5431907915884118, 0.9514039641368535, 0.9599028786233561, 0.9703152109391081, 0.9689783778060999, 0.9634907937987585, 0.977053323860018, 0.9713025537743715, 0.9710944937497286, 0.9623817898603562, 0.4811632069641354, 0.4861494717101681, 0.5181873123716814, 0.4868754422257876, 0.5064595460782774, 0.5046714079907575, 0.48795856678445426, 0.5189360989349037, 0.4807572622639553, 0.5868371238845421, 0.6828198726555201, 0.6456891282317958, 0.5918970855007204, 0.6200859298334032, 0.7702265673351774, 0.6958432356063231, 0.7340731017036689, 0.7310117434316175, 0.2595202684690293, 0.2665440796289047, 0.3447806314120041, 0.2259147785649217, 0.27326843203813556, 0.32150330600063937, 0.28010562289764906, 0.29552641888490006, 0.32221432565738917, 0.12522923128088514, 0.21777955819361272, 0.14790043362950323, 0.27777879274959094, 0.2221290906104053, 0.2546698244616422, 0.28777697821894743, 0.25863731044319516, 0.20839151152332214, 0.08200045947449175, 0.1233998060896413, 0.09590295126948145, 0.11121731514540156, 0.08626097600496452, 0.10894404364206134, 0.08859205077834265, 0.08679856412127096, 0.07606669628336682, 0.21252808557164604, 0.22412982702821604, 0.20390446043916832, 0.22534486281182287, 0.16119227720619456, 0.17904454354061838, 0.21567655887183101, 0.3290280255186192, 0.2655490022975723, 0.00656682574070222, 0.030575438199327065, 0.03955418050983606, 0.02393376066661479, 0.05524215352879602, 0.07951077146365404, 0.03032444160772796, 0.08778729249895956, 0.07737778818842145, 0.19516566745890707, 0.1937347077828694, 0.19411490582830215, 0.21154639947615128, 0.20856128496804915, 0.20410401494498287, 0.18879954051159387, 0.188381415941566, 0.1647126038437029, 0.5421754637547482, 0.5695984805554837, 0.5600632409189952, 0.5552469615873223, 0.5724375125380385, 0.5565735237775709, 0.5734271428993263, 0.5412097018577038, 0.5590617205170056, 0.11818203894987733, 0.11594934899816578, 0.11548728812285303, 0.10910969597536346, 0.09324766404200946, 0.11297884701366567, 0.11728542093218253, 0.1358283894323311, 0.11510974598355384, 0.22540283983726417, 0.17103914371889317, 0.24935010026333593, 0.23139708991581187, 0.2116834840101205, 0.19135882098285617, 0.1705371299341023, 0.2126699572935572, 0.2209012418595554, 0.32685385154538005, 0.34314356484347186, 0.36988756800192024, 0.36952191550610336, 0.375443089514364, 0.2927272068359621, 0.41490454614332994, 0.4348893987389886, 0.3834987498697635, 0.23366535767951468, 0.25866189725809885, 0.2302530339437413, 0.22554822145020648, 0.24019648694644558, 0.21306175836868801, 0.23819888206020323, 0.27857543630058956, 0.25651404700229974, 0.18477259167128046, 0.19426795408013386, 0.19595086844004206, 0.20717051734986014, 0.19752325636984547, 0.22049037243377545, 0.19758769044907198, 0.2250685195540405, 0.1757806127460937, 0.2890409684568702, 0.2535622220515875, 0.40789869201342577, 0.36433021362110163, 0.43665556031431274, 0.20627895553091957, 0.2585961925724303, 0.43201072946856545, 0.21213576965002445, 0.7592369689589549, 0.2000334564012084, 0.19557859121926469, 0.7296203247163067, 0.18252932373457764, 0.6585988508625994, 0.6557618619990195, 0.17668310156736222, 0.4550001636102414, 0.1977316349668934, 0.20570403192958386, 0.6376280177947713, 0.5364671950603048, 0.5697194968937839, 0.6046270222808969, 0.19828487166059172, 0.20677602317736232, 0.20952700098802524, 0.17504352135657608, 0.1872149123248671, 0.18058125767784716, 0.18363327923768158, 0.1734550494379451, 0.1811900681761487, 0.19638412069592814, 0.1784610582440399, 0.1785635020273718, 0.08375222723055509, 0.08554103552116354, 0.07667496226636883, 0.08154009881601743, 0.07207178993966012, 0.06753014772800747, 0.09178354426194113, 0.08273888202466828, 0.08214048838785781]}, "mutation_prompt": null}
{"id": "ac052079-c1df-4b61-9f90-d559139c9ec1", "solution": "import numpy as np\n\nclass HybridStochasticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 24  # Adjusted population size for better exploration\n        self.scale_factor = 0.85  # Slightly increased for more aggressive mutation\n        self.crossover_prob = 0.75  # Slightly increased crossover probability\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n    \n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.evaluations += 1\n    \n    def select_parents(self, idx):\n        indices = [i for i in range(self.population_size) if i != idx]\n        return np.random.choice(indices, 3, replace=False)\n    \n    def differential_evolution_mutation(self, idx):\n        a, b, c = self.select_parents(idx)\n        mutation_scale = self.scale_factor + 0.1 * (np.random.rand() - 0.5)  # Adaptive mutation\n        mutant_vector = self.population[a] + mutation_scale * (self.population[b] - self.population[c])\n        return np.clip(mutant_vector, self.lower_bound, self.upper_bound)\n    \n    def crossover(self, target_vector, mutant_vector):\n        crossover_mask = np.random.rand(self.dim) < self.crossover_prob\n        return np.where(crossover_mask, mutant_vector, target_vector)\n    \n    def local_search(self, vector):\n        perturbation = np.random.normal(0, 0.15, self.dim)  # More intense perturbations\n        candidate_vector = vector + perturbation\n        return np.clip(candidate_vector, self.lower_bound, self.upper_bound)\n    \n    def __call__(self, func):\n        self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n                \n                mutant_vector = self.differential_evolution_mutation(i)\n                trial_vector = self.crossover(self.population[i], mutant_vector)\n                trial_fitness = func(trial_vector)\n                self.evaluations += 1\n                \n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial_vector\n                    self.fitness[i] = trial_fitness\n                \n                if np.random.rand() < 0.25:  # Increased probability of local search\n                    candidate_vector = self.local_search(self.population[i])\n                    candidate_fitness = func(candidate_vector)\n                    self.evaluations += 1\n                    if candidate_fitness < self.fitness[i]:\n                        self.population[i] = candidate_vector\n                        self.fitness[i] = candidate_fitness\n        \n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx]", "name": "HybridStochasticOptimizer", "description": "Enhanced Hybrid Stochastic Optimizer with Adaptive Mutation and Selection Pressure for improved convergence speed.", "configspace": "", "generation": 9, "fitness": 0.32303487196555053, "feedback": "The algorithm HybridStochasticOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.32 with standard deviation 0.24.", "error": "", "parent_id": "2778f81b-4fe3-49b5-ac9f-5e39c1e0f4f7", "metadata": {"aucs": [0.7903706339602905, 0.7891855196591346, 0.7859643445719268, 0.788418092817518, 0.7852462133935287, 0.7824544421012976, 0.7995355934814229, 0.7957146236063743, 0.7797085470010588, 0.6233125329420883, 0.6290339055974995, 0.6618739516119136, 0.6143572857980442, 0.6240053708581986, 0.6369894852393441, 0.6360754677753033, 0.6409483143491363, 0.6387368955831421, 0.18144153979746802, 0.25027290656096235, 0.21074797872515583, 0.272928001505944, 0.2831023894743445, 0.18829064084075176, 0.22555744749022033, 0.28198992340725026, 0.2534594785315071, 0.130184470738677, 0.1993019494406516, 0.12959991438183405, 0.16632449126823134, 0.14513808543987483, 0.12792304337971094, 0.16143730669742873, 0.25671778683202073, 0.16389055754942783, 0.9843881924287631, 0.9718537156613376, 0.9606767452034042, 0.9663347902355149, 0.98021196827437, 0.9816144439834689, 0.9612662757884005, 0.968697449784625, 0.9704383874923627, 0.3711298045659708, 0.3985382908442584, 0.3806656136132701, 0.4004553953396389, 0.3753206099556188, 0.41913312771404676, 0.4111494618357062, 0.41418747128870026, 0.4072611053438878, 0.5238260447372078, 0.46852695113564047, 0.5887310360934734, 0.5768051124943198, 0.558577416865168, 0.6557448796962332, 0.5243125061280383, 0.614273213028877, 0.5709947115097211, 0.30658507484534125, 0.23570331898134156, 0.23864413098409587, 0.22934156293876118, 0.2750431385232186, 0.19662293045576296, 0.2311256916921861, 0.21699610288677318, 0.24457108516094073, 0.11727033958998667, 0.26381646157524385, 0.3519841776980801, 0.18797291306782515, 0.2019274757630285, 0.25694285592226174, 0.22669664205312479, 0.2097151416585441, 0.22843542115627902, 0.09182912307644076, 0.0747218601710834, 0.08982623742697693, 0.06536277745486718, 0.04346785478573456, 0.07926264169599728, 0.09091125914076215, 0.0874782386373496, 0.12901506083521652, 0.20869499473092135, 0.1901833081499702, 0.20801321626807823, 0.19303658417338088, 0.20714580704769125, 0.18075794506284493, 0.22458075559479562, 0.21926878831135144, 0.2528635172317726, 0.05226256728643963, 0.05630225140033274, 0.035266528035056255, 0.04188295358928462, 0.0390528572417177, 0.010629773264612563, 0.03530517978161518, 0.031418402486789065, 0.010455179406320347, 0.15053527702118774, 0.18428614736089388, 0.1793649298008071, 0.17713996646905739, 0.18040130649649178, 0.18085584802884114, 0.16640391859850956, 0.18021286622604749, 0.16166831629270784, 0.5251579371216055, 0.5360010093751095, 0.5239408279401578, 0.5255970252531557, 0.5277315723714362, 0.5296034275171202, 0.5201857318829097, 0.5542215217640386, 0.5414296733633357, 0.10563812958044905, 0.10980752434241836, 0.09259157620043701, 0.11397736170497952, 0.08880254401606469, 0.09287447706092866, 0.11337879807833506, 0.08961465359278586, 0.08988271218601673, 0.20535707325573227, 0.20038054277374984, 0.16401067877389763, 0.20677372901100588, 0.19125159830867067, 0.18329791771275927, 0.16002500148505627, 0.20067475026633919, 0.18682145249625293, 0.2893867980075272, 0.3084513320433695, 0.3198022261718887, 0.3223719565266824, 0.311760301232056, 0.2822151691204293, 0.33826704435645893, 0.32298565205069685, 0.32729629145838357, 0.21455822704943983, 0.22594969609025262, 0.22709139586143356, 0.21553150027033907, 0.23426605298558845, 0.22052253469271998, 0.24536519233267862, 0.2511970530179871, 0.2470297114357377, 0.17706157287727708, 0.18011626096563793, 0.18319258284558038, 0.19051230842412648, 0.2335309115527887, 0.18439097522938408, 0.20012088732307798, 0.19044693308170713, 0.20854495528450356, 0.229083912985666, 0.1952137986514384, 0.2273137738531461, 0.20783863852273865, 0.19346350270053358, 0.21077298709530046, 0.22788203886589498, 0.20989820126544878, 0.20664240979206017, 0.6527665713242046, 0.6799643676090085, 0.5926832775937588, 0.6329567963874589, 0.18933689320753744, 0.193208639723643, 0.4808802068650312, 0.5844952302154061, 0.5869141167020704, 0.5092155178299617, 0.20051122518390307, 0.20351897047624856, 0.3990620691456884, 0.43927419549125934, 0.5626553250556214, 0.6283407146018116, 0.7132056134322844, 0.6567263040711653, 0.1836421562126066, 0.17529876071147066, 0.21211480191404686, 0.1912348591271471, 0.18258108958548291, 0.1987878958521283, 0.18131053261940655, 0.17648345371565044, 0.20157889029303555, 0.07032762052216535, 0.0841200967924387, 0.07380903387455906, 0.0718179559021559, 0.0685809676710778, 0.07322306016610691, 0.07280503632648727, 0.07373554579199137, 0.07464796212471025]}, "mutation_prompt": null}
{"id": "01664f23-d26c-46b5-8923-e0ca0dc27ccf", "solution": "import numpy as np\n\nclass HybridStochasticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20  # could be tuned\n        self.scale_factor = 0.8\n        self.crossover_prob = 0.7\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n    \n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:  # If not evaluated yet\n                self.fitness[i] = func(self.population[i])\n                self.evaluations += 1\n    \n    def select_parents(self, idx):\n        indices = [i for i in range(self.population_size) if i != idx]\n        return np.random.choice(indices, 3, replace=False)\n    \n    def differential_evolution_mutation(self, idx):\n        a, b, c = self.select_parents(idx)\n        mutant_vector = self.population[a] + self.scale_factor * (self.population[b] - self.population[c])\n        return np.clip(mutant_vector, self.lower_bound, self.upper_bound)\n    \n    def crossover(self, target_vector, mutant_vector):\n        crossover_mask = np.random.rand(self.dim) < self.crossover_prob\n        return np.where(crossover_mask, mutant_vector, target_vector)\n    \n    def local_search(self, vector):\n        perturbation = np.random.normal(0, 0.1, self.dim)  # Small Gaussian perturbations\n        candidate_vector = vector + perturbation\n        return np.clip(candidate_vector, self.lower_bound, self.upper_bound)\n    \n    def __call__(self, func):\n        self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n                \n                mutant_vector = self.differential_evolution_mutation(i)\n                trial_vector = self.crossover(self.population[i], mutant_vector)\n                trial_fitness = func(trial_vector)\n                self.evaluations += 1\n                \n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial_vector\n                    self.fitness[i] = trial_fitness\n                \n                # Adaptive Local Search\n                if np.random.rand() < 0.2:  # Probability of local search\n                    candidate_vector = self.local_search(self.population[i])\n                    candidate_fitness = func(candidate_vector)\n                    self.evaluations += 1\n                    if candidate_fitness < self.fitness[i]:\n                        self.population[i] = candidate_vector\n                        self.fitness[i] = candidate_fitness\n        \n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx]", "name": "HybridStochasticOptimizer", "description": "A novel hybrid stochastic optimization algorithm combining Differential Evolution with Adaptive Local Search for efficient exploration and exploitation in high-dimensional spaces.", "configspace": "", "generation": 1, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "2778f81b-4fe3-49b5-ac9f-5e39c1e0f4f7", "metadata": {"aucs": [0.838550702247986, 0.8335387980441803, 0.8392883035317981, 0.8448220232438199, 0.8692280794153219, 0.86492167545159, 0.8423065034848981, 0.8684684289902511, 0.8603227149202752, 0.7334709241625984, 0.7334474051410083, 0.7264747402639247, 0.7344716495540463, 0.7333723373934493, 0.7400257469840834, 0.7326254850725233, 0.7347331739693774, 0.7361787942160427, 0.2900640312488526, 0.47241555583359396, 0.4068320718407943, 0.38522901052495495, 0.4980851072709567, 0.36017996570170985, 0.3855858146223492, 0.5301915371647596, 0.45367220095702276, 0.15700893469067034, 0.15086174717667622, 0.3606580261151082, 0.3820765391627686, 0.37857498203107787, 0.24358195595397236, 0.44006871328822617, 0.15619687169305285, 0.5431907915884118, 0.9514039641368535, 0.9599028786233561, 0.9703152109391081, 0.9689783778060999, 0.9634907937987585, 0.977053323860018, 0.9713025537743715, 0.9710944937497286, 0.9623817898603562, 0.4811632069641354, 0.4861494717101681, 0.5181873123716814, 0.4868754422257876, 0.5064595460782774, 0.5046714079907575, 0.48795856678445426, 0.5189360989349037, 0.4807572622639553, 0.5868371238845421, 0.6828198726555201, 0.6456891282317958, 0.5918970855007204, 0.6200859298334032, 0.7702265673351774, 0.6958432356063231, 0.7340731017036689, 0.7310117434316175, 0.2595202684690293, 0.2665440796289047, 0.3447806314120041, 0.2259147785649217, 0.27326843203813556, 0.32150330600063937, 0.28010562289764906, 0.29552641888490006, 0.32221432565738917, 0.12522923128088514, 0.21777955819361272, 0.14790043362950323, 0.27777879274959094, 0.2221290906104053, 0.2546698244616422, 0.28777697821894743, 0.25863731044319516, 0.20839151152332214, 0.08200045947449175, 0.1233998060896413, 0.09590295126948145, 0.11121731514540156, 0.08626097600496452, 0.10894404364206134, 0.08859205077834265, 0.08679856412127096, 0.07606669628336682, 0.21252808557164604, 0.22412982702821604, 0.20390446043916832, 0.22534486281182287, 0.16119227720619456, 0.17904454354061838, 0.21567655887183101, 0.3290280255186192, 0.2655490022975723, 0.00656682574070222, 0.030575438199327065, 0.03955418050983606, 0.02393376066661479, 0.05524215352879602, 0.07951077146365404, 0.03032444160772796, 0.08778729249895956, 0.07737778818842145, 0.19516566745890707, 0.1937347077828694, 0.19411490582830215, 0.21154639947615128, 0.20856128496804915, 0.20410401494498287, 0.18879954051159387, 0.188381415941566, 0.1647126038437029, 0.5421754637547482, 0.5695984805554837, 0.5600632409189952, 0.5552469615873223, 0.5724375125380385, 0.5565735237775709, 0.5734271428993263, 0.5412097018577038, 0.5590617205170056, 0.11818203894987733, 0.11594934899816578, 0.11548728812285303, 0.10910969597536346, 0.09324766404200946, 0.11297884701366567, 0.11728542093218253, 0.1358283894323311, 0.11510974598355384, 0.22540283983726417, 0.17103914371889317, 0.24935010026333593, 0.23139708991581187, 0.2116834840101205, 0.19135882098285617, 0.1705371299341023, 0.2126699572935572, 0.2209012418595554, 0.32685385154538005, 0.34314356484347186, 0.36988756800192024, 0.36952191550610336, 0.375443089514364, 0.2927272068359621, 0.41490454614332994, 0.4348893987389886, 0.3834987498697635, 0.23366535767951468, 0.25866189725809885, 0.2302530339437413, 0.22554822145020648, 0.24019648694644558, 0.21306175836868801, 0.23819888206020323, 0.27857543630058956, 0.25651404700229974, 0.18477259167128046, 0.19426795408013386, 0.19595086844004206, 0.20717051734986014, 0.19752325636984547, 0.22049037243377545, 0.19758769044907198, 0.2250685195540405, 0.1757806127460937, 0.2890409684568702, 0.2535622220515875, 0.40789869201342577, 0.36433021362110163, 0.43665556031431274, 0.20627895553091957, 0.2585961925724303, 0.43201072946856545, 0.21213576965002445, 0.7592369689589549, 0.2000334564012084, 0.19557859121926469, 0.7296203247163067, 0.18252932373457764, 0.6585988508625994, 0.6557618619990195, 0.17668310156736222, 0.4550001636102414, 0.1977316349668934, 0.20570403192958386, 0.6376280177947713, 0.5364671950603048, 0.5697194968937839, 0.6046270222808969, 0.19828487166059172, 0.20677602317736232, 0.20952700098802524, 0.17504352135657608, 0.1872149123248671, 0.18058125767784716, 0.18363327923768158, 0.1734550494379451, 0.1811900681761487, 0.19638412069592814, 0.1784610582440399, 0.1785635020273718, 0.08375222723055509, 0.08554103552116354, 0.07667496226636883, 0.08154009881601743, 0.07207178993966012, 0.06753014772800747, 0.09178354426194113, 0.08273888202466828, 0.08214048838785781]}, "mutation_prompt": null}
{"id": "9d93bec8-4e69-4a37-902f-e8ee5704ff5f", "solution": "import numpy as np\n\nclass HybridStochasticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20  # could be tuned\n        self.scale_factor = 0.8\n        self.crossover_prob = 0.7\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n    \n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:  # If not evaluated yet\n                self.fitness[i] = func(self.population[i])\n                self.evaluations += 1\n    \n    def select_parents(self, idx):\n        indices = [i for i in range(self.population_size) if i != idx]\n        return np.random.choice(indices, 3, replace=False)\n    \n    def differential_evolution_mutation(self, idx):\n        a, b, c = self.select_parents(idx)\n        mutant_vector = self.population[a] + self.scale_factor * (self.population[b] - self.population[c])\n        return np.clip(mutant_vector, self.lower_bound, self.upper_bound)\n    \n    def crossover(self, target_vector, mutant_vector):\n        crossover_mask = np.random.rand(self.dim) < self.crossover_prob\n        return np.where(crossover_mask, mutant_vector, target_vector)\n    \n    def local_search(self, vector):\n        perturbation = np.random.normal(0, 0.1, self.dim)  # Small Gaussian perturbations\n        candidate_vector = vector + perturbation\n        return np.clip(candidate_vector, self.lower_bound, self.upper_bound)\n    \n    def __call__(self, func):\n        self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n                \n                mutant_vector = self.differential_evolution_mutation(i)\n                trial_vector = self.crossover(self.population[i], mutant_vector)\n                trial_fitness = func(trial_vector)\n                self.evaluations += 1\n                \n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial_vector\n                    self.fitness[i] = trial_fitness\n                \n                # Adaptive Local Search\n                if np.random.rand() < 0.2:  # Probability of local search\n                    candidate_vector = self.local_search(self.population[i])\n                    candidate_fitness = func(candidate_vector)\n                    self.evaluations += 1\n                    if candidate_fitness < self.fitness[i]:\n                        self.population[i] = candidate_vector\n                        self.fitness[i] = candidate_fitness\n        \n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx]", "name": "HybridStochasticOptimizer", "description": "A novel hybrid stochastic optimization algorithm combining Differential Evolution with Adaptive Local Search for efficient exploration and exploitation in high-dimensional spaces.", "configspace": "", "generation": 1, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "2778f81b-4fe3-49b5-ac9f-5e39c1e0f4f7", "metadata": {"aucs": [0.838550702247986, 0.8335387980441803, 0.8392883035317981, 0.8448220232438199, 0.8692280794153219, 0.86492167545159, 0.8423065034848981, 0.8684684289902511, 0.8603227149202752, 0.7334709241625984, 0.7334474051410083, 0.7264747402639247, 0.7344716495540463, 0.7333723373934493, 0.7400257469840834, 0.7326254850725233, 0.7347331739693774, 0.7361787942160427, 0.2900640312488526, 0.47241555583359396, 0.4068320718407943, 0.38522901052495495, 0.4980851072709567, 0.36017996570170985, 0.3855858146223492, 0.5301915371647596, 0.45367220095702276, 0.15700893469067034, 0.15086174717667622, 0.3606580261151082, 0.3820765391627686, 0.37857498203107787, 0.24358195595397236, 0.44006871328822617, 0.15619687169305285, 0.5431907915884118, 0.9514039641368535, 0.9599028786233561, 0.9703152109391081, 0.9689783778060999, 0.9634907937987585, 0.977053323860018, 0.9713025537743715, 0.9710944937497286, 0.9623817898603562, 0.4811632069641354, 0.4861494717101681, 0.5181873123716814, 0.4868754422257876, 0.5064595460782774, 0.5046714079907575, 0.48795856678445426, 0.5189360989349037, 0.4807572622639553, 0.5868371238845421, 0.6828198726555201, 0.6456891282317958, 0.5918970855007204, 0.6200859298334032, 0.7702265673351774, 0.6958432356063231, 0.7340731017036689, 0.7310117434316175, 0.2595202684690293, 0.2665440796289047, 0.3447806314120041, 0.2259147785649217, 0.27326843203813556, 0.32150330600063937, 0.28010562289764906, 0.29552641888490006, 0.32221432565738917, 0.12522923128088514, 0.21777955819361272, 0.14790043362950323, 0.27777879274959094, 0.2221290906104053, 0.2546698244616422, 0.28777697821894743, 0.25863731044319516, 0.20839151152332214, 0.08200045947449175, 0.1233998060896413, 0.09590295126948145, 0.11121731514540156, 0.08626097600496452, 0.10894404364206134, 0.08859205077834265, 0.08679856412127096, 0.07606669628336682, 0.21252808557164604, 0.22412982702821604, 0.20390446043916832, 0.22534486281182287, 0.16119227720619456, 0.17904454354061838, 0.21567655887183101, 0.3290280255186192, 0.2655490022975723, 0.00656682574070222, 0.030575438199327065, 0.03955418050983606, 0.02393376066661479, 0.05524215352879602, 0.07951077146365404, 0.03032444160772796, 0.08778729249895956, 0.07737778818842145, 0.19516566745890707, 0.1937347077828694, 0.19411490582830215, 0.21154639947615128, 0.20856128496804915, 0.20410401494498287, 0.18879954051159387, 0.188381415941566, 0.1647126038437029, 0.5421754637547482, 0.5695984805554837, 0.5600632409189952, 0.5552469615873223, 0.5724375125380385, 0.5565735237775709, 0.5734271428993263, 0.5412097018577038, 0.5590617205170056, 0.11818203894987733, 0.11594934899816578, 0.11548728812285303, 0.10910969597536346, 0.09324766404200946, 0.11297884701366567, 0.11728542093218253, 0.1358283894323311, 0.11510974598355384, 0.22540283983726417, 0.17103914371889317, 0.24935010026333593, 0.23139708991581187, 0.2116834840101205, 0.19135882098285617, 0.1705371299341023, 0.2126699572935572, 0.2209012418595554, 0.32685385154538005, 0.34314356484347186, 0.36988756800192024, 0.36952191550610336, 0.375443089514364, 0.2927272068359621, 0.41490454614332994, 0.4348893987389886, 0.3834987498697635, 0.23366535767951468, 0.25866189725809885, 0.2302530339437413, 0.22554822145020648, 0.24019648694644558, 0.21306175836868801, 0.23819888206020323, 0.27857543630058956, 0.25651404700229974, 0.18477259167128046, 0.19426795408013386, 0.19595086844004206, 0.20717051734986014, 0.19752325636984547, 0.22049037243377545, 0.19758769044907198, 0.2250685195540405, 0.1757806127460937, 0.2890409684568702, 0.2535622220515875, 0.40789869201342577, 0.36433021362110163, 0.43665556031431274, 0.20627895553091957, 0.2585961925724303, 0.43201072946856545, 0.21213576965002445, 0.7592369689589549, 0.2000334564012084, 0.19557859121926469, 0.7296203247163067, 0.18252932373457764, 0.6585988508625994, 0.6557618619990195, 0.17668310156736222, 0.4550001636102414, 0.1977316349668934, 0.20570403192958386, 0.6376280177947713, 0.5364671950603048, 0.5697194968937839, 0.6046270222808969, 0.19828487166059172, 0.20677602317736232, 0.20952700098802524, 0.17504352135657608, 0.1872149123248671, 0.18058125767784716, 0.18363327923768158, 0.1734550494379451, 0.1811900681761487, 0.19638412069592814, 0.1784610582440399, 0.1785635020273718, 0.08375222723055509, 0.08554103552116354, 0.07667496226636883, 0.08154009881601743, 0.07207178993966012, 0.06753014772800747, 0.09178354426194113, 0.08273888202466828, 0.08214048838785781]}, "mutation_prompt": null}
{"id": "27772de7-e9cb-4a83-b5ba-927d005c23e3", "solution": "import numpy as np\n\nclass HybridStochasticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20  # could be tuned\n        self.scale_factor = 0.8\n        self.crossover_prob = 0.7\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n    \n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:  # If not evaluated yet\n                self.fitness[i] = func(self.population[i])\n                self.evaluations += 1\n    \n    def select_parents(self, idx):\n        indices = [i for i in range(self.population_size) if i != idx]\n        return np.random.choice(indices, 3, replace=False)\n    \n    def differential_evolution_mutation(self, idx):\n        a, b, c = self.select_parents(idx)\n        mutant_vector = self.population[a] + self.scale_factor * (self.population[b] - self.population[c])\n        return np.clip(mutant_vector, self.lower_bound, self.upper_bound)\n    \n    def crossover(self, target_vector, mutant_vector):\n        crossover_mask = np.random.rand(self.dim) < self.crossover_prob\n        return np.where(crossover_mask, mutant_vector, target_vector)\n    \n    def local_search(self, vector):\n        perturbation = np.random.normal(0, 0.1, self.dim)  # Small Gaussian perturbations\n        candidate_vector = vector + perturbation\n        return np.clip(candidate_vector, self.lower_bound, self.upper_bound)\n    \n    def __call__(self, func):\n        self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n                \n                mutant_vector = self.differential_evolution_mutation(i)\n                trial_vector = self.crossover(self.population[i], mutant_vector)\n                trial_fitness = func(trial_vector)\n                self.evaluations += 1\n                \n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial_vector\n                    self.fitness[i] = trial_fitness\n                \n                # Adaptive Local Search\n                if np.random.rand() < 0.2:  # Probability of local search\n                    candidate_vector = self.local_search(self.population[i])\n                    candidate_fitness = func(candidate_vector)\n                    self.evaluations += 1\n                    if candidate_fitness < self.fitness[i]:\n                        self.population[i] = candidate_vector\n                        self.fitness[i] = candidate_fitness\n        \n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx]", "name": "HybridStochasticOptimizer", "description": "A novel hybrid stochastic optimization algorithm combining Differential Evolution with Adaptive Local Search for efficient exploration and exploitation in high-dimensional spaces.", "configspace": "", "generation": 1, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "2778f81b-4fe3-49b5-ac9f-5e39c1e0f4f7", "metadata": {"aucs": [0.838550702247986, 0.8335387980441803, 0.8392883035317981, 0.8448220232438199, 0.8692280794153219, 0.86492167545159, 0.8423065034848981, 0.8684684289902511, 0.8603227149202752, 0.7334709241625984, 0.7334474051410083, 0.7264747402639247, 0.7344716495540463, 0.7333723373934493, 0.7400257469840834, 0.7326254850725233, 0.7347331739693774, 0.7361787942160427, 0.2900640312488526, 0.47241555583359396, 0.4068320718407943, 0.38522901052495495, 0.4980851072709567, 0.36017996570170985, 0.3855858146223492, 0.5301915371647596, 0.45367220095702276, 0.15700893469067034, 0.15086174717667622, 0.3606580261151082, 0.3820765391627686, 0.37857498203107787, 0.24358195595397236, 0.44006871328822617, 0.15619687169305285, 0.5431907915884118, 0.9514039641368535, 0.9599028786233561, 0.9703152109391081, 0.9689783778060999, 0.9634907937987585, 0.977053323860018, 0.9713025537743715, 0.9710944937497286, 0.9623817898603562, 0.4811632069641354, 0.4861494717101681, 0.5181873123716814, 0.4868754422257876, 0.5064595460782774, 0.5046714079907575, 0.48795856678445426, 0.5189360989349037, 0.4807572622639553, 0.5868371238845421, 0.6828198726555201, 0.6456891282317958, 0.5918970855007204, 0.6200859298334032, 0.7702265673351774, 0.6958432356063231, 0.7340731017036689, 0.7310117434316175, 0.2595202684690293, 0.2665440796289047, 0.3447806314120041, 0.2259147785649217, 0.27326843203813556, 0.32150330600063937, 0.28010562289764906, 0.29552641888490006, 0.32221432565738917, 0.12522923128088514, 0.21777955819361272, 0.14790043362950323, 0.27777879274959094, 0.2221290906104053, 0.2546698244616422, 0.28777697821894743, 0.25863731044319516, 0.20839151152332214, 0.08200045947449175, 0.1233998060896413, 0.09590295126948145, 0.11121731514540156, 0.08626097600496452, 0.10894404364206134, 0.08859205077834265, 0.08679856412127096, 0.07606669628336682, 0.21252808557164604, 0.22412982702821604, 0.20390446043916832, 0.22534486281182287, 0.16119227720619456, 0.17904454354061838, 0.21567655887183101, 0.3290280255186192, 0.2655490022975723, 0.00656682574070222, 0.030575438199327065, 0.03955418050983606, 0.02393376066661479, 0.05524215352879602, 0.07951077146365404, 0.03032444160772796, 0.08778729249895956, 0.07737778818842145, 0.19516566745890707, 0.1937347077828694, 0.19411490582830215, 0.21154639947615128, 0.20856128496804915, 0.20410401494498287, 0.18879954051159387, 0.188381415941566, 0.1647126038437029, 0.5421754637547482, 0.5695984805554837, 0.5600632409189952, 0.5552469615873223, 0.5724375125380385, 0.5565735237775709, 0.5734271428993263, 0.5412097018577038, 0.5590617205170056, 0.11818203894987733, 0.11594934899816578, 0.11548728812285303, 0.10910969597536346, 0.09324766404200946, 0.11297884701366567, 0.11728542093218253, 0.1358283894323311, 0.11510974598355384, 0.22540283983726417, 0.17103914371889317, 0.24935010026333593, 0.23139708991581187, 0.2116834840101205, 0.19135882098285617, 0.1705371299341023, 0.2126699572935572, 0.2209012418595554, 0.32685385154538005, 0.34314356484347186, 0.36988756800192024, 0.36952191550610336, 0.375443089514364, 0.2927272068359621, 0.41490454614332994, 0.4348893987389886, 0.3834987498697635, 0.23366535767951468, 0.25866189725809885, 0.2302530339437413, 0.22554822145020648, 0.24019648694644558, 0.21306175836868801, 0.23819888206020323, 0.27857543630058956, 0.25651404700229974, 0.18477259167128046, 0.19426795408013386, 0.19595086844004206, 0.20717051734986014, 0.19752325636984547, 0.22049037243377545, 0.19758769044907198, 0.2250685195540405, 0.1757806127460937, 0.2890409684568702, 0.2535622220515875, 0.40789869201342577, 0.36433021362110163, 0.43665556031431274, 0.20627895553091957, 0.2585961925724303, 0.43201072946856545, 0.21213576965002445, 0.7592369689589549, 0.2000334564012084, 0.19557859121926469, 0.7296203247163067, 0.18252932373457764, 0.6585988508625994, 0.6557618619990195, 0.17668310156736222, 0.4550001636102414, 0.1977316349668934, 0.20570403192958386, 0.6376280177947713, 0.5364671950603048, 0.5697194968937839, 0.6046270222808969, 0.19828487166059172, 0.20677602317736232, 0.20952700098802524, 0.17504352135657608, 0.1872149123248671, 0.18058125767784716, 0.18363327923768158, 0.1734550494379451, 0.1811900681761487, 0.19638412069592814, 0.1784610582440399, 0.1785635020273718, 0.08375222723055509, 0.08554103552116354, 0.07667496226636883, 0.08154009881601743, 0.07207178993966012, 0.06753014772800747, 0.09178354426194113, 0.08273888202466828, 0.08214048838785781]}, "mutation_prompt": null}
{"id": "7bdccaef-7cbd-4ff2-a531-59652fcc7962", "solution": "import numpy as np\n\nclass HybridStochasticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20  # could be tuned\n        self.scale_factor = 0.8\n        self.crossover_prob = 0.7\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n    \n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:  # If not evaluated yet\n                self.fitness[i] = func(self.population[i])\n                self.evaluations += 1\n    \n    def select_parents(self, idx):\n        indices = [i for i in range(self.population_size) if i != idx]\n        return np.random.choice(indices, 3, replace=False)\n    \n    def differential_evolution_mutation(self, idx):\n        a, b, c = self.select_parents(idx)\n        mutant_vector = self.population[a] + self.scale_factor * (self.population[b] - self.population[c])\n        return np.clip(mutant_vector, self.lower_bound, self.upper_bound)\n    \n    def crossover(self, target_vector, mutant_vector):\n        crossover_mask = np.random.rand(self.dim) < self.crossover_prob\n        return np.where(crossover_mask, mutant_vector, target_vector)\n    \n    def local_search(self, vector):\n        perturbation = np.random.normal(0, 0.1, self.dim)  # Small Gaussian perturbations\n        candidate_vector = vector + perturbation\n        return np.clip(candidate_vector, self.lower_bound, self.upper_bound)\n    \n    def __call__(self, func):\n        self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n                \n                mutant_vector = self.differential_evolution_mutation(i)\n                trial_vector = self.crossover(self.population[i], mutant_vector)\n                trial_fitness = func(trial_vector)\n                self.evaluations += 1\n                \n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial_vector\n                    self.fitness[i] = trial_fitness\n                \n                # Adaptive Local Search\n                if np.random.rand() < 0.2:  # Probability of local search\n                    candidate_vector = self.local_search(self.population[i])\n                    candidate_fitness = func(candidate_vector)\n                    self.evaluations += 1\n                    if candidate_fitness < self.fitness[i]:\n                        self.population[i] = candidate_vector\n                        self.fitness[i] = candidate_fitness\n        \n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx]", "name": "HybridStochasticOptimizer", "description": "A novel hybrid stochastic optimization algorithm combining Differential Evolution with Adaptive Local Search for efficient exploration and exploitation in high-dimensional spaces.", "configspace": "", "generation": 1, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "2778f81b-4fe3-49b5-ac9f-5e39c1e0f4f7", "metadata": {"aucs": [0.838550702247986, 0.8335387980441803, 0.8392883035317981, 0.8448220232438199, 0.8692280794153219, 0.86492167545159, 0.8423065034848981, 0.8684684289902511, 0.8603227149202752, 0.7334709241625984, 0.7334474051410083, 0.7264747402639247, 0.7344716495540463, 0.7333723373934493, 0.7400257469840834, 0.7326254850725233, 0.7347331739693774, 0.7361787942160427, 0.2900640312488526, 0.47241555583359396, 0.4068320718407943, 0.38522901052495495, 0.4980851072709567, 0.36017996570170985, 0.3855858146223492, 0.5301915371647596, 0.45367220095702276, 0.15700893469067034, 0.15086174717667622, 0.3606580261151082, 0.3820765391627686, 0.37857498203107787, 0.24358195595397236, 0.44006871328822617, 0.15619687169305285, 0.5431907915884118, 0.9514039641368535, 0.9599028786233561, 0.9703152109391081, 0.9689783778060999, 0.9634907937987585, 0.977053323860018, 0.9713025537743715, 0.9710944937497286, 0.9623817898603562, 0.4811632069641354, 0.4861494717101681, 0.5181873123716814, 0.4868754422257876, 0.5064595460782774, 0.5046714079907575, 0.48795856678445426, 0.5189360989349037, 0.4807572622639553, 0.5868371238845421, 0.6828198726555201, 0.6456891282317958, 0.5918970855007204, 0.6200859298334032, 0.7702265673351774, 0.6958432356063231, 0.7340731017036689, 0.7310117434316175, 0.2595202684690293, 0.2665440796289047, 0.3447806314120041, 0.2259147785649217, 0.27326843203813556, 0.32150330600063937, 0.28010562289764906, 0.29552641888490006, 0.32221432565738917, 0.12522923128088514, 0.21777955819361272, 0.14790043362950323, 0.27777879274959094, 0.2221290906104053, 0.2546698244616422, 0.28777697821894743, 0.25863731044319516, 0.20839151152332214, 0.08200045947449175, 0.1233998060896413, 0.09590295126948145, 0.11121731514540156, 0.08626097600496452, 0.10894404364206134, 0.08859205077834265, 0.08679856412127096, 0.07606669628336682, 0.21252808557164604, 0.22412982702821604, 0.20390446043916832, 0.22534486281182287, 0.16119227720619456, 0.17904454354061838, 0.21567655887183101, 0.3290280255186192, 0.2655490022975723, 0.00656682574070222, 0.030575438199327065, 0.03955418050983606, 0.02393376066661479, 0.05524215352879602, 0.07951077146365404, 0.03032444160772796, 0.08778729249895956, 0.07737778818842145, 0.19516566745890707, 0.1937347077828694, 0.19411490582830215, 0.21154639947615128, 0.20856128496804915, 0.20410401494498287, 0.18879954051159387, 0.188381415941566, 0.1647126038437029, 0.5421754637547482, 0.5695984805554837, 0.5600632409189952, 0.5552469615873223, 0.5724375125380385, 0.5565735237775709, 0.5734271428993263, 0.5412097018577038, 0.5590617205170056, 0.11818203894987733, 0.11594934899816578, 0.11548728812285303, 0.10910969597536346, 0.09324766404200946, 0.11297884701366567, 0.11728542093218253, 0.1358283894323311, 0.11510974598355384, 0.22540283983726417, 0.17103914371889317, 0.24935010026333593, 0.23139708991581187, 0.2116834840101205, 0.19135882098285617, 0.1705371299341023, 0.2126699572935572, 0.2209012418595554, 0.32685385154538005, 0.34314356484347186, 0.36988756800192024, 0.36952191550610336, 0.375443089514364, 0.2927272068359621, 0.41490454614332994, 0.4348893987389886, 0.3834987498697635, 0.23366535767951468, 0.25866189725809885, 0.2302530339437413, 0.22554822145020648, 0.24019648694644558, 0.21306175836868801, 0.23819888206020323, 0.27857543630058956, 0.25651404700229974, 0.18477259167128046, 0.19426795408013386, 0.19595086844004206, 0.20717051734986014, 0.19752325636984547, 0.22049037243377545, 0.19758769044907198, 0.2250685195540405, 0.1757806127460937, 0.2890409684568702, 0.2535622220515875, 0.40789869201342577, 0.36433021362110163, 0.43665556031431274, 0.20627895553091957, 0.2585961925724303, 0.43201072946856545, 0.21213576965002445, 0.7592369689589549, 0.2000334564012084, 0.19557859121926469, 0.7296203247163067, 0.18252932373457764, 0.6585988508625994, 0.6557618619990195, 0.17668310156736222, 0.4550001636102414, 0.1977316349668934, 0.20570403192958386, 0.6376280177947713, 0.5364671950603048, 0.5697194968937839, 0.6046270222808969, 0.19828487166059172, 0.20677602317736232, 0.20952700098802524, 0.17504352135657608, 0.1872149123248671, 0.18058125767784716, 0.18363327923768158, 0.1734550494379451, 0.1811900681761487, 0.19638412069592814, 0.1784610582440399, 0.1785635020273718, 0.08375222723055509, 0.08554103552116354, 0.07667496226636883, 0.08154009881601743, 0.07207178993966012, 0.06753014772800747, 0.09178354426194113, 0.08273888202466828, 0.08214048838785781]}, "mutation_prompt": null}
{"id": "8549d56c-9802-4cc5-935a-99fa7c46295e", "solution": "import numpy as np\n\nclass HybridStochasticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20  # could be tuned\n        self.scale_factor = 0.8\n        self.crossover_prob = 0.7\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n    \n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:  # If not evaluated yet\n                self.fitness[i] = func(self.population[i])\n                self.evaluations += 1\n    \n    def select_parents(self, idx):\n        indices = [i for i in range(self.population_size) if i != idx]\n        return np.random.choice(indices, 3, replace=False)\n    \n    def differential_evolution_mutation(self, idx):\n        a, b, c = self.select_parents(idx)\n        mutant_vector = self.population[a] + self.scale_factor * (self.population[b] - self.population[c])\n        return np.clip(mutant_vector, self.lower_bound, self.upper_bound)\n    \n    def crossover(self, target_vector, mutant_vector):\n        crossover_mask = np.random.rand(self.dim) < self.crossover_prob\n        return np.where(crossover_mask, mutant_vector, target_vector)\n    \n    def local_search(self, vector):\n        perturbation = np.random.normal(0, 0.1, self.dim)  # Small Gaussian perturbations\n        candidate_vector = vector + perturbation\n        return np.clip(candidate_vector, self.lower_bound, self.upper_bound)\n    \n    def __call__(self, func):\n        self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n                \n                mutant_vector = self.differential_evolution_mutation(i)\n                trial_vector = self.crossover(self.population[i], mutant_vector)\n                trial_fitness = func(trial_vector)\n                self.evaluations += 1\n                \n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial_vector\n                    self.fitness[i] = trial_fitness\n                \n                # Adaptive Local Search\n                if np.random.rand() < 0.2:  # Probability of local search\n                    candidate_vector = self.local_search(self.population[i])\n                    candidate_fitness = func(candidate_vector)\n                    self.evaluations += 1\n                    if candidate_fitness < self.fitness[i]:\n                        self.population[i] = candidate_vector\n                        self.fitness[i] = candidate_fitness\n        \n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx]", "name": "HybridStochasticOptimizer", "description": "A novel hybrid stochastic optimization algorithm combining Differential Evolution with Adaptive Local Search for efficient exploration and exploitation in high-dimensional spaces.", "configspace": "", "generation": 1, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "2778f81b-4fe3-49b5-ac9f-5e39c1e0f4f7", "metadata": {"aucs": [0.838550702247986, 0.8335387980441803, 0.8392883035317981, 0.8448220232438199, 0.8692280794153219, 0.86492167545159, 0.8423065034848981, 0.8684684289902511, 0.8603227149202752, 0.7334709241625984, 0.7334474051410083, 0.7264747402639247, 0.7344716495540463, 0.7333723373934493, 0.7400257469840834, 0.7326254850725233, 0.7347331739693774, 0.7361787942160427, 0.2900640312488526, 0.47241555583359396, 0.4068320718407943, 0.38522901052495495, 0.4980851072709567, 0.36017996570170985, 0.3855858146223492, 0.5301915371647596, 0.45367220095702276, 0.15700893469067034, 0.15086174717667622, 0.3606580261151082, 0.3820765391627686, 0.37857498203107787, 0.24358195595397236, 0.44006871328822617, 0.15619687169305285, 0.5431907915884118, 0.9514039641368535, 0.9599028786233561, 0.9703152109391081, 0.9689783778060999, 0.9634907937987585, 0.977053323860018, 0.9713025537743715, 0.9710944937497286, 0.9623817898603562, 0.4811632069641354, 0.4861494717101681, 0.5181873123716814, 0.4868754422257876, 0.5064595460782774, 0.5046714079907575, 0.48795856678445426, 0.5189360989349037, 0.4807572622639553, 0.5868371238845421, 0.6828198726555201, 0.6456891282317958, 0.5918970855007204, 0.6200859298334032, 0.7702265673351774, 0.6958432356063231, 0.7340731017036689, 0.7310117434316175, 0.2595202684690293, 0.2665440796289047, 0.3447806314120041, 0.2259147785649217, 0.27326843203813556, 0.32150330600063937, 0.28010562289764906, 0.29552641888490006, 0.32221432565738917, 0.12522923128088514, 0.21777955819361272, 0.14790043362950323, 0.27777879274959094, 0.2221290906104053, 0.2546698244616422, 0.28777697821894743, 0.25863731044319516, 0.20839151152332214, 0.08200045947449175, 0.1233998060896413, 0.09590295126948145, 0.11121731514540156, 0.08626097600496452, 0.10894404364206134, 0.08859205077834265, 0.08679856412127096, 0.07606669628336682, 0.21252808557164604, 0.22412982702821604, 0.20390446043916832, 0.22534486281182287, 0.16119227720619456, 0.17904454354061838, 0.21567655887183101, 0.3290280255186192, 0.2655490022975723, 0.00656682574070222, 0.030575438199327065, 0.03955418050983606, 0.02393376066661479, 0.05524215352879602, 0.07951077146365404, 0.03032444160772796, 0.08778729249895956, 0.07737778818842145, 0.19516566745890707, 0.1937347077828694, 0.19411490582830215, 0.21154639947615128, 0.20856128496804915, 0.20410401494498287, 0.18879954051159387, 0.188381415941566, 0.1647126038437029, 0.5421754637547482, 0.5695984805554837, 0.5600632409189952, 0.5552469615873223, 0.5724375125380385, 0.5565735237775709, 0.5734271428993263, 0.5412097018577038, 0.5590617205170056, 0.11818203894987733, 0.11594934899816578, 0.11548728812285303, 0.10910969597536346, 0.09324766404200946, 0.11297884701366567, 0.11728542093218253, 0.1358283894323311, 0.11510974598355384, 0.22540283983726417, 0.17103914371889317, 0.24935010026333593, 0.23139708991581187, 0.2116834840101205, 0.19135882098285617, 0.1705371299341023, 0.2126699572935572, 0.2209012418595554, 0.32685385154538005, 0.34314356484347186, 0.36988756800192024, 0.36952191550610336, 0.375443089514364, 0.2927272068359621, 0.41490454614332994, 0.4348893987389886, 0.3834987498697635, 0.23366535767951468, 0.25866189725809885, 0.2302530339437413, 0.22554822145020648, 0.24019648694644558, 0.21306175836868801, 0.23819888206020323, 0.27857543630058956, 0.25651404700229974, 0.18477259167128046, 0.19426795408013386, 0.19595086844004206, 0.20717051734986014, 0.19752325636984547, 0.22049037243377545, 0.19758769044907198, 0.2250685195540405, 0.1757806127460937, 0.2890409684568702, 0.2535622220515875, 0.40789869201342577, 0.36433021362110163, 0.43665556031431274, 0.20627895553091957, 0.2585961925724303, 0.43201072946856545, 0.21213576965002445, 0.7592369689589549, 0.2000334564012084, 0.19557859121926469, 0.7296203247163067, 0.18252932373457764, 0.6585988508625994, 0.6557618619990195, 0.17668310156736222, 0.4550001636102414, 0.1977316349668934, 0.20570403192958386, 0.6376280177947713, 0.5364671950603048, 0.5697194968937839, 0.6046270222808969, 0.19828487166059172, 0.20677602317736232, 0.20952700098802524, 0.17504352135657608, 0.1872149123248671, 0.18058125767784716, 0.18363327923768158, 0.1734550494379451, 0.1811900681761487, 0.19638412069592814, 0.1784610582440399, 0.1785635020273718, 0.08375222723055509, 0.08554103552116354, 0.07667496226636883, 0.08154009881601743, 0.07207178993966012, 0.06753014772800747, 0.09178354426194113, 0.08273888202466828, 0.08214048838785781]}, "mutation_prompt": null}
{"id": "f9e5a6f9-15f1-478b-abbd-3fc731d48167", "solution": "import numpy as np\n\nclass HybridStochasticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20  # could be tuned\n        self.scale_factor = 0.8\n        self.crossover_prob = 0.7\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n    \n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:  # If not evaluated yet\n                self.fitness[i] = func(self.population[i])\n                self.evaluations += 1\n    \n    def select_parents(self, idx):\n        indices = [i for i in range(self.population_size) if i != idx]\n        return np.random.choice(indices, 3, replace=False)\n    \n    def differential_evolution_mutation(self, idx):\n        a, b, c = self.select_parents(idx)\n        mutant_vector = self.population[a] + self.scale_factor * (self.population[b] - self.population[c])\n        return np.clip(mutant_vector, self.lower_bound, self.upper_bound)\n    \n    def crossover(self, target_vector, mutant_vector):\n        crossover_mask = np.random.rand(self.dim) < self.crossover_prob\n        return np.where(crossover_mask, mutant_vector, target_vector)\n    \n    def local_search(self, vector):\n        perturbation = np.random.normal(0, 0.1, self.dim)  # Small Gaussian perturbations\n        candidate_vector = vector + perturbation\n        return np.clip(candidate_vector, self.lower_bound, self.upper_bound)\n    \n    def __call__(self, func):\n        self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n                \n                mutant_vector = self.differential_evolution_mutation(i)\n                trial_vector = self.crossover(self.population[i], mutant_vector)\n                trial_fitness = func(trial_vector)\n                self.evaluations += 1\n                \n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial_vector\n                    self.fitness[i] = trial_fitness\n                \n                # Adaptive Local Search\n                if np.random.rand() < 0.2:  # Probability of local search\n                    candidate_vector = self.local_search(self.population[i])\n                    candidate_fitness = func(candidate_vector)\n                    self.evaluations += 1\n                    if candidate_fitness < self.fitness[i]:\n                        self.population[i] = candidate_vector\n                        self.fitness[i] = candidate_fitness\n        \n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx]", "name": "HybridStochasticOptimizer", "description": "A novel hybrid stochastic optimization algorithm combining Differential Evolution with Adaptive Local Search for efficient exploration and exploitation in high-dimensional spaces.", "configspace": "", "generation": 1, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "2778f81b-4fe3-49b5-ac9f-5e39c1e0f4f7", "metadata": {"aucs": [0.838550702247986, 0.8335387980441803, 0.8392883035317981, 0.8448220232438199, 0.8692280794153219, 0.86492167545159, 0.8423065034848981, 0.8684684289902511, 0.8603227149202752, 0.7334709241625984, 0.7334474051410083, 0.7264747402639247, 0.7344716495540463, 0.7333723373934493, 0.7400257469840834, 0.7326254850725233, 0.7347331739693774, 0.7361787942160427, 0.2900640312488526, 0.47241555583359396, 0.4068320718407943, 0.38522901052495495, 0.4980851072709567, 0.36017996570170985, 0.3855858146223492, 0.5301915371647596, 0.45367220095702276, 0.15700893469067034, 0.15086174717667622, 0.3606580261151082, 0.3820765391627686, 0.37857498203107787, 0.24358195595397236, 0.44006871328822617, 0.15619687169305285, 0.5431907915884118, 0.9514039641368535, 0.9599028786233561, 0.9703152109391081, 0.9689783778060999, 0.9634907937987585, 0.977053323860018, 0.9713025537743715, 0.9710944937497286, 0.9623817898603562, 0.4811632069641354, 0.4861494717101681, 0.5181873123716814, 0.4868754422257876, 0.5064595460782774, 0.5046714079907575, 0.48795856678445426, 0.5189360989349037, 0.4807572622639553, 0.5868371238845421, 0.6828198726555201, 0.6456891282317958, 0.5918970855007204, 0.6200859298334032, 0.7702265673351774, 0.6958432356063231, 0.7340731017036689, 0.7310117434316175, 0.2595202684690293, 0.2665440796289047, 0.3447806314120041, 0.2259147785649217, 0.27326843203813556, 0.32150330600063937, 0.28010562289764906, 0.29552641888490006, 0.32221432565738917, 0.12522923128088514, 0.21777955819361272, 0.14790043362950323, 0.27777879274959094, 0.2221290906104053, 0.2546698244616422, 0.28777697821894743, 0.25863731044319516, 0.20839151152332214, 0.08200045947449175, 0.1233998060896413, 0.09590295126948145, 0.11121731514540156, 0.08626097600496452, 0.10894404364206134, 0.08859205077834265, 0.08679856412127096, 0.07606669628336682, 0.21252808557164604, 0.22412982702821604, 0.20390446043916832, 0.22534486281182287, 0.16119227720619456, 0.17904454354061838, 0.21567655887183101, 0.3290280255186192, 0.2655490022975723, 0.00656682574070222, 0.030575438199327065, 0.03955418050983606, 0.02393376066661479, 0.05524215352879602, 0.07951077146365404, 0.03032444160772796, 0.08778729249895956, 0.07737778818842145, 0.19516566745890707, 0.1937347077828694, 0.19411490582830215, 0.21154639947615128, 0.20856128496804915, 0.20410401494498287, 0.18879954051159387, 0.188381415941566, 0.1647126038437029, 0.5421754637547482, 0.5695984805554837, 0.5600632409189952, 0.5552469615873223, 0.5724375125380385, 0.5565735237775709, 0.5734271428993263, 0.5412097018577038, 0.5590617205170056, 0.11818203894987733, 0.11594934899816578, 0.11548728812285303, 0.10910969597536346, 0.09324766404200946, 0.11297884701366567, 0.11728542093218253, 0.1358283894323311, 0.11510974598355384, 0.22540283983726417, 0.17103914371889317, 0.24935010026333593, 0.23139708991581187, 0.2116834840101205, 0.19135882098285617, 0.1705371299341023, 0.2126699572935572, 0.2209012418595554, 0.32685385154538005, 0.34314356484347186, 0.36988756800192024, 0.36952191550610336, 0.375443089514364, 0.2927272068359621, 0.41490454614332994, 0.4348893987389886, 0.3834987498697635, 0.23366535767951468, 0.25866189725809885, 0.2302530339437413, 0.22554822145020648, 0.24019648694644558, 0.21306175836868801, 0.23819888206020323, 0.27857543630058956, 0.25651404700229974, 0.18477259167128046, 0.19426795408013386, 0.19595086844004206, 0.20717051734986014, 0.19752325636984547, 0.22049037243377545, 0.19758769044907198, 0.2250685195540405, 0.1757806127460937, 0.2890409684568702, 0.2535622220515875, 0.40789869201342577, 0.36433021362110163, 0.43665556031431274, 0.20627895553091957, 0.2585961925724303, 0.43201072946856545, 0.21213576965002445, 0.7592369689589549, 0.2000334564012084, 0.19557859121926469, 0.7296203247163067, 0.18252932373457764, 0.6585988508625994, 0.6557618619990195, 0.17668310156736222, 0.4550001636102414, 0.1977316349668934, 0.20570403192958386, 0.6376280177947713, 0.5364671950603048, 0.5697194968937839, 0.6046270222808969, 0.19828487166059172, 0.20677602317736232, 0.20952700098802524, 0.17504352135657608, 0.1872149123248671, 0.18058125767784716, 0.18363327923768158, 0.1734550494379451, 0.1811900681761487, 0.19638412069592814, 0.1784610582440399, 0.1785635020273718, 0.08375222723055509, 0.08554103552116354, 0.07667496226636883, 0.08154009881601743, 0.07207178993966012, 0.06753014772800747, 0.09178354426194113, 0.08273888202466828, 0.08214048838785781]}, "mutation_prompt": null}
{"id": "ab124c04-6c86-4f24-9f5d-d84c476efe27", "solution": "import numpy as np\n\nclass HybridStochasticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20  # could be tuned\n        self.scale_factor = 0.8\n        self.crossover_prob = 0.7\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n    \n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:  # If not evaluated yet\n                self.fitness[i] = func(self.population[i])\n                self.evaluations += 1\n    \n    def select_parents(self, idx):\n        indices = [i for i in range(self.population_size) if i != idx]\n        return np.random.choice(indices, 3, replace=False)\n    \n    def differential_evolution_mutation(self, idx):\n        a, b, c = self.select_parents(idx)\n        mutant_vector = self.population[a] + self.scale_factor * (self.population[b] - self.population[c])\n        return np.clip(mutant_vector, self.lower_bound, self.upper_bound)\n    \n    def crossover(self, target_vector, mutant_vector):\n        crossover_mask = np.random.rand(self.dim) < self.crossover_prob\n        return np.where(crossover_mask, mutant_vector, target_vector)\n    \n    def local_search(self, vector):\n        perturbation = np.random.normal(0, 0.1, self.dim)  # Small Gaussian perturbations\n        candidate_vector = vector + perturbation\n        return np.clip(candidate_vector, self.lower_bound, self.upper_bound)\n    \n    def __call__(self, func):\n        self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n                \n                mutant_vector = self.differential_evolution_mutation(i)\n                trial_vector = self.crossover(self.population[i], mutant_vector)\n                trial_fitness = func(trial_vector)\n                self.evaluations += 1\n                \n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial_vector\n                    self.fitness[i] = trial_fitness\n                \n                # Adaptive Local Search\n                if np.random.rand() < 0.2:  # Probability of local search\n                    candidate_vector = self.local_search(self.population[i])\n                    candidate_fitness = func(candidate_vector)\n                    self.evaluations += 1\n                    if candidate_fitness < self.fitness[i]:\n                        self.population[i] = candidate_vector\n                        self.fitness[i] = candidate_fitness\n        \n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx]", "name": "HybridStochasticOptimizer", "description": "A novel hybrid stochastic optimization algorithm combining Differential Evolution with Adaptive Local Search for efficient exploration and exploitation in high-dimensional spaces.", "configspace": "", "generation": 1, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "2778f81b-4fe3-49b5-ac9f-5e39c1e0f4f7", "metadata": {"aucs": [0.838550702247986, 0.8335387980441803, 0.8392883035317981, 0.8448220232438199, 0.8692280794153219, 0.86492167545159, 0.8423065034848981, 0.8684684289902511, 0.8603227149202752, 0.7334709241625984, 0.7334474051410083, 0.7264747402639247, 0.7344716495540463, 0.7333723373934493, 0.7400257469840834, 0.7326254850725233, 0.7347331739693774, 0.7361787942160427, 0.2900640312488526, 0.47241555583359396, 0.4068320718407943, 0.38522901052495495, 0.4980851072709567, 0.36017996570170985, 0.3855858146223492, 0.5301915371647596, 0.45367220095702276, 0.15700893469067034, 0.15086174717667622, 0.3606580261151082, 0.3820765391627686, 0.37857498203107787, 0.24358195595397236, 0.44006871328822617, 0.15619687169305285, 0.5431907915884118, 0.9514039641368535, 0.9599028786233561, 0.9703152109391081, 0.9689783778060999, 0.9634907937987585, 0.977053323860018, 0.9713025537743715, 0.9710944937497286, 0.9623817898603562, 0.4811632069641354, 0.4861494717101681, 0.5181873123716814, 0.4868754422257876, 0.5064595460782774, 0.5046714079907575, 0.48795856678445426, 0.5189360989349037, 0.4807572622639553, 0.5868371238845421, 0.6828198726555201, 0.6456891282317958, 0.5918970855007204, 0.6200859298334032, 0.7702265673351774, 0.6958432356063231, 0.7340731017036689, 0.7310117434316175, 0.2595202684690293, 0.2665440796289047, 0.3447806314120041, 0.2259147785649217, 0.27326843203813556, 0.32150330600063937, 0.28010562289764906, 0.29552641888490006, 0.32221432565738917, 0.12522923128088514, 0.21777955819361272, 0.14790043362950323, 0.27777879274959094, 0.2221290906104053, 0.2546698244616422, 0.28777697821894743, 0.25863731044319516, 0.20839151152332214, 0.08200045947449175, 0.1233998060896413, 0.09590295126948145, 0.11121731514540156, 0.08626097600496452, 0.10894404364206134, 0.08859205077834265, 0.08679856412127096, 0.07606669628336682, 0.21252808557164604, 0.22412982702821604, 0.20390446043916832, 0.22534486281182287, 0.16119227720619456, 0.17904454354061838, 0.21567655887183101, 0.3290280255186192, 0.2655490022975723, 0.00656682574070222, 0.030575438199327065, 0.03955418050983606, 0.02393376066661479, 0.05524215352879602, 0.07951077146365404, 0.03032444160772796, 0.08778729249895956, 0.07737778818842145, 0.19516566745890707, 0.1937347077828694, 0.19411490582830215, 0.21154639947615128, 0.20856128496804915, 0.20410401494498287, 0.18879954051159387, 0.188381415941566, 0.1647126038437029, 0.5421754637547482, 0.5695984805554837, 0.5600632409189952, 0.5552469615873223, 0.5724375125380385, 0.5565735237775709, 0.5734271428993263, 0.5412097018577038, 0.5590617205170056, 0.11818203894987733, 0.11594934899816578, 0.11548728812285303, 0.10910969597536346, 0.09324766404200946, 0.11297884701366567, 0.11728542093218253, 0.1358283894323311, 0.11510974598355384, 0.22540283983726417, 0.17103914371889317, 0.24935010026333593, 0.23139708991581187, 0.2116834840101205, 0.19135882098285617, 0.1705371299341023, 0.2126699572935572, 0.2209012418595554, 0.32685385154538005, 0.34314356484347186, 0.36988756800192024, 0.36952191550610336, 0.375443089514364, 0.2927272068359621, 0.41490454614332994, 0.4348893987389886, 0.3834987498697635, 0.23366535767951468, 0.25866189725809885, 0.2302530339437413, 0.22554822145020648, 0.24019648694644558, 0.21306175836868801, 0.23819888206020323, 0.27857543630058956, 0.25651404700229974, 0.18477259167128046, 0.19426795408013386, 0.19595086844004206, 0.20717051734986014, 0.19752325636984547, 0.22049037243377545, 0.19758769044907198, 0.2250685195540405, 0.1757806127460937, 0.2890409684568702, 0.2535622220515875, 0.40789869201342577, 0.36433021362110163, 0.43665556031431274, 0.20627895553091957, 0.2585961925724303, 0.43201072946856545, 0.21213576965002445, 0.7592369689589549, 0.2000334564012084, 0.19557859121926469, 0.7296203247163067, 0.18252932373457764, 0.6585988508625994, 0.6557618619990195, 0.17668310156736222, 0.4550001636102414, 0.1977316349668934, 0.20570403192958386, 0.6376280177947713, 0.5364671950603048, 0.5697194968937839, 0.6046270222808969, 0.19828487166059172, 0.20677602317736232, 0.20952700098802524, 0.17504352135657608, 0.1872149123248671, 0.18058125767784716, 0.18363327923768158, 0.1734550494379451, 0.1811900681761487, 0.19638412069592814, 0.1784610582440399, 0.1785635020273718, 0.08375222723055509, 0.08554103552116354, 0.07667496226636883, 0.08154009881601743, 0.07207178993966012, 0.06753014772800747, 0.09178354426194113, 0.08273888202466828, 0.08214048838785781]}, "mutation_prompt": null}
{"id": "73570a95-691e-4a96-8b81-fe66f15a3ab3", "solution": "import numpy as np\n\nclass EnhancedHybridStochasticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.scale_factor = 0.8\n        self.crossover_prob = 0.7\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n    \n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.evaluations += 1\n    \n    def select_parents(self, idx):\n        indices = [i for i in range(self.population_size) if i != idx]\n        return np.random.choice(indices, 3, replace=False)\n    \n    def differential_evolution_mutation(self, idx):\n        a, b, c = self.select_parents(idx)\n        F_dynamic = self.scale_factor + np.random.rand() * 0.2  # Adaptive scaling\n        mutant_vector = self.population[a] + F_dynamic * (self.population[b] - self.population[c])\n        return np.clip(mutant_vector, self.lower_bound, self.upper_bound)\n    \n    def crossover(self, target_vector, mutant_vector):\n        crossover_mask = np.random.rand(self.dim) < self.crossover_prob\n        return np.where(crossover_mask, mutant_vector, target_vector)\n    \n    def local_search(self, vector):\n        perturbation = np.random.normal(0, 0.1, self.dim)\n        candidate_vector = vector + perturbation\n        return np.clip(candidate_vector, self.lower_bound, self.upper_bound)\n    \n    def competitive_selection(self, trial_vector, trial_fitness, idx):\n        if trial_fitness < self.fitness[idx]:\n            self.population[idx] = trial_vector\n            self.fitness[idx] = trial_fitness\n        elif np.random.rand() < 0.1:  # Explore less fit solutions with low probability\n            self.population[idx] = trial_vector\n            self.fitness[idx] = trial_fitness\n    \n    def __call__(self, func):\n        self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n                \n                mutant_vector = self.differential_evolution_mutation(i)\n                trial_vector = self.crossover(self.population[i], mutant_vector)\n                trial_fitness = func(trial_vector)\n                self.evaluations += 1\n                \n                self.competitive_selection(trial_vector, trial_fitness, i)\n                \n                if np.random.rand() < 0.2:\n                    candidate_vector = self.local_search(self.population[i])\n                    candidate_fitness = func(candidate_vector)\n                    self.evaluations += 1\n                    self.competitive_selection(candidate_vector, candidate_fitness, i)\n        \n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx]", "name": "EnhancedHybridStochasticOptimizer", "description": "Enhanced hybrid stochastic optimizer using competitive selection and adaptive scaling for improved convergence speed.", "configspace": "", "generation": 17, "fitness": 0.1788418654605278, "feedback": "The algorithm EnhancedHybridStochasticOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.18 with standard deviation 0.20.", "error": "", "parent_id": "2778f81b-4fe3-49b5-ac9f-5e39c1e0f4f7", "metadata": {"aucs": [0.48844598447967924, 0.44618508346139973, 0.43468782964767694, 0.5070688784616971, 0.5077626351467285, 0.4801209589111336, 0.4868356611340934, 0.4923108148715625, 0.47602553262258773, 0.1570396290734457, 0.11472632436978003, 0.17370347333127956, 0.1791969690448748, 0.133960954654194, 0.18297060582373437, 0.16206780550084443, 0.14970108919291836, 0.19344166451486733, 0.07675019432203423, 0.08324118504523614, 0.09275430681825092, 0.10473200571306984, 0.10304180896244886, 0.09154913077936944, 0.08880719068571163, 0.09450385136134187, 0.08877915672965553, 0.0678839458193724, 0.08329316531865971, 0.07249087946968902, 0.07957131001529838, 0.08556023750155328, 0.07996225997789441, 0.07932352067959603, 0.0700857819648929, 0.08129798542162647, 0.9784194852593676, 0.9960667788082083, 0.9692730707107745, 0.9339043374505845, 0.9731153553812044, 0.9757722748649559, 0.9740846713204687, 0.974591111663756, 0.9704113454694079, 0.11132053730466673, 0.13381635777871292, 0.13073953362410395, 0.14397043489255812, 0.13487078806795783, 0.15127148088843856, 0.16060874389877555, 0.17660064654974672, 0.12940754912910124, 0.18840300810887256, 0.13984133612980187, 0.15724586673282137, 0.24090423924252535, 0.17697516312524553, 0.17479085387516402, 0.14895999825731943, 0.18235084136465418, 0.17899779933393256, 0.11426830963461776, 0.11209656479123209, 0.10895981535664045, 0.08046236116149319, 0.10277751983167893, 0.13429625329693773, 0.13625915315933557, 0.08371236703590179, 0.10890394879328047, 0.10405104230732198, 0.09052736906559877, 0.09397705003257417, 0.08490769774375062, 0.07212367543279619, 0.06570013636455618, 0.058816225486978735, 0.06759254659500324, 0.033115241699886444, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.03732337980674061, 0.045090127239197475, 0.07481872350696572, 0.047535570887025314, 0.04227817627498931, 0.05032360012798687, 0.0448996667893532, 0.047745440307600306, 0.058235226670530715, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.051559319926179015, 0.030930987457567616, 0.04681815813067802, 0.06378522423551936, 0.05514260554885009, 0.03559296588840444, 0.054416380880166204, 0.06692849565593462, 0.05894494061122857, 0.3724061273336685, 0.36157297077673434, 0.35374367526270445, 0.382902406472291, 0.3739914183108436, 0.3809926704391088, 0.3657209975532647, 0.36349758743944927, 0.3524027574629449, 0.07850274883198094, 0.07719620079327782, 0.08363153810369472, 0.0703067715506066, 0.08588274351286784, 0.0772966046863045, 0.06729131383903497, 0.07145880163434393, 0.05969364923232934, 0.16023701774017374, 0.16752059877544456, 0.1470788608871948, 0.17009019171229045, 0.1504744154209544, 0.23736160931945283, 0.15319258202341846, 0.17314612548471664, 0.1429285957369899, 0.1903380790938105, 0.20653370863369025, 0.20224536454289632, 0.1657421358347999, 0.17391494531053941, 0.17469988269345993, 0.2131573149986864, 0.22964059573860574, 0.211679891020497, 0.1396744084598639, 0.1213522128430925, 0.13444073448737703, 0.12726150265757674, 0.13744796965811912, 0.1303703972066922, 0.14664324883807034, 0.13822357221447767, 0.1736340545581766, 0.16861132362891662, 0.1767711312787016, 0.16925505690413356, 0.1706220745308017, 0.1779694579175357, 0.18938546740196593, 0.1722716025143285, 0.17788930130919212, 0.1699021535696369, 0.172709671072394, 0.16954678822872415, 0.17294146290636092, 0.17814922906292485, 0.16520518182997257, 0.18555950989063086, 0.17886887256256834, 0.17883744394373335, 0.1881735051777158, 0.16383140313350686, 0.14694257985120018, 0.15171369004988833, 0.18238681283924751, 0.18511181769812723, 0.27049765212561394, 0.17653376850144964, 0.16612768208471373, 0.16176148307080707, 0.1783863517047064, 0.1651816307407895, 0.15491434732677534, 0.14558252220418122, 0.18890531204630534, 0.1526733995298658, 0.23683957069373895, 0.3128296348035454, 0.12422349213691031, 0.1852799093320825, 0.17874297594105482, 0.1795667998126571, 0.19758030746709865, 0.18058780010446263, 0.16795863879830375, 0.18862328867134348, 0.1808198282920791, 0.1947120666452652, 0.07436771350214821, 0.057829587873890365, 0.08080491457381667, 0.05812711550274108, 0.0641759560839733, 0.06322111364771332, 0.06851074232092746, 0.06213490714426351, 0.05885620256623425]}, "mutation_prompt": null}
{"id": "1e314ed2-cd52-4000-9d5b-c03f4cce2b4f", "solution": "import numpy as np\n\nclass EnhancedHybridStochasticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20  # Starting size\n        self.scale_factor = 0.8\n        self.crossover_prob = 0.7\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:  # If not evaluated yet\n                self.fitness[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def select_parents(self, idx):\n        indices = [i for i in range(self.population_size) if i != idx]\n        return np.random.choice(indices, 3, replace=False)\n\n    def differential_evolution_mutation(self, idx):\n        a, b, c = self.select_parents(idx)\n        mutant_vector = self.population[a] + self.scale_factor * (self.population[b] - self.population[c])\n        return np.clip(mutant_vector, self.lower_bound, self.upper_bound)\n\n    def crossover(self, target_vector, mutant_vector):\n        crossover_mask = np.random.rand(self.dim) < self.adaptive_crossover_prob()\n        return np.where(crossover_mask, mutant_vector, target_vector)\n\n    def adaptive_crossover_prob(self):\n        return 0.7 + 0.3 * np.random.rand()\n\n    def local_search(self, vector):\n        perturbation = np.random.normal(0, 0.1, self.dim)  # Small Gaussian perturbations\n        candidate_vector = vector + perturbation\n        return np.clip(candidate_vector, self.lower_bound, self.upper_bound)\n\n    def dynamic_population_adjustment(self):\n        if self.evaluations < self.budget * 0.5:\n            return 20 + int(5 * (1 - (self.evaluations / self.budget)))\n        else:\n            return max(10, self.population_size - int(5 * ((self.evaluations - self.budget * 0.5) / (self.budget * 0.5))))\n\n    def __call__(self, func):\n        self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            self.population_size = self.dynamic_population_adjustment()\n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                mutant_vector = self.differential_evolution_mutation(i)\n                trial_vector = self.crossover(self.population[i], mutant_vector)\n                trial_fitness = func(trial_vector)\n                self.evaluations += 1\n\n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial_vector\n                    self.fitness[i] = trial_fitness\n\n                if np.random.rand() < 0.2:  # Probability of local search\n                    candidate_vector = self.local_search(self.population[i])\n                    candidate_fitness = func(candidate_vector)\n                    self.evaluations += 1\n                    if candidate_fitness < self.fitness[i]:\n                        self.population[i] = candidate_vector\n                        self.fitness[i] = candidate_fitness\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx]", "name": "EnhancedHybridStochasticOptimizer", "description": "Enhanced Hybrid Stochastic Optimizer with dynamic population size and adaptive crossover for faster convergence.", "configspace": "", "generation": 18, "fitness": -Infinity, "feedback": "An exception occurred: IndexError('index 20 is out of bounds for axis 0 with size 20').", "error": "IndexError('index 20 is out of bounds for axis 0 with size 20')", "parent_id": "2778f81b-4fe3-49b5-ac9f-5e39c1e0f4f7", "metadata": {}, "mutation_prompt": null}
{"id": "1dd8d655-9f24-41f6-938e-4e4facb87d63", "solution": "import numpy as np\n\nclass HybridStochasticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20  # could be tuned\n        self.scale_factor = 0.8\n        self.crossover_prob = 0.7\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n    \n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:  # If not evaluated yet\n                self.fitness[i] = func(self.population[i])\n                self.evaluations += 1\n    \n    def select_parents(self, idx):\n        indices = [i for i in range(self.population_size) if i != idx]\n        return np.random.choice(indices, 3, replace=False)\n    \n    def differential_evolution_mutation(self, idx):\n        a, b, c = self.select_parents(idx)\n        mutant_vector = self.population[a] + self.scale_factor * (self.population[b] - self.population[c])\n        return np.clip(mutant_vector, self.lower_bound, self.upper_bound)\n    \n    def crossover(self, target_vector, mutant_vector):\n        crossover_mask = np.random.rand(self.dim) < self.crossover_prob\n        return np.where(crossover_mask, mutant_vector, target_vector)\n    \n    def local_search(self, vector):\n        perturbation = np.random.normal(0, 0.1, self.dim)  # Small Gaussian perturbations\n        candidate_vector = vector + perturbation\n        return np.clip(candidate_vector, self.lower_bound, self.upper_bound)\n    \n    def __call__(self, func):\n        self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n                \n                mutant_vector = self.differential_evolution_mutation(i)\n                trial_vector = self.crossover(self.population[i], mutant_vector)\n                trial_fitness = func(trial_vector)\n                self.evaluations += 1\n                \n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial_vector\n                    self.fitness[i] = trial_fitness\n                \n                # Adaptive Local Search\n                if np.random.rand() < 0.2:  # Probability of local search\n                    candidate_vector = self.local_search(self.population[i])\n                    candidate_fitness = func(candidate_vector)\n                    self.evaluations += 1\n                    if candidate_fitness < self.fitness[i]:\n                        self.population[i] = candidate_vector\n                        self.fitness[i] = candidate_fitness\n        \n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx]", "name": "HybridStochasticOptimizer", "description": "A novel hybrid stochastic optimization algorithm combining Differential Evolution with Adaptive Local Search for efficient exploration and exploitation in high-dimensional spaces.", "configspace": "", "generation": 1, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "2778f81b-4fe3-49b5-ac9f-5e39c1e0f4f7", "metadata": {"aucs": [0.838550702247986, 0.8335387980441803, 0.8392883035317981, 0.8448220232438199, 0.8692280794153219, 0.86492167545159, 0.8423065034848981, 0.8684684289902511, 0.8603227149202752, 0.7334709241625984, 0.7334474051410083, 0.7264747402639247, 0.7344716495540463, 0.7333723373934493, 0.7400257469840834, 0.7326254850725233, 0.7347331739693774, 0.7361787942160427, 0.2900640312488526, 0.47241555583359396, 0.4068320718407943, 0.38522901052495495, 0.4980851072709567, 0.36017996570170985, 0.3855858146223492, 0.5301915371647596, 0.45367220095702276, 0.15700893469067034, 0.15086174717667622, 0.3606580261151082, 0.3820765391627686, 0.37857498203107787, 0.24358195595397236, 0.44006871328822617, 0.15619687169305285, 0.5431907915884118, 0.9514039641368535, 0.9599028786233561, 0.9703152109391081, 0.9689783778060999, 0.9634907937987585, 0.977053323860018, 0.9713025537743715, 0.9710944937497286, 0.9623817898603562, 0.4811632069641354, 0.4861494717101681, 0.5181873123716814, 0.4868754422257876, 0.5064595460782774, 0.5046714079907575, 0.48795856678445426, 0.5189360989349037, 0.4807572622639553, 0.5868371238845421, 0.6828198726555201, 0.6456891282317958, 0.5918970855007204, 0.6200859298334032, 0.7702265673351774, 0.6958432356063231, 0.7340731017036689, 0.7310117434316175, 0.2595202684690293, 0.2665440796289047, 0.3447806314120041, 0.2259147785649217, 0.27326843203813556, 0.32150330600063937, 0.28010562289764906, 0.29552641888490006, 0.32221432565738917, 0.12522923128088514, 0.21777955819361272, 0.14790043362950323, 0.27777879274959094, 0.2221290906104053, 0.2546698244616422, 0.28777697821894743, 0.25863731044319516, 0.20839151152332214, 0.08200045947449175, 0.1233998060896413, 0.09590295126948145, 0.11121731514540156, 0.08626097600496452, 0.10894404364206134, 0.08859205077834265, 0.08679856412127096, 0.07606669628336682, 0.21252808557164604, 0.22412982702821604, 0.20390446043916832, 0.22534486281182287, 0.16119227720619456, 0.17904454354061838, 0.21567655887183101, 0.3290280255186192, 0.2655490022975723, 0.00656682574070222, 0.030575438199327065, 0.03955418050983606, 0.02393376066661479, 0.05524215352879602, 0.07951077146365404, 0.03032444160772796, 0.08778729249895956, 0.07737778818842145, 0.19516566745890707, 0.1937347077828694, 0.19411490582830215, 0.21154639947615128, 0.20856128496804915, 0.20410401494498287, 0.18879954051159387, 0.188381415941566, 0.1647126038437029, 0.5421754637547482, 0.5695984805554837, 0.5600632409189952, 0.5552469615873223, 0.5724375125380385, 0.5565735237775709, 0.5734271428993263, 0.5412097018577038, 0.5590617205170056, 0.11818203894987733, 0.11594934899816578, 0.11548728812285303, 0.10910969597536346, 0.09324766404200946, 0.11297884701366567, 0.11728542093218253, 0.1358283894323311, 0.11510974598355384, 0.22540283983726417, 0.17103914371889317, 0.24935010026333593, 0.23139708991581187, 0.2116834840101205, 0.19135882098285617, 0.1705371299341023, 0.2126699572935572, 0.2209012418595554, 0.32685385154538005, 0.34314356484347186, 0.36988756800192024, 0.36952191550610336, 0.375443089514364, 0.2927272068359621, 0.41490454614332994, 0.4348893987389886, 0.3834987498697635, 0.23366535767951468, 0.25866189725809885, 0.2302530339437413, 0.22554822145020648, 0.24019648694644558, 0.21306175836868801, 0.23819888206020323, 0.27857543630058956, 0.25651404700229974, 0.18477259167128046, 0.19426795408013386, 0.19595086844004206, 0.20717051734986014, 0.19752325636984547, 0.22049037243377545, 0.19758769044907198, 0.2250685195540405, 0.1757806127460937, 0.2890409684568702, 0.2535622220515875, 0.40789869201342577, 0.36433021362110163, 0.43665556031431274, 0.20627895553091957, 0.2585961925724303, 0.43201072946856545, 0.21213576965002445, 0.7592369689589549, 0.2000334564012084, 0.19557859121926469, 0.7296203247163067, 0.18252932373457764, 0.6585988508625994, 0.6557618619990195, 0.17668310156736222, 0.4550001636102414, 0.1977316349668934, 0.20570403192958386, 0.6376280177947713, 0.5364671950603048, 0.5697194968937839, 0.6046270222808969, 0.19828487166059172, 0.20677602317736232, 0.20952700098802524, 0.17504352135657608, 0.1872149123248671, 0.18058125767784716, 0.18363327923768158, 0.1734550494379451, 0.1811900681761487, 0.19638412069592814, 0.1784610582440399, 0.1785635020273718, 0.08375222723055509, 0.08554103552116354, 0.07667496226636883, 0.08154009881601743, 0.07207178993966012, 0.06753014772800747, 0.09178354426194113, 0.08273888202466828, 0.08214048838785781]}, "mutation_prompt": null}
{"id": "50909ba6-599a-425f-87b5-51a67a7788a9", "solution": "import numpy as np\n\nclass HybridStochasticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20  # could be tuned\n        self.scale_factor = 0.8\n        self.crossover_prob = 0.7\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n    \n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:  # If not evaluated yet\n                self.fitness[i] = func(self.population[i])\n                self.evaluations += 1\n    \n    def select_parents(self, idx):\n        indices = [i for i in range(self.population_size) if i != idx]\n        return np.random.choice(indices, 3, replace=False)\n    \n    def differential_evolution_mutation(self, idx):\n        a, b, c = self.select_parents(idx)\n        mutant_vector = self.population[a] + self.scale_factor * (self.population[b] - self.population[c])\n        return np.clip(mutant_vector, self.lower_bound, self.upper_bound)\n    \n    def crossover(self, target_vector, mutant_vector):\n        crossover_mask = np.random.rand(self.dim) < self.crossover_prob\n        return np.where(crossover_mask, mutant_vector, target_vector)\n    \n    def local_search(self, vector):\n        perturbation = np.random.normal(0, 0.1, self.dim)  # Small Gaussian perturbations\n        candidate_vector = vector + perturbation\n        return np.clip(candidate_vector, self.lower_bound, self.upper_bound)\n    \n    def __call__(self, func):\n        self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n                \n                mutant_vector = self.differential_evolution_mutation(i)\n                trial_vector = self.crossover(self.population[i], mutant_vector)\n                trial_fitness = func(trial_vector)\n                self.evaluations += 1\n                \n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial_vector\n                    self.fitness[i] = trial_fitness\n                \n                # Adaptive Local Search\n                if np.random.rand() < 0.2:  # Probability of local search\n                    candidate_vector = self.local_search(self.population[i])\n                    candidate_fitness = func(candidate_vector)\n                    self.evaluations += 1\n                    if candidate_fitness < self.fitness[i]:\n                        self.population[i] = candidate_vector\n                        self.fitness[i] = candidate_fitness\n        \n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx]", "name": "HybridStochasticOptimizer", "description": "A novel hybrid stochastic optimization algorithm combining Differential Evolution with Adaptive Local Search for efficient exploration and exploitation in high-dimensional spaces.", "configspace": "", "generation": 1, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "2778f81b-4fe3-49b5-ac9f-5e39c1e0f4f7", "metadata": {"aucs": [0.838550702247986, 0.8335387980441803, 0.8392883035317981, 0.8448220232438199, 0.8692280794153219, 0.86492167545159, 0.8423065034848981, 0.8684684289902511, 0.8603227149202752, 0.7334709241625984, 0.7334474051410083, 0.7264747402639247, 0.7344716495540463, 0.7333723373934493, 0.7400257469840834, 0.7326254850725233, 0.7347331739693774, 0.7361787942160427, 0.2900640312488526, 0.47241555583359396, 0.4068320718407943, 0.38522901052495495, 0.4980851072709567, 0.36017996570170985, 0.3855858146223492, 0.5301915371647596, 0.45367220095702276, 0.15700893469067034, 0.15086174717667622, 0.3606580261151082, 0.3820765391627686, 0.37857498203107787, 0.24358195595397236, 0.44006871328822617, 0.15619687169305285, 0.5431907915884118, 0.9514039641368535, 0.9599028786233561, 0.9703152109391081, 0.9689783778060999, 0.9634907937987585, 0.977053323860018, 0.9713025537743715, 0.9710944937497286, 0.9623817898603562, 0.4811632069641354, 0.4861494717101681, 0.5181873123716814, 0.4868754422257876, 0.5064595460782774, 0.5046714079907575, 0.48795856678445426, 0.5189360989349037, 0.4807572622639553, 0.5868371238845421, 0.6828198726555201, 0.6456891282317958, 0.5918970855007204, 0.6200859298334032, 0.7702265673351774, 0.6958432356063231, 0.7340731017036689, 0.7310117434316175, 0.2595202684690293, 0.2665440796289047, 0.3447806314120041, 0.2259147785649217, 0.27326843203813556, 0.32150330600063937, 0.28010562289764906, 0.29552641888490006, 0.32221432565738917, 0.12522923128088514, 0.21777955819361272, 0.14790043362950323, 0.27777879274959094, 0.2221290906104053, 0.2546698244616422, 0.28777697821894743, 0.25863731044319516, 0.20839151152332214, 0.08200045947449175, 0.1233998060896413, 0.09590295126948145, 0.11121731514540156, 0.08626097600496452, 0.10894404364206134, 0.08859205077834265, 0.08679856412127096, 0.07606669628336682, 0.21252808557164604, 0.22412982702821604, 0.20390446043916832, 0.22534486281182287, 0.16119227720619456, 0.17904454354061838, 0.21567655887183101, 0.3290280255186192, 0.2655490022975723, 0.00656682574070222, 0.030575438199327065, 0.03955418050983606, 0.02393376066661479, 0.05524215352879602, 0.07951077146365404, 0.03032444160772796, 0.08778729249895956, 0.07737778818842145, 0.19516566745890707, 0.1937347077828694, 0.19411490582830215, 0.21154639947615128, 0.20856128496804915, 0.20410401494498287, 0.18879954051159387, 0.188381415941566, 0.1647126038437029, 0.5421754637547482, 0.5695984805554837, 0.5600632409189952, 0.5552469615873223, 0.5724375125380385, 0.5565735237775709, 0.5734271428993263, 0.5412097018577038, 0.5590617205170056, 0.11818203894987733, 0.11594934899816578, 0.11548728812285303, 0.10910969597536346, 0.09324766404200946, 0.11297884701366567, 0.11728542093218253, 0.1358283894323311, 0.11510974598355384, 0.22540283983726417, 0.17103914371889317, 0.24935010026333593, 0.23139708991581187, 0.2116834840101205, 0.19135882098285617, 0.1705371299341023, 0.2126699572935572, 0.2209012418595554, 0.32685385154538005, 0.34314356484347186, 0.36988756800192024, 0.36952191550610336, 0.375443089514364, 0.2927272068359621, 0.41490454614332994, 0.4348893987389886, 0.3834987498697635, 0.23366535767951468, 0.25866189725809885, 0.2302530339437413, 0.22554822145020648, 0.24019648694644558, 0.21306175836868801, 0.23819888206020323, 0.27857543630058956, 0.25651404700229974, 0.18477259167128046, 0.19426795408013386, 0.19595086844004206, 0.20717051734986014, 0.19752325636984547, 0.22049037243377545, 0.19758769044907198, 0.2250685195540405, 0.1757806127460937, 0.2890409684568702, 0.2535622220515875, 0.40789869201342577, 0.36433021362110163, 0.43665556031431274, 0.20627895553091957, 0.2585961925724303, 0.43201072946856545, 0.21213576965002445, 0.7592369689589549, 0.2000334564012084, 0.19557859121926469, 0.7296203247163067, 0.18252932373457764, 0.6585988508625994, 0.6557618619990195, 0.17668310156736222, 0.4550001636102414, 0.1977316349668934, 0.20570403192958386, 0.6376280177947713, 0.5364671950603048, 0.5697194968937839, 0.6046270222808969, 0.19828487166059172, 0.20677602317736232, 0.20952700098802524, 0.17504352135657608, 0.1872149123248671, 0.18058125767784716, 0.18363327923768158, 0.1734550494379451, 0.1811900681761487, 0.19638412069592814, 0.1784610582440399, 0.1785635020273718, 0.08375222723055509, 0.08554103552116354, 0.07667496226636883, 0.08154009881601743, 0.07207178993966012, 0.06753014772800747, 0.09178354426194113, 0.08273888202466828, 0.08214048838785781]}, "mutation_prompt": null}
{"id": "c2064633-96a1-4e2e-8058-64ed2dcb1b42", "solution": "import numpy as np\n\nclass HybridStochasticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20  # could be tuned\n        self.scale_factor = 0.8\n        self.crossover_prob = 0.7\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n    \n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:  # If not evaluated yet\n                self.fitness[i] = func(self.population[i])\n                self.evaluations += 1\n    \n    def select_parents(self, idx):\n        indices = [i for i in range(self.population_size) if i != idx]\n        return np.random.choice(indices, 3, replace=False)\n    \n    def differential_evolution_mutation(self, idx):\n        a, b, c = self.select_parents(idx)\n        mutant_vector = self.population[a] + self.scale_factor * (self.population[b] - self.population[c])\n        return np.clip(mutant_vector, self.lower_bound, self.upper_bound)\n    \n    def crossover(self, target_vector, mutant_vector):\n        crossover_mask = np.random.rand(self.dim) < self.crossover_prob\n        return np.where(crossover_mask, mutant_vector, target_vector)\n    \n    def local_search(self, vector):\n        perturbation = np.random.normal(0, 0.1, self.dim)  # Small Gaussian perturbations\n        candidate_vector = vector + perturbation\n        return np.clip(candidate_vector, self.lower_bound, self.upper_bound)\n    \n    def __call__(self, func):\n        self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n                \n                mutant_vector = self.differential_evolution_mutation(i)\n                trial_vector = self.crossover(self.population[i], mutant_vector)\n                trial_fitness = func(trial_vector)\n                self.evaluations += 1\n                \n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial_vector\n                    self.fitness[i] = trial_fitness\n                \n                # Adaptive Local Search\n                if np.random.rand() < 0.2:  # Probability of local search\n                    candidate_vector = self.local_search(self.population[i])\n                    candidate_fitness = func(candidate_vector)\n                    self.evaluations += 1\n                    if candidate_fitness < self.fitness[i]:\n                        self.population[i] = candidate_vector\n                        self.fitness[i] = candidate_fitness\n        \n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx]", "name": "HybridStochasticOptimizer", "description": "A novel hybrid stochastic optimization algorithm combining Differential Evolution with Adaptive Local Search for efficient exploration and exploitation in high-dimensional spaces.", "configspace": "", "generation": 1, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "2778f81b-4fe3-49b5-ac9f-5e39c1e0f4f7", "metadata": {"aucs": [0.838550702247986, 0.8335387980441803, 0.8392883035317981, 0.8448220232438199, 0.8692280794153219, 0.86492167545159, 0.8423065034848981, 0.8684684289902511, 0.8603227149202752, 0.7334709241625984, 0.7334474051410083, 0.7264747402639247, 0.7344716495540463, 0.7333723373934493, 0.7400257469840834, 0.7326254850725233, 0.7347331739693774, 0.7361787942160427, 0.2900640312488526, 0.47241555583359396, 0.4068320718407943, 0.38522901052495495, 0.4980851072709567, 0.36017996570170985, 0.3855858146223492, 0.5301915371647596, 0.45367220095702276, 0.15700893469067034, 0.15086174717667622, 0.3606580261151082, 0.3820765391627686, 0.37857498203107787, 0.24358195595397236, 0.44006871328822617, 0.15619687169305285, 0.5431907915884118, 0.9514039641368535, 0.9599028786233561, 0.9703152109391081, 0.9689783778060999, 0.9634907937987585, 0.977053323860018, 0.9713025537743715, 0.9710944937497286, 0.9623817898603562, 0.4811632069641354, 0.4861494717101681, 0.5181873123716814, 0.4868754422257876, 0.5064595460782774, 0.5046714079907575, 0.48795856678445426, 0.5189360989349037, 0.4807572622639553, 0.5868371238845421, 0.6828198726555201, 0.6456891282317958, 0.5918970855007204, 0.6200859298334032, 0.7702265673351774, 0.6958432356063231, 0.7340731017036689, 0.7310117434316175, 0.2595202684690293, 0.2665440796289047, 0.3447806314120041, 0.2259147785649217, 0.27326843203813556, 0.32150330600063937, 0.28010562289764906, 0.29552641888490006, 0.32221432565738917, 0.12522923128088514, 0.21777955819361272, 0.14790043362950323, 0.27777879274959094, 0.2221290906104053, 0.2546698244616422, 0.28777697821894743, 0.25863731044319516, 0.20839151152332214, 0.08200045947449175, 0.1233998060896413, 0.09590295126948145, 0.11121731514540156, 0.08626097600496452, 0.10894404364206134, 0.08859205077834265, 0.08679856412127096, 0.07606669628336682, 0.21252808557164604, 0.22412982702821604, 0.20390446043916832, 0.22534486281182287, 0.16119227720619456, 0.17904454354061838, 0.21567655887183101, 0.3290280255186192, 0.2655490022975723, 0.00656682574070222, 0.030575438199327065, 0.03955418050983606, 0.02393376066661479, 0.05524215352879602, 0.07951077146365404, 0.03032444160772796, 0.08778729249895956, 0.07737778818842145, 0.19516566745890707, 0.1937347077828694, 0.19411490582830215, 0.21154639947615128, 0.20856128496804915, 0.20410401494498287, 0.18879954051159387, 0.188381415941566, 0.1647126038437029, 0.5421754637547482, 0.5695984805554837, 0.5600632409189952, 0.5552469615873223, 0.5724375125380385, 0.5565735237775709, 0.5734271428993263, 0.5412097018577038, 0.5590617205170056, 0.11818203894987733, 0.11594934899816578, 0.11548728812285303, 0.10910969597536346, 0.09324766404200946, 0.11297884701366567, 0.11728542093218253, 0.1358283894323311, 0.11510974598355384, 0.22540283983726417, 0.17103914371889317, 0.24935010026333593, 0.23139708991581187, 0.2116834840101205, 0.19135882098285617, 0.1705371299341023, 0.2126699572935572, 0.2209012418595554, 0.32685385154538005, 0.34314356484347186, 0.36988756800192024, 0.36952191550610336, 0.375443089514364, 0.2927272068359621, 0.41490454614332994, 0.4348893987389886, 0.3834987498697635, 0.23366535767951468, 0.25866189725809885, 0.2302530339437413, 0.22554822145020648, 0.24019648694644558, 0.21306175836868801, 0.23819888206020323, 0.27857543630058956, 0.25651404700229974, 0.18477259167128046, 0.19426795408013386, 0.19595086844004206, 0.20717051734986014, 0.19752325636984547, 0.22049037243377545, 0.19758769044907198, 0.2250685195540405, 0.1757806127460937, 0.2890409684568702, 0.2535622220515875, 0.40789869201342577, 0.36433021362110163, 0.43665556031431274, 0.20627895553091957, 0.2585961925724303, 0.43201072946856545, 0.21213576965002445, 0.7592369689589549, 0.2000334564012084, 0.19557859121926469, 0.7296203247163067, 0.18252932373457764, 0.6585988508625994, 0.6557618619990195, 0.17668310156736222, 0.4550001636102414, 0.1977316349668934, 0.20570403192958386, 0.6376280177947713, 0.5364671950603048, 0.5697194968937839, 0.6046270222808969, 0.19828487166059172, 0.20677602317736232, 0.20952700098802524, 0.17504352135657608, 0.1872149123248671, 0.18058125767784716, 0.18363327923768158, 0.1734550494379451, 0.1811900681761487, 0.19638412069592814, 0.1784610582440399, 0.1785635020273718, 0.08375222723055509, 0.08554103552116354, 0.07667496226636883, 0.08154009881601743, 0.07207178993966012, 0.06753014772800747, 0.09178354426194113, 0.08273888202466828, 0.08214048838785781]}, "mutation_prompt": null}
{"id": "fde6cbef-396b-4cd6-a090-a60787e29c23", "solution": "import numpy as np\n\nclass HybridStochasticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20  # could be tuned\n        self.scale_factor = 0.8\n        self.crossover_prob = 0.7\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n    \n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:  # If not evaluated yet\n                self.fitness[i] = func(self.population[i])\n                self.evaluations += 1\n    \n    def select_parents(self, idx):\n        indices = [i for i in range(self.population_size) if i != idx]\n        return np.random.choice(indices, 3, replace=False)\n    \n    def differential_evolution_mutation(self, idx):\n        a, b, c = self.select_parents(idx)\n        mutant_vector = self.population[a] + self.scale_factor * (self.population[b] - self.population[c])\n        return np.clip(mutant_vector, self.lower_bound, self.upper_bound)\n    \n    def crossover(self, target_vector, mutant_vector):\n        crossover_mask = np.random.rand(self.dim) < self.crossover_prob\n        return np.where(crossover_mask, mutant_vector, target_vector)\n    \n    def local_search(self, vector):\n        perturbation = np.random.normal(0, 0.1, self.dim)  # Small Gaussian perturbations\n        candidate_vector = vector + perturbation\n        return np.clip(candidate_vector, self.lower_bound, self.upper_bound)\n    \n    def __call__(self, func):\n        self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n                \n                mutant_vector = self.differential_evolution_mutation(i)\n                trial_vector = self.crossover(self.population[i], mutant_vector)\n                trial_fitness = func(trial_vector)\n                self.evaluations += 1\n                \n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial_vector\n                    self.fitness[i] = trial_fitness\n                \n                # Adaptive Local Search\n                if np.random.rand() < 0.2:  # Probability of local search\n                    candidate_vector = self.local_search(self.population[i])\n                    candidate_fitness = func(candidate_vector)\n                    self.evaluations += 1\n                    if candidate_fitness < self.fitness[i]:\n                        self.population[i] = candidate_vector\n                        self.fitness[i] = candidate_fitness\n        \n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx]", "name": "HybridStochasticOptimizer", "description": "A novel hybrid stochastic optimization algorithm combining Differential Evolution with Adaptive Local Search for efficient exploration and exploitation in high-dimensional spaces.", "configspace": "", "generation": 1, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "2778f81b-4fe3-49b5-ac9f-5e39c1e0f4f7", "metadata": {"aucs": [0.838550702247986, 0.8335387980441803, 0.8392883035317981, 0.8448220232438199, 0.8692280794153219, 0.86492167545159, 0.8423065034848981, 0.8684684289902511, 0.8603227149202752, 0.7334709241625984, 0.7334474051410083, 0.7264747402639247, 0.7344716495540463, 0.7333723373934493, 0.7400257469840834, 0.7326254850725233, 0.7347331739693774, 0.7361787942160427, 0.2900640312488526, 0.47241555583359396, 0.4068320718407943, 0.38522901052495495, 0.4980851072709567, 0.36017996570170985, 0.3855858146223492, 0.5301915371647596, 0.45367220095702276, 0.15700893469067034, 0.15086174717667622, 0.3606580261151082, 0.3820765391627686, 0.37857498203107787, 0.24358195595397236, 0.44006871328822617, 0.15619687169305285, 0.5431907915884118, 0.9514039641368535, 0.9599028786233561, 0.9703152109391081, 0.9689783778060999, 0.9634907937987585, 0.977053323860018, 0.9713025537743715, 0.9710944937497286, 0.9623817898603562, 0.4811632069641354, 0.4861494717101681, 0.5181873123716814, 0.4868754422257876, 0.5064595460782774, 0.5046714079907575, 0.48795856678445426, 0.5189360989349037, 0.4807572622639553, 0.5868371238845421, 0.6828198726555201, 0.6456891282317958, 0.5918970855007204, 0.6200859298334032, 0.7702265673351774, 0.6958432356063231, 0.7340731017036689, 0.7310117434316175, 0.2595202684690293, 0.2665440796289047, 0.3447806314120041, 0.2259147785649217, 0.27326843203813556, 0.32150330600063937, 0.28010562289764906, 0.29552641888490006, 0.32221432565738917, 0.12522923128088514, 0.21777955819361272, 0.14790043362950323, 0.27777879274959094, 0.2221290906104053, 0.2546698244616422, 0.28777697821894743, 0.25863731044319516, 0.20839151152332214, 0.08200045947449175, 0.1233998060896413, 0.09590295126948145, 0.11121731514540156, 0.08626097600496452, 0.10894404364206134, 0.08859205077834265, 0.08679856412127096, 0.07606669628336682, 0.21252808557164604, 0.22412982702821604, 0.20390446043916832, 0.22534486281182287, 0.16119227720619456, 0.17904454354061838, 0.21567655887183101, 0.3290280255186192, 0.2655490022975723, 0.00656682574070222, 0.030575438199327065, 0.03955418050983606, 0.02393376066661479, 0.05524215352879602, 0.07951077146365404, 0.03032444160772796, 0.08778729249895956, 0.07737778818842145, 0.19516566745890707, 0.1937347077828694, 0.19411490582830215, 0.21154639947615128, 0.20856128496804915, 0.20410401494498287, 0.18879954051159387, 0.188381415941566, 0.1647126038437029, 0.5421754637547482, 0.5695984805554837, 0.5600632409189952, 0.5552469615873223, 0.5724375125380385, 0.5565735237775709, 0.5734271428993263, 0.5412097018577038, 0.5590617205170056, 0.11818203894987733, 0.11594934899816578, 0.11548728812285303, 0.10910969597536346, 0.09324766404200946, 0.11297884701366567, 0.11728542093218253, 0.1358283894323311, 0.11510974598355384, 0.22540283983726417, 0.17103914371889317, 0.24935010026333593, 0.23139708991581187, 0.2116834840101205, 0.19135882098285617, 0.1705371299341023, 0.2126699572935572, 0.2209012418595554, 0.32685385154538005, 0.34314356484347186, 0.36988756800192024, 0.36952191550610336, 0.375443089514364, 0.2927272068359621, 0.41490454614332994, 0.4348893987389886, 0.3834987498697635, 0.23366535767951468, 0.25866189725809885, 0.2302530339437413, 0.22554822145020648, 0.24019648694644558, 0.21306175836868801, 0.23819888206020323, 0.27857543630058956, 0.25651404700229974, 0.18477259167128046, 0.19426795408013386, 0.19595086844004206, 0.20717051734986014, 0.19752325636984547, 0.22049037243377545, 0.19758769044907198, 0.2250685195540405, 0.1757806127460937, 0.2890409684568702, 0.2535622220515875, 0.40789869201342577, 0.36433021362110163, 0.43665556031431274, 0.20627895553091957, 0.2585961925724303, 0.43201072946856545, 0.21213576965002445, 0.7592369689589549, 0.2000334564012084, 0.19557859121926469, 0.7296203247163067, 0.18252932373457764, 0.6585988508625994, 0.6557618619990195, 0.17668310156736222, 0.4550001636102414, 0.1977316349668934, 0.20570403192958386, 0.6376280177947713, 0.5364671950603048, 0.5697194968937839, 0.6046270222808969, 0.19828487166059172, 0.20677602317736232, 0.20952700098802524, 0.17504352135657608, 0.1872149123248671, 0.18058125767784716, 0.18363327923768158, 0.1734550494379451, 0.1811900681761487, 0.19638412069592814, 0.1784610582440399, 0.1785635020273718, 0.08375222723055509, 0.08554103552116354, 0.07667496226636883, 0.08154009881601743, 0.07207178993966012, 0.06753014772800747, 0.09178354426194113, 0.08273888202466828, 0.08214048838785781]}, "mutation_prompt": null}
{"id": "88a6458e-9f34-49da-b12c-b7f600cd980d", "solution": "import numpy as np\n\nclass HybridStochasticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20  # could be tuned\n        self.scale_factor = 0.8\n        self.crossover_prob = 0.7\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n    \n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:  # If not evaluated yet\n                self.fitness[i] = func(self.population[i])\n                self.evaluations += 1\n    \n    def select_parents(self, idx):\n        indices = [i for i in range(self.population_size) if i != idx]\n        return np.random.choice(indices, 3, replace=False)\n    \n    def differential_evolution_mutation(self, idx):\n        a, b, c = self.select_parents(idx)\n        mutant_vector = self.population[a] + self.scale_factor * (self.population[b] - self.population[c])\n        return np.clip(mutant_vector, self.lower_bound, self.upper_bound)\n    \n    def crossover(self, target_vector, mutant_vector):\n        crossover_mask = np.random.rand(self.dim) < self.crossover_prob\n        return np.where(crossover_mask, mutant_vector, target_vector)\n    \n    def local_search(self, vector):\n        perturbation = np.random.normal(0, 0.1, self.dim)  # Small Gaussian perturbations\n        candidate_vector = vector + perturbation\n        return np.clip(candidate_vector, self.lower_bound, self.upper_bound)\n    \n    def __call__(self, func):\n        self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n                \n                mutant_vector = self.differential_evolution_mutation(i)\n                trial_vector = self.crossover(self.population[i], mutant_vector)\n                trial_fitness = func(trial_vector)\n                self.evaluations += 1\n                \n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial_vector\n                    self.fitness[i] = trial_fitness\n                \n                # Adaptive Local Search\n                if np.random.rand() < 0.2:  # Probability of local search\n                    candidate_vector = self.local_search(self.population[i])\n                    candidate_fitness = func(candidate_vector)\n                    self.evaluations += 1\n                    if candidate_fitness < self.fitness[i]:\n                        self.population[i] = candidate_vector\n                        self.fitness[i] = candidate_fitness\n        \n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx]", "name": "HybridStochasticOptimizer", "description": "A novel hybrid stochastic optimization algorithm combining Differential Evolution with Adaptive Local Search for efficient exploration and exploitation in high-dimensional spaces.", "configspace": "", "generation": 1, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "2778f81b-4fe3-49b5-ac9f-5e39c1e0f4f7", "metadata": {"aucs": [0.838550702247986, 0.8335387980441803, 0.8392883035317981, 0.8448220232438199, 0.8692280794153219, 0.86492167545159, 0.8423065034848981, 0.8684684289902511, 0.8603227149202752, 0.7334709241625984, 0.7334474051410083, 0.7264747402639247, 0.7344716495540463, 0.7333723373934493, 0.7400257469840834, 0.7326254850725233, 0.7347331739693774, 0.7361787942160427, 0.2900640312488526, 0.47241555583359396, 0.4068320718407943, 0.38522901052495495, 0.4980851072709567, 0.36017996570170985, 0.3855858146223492, 0.5301915371647596, 0.45367220095702276, 0.15700893469067034, 0.15086174717667622, 0.3606580261151082, 0.3820765391627686, 0.37857498203107787, 0.24358195595397236, 0.44006871328822617, 0.15619687169305285, 0.5431907915884118, 0.9514039641368535, 0.9599028786233561, 0.9703152109391081, 0.9689783778060999, 0.9634907937987585, 0.977053323860018, 0.9713025537743715, 0.9710944937497286, 0.9623817898603562, 0.4811632069641354, 0.4861494717101681, 0.5181873123716814, 0.4868754422257876, 0.5064595460782774, 0.5046714079907575, 0.48795856678445426, 0.5189360989349037, 0.4807572622639553, 0.5868371238845421, 0.6828198726555201, 0.6456891282317958, 0.5918970855007204, 0.6200859298334032, 0.7702265673351774, 0.6958432356063231, 0.7340731017036689, 0.7310117434316175, 0.2595202684690293, 0.2665440796289047, 0.3447806314120041, 0.2259147785649217, 0.27326843203813556, 0.32150330600063937, 0.28010562289764906, 0.29552641888490006, 0.32221432565738917, 0.12522923128088514, 0.21777955819361272, 0.14790043362950323, 0.27777879274959094, 0.2221290906104053, 0.2546698244616422, 0.28777697821894743, 0.25863731044319516, 0.20839151152332214, 0.08200045947449175, 0.1233998060896413, 0.09590295126948145, 0.11121731514540156, 0.08626097600496452, 0.10894404364206134, 0.08859205077834265, 0.08679856412127096, 0.07606669628336682, 0.21252808557164604, 0.22412982702821604, 0.20390446043916832, 0.22534486281182287, 0.16119227720619456, 0.17904454354061838, 0.21567655887183101, 0.3290280255186192, 0.2655490022975723, 0.00656682574070222, 0.030575438199327065, 0.03955418050983606, 0.02393376066661479, 0.05524215352879602, 0.07951077146365404, 0.03032444160772796, 0.08778729249895956, 0.07737778818842145, 0.19516566745890707, 0.1937347077828694, 0.19411490582830215, 0.21154639947615128, 0.20856128496804915, 0.20410401494498287, 0.18879954051159387, 0.188381415941566, 0.1647126038437029, 0.5421754637547482, 0.5695984805554837, 0.5600632409189952, 0.5552469615873223, 0.5724375125380385, 0.5565735237775709, 0.5734271428993263, 0.5412097018577038, 0.5590617205170056, 0.11818203894987733, 0.11594934899816578, 0.11548728812285303, 0.10910969597536346, 0.09324766404200946, 0.11297884701366567, 0.11728542093218253, 0.1358283894323311, 0.11510974598355384, 0.22540283983726417, 0.17103914371889317, 0.24935010026333593, 0.23139708991581187, 0.2116834840101205, 0.19135882098285617, 0.1705371299341023, 0.2126699572935572, 0.2209012418595554, 0.32685385154538005, 0.34314356484347186, 0.36988756800192024, 0.36952191550610336, 0.375443089514364, 0.2927272068359621, 0.41490454614332994, 0.4348893987389886, 0.3834987498697635, 0.23366535767951468, 0.25866189725809885, 0.2302530339437413, 0.22554822145020648, 0.24019648694644558, 0.21306175836868801, 0.23819888206020323, 0.27857543630058956, 0.25651404700229974, 0.18477259167128046, 0.19426795408013386, 0.19595086844004206, 0.20717051734986014, 0.19752325636984547, 0.22049037243377545, 0.19758769044907198, 0.2250685195540405, 0.1757806127460937, 0.2890409684568702, 0.2535622220515875, 0.40789869201342577, 0.36433021362110163, 0.43665556031431274, 0.20627895553091957, 0.2585961925724303, 0.43201072946856545, 0.21213576965002445, 0.7592369689589549, 0.2000334564012084, 0.19557859121926469, 0.7296203247163067, 0.18252932373457764, 0.6585988508625994, 0.6557618619990195, 0.17668310156736222, 0.4550001636102414, 0.1977316349668934, 0.20570403192958386, 0.6376280177947713, 0.5364671950603048, 0.5697194968937839, 0.6046270222808969, 0.19828487166059172, 0.20677602317736232, 0.20952700098802524, 0.17504352135657608, 0.1872149123248671, 0.18058125767784716, 0.18363327923768158, 0.1734550494379451, 0.1811900681761487, 0.19638412069592814, 0.1784610582440399, 0.1785635020273718, 0.08375222723055509, 0.08554103552116354, 0.07667496226636883, 0.08154009881601743, 0.07207178993966012, 0.06753014772800747, 0.09178354426194113, 0.08273888202466828, 0.08214048838785781]}, "mutation_prompt": null}
{"id": "423013c1-e76b-4a46-a974-0a1e87c32656", "solution": "import numpy as np\n\nclass HybridStochasticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.initial_scale_factor = 0.8\n        self.initial_crossover_prob = 0.7\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def select_parents(self, idx):\n        indices = [i for i in range(self.population_size) if i != idx]\n        return np.random.choice(indices, 3, replace=False)\n\n    def differential_evolution_mutation(self, idx, scale_factor):\n        a, b, c = self.select_parents(idx)\n        mutant_vector = self.population[a] + scale_factor * (self.population[b] - self.population[c])\n        return np.clip(mutant_vector, self.lower_bound, self.upper_bound)\n\n    def crossover(self, target_vector, mutant_vector, crossover_prob):\n        crossover_mask = np.random.rand(self.dim) < crossover_prob\n        return np.where(crossover_mask, mutant_vector, target_vector)\n\n    def local_search(self, vector):\n        perturbation = np.random.normal(0, 0.1, self.dim)\n        candidate_vector = vector + perturbation\n        return np.clip(candidate_vector, self.lower_bound, self.upper_bound)\n\n    def __call__(self, func):\n        self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                # Dynamic adjustment of parameters\n                scale_factor = self.initial_scale_factor * (1 - self.evaluations / self.budget)\n                crossover_prob = self.initial_crossover_prob + 0.2 * np.random.rand()\n\n                mutant_vector = self.differential_evolution_mutation(i, scale_factor)\n                trial_vector = self.crossover(self.population[i], mutant_vector, crossover_prob)\n                trial_fitness = func(trial_vector)\n                self.evaluations += 1\n\n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial_vector\n                    self.fitness[i] = trial_fitness\n\n                if np.random.rand() < 0.2:\n                    candidate_vector = self.local_search(self.population[i])\n                    candidate_fitness = func(candidate_vector)\n                    self.evaluations += 1\n                    if candidate_fitness < self.fitness[i]:\n                        self.population[i] = candidate_vector\n                        self.fitness[i] = candidate_fitness\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx]", "name": "HybridStochasticOptimizer", "description": "Enhanced hybrid optimizer with dynamic adaptation of crossover probability and scaling factor for faster convergence.", "configspace": "", "generation": 24, "fitness": 0.3976543160003503, "feedback": "The algorithm HybridStochasticOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.40 with standard deviation 0.27.", "error": "", "parent_id": "2778f81b-4fe3-49b5-ac9f-5e39c1e0f4f7", "metadata": {"aucs": [0.8713253338612517, 0.8512264743009399, 0.8665121170890773, 0.8796666233130291, 0.8761709853642955, 0.8810455847898071, 0.8634015516116982, 0.8659759235104127, 0.8658848386058079, 0.776022590581398, 0.7775385630940745, 0.7715872013079406, 0.047937183521818194, 0.7897493501520839, 0.7767867190753118, 0.7886305728092413, 0.7940131006001782, 0.7926813538126418, 0.2543563301263122, 0.26386842411152445, 0.1645250607126243, 0.4926752876196465, 0.39390858976277254, 0.23089210161864804, 0.5519002442996506, 0.16253539159378672, 0.16045598739732325, 0.1815564450288426, 0.16204045436991144, 0.14067199300734445, 0.13864525641376824, 0.17606506326955895, 0.1957449332854192, 0.1355796022600363, 0.15333624021469605, 0.19466446030704987, 0.9685443461788876, 0.9826362409248943, 0.9718102519988508, 0.9542477023870188, 0.9651450868978637, 0.9656076584362225, 0.9771087121751363, 0.9511676926846383, 0.9475409484430742, 0.5607961727722646, 0.5801208623770812, 0.4205876330708098, 0.5598664192930076, 0.641736637347754, 0.6059314552023902, 0.6535048493722003, 0.4301252012365522, 0.593391845674108, 0.775267609832766, 0.8220462344793056, 0.8165110291416555, 0.8099276940402154, 0.791912050573996, 0.7779888994355462, 0.814979996330171, 0.839390998760543, 0.8266411896693742, 0.20622207396884407, 0.19244687757537238, 0.2291070214062273, 0.18910464301479657, 0.2541653190233828, 0.1928968623624615, 0.1871271567996714, 0.17729889003910626, 0.22875603210790119, 0.12377566933015582, 0.2950653070917044, 0.23538384513456279, 0.15493565340425008, 0.19422285439103282, 0.1502145052953019, 0.2135497527157869, 0.1784558028745068, 0.199881013024794, 0.27160560898079356, 0.1600610706552127, 0.2966798941009102, 0.33197548409745026, 0.3273123800343034, 0.13175146719037434, 0.12734955559720107, 0.29523541535899633, 0.2581989142537512, 0.3057281543095325, 0.33719377902877445, 0.3273073264000551, 0.5237285877417179, 0.22812274619373252, 0.4261137114170931, 0.5799324829661097, 0.48369941272309647, 0.40326481318610863, 0.22327653681808668, 0.08132520060140336, 0.14102936435841418, 0.14786063109413006, 0.2157013252409571, 0.18220254459858531, 0.21794919718423955, 0.14371731913855168, 0.11264274046047196, 0.3238631138742195, 0.22699529900734483, 0.35859983983891375, 0.31015402062999375, 0.342704152103683, 0.30880869674110023, 0.30063474713777893, 0.2537866527590352, 0.32753736436104197, 0.6360732566039109, 0.7169071184070772, 0.6514127383341239, 0.719287291870321, 0.6970133694714913, 0.7270834504024826, 0.710179103041517, 0.6668939850533554, 0.7510629933966516, 0.11880847619771395, 0.1275411723472576, 0.14140762050142752, 0.11942035574005871, 0.12173741895648504, 0.1144597314207404, 0.11282612959351568, 0.11793734558544844, 0.13838865877543505, 0.2136322473775769, 0.18818818345769583, 0.16984149507488888, 0.21135743604782797, 0.3332439425963074, 0.2707205189155363, 0.19645333708691226, 0.5255360345277968, 0.17491835873550066, 0.491136123556865, 0.5455620249867844, 0.4823667625503876, 0.5137745939223461, 0.6007429826145017, 0.5373075928905169, 0.5325703968031614, 0.5471600999633488, 0.5520970820614712, 0.34138421101666905, 0.344345094422873, 0.4386890897074446, 0.3927652946470016, 0.3559656722913527, 0.3312468809292709, 0.4168609025141642, 0.43050715057973565, 0.3995219869727502, 0.20188303468677726, 0.19074883185916647, 0.22555193488946368, 0.197801756120222, 0.18776985961827763, 0.21111851559573747, 0.19943543047996204, 0.2100039743419102, 0.22390992876081084, 0.5191491482875619, 0.30489357992928057, 0.36033594079926956, 0.2286161552903242, 0.5548591242408711, 0.548446861384704, 0.233481806613284, 0.27773770979346313, 0.6660977084626925, 0.7802746310921849, 0.16495810395294486, 0.838783034616159, 0.7762080583089825, 0.19309572939016661, 0.18732679941670738, 0.838996832827352, 0.19522887632217467, 0.16643636185166943, 0.1657327490812559, 0.3878812419136092, 0.2063933306589294, 0.20057790551290378, 0.767187864707388, 0.6356942092227594, 0.20992783516937807, 0.21115377384679346, 0.1252279539817771, 0.18052555363699052, 0.19692669640269667, 0.1876866139231771, 0.18697424131949403, 0.1844832151097614, 0.18881261937043448, 0.21058133373873567, 0.18701017860257085, 0.18449711484346276, 0.07448065883908206, 0.07847839379784016, 0.07977837032412294, 0.10174659968374122, 0.08096874614344629, 0.08136994327246028, 0.08321262575864308, 0.08622905371754608, 0.06953581133556275]}, "mutation_prompt": null}
{"id": "77af05b9-ec36-422c-a7ce-850e4806136a", "solution": "import numpy as np\n\nclass HybridStochasticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.initial_scale_factor = 0.8\n        self.initial_crossover_prob = 0.7\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def select_parents(self, idx):\n        indices = [i for i in range(self.population_size) if i != idx]\n        return np.random.choice(indices, 3, replace=False)\n\n    def differential_evolution_mutation(self, idx, scale_factor):\n        a, b, c = self.select_parents(idx)\n        mutant_vector = self.population[a] + scale_factor * (self.population[b] - self.population[c])\n        return np.clip(mutant_vector, self.lower_bound, self.upper_bound)\n\n    def crossover(self, target_vector, mutant_vector, crossover_prob):\n        crossover_mask = np.random.rand(self.dim) < crossover_prob\n        return np.where(crossover_mask, mutant_vector, target_vector)\n\n    def local_search(self, vector):\n        perturbation = np.random.normal(0, 0.1, self.dim)\n        candidate_vector = vector + perturbation\n        return np.clip(candidate_vector, self.lower_bound, self.upper_bound)\n\n    def __call__(self, func):\n        self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                # Dynamic adjustment of parameters\n                scale_factor = self.initial_scale_factor * (1 - self.evaluations / self.budget)\n                crossover_prob = self.initial_crossover_prob + 0.2 * np.random.rand()\n\n                mutant_vector = self.differential_evolution_mutation(i, scale_factor)\n                trial_vector = self.crossover(self.population[i], mutant_vector, crossover_prob)\n                trial_fitness = func(trial_vector)\n                self.evaluations += 1\n\n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial_vector\n                    self.fitness[i] = trial_fitness\n\n                if np.random.rand() < 0.2:\n                    candidate_vector = self.local_search(self.population[i])\n                    candidate_fitness = func(candidate_vector)\n                    self.evaluations += 1\n                    if candidate_fitness < self.fitness[i]:\n                        self.population[i] = candidate_vector\n                        self.fitness[i] = candidate_fitness\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx]", "name": "HybridStochasticOptimizer", "description": "Enhanced hybrid optimizer with dynamic adaptation of crossover probability and scaling factor for faster convergence.", "configspace": "", "generation": 25, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "423013c1-e76b-4a46-a974-0a1e87c32656", "metadata": {"aucs": [0.8713253338612517, 0.8512264743009399, 0.8665121170890773, 0.8796666233130291, 0.8761709853642955, 0.8810455847898071, 0.8634015516116982, 0.8659759235104127, 0.8658848386058079, 0.776022590581398, 0.7775385630940745, 0.7715872013079406, 0.047937183521818194, 0.7897493501520839, 0.7767867190753118, 0.7886305728092413, 0.7940131006001782, 0.7926813538126418, 0.2543563301263122, 0.26386842411152445, 0.1645250607126243, 0.4926752876196465, 0.39390858976277254, 0.23089210161864804, 0.5519002442996506, 0.16253539159378672, 0.16045598739732325, 0.1815564450288426, 0.16204045436991144, 0.14067199300734445, 0.13864525641376824, 0.17606506326955895, 0.1957449332854192, 0.1355796022600363, 0.15333624021469605, 0.19466446030704987, 0.9685443461788876, 0.9826362409248943, 0.9718102519988508, 0.9542477023870188, 0.9651450868978637, 0.9656076584362225, 0.9771087121751363, 0.9511676926846383, 0.9475409484430742, 0.5607961727722646, 0.5801208623770812, 0.4205876330708098, 0.5598664192930076, 0.641736637347754, 0.6059314552023902, 0.6535048493722003, 0.4301252012365522, 0.593391845674108, 0.775267609832766, 0.8220462344793056, 0.8165110291416555, 0.8099276940402154, 0.791912050573996, 0.7779888994355462, 0.814979996330171, 0.839390998760543, 0.8266411896693742, 0.20622207396884407, 0.19244687757537238, 0.2291070214062273, 0.18910464301479657, 0.2541653190233828, 0.1928968623624615, 0.1871271567996714, 0.17729889003910626, 0.22875603210790119, 0.12377566933015582, 0.2950653070917044, 0.23538384513456279, 0.15493565340425008, 0.19422285439103282, 0.1502145052953019, 0.2135497527157869, 0.1784558028745068, 0.199881013024794, 0.27160560898079356, 0.1600610706552127, 0.2966798941009102, 0.33197548409745026, 0.3273123800343034, 0.13175146719037434, 0.12734955559720107, 0.29523541535899633, 0.2581989142537512, 0.3057281543095325, 0.33719377902877445, 0.3273073264000551, 0.5237285877417179, 0.22812274619373252, 0.4261137114170931, 0.5799324829661097, 0.48369941272309647, 0.40326481318610863, 0.22327653681808668, 0.08132520060140336, 0.14102936435841418, 0.14786063109413006, 0.2157013252409571, 0.18220254459858531, 0.21794919718423955, 0.14371731913855168, 0.11264274046047196, 0.3238631138742195, 0.22699529900734483, 0.35859983983891375, 0.31015402062999375, 0.342704152103683, 0.30880869674110023, 0.30063474713777893, 0.2537866527590352, 0.32753736436104197, 0.6360732566039109, 0.7169071184070772, 0.6514127383341239, 0.719287291870321, 0.6970133694714913, 0.7270834504024826, 0.710179103041517, 0.6668939850533554, 0.7510629933966516, 0.11880847619771395, 0.1275411723472576, 0.14140762050142752, 0.11942035574005871, 0.12173741895648504, 0.1144597314207404, 0.11282612959351568, 0.11793734558544844, 0.13838865877543505, 0.2136322473775769, 0.18818818345769583, 0.16984149507488888, 0.21135743604782797, 0.3332439425963074, 0.2707205189155363, 0.19645333708691226, 0.5255360345277968, 0.17491835873550066, 0.491136123556865, 0.5455620249867844, 0.4823667625503876, 0.5137745939223461, 0.6007429826145017, 0.5373075928905169, 0.5325703968031614, 0.5471600999633488, 0.5520970820614712, 0.34138421101666905, 0.344345094422873, 0.4386890897074446, 0.3927652946470016, 0.3559656722913527, 0.3312468809292709, 0.4168609025141642, 0.43050715057973565, 0.3995219869727502, 0.20188303468677726, 0.19074883185916647, 0.22555193488946368, 0.197801756120222, 0.18776985961827763, 0.21111851559573747, 0.19943543047996204, 0.2100039743419102, 0.22390992876081084, 0.5191491482875619, 0.30489357992928057, 0.36033594079926956, 0.2286161552903242, 0.5548591242408711, 0.548446861384704, 0.233481806613284, 0.27773770979346313, 0.6660977084626925, 0.7802746310921849, 0.16495810395294486, 0.838783034616159, 0.7762080583089825, 0.19309572939016661, 0.18732679941670738, 0.838996832827352, 0.19522887632217467, 0.16643636185166943, 0.1657327490812559, 0.3878812419136092, 0.2063933306589294, 0.20057790551290378, 0.767187864707388, 0.6356942092227594, 0.20992783516937807, 0.21115377384679346, 0.1252279539817771, 0.18052555363699052, 0.19692669640269667, 0.1876866139231771, 0.18697424131949403, 0.1844832151097614, 0.18881261937043448, 0.21058133373873567, 0.18701017860257085, 0.18449711484346276, 0.07448065883908206, 0.07847839379784016, 0.07977837032412294, 0.10174659968374122, 0.08096874614344629, 0.08136994327246028, 0.08321262575864308, 0.08622905371754608, 0.06953581133556275]}, "mutation_prompt": null}
{"id": "6ed9defc-a376-419f-9c8e-d25792a85302", "solution": "import numpy as np\n\nclass HybridStochasticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 25  # Increased for better diversity\n        self.initial_scale_factor = 0.9  # Slightly higher for more exploration\n        self.initial_crossover_prob = 0.75  # More stringent crossover probability\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def select_parents(self, idx):\n        indices = [i for i in range(self.population_size) if i != idx]\n        return np.random.choice(indices, 3, replace=False)\n\n    def differential_evolution_mutation(self, idx, scale_factor):\n        a, b, c = self.select_parents(idx)\n        mutant_vector = self.population[a] + scale_factor * (self.population[b] - self.population[c])\n        return np.clip(mutant_vector, self.lower_bound, self.upper_bound)\n\n    def crossover(self, target_vector, mutant_vector, crossover_prob):\n        crossover_mask = np.random.rand(self.dim) < crossover_prob\n        return np.where(crossover_mask, mutant_vector, target_vector)\n\n    def gradient_inspired_local_search(self, vector, func):\n        gradient_step = np.random.normal(0, 0.05, self.dim)  # Smaller step for fine-tuning\n        perturbed_vector = vector + gradient_step\n        current_fitness = func(vector)\n        perturbed_fitness = func(perturbed_vector)\n        self.evaluations += 2\n        if perturbed_fitness < current_fitness:\n            return np.clip(perturbed_vector, self.lower_bound, self.upper_bound)\n        return vector\n\n    def __call__(self, func):\n        self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                # Dynamic adjustment of parameters\n                scale_factor = self.initial_scale_factor * (1 - self.evaluations / self.budget)\n                crossover_prob = self.initial_crossover_prob + 0.1 * np.random.rand()  # Reduced randomness\n\n                mutant_vector = self.differential_evolution_mutation(i, scale_factor)\n                trial_vector = self.crossover(self.population[i], mutant_vector, crossover_prob)\n                trial_fitness = func(trial_vector)\n                self.evaluations += 1\n\n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial_vector\n                    self.fitness[i] = trial_fitness\n\n                if np.random.rand() < 0.25:  # Slightly increased probability for local search\n                    candidate_vector = self.gradient_inspired_local_search(self.population[i], func)\n                    candidate_fitness = func(candidate_vector)\n                    self.evaluations += 1\n                    if candidate_fitness < self.fitness[i]:\n                        self.population[i] = candidate_vector\n                        self.fitness[i] = candidate_fitness\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx]", "name": "HybridStochasticOptimizer", "description": "Adaptive Differential Evolution with Gradient-Inspired Local Search for accelerated convergence.", "configspace": "", "generation": 26, "fitness": 0.31135413479076146, "feedback": "The algorithm HybridStochasticOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.31 with standard deviation 0.23.", "error": "", "parent_id": "423013c1-e76b-4a46-a974-0a1e87c32656", "metadata": {"aucs": [0.7329257143861919, 0.7591293962558138, 0.7580416694834646, 0.7859760332384469, 0.7588497018670584, 0.7806786273883892, 0.768124696513117, 0.7407398101799593, 0.7638439611002793, 0.6388081536184244, 0.6297033948717532, 0.6232771318069379, 0.6256331433033075, 0.6234485149799356, 0.6026993855959273, 0.6253957231444229, 0.5876237569598244, 0.6141159965856122, 0.13759675079663303, 0.16430938561512654, 0.13005975320501584, 0.14299694477054203, 0.17751520742888538, 0.1869190155788154, 0.1896019041651319, 0.160380163756917, 0.15875425500333062, 0.11540563313361274, 0.12651645073067574, 0.11990563163756085, 0.17735560514103088, 0.1250959130648054, 0.13303783098859645, 0.15307051817210882, 0.10810592176873013, 0.11520969326081054, 0.9567664371495217, 0.9773659070967657, 0.9653675762324477, 0.9538958681754236, 0.975292881519738, 0.9622918182214295, 0.9351380296136285, 0.9555804897306159, 0.9538866179515785, 0.40748359179331517, 0.3352019321844757, 0.3884888451115315, 0.34799468014030677, 0.34766798325405646, 0.4259300686184111, 0.33141117354643235, 0.32961146523132223, 0.3896611523554949, 0.653322202493267, 0.5433980145456216, 0.5598348425153868, 0.6294572891681969, 0.677152645655708, 0.6316419586160009, 0.6672076349456176, 0.5993395375672579, 0.6198508701609253, 0.17179420952458224, 0.16111119727428758, 0.2707656162841062, 0.15424225265531732, 0.17205936872347505, 0.2724622460299668, 0.18230994499678832, 0.20821922981567487, 0.18737253342263316, 0.1703397976129516, 0.24997236557324476, 0.19546331556239027, 0.1554772024034644, 0.16591690353125144, 0.19905145607938335, 0.13374716416492027, 0.15882526297779898, 0.14492405512549011, 0.2187510411129414, 0.12035474028282567, 0.11492041518186213, 0.13700400126550316, 0.1814249023432637, 0.15963393240765467, 0.095462071510926, 0.13883938318658007, 0.17999214839725064, 0.3113235197130284, 0.26987686500210306, 0.25069625560305153, 0.2504695337903434, 0.24347350591358652, 0.23020082625822547, 0.23724588409137937, 0.30192426608868195, 0.33470563250809715, 0.059531448822336075, 0.06663093935582198, 0.07310366802552015, 0.12283289863592728, 0.1115913208114343, 0.11078449980182126, 0.08759877746685052, 0.08700641279057819, 0.14259193842924944, 0.20749884526702078, 0.1783661893662296, 0.20126362629141403, 0.2569706966096301, 0.21683797308421804, 0.1933729635430007, 0.15221236118516712, 0.19638096684729134, 0.21685618482819102, 0.5689804062137573, 0.5650886674618208, 0.5880539930209823, 0.549079469704192, 0.579973457600528, 0.5259902101400835, 0.5485858509583361, 0.6033119059605283, 0.5418495284859521, 0.10586239001453468, 0.1176369998370792, 0.10547192406850903, 0.10322197890724494, 0.09635897460957188, 0.11550212509920743, 0.11093155256564691, 0.10907237978467965, 0.11184349220795098, 0.2931744479488998, 0.17616763380544198, 0.2666559554982718, 0.19658822279871435, 0.1934053501002735, 0.19396996249009657, 0.23848335870913429, 0.2001628499958239, 0.16356825152390975, 0.3605002894743171, 0.36204875594486496, 0.3818796091911474, 0.3468567402162831, 0.33917384656814176, 0.34353297785989667, 0.4303879784877017, 0.4067555536844716, 0.4122303995477341, 0.2577847869814708, 0.25899591224111007, 0.2311015970865129, 0.2544135311969016, 0.2588857718516797, 0.23613639476923376, 0.30027306101724693, 0.25671749505712405, 0.2813814050135335, 0.18091604395485084, 0.18634176294343063, 0.19888849894162564, 0.16759385140188787, 0.2102954855051079, 0.2020512077424722, 0.17542695171913858, 0.20433198675125241, 0.18486823911676598, 0.19578829390124497, 0.2160761321241813, 0.19948771470547, 0.19900366448446127, 0.19758576222128255, 0.1965015088793255, 0.24490284198903955, 0.27183118051451716, 0.21018637093614823, 0.34907072941315365, 0.1534198773143013, 0.15931059528683822, 0.1764876454152431, 0.17815692950263273, 0.1743608990263973, 0.17614830631453404, 0.2836883071661003, 0.4615760918526025, 0.1546300906072906, 0.2524335271995707, 0.1912453969221969, 0.6339412923643367, 0.39701558341957066, 0.18737610582109532, 0.20191985398307977, 0.691390250370937, 0.35809021853645073, 0.1794603913624696, 0.17441974837579688, 0.18271252781067382, 0.1866083275948266, 0.1841281365056856, 0.18972785753859034, 0.19628572327117844, 0.19640741878852064, 0.18443184788427058, 0.07289603198587369, 0.09076461044319917, 0.08080859168138776, 0.07438228980102357, 0.07254992508910763, 0.07353452605461586, 0.07745289241364317, 0.07422090902875567, 0.07843705445737736]}, "mutation_prompt": null}
{"id": "de4ce810-8803-4663-b810-12eb2c0a0b61", "solution": "import numpy as np\n\nclass HybridStochasticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.initial_scale_factor = 0.8\n        self.initial_crossover_prob = 0.7\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def select_parents(self, idx):\n        indices = [i for i in range(self.population_size) if i != idx]\n        return np.random.choice(indices, 3, replace=False)\n\n    def differential_evolution_mutation(self, idx, scale_factor):\n        a, b, c = self.select_parents(idx)\n        mutant_vector = self.population[a] + scale_factor * (self.population[b] - self.population[c])\n        return np.clip(mutant_vector, self.lower_bound, self.upper_bound)\n\n    def crossover(self, target_vector, mutant_vector, crossover_prob):\n        crossover_mask = np.random.rand(self.dim) < crossover_prob\n        return np.where(crossover_mask, mutant_vector, target_vector)\n\n    def local_search(self, vector):\n        perturbation = np.random.normal(0, 0.1, self.dim)\n        candidate_vector = vector + perturbation\n        return np.clip(candidate_vector, self.lower_bound, self.upper_bound)\n\n    def __call__(self, func):\n        self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                # Dynamic adjustment of parameters\n                scale_factor = self.initial_scale_factor * (1 - self.evaluations / self.budget)\n                crossover_prob = self.initial_crossover_prob + 0.2 * np.random.rand()\n\n                mutant_vector = self.differential_evolution_mutation(i, scale_factor)\n                trial_vector = self.crossover(self.population[i], mutant_vector, crossover_prob)\n                trial_fitness = func(trial_vector)\n                self.evaluations += 1\n\n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial_vector\n                    self.fitness[i] = trial_fitness\n\n                if np.random.rand() < 0.2:\n                    candidate_vector = self.local_search(self.population[i])\n                    candidate_fitness = func(candidate_vector)\n                    self.evaluations += 1\n                    if candidate_fitness < self.fitness[i]:\n                        self.population[i] = candidate_vector\n                        self.fitness[i] = candidate_fitness\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx]", "name": "HybridStochasticOptimizer", "description": "Enhanced hybrid optimizer with dynamic adaptation of crossover probability and scaling factor for faster convergence.", "configspace": "", "generation": 25, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "423013c1-e76b-4a46-a974-0a1e87c32656", "metadata": {"aucs": [0.8713253338612517, 0.8512264743009399, 0.8665121170890773, 0.8796666233130291, 0.8761709853642955, 0.8810455847898071, 0.8634015516116982, 0.8659759235104127, 0.8658848386058079, 0.776022590581398, 0.7775385630940745, 0.7715872013079406, 0.047937183521818194, 0.7897493501520839, 0.7767867190753118, 0.7886305728092413, 0.7940131006001782, 0.7926813538126418, 0.2543563301263122, 0.26386842411152445, 0.1645250607126243, 0.4926752876196465, 0.39390858976277254, 0.23089210161864804, 0.5519002442996506, 0.16253539159378672, 0.16045598739732325, 0.1815564450288426, 0.16204045436991144, 0.14067199300734445, 0.13864525641376824, 0.17606506326955895, 0.1957449332854192, 0.1355796022600363, 0.15333624021469605, 0.19466446030704987, 0.9685443461788876, 0.9826362409248943, 0.9718102519988508, 0.9542477023870188, 0.9651450868978637, 0.9656076584362225, 0.9771087121751363, 0.9511676926846383, 0.9475409484430742, 0.5607961727722646, 0.5801208623770812, 0.4205876330708098, 0.5598664192930076, 0.641736637347754, 0.6059314552023902, 0.6535048493722003, 0.4301252012365522, 0.593391845674108, 0.775267609832766, 0.8220462344793056, 0.8165110291416555, 0.8099276940402154, 0.791912050573996, 0.7779888994355462, 0.814979996330171, 0.839390998760543, 0.8266411896693742, 0.20622207396884407, 0.19244687757537238, 0.2291070214062273, 0.18910464301479657, 0.2541653190233828, 0.1928968623624615, 0.1871271567996714, 0.17729889003910626, 0.22875603210790119, 0.12377566933015582, 0.2950653070917044, 0.23538384513456279, 0.15493565340425008, 0.19422285439103282, 0.1502145052953019, 0.2135497527157869, 0.1784558028745068, 0.199881013024794, 0.27160560898079356, 0.1600610706552127, 0.2966798941009102, 0.33197548409745026, 0.3273123800343034, 0.13175146719037434, 0.12734955559720107, 0.29523541535899633, 0.2581989142537512, 0.3057281543095325, 0.33719377902877445, 0.3273073264000551, 0.5237285877417179, 0.22812274619373252, 0.4261137114170931, 0.5799324829661097, 0.48369941272309647, 0.40326481318610863, 0.22327653681808668, 0.08132520060140336, 0.14102936435841418, 0.14786063109413006, 0.2157013252409571, 0.18220254459858531, 0.21794919718423955, 0.14371731913855168, 0.11264274046047196, 0.3238631138742195, 0.22699529900734483, 0.35859983983891375, 0.31015402062999375, 0.342704152103683, 0.30880869674110023, 0.30063474713777893, 0.2537866527590352, 0.32753736436104197, 0.6360732566039109, 0.7169071184070772, 0.6514127383341239, 0.719287291870321, 0.6970133694714913, 0.7270834504024826, 0.710179103041517, 0.6668939850533554, 0.7510629933966516, 0.11880847619771395, 0.1275411723472576, 0.14140762050142752, 0.11942035574005871, 0.12173741895648504, 0.1144597314207404, 0.11282612959351568, 0.11793734558544844, 0.13838865877543505, 0.2136322473775769, 0.18818818345769583, 0.16984149507488888, 0.21135743604782797, 0.3332439425963074, 0.2707205189155363, 0.19645333708691226, 0.5255360345277968, 0.17491835873550066, 0.491136123556865, 0.5455620249867844, 0.4823667625503876, 0.5137745939223461, 0.6007429826145017, 0.5373075928905169, 0.5325703968031614, 0.5471600999633488, 0.5520970820614712, 0.34138421101666905, 0.344345094422873, 0.4386890897074446, 0.3927652946470016, 0.3559656722913527, 0.3312468809292709, 0.4168609025141642, 0.43050715057973565, 0.3995219869727502, 0.20188303468677726, 0.19074883185916647, 0.22555193488946368, 0.197801756120222, 0.18776985961827763, 0.21111851559573747, 0.19943543047996204, 0.2100039743419102, 0.22390992876081084, 0.5191491482875619, 0.30489357992928057, 0.36033594079926956, 0.2286161552903242, 0.5548591242408711, 0.548446861384704, 0.233481806613284, 0.27773770979346313, 0.6660977084626925, 0.7802746310921849, 0.16495810395294486, 0.838783034616159, 0.7762080583089825, 0.19309572939016661, 0.18732679941670738, 0.838996832827352, 0.19522887632217467, 0.16643636185166943, 0.1657327490812559, 0.3878812419136092, 0.2063933306589294, 0.20057790551290378, 0.767187864707388, 0.6356942092227594, 0.20992783516937807, 0.21115377384679346, 0.1252279539817771, 0.18052555363699052, 0.19692669640269667, 0.1876866139231771, 0.18697424131949403, 0.1844832151097614, 0.18881261937043448, 0.21058133373873567, 0.18701017860257085, 0.18449711484346276, 0.07448065883908206, 0.07847839379784016, 0.07977837032412294, 0.10174659968374122, 0.08096874614344629, 0.08136994327246028, 0.08321262575864308, 0.08622905371754608, 0.06953581133556275]}, "mutation_prompt": null}
{"id": "7ca452c3-8145-4f8c-abfb-cd792b551786", "solution": "import numpy as np\n\nclass HybridStochasticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.initial_scale_factor = 0.8\n        self.initial_crossover_prob = 0.7\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def select_parents(self, idx):\n        indices = [i for i in range(self.population_size) if i != idx]\n        return np.random.choice(indices, 3, replace=False)\n\n    def differential_evolution_mutation(self, idx, scale_factor):\n        a, b, c = self.select_parents(idx)\n        mutant_vector = self.population[a] + scale_factor * (self.population[b] - self.population[c])\n        return np.clip(mutant_vector, self.lower_bound, self.upper_bound)\n\n    def crossover(self, target_vector, mutant_vector, crossover_prob):\n        crossover_mask = np.random.rand(self.dim) < crossover_prob\n        return np.where(crossover_mask, mutant_vector, target_vector)\n\n    def local_search(self, vector):\n        perturbation = np.random.normal(0, 0.1, self.dim)\n        candidate_vector = vector + perturbation\n        return np.clip(candidate_vector, self.lower_bound, self.upper_bound)\n\n    def __call__(self, func):\n        self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                # Dynamic adjustment of parameters\n                scale_factor = self.initial_scale_factor * (1 - self.evaluations / self.budget)\n                crossover_prob = self.initial_crossover_prob + 0.2 * np.random.rand()\n\n                mutant_vector = self.differential_evolution_mutation(i, scale_factor)\n                trial_vector = self.crossover(self.population[i], mutant_vector, crossover_prob)\n                trial_fitness = func(trial_vector)\n                self.evaluations += 1\n\n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial_vector\n                    self.fitness[i] = trial_fitness\n\n                if np.random.rand() < 0.2:\n                    candidate_vector = self.local_search(self.population[i])\n                    candidate_fitness = func(candidate_vector)\n                    self.evaluations += 1\n                    if candidate_fitness < self.fitness[i]:\n                        self.population[i] = candidate_vector\n                        self.fitness[i] = candidate_fitness\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx]", "name": "HybridStochasticOptimizer", "description": "Enhanced hybrid optimizer with dynamic adaptation of crossover probability and scaling factor for faster convergence.", "configspace": "", "generation": 25, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "423013c1-e76b-4a46-a974-0a1e87c32656", "metadata": {"aucs": [0.8713253338612517, 0.8512264743009399, 0.8665121170890773, 0.8796666233130291, 0.8761709853642955, 0.8810455847898071, 0.8634015516116982, 0.8659759235104127, 0.8658848386058079, 0.776022590581398, 0.7775385630940745, 0.7715872013079406, 0.047937183521818194, 0.7897493501520839, 0.7767867190753118, 0.7886305728092413, 0.7940131006001782, 0.7926813538126418, 0.2543563301263122, 0.26386842411152445, 0.1645250607126243, 0.4926752876196465, 0.39390858976277254, 0.23089210161864804, 0.5519002442996506, 0.16253539159378672, 0.16045598739732325, 0.1815564450288426, 0.16204045436991144, 0.14067199300734445, 0.13864525641376824, 0.17606506326955895, 0.1957449332854192, 0.1355796022600363, 0.15333624021469605, 0.19466446030704987, 0.9685443461788876, 0.9826362409248943, 0.9718102519988508, 0.9542477023870188, 0.9651450868978637, 0.9656076584362225, 0.9771087121751363, 0.9511676926846383, 0.9475409484430742, 0.5607961727722646, 0.5801208623770812, 0.4205876330708098, 0.5598664192930076, 0.641736637347754, 0.6059314552023902, 0.6535048493722003, 0.4301252012365522, 0.593391845674108, 0.775267609832766, 0.8220462344793056, 0.8165110291416555, 0.8099276940402154, 0.791912050573996, 0.7779888994355462, 0.814979996330171, 0.839390998760543, 0.8266411896693742, 0.20622207396884407, 0.19244687757537238, 0.2291070214062273, 0.18910464301479657, 0.2541653190233828, 0.1928968623624615, 0.1871271567996714, 0.17729889003910626, 0.22875603210790119, 0.12377566933015582, 0.2950653070917044, 0.23538384513456279, 0.15493565340425008, 0.19422285439103282, 0.1502145052953019, 0.2135497527157869, 0.1784558028745068, 0.199881013024794, 0.27160560898079356, 0.1600610706552127, 0.2966798941009102, 0.33197548409745026, 0.3273123800343034, 0.13175146719037434, 0.12734955559720107, 0.29523541535899633, 0.2581989142537512, 0.3057281543095325, 0.33719377902877445, 0.3273073264000551, 0.5237285877417179, 0.22812274619373252, 0.4261137114170931, 0.5799324829661097, 0.48369941272309647, 0.40326481318610863, 0.22327653681808668, 0.08132520060140336, 0.14102936435841418, 0.14786063109413006, 0.2157013252409571, 0.18220254459858531, 0.21794919718423955, 0.14371731913855168, 0.11264274046047196, 0.3238631138742195, 0.22699529900734483, 0.35859983983891375, 0.31015402062999375, 0.342704152103683, 0.30880869674110023, 0.30063474713777893, 0.2537866527590352, 0.32753736436104197, 0.6360732566039109, 0.7169071184070772, 0.6514127383341239, 0.719287291870321, 0.6970133694714913, 0.7270834504024826, 0.710179103041517, 0.6668939850533554, 0.7510629933966516, 0.11880847619771395, 0.1275411723472576, 0.14140762050142752, 0.11942035574005871, 0.12173741895648504, 0.1144597314207404, 0.11282612959351568, 0.11793734558544844, 0.13838865877543505, 0.2136322473775769, 0.18818818345769583, 0.16984149507488888, 0.21135743604782797, 0.3332439425963074, 0.2707205189155363, 0.19645333708691226, 0.5255360345277968, 0.17491835873550066, 0.491136123556865, 0.5455620249867844, 0.4823667625503876, 0.5137745939223461, 0.6007429826145017, 0.5373075928905169, 0.5325703968031614, 0.5471600999633488, 0.5520970820614712, 0.34138421101666905, 0.344345094422873, 0.4386890897074446, 0.3927652946470016, 0.3559656722913527, 0.3312468809292709, 0.4168609025141642, 0.43050715057973565, 0.3995219869727502, 0.20188303468677726, 0.19074883185916647, 0.22555193488946368, 0.197801756120222, 0.18776985961827763, 0.21111851559573747, 0.19943543047996204, 0.2100039743419102, 0.22390992876081084, 0.5191491482875619, 0.30489357992928057, 0.36033594079926956, 0.2286161552903242, 0.5548591242408711, 0.548446861384704, 0.233481806613284, 0.27773770979346313, 0.6660977084626925, 0.7802746310921849, 0.16495810395294486, 0.838783034616159, 0.7762080583089825, 0.19309572939016661, 0.18732679941670738, 0.838996832827352, 0.19522887632217467, 0.16643636185166943, 0.1657327490812559, 0.3878812419136092, 0.2063933306589294, 0.20057790551290378, 0.767187864707388, 0.6356942092227594, 0.20992783516937807, 0.21115377384679346, 0.1252279539817771, 0.18052555363699052, 0.19692669640269667, 0.1876866139231771, 0.18697424131949403, 0.1844832151097614, 0.18881261937043448, 0.21058133373873567, 0.18701017860257085, 0.18449711484346276, 0.07448065883908206, 0.07847839379784016, 0.07977837032412294, 0.10174659968374122, 0.08096874614344629, 0.08136994327246028, 0.08321262575864308, 0.08622905371754608, 0.06953581133556275]}, "mutation_prompt": null}
{"id": "29424f72-384f-4c26-a295-632bab9ae9a2", "solution": "import numpy as np\n\nclass HybridStochasticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.initial_scale_factor = 0.8\n        self.initial_crossover_prob = 0.7\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def select_parents(self, idx):\n        indices = [i for i in range(self.population_size) if i != idx]\n        return np.random.choice(indices, 3, replace=False)\n\n    def differential_evolution_mutation(self, idx, scale_factor):\n        a, b, c = self.select_parents(idx)\n        mutant_vector = self.population[a] + scale_factor * (self.population[b] - self.population[c])\n        return np.clip(mutant_vector, self.lower_bound, self.upper_bound)\n\n    def crossover(self, target_vector, mutant_vector, crossover_prob):\n        crossover_mask = np.random.rand(self.dim) < crossover_prob\n        return np.where(crossover_mask, mutant_vector, target_vector)\n\n    def local_search(self, vector):\n        perturbation = np.random.normal(0, 0.1, self.dim)\n        candidate_vector = vector + perturbation\n        return np.clip(candidate_vector, self.lower_bound, self.upper_bound)\n\n    def __call__(self, func):\n        self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                # Dynamic adjustment of parameters\n                scale_factor = self.initial_scale_factor * (1 - self.evaluations / self.budget)\n                crossover_prob = self.initial_crossover_prob + 0.2 * np.random.rand()\n\n                mutant_vector = self.differential_evolution_mutation(i, scale_factor)\n                trial_vector = self.crossover(self.population[i], mutant_vector, crossover_prob)\n                trial_fitness = func(trial_vector)\n                self.evaluations += 1\n\n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial_vector\n                    self.fitness[i] = trial_fitness\n\n                if np.random.rand() < 0.2:\n                    candidate_vector = self.local_search(self.population[i])\n                    candidate_fitness = func(candidate_vector)\n                    self.evaluations += 1\n                    if candidate_fitness < self.fitness[i]:\n                        self.population[i] = candidate_vector\n                        self.fitness[i] = candidate_fitness\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx]", "name": "HybridStochasticOptimizer", "description": "Enhanced hybrid optimizer with dynamic adaptation of crossover probability and scaling factor for faster convergence.", "configspace": "", "generation": 25, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "423013c1-e76b-4a46-a974-0a1e87c32656", "metadata": {"aucs": [0.8713253338612517, 0.8512264743009399, 0.8665121170890773, 0.8796666233130291, 0.8761709853642955, 0.8810455847898071, 0.8634015516116982, 0.8659759235104127, 0.8658848386058079, 0.776022590581398, 0.7775385630940745, 0.7715872013079406, 0.047937183521818194, 0.7897493501520839, 0.7767867190753118, 0.7886305728092413, 0.7940131006001782, 0.7926813538126418, 0.2543563301263122, 0.26386842411152445, 0.1645250607126243, 0.4926752876196465, 0.39390858976277254, 0.23089210161864804, 0.5519002442996506, 0.16253539159378672, 0.16045598739732325, 0.1815564450288426, 0.16204045436991144, 0.14067199300734445, 0.13864525641376824, 0.17606506326955895, 0.1957449332854192, 0.1355796022600363, 0.15333624021469605, 0.19466446030704987, 0.9685443461788876, 0.9826362409248943, 0.9718102519988508, 0.9542477023870188, 0.9651450868978637, 0.9656076584362225, 0.9771087121751363, 0.9511676926846383, 0.9475409484430742, 0.5607961727722646, 0.5801208623770812, 0.4205876330708098, 0.5598664192930076, 0.641736637347754, 0.6059314552023902, 0.6535048493722003, 0.4301252012365522, 0.593391845674108, 0.775267609832766, 0.8220462344793056, 0.8165110291416555, 0.8099276940402154, 0.791912050573996, 0.7779888994355462, 0.814979996330171, 0.839390998760543, 0.8266411896693742, 0.20622207396884407, 0.19244687757537238, 0.2291070214062273, 0.18910464301479657, 0.2541653190233828, 0.1928968623624615, 0.1871271567996714, 0.17729889003910626, 0.22875603210790119, 0.12377566933015582, 0.2950653070917044, 0.23538384513456279, 0.15493565340425008, 0.19422285439103282, 0.1502145052953019, 0.2135497527157869, 0.1784558028745068, 0.199881013024794, 0.27160560898079356, 0.1600610706552127, 0.2966798941009102, 0.33197548409745026, 0.3273123800343034, 0.13175146719037434, 0.12734955559720107, 0.29523541535899633, 0.2581989142537512, 0.3057281543095325, 0.33719377902877445, 0.3273073264000551, 0.5237285877417179, 0.22812274619373252, 0.4261137114170931, 0.5799324829661097, 0.48369941272309647, 0.40326481318610863, 0.22327653681808668, 0.08132520060140336, 0.14102936435841418, 0.14786063109413006, 0.2157013252409571, 0.18220254459858531, 0.21794919718423955, 0.14371731913855168, 0.11264274046047196, 0.3238631138742195, 0.22699529900734483, 0.35859983983891375, 0.31015402062999375, 0.342704152103683, 0.30880869674110023, 0.30063474713777893, 0.2537866527590352, 0.32753736436104197, 0.6360732566039109, 0.7169071184070772, 0.6514127383341239, 0.719287291870321, 0.6970133694714913, 0.7270834504024826, 0.710179103041517, 0.6668939850533554, 0.7510629933966516, 0.11880847619771395, 0.1275411723472576, 0.14140762050142752, 0.11942035574005871, 0.12173741895648504, 0.1144597314207404, 0.11282612959351568, 0.11793734558544844, 0.13838865877543505, 0.2136322473775769, 0.18818818345769583, 0.16984149507488888, 0.21135743604782797, 0.3332439425963074, 0.2707205189155363, 0.19645333708691226, 0.5255360345277968, 0.17491835873550066, 0.491136123556865, 0.5455620249867844, 0.4823667625503876, 0.5137745939223461, 0.6007429826145017, 0.5373075928905169, 0.5325703968031614, 0.5471600999633488, 0.5520970820614712, 0.34138421101666905, 0.344345094422873, 0.4386890897074446, 0.3927652946470016, 0.3559656722913527, 0.3312468809292709, 0.4168609025141642, 0.43050715057973565, 0.3995219869727502, 0.20188303468677726, 0.19074883185916647, 0.22555193488946368, 0.197801756120222, 0.18776985961827763, 0.21111851559573747, 0.19943543047996204, 0.2100039743419102, 0.22390992876081084, 0.5191491482875619, 0.30489357992928057, 0.36033594079926956, 0.2286161552903242, 0.5548591242408711, 0.548446861384704, 0.233481806613284, 0.27773770979346313, 0.6660977084626925, 0.7802746310921849, 0.16495810395294486, 0.838783034616159, 0.7762080583089825, 0.19309572939016661, 0.18732679941670738, 0.838996832827352, 0.19522887632217467, 0.16643636185166943, 0.1657327490812559, 0.3878812419136092, 0.2063933306589294, 0.20057790551290378, 0.767187864707388, 0.6356942092227594, 0.20992783516937807, 0.21115377384679346, 0.1252279539817771, 0.18052555363699052, 0.19692669640269667, 0.1876866139231771, 0.18697424131949403, 0.1844832151097614, 0.18881261937043448, 0.21058133373873567, 0.18701017860257085, 0.18449711484346276, 0.07448065883908206, 0.07847839379784016, 0.07977837032412294, 0.10174659968374122, 0.08096874614344629, 0.08136994327246028, 0.08321262575864308, 0.08622905371754608, 0.06953581133556275]}, "mutation_prompt": null}
{"id": "80f156e2-ec28-4d5a-84ee-2b23f7522671", "solution": "import numpy as np\n\nclass HybridStochasticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.initial_scale_factor = 0.8\n        self.initial_crossover_prob = 0.7\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def select_parents(self, idx):\n        indices = [i for i in range(self.population_size) if i != idx]\n        return np.random.choice(indices, 3, replace=False)\n\n    def differential_evolution_mutation(self, idx, scale_factor):\n        a, b, c = self.select_parents(idx)\n        mutant_vector = self.population[a] + scale_factor * (self.population[b] - self.population[c])\n        return np.clip(mutant_vector, self.lower_bound, self.upper_bound)\n\n    def crossover(self, target_vector, mutant_vector, crossover_prob):\n        crossover_mask = np.random.rand(self.dim) < crossover_prob\n        return np.where(crossover_mask, mutant_vector, target_vector)\n\n    def local_search(self, vector):\n        perturbation = np.random.normal(0, 0.1, self.dim)\n        candidate_vector = vector + perturbation\n        return np.clip(candidate_vector, self.lower_bound, self.upper_bound)\n\n    def __call__(self, func):\n        self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                # Dynamic adjustment of parameters\n                scale_factor = self.initial_scale_factor * (1 - self.evaluations / self.budget)\n                crossover_prob = self.initial_crossover_prob + 0.2 * np.random.rand()\n\n                mutant_vector = self.differential_evolution_mutation(i, scale_factor)\n                trial_vector = self.crossover(self.population[i], mutant_vector, crossover_prob)\n                trial_fitness = func(trial_vector)\n                self.evaluations += 1\n\n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial_vector\n                    self.fitness[i] = trial_fitness\n\n                if np.random.rand() < 0.2:\n                    candidate_vector = self.local_search(self.population[i])\n                    candidate_fitness = func(candidate_vector)\n                    self.evaluations += 1\n                    if candidate_fitness < self.fitness[i]:\n                        self.population[i] = candidate_vector\n                        self.fitness[i] = candidate_fitness\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx]", "name": "HybridStochasticOptimizer", "description": "Enhanced hybrid optimizer with dynamic adaptation of crossover probability and scaling factor for faster convergence.", "configspace": "", "generation": 25, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "423013c1-e76b-4a46-a974-0a1e87c32656", "metadata": {"aucs": [0.8713253338612517, 0.8512264743009399, 0.8665121170890773, 0.8796666233130291, 0.8761709853642955, 0.8810455847898071, 0.8634015516116982, 0.8659759235104127, 0.8658848386058079, 0.776022590581398, 0.7775385630940745, 0.7715872013079406, 0.047937183521818194, 0.7897493501520839, 0.7767867190753118, 0.7886305728092413, 0.7940131006001782, 0.7926813538126418, 0.2543563301263122, 0.26386842411152445, 0.1645250607126243, 0.4926752876196465, 0.39390858976277254, 0.23089210161864804, 0.5519002442996506, 0.16253539159378672, 0.16045598739732325, 0.1815564450288426, 0.16204045436991144, 0.14067199300734445, 0.13864525641376824, 0.17606506326955895, 0.1957449332854192, 0.1355796022600363, 0.15333624021469605, 0.19466446030704987, 0.9685443461788876, 0.9826362409248943, 0.9718102519988508, 0.9542477023870188, 0.9651450868978637, 0.9656076584362225, 0.9771087121751363, 0.9511676926846383, 0.9475409484430742, 0.5607961727722646, 0.5801208623770812, 0.4205876330708098, 0.5598664192930076, 0.641736637347754, 0.6059314552023902, 0.6535048493722003, 0.4301252012365522, 0.593391845674108, 0.775267609832766, 0.8220462344793056, 0.8165110291416555, 0.8099276940402154, 0.791912050573996, 0.7779888994355462, 0.814979996330171, 0.839390998760543, 0.8266411896693742, 0.20622207396884407, 0.19244687757537238, 0.2291070214062273, 0.18910464301479657, 0.2541653190233828, 0.1928968623624615, 0.1871271567996714, 0.17729889003910626, 0.22875603210790119, 0.12377566933015582, 0.2950653070917044, 0.23538384513456279, 0.15493565340425008, 0.19422285439103282, 0.1502145052953019, 0.2135497527157869, 0.1784558028745068, 0.199881013024794, 0.27160560898079356, 0.1600610706552127, 0.2966798941009102, 0.33197548409745026, 0.3273123800343034, 0.13175146719037434, 0.12734955559720107, 0.29523541535899633, 0.2581989142537512, 0.3057281543095325, 0.33719377902877445, 0.3273073264000551, 0.5237285877417179, 0.22812274619373252, 0.4261137114170931, 0.5799324829661097, 0.48369941272309647, 0.40326481318610863, 0.22327653681808668, 0.08132520060140336, 0.14102936435841418, 0.14786063109413006, 0.2157013252409571, 0.18220254459858531, 0.21794919718423955, 0.14371731913855168, 0.11264274046047196, 0.3238631138742195, 0.22699529900734483, 0.35859983983891375, 0.31015402062999375, 0.342704152103683, 0.30880869674110023, 0.30063474713777893, 0.2537866527590352, 0.32753736436104197, 0.6360732566039109, 0.7169071184070772, 0.6514127383341239, 0.719287291870321, 0.6970133694714913, 0.7270834504024826, 0.710179103041517, 0.6668939850533554, 0.7510629933966516, 0.11880847619771395, 0.1275411723472576, 0.14140762050142752, 0.11942035574005871, 0.12173741895648504, 0.1144597314207404, 0.11282612959351568, 0.11793734558544844, 0.13838865877543505, 0.2136322473775769, 0.18818818345769583, 0.16984149507488888, 0.21135743604782797, 0.3332439425963074, 0.2707205189155363, 0.19645333708691226, 0.5255360345277968, 0.17491835873550066, 0.491136123556865, 0.5455620249867844, 0.4823667625503876, 0.5137745939223461, 0.6007429826145017, 0.5373075928905169, 0.5325703968031614, 0.5471600999633488, 0.5520970820614712, 0.34138421101666905, 0.344345094422873, 0.4386890897074446, 0.3927652946470016, 0.3559656722913527, 0.3312468809292709, 0.4168609025141642, 0.43050715057973565, 0.3995219869727502, 0.20188303468677726, 0.19074883185916647, 0.22555193488946368, 0.197801756120222, 0.18776985961827763, 0.21111851559573747, 0.19943543047996204, 0.2100039743419102, 0.22390992876081084, 0.5191491482875619, 0.30489357992928057, 0.36033594079926956, 0.2286161552903242, 0.5548591242408711, 0.548446861384704, 0.233481806613284, 0.27773770979346313, 0.6660977084626925, 0.7802746310921849, 0.16495810395294486, 0.838783034616159, 0.7762080583089825, 0.19309572939016661, 0.18732679941670738, 0.838996832827352, 0.19522887632217467, 0.16643636185166943, 0.1657327490812559, 0.3878812419136092, 0.2063933306589294, 0.20057790551290378, 0.767187864707388, 0.6356942092227594, 0.20992783516937807, 0.21115377384679346, 0.1252279539817771, 0.18052555363699052, 0.19692669640269667, 0.1876866139231771, 0.18697424131949403, 0.1844832151097614, 0.18881261937043448, 0.21058133373873567, 0.18701017860257085, 0.18449711484346276, 0.07448065883908206, 0.07847839379784016, 0.07977837032412294, 0.10174659968374122, 0.08096874614344629, 0.08136994327246028, 0.08321262575864308, 0.08622905371754608, 0.06953581133556275]}, "mutation_prompt": null}
{"id": "c9aafe6b-332b-4283-9cc1-4b2e50c44e27", "solution": "import numpy as np\n\nclass HybridStochasticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.initial_scale_factor = 0.8\n        self.initial_crossover_prob = 0.7\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population = self.opposition_based_initialization()\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.learning_rate = 0.1\n\n    def opposition_based_initialization(self):\n        initial_population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        opposition_population = self.lower_bound + self.upper_bound - initial_population\n        return np.vstack((initial_population, opposition_population))[:self.population_size]\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def select_parents(self, idx):\n        indices = [i for i in range(self.population_size) if i != idx]\n        return np.random.choice(indices, 3, replace=False)\n\n    def differential_evolution_mutation(self, idx, scale_factor):\n        a, b, c = self.select_parents(idx)\n        mutant_vector = self.population[a] + scale_factor * (self.population[b] - self.population[c])\n        return np.clip(mutant_vector, self.lower_bound, self.upper_bound)\n\n    def crossover(self, target_vector, mutant_vector, crossover_prob):\n        crossover_mask = np.random.rand(self.dim) < crossover_prob\n        return np.where(crossover_mask, mutant_vector, target_vector)\n\n    def local_search(self, vector):\n        perturbation = np.random.normal(0, self.learning_rate, self.dim)\n        candidate_vector = vector + perturbation\n        return np.clip(candidate_vector, self.lower_bound, self.upper_bound)\n\n    def __call__(self, func):\n        self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                # Dynamic adjustment of parameters\n                scale_factor = self.initial_scale_factor * (1 - self.evaluations / self.budget)\n                crossover_prob = self.initial_crossover_prob + 0.2 * np.random.rand()\n\n                mutant_vector = self.differential_evolution_mutation(i, scale_factor)\n                trial_vector = self.crossover(self.population[i], mutant_vector, crossover_prob)\n                trial_fitness = func(trial_vector)\n                self.evaluations += 1\n\n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial_vector\n                    self.fitness[i] = trial_fitness\n\n                if np.random.rand() < 0.2:\n                    candidate_vector = self.local_search(self.population[i])\n                    candidate_fitness = func(candidate_vector)\n                    self.evaluations += 1\n                    if candidate_fitness < self.fitness[i]:\n                        self.population[i] = candidate_vector\n                        self.fitness[i] = candidate_fitness\n\n                # Update learning rate adaptively\n                self.learning_rate = max(0.01, self.learning_rate * (1 - 0.05 * np.random.rand()))\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx]", "name": "HybridStochasticOptimizer", "description": "Hybrid optimizer with adaptive learning rate and opposition-based initialization for faster convergence.", "configspace": "", "generation": 31, "fitness": 0.4159602567786503, "feedback": "The algorithm HybridStochasticOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.42 with standard deviation 0.26.", "error": "", "parent_id": "423013c1-e76b-4a46-a974-0a1e87c32656", "metadata": {"aucs": [0.8614318631710612, 0.8615612395806681, 0.8648092636329068, 0.8836823589578061, 0.8809302815417128, 0.8537149049105373, 0.8649927264859298, 0.8661719092889808, 0.8732297035052712, 0.7784313840335078, 0.7934709027220181, 0.7709929153086685, 0.7666377215322294, 0.7792354193630129, 0.7955283567004858, 0.7874175449423783, 0.7934505999095464, 0.7963508686605465, 0.25196533963541234, 0.603323462921044, 0.31189512717187695, 0.45254508955474515, 0.3855834591332875, 0.6324217482611867, 0.38087210189123466, 0.37101210401952667, 0.37832843410229366, 0.3374576510463305, 0.1599578735010866, 0.25181667196560353, 0.15598715927116646, 0.15926218502651568, 0.14742944278799264, 0.33123888835847937, 0.3066524419836607, 0.16048855283955032, 0.9738501285938751, 0.9538242295949432, 0.9811516406082638, 0.9649564447290745, 0.9726531518905077, 0.9814481843774895, 0.9732999615150277, 0.9721568744354393, 0.9772487896256388, 0.5602477359628759, 0.5166699154516793, 0.49955145811133295, 0.4607762689339385, 0.5168840480364095, 0.5837268635955544, 0.5848178673193583, 0.44902566155463985, 0.6535189332618503, 0.8108564290388902, 0.7472995471422044, 0.7832292739258102, 0.780298104186407, 0.7997326258786164, 0.8307049265401217, 0.7670254946306878, 0.7966436314344594, 0.7737581975614084, 0.26793793612173356, 0.24955593945589905, 0.19834760026021148, 0.22530649729727037, 0.21576791524353967, 0.22803418538233733, 0.23532474250416835, 0.20160318462124072, 0.1920099262564383, 0.19613605298744063, 0.23556007360063813, 0.11440376677945807, 0.22544642163615658, 0.24862040742714708, 0.1874776391649946, 0.19655907028592323, 0.19284064583165672, 0.19577311722397894, 0.3108121261595491, 0.2566448530161354, 0.36431074059207347, 0.24025452278021453, 0.4802140919950957, 0.0007248673300417341, 0.35401012029935763, 0.3692632979788284, 0.276018593628536, 0.3623824055587753, 0.3924469974919602, 0.23556071143414548, 0.35272611497922335, 0.44565667864537895, 0.39468163509167276, 0.4061903350584134, 0.36215287040133537, 0.3973933722095969, 0.11732286633565558, 0.20172957542003978, 0.1498188877382276, 0.24772296138908012, 0.1546062088760296, 0.10282719367721671, 0.20924019897046064, 0.1183165114034116, 0.20632727477440505, 0.2809986328144657, 0.29684536408175155, 0.3175185646986213, 0.3737401167683222, 0.33517695108836454, 0.3271148862228106, 0.36693310045283345, 0.3166293273038533, 0.24146562201228117, 0.6055800258234042, 0.7234496228479592, 0.6438958422770305, 0.7507872332161957, 0.7324506280759618, 0.6589215501985713, 0.7496261142206224, 0.7209822579527148, 0.6855782937737074, 0.1371651233838116, 0.12075132229211483, 0.13780801853641733, 0.12584800341409697, 0.13850225460242616, 0.11675557226216027, 0.12863432580056944, 0.1273098808292441, 0.14268148704984884, 0.19934365172377466, 0.18594466654988895, 0.32832255345101913, 0.23469770515038102, 0.14866333116079655, 0.20267106519038025, 0.20152641892425394, 0.23576091839252133, 0.21421383347767264, 0.5391071467719948, 0.5042943805711715, 0.4612246809606788, 0.47676447358121665, 0.5291640446387131, 0.602638537448555, 0.6246427601161761, 0.5714078963023623, 0.6225968922190244, 0.33304162231889356, 0.3304783841194173, 0.4141875902408757, 0.354629285337982, 0.33990446568261656, 0.27919480653858797, 0.41214607585167573, 0.4488101082230267, 0.4070679191988382, 0.2075439556387948, 0.21303390663090027, 0.21635748499617935, 0.19827783015642464, 0.20259798106785998, 0.2337489300339528, 0.1995085629275789, 0.23000732022010184, 0.20258936164228614, 0.3583455868837565, 0.5974983189015708, 0.6315004735669381, 0.48375114306220435, 0.5228073851654269, 0.6765804181616237, 0.44806218567436396, 0.4405846418933689, 0.2231537031442834, 0.15177066878967904, 0.19069875851639195, 0.8538083672200958, 0.8489568076127689, 0.8369418101551436, 0.7959958487406582, 0.1809055777063855, 0.19441884874194382, 0.6735733685324677, 0.653629308683124, 0.16602485673297018, 0.20335804099962917, 0.20133806574003343, 0.20063819684592765, 0.4267483879660453, 0.203500441501264, 0.12468842409948466, 0.7601496747285006, 0.21347654253114523, 0.20557946271286787, 0.20946166959342294, 0.2037521178560513, 0.19899119990835357, 0.20502188897743512, 0.21185012528730773, 0.23029761548989025, 0.2226957927771256, 0.10106476573254897, 0.08282313409598063, 0.0927463241901183, 0.0892673049309205, 0.09187005772305634, 0.11808457473018663, 0.09136297465966581, 0.09200662746312827, 0.09667808533660249]}, "mutation_prompt": null}
{"id": "54ef2870-efe1-4b3e-8368-2ed33dfca678", "solution": "import numpy as np\n\nclass HybridStochasticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.initial_scale_factor = 0.8\n        self.initial_crossover_prob = 0.7\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population = self.opposition_based_initialization()\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.learning_rate = 0.1\n\n    def opposition_based_initialization(self):\n        initial_population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        opposition_population = self.lower_bound + self.upper_bound - initial_population\n        return np.vstack((initial_population, opposition_population))[:self.population_size]\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def select_parents(self, idx):\n        indices = [i for i in range(self.population_size) if i != idx]\n        return np.random.choice(indices, 3, replace=False)\n\n    def differential_evolution_mutation(self, idx, scale_factor):\n        a, b, c = self.select_parents(idx)\n        mutant_vector = self.population[a] + scale_factor * (self.population[b] - self.population[c])\n        return np.clip(mutant_vector, self.lower_bound, self.upper_bound)\n\n    def crossover(self, target_vector, mutant_vector, crossover_prob):\n        crossover_mask = np.random.rand(self.dim) < crossover_prob\n        return np.where(crossover_mask, mutant_vector, target_vector)\n\n    def local_search(self, vector):\n        perturbation = np.random.normal(0, self.learning_rate, self.dim)\n        candidate_vector = vector + perturbation\n        return np.clip(candidate_vector, self.lower_bound, self.upper_bound)\n\n    def __call__(self, func):\n        self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                # Dynamic adjustment of parameters\n                scale_factor = self.initial_scale_factor * (1 - self.evaluations / self.budget)\n                crossover_prob = self.initial_crossover_prob + 0.2 * np.random.rand()\n\n                mutant_vector = self.differential_evolution_mutation(i, scale_factor)\n                trial_vector = self.crossover(self.population[i], mutant_vector, crossover_prob)\n                trial_fitness = func(trial_vector)\n                self.evaluations += 1\n\n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial_vector\n                    self.fitness[i] = trial_fitness\n\n                if np.random.rand() < 0.2:\n                    candidate_vector = self.local_search(self.population[i])\n                    candidate_fitness = func(candidate_vector)\n                    self.evaluations += 1\n                    if candidate_fitness < self.fitness[i]:\n                        self.population[i] = candidate_vector\n                        self.fitness[i] = candidate_fitness\n\n                # Update learning rate adaptively\n                self.learning_rate = max(0.01, self.learning_rate * (1 - 0.05 * np.random.rand()))\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx]", "name": "HybridStochasticOptimizer", "description": "Hybrid optimizer with adaptive learning rate and opposition-based initialization for faster convergence.", "configspace": "", "generation": 32, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "c9aafe6b-332b-4283-9cc1-4b2e50c44e27", "metadata": {"aucs": [0.8614318631710612, 0.8615612395806681, 0.8648092636329068, 0.8836823589578061, 0.8809302815417128, 0.8537149049105373, 0.8649927264859298, 0.8661719092889808, 0.8732297035052712, 0.7784313840335078, 0.7934709027220181, 0.7709929153086685, 0.7666377215322294, 0.7792354193630129, 0.7955283567004858, 0.7874175449423783, 0.7934505999095464, 0.7963508686605465, 0.25196533963541234, 0.603323462921044, 0.31189512717187695, 0.45254508955474515, 0.3855834591332875, 0.6324217482611867, 0.38087210189123466, 0.37101210401952667, 0.37832843410229366, 0.3374576510463305, 0.1599578735010866, 0.25181667196560353, 0.15598715927116646, 0.15926218502651568, 0.14742944278799264, 0.33123888835847937, 0.3066524419836607, 0.16048855283955032, 0.9738501285938751, 0.9538242295949432, 0.9811516406082638, 0.9649564447290745, 0.9726531518905077, 0.9814481843774895, 0.9732999615150277, 0.9721568744354393, 0.9772487896256388, 0.5602477359628759, 0.5166699154516793, 0.49955145811133295, 0.4607762689339385, 0.5168840480364095, 0.5837268635955544, 0.5848178673193583, 0.44902566155463985, 0.6535189332618503, 0.8108564290388902, 0.7472995471422044, 0.7832292739258102, 0.780298104186407, 0.7997326258786164, 0.8307049265401217, 0.7670254946306878, 0.7966436314344594, 0.7737581975614084, 0.26793793612173356, 0.24955593945589905, 0.19834760026021148, 0.22530649729727037, 0.21576791524353967, 0.22803418538233733, 0.23532474250416835, 0.20160318462124072, 0.1920099262564383, 0.19613605298744063, 0.23556007360063813, 0.11440376677945807, 0.22544642163615658, 0.24862040742714708, 0.1874776391649946, 0.19655907028592323, 0.19284064583165672, 0.19577311722397894, 0.3108121261595491, 0.2566448530161354, 0.36431074059207347, 0.24025452278021453, 0.4802140919950957, 0.0007248673300417341, 0.35401012029935763, 0.3692632979788284, 0.276018593628536, 0.3623824055587753, 0.3924469974919602, 0.23556071143414548, 0.35272611497922335, 0.44565667864537895, 0.39468163509167276, 0.4061903350584134, 0.36215287040133537, 0.3973933722095969, 0.11732286633565558, 0.20172957542003978, 0.1498188877382276, 0.24772296138908012, 0.1546062088760296, 0.10282719367721671, 0.20924019897046064, 0.1183165114034116, 0.20632727477440505, 0.2809986328144657, 0.29684536408175155, 0.3175185646986213, 0.3737401167683222, 0.33517695108836454, 0.3271148862228106, 0.36693310045283345, 0.3166293273038533, 0.24146562201228117, 0.6055800258234042, 0.7234496228479592, 0.6438958422770305, 0.7507872332161957, 0.7324506280759618, 0.6589215501985713, 0.7496261142206224, 0.7209822579527148, 0.6855782937737074, 0.1371651233838116, 0.12075132229211483, 0.13780801853641733, 0.12584800341409697, 0.13850225460242616, 0.11675557226216027, 0.12863432580056944, 0.1273098808292441, 0.14268148704984884, 0.19934365172377466, 0.18594466654988895, 0.32832255345101913, 0.23469770515038102, 0.14866333116079655, 0.20267106519038025, 0.20152641892425394, 0.23576091839252133, 0.21421383347767264, 0.5391071467719948, 0.5042943805711715, 0.4612246809606788, 0.47676447358121665, 0.5291640446387131, 0.602638537448555, 0.6246427601161761, 0.5714078963023623, 0.6225968922190244, 0.33304162231889356, 0.3304783841194173, 0.4141875902408757, 0.354629285337982, 0.33990446568261656, 0.27919480653858797, 0.41214607585167573, 0.4488101082230267, 0.4070679191988382, 0.2075439556387948, 0.21303390663090027, 0.21635748499617935, 0.19827783015642464, 0.20259798106785998, 0.2337489300339528, 0.1995085629275789, 0.23000732022010184, 0.20258936164228614, 0.3583455868837565, 0.5974983189015708, 0.6315004735669381, 0.48375114306220435, 0.5228073851654269, 0.6765804181616237, 0.44806218567436396, 0.4405846418933689, 0.2231537031442834, 0.15177066878967904, 0.19069875851639195, 0.8538083672200958, 0.8489568076127689, 0.8369418101551436, 0.7959958487406582, 0.1809055777063855, 0.19441884874194382, 0.6735733685324677, 0.653629308683124, 0.16602485673297018, 0.20335804099962917, 0.20133806574003343, 0.20063819684592765, 0.4267483879660453, 0.203500441501264, 0.12468842409948466, 0.7601496747285006, 0.21347654253114523, 0.20557946271286787, 0.20946166959342294, 0.2037521178560513, 0.19899119990835357, 0.20502188897743512, 0.21185012528730773, 0.23029761548989025, 0.2226957927771256, 0.10106476573254897, 0.08282313409598063, 0.0927463241901183, 0.0892673049309205, 0.09187005772305634, 0.11808457473018663, 0.09136297465966581, 0.09200662746312827, 0.09667808533660249]}, "mutation_prompt": null}
{"id": "60a6c6e8-6720-4b76-abb0-d4fef3b4eea1", "solution": "import numpy as np\n\nclass HybridStochasticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.initial_scale_factor = 0.8\n        self.initial_crossover_prob = 0.7\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population = self.opposition_based_initialization()\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.learning_rate = 0.1\n\n    def opposition_based_initialization(self):\n        initial_population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        opposition_population = self.lower_bound + self.upper_bound - initial_population\n        return np.vstack((initial_population, opposition_population))[:self.population_size]\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def select_parents(self, idx):\n        indices = [i for i in range(self.population_size) if i != idx]\n        return np.random.choice(indices, 3, replace=False)\n\n    def differential_evolution_mutation(self, idx, scale_factor):\n        a, b, c = self.select_parents(idx)\n        mutant_vector = self.population[a] + scale_factor * (self.population[b] - self.population[c])\n        return np.clip(mutant_vector, self.lower_bound, self.upper_bound)\n\n    def crossover(self, target_vector, mutant_vector, crossover_prob):\n        crossover_mask = np.random.rand(self.dim) < crossover_prob\n        return np.where(crossover_mask, mutant_vector, target_vector)\n\n    def local_search(self, vector):\n        perturbation = np.random.normal(0, self.learning_rate, self.dim)\n        candidate_vector = vector + perturbation\n        return np.clip(candidate_vector, self.lower_bound, self.upper_bound)\n\n    def __call__(self, func):\n        self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                # Dynamic adjustment of parameters\n                scale_factor = self.initial_scale_factor * (1 - self.evaluations / self.budget)\n                crossover_prob = self.initial_crossover_prob + 0.2 * np.random.rand()\n\n                mutant_vector = self.differential_evolution_mutation(i, scale_factor)\n                trial_vector = self.crossover(self.population[i], mutant_vector, crossover_prob)\n                trial_fitness = func(trial_vector)\n                self.evaluations += 1\n\n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial_vector\n                    self.fitness[i] = trial_fitness\n\n                if np.random.rand() < 0.2:\n                    candidate_vector = self.local_search(self.population[i])\n                    candidate_fitness = func(candidate_vector)\n                    self.evaluations += 1\n                    if candidate_fitness < self.fitness[i]:\n                        self.population[i] = candidate_vector\n                        self.fitness[i] = candidate_fitness\n\n                # Update learning rate adaptively\n                self.learning_rate = max(0.01, self.learning_rate * (1 - 0.05 * np.random.rand()))\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx]", "name": "HybridStochasticOptimizer", "description": "Hybrid optimizer with adaptive learning rate and opposition-based initialization for faster convergence.", "configspace": "", "generation": 32, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "c9aafe6b-332b-4283-9cc1-4b2e50c44e27", "metadata": {"aucs": [0.8614318631710612, 0.8615612395806681, 0.8648092636329068, 0.8836823589578061, 0.8809302815417128, 0.8537149049105373, 0.8649927264859298, 0.8661719092889808, 0.8732297035052712, 0.7784313840335078, 0.7934709027220181, 0.7709929153086685, 0.7666377215322294, 0.7792354193630129, 0.7955283567004858, 0.7874175449423783, 0.7934505999095464, 0.7963508686605465, 0.25196533963541234, 0.603323462921044, 0.31189512717187695, 0.45254508955474515, 0.3855834591332875, 0.6324217482611867, 0.38087210189123466, 0.37101210401952667, 0.37832843410229366, 0.3374576510463305, 0.1599578735010866, 0.25181667196560353, 0.15598715927116646, 0.15926218502651568, 0.14742944278799264, 0.33123888835847937, 0.3066524419836607, 0.16048855283955032, 0.9738501285938751, 0.9538242295949432, 0.9811516406082638, 0.9649564447290745, 0.9726531518905077, 0.9814481843774895, 0.9732999615150277, 0.9721568744354393, 0.9772487896256388, 0.5602477359628759, 0.5166699154516793, 0.49955145811133295, 0.4607762689339385, 0.5168840480364095, 0.5837268635955544, 0.5848178673193583, 0.44902566155463985, 0.6535189332618503, 0.8108564290388902, 0.7472995471422044, 0.7832292739258102, 0.780298104186407, 0.7997326258786164, 0.8307049265401217, 0.7670254946306878, 0.7966436314344594, 0.7737581975614084, 0.26793793612173356, 0.24955593945589905, 0.19834760026021148, 0.22530649729727037, 0.21576791524353967, 0.22803418538233733, 0.23532474250416835, 0.20160318462124072, 0.1920099262564383, 0.19613605298744063, 0.23556007360063813, 0.11440376677945807, 0.22544642163615658, 0.24862040742714708, 0.1874776391649946, 0.19655907028592323, 0.19284064583165672, 0.19577311722397894, 0.3108121261595491, 0.2566448530161354, 0.36431074059207347, 0.24025452278021453, 0.4802140919950957, 0.0007248673300417341, 0.35401012029935763, 0.3692632979788284, 0.276018593628536, 0.3623824055587753, 0.3924469974919602, 0.23556071143414548, 0.35272611497922335, 0.44565667864537895, 0.39468163509167276, 0.4061903350584134, 0.36215287040133537, 0.3973933722095969, 0.11732286633565558, 0.20172957542003978, 0.1498188877382276, 0.24772296138908012, 0.1546062088760296, 0.10282719367721671, 0.20924019897046064, 0.1183165114034116, 0.20632727477440505, 0.2809986328144657, 0.29684536408175155, 0.3175185646986213, 0.3737401167683222, 0.33517695108836454, 0.3271148862228106, 0.36693310045283345, 0.3166293273038533, 0.24146562201228117, 0.6055800258234042, 0.7234496228479592, 0.6438958422770305, 0.7507872332161957, 0.7324506280759618, 0.6589215501985713, 0.7496261142206224, 0.7209822579527148, 0.6855782937737074, 0.1371651233838116, 0.12075132229211483, 0.13780801853641733, 0.12584800341409697, 0.13850225460242616, 0.11675557226216027, 0.12863432580056944, 0.1273098808292441, 0.14268148704984884, 0.19934365172377466, 0.18594466654988895, 0.32832255345101913, 0.23469770515038102, 0.14866333116079655, 0.20267106519038025, 0.20152641892425394, 0.23576091839252133, 0.21421383347767264, 0.5391071467719948, 0.5042943805711715, 0.4612246809606788, 0.47676447358121665, 0.5291640446387131, 0.602638537448555, 0.6246427601161761, 0.5714078963023623, 0.6225968922190244, 0.33304162231889356, 0.3304783841194173, 0.4141875902408757, 0.354629285337982, 0.33990446568261656, 0.27919480653858797, 0.41214607585167573, 0.4488101082230267, 0.4070679191988382, 0.2075439556387948, 0.21303390663090027, 0.21635748499617935, 0.19827783015642464, 0.20259798106785998, 0.2337489300339528, 0.1995085629275789, 0.23000732022010184, 0.20258936164228614, 0.3583455868837565, 0.5974983189015708, 0.6315004735669381, 0.48375114306220435, 0.5228073851654269, 0.6765804181616237, 0.44806218567436396, 0.4405846418933689, 0.2231537031442834, 0.15177066878967904, 0.19069875851639195, 0.8538083672200958, 0.8489568076127689, 0.8369418101551436, 0.7959958487406582, 0.1809055777063855, 0.19441884874194382, 0.6735733685324677, 0.653629308683124, 0.16602485673297018, 0.20335804099962917, 0.20133806574003343, 0.20063819684592765, 0.4267483879660453, 0.203500441501264, 0.12468842409948466, 0.7601496747285006, 0.21347654253114523, 0.20557946271286787, 0.20946166959342294, 0.2037521178560513, 0.19899119990835357, 0.20502188897743512, 0.21185012528730773, 0.23029761548989025, 0.2226957927771256, 0.10106476573254897, 0.08282313409598063, 0.0927463241901183, 0.0892673049309205, 0.09187005772305634, 0.11808457473018663, 0.09136297465966581, 0.09200662746312827, 0.09667808533660249]}, "mutation_prompt": null}
{"id": "90ca740f-8517-4137-a1d7-1473bfde4de5", "solution": "import numpy as np\n\nclass HybridStochasticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.initial_scale_factor = 0.8\n        self.initial_crossover_prob = 0.7\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population = self.opposition_based_initialization()\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.learning_rate = 0.1\n\n    def opposition_based_initialization(self):\n        initial_population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        opposition_population = self.lower_bound + self.upper_bound - initial_population\n        return np.vstack((initial_population, opposition_population))[:self.population_size]\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def select_parents(self, idx):\n        indices = [i for i in range(self.population_size) if i != idx]\n        return np.random.choice(indices, 3, replace=False)\n\n    def differential_evolution_mutation(self, idx, scale_factor):\n        a, b, c = self.select_parents(idx)\n        mutant_vector = self.population[a] + scale_factor * (self.population[b] - self.population[c])\n        return np.clip(mutant_vector, self.lower_bound, self.upper_bound)\n\n    def crossover(self, target_vector, mutant_vector, crossover_prob):\n        crossover_mask = np.random.rand(self.dim) < crossover_prob\n        return np.where(crossover_mask, mutant_vector, target_vector)\n\n    def local_search(self, vector):\n        perturbation = np.random.normal(0, self.learning_rate, self.dim)\n        candidate_vector = vector + perturbation\n        return np.clip(candidate_vector, self.lower_bound, self.upper_bound)\n\n    def __call__(self, func):\n        self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                # Dynamic adjustment of parameters\n                scale_factor = self.initial_scale_factor * (1 - self.evaluations / self.budget)\n                crossover_prob = self.initial_crossover_prob + 0.2 * np.random.rand()\n\n                mutant_vector = self.differential_evolution_mutation(i, scale_factor)\n                trial_vector = self.crossover(self.population[i], mutant_vector, crossover_prob)\n                trial_fitness = func(trial_vector)\n                self.evaluations += 1\n\n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial_vector\n                    self.fitness[i] = trial_fitness\n\n                if np.random.rand() < 0.2:\n                    candidate_vector = self.local_search(self.population[i])\n                    candidate_fitness = func(candidate_vector)\n                    self.evaluations += 1\n                    if candidate_fitness < self.fitness[i]:\n                        self.population[i] = candidate_vector\n                        self.fitness[i] = candidate_fitness\n\n                # Update learning rate adaptively\n                self.learning_rate = max(0.01, self.learning_rate * (1 - 0.05 * np.random.rand()))\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx]", "name": "HybridStochasticOptimizer", "description": "Hybrid optimizer with adaptive learning rate and opposition-based initialization for faster convergence.", "configspace": "", "generation": 32, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "c9aafe6b-332b-4283-9cc1-4b2e50c44e27", "metadata": {"aucs": [0.8614318631710612, 0.8615612395806681, 0.8648092636329068, 0.8836823589578061, 0.8809302815417128, 0.8537149049105373, 0.8649927264859298, 0.8661719092889808, 0.8732297035052712, 0.7784313840335078, 0.7934709027220181, 0.7709929153086685, 0.7666377215322294, 0.7792354193630129, 0.7955283567004858, 0.7874175449423783, 0.7934505999095464, 0.7963508686605465, 0.25196533963541234, 0.603323462921044, 0.31189512717187695, 0.45254508955474515, 0.3855834591332875, 0.6324217482611867, 0.38087210189123466, 0.37101210401952667, 0.37832843410229366, 0.3374576510463305, 0.1599578735010866, 0.25181667196560353, 0.15598715927116646, 0.15926218502651568, 0.14742944278799264, 0.33123888835847937, 0.3066524419836607, 0.16048855283955032, 0.9738501285938751, 0.9538242295949432, 0.9811516406082638, 0.9649564447290745, 0.9726531518905077, 0.9814481843774895, 0.9732999615150277, 0.9721568744354393, 0.9772487896256388, 0.5602477359628759, 0.5166699154516793, 0.49955145811133295, 0.4607762689339385, 0.5168840480364095, 0.5837268635955544, 0.5848178673193583, 0.44902566155463985, 0.6535189332618503, 0.8108564290388902, 0.7472995471422044, 0.7832292739258102, 0.780298104186407, 0.7997326258786164, 0.8307049265401217, 0.7670254946306878, 0.7966436314344594, 0.7737581975614084, 0.26793793612173356, 0.24955593945589905, 0.19834760026021148, 0.22530649729727037, 0.21576791524353967, 0.22803418538233733, 0.23532474250416835, 0.20160318462124072, 0.1920099262564383, 0.19613605298744063, 0.23556007360063813, 0.11440376677945807, 0.22544642163615658, 0.24862040742714708, 0.1874776391649946, 0.19655907028592323, 0.19284064583165672, 0.19577311722397894, 0.3108121261595491, 0.2566448530161354, 0.36431074059207347, 0.24025452278021453, 0.4802140919950957, 0.0007248673300417341, 0.35401012029935763, 0.3692632979788284, 0.276018593628536, 0.3623824055587753, 0.3924469974919602, 0.23556071143414548, 0.35272611497922335, 0.44565667864537895, 0.39468163509167276, 0.4061903350584134, 0.36215287040133537, 0.3973933722095969, 0.11732286633565558, 0.20172957542003978, 0.1498188877382276, 0.24772296138908012, 0.1546062088760296, 0.10282719367721671, 0.20924019897046064, 0.1183165114034116, 0.20632727477440505, 0.2809986328144657, 0.29684536408175155, 0.3175185646986213, 0.3737401167683222, 0.33517695108836454, 0.3271148862228106, 0.36693310045283345, 0.3166293273038533, 0.24146562201228117, 0.6055800258234042, 0.7234496228479592, 0.6438958422770305, 0.7507872332161957, 0.7324506280759618, 0.6589215501985713, 0.7496261142206224, 0.7209822579527148, 0.6855782937737074, 0.1371651233838116, 0.12075132229211483, 0.13780801853641733, 0.12584800341409697, 0.13850225460242616, 0.11675557226216027, 0.12863432580056944, 0.1273098808292441, 0.14268148704984884, 0.19934365172377466, 0.18594466654988895, 0.32832255345101913, 0.23469770515038102, 0.14866333116079655, 0.20267106519038025, 0.20152641892425394, 0.23576091839252133, 0.21421383347767264, 0.5391071467719948, 0.5042943805711715, 0.4612246809606788, 0.47676447358121665, 0.5291640446387131, 0.602638537448555, 0.6246427601161761, 0.5714078963023623, 0.6225968922190244, 0.33304162231889356, 0.3304783841194173, 0.4141875902408757, 0.354629285337982, 0.33990446568261656, 0.27919480653858797, 0.41214607585167573, 0.4488101082230267, 0.4070679191988382, 0.2075439556387948, 0.21303390663090027, 0.21635748499617935, 0.19827783015642464, 0.20259798106785998, 0.2337489300339528, 0.1995085629275789, 0.23000732022010184, 0.20258936164228614, 0.3583455868837565, 0.5974983189015708, 0.6315004735669381, 0.48375114306220435, 0.5228073851654269, 0.6765804181616237, 0.44806218567436396, 0.4405846418933689, 0.2231537031442834, 0.15177066878967904, 0.19069875851639195, 0.8538083672200958, 0.8489568076127689, 0.8369418101551436, 0.7959958487406582, 0.1809055777063855, 0.19441884874194382, 0.6735733685324677, 0.653629308683124, 0.16602485673297018, 0.20335804099962917, 0.20133806574003343, 0.20063819684592765, 0.4267483879660453, 0.203500441501264, 0.12468842409948466, 0.7601496747285006, 0.21347654253114523, 0.20557946271286787, 0.20946166959342294, 0.2037521178560513, 0.19899119990835357, 0.20502188897743512, 0.21185012528730773, 0.23029761548989025, 0.2226957927771256, 0.10106476573254897, 0.08282313409598063, 0.0927463241901183, 0.0892673049309205, 0.09187005772305634, 0.11808457473018663, 0.09136297465966581, 0.09200662746312827, 0.09667808533660249]}, "mutation_prompt": null}
{"id": "c7ba15d2-095c-4171-87cb-92a71a9f207d", "solution": "import numpy as np\n\nclass HybridStochasticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.initial_scale_factor = 0.8\n        self.initial_crossover_prob = 0.7\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population = self.opposition_based_initialization()\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.learning_rate = 0.1\n\n    def opposition_based_initialization(self):\n        initial_population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        opposition_population = self.lower_bound + self.upper_bound - initial_population\n        return np.vstack((initial_population, opposition_population))[:self.population_size]\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def select_parents(self, idx):\n        indices = [i for i in range(self.population_size) if i != idx]\n        return np.random.choice(indices, 3, replace=False)\n\n    def differential_evolution_mutation(self, idx, scale_factor):\n        a, b, c = self.select_parents(idx)\n        mutant_vector = self.population[a] + scale_factor * (self.population[b] - self.population[c])\n        return np.clip(mutant_vector, self.lower_bound, self.upper_bound)\n\n    def crossover(self, target_vector, mutant_vector, crossover_prob):\n        crossover_mask = np.random.rand(self.dim) < crossover_prob\n        return np.where(crossover_mask, mutant_vector, target_vector)\n\n    def local_search(self, vector):\n        perturbation = np.random.normal(0, self.learning_rate, self.dim)\n        candidate_vector = vector + perturbation\n        return np.clip(candidate_vector, self.lower_bound, self.upper_bound)\n\n    def __call__(self, func):\n        self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                # Dynamic adjustment of parameters\n                scale_factor = self.initial_scale_factor * (1 - self.evaluations / self.budget)\n                crossover_prob = self.initial_crossover_prob + 0.2 * np.random.rand()\n\n                mutant_vector = self.differential_evolution_mutation(i, scale_factor)\n                trial_vector = self.crossover(self.population[i], mutant_vector, crossover_prob)\n                trial_fitness = func(trial_vector)\n                self.evaluations += 1\n\n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial_vector\n                    self.fitness[i] = trial_fitness\n\n                if np.random.rand() < 0.2:\n                    candidate_vector = self.local_search(self.population[i])\n                    candidate_fitness = func(candidate_vector)\n                    self.evaluations += 1\n                    if candidate_fitness < self.fitness[i]:\n                        self.population[i] = candidate_vector\n                        self.fitness[i] = candidate_fitness\n\n                # Update learning rate adaptively\n                self.learning_rate = max(0.01, self.learning_rate * (1 - 0.05 * np.random.rand()))\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx]", "name": "HybridStochasticOptimizer", "description": "Hybrid optimizer with adaptive learning rate and opposition-based initialization for faster convergence.", "configspace": "", "generation": 32, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "c9aafe6b-332b-4283-9cc1-4b2e50c44e27", "metadata": {"aucs": [0.8614318631710612, 0.8615612395806681, 0.8648092636329068, 0.8836823589578061, 0.8809302815417128, 0.8537149049105373, 0.8649927264859298, 0.8661719092889808, 0.8732297035052712, 0.7784313840335078, 0.7934709027220181, 0.7709929153086685, 0.7666377215322294, 0.7792354193630129, 0.7955283567004858, 0.7874175449423783, 0.7934505999095464, 0.7963508686605465, 0.25196533963541234, 0.603323462921044, 0.31189512717187695, 0.45254508955474515, 0.3855834591332875, 0.6324217482611867, 0.38087210189123466, 0.37101210401952667, 0.37832843410229366, 0.3374576510463305, 0.1599578735010866, 0.25181667196560353, 0.15598715927116646, 0.15926218502651568, 0.14742944278799264, 0.33123888835847937, 0.3066524419836607, 0.16048855283955032, 0.9738501285938751, 0.9538242295949432, 0.9811516406082638, 0.9649564447290745, 0.9726531518905077, 0.9814481843774895, 0.9732999615150277, 0.9721568744354393, 0.9772487896256388, 0.5602477359628759, 0.5166699154516793, 0.49955145811133295, 0.4607762689339385, 0.5168840480364095, 0.5837268635955544, 0.5848178673193583, 0.44902566155463985, 0.6535189332618503, 0.8108564290388902, 0.7472995471422044, 0.7832292739258102, 0.780298104186407, 0.7997326258786164, 0.8307049265401217, 0.7670254946306878, 0.7966436314344594, 0.7737581975614084, 0.26793793612173356, 0.24955593945589905, 0.19834760026021148, 0.22530649729727037, 0.21576791524353967, 0.22803418538233733, 0.23532474250416835, 0.20160318462124072, 0.1920099262564383, 0.19613605298744063, 0.23556007360063813, 0.11440376677945807, 0.22544642163615658, 0.24862040742714708, 0.1874776391649946, 0.19655907028592323, 0.19284064583165672, 0.19577311722397894, 0.3108121261595491, 0.2566448530161354, 0.36431074059207347, 0.24025452278021453, 0.4802140919950957, 0.0007248673300417341, 0.35401012029935763, 0.3692632979788284, 0.276018593628536, 0.3623824055587753, 0.3924469974919602, 0.23556071143414548, 0.35272611497922335, 0.44565667864537895, 0.39468163509167276, 0.4061903350584134, 0.36215287040133537, 0.3973933722095969, 0.11732286633565558, 0.20172957542003978, 0.1498188877382276, 0.24772296138908012, 0.1546062088760296, 0.10282719367721671, 0.20924019897046064, 0.1183165114034116, 0.20632727477440505, 0.2809986328144657, 0.29684536408175155, 0.3175185646986213, 0.3737401167683222, 0.33517695108836454, 0.3271148862228106, 0.36693310045283345, 0.3166293273038533, 0.24146562201228117, 0.6055800258234042, 0.7234496228479592, 0.6438958422770305, 0.7507872332161957, 0.7324506280759618, 0.6589215501985713, 0.7496261142206224, 0.7209822579527148, 0.6855782937737074, 0.1371651233838116, 0.12075132229211483, 0.13780801853641733, 0.12584800341409697, 0.13850225460242616, 0.11675557226216027, 0.12863432580056944, 0.1273098808292441, 0.14268148704984884, 0.19934365172377466, 0.18594466654988895, 0.32832255345101913, 0.23469770515038102, 0.14866333116079655, 0.20267106519038025, 0.20152641892425394, 0.23576091839252133, 0.21421383347767264, 0.5391071467719948, 0.5042943805711715, 0.4612246809606788, 0.47676447358121665, 0.5291640446387131, 0.602638537448555, 0.6246427601161761, 0.5714078963023623, 0.6225968922190244, 0.33304162231889356, 0.3304783841194173, 0.4141875902408757, 0.354629285337982, 0.33990446568261656, 0.27919480653858797, 0.41214607585167573, 0.4488101082230267, 0.4070679191988382, 0.2075439556387948, 0.21303390663090027, 0.21635748499617935, 0.19827783015642464, 0.20259798106785998, 0.2337489300339528, 0.1995085629275789, 0.23000732022010184, 0.20258936164228614, 0.3583455868837565, 0.5974983189015708, 0.6315004735669381, 0.48375114306220435, 0.5228073851654269, 0.6765804181616237, 0.44806218567436396, 0.4405846418933689, 0.2231537031442834, 0.15177066878967904, 0.19069875851639195, 0.8538083672200958, 0.8489568076127689, 0.8369418101551436, 0.7959958487406582, 0.1809055777063855, 0.19441884874194382, 0.6735733685324677, 0.653629308683124, 0.16602485673297018, 0.20335804099962917, 0.20133806574003343, 0.20063819684592765, 0.4267483879660453, 0.203500441501264, 0.12468842409948466, 0.7601496747285006, 0.21347654253114523, 0.20557946271286787, 0.20946166959342294, 0.2037521178560513, 0.19899119990835357, 0.20502188897743512, 0.21185012528730773, 0.23029761548989025, 0.2226957927771256, 0.10106476573254897, 0.08282313409598063, 0.0927463241901183, 0.0892673049309205, 0.09187005772305634, 0.11808457473018663, 0.09136297465966581, 0.09200662746312827, 0.09667808533660249]}, "mutation_prompt": null}
{"id": "57e0faac-5c16-40eb-b3ff-f8ac5021e782", "solution": "import numpy as np\n\nclass HybridStochasticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.initial_scale_factor = 0.8\n        self.initial_crossover_prob = 0.7\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population = self.opposition_based_initialization()\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.learning_rate = 0.1\n\n    def opposition_based_initialization(self):\n        initial_population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        opposition_population = self.lower_bound + self.upper_bound - initial_population\n        return np.vstack((initial_population, opposition_population))[:self.population_size]\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def select_parents(self, idx):\n        indices = [i for i in range(self.population_size) if i != idx]\n        return np.random.choice(indices, 3, replace=False)\n\n    def differential_evolution_mutation(self, idx, scale_factor):\n        a, b, c = self.select_parents(idx)\n        mutant_vector = self.population[a] + scale_factor * (self.population[b] - self.population[c])\n        return np.clip(mutant_vector, self.lower_bound, self.upper_bound)\n\n    def crossover(self, target_vector, mutant_vector, crossover_prob):\n        crossover_mask = np.random.rand(self.dim) < crossover_prob\n        return np.where(crossover_mask, mutant_vector, target_vector)\n\n    def local_search(self, vector):\n        perturbation = np.random.normal(0, self.learning_rate, self.dim)\n        candidate_vector = vector + perturbation\n        return np.clip(candidate_vector, self.lower_bound, self.upper_bound)\n\n    def __call__(self, func):\n        self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                # Dynamic adjustment of parameters\n                scale_factor = self.initial_scale_factor * (1 - self.evaluations / self.budget)\n                crossover_prob = self.initial_crossover_prob + 0.2 * np.random.rand()\n\n                mutant_vector = self.differential_evolution_mutation(i, scale_factor)\n                trial_vector = self.crossover(self.population[i], mutant_vector, crossover_prob)\n                trial_fitness = func(trial_vector)\n                self.evaluations += 1\n\n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial_vector\n                    self.fitness[i] = trial_fitness\n\n                if np.random.rand() < 0.2:\n                    candidate_vector = self.local_search(self.population[i])\n                    candidate_fitness = func(candidate_vector)\n                    self.evaluations += 1\n                    if candidate_fitness < self.fitness[i]:\n                        self.population[i] = candidate_vector\n                        self.fitness[i] = candidate_fitness\n\n                # Update learning rate adaptively\n                self.learning_rate = max(0.01, self.learning_rate * (1 - 0.05 * np.random.rand()))\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx]", "name": "HybridStochasticOptimizer", "description": "Hybrid optimizer with adaptive learning rate and opposition-based initialization for faster convergence.", "configspace": "", "generation": 32, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "c9aafe6b-332b-4283-9cc1-4b2e50c44e27", "metadata": {"aucs": [0.8614318631710612, 0.8615612395806681, 0.8648092636329068, 0.8836823589578061, 0.8809302815417128, 0.8537149049105373, 0.8649927264859298, 0.8661719092889808, 0.8732297035052712, 0.7784313840335078, 0.7934709027220181, 0.7709929153086685, 0.7666377215322294, 0.7792354193630129, 0.7955283567004858, 0.7874175449423783, 0.7934505999095464, 0.7963508686605465, 0.25196533963541234, 0.603323462921044, 0.31189512717187695, 0.45254508955474515, 0.3855834591332875, 0.6324217482611867, 0.38087210189123466, 0.37101210401952667, 0.37832843410229366, 0.3374576510463305, 0.1599578735010866, 0.25181667196560353, 0.15598715927116646, 0.15926218502651568, 0.14742944278799264, 0.33123888835847937, 0.3066524419836607, 0.16048855283955032, 0.9738501285938751, 0.9538242295949432, 0.9811516406082638, 0.9649564447290745, 0.9726531518905077, 0.9814481843774895, 0.9732999615150277, 0.9721568744354393, 0.9772487896256388, 0.5602477359628759, 0.5166699154516793, 0.49955145811133295, 0.4607762689339385, 0.5168840480364095, 0.5837268635955544, 0.5848178673193583, 0.44902566155463985, 0.6535189332618503, 0.8108564290388902, 0.7472995471422044, 0.7832292739258102, 0.780298104186407, 0.7997326258786164, 0.8307049265401217, 0.7670254946306878, 0.7966436314344594, 0.7737581975614084, 0.26793793612173356, 0.24955593945589905, 0.19834760026021148, 0.22530649729727037, 0.21576791524353967, 0.22803418538233733, 0.23532474250416835, 0.20160318462124072, 0.1920099262564383, 0.19613605298744063, 0.23556007360063813, 0.11440376677945807, 0.22544642163615658, 0.24862040742714708, 0.1874776391649946, 0.19655907028592323, 0.19284064583165672, 0.19577311722397894, 0.3108121261595491, 0.2566448530161354, 0.36431074059207347, 0.24025452278021453, 0.4802140919950957, 0.0007248673300417341, 0.35401012029935763, 0.3692632979788284, 0.276018593628536, 0.3623824055587753, 0.3924469974919602, 0.23556071143414548, 0.35272611497922335, 0.44565667864537895, 0.39468163509167276, 0.4061903350584134, 0.36215287040133537, 0.3973933722095969, 0.11732286633565558, 0.20172957542003978, 0.1498188877382276, 0.24772296138908012, 0.1546062088760296, 0.10282719367721671, 0.20924019897046064, 0.1183165114034116, 0.20632727477440505, 0.2809986328144657, 0.29684536408175155, 0.3175185646986213, 0.3737401167683222, 0.33517695108836454, 0.3271148862228106, 0.36693310045283345, 0.3166293273038533, 0.24146562201228117, 0.6055800258234042, 0.7234496228479592, 0.6438958422770305, 0.7507872332161957, 0.7324506280759618, 0.6589215501985713, 0.7496261142206224, 0.7209822579527148, 0.6855782937737074, 0.1371651233838116, 0.12075132229211483, 0.13780801853641733, 0.12584800341409697, 0.13850225460242616, 0.11675557226216027, 0.12863432580056944, 0.1273098808292441, 0.14268148704984884, 0.19934365172377466, 0.18594466654988895, 0.32832255345101913, 0.23469770515038102, 0.14866333116079655, 0.20267106519038025, 0.20152641892425394, 0.23576091839252133, 0.21421383347767264, 0.5391071467719948, 0.5042943805711715, 0.4612246809606788, 0.47676447358121665, 0.5291640446387131, 0.602638537448555, 0.6246427601161761, 0.5714078963023623, 0.6225968922190244, 0.33304162231889356, 0.3304783841194173, 0.4141875902408757, 0.354629285337982, 0.33990446568261656, 0.27919480653858797, 0.41214607585167573, 0.4488101082230267, 0.4070679191988382, 0.2075439556387948, 0.21303390663090027, 0.21635748499617935, 0.19827783015642464, 0.20259798106785998, 0.2337489300339528, 0.1995085629275789, 0.23000732022010184, 0.20258936164228614, 0.3583455868837565, 0.5974983189015708, 0.6315004735669381, 0.48375114306220435, 0.5228073851654269, 0.6765804181616237, 0.44806218567436396, 0.4405846418933689, 0.2231537031442834, 0.15177066878967904, 0.19069875851639195, 0.8538083672200958, 0.8489568076127689, 0.8369418101551436, 0.7959958487406582, 0.1809055777063855, 0.19441884874194382, 0.6735733685324677, 0.653629308683124, 0.16602485673297018, 0.20335804099962917, 0.20133806574003343, 0.20063819684592765, 0.4267483879660453, 0.203500441501264, 0.12468842409948466, 0.7601496747285006, 0.21347654253114523, 0.20557946271286787, 0.20946166959342294, 0.2037521178560513, 0.19899119990835357, 0.20502188897743512, 0.21185012528730773, 0.23029761548989025, 0.2226957927771256, 0.10106476573254897, 0.08282313409598063, 0.0927463241901183, 0.0892673049309205, 0.09187005772305634, 0.11808457473018663, 0.09136297465966581, 0.09200662746312827, 0.09667808533660249]}, "mutation_prompt": null}
{"id": "08c87dbe-8302-4e14-9420-5b5e5d40901d", "solution": "import numpy as np\n\nclass HybridStochasticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.initial_scale_factor = 0.8\n        self.initial_crossover_prob = 0.7\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population = self.opposition_based_initialization()\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.learning_rate = 0.1\n\n    def opposition_based_initialization(self):\n        initial_population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        opposition_population = self.lower_bound + self.upper_bound - initial_population\n        return np.vstack((initial_population, opposition_population))[:self.population_size]\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def select_parents(self, idx):\n        indices = [i for i in range(self.population_size) if i != idx]\n        return np.random.choice(indices, 3, replace=False)\n\n    def differential_evolution_mutation(self, idx, scale_factor):\n        a, b, c = self.select_parents(idx)\n        mutant_vector = self.population[a] + scale_factor * (self.population[b] - self.population[c])\n        return np.clip(mutant_vector, self.lower_bound, self.upper_bound)\n\n    def crossover(self, target_vector, mutant_vector, crossover_prob):\n        crossover_mask = np.random.rand(self.dim) < crossover_prob\n        return np.where(crossover_mask, mutant_vector, target_vector)\n\n    def local_search(self, vector):\n        perturbation = np.random.normal(0, self.learning_rate, self.dim)\n        candidate_vector = vector + perturbation\n        return np.clip(candidate_vector, self.lower_bound, self.upper_bound)\n\n    def __call__(self, func):\n        self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                # Dynamic adjustment of parameters\n                scale_factor = self.initial_scale_factor * (1 - self.evaluations / self.budget)\n                crossover_prob = self.initial_crossover_prob + 0.2 * np.random.rand()\n\n                mutant_vector = self.differential_evolution_mutation(i, scale_factor)\n                trial_vector = self.crossover(self.population[i], mutant_vector, crossover_prob)\n                trial_fitness = func(trial_vector)\n                self.evaluations += 1\n\n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial_vector\n                    self.fitness[i] = trial_fitness\n\n                if np.random.rand() < 0.2:\n                    candidate_vector = self.local_search(self.population[i])\n                    candidate_fitness = func(candidate_vector)\n                    self.evaluations += 1\n                    if candidate_fitness < self.fitness[i]:\n                        self.population[i] = candidate_vector\n                        self.fitness[i] = candidate_fitness\n\n                # Update learning rate adaptively\n                self.learning_rate = max(0.01, self.learning_rate * (1 - 0.05 * np.random.rand()))\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx]", "name": "HybridStochasticOptimizer", "description": "Hybrid optimizer with adaptive learning rate and opposition-based initialization for faster convergence.", "configspace": "", "generation": 32, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "c9aafe6b-332b-4283-9cc1-4b2e50c44e27", "metadata": {"aucs": [0.8614318631710612, 0.8615612395806681, 0.8648092636329068, 0.8836823589578061, 0.8809302815417128, 0.8537149049105373, 0.8649927264859298, 0.8661719092889808, 0.8732297035052712, 0.7784313840335078, 0.7934709027220181, 0.7709929153086685, 0.7666377215322294, 0.7792354193630129, 0.7955283567004858, 0.7874175449423783, 0.7934505999095464, 0.7963508686605465, 0.25196533963541234, 0.603323462921044, 0.31189512717187695, 0.45254508955474515, 0.3855834591332875, 0.6324217482611867, 0.38087210189123466, 0.37101210401952667, 0.37832843410229366, 0.3374576510463305, 0.1599578735010866, 0.25181667196560353, 0.15598715927116646, 0.15926218502651568, 0.14742944278799264, 0.33123888835847937, 0.3066524419836607, 0.16048855283955032, 0.9738501285938751, 0.9538242295949432, 0.9811516406082638, 0.9649564447290745, 0.9726531518905077, 0.9814481843774895, 0.9732999615150277, 0.9721568744354393, 0.9772487896256388, 0.5602477359628759, 0.5166699154516793, 0.49955145811133295, 0.4607762689339385, 0.5168840480364095, 0.5837268635955544, 0.5848178673193583, 0.44902566155463985, 0.6535189332618503, 0.8108564290388902, 0.7472995471422044, 0.7832292739258102, 0.780298104186407, 0.7997326258786164, 0.8307049265401217, 0.7670254946306878, 0.7966436314344594, 0.7737581975614084, 0.26793793612173356, 0.24955593945589905, 0.19834760026021148, 0.22530649729727037, 0.21576791524353967, 0.22803418538233733, 0.23532474250416835, 0.20160318462124072, 0.1920099262564383, 0.19613605298744063, 0.23556007360063813, 0.11440376677945807, 0.22544642163615658, 0.24862040742714708, 0.1874776391649946, 0.19655907028592323, 0.19284064583165672, 0.19577311722397894, 0.3108121261595491, 0.2566448530161354, 0.36431074059207347, 0.24025452278021453, 0.4802140919950957, 0.0007248673300417341, 0.35401012029935763, 0.3692632979788284, 0.276018593628536, 0.3623824055587753, 0.3924469974919602, 0.23556071143414548, 0.35272611497922335, 0.44565667864537895, 0.39468163509167276, 0.4061903350584134, 0.36215287040133537, 0.3973933722095969, 0.11732286633565558, 0.20172957542003978, 0.1498188877382276, 0.24772296138908012, 0.1546062088760296, 0.10282719367721671, 0.20924019897046064, 0.1183165114034116, 0.20632727477440505, 0.2809986328144657, 0.29684536408175155, 0.3175185646986213, 0.3737401167683222, 0.33517695108836454, 0.3271148862228106, 0.36693310045283345, 0.3166293273038533, 0.24146562201228117, 0.6055800258234042, 0.7234496228479592, 0.6438958422770305, 0.7507872332161957, 0.7324506280759618, 0.6589215501985713, 0.7496261142206224, 0.7209822579527148, 0.6855782937737074, 0.1371651233838116, 0.12075132229211483, 0.13780801853641733, 0.12584800341409697, 0.13850225460242616, 0.11675557226216027, 0.12863432580056944, 0.1273098808292441, 0.14268148704984884, 0.19934365172377466, 0.18594466654988895, 0.32832255345101913, 0.23469770515038102, 0.14866333116079655, 0.20267106519038025, 0.20152641892425394, 0.23576091839252133, 0.21421383347767264, 0.5391071467719948, 0.5042943805711715, 0.4612246809606788, 0.47676447358121665, 0.5291640446387131, 0.602638537448555, 0.6246427601161761, 0.5714078963023623, 0.6225968922190244, 0.33304162231889356, 0.3304783841194173, 0.4141875902408757, 0.354629285337982, 0.33990446568261656, 0.27919480653858797, 0.41214607585167573, 0.4488101082230267, 0.4070679191988382, 0.2075439556387948, 0.21303390663090027, 0.21635748499617935, 0.19827783015642464, 0.20259798106785998, 0.2337489300339528, 0.1995085629275789, 0.23000732022010184, 0.20258936164228614, 0.3583455868837565, 0.5974983189015708, 0.6315004735669381, 0.48375114306220435, 0.5228073851654269, 0.6765804181616237, 0.44806218567436396, 0.4405846418933689, 0.2231537031442834, 0.15177066878967904, 0.19069875851639195, 0.8538083672200958, 0.8489568076127689, 0.8369418101551436, 0.7959958487406582, 0.1809055777063855, 0.19441884874194382, 0.6735733685324677, 0.653629308683124, 0.16602485673297018, 0.20335804099962917, 0.20133806574003343, 0.20063819684592765, 0.4267483879660453, 0.203500441501264, 0.12468842409948466, 0.7601496747285006, 0.21347654253114523, 0.20557946271286787, 0.20946166959342294, 0.2037521178560513, 0.19899119990835357, 0.20502188897743512, 0.21185012528730773, 0.23029761548989025, 0.2226957927771256, 0.10106476573254897, 0.08282313409598063, 0.0927463241901183, 0.0892673049309205, 0.09187005772305634, 0.11808457473018663, 0.09136297465966581, 0.09200662746312827, 0.09667808533660249]}, "mutation_prompt": null}
{"id": "c349e503-6ffe-413f-bc0c-d3bc2e2aa811", "solution": "import numpy as np\n\nclass HybridStochasticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.initial_scale_factor = 0.8\n        self.initial_crossover_prob = 0.7\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population = self.opposition_based_initialization()\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.learning_rate = 0.1\n\n    def opposition_based_initialization(self):\n        initial_population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        opposition_population = self.lower_bound + self.upper_bound - initial_population\n        return np.vstack((initial_population, opposition_population))[:self.population_size]\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def select_parents(self, idx):\n        indices = [i for i in range(self.population_size) if i != idx]\n        return np.random.choice(indices, 3, replace=False)\n\n    def differential_evolution_mutation(self, idx, scale_factor):\n        a, b, c = self.select_parents(idx)\n        mutant_vector = self.population[a] + scale_factor * (self.population[b] - self.population[c])\n        return np.clip(mutant_vector, self.lower_bound, self.upper_bound)\n\n    def crossover(self, target_vector, mutant_vector, crossover_prob):\n        crossover_mask = np.random.rand(self.dim) < crossover_prob\n        return np.where(crossover_mask, mutant_vector, target_vector)\n\n    def local_search(self, vector):\n        perturbation = np.random.normal(0, self.learning_rate, self.dim)\n        candidate_vector = vector + perturbation\n        return np.clip(candidate_vector, self.lower_bound, self.upper_bound)\n\n    def __call__(self, func):\n        self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                # Dynamic adjustment of parameters\n                scale_factor = self.initial_scale_factor * (1 - self.evaluations / self.budget)\n                crossover_prob = self.initial_crossover_prob + 0.2 * np.random.rand()\n\n                mutant_vector = self.differential_evolution_mutation(i, scale_factor)\n                trial_vector = self.crossover(self.population[i], mutant_vector, crossover_prob)\n                trial_fitness = func(trial_vector)\n                self.evaluations += 1\n\n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial_vector\n                    self.fitness[i] = trial_fitness\n\n                if np.random.rand() < 0.2:\n                    candidate_vector = self.local_search(self.population[i])\n                    candidate_fitness = func(candidate_vector)\n                    self.evaluations += 1\n                    if candidate_fitness < self.fitness[i]:\n                        self.population[i] = candidate_vector\n                        self.fitness[i] = candidate_fitness\n\n                # Update learning rate adaptively\n                self.learning_rate = max(0.01, self.learning_rate * (1 - 0.05 * np.random.rand()))\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx]", "name": "HybridStochasticOptimizer", "description": "Hybrid optimizer with adaptive learning rate and opposition-based initialization for faster convergence.", "configspace": "", "generation": 32, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "c9aafe6b-332b-4283-9cc1-4b2e50c44e27", "metadata": {"aucs": [0.8614318631710612, 0.8615612395806681, 0.8648092636329068, 0.8836823589578061, 0.8809302815417128, 0.8537149049105373, 0.8649927264859298, 0.8661719092889808, 0.8732297035052712, 0.7784313840335078, 0.7934709027220181, 0.7709929153086685, 0.7666377215322294, 0.7792354193630129, 0.7955283567004858, 0.7874175449423783, 0.7934505999095464, 0.7963508686605465, 0.25196533963541234, 0.603323462921044, 0.31189512717187695, 0.45254508955474515, 0.3855834591332875, 0.6324217482611867, 0.38087210189123466, 0.37101210401952667, 0.37832843410229366, 0.3374576510463305, 0.1599578735010866, 0.25181667196560353, 0.15598715927116646, 0.15926218502651568, 0.14742944278799264, 0.33123888835847937, 0.3066524419836607, 0.16048855283955032, 0.9738501285938751, 0.9538242295949432, 0.9811516406082638, 0.9649564447290745, 0.9726531518905077, 0.9814481843774895, 0.9732999615150277, 0.9721568744354393, 0.9772487896256388, 0.5602477359628759, 0.5166699154516793, 0.49955145811133295, 0.4607762689339385, 0.5168840480364095, 0.5837268635955544, 0.5848178673193583, 0.44902566155463985, 0.6535189332618503, 0.8108564290388902, 0.7472995471422044, 0.7832292739258102, 0.780298104186407, 0.7997326258786164, 0.8307049265401217, 0.7670254946306878, 0.7966436314344594, 0.7737581975614084, 0.26793793612173356, 0.24955593945589905, 0.19834760026021148, 0.22530649729727037, 0.21576791524353967, 0.22803418538233733, 0.23532474250416835, 0.20160318462124072, 0.1920099262564383, 0.19613605298744063, 0.23556007360063813, 0.11440376677945807, 0.22544642163615658, 0.24862040742714708, 0.1874776391649946, 0.19655907028592323, 0.19284064583165672, 0.19577311722397894, 0.3108121261595491, 0.2566448530161354, 0.36431074059207347, 0.24025452278021453, 0.4802140919950957, 0.0007248673300417341, 0.35401012029935763, 0.3692632979788284, 0.276018593628536, 0.3623824055587753, 0.3924469974919602, 0.23556071143414548, 0.35272611497922335, 0.44565667864537895, 0.39468163509167276, 0.4061903350584134, 0.36215287040133537, 0.3973933722095969, 0.11732286633565558, 0.20172957542003978, 0.1498188877382276, 0.24772296138908012, 0.1546062088760296, 0.10282719367721671, 0.20924019897046064, 0.1183165114034116, 0.20632727477440505, 0.2809986328144657, 0.29684536408175155, 0.3175185646986213, 0.3737401167683222, 0.33517695108836454, 0.3271148862228106, 0.36693310045283345, 0.3166293273038533, 0.24146562201228117, 0.6055800258234042, 0.7234496228479592, 0.6438958422770305, 0.7507872332161957, 0.7324506280759618, 0.6589215501985713, 0.7496261142206224, 0.7209822579527148, 0.6855782937737074, 0.1371651233838116, 0.12075132229211483, 0.13780801853641733, 0.12584800341409697, 0.13850225460242616, 0.11675557226216027, 0.12863432580056944, 0.1273098808292441, 0.14268148704984884, 0.19934365172377466, 0.18594466654988895, 0.32832255345101913, 0.23469770515038102, 0.14866333116079655, 0.20267106519038025, 0.20152641892425394, 0.23576091839252133, 0.21421383347767264, 0.5391071467719948, 0.5042943805711715, 0.4612246809606788, 0.47676447358121665, 0.5291640446387131, 0.602638537448555, 0.6246427601161761, 0.5714078963023623, 0.6225968922190244, 0.33304162231889356, 0.3304783841194173, 0.4141875902408757, 0.354629285337982, 0.33990446568261656, 0.27919480653858797, 0.41214607585167573, 0.4488101082230267, 0.4070679191988382, 0.2075439556387948, 0.21303390663090027, 0.21635748499617935, 0.19827783015642464, 0.20259798106785998, 0.2337489300339528, 0.1995085629275789, 0.23000732022010184, 0.20258936164228614, 0.3583455868837565, 0.5974983189015708, 0.6315004735669381, 0.48375114306220435, 0.5228073851654269, 0.6765804181616237, 0.44806218567436396, 0.4405846418933689, 0.2231537031442834, 0.15177066878967904, 0.19069875851639195, 0.8538083672200958, 0.8489568076127689, 0.8369418101551436, 0.7959958487406582, 0.1809055777063855, 0.19441884874194382, 0.6735733685324677, 0.653629308683124, 0.16602485673297018, 0.20335804099962917, 0.20133806574003343, 0.20063819684592765, 0.4267483879660453, 0.203500441501264, 0.12468842409948466, 0.7601496747285006, 0.21347654253114523, 0.20557946271286787, 0.20946166959342294, 0.2037521178560513, 0.19899119990835357, 0.20502188897743512, 0.21185012528730773, 0.23029761548989025, 0.2226957927771256, 0.10106476573254897, 0.08282313409598063, 0.0927463241901183, 0.0892673049309205, 0.09187005772305634, 0.11808457473018663, 0.09136297465966581, 0.09200662746312827, 0.09667808533660249]}, "mutation_prompt": null}
{"id": "c31ec8ef-de7e-4545-a8af-82b1a1685638", "solution": "import numpy as np\n\nclass HybridStochasticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.initial_scale_factor = 0.8\n        self.initial_crossover_prob = 0.7\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population = self.opposition_based_initialization()\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.learning_rate = 0.1\n\n    def opposition_based_initialization(self):\n        initial_population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        opposition_population = self.lower_bound + self.upper_bound - initial_population\n        return np.vstack((initial_population, opposition_population))[:self.population_size]\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def select_parents(self, idx):\n        indices = [i for i in range(self.population_size) if i != idx]\n        return np.random.choice(indices, 3, replace=False)\n\n    def differential_evolution_mutation(self, idx, scale_factor):\n        a, b, c = self.select_parents(idx)\n        mutant_vector = self.population[a] + scale_factor * (self.population[b] - self.population[c])\n        return np.clip(mutant_vector, self.lower_bound, self.upper_bound)\n\n    def crossover(self, target_vector, mutant_vector, crossover_prob):\n        crossover_mask = np.random.rand(self.dim) < crossover_prob\n        return np.where(crossover_mask, mutant_vector, target_vector)\n\n    def local_search(self, vector):\n        perturbation = np.random.normal(0, self.learning_rate, self.dim)\n        candidate_vector = vector + perturbation\n        return np.clip(candidate_vector, self.lower_bound, self.upper_bound)\n\n    def __call__(self, func):\n        self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                # Dynamic adjustment of parameters\n                scale_factor = self.initial_scale_factor * (1 - self.evaluations / self.budget)\n                crossover_prob = self.initial_crossover_prob + 0.2 * np.random.rand()\n\n                mutant_vector = self.differential_evolution_mutation(i, scale_factor)\n                trial_vector = self.crossover(self.population[i], mutant_vector, crossover_prob)\n                trial_fitness = func(trial_vector)\n                self.evaluations += 1\n\n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial_vector\n                    self.fitness[i] = trial_fitness\n\n                if np.random.rand() < 0.2:\n                    candidate_vector = self.local_search(self.population[i])\n                    candidate_fitness = func(candidate_vector)\n                    self.evaluations += 1\n                    if candidate_fitness < self.fitness[i]:\n                        self.population[i] = candidate_vector\n                        self.fitness[i] = candidate_fitness\n\n                # Update learning rate adaptively\n                self.learning_rate = max(0.01, self.learning_rate * (1 - 0.05 * np.random.rand()))\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx]", "name": "HybridStochasticOptimizer", "description": "Hybrid optimizer with adaptive learning rate and opposition-based initialization for faster convergence.", "configspace": "", "generation": 32, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "c9aafe6b-332b-4283-9cc1-4b2e50c44e27", "metadata": {"aucs": [0.8614318631710612, 0.8615612395806681, 0.8648092636329068, 0.8836823589578061, 0.8809302815417128, 0.8537149049105373, 0.8649927264859298, 0.8661719092889808, 0.8732297035052712, 0.7784313840335078, 0.7934709027220181, 0.7709929153086685, 0.7666377215322294, 0.7792354193630129, 0.7955283567004858, 0.7874175449423783, 0.7934505999095464, 0.7963508686605465, 0.25196533963541234, 0.603323462921044, 0.31189512717187695, 0.45254508955474515, 0.3855834591332875, 0.6324217482611867, 0.38087210189123466, 0.37101210401952667, 0.37832843410229366, 0.3374576510463305, 0.1599578735010866, 0.25181667196560353, 0.15598715927116646, 0.15926218502651568, 0.14742944278799264, 0.33123888835847937, 0.3066524419836607, 0.16048855283955032, 0.9738501285938751, 0.9538242295949432, 0.9811516406082638, 0.9649564447290745, 0.9726531518905077, 0.9814481843774895, 0.9732999615150277, 0.9721568744354393, 0.9772487896256388, 0.5602477359628759, 0.5166699154516793, 0.49955145811133295, 0.4607762689339385, 0.5168840480364095, 0.5837268635955544, 0.5848178673193583, 0.44902566155463985, 0.6535189332618503, 0.8108564290388902, 0.7472995471422044, 0.7832292739258102, 0.780298104186407, 0.7997326258786164, 0.8307049265401217, 0.7670254946306878, 0.7966436314344594, 0.7737581975614084, 0.26793793612173356, 0.24955593945589905, 0.19834760026021148, 0.22530649729727037, 0.21576791524353967, 0.22803418538233733, 0.23532474250416835, 0.20160318462124072, 0.1920099262564383, 0.19613605298744063, 0.23556007360063813, 0.11440376677945807, 0.22544642163615658, 0.24862040742714708, 0.1874776391649946, 0.19655907028592323, 0.19284064583165672, 0.19577311722397894, 0.3108121261595491, 0.2566448530161354, 0.36431074059207347, 0.24025452278021453, 0.4802140919950957, 0.0007248673300417341, 0.35401012029935763, 0.3692632979788284, 0.276018593628536, 0.3623824055587753, 0.3924469974919602, 0.23556071143414548, 0.35272611497922335, 0.44565667864537895, 0.39468163509167276, 0.4061903350584134, 0.36215287040133537, 0.3973933722095969, 0.11732286633565558, 0.20172957542003978, 0.1498188877382276, 0.24772296138908012, 0.1546062088760296, 0.10282719367721671, 0.20924019897046064, 0.1183165114034116, 0.20632727477440505, 0.2809986328144657, 0.29684536408175155, 0.3175185646986213, 0.3737401167683222, 0.33517695108836454, 0.3271148862228106, 0.36693310045283345, 0.3166293273038533, 0.24146562201228117, 0.6055800258234042, 0.7234496228479592, 0.6438958422770305, 0.7507872332161957, 0.7324506280759618, 0.6589215501985713, 0.7496261142206224, 0.7209822579527148, 0.6855782937737074, 0.1371651233838116, 0.12075132229211483, 0.13780801853641733, 0.12584800341409697, 0.13850225460242616, 0.11675557226216027, 0.12863432580056944, 0.1273098808292441, 0.14268148704984884, 0.19934365172377466, 0.18594466654988895, 0.32832255345101913, 0.23469770515038102, 0.14866333116079655, 0.20267106519038025, 0.20152641892425394, 0.23576091839252133, 0.21421383347767264, 0.5391071467719948, 0.5042943805711715, 0.4612246809606788, 0.47676447358121665, 0.5291640446387131, 0.602638537448555, 0.6246427601161761, 0.5714078963023623, 0.6225968922190244, 0.33304162231889356, 0.3304783841194173, 0.4141875902408757, 0.354629285337982, 0.33990446568261656, 0.27919480653858797, 0.41214607585167573, 0.4488101082230267, 0.4070679191988382, 0.2075439556387948, 0.21303390663090027, 0.21635748499617935, 0.19827783015642464, 0.20259798106785998, 0.2337489300339528, 0.1995085629275789, 0.23000732022010184, 0.20258936164228614, 0.3583455868837565, 0.5974983189015708, 0.6315004735669381, 0.48375114306220435, 0.5228073851654269, 0.6765804181616237, 0.44806218567436396, 0.4405846418933689, 0.2231537031442834, 0.15177066878967904, 0.19069875851639195, 0.8538083672200958, 0.8489568076127689, 0.8369418101551436, 0.7959958487406582, 0.1809055777063855, 0.19441884874194382, 0.6735733685324677, 0.653629308683124, 0.16602485673297018, 0.20335804099962917, 0.20133806574003343, 0.20063819684592765, 0.4267483879660453, 0.203500441501264, 0.12468842409948466, 0.7601496747285006, 0.21347654253114523, 0.20557946271286787, 0.20946166959342294, 0.2037521178560513, 0.19899119990835357, 0.20502188897743512, 0.21185012528730773, 0.23029761548989025, 0.2226957927771256, 0.10106476573254897, 0.08282313409598063, 0.0927463241901183, 0.0892673049309205, 0.09187005772305634, 0.11808457473018663, 0.09136297465966581, 0.09200662746312827, 0.09667808533660249]}, "mutation_prompt": null}
{"id": "2976618f-905b-4074-b166-6bcb70be9db1", "solution": "import numpy as np\n\nclass HybridStochasticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.initial_scale_factor = 0.8\n        self.initial_crossover_prob = 0.7\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population = self.opposition_based_initialization()\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.learning_rate = 0.1\n\n    def opposition_based_initialization(self):\n        initial_population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        opposition_population = self.lower_bound + self.upper_bound - initial_population\n        return np.vstack((initial_population, opposition_population))[:self.population_size]\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def select_parents(self, idx):\n        indices = [i for i in range(self.population_size) if i != idx]\n        return np.random.choice(indices, 3, replace=False)\n\n    def differential_evolution_mutation(self, idx, scale_factor):\n        a, b, c = self.select_parents(idx)\n        mutant_vector = self.population[a] + scale_factor * (self.population[b] - self.population[c])\n        return np.clip(mutant_vector, self.lower_bound, self.upper_bound)\n\n    def crossover(self, target_vector, mutant_vector, crossover_prob):\n        crossover_mask = np.random.rand(self.dim) < crossover_prob\n        return np.where(crossover_mask, mutant_vector, target_vector)\n\n    def local_search(self, vector):\n        perturbation = np.random.normal(0, self.learning_rate, self.dim)\n        candidate_vector = vector + perturbation\n        return np.clip(candidate_vector, self.lower_bound, self.upper_bound)\n\n    def __call__(self, func):\n        self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                # Dynamic adjustment of parameters\n                scale_factor = self.initial_scale_factor * (1 - self.evaluations / self.budget)\n                crossover_prob = self.initial_crossover_prob + 0.2 * np.random.rand()\n\n                mutant_vector = self.differential_evolution_mutation(i, scale_factor)\n                trial_vector = self.crossover(self.population[i], mutant_vector, crossover_prob)\n                trial_fitness = func(trial_vector)\n                self.evaluations += 1\n\n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial_vector\n                    self.fitness[i] = trial_fitness\n\n                if np.random.rand() < 0.2:\n                    candidate_vector = self.local_search(self.population[i])\n                    candidate_fitness = func(candidate_vector)\n                    self.evaluations += 1\n                    if candidate_fitness < self.fitness[i]:\n                        self.population[i] = candidate_vector\n                        self.fitness[i] = candidate_fitness\n\n                # Update learning rate adaptively\n                self.learning_rate = max(0.01, self.learning_rate * (1 - 0.05 * np.random.rand()))\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx]", "name": "HybridStochasticOptimizer", "description": "Hybrid optimizer with adaptive learning rate and opposition-based initialization for faster convergence.", "configspace": "", "generation": 32, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "c9aafe6b-332b-4283-9cc1-4b2e50c44e27", "metadata": {"aucs": [0.8614318631710612, 0.8615612395806681, 0.8648092636329068, 0.8836823589578061, 0.8809302815417128, 0.8537149049105373, 0.8649927264859298, 0.8661719092889808, 0.8732297035052712, 0.7784313840335078, 0.7934709027220181, 0.7709929153086685, 0.7666377215322294, 0.7792354193630129, 0.7955283567004858, 0.7874175449423783, 0.7934505999095464, 0.7963508686605465, 0.25196533963541234, 0.603323462921044, 0.31189512717187695, 0.45254508955474515, 0.3855834591332875, 0.6324217482611867, 0.38087210189123466, 0.37101210401952667, 0.37832843410229366, 0.3374576510463305, 0.1599578735010866, 0.25181667196560353, 0.15598715927116646, 0.15926218502651568, 0.14742944278799264, 0.33123888835847937, 0.3066524419836607, 0.16048855283955032, 0.9738501285938751, 0.9538242295949432, 0.9811516406082638, 0.9649564447290745, 0.9726531518905077, 0.9814481843774895, 0.9732999615150277, 0.9721568744354393, 0.9772487896256388, 0.5602477359628759, 0.5166699154516793, 0.49955145811133295, 0.4607762689339385, 0.5168840480364095, 0.5837268635955544, 0.5848178673193583, 0.44902566155463985, 0.6535189332618503, 0.8108564290388902, 0.7472995471422044, 0.7832292739258102, 0.780298104186407, 0.7997326258786164, 0.8307049265401217, 0.7670254946306878, 0.7966436314344594, 0.7737581975614084, 0.26793793612173356, 0.24955593945589905, 0.19834760026021148, 0.22530649729727037, 0.21576791524353967, 0.22803418538233733, 0.23532474250416835, 0.20160318462124072, 0.1920099262564383, 0.19613605298744063, 0.23556007360063813, 0.11440376677945807, 0.22544642163615658, 0.24862040742714708, 0.1874776391649946, 0.19655907028592323, 0.19284064583165672, 0.19577311722397894, 0.3108121261595491, 0.2566448530161354, 0.36431074059207347, 0.24025452278021453, 0.4802140919950957, 0.0007248673300417341, 0.35401012029935763, 0.3692632979788284, 0.276018593628536, 0.3623824055587753, 0.3924469974919602, 0.23556071143414548, 0.35272611497922335, 0.44565667864537895, 0.39468163509167276, 0.4061903350584134, 0.36215287040133537, 0.3973933722095969, 0.11732286633565558, 0.20172957542003978, 0.1498188877382276, 0.24772296138908012, 0.1546062088760296, 0.10282719367721671, 0.20924019897046064, 0.1183165114034116, 0.20632727477440505, 0.2809986328144657, 0.29684536408175155, 0.3175185646986213, 0.3737401167683222, 0.33517695108836454, 0.3271148862228106, 0.36693310045283345, 0.3166293273038533, 0.24146562201228117, 0.6055800258234042, 0.7234496228479592, 0.6438958422770305, 0.7507872332161957, 0.7324506280759618, 0.6589215501985713, 0.7496261142206224, 0.7209822579527148, 0.6855782937737074, 0.1371651233838116, 0.12075132229211483, 0.13780801853641733, 0.12584800341409697, 0.13850225460242616, 0.11675557226216027, 0.12863432580056944, 0.1273098808292441, 0.14268148704984884, 0.19934365172377466, 0.18594466654988895, 0.32832255345101913, 0.23469770515038102, 0.14866333116079655, 0.20267106519038025, 0.20152641892425394, 0.23576091839252133, 0.21421383347767264, 0.5391071467719948, 0.5042943805711715, 0.4612246809606788, 0.47676447358121665, 0.5291640446387131, 0.602638537448555, 0.6246427601161761, 0.5714078963023623, 0.6225968922190244, 0.33304162231889356, 0.3304783841194173, 0.4141875902408757, 0.354629285337982, 0.33990446568261656, 0.27919480653858797, 0.41214607585167573, 0.4488101082230267, 0.4070679191988382, 0.2075439556387948, 0.21303390663090027, 0.21635748499617935, 0.19827783015642464, 0.20259798106785998, 0.2337489300339528, 0.1995085629275789, 0.23000732022010184, 0.20258936164228614, 0.3583455868837565, 0.5974983189015708, 0.6315004735669381, 0.48375114306220435, 0.5228073851654269, 0.6765804181616237, 0.44806218567436396, 0.4405846418933689, 0.2231537031442834, 0.15177066878967904, 0.19069875851639195, 0.8538083672200958, 0.8489568076127689, 0.8369418101551436, 0.7959958487406582, 0.1809055777063855, 0.19441884874194382, 0.6735733685324677, 0.653629308683124, 0.16602485673297018, 0.20335804099962917, 0.20133806574003343, 0.20063819684592765, 0.4267483879660453, 0.203500441501264, 0.12468842409948466, 0.7601496747285006, 0.21347654253114523, 0.20557946271286787, 0.20946166959342294, 0.2037521178560513, 0.19899119990835357, 0.20502188897743512, 0.21185012528730773, 0.23029761548989025, 0.2226957927771256, 0.10106476573254897, 0.08282313409598063, 0.0927463241901183, 0.0892673049309205, 0.09187005772305634, 0.11808457473018663, 0.09136297465966581, 0.09200662746312827, 0.09667808533660249]}, "mutation_prompt": null}
{"id": "4298b7aa-2573-42de-9a67-b1915634bc59", "solution": "import numpy as np\n\nclass HybridStochasticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.initial_scale_factor = 0.8\n        self.initial_crossover_prob = 0.7\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population = self.opposition_based_initialization()\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.learning_rate = 0.1\n\n    def opposition_based_initialization(self):\n        initial_population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        opposition_population = self.lower_bound + self.upper_bound - initial_population\n        return np.vstack((initial_population, opposition_population))[:self.population_size]\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def select_parents(self, idx):\n        indices = [i for i in range(self.population_size) if i != idx]\n        return np.random.choice(indices, 3, replace=False)\n\n    def differential_evolution_mutation(self, idx, scale_factor):\n        a, b, c = self.select_parents(idx)\n        mutant_vector = self.population[a] + scale_factor * (self.population[b] - self.population[c])\n        return np.clip(mutant_vector, self.lower_bound, self.upper_bound)\n\n    def crossover(self, target_vector, mutant_vector, crossover_prob):\n        crossover_mask = np.random.rand(self.dim) < crossover_prob\n        return np.where(crossover_mask, mutant_vector, target_vector)\n\n    def local_search(self, vector):\n        perturbation = np.random.normal(0, self.learning_rate, self.dim)\n        candidate_vector = vector + perturbation\n        return np.clip(candidate_vector, self.lower_bound, self.upper_bound)\n\n    def __call__(self, func):\n        self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                # Dynamic adjustment of parameters\n                scale_factor = self.initial_scale_factor * (1 - self.evaluations / self.budget)\n                crossover_prob = self.initial_crossover_prob + 0.2 * np.random.rand()\n\n                mutant_vector = self.differential_evolution_mutation(i, scale_factor)\n                trial_vector = self.crossover(self.population[i], mutant_vector, crossover_prob)\n                trial_fitness = func(trial_vector)\n                self.evaluations += 1\n\n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial_vector\n                    self.fitness[i] = trial_fitness\n\n                if np.random.rand() < 0.2:\n                    candidate_vector = self.local_search(self.population[i])\n                    candidate_fitness = func(candidate_vector)\n                    self.evaluations += 1\n                    if candidate_fitness < self.fitness[i]:\n                        self.population[i] = candidate_vector\n                        self.fitness[i] = candidate_fitness\n\n                # Update learning rate adaptively\n                self.learning_rate = max(0.01, self.learning_rate * (1 - 0.05 * np.random.rand()))\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx]", "name": "HybridStochasticOptimizer", "description": "Hybrid optimizer with adaptive learning rate and opposition-based initialization for faster convergence.", "configspace": "", "generation": 32, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "c9aafe6b-332b-4283-9cc1-4b2e50c44e27", "metadata": {"aucs": [0.8614318631710612, 0.8615612395806681, 0.8648092636329068, 0.8836823589578061, 0.8809302815417128, 0.8537149049105373, 0.8649927264859298, 0.8661719092889808, 0.8732297035052712, 0.7784313840335078, 0.7934709027220181, 0.7709929153086685, 0.7666377215322294, 0.7792354193630129, 0.7955283567004858, 0.7874175449423783, 0.7934505999095464, 0.7963508686605465, 0.25196533963541234, 0.603323462921044, 0.31189512717187695, 0.45254508955474515, 0.3855834591332875, 0.6324217482611867, 0.38087210189123466, 0.37101210401952667, 0.37832843410229366, 0.3374576510463305, 0.1599578735010866, 0.25181667196560353, 0.15598715927116646, 0.15926218502651568, 0.14742944278799264, 0.33123888835847937, 0.3066524419836607, 0.16048855283955032, 0.9738501285938751, 0.9538242295949432, 0.9811516406082638, 0.9649564447290745, 0.9726531518905077, 0.9814481843774895, 0.9732999615150277, 0.9721568744354393, 0.9772487896256388, 0.5602477359628759, 0.5166699154516793, 0.49955145811133295, 0.4607762689339385, 0.5168840480364095, 0.5837268635955544, 0.5848178673193583, 0.44902566155463985, 0.6535189332618503, 0.8108564290388902, 0.7472995471422044, 0.7832292739258102, 0.780298104186407, 0.7997326258786164, 0.8307049265401217, 0.7670254946306878, 0.7966436314344594, 0.7737581975614084, 0.26793793612173356, 0.24955593945589905, 0.19834760026021148, 0.22530649729727037, 0.21576791524353967, 0.22803418538233733, 0.23532474250416835, 0.20160318462124072, 0.1920099262564383, 0.19613605298744063, 0.23556007360063813, 0.11440376677945807, 0.22544642163615658, 0.24862040742714708, 0.1874776391649946, 0.19655907028592323, 0.19284064583165672, 0.19577311722397894, 0.3108121261595491, 0.2566448530161354, 0.36431074059207347, 0.24025452278021453, 0.4802140919950957, 0.0007248673300417341, 0.35401012029935763, 0.3692632979788284, 0.276018593628536, 0.3623824055587753, 0.3924469974919602, 0.23556071143414548, 0.35272611497922335, 0.44565667864537895, 0.39468163509167276, 0.4061903350584134, 0.36215287040133537, 0.3973933722095969, 0.11732286633565558, 0.20172957542003978, 0.1498188877382276, 0.24772296138908012, 0.1546062088760296, 0.10282719367721671, 0.20924019897046064, 0.1183165114034116, 0.20632727477440505, 0.2809986328144657, 0.29684536408175155, 0.3175185646986213, 0.3737401167683222, 0.33517695108836454, 0.3271148862228106, 0.36693310045283345, 0.3166293273038533, 0.24146562201228117, 0.6055800258234042, 0.7234496228479592, 0.6438958422770305, 0.7507872332161957, 0.7324506280759618, 0.6589215501985713, 0.7496261142206224, 0.7209822579527148, 0.6855782937737074, 0.1371651233838116, 0.12075132229211483, 0.13780801853641733, 0.12584800341409697, 0.13850225460242616, 0.11675557226216027, 0.12863432580056944, 0.1273098808292441, 0.14268148704984884, 0.19934365172377466, 0.18594466654988895, 0.32832255345101913, 0.23469770515038102, 0.14866333116079655, 0.20267106519038025, 0.20152641892425394, 0.23576091839252133, 0.21421383347767264, 0.5391071467719948, 0.5042943805711715, 0.4612246809606788, 0.47676447358121665, 0.5291640446387131, 0.602638537448555, 0.6246427601161761, 0.5714078963023623, 0.6225968922190244, 0.33304162231889356, 0.3304783841194173, 0.4141875902408757, 0.354629285337982, 0.33990446568261656, 0.27919480653858797, 0.41214607585167573, 0.4488101082230267, 0.4070679191988382, 0.2075439556387948, 0.21303390663090027, 0.21635748499617935, 0.19827783015642464, 0.20259798106785998, 0.2337489300339528, 0.1995085629275789, 0.23000732022010184, 0.20258936164228614, 0.3583455868837565, 0.5974983189015708, 0.6315004735669381, 0.48375114306220435, 0.5228073851654269, 0.6765804181616237, 0.44806218567436396, 0.4405846418933689, 0.2231537031442834, 0.15177066878967904, 0.19069875851639195, 0.8538083672200958, 0.8489568076127689, 0.8369418101551436, 0.7959958487406582, 0.1809055777063855, 0.19441884874194382, 0.6735733685324677, 0.653629308683124, 0.16602485673297018, 0.20335804099962917, 0.20133806574003343, 0.20063819684592765, 0.4267483879660453, 0.203500441501264, 0.12468842409948466, 0.7601496747285006, 0.21347654253114523, 0.20557946271286787, 0.20946166959342294, 0.2037521178560513, 0.19899119990835357, 0.20502188897743512, 0.21185012528730773, 0.23029761548989025, 0.2226957927771256, 0.10106476573254897, 0.08282313409598063, 0.0927463241901183, 0.0892673049309205, 0.09187005772305634, 0.11808457473018663, 0.09136297465966581, 0.09200662746312827, 0.09667808533660249]}, "mutation_prompt": null}
{"id": "6e431c9c-f7dc-4945-a7d3-97716a80c5fd", "solution": "import numpy as np\n\nclass HybridStochasticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.initial_scale_factor = 0.8\n        self.initial_crossover_prob = 0.7\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population = self.opposition_based_initialization()\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.learning_rate = 0.1\n\n    def opposition_based_initialization(self):\n        initial_population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        opposition_population = self.lower_bound + self.upper_bound - initial_population\n        return np.vstack((initial_population, opposition_population))[:self.population_size]\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def select_parents(self, idx):\n        indices = [i for i in range(self.population_size) if i != idx]\n        return np.random.choice(indices, 3, replace=False)\n\n    def differential_evolution_mutation(self, idx, scale_factor):\n        a, b, c = self.select_parents(idx)\n        mutant_vector = self.population[a] + scale_factor * (self.population[b] - self.population[c])\n        return np.clip(mutant_vector, self.lower_bound, self.upper_bound)\n\n    def crossover(self, target_vector, mutant_vector, crossover_prob):\n        crossover_mask = np.random.rand(self.dim) < crossover_prob\n        return np.where(crossover_mask, mutant_vector, target_vector)\n\n    def local_search(self, vector):\n        perturbation = np.random.normal(0, self.learning_rate, self.dim)\n        candidate_vector = vector + perturbation\n        return np.clip(candidate_vector, self.lower_bound, self.upper_bound)\n\n    def __call__(self, func):\n        self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                # Dynamic adjustment of parameters\n                scale_factor = self.initial_scale_factor * (1 - self.evaluations / self.budget)\n                crossover_prob = self.initial_crossover_prob + 0.2 * np.random.rand()\n\n                mutant_vector = self.differential_evolution_mutation(i, scale_factor)\n                trial_vector = self.crossover(self.population[i], mutant_vector, crossover_prob)\n                trial_fitness = func(trial_vector)\n                self.evaluations += 1\n\n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial_vector\n                    self.fitness[i] = trial_fitness\n\n                if np.random.rand() < 0.2:\n                    candidate_vector = self.local_search(self.population[i])\n                    candidate_fitness = func(candidate_vector)\n                    self.evaluations += 1\n                    if candidate_fitness < self.fitness[i]:\n                        self.population[i] = candidate_vector\n                        self.fitness[i] = candidate_fitness\n\n                # Update learning rate adaptively\n                self.learning_rate = max(0.01, self.learning_rate * (1 - 0.05 * np.random.rand()))\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx]", "name": "HybridStochasticOptimizer", "description": "Hybrid optimizer with adaptive learning rate and opposition-based initialization for faster convergence.", "configspace": "", "generation": 32, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "c9aafe6b-332b-4283-9cc1-4b2e50c44e27", "metadata": {"aucs": [0.8614318631710612, 0.8615612395806681, 0.8648092636329068, 0.8836823589578061, 0.8809302815417128, 0.8537149049105373, 0.8649927264859298, 0.8661719092889808, 0.8732297035052712, 0.7784313840335078, 0.7934709027220181, 0.7709929153086685, 0.7666377215322294, 0.7792354193630129, 0.7955283567004858, 0.7874175449423783, 0.7934505999095464, 0.7963508686605465, 0.25196533963541234, 0.603323462921044, 0.31189512717187695, 0.45254508955474515, 0.3855834591332875, 0.6324217482611867, 0.38087210189123466, 0.37101210401952667, 0.37832843410229366, 0.3374576510463305, 0.1599578735010866, 0.25181667196560353, 0.15598715927116646, 0.15926218502651568, 0.14742944278799264, 0.33123888835847937, 0.3066524419836607, 0.16048855283955032, 0.9738501285938751, 0.9538242295949432, 0.9811516406082638, 0.9649564447290745, 0.9726531518905077, 0.9814481843774895, 0.9732999615150277, 0.9721568744354393, 0.9772487896256388, 0.5602477359628759, 0.5166699154516793, 0.49955145811133295, 0.4607762689339385, 0.5168840480364095, 0.5837268635955544, 0.5848178673193583, 0.44902566155463985, 0.6535189332618503, 0.8108564290388902, 0.7472995471422044, 0.7832292739258102, 0.780298104186407, 0.7997326258786164, 0.8307049265401217, 0.7670254946306878, 0.7966436314344594, 0.7737581975614084, 0.26793793612173356, 0.24955593945589905, 0.19834760026021148, 0.22530649729727037, 0.21576791524353967, 0.22803418538233733, 0.23532474250416835, 0.20160318462124072, 0.1920099262564383, 0.19613605298744063, 0.23556007360063813, 0.11440376677945807, 0.22544642163615658, 0.24862040742714708, 0.1874776391649946, 0.19655907028592323, 0.19284064583165672, 0.19577311722397894, 0.3108121261595491, 0.2566448530161354, 0.36431074059207347, 0.24025452278021453, 0.4802140919950957, 0.0007248673300417341, 0.35401012029935763, 0.3692632979788284, 0.276018593628536, 0.3623824055587753, 0.3924469974919602, 0.23556071143414548, 0.35272611497922335, 0.44565667864537895, 0.39468163509167276, 0.4061903350584134, 0.36215287040133537, 0.3973933722095969, 0.11732286633565558, 0.20172957542003978, 0.1498188877382276, 0.24772296138908012, 0.1546062088760296, 0.10282719367721671, 0.20924019897046064, 0.1183165114034116, 0.20632727477440505, 0.2809986328144657, 0.29684536408175155, 0.3175185646986213, 0.3737401167683222, 0.33517695108836454, 0.3271148862228106, 0.36693310045283345, 0.3166293273038533, 0.24146562201228117, 0.6055800258234042, 0.7234496228479592, 0.6438958422770305, 0.7507872332161957, 0.7324506280759618, 0.6589215501985713, 0.7496261142206224, 0.7209822579527148, 0.6855782937737074, 0.1371651233838116, 0.12075132229211483, 0.13780801853641733, 0.12584800341409697, 0.13850225460242616, 0.11675557226216027, 0.12863432580056944, 0.1273098808292441, 0.14268148704984884, 0.19934365172377466, 0.18594466654988895, 0.32832255345101913, 0.23469770515038102, 0.14866333116079655, 0.20267106519038025, 0.20152641892425394, 0.23576091839252133, 0.21421383347767264, 0.5391071467719948, 0.5042943805711715, 0.4612246809606788, 0.47676447358121665, 0.5291640446387131, 0.602638537448555, 0.6246427601161761, 0.5714078963023623, 0.6225968922190244, 0.33304162231889356, 0.3304783841194173, 0.4141875902408757, 0.354629285337982, 0.33990446568261656, 0.27919480653858797, 0.41214607585167573, 0.4488101082230267, 0.4070679191988382, 0.2075439556387948, 0.21303390663090027, 0.21635748499617935, 0.19827783015642464, 0.20259798106785998, 0.2337489300339528, 0.1995085629275789, 0.23000732022010184, 0.20258936164228614, 0.3583455868837565, 0.5974983189015708, 0.6315004735669381, 0.48375114306220435, 0.5228073851654269, 0.6765804181616237, 0.44806218567436396, 0.4405846418933689, 0.2231537031442834, 0.15177066878967904, 0.19069875851639195, 0.8538083672200958, 0.8489568076127689, 0.8369418101551436, 0.7959958487406582, 0.1809055777063855, 0.19441884874194382, 0.6735733685324677, 0.653629308683124, 0.16602485673297018, 0.20335804099962917, 0.20133806574003343, 0.20063819684592765, 0.4267483879660453, 0.203500441501264, 0.12468842409948466, 0.7601496747285006, 0.21347654253114523, 0.20557946271286787, 0.20946166959342294, 0.2037521178560513, 0.19899119990835357, 0.20502188897743512, 0.21185012528730773, 0.23029761548989025, 0.2226957927771256, 0.10106476573254897, 0.08282313409598063, 0.0927463241901183, 0.0892673049309205, 0.09187005772305634, 0.11808457473018663, 0.09136297465966581, 0.09200662746312827, 0.09667808533660249]}, "mutation_prompt": null}
{"id": "5a905149-3e57-4ce9-b4fd-dc0af0b7e905", "solution": "import numpy as np\n\nclass EnhancedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.initial_scale_factor = 0.8\n        self.initial_crossover_prob = 0.7\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population = self.opposition_based_initialization()\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.learning_rate = 0.1\n        self.restart_threshold = 0.1 * self.budget\n\n    def opposition_based_initialization(self):\n        initial_population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        opposition_population = self.lower_bound + self.upper_bound - initial_population\n        return np.vstack((initial_population, opposition_population))[:self.population_size]\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def select_parents(self, idx):\n        indices = [i for i in range(self.population_size) if i != idx]\n        return np.random.choice(indices, 3, replace=False)\n\n    def differential_evolution_mutation(self, idx, scale_factor):\n        a, b, c = self.select_parents(idx)\n        mutant_vector = self.population[a] + scale_factor * (self.population[b] - self.population[c])\n        return np.clip(mutant_vector, self.lower_bound, self.upper_bound)\n\n    def crossover(self, target_vector, mutant_vector, crossover_prob):\n        crossover_mask = np.random.rand(self.dim) < crossover_prob\n        return np.where(crossover_mask, mutant_vector, target_vector)\n\n    def local_search(self, vector):\n        perturbation = np.random.normal(0, self.learning_rate, self.dim)\n        candidate_vector = vector + perturbation\n        return np.clip(candidate_vector, self.lower_bound, self.upper_bound)\n\n    def random_restart(self):\n        restart_population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        self.population = self.opposition_based_initialization()\n        self.fitness.fill(np.inf)\n        self.evaluations += 1\n\n    def __call__(self, func):\n        self.evaluate_population(func)\n        stagnation_counter = 0\n        while self.evaluations < self.budget:\n            previous_best_fitness = np.min(self.fitness)\n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                scale_factor = self.initial_scale_factor * (1 - self.evaluations / self.budget)\n                crossover_prob = self.initial_crossover_prob + 0.2 * np.random.rand()\n\n                mutant_vector = self.differential_evolution_mutation(i, scale_factor)\n                trial_vector = self.crossover(self.population[i], mutant_vector, crossover_prob)\n                trial_fitness = func(trial_vector)\n                self.evaluations += 1\n\n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial_vector\n                    self.fitness[i] = trial_fitness\n\n                if np.random.rand() < 0.2:\n                    candidate_vector = self.local_search(self.population[i])\n                    candidate_fitness = func(candidate_vector)\n                    self.evaluations += 1\n                    if candidate_fitness < self.fitness[i]:\n                        self.population[i] = candidate_vector\n                        self.fitness[i] = candidate_fitness\n\n                self.learning_rate = max(0.01, self.learning_rate * (1 - 0.05 * np.random.rand()))\n\n            current_best_fitness = np.min(self.fitness)\n            if current_best_fitness >= previous_best_fitness:\n                stagnation_counter += 1\n            else:\n                stagnation_counter = 0\n\n            if stagnation_counter > self.restart_threshold:\n                self.random_restart()\n                stagnation_counter = 0\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx]", "name": "EnhancedHybridOptimizer", "description": "Enhanced hybrid optimizer with adaptive parameter tuning and random restarts for improved convergence.", "configspace": "", "generation": 43, "fitness": 0.4159602567786503, "feedback": "The algorithm EnhancedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.42 with standard deviation 0.26.", "error": "", "parent_id": "c9aafe6b-332b-4283-9cc1-4b2e50c44e27", "metadata": {"aucs": [0.8614318631710612, 0.8615612395806681, 0.8648092636329068, 0.8836823589578061, 0.8809302815417128, 0.8537149049105373, 0.8649927264859298, 0.8661719092889808, 0.8732297035052712, 0.7784313840335078, 0.7934709027220181, 0.7709929153086685, 0.7666377215322294, 0.7792354193630129, 0.7955283567004858, 0.7874175449423783, 0.7934505999095464, 0.7963508686605465, 0.25196533963541234, 0.603323462921044, 0.31189512717187695, 0.45254508955474515, 0.3855834591332875, 0.6324217482611867, 0.38087210189123466, 0.37101210401952667, 0.37832843410229366, 0.3374576510463305, 0.1599578735010866, 0.25181667196560353, 0.15598715927116646, 0.15926218502651568, 0.14742944278799264, 0.33123888835847937, 0.3066524419836607, 0.16048855283955032, 0.9738501285938751, 0.9538242295949432, 0.9811516406082638, 0.9649564447290745, 0.9726531518905077, 0.9814481843774895, 0.9732999615150277, 0.9721568744354393, 0.9772487896256388, 0.5602477359628759, 0.5166699154516793, 0.49955145811133295, 0.4607762689339385, 0.5168840480364095, 0.5837268635955544, 0.5848178673193583, 0.44902566155463985, 0.6535189332618503, 0.8108564290388902, 0.7472995471422044, 0.7832292739258102, 0.780298104186407, 0.7997326258786164, 0.8307049265401217, 0.7670254946306878, 0.7966436314344594, 0.7737581975614084, 0.26793793612173356, 0.24955593945589905, 0.19834760026021148, 0.22530649729727037, 0.21576791524353967, 0.22803418538233733, 0.23532474250416835, 0.20160318462124072, 0.1920099262564383, 0.19613605298744063, 0.23556007360063813, 0.11440376677945807, 0.22544642163615658, 0.24862040742714708, 0.1874776391649946, 0.19655907028592323, 0.19284064583165672, 0.19577311722397894, 0.3108121261595491, 0.2566448530161354, 0.36431074059207347, 0.24025452278021453, 0.4802140919950957, 0.0007248673300417341, 0.35401012029935763, 0.3692632979788284, 0.276018593628536, 0.3623824055587753, 0.3924469974919602, 0.23556071143414548, 0.35272611497922335, 0.44565667864537895, 0.39468163509167276, 0.4061903350584134, 0.36215287040133537, 0.3973933722095969, 0.11732286633565558, 0.20172957542003978, 0.1498188877382276, 0.24772296138908012, 0.1546062088760296, 0.10282719367721671, 0.20924019897046064, 0.1183165114034116, 0.20632727477440505, 0.2809986328144657, 0.29684536408175155, 0.3175185646986213, 0.3737401167683222, 0.33517695108836454, 0.3271148862228106, 0.36693310045283345, 0.3166293273038533, 0.24146562201228117, 0.6055800258234042, 0.7234496228479592, 0.6438958422770305, 0.7507872332161957, 0.7324506280759618, 0.6589215501985713, 0.7496261142206224, 0.7209822579527148, 0.6855782937737074, 0.1371651233838116, 0.12075132229211483, 0.13780801853641733, 0.12584800341409697, 0.13850225460242616, 0.11675557226216027, 0.12863432580056944, 0.1273098808292441, 0.14268148704984884, 0.19934365172377466, 0.18594466654988895, 0.32832255345101913, 0.23469770515038102, 0.14866333116079655, 0.20267106519038025, 0.20152641892425394, 0.23576091839252133, 0.21421383347767264, 0.5391071467719948, 0.5042943805711715, 0.4612246809606788, 0.47676447358121665, 0.5291640446387131, 0.602638537448555, 0.6246427601161761, 0.5714078963023623, 0.6225968922190244, 0.33304162231889356, 0.3304783841194173, 0.4141875902408757, 0.354629285337982, 0.33990446568261656, 0.27919480653858797, 0.41214607585167573, 0.4488101082230267, 0.4070679191988382, 0.2075439556387948, 0.21303390663090027, 0.21635748499617935, 0.19827783015642464, 0.20259798106785998, 0.2337489300339528, 0.1995085629275789, 0.23000732022010184, 0.20258936164228614, 0.3583455868837565, 0.5974983189015708, 0.6315004735669381, 0.48375114306220435, 0.5228073851654269, 0.6765804181616237, 0.44806218567436396, 0.4405846418933689, 0.2231537031442834, 0.15177066878967904, 0.19069875851639195, 0.8538083672200958, 0.8489568076127689, 0.8369418101551436, 0.7959958487406582, 0.1809055777063855, 0.19441884874194382, 0.6735733685324677, 0.653629308683124, 0.16602485673297018, 0.20335804099962917, 0.20133806574003343, 0.20063819684592765, 0.4267483879660453, 0.203500441501264, 0.12468842409948466, 0.7601496747285006, 0.21347654253114523, 0.20557946271286787, 0.20946166959342294, 0.2037521178560513, 0.19899119990835357, 0.20502188897743512, 0.21185012528730773, 0.23029761548989025, 0.2226957927771256, 0.10106476573254897, 0.08282313409598063, 0.0927463241901183, 0.0892673049309205, 0.09187005772305634, 0.11808457473018663, 0.09136297465966581, 0.09200662746312827, 0.09667808533660249]}, "mutation_prompt": null}
{"id": "01301d7b-87d3-434e-bd5e-1bc9b5780991", "solution": "import numpy as np\n\nclass HybridStochasticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.initial_scale_factor = 0.8\n        self.initial_crossover_prob = 0.7\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population = self.opposition_based_initialization()\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.learning_rate = 0.1\n\n    def opposition_based_initialization(self):\n        initial_population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        opposition_population = self.lower_bound + self.upper_bound - initial_population\n        return np.vstack((initial_population, opposition_population))[:self.population_size]\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def select_parents(self, idx):\n        indices = [i for i in range(self.population_size) if i != idx]\n        return np.random.choice(indices, 3, replace=False)\n\n    def differential_evolution_mutation(self, idx, scale_factor):\n        a, b, c = self.select_parents(idx)\n        mutant_vector = self.population[a] + scale_factor * (self.population[b] - self.population[c])\n        return np.clip(mutant_vector, self.lower_bound, self.upper_bound)\n\n    def crossover(self, target_vector, mutant_vector, crossover_prob):\n        crossover_mask = np.random.rand(self.dim) < crossover_prob\n        return np.where(crossover_mask, mutant_vector, target_vector)\n\n    def local_search(self, vector):\n        perturbation = np.random.normal(0, self.learning_rate, self.dim)\n        candidate_vector = vector + perturbation\n        return np.clip(candidate_vector, self.lower_bound, self.upper_bound)\n\n    def __call__(self, func):\n        self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                # Dynamic adjustment of parameters\n                scale_factor = self.initial_scale_factor * (1 - self.evaluations / self.budget)\n                crossover_prob = self.initial_crossover_prob + 0.2 * np.random.rand()\n\n                mutant_vector = self.differential_evolution_mutation(i, scale_factor)\n                trial_vector = self.crossover(self.population[i], mutant_vector, crossover_prob)\n                trial_fitness = func(trial_vector)\n                self.evaluations += 1\n\n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial_vector\n                    self.fitness[i] = trial_fitness\n\n                if np.random.rand() < 0.2:\n                    candidate_vector = self.local_search(self.population[i])\n                    candidate_fitness = func(candidate_vector)\n                    self.evaluations += 1\n                    if candidate_fitness < self.fitness[i]:\n                        self.population[i] = candidate_vector\n                        self.fitness[i] = candidate_fitness\n\n                # Update learning rate adaptively\n                self.learning_rate = max(0.01, self.learning_rate * (1 - 0.05 * np.random.rand()))\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx]", "name": "HybridStochasticOptimizer", "description": "Hybrid optimizer with adaptive learning rate and opposition-based initialization for faster convergence.", "configspace": "", "generation": 32, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "c9aafe6b-332b-4283-9cc1-4b2e50c44e27", "metadata": {"aucs": [0.8614318631710612, 0.8615612395806681, 0.8648092636329068, 0.8836823589578061, 0.8809302815417128, 0.8537149049105373, 0.8649927264859298, 0.8661719092889808, 0.8732297035052712, 0.7784313840335078, 0.7934709027220181, 0.7709929153086685, 0.7666377215322294, 0.7792354193630129, 0.7955283567004858, 0.7874175449423783, 0.7934505999095464, 0.7963508686605465, 0.25196533963541234, 0.603323462921044, 0.31189512717187695, 0.45254508955474515, 0.3855834591332875, 0.6324217482611867, 0.38087210189123466, 0.37101210401952667, 0.37832843410229366, 0.3374576510463305, 0.1599578735010866, 0.25181667196560353, 0.15598715927116646, 0.15926218502651568, 0.14742944278799264, 0.33123888835847937, 0.3066524419836607, 0.16048855283955032, 0.9738501285938751, 0.9538242295949432, 0.9811516406082638, 0.9649564447290745, 0.9726531518905077, 0.9814481843774895, 0.9732999615150277, 0.9721568744354393, 0.9772487896256388, 0.5602477359628759, 0.5166699154516793, 0.49955145811133295, 0.4607762689339385, 0.5168840480364095, 0.5837268635955544, 0.5848178673193583, 0.44902566155463985, 0.6535189332618503, 0.8108564290388902, 0.7472995471422044, 0.7832292739258102, 0.780298104186407, 0.7997326258786164, 0.8307049265401217, 0.7670254946306878, 0.7966436314344594, 0.7737581975614084, 0.26793793612173356, 0.24955593945589905, 0.19834760026021148, 0.22530649729727037, 0.21576791524353967, 0.22803418538233733, 0.23532474250416835, 0.20160318462124072, 0.1920099262564383, 0.19613605298744063, 0.23556007360063813, 0.11440376677945807, 0.22544642163615658, 0.24862040742714708, 0.1874776391649946, 0.19655907028592323, 0.19284064583165672, 0.19577311722397894, 0.3108121261595491, 0.2566448530161354, 0.36431074059207347, 0.24025452278021453, 0.4802140919950957, 0.0007248673300417341, 0.35401012029935763, 0.3692632979788284, 0.276018593628536, 0.3623824055587753, 0.3924469974919602, 0.23556071143414548, 0.35272611497922335, 0.44565667864537895, 0.39468163509167276, 0.4061903350584134, 0.36215287040133537, 0.3973933722095969, 0.11732286633565558, 0.20172957542003978, 0.1498188877382276, 0.24772296138908012, 0.1546062088760296, 0.10282719367721671, 0.20924019897046064, 0.1183165114034116, 0.20632727477440505, 0.2809986328144657, 0.29684536408175155, 0.3175185646986213, 0.3737401167683222, 0.33517695108836454, 0.3271148862228106, 0.36693310045283345, 0.3166293273038533, 0.24146562201228117, 0.6055800258234042, 0.7234496228479592, 0.6438958422770305, 0.7507872332161957, 0.7324506280759618, 0.6589215501985713, 0.7496261142206224, 0.7209822579527148, 0.6855782937737074, 0.1371651233838116, 0.12075132229211483, 0.13780801853641733, 0.12584800341409697, 0.13850225460242616, 0.11675557226216027, 0.12863432580056944, 0.1273098808292441, 0.14268148704984884, 0.19934365172377466, 0.18594466654988895, 0.32832255345101913, 0.23469770515038102, 0.14866333116079655, 0.20267106519038025, 0.20152641892425394, 0.23576091839252133, 0.21421383347767264, 0.5391071467719948, 0.5042943805711715, 0.4612246809606788, 0.47676447358121665, 0.5291640446387131, 0.602638537448555, 0.6246427601161761, 0.5714078963023623, 0.6225968922190244, 0.33304162231889356, 0.3304783841194173, 0.4141875902408757, 0.354629285337982, 0.33990446568261656, 0.27919480653858797, 0.41214607585167573, 0.4488101082230267, 0.4070679191988382, 0.2075439556387948, 0.21303390663090027, 0.21635748499617935, 0.19827783015642464, 0.20259798106785998, 0.2337489300339528, 0.1995085629275789, 0.23000732022010184, 0.20258936164228614, 0.3583455868837565, 0.5974983189015708, 0.6315004735669381, 0.48375114306220435, 0.5228073851654269, 0.6765804181616237, 0.44806218567436396, 0.4405846418933689, 0.2231537031442834, 0.15177066878967904, 0.19069875851639195, 0.8538083672200958, 0.8489568076127689, 0.8369418101551436, 0.7959958487406582, 0.1809055777063855, 0.19441884874194382, 0.6735733685324677, 0.653629308683124, 0.16602485673297018, 0.20335804099962917, 0.20133806574003343, 0.20063819684592765, 0.4267483879660453, 0.203500441501264, 0.12468842409948466, 0.7601496747285006, 0.21347654253114523, 0.20557946271286787, 0.20946166959342294, 0.2037521178560513, 0.19899119990835357, 0.20502188897743512, 0.21185012528730773, 0.23029761548989025, 0.2226957927771256, 0.10106476573254897, 0.08282313409598063, 0.0927463241901183, 0.0892673049309205, 0.09187005772305634, 0.11808457473018663, 0.09136297465966581, 0.09200662746312827, 0.09667808533660249]}, "mutation_prompt": null}
{"id": "5622e8cc-5440-4eb6-b759-498c47d16e0b", "solution": "import numpy as np\n\nclass HybridStochasticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.initial_scale_factor = 0.8\n        self.initial_crossover_prob = 0.7\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population = self.opposition_based_initialization()\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.learning_rate = 0.1\n\n    def opposition_based_initialization(self):\n        initial_population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        opposition_population = self.lower_bound + self.upper_bound - initial_population\n        return np.vstack((initial_population, opposition_population))[:self.population_size]\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def select_parents(self, idx):\n        indices = [i for i in range(self.population_size) if i != idx]\n        return np.random.choice(indices, 3, replace=False)\n\n    def differential_evolution_mutation(self, idx, scale_factor):\n        a, b, c = self.select_parents(idx)\n        mutant_vector = self.population[a] + scale_factor * (self.population[b] - self.population[c])\n        return np.clip(mutant_vector, self.lower_bound, self.upper_bound)\n\n    def crossover(self, target_vector, mutant_vector, crossover_prob):\n        crossover_mask = np.random.rand(self.dim) < crossover_prob\n        return np.where(crossover_mask, mutant_vector, target_vector)\n\n    def local_search(self, vector):\n        perturbation = np.random.normal(0, self.learning_rate, self.dim)\n        candidate_vector = vector + perturbation\n        return np.clip(candidate_vector, self.lower_bound, self.upper_bound)\n\n    def __call__(self, func):\n        self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                # Dynamic adjustment of parameters\n                scale_factor = self.initial_scale_factor * (1 - self.evaluations / self.budget)\n                crossover_prob = self.initial_crossover_prob + 0.2 * np.random.rand()\n\n                mutant_vector = self.differential_evolution_mutation(i, scale_factor)\n                trial_vector = self.crossover(self.population[i], mutant_vector, crossover_prob)\n                trial_fitness = func(trial_vector)\n                self.evaluations += 1\n\n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial_vector\n                    self.fitness[i] = trial_fitness\n\n                if np.random.rand() < 0.2:\n                    candidate_vector = self.local_search(self.population[i])\n                    candidate_fitness = func(candidate_vector)\n                    self.evaluations += 1\n                    if candidate_fitness < self.fitness[i]:\n                        self.population[i] = candidate_vector\n                        self.fitness[i] = candidate_fitness\n\n                # Update learning rate adaptively\n                self.learning_rate = max(0.01, self.learning_rate * (1 - 0.05 * np.random.rand()))\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx]", "name": "HybridStochasticOptimizer", "description": "Hybrid optimizer with adaptive learning rate and opposition-based initialization for faster convergence.", "configspace": "", "generation": 32, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "c9aafe6b-332b-4283-9cc1-4b2e50c44e27", "metadata": {"aucs": [0.8614318631710612, 0.8615612395806681, 0.8648092636329068, 0.8836823589578061, 0.8809302815417128, 0.8537149049105373, 0.8649927264859298, 0.8661719092889808, 0.8732297035052712, 0.7784313840335078, 0.7934709027220181, 0.7709929153086685, 0.7666377215322294, 0.7792354193630129, 0.7955283567004858, 0.7874175449423783, 0.7934505999095464, 0.7963508686605465, 0.25196533963541234, 0.603323462921044, 0.31189512717187695, 0.45254508955474515, 0.3855834591332875, 0.6324217482611867, 0.38087210189123466, 0.37101210401952667, 0.37832843410229366, 0.3374576510463305, 0.1599578735010866, 0.25181667196560353, 0.15598715927116646, 0.15926218502651568, 0.14742944278799264, 0.33123888835847937, 0.3066524419836607, 0.16048855283955032, 0.9738501285938751, 0.9538242295949432, 0.9811516406082638, 0.9649564447290745, 0.9726531518905077, 0.9814481843774895, 0.9732999615150277, 0.9721568744354393, 0.9772487896256388, 0.5602477359628759, 0.5166699154516793, 0.49955145811133295, 0.4607762689339385, 0.5168840480364095, 0.5837268635955544, 0.5848178673193583, 0.44902566155463985, 0.6535189332618503, 0.8108564290388902, 0.7472995471422044, 0.7832292739258102, 0.780298104186407, 0.7997326258786164, 0.8307049265401217, 0.7670254946306878, 0.7966436314344594, 0.7737581975614084, 0.26793793612173356, 0.24955593945589905, 0.19834760026021148, 0.22530649729727037, 0.21576791524353967, 0.22803418538233733, 0.23532474250416835, 0.20160318462124072, 0.1920099262564383, 0.19613605298744063, 0.23556007360063813, 0.11440376677945807, 0.22544642163615658, 0.24862040742714708, 0.1874776391649946, 0.19655907028592323, 0.19284064583165672, 0.19577311722397894, 0.3108121261595491, 0.2566448530161354, 0.36431074059207347, 0.24025452278021453, 0.4802140919950957, 0.0007248673300417341, 0.35401012029935763, 0.3692632979788284, 0.276018593628536, 0.3623824055587753, 0.3924469974919602, 0.23556071143414548, 0.35272611497922335, 0.44565667864537895, 0.39468163509167276, 0.4061903350584134, 0.36215287040133537, 0.3973933722095969, 0.11732286633565558, 0.20172957542003978, 0.1498188877382276, 0.24772296138908012, 0.1546062088760296, 0.10282719367721671, 0.20924019897046064, 0.1183165114034116, 0.20632727477440505, 0.2809986328144657, 0.29684536408175155, 0.3175185646986213, 0.3737401167683222, 0.33517695108836454, 0.3271148862228106, 0.36693310045283345, 0.3166293273038533, 0.24146562201228117, 0.6055800258234042, 0.7234496228479592, 0.6438958422770305, 0.7507872332161957, 0.7324506280759618, 0.6589215501985713, 0.7496261142206224, 0.7209822579527148, 0.6855782937737074, 0.1371651233838116, 0.12075132229211483, 0.13780801853641733, 0.12584800341409697, 0.13850225460242616, 0.11675557226216027, 0.12863432580056944, 0.1273098808292441, 0.14268148704984884, 0.19934365172377466, 0.18594466654988895, 0.32832255345101913, 0.23469770515038102, 0.14866333116079655, 0.20267106519038025, 0.20152641892425394, 0.23576091839252133, 0.21421383347767264, 0.5391071467719948, 0.5042943805711715, 0.4612246809606788, 0.47676447358121665, 0.5291640446387131, 0.602638537448555, 0.6246427601161761, 0.5714078963023623, 0.6225968922190244, 0.33304162231889356, 0.3304783841194173, 0.4141875902408757, 0.354629285337982, 0.33990446568261656, 0.27919480653858797, 0.41214607585167573, 0.4488101082230267, 0.4070679191988382, 0.2075439556387948, 0.21303390663090027, 0.21635748499617935, 0.19827783015642464, 0.20259798106785998, 0.2337489300339528, 0.1995085629275789, 0.23000732022010184, 0.20258936164228614, 0.3583455868837565, 0.5974983189015708, 0.6315004735669381, 0.48375114306220435, 0.5228073851654269, 0.6765804181616237, 0.44806218567436396, 0.4405846418933689, 0.2231537031442834, 0.15177066878967904, 0.19069875851639195, 0.8538083672200958, 0.8489568076127689, 0.8369418101551436, 0.7959958487406582, 0.1809055777063855, 0.19441884874194382, 0.6735733685324677, 0.653629308683124, 0.16602485673297018, 0.20335804099962917, 0.20133806574003343, 0.20063819684592765, 0.4267483879660453, 0.203500441501264, 0.12468842409948466, 0.7601496747285006, 0.21347654253114523, 0.20557946271286787, 0.20946166959342294, 0.2037521178560513, 0.19899119990835357, 0.20502188897743512, 0.21185012528730773, 0.23029761548989025, 0.2226957927771256, 0.10106476573254897, 0.08282313409598063, 0.0927463241901183, 0.0892673049309205, 0.09187005772305634, 0.11808457473018663, 0.09136297465966581, 0.09200662746312827, 0.09667808533660249]}, "mutation_prompt": null}
{"id": "80b4caae-f865-4c40-ae83-237c43a9b306", "solution": "import numpy as np\n\nclass HybridStochasticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.initial_scale_factor = 0.8\n        self.initial_crossover_prob = 0.7\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population = self.opposition_based_initialization()\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.learning_rate = 0.1\n\n    def opposition_based_initialization(self):\n        initial_population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        opposition_population = self.lower_bound + self.upper_bound - initial_population\n        return np.vstack((initial_population, opposition_population))[:self.population_size]\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def select_parents(self, idx):\n        indices = [i for i in range(self.population_size) if i != idx]\n        return np.random.choice(indices, 3, replace=False)\n\n    def differential_evolution_mutation(self, idx, scale_factor):\n        a, b, c = self.select_parents(idx)\n        mutant_vector = self.population[a] + scale_factor * (self.population[b] - self.population[c])\n        return np.clip(mutant_vector, self.lower_bound, self.upper_bound)\n\n    def crossover(self, target_vector, mutant_vector, crossover_prob):\n        crossover_mask = np.random.rand(self.dim) < crossover_prob\n        return np.where(crossover_mask, mutant_vector, target_vector)\n\n    def local_search(self, vector):\n        perturbation = np.random.normal(0, self.learning_rate, self.dim)\n        candidate_vector = vector + perturbation\n        return np.clip(candidate_vector, self.lower_bound, self.upper_bound)\n\n    def __call__(self, func):\n        self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                # Dynamic adjustment of parameters\n                scale_factor = self.initial_scale_factor * (1 - self.evaluations / self.budget)\n                crossover_prob = self.initial_crossover_prob + 0.2 * np.random.rand()\n\n                mutant_vector = self.differential_evolution_mutation(i, scale_factor)\n                trial_vector = self.crossover(self.population[i], mutant_vector, crossover_prob)\n                trial_fitness = func(trial_vector)\n                self.evaluations += 1\n\n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial_vector\n                    self.fitness[i] = trial_fitness\n\n                if np.random.rand() < 0.2:\n                    candidate_vector = self.local_search(self.population[i])\n                    candidate_fitness = func(candidate_vector)\n                    self.evaluations += 1\n                    if candidate_fitness < self.fitness[i]:\n                        self.population[i] = candidate_vector\n                        self.fitness[i] = candidate_fitness\n\n                # Update learning rate adaptively\n                self.learning_rate = max(0.01, self.learning_rate * (1 - 0.05 * np.random.rand()))\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx]", "name": "HybridStochasticOptimizer", "description": "Hybrid optimizer with adaptive learning rate and opposition-based initialization for faster convergence.", "configspace": "", "generation": 32, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "c9aafe6b-332b-4283-9cc1-4b2e50c44e27", "metadata": {"aucs": [0.8614318631710612, 0.8615612395806681, 0.8648092636329068, 0.8836823589578061, 0.8809302815417128, 0.8537149049105373, 0.8649927264859298, 0.8661719092889808, 0.8732297035052712, 0.7784313840335078, 0.7934709027220181, 0.7709929153086685, 0.7666377215322294, 0.7792354193630129, 0.7955283567004858, 0.7874175449423783, 0.7934505999095464, 0.7963508686605465, 0.25196533963541234, 0.603323462921044, 0.31189512717187695, 0.45254508955474515, 0.3855834591332875, 0.6324217482611867, 0.38087210189123466, 0.37101210401952667, 0.37832843410229366, 0.3374576510463305, 0.1599578735010866, 0.25181667196560353, 0.15598715927116646, 0.15926218502651568, 0.14742944278799264, 0.33123888835847937, 0.3066524419836607, 0.16048855283955032, 0.9738501285938751, 0.9538242295949432, 0.9811516406082638, 0.9649564447290745, 0.9726531518905077, 0.9814481843774895, 0.9732999615150277, 0.9721568744354393, 0.9772487896256388, 0.5602477359628759, 0.5166699154516793, 0.49955145811133295, 0.4607762689339385, 0.5168840480364095, 0.5837268635955544, 0.5848178673193583, 0.44902566155463985, 0.6535189332618503, 0.8108564290388902, 0.7472995471422044, 0.7832292739258102, 0.780298104186407, 0.7997326258786164, 0.8307049265401217, 0.7670254946306878, 0.7966436314344594, 0.7737581975614084, 0.26793793612173356, 0.24955593945589905, 0.19834760026021148, 0.22530649729727037, 0.21576791524353967, 0.22803418538233733, 0.23532474250416835, 0.20160318462124072, 0.1920099262564383, 0.19613605298744063, 0.23556007360063813, 0.11440376677945807, 0.22544642163615658, 0.24862040742714708, 0.1874776391649946, 0.19655907028592323, 0.19284064583165672, 0.19577311722397894, 0.3108121261595491, 0.2566448530161354, 0.36431074059207347, 0.24025452278021453, 0.4802140919950957, 0.0007248673300417341, 0.35401012029935763, 0.3692632979788284, 0.276018593628536, 0.3623824055587753, 0.3924469974919602, 0.23556071143414548, 0.35272611497922335, 0.44565667864537895, 0.39468163509167276, 0.4061903350584134, 0.36215287040133537, 0.3973933722095969, 0.11732286633565558, 0.20172957542003978, 0.1498188877382276, 0.24772296138908012, 0.1546062088760296, 0.10282719367721671, 0.20924019897046064, 0.1183165114034116, 0.20632727477440505, 0.2809986328144657, 0.29684536408175155, 0.3175185646986213, 0.3737401167683222, 0.33517695108836454, 0.3271148862228106, 0.36693310045283345, 0.3166293273038533, 0.24146562201228117, 0.6055800258234042, 0.7234496228479592, 0.6438958422770305, 0.7507872332161957, 0.7324506280759618, 0.6589215501985713, 0.7496261142206224, 0.7209822579527148, 0.6855782937737074, 0.1371651233838116, 0.12075132229211483, 0.13780801853641733, 0.12584800341409697, 0.13850225460242616, 0.11675557226216027, 0.12863432580056944, 0.1273098808292441, 0.14268148704984884, 0.19934365172377466, 0.18594466654988895, 0.32832255345101913, 0.23469770515038102, 0.14866333116079655, 0.20267106519038025, 0.20152641892425394, 0.23576091839252133, 0.21421383347767264, 0.5391071467719948, 0.5042943805711715, 0.4612246809606788, 0.47676447358121665, 0.5291640446387131, 0.602638537448555, 0.6246427601161761, 0.5714078963023623, 0.6225968922190244, 0.33304162231889356, 0.3304783841194173, 0.4141875902408757, 0.354629285337982, 0.33990446568261656, 0.27919480653858797, 0.41214607585167573, 0.4488101082230267, 0.4070679191988382, 0.2075439556387948, 0.21303390663090027, 0.21635748499617935, 0.19827783015642464, 0.20259798106785998, 0.2337489300339528, 0.1995085629275789, 0.23000732022010184, 0.20258936164228614, 0.3583455868837565, 0.5974983189015708, 0.6315004735669381, 0.48375114306220435, 0.5228073851654269, 0.6765804181616237, 0.44806218567436396, 0.4405846418933689, 0.2231537031442834, 0.15177066878967904, 0.19069875851639195, 0.8538083672200958, 0.8489568076127689, 0.8369418101551436, 0.7959958487406582, 0.1809055777063855, 0.19441884874194382, 0.6735733685324677, 0.653629308683124, 0.16602485673297018, 0.20335804099962917, 0.20133806574003343, 0.20063819684592765, 0.4267483879660453, 0.203500441501264, 0.12468842409948466, 0.7601496747285006, 0.21347654253114523, 0.20557946271286787, 0.20946166959342294, 0.2037521178560513, 0.19899119990835357, 0.20502188897743512, 0.21185012528730773, 0.23029761548989025, 0.2226957927771256, 0.10106476573254897, 0.08282313409598063, 0.0927463241901183, 0.0892673049309205, 0.09187005772305634, 0.11808457473018663, 0.09136297465966581, 0.09200662746312827, 0.09667808533660249]}, "mutation_prompt": null}
{"id": "70bb3628-7d55-44b8-a1ef-6d937b7f30ea", "solution": "import numpy as np\n\nclass HybridStochasticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.initial_scale_factor = 0.8\n        self.initial_crossover_prob = 0.7\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population = self.opposition_based_initialization()\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.learning_rate = 0.1\n\n    def opposition_based_initialization(self):\n        initial_population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        opposition_population = self.lower_bound + self.upper_bound - initial_population\n        return np.vstack((initial_population, opposition_population))[:self.population_size]\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def select_parents(self, idx):\n        indices = [i for i in range(self.population_size) if i != idx]\n        return np.random.choice(indices, 3, replace=False)\n\n    def differential_evolution_mutation(self, idx, scale_factor):\n        a, b, c = self.select_parents(idx)\n        mutant_vector = self.population[a] + scale_factor * (self.population[b] - self.population[c])\n        return np.clip(mutant_vector, self.lower_bound, self.upper_bound)\n\n    def crossover(self, target_vector, mutant_vector, crossover_prob):\n        crossover_mask = np.random.rand(self.dim) < crossover_prob\n        return np.where(crossover_mask, mutant_vector, target_vector)\n\n    def local_search(self, vector):\n        perturbation = np.random.normal(0, self.learning_rate, self.dim)\n        candidate_vector = vector + perturbation\n        return np.clip(candidate_vector, self.lower_bound, self.upper_bound)\n\n    def __call__(self, func):\n        self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                # Dynamic adjustment of parameters\n                scale_factor = self.initial_scale_factor * (1 - self.evaluations / self.budget)\n                crossover_prob = self.initial_crossover_prob + 0.2 * np.random.rand()\n\n                mutant_vector = self.differential_evolution_mutation(i, scale_factor)\n                trial_vector = self.crossover(self.population[i], mutant_vector, crossover_prob)\n                trial_fitness = func(trial_vector)\n                self.evaluations += 1\n\n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial_vector\n                    self.fitness[i] = trial_fitness\n\n                if np.random.rand() < 0.2:\n                    candidate_vector = self.local_search(self.population[i])\n                    candidate_fitness = func(candidate_vector)\n                    self.evaluations += 1\n                    if candidate_fitness < self.fitness[i]:\n                        self.population[i] = candidate_vector\n                        self.fitness[i] = candidate_fitness\n\n                # Update learning rate adaptively\n                self.learning_rate = max(0.01, self.learning_rate * (1 - 0.05 * np.random.rand()))\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx]", "name": "HybridStochasticOptimizer", "description": "Hybrid optimizer with adaptive learning rate and opposition-based initialization for faster convergence.", "configspace": "", "generation": 32, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "c9aafe6b-332b-4283-9cc1-4b2e50c44e27", "metadata": {"aucs": [0.8614318631710612, 0.8615612395806681, 0.8648092636329068, 0.8836823589578061, 0.8809302815417128, 0.8537149049105373, 0.8649927264859298, 0.8661719092889808, 0.8732297035052712, 0.7784313840335078, 0.7934709027220181, 0.7709929153086685, 0.7666377215322294, 0.7792354193630129, 0.7955283567004858, 0.7874175449423783, 0.7934505999095464, 0.7963508686605465, 0.25196533963541234, 0.603323462921044, 0.31189512717187695, 0.45254508955474515, 0.3855834591332875, 0.6324217482611867, 0.38087210189123466, 0.37101210401952667, 0.37832843410229366, 0.3374576510463305, 0.1599578735010866, 0.25181667196560353, 0.15598715927116646, 0.15926218502651568, 0.14742944278799264, 0.33123888835847937, 0.3066524419836607, 0.16048855283955032, 0.9738501285938751, 0.9538242295949432, 0.9811516406082638, 0.9649564447290745, 0.9726531518905077, 0.9814481843774895, 0.9732999615150277, 0.9721568744354393, 0.9772487896256388, 0.5602477359628759, 0.5166699154516793, 0.49955145811133295, 0.4607762689339385, 0.5168840480364095, 0.5837268635955544, 0.5848178673193583, 0.44902566155463985, 0.6535189332618503, 0.8108564290388902, 0.7472995471422044, 0.7832292739258102, 0.780298104186407, 0.7997326258786164, 0.8307049265401217, 0.7670254946306878, 0.7966436314344594, 0.7737581975614084, 0.26793793612173356, 0.24955593945589905, 0.19834760026021148, 0.22530649729727037, 0.21576791524353967, 0.22803418538233733, 0.23532474250416835, 0.20160318462124072, 0.1920099262564383, 0.19613605298744063, 0.23556007360063813, 0.11440376677945807, 0.22544642163615658, 0.24862040742714708, 0.1874776391649946, 0.19655907028592323, 0.19284064583165672, 0.19577311722397894, 0.3108121261595491, 0.2566448530161354, 0.36431074059207347, 0.24025452278021453, 0.4802140919950957, 0.0007248673300417341, 0.35401012029935763, 0.3692632979788284, 0.276018593628536, 0.3623824055587753, 0.3924469974919602, 0.23556071143414548, 0.35272611497922335, 0.44565667864537895, 0.39468163509167276, 0.4061903350584134, 0.36215287040133537, 0.3973933722095969, 0.11732286633565558, 0.20172957542003978, 0.1498188877382276, 0.24772296138908012, 0.1546062088760296, 0.10282719367721671, 0.20924019897046064, 0.1183165114034116, 0.20632727477440505, 0.2809986328144657, 0.29684536408175155, 0.3175185646986213, 0.3737401167683222, 0.33517695108836454, 0.3271148862228106, 0.36693310045283345, 0.3166293273038533, 0.24146562201228117, 0.6055800258234042, 0.7234496228479592, 0.6438958422770305, 0.7507872332161957, 0.7324506280759618, 0.6589215501985713, 0.7496261142206224, 0.7209822579527148, 0.6855782937737074, 0.1371651233838116, 0.12075132229211483, 0.13780801853641733, 0.12584800341409697, 0.13850225460242616, 0.11675557226216027, 0.12863432580056944, 0.1273098808292441, 0.14268148704984884, 0.19934365172377466, 0.18594466654988895, 0.32832255345101913, 0.23469770515038102, 0.14866333116079655, 0.20267106519038025, 0.20152641892425394, 0.23576091839252133, 0.21421383347767264, 0.5391071467719948, 0.5042943805711715, 0.4612246809606788, 0.47676447358121665, 0.5291640446387131, 0.602638537448555, 0.6246427601161761, 0.5714078963023623, 0.6225968922190244, 0.33304162231889356, 0.3304783841194173, 0.4141875902408757, 0.354629285337982, 0.33990446568261656, 0.27919480653858797, 0.41214607585167573, 0.4488101082230267, 0.4070679191988382, 0.2075439556387948, 0.21303390663090027, 0.21635748499617935, 0.19827783015642464, 0.20259798106785998, 0.2337489300339528, 0.1995085629275789, 0.23000732022010184, 0.20258936164228614, 0.3583455868837565, 0.5974983189015708, 0.6315004735669381, 0.48375114306220435, 0.5228073851654269, 0.6765804181616237, 0.44806218567436396, 0.4405846418933689, 0.2231537031442834, 0.15177066878967904, 0.19069875851639195, 0.8538083672200958, 0.8489568076127689, 0.8369418101551436, 0.7959958487406582, 0.1809055777063855, 0.19441884874194382, 0.6735733685324677, 0.653629308683124, 0.16602485673297018, 0.20335804099962917, 0.20133806574003343, 0.20063819684592765, 0.4267483879660453, 0.203500441501264, 0.12468842409948466, 0.7601496747285006, 0.21347654253114523, 0.20557946271286787, 0.20946166959342294, 0.2037521178560513, 0.19899119990835357, 0.20502188897743512, 0.21185012528730773, 0.23029761548989025, 0.2226957927771256, 0.10106476573254897, 0.08282313409598063, 0.0927463241901183, 0.0892673049309205, 0.09187005772305634, 0.11808457473018663, 0.09136297465966581, 0.09200662746312827, 0.09667808533660249]}, "mutation_prompt": null}
{"id": "d1095141-9167-4508-b117-419283ca9f13", "solution": "import numpy as np\n\nclass HybridStochasticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.initial_scale_factor = 0.8\n        self.initial_crossover_prob = 0.7\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population = self.opposition_based_initialization()\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.learning_rate = 0.1\n\n    def opposition_based_initialization(self):\n        initial_population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        opposition_population = self.lower_bound + self.upper_bound - initial_population\n        return np.vstack((initial_population, opposition_population))[:self.population_size]\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def select_parents(self, idx):\n        indices = [i for i in range(self.population_size) if i != idx]\n        return np.random.choice(indices, 3, replace=False)\n\n    def differential_evolution_mutation(self, idx, scale_factor):\n        a, b, c = self.select_parents(idx)\n        mutant_vector = self.population[a] + scale_factor * (self.population[b] - self.population[c])\n        return np.clip(mutant_vector, self.lower_bound, self.upper_bound)\n\n    def crossover(self, target_vector, mutant_vector, crossover_prob):\n        crossover_mask = np.random.rand(self.dim) < crossover_prob\n        return np.where(crossover_mask, mutant_vector, target_vector)\n\n    def local_search(self, vector):\n        perturbation = np.random.normal(0, self.learning_rate, self.dim)\n        candidate_vector = vector + perturbation\n        return np.clip(candidate_vector, self.lower_bound, self.upper_bound)\n\n    def __call__(self, func):\n        self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                # Dynamic adjustment of parameters\n                scale_factor = self.initial_scale_factor * (1 - self.evaluations / self.budget)\n                crossover_prob = self.initial_crossover_prob + 0.2 * np.random.rand()\n\n                mutant_vector = self.differential_evolution_mutation(i, scale_factor)\n                trial_vector = self.crossover(self.population[i], mutant_vector, crossover_prob)\n                trial_fitness = func(trial_vector)\n                self.evaluations += 1\n\n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial_vector\n                    self.fitness[i] = trial_fitness\n\n                if np.random.rand() < 0.2:\n                    candidate_vector = self.local_search(self.population[i])\n                    candidate_fitness = func(candidate_vector)\n                    self.evaluations += 1\n                    if candidate_fitness < self.fitness[i]:\n                        self.population[i] = candidate_vector\n                        self.fitness[i] = candidate_fitness\n\n                # Update learning rate adaptively\n                self.learning_rate = max(0.01, self.learning_rate * (1 - 0.05 * np.random.rand()))\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx]", "name": "HybridStochasticOptimizer", "description": "Hybrid optimizer with adaptive learning rate and opposition-based initialization for faster convergence.", "configspace": "", "generation": 32, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "c9aafe6b-332b-4283-9cc1-4b2e50c44e27", "metadata": {"aucs": [0.8614318631710612, 0.8615612395806681, 0.8648092636329068, 0.8836823589578061, 0.8809302815417128, 0.8537149049105373, 0.8649927264859298, 0.8661719092889808, 0.8732297035052712, 0.7784313840335078, 0.7934709027220181, 0.7709929153086685, 0.7666377215322294, 0.7792354193630129, 0.7955283567004858, 0.7874175449423783, 0.7934505999095464, 0.7963508686605465, 0.25196533963541234, 0.603323462921044, 0.31189512717187695, 0.45254508955474515, 0.3855834591332875, 0.6324217482611867, 0.38087210189123466, 0.37101210401952667, 0.37832843410229366, 0.3374576510463305, 0.1599578735010866, 0.25181667196560353, 0.15598715927116646, 0.15926218502651568, 0.14742944278799264, 0.33123888835847937, 0.3066524419836607, 0.16048855283955032, 0.9738501285938751, 0.9538242295949432, 0.9811516406082638, 0.9649564447290745, 0.9726531518905077, 0.9814481843774895, 0.9732999615150277, 0.9721568744354393, 0.9772487896256388, 0.5602477359628759, 0.5166699154516793, 0.49955145811133295, 0.4607762689339385, 0.5168840480364095, 0.5837268635955544, 0.5848178673193583, 0.44902566155463985, 0.6535189332618503, 0.8108564290388902, 0.7472995471422044, 0.7832292739258102, 0.780298104186407, 0.7997326258786164, 0.8307049265401217, 0.7670254946306878, 0.7966436314344594, 0.7737581975614084, 0.26793793612173356, 0.24955593945589905, 0.19834760026021148, 0.22530649729727037, 0.21576791524353967, 0.22803418538233733, 0.23532474250416835, 0.20160318462124072, 0.1920099262564383, 0.19613605298744063, 0.23556007360063813, 0.11440376677945807, 0.22544642163615658, 0.24862040742714708, 0.1874776391649946, 0.19655907028592323, 0.19284064583165672, 0.19577311722397894, 0.3108121261595491, 0.2566448530161354, 0.36431074059207347, 0.24025452278021453, 0.4802140919950957, 0.0007248673300417341, 0.35401012029935763, 0.3692632979788284, 0.276018593628536, 0.3623824055587753, 0.3924469974919602, 0.23556071143414548, 0.35272611497922335, 0.44565667864537895, 0.39468163509167276, 0.4061903350584134, 0.36215287040133537, 0.3973933722095969, 0.11732286633565558, 0.20172957542003978, 0.1498188877382276, 0.24772296138908012, 0.1546062088760296, 0.10282719367721671, 0.20924019897046064, 0.1183165114034116, 0.20632727477440505, 0.2809986328144657, 0.29684536408175155, 0.3175185646986213, 0.3737401167683222, 0.33517695108836454, 0.3271148862228106, 0.36693310045283345, 0.3166293273038533, 0.24146562201228117, 0.6055800258234042, 0.7234496228479592, 0.6438958422770305, 0.7507872332161957, 0.7324506280759618, 0.6589215501985713, 0.7496261142206224, 0.7209822579527148, 0.6855782937737074, 0.1371651233838116, 0.12075132229211483, 0.13780801853641733, 0.12584800341409697, 0.13850225460242616, 0.11675557226216027, 0.12863432580056944, 0.1273098808292441, 0.14268148704984884, 0.19934365172377466, 0.18594466654988895, 0.32832255345101913, 0.23469770515038102, 0.14866333116079655, 0.20267106519038025, 0.20152641892425394, 0.23576091839252133, 0.21421383347767264, 0.5391071467719948, 0.5042943805711715, 0.4612246809606788, 0.47676447358121665, 0.5291640446387131, 0.602638537448555, 0.6246427601161761, 0.5714078963023623, 0.6225968922190244, 0.33304162231889356, 0.3304783841194173, 0.4141875902408757, 0.354629285337982, 0.33990446568261656, 0.27919480653858797, 0.41214607585167573, 0.4488101082230267, 0.4070679191988382, 0.2075439556387948, 0.21303390663090027, 0.21635748499617935, 0.19827783015642464, 0.20259798106785998, 0.2337489300339528, 0.1995085629275789, 0.23000732022010184, 0.20258936164228614, 0.3583455868837565, 0.5974983189015708, 0.6315004735669381, 0.48375114306220435, 0.5228073851654269, 0.6765804181616237, 0.44806218567436396, 0.4405846418933689, 0.2231537031442834, 0.15177066878967904, 0.19069875851639195, 0.8538083672200958, 0.8489568076127689, 0.8369418101551436, 0.7959958487406582, 0.1809055777063855, 0.19441884874194382, 0.6735733685324677, 0.653629308683124, 0.16602485673297018, 0.20335804099962917, 0.20133806574003343, 0.20063819684592765, 0.4267483879660453, 0.203500441501264, 0.12468842409948466, 0.7601496747285006, 0.21347654253114523, 0.20557946271286787, 0.20946166959342294, 0.2037521178560513, 0.19899119990835357, 0.20502188897743512, 0.21185012528730773, 0.23029761548989025, 0.2226957927771256, 0.10106476573254897, 0.08282313409598063, 0.0927463241901183, 0.0892673049309205, 0.09187005772305634, 0.11808457473018663, 0.09136297465966581, 0.09200662746312827, 0.09667808533660249]}, "mutation_prompt": null}
{"id": "87f7043e-e6e2-4aa4-a9dc-c3da12b03487", "solution": "import numpy as np\n\nclass HybridStochasticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.initial_scale_factor = 0.8\n        self.initial_crossover_prob = 0.7\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population = self.opposition_based_initialization()\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.learning_rate = 0.1\n\n    def opposition_based_initialization(self):\n        initial_population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        opposition_population = self.lower_bound + self.upper_bound - initial_population\n        return np.vstack((initial_population, opposition_population))[:self.population_size]\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def select_parents(self, idx):\n        indices = [i for i in range(self.population_size) if i != idx]\n        return np.random.choice(indices, 3, replace=False)\n\n    def differential_evolution_mutation(self, idx, scale_factor):\n        a, b, c = self.select_parents(idx)\n        mutant_vector = self.population[a] + scale_factor * (self.population[b] - self.population[c])\n        return np.clip(mutant_vector, self.lower_bound, self.upper_bound)\n\n    def crossover(self, target_vector, mutant_vector, crossover_prob):\n        crossover_mask = np.random.rand(self.dim) < crossover_prob\n        return np.where(crossover_mask, mutant_vector, target_vector)\n\n    def local_search(self, vector):\n        perturbation = np.random.normal(0, self.learning_rate, self.dim)\n        candidate_vector = vector + perturbation\n        return np.clip(candidate_vector, self.lower_bound, self.upper_bound)\n\n    def __call__(self, func):\n        self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                # Dynamic adjustment of parameters\n                scale_factor = self.initial_scale_factor * (1 - self.evaluations / self.budget)\n                crossover_prob = self.initial_crossover_prob + 0.2 * np.random.rand()\n\n                mutant_vector = self.differential_evolution_mutation(i, scale_factor)\n                trial_vector = self.crossover(self.population[i], mutant_vector, crossover_prob)\n                trial_fitness = func(trial_vector)\n                self.evaluations += 1\n\n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial_vector\n                    self.fitness[i] = trial_fitness\n\n                if np.random.rand() < 0.2:\n                    candidate_vector = self.local_search(self.population[i])\n                    candidate_fitness = func(candidate_vector)\n                    self.evaluations += 1\n                    if candidate_fitness < self.fitness[i]:\n                        self.population[i] = candidate_vector\n                        self.fitness[i] = candidate_fitness\n\n                # Update learning rate adaptively\n                self.learning_rate = max(0.01, self.learning_rate * (1 - 0.05 * np.random.rand()))\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx]", "name": "HybridStochasticOptimizer", "description": "Hybrid optimizer with adaptive learning rate and opposition-based initialization for faster convergence.", "configspace": "", "generation": 32, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "c9aafe6b-332b-4283-9cc1-4b2e50c44e27", "metadata": {"aucs": [0.8614318631710612, 0.8615612395806681, 0.8648092636329068, 0.8836823589578061, 0.8809302815417128, 0.8537149049105373, 0.8649927264859298, 0.8661719092889808, 0.8732297035052712, 0.7784313840335078, 0.7934709027220181, 0.7709929153086685, 0.7666377215322294, 0.7792354193630129, 0.7955283567004858, 0.7874175449423783, 0.7934505999095464, 0.7963508686605465, 0.25196533963541234, 0.603323462921044, 0.31189512717187695, 0.45254508955474515, 0.3855834591332875, 0.6324217482611867, 0.38087210189123466, 0.37101210401952667, 0.37832843410229366, 0.3374576510463305, 0.1599578735010866, 0.25181667196560353, 0.15598715927116646, 0.15926218502651568, 0.14742944278799264, 0.33123888835847937, 0.3066524419836607, 0.16048855283955032, 0.9738501285938751, 0.9538242295949432, 0.9811516406082638, 0.9649564447290745, 0.9726531518905077, 0.9814481843774895, 0.9732999615150277, 0.9721568744354393, 0.9772487896256388, 0.5602477359628759, 0.5166699154516793, 0.49955145811133295, 0.4607762689339385, 0.5168840480364095, 0.5837268635955544, 0.5848178673193583, 0.44902566155463985, 0.6535189332618503, 0.8108564290388902, 0.7472995471422044, 0.7832292739258102, 0.780298104186407, 0.7997326258786164, 0.8307049265401217, 0.7670254946306878, 0.7966436314344594, 0.7737581975614084, 0.26793793612173356, 0.24955593945589905, 0.19834760026021148, 0.22530649729727037, 0.21576791524353967, 0.22803418538233733, 0.23532474250416835, 0.20160318462124072, 0.1920099262564383, 0.19613605298744063, 0.23556007360063813, 0.11440376677945807, 0.22544642163615658, 0.24862040742714708, 0.1874776391649946, 0.19655907028592323, 0.19284064583165672, 0.19577311722397894, 0.3108121261595491, 0.2566448530161354, 0.36431074059207347, 0.24025452278021453, 0.4802140919950957, 0.0007248673300417341, 0.35401012029935763, 0.3692632979788284, 0.276018593628536, 0.3623824055587753, 0.3924469974919602, 0.23556071143414548, 0.35272611497922335, 0.44565667864537895, 0.39468163509167276, 0.4061903350584134, 0.36215287040133537, 0.3973933722095969, 0.11732286633565558, 0.20172957542003978, 0.1498188877382276, 0.24772296138908012, 0.1546062088760296, 0.10282719367721671, 0.20924019897046064, 0.1183165114034116, 0.20632727477440505, 0.2809986328144657, 0.29684536408175155, 0.3175185646986213, 0.3737401167683222, 0.33517695108836454, 0.3271148862228106, 0.36693310045283345, 0.3166293273038533, 0.24146562201228117, 0.6055800258234042, 0.7234496228479592, 0.6438958422770305, 0.7507872332161957, 0.7324506280759618, 0.6589215501985713, 0.7496261142206224, 0.7209822579527148, 0.6855782937737074, 0.1371651233838116, 0.12075132229211483, 0.13780801853641733, 0.12584800341409697, 0.13850225460242616, 0.11675557226216027, 0.12863432580056944, 0.1273098808292441, 0.14268148704984884, 0.19934365172377466, 0.18594466654988895, 0.32832255345101913, 0.23469770515038102, 0.14866333116079655, 0.20267106519038025, 0.20152641892425394, 0.23576091839252133, 0.21421383347767264, 0.5391071467719948, 0.5042943805711715, 0.4612246809606788, 0.47676447358121665, 0.5291640446387131, 0.602638537448555, 0.6246427601161761, 0.5714078963023623, 0.6225968922190244, 0.33304162231889356, 0.3304783841194173, 0.4141875902408757, 0.354629285337982, 0.33990446568261656, 0.27919480653858797, 0.41214607585167573, 0.4488101082230267, 0.4070679191988382, 0.2075439556387948, 0.21303390663090027, 0.21635748499617935, 0.19827783015642464, 0.20259798106785998, 0.2337489300339528, 0.1995085629275789, 0.23000732022010184, 0.20258936164228614, 0.3583455868837565, 0.5974983189015708, 0.6315004735669381, 0.48375114306220435, 0.5228073851654269, 0.6765804181616237, 0.44806218567436396, 0.4405846418933689, 0.2231537031442834, 0.15177066878967904, 0.19069875851639195, 0.8538083672200958, 0.8489568076127689, 0.8369418101551436, 0.7959958487406582, 0.1809055777063855, 0.19441884874194382, 0.6735733685324677, 0.653629308683124, 0.16602485673297018, 0.20335804099962917, 0.20133806574003343, 0.20063819684592765, 0.4267483879660453, 0.203500441501264, 0.12468842409948466, 0.7601496747285006, 0.21347654253114523, 0.20557946271286787, 0.20946166959342294, 0.2037521178560513, 0.19899119990835357, 0.20502188897743512, 0.21185012528730773, 0.23029761548989025, 0.2226957927771256, 0.10106476573254897, 0.08282313409598063, 0.0927463241901183, 0.0892673049309205, 0.09187005772305634, 0.11808457473018663, 0.09136297465966581, 0.09200662746312827, 0.09667808533660249]}, "mutation_prompt": null}
{"id": "7d5e1f78-a5a3-44ad-a93f-2d539cd556b0", "solution": "import numpy as np\n\nclass HybridStochasticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.initial_scale_factor = 0.8\n        self.initial_crossover_prob = 0.7\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population = self.opposition_based_initialization()\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.learning_rate = 0.1\n\n    def opposition_based_initialization(self):\n        initial_population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        opposition_population = self.lower_bound + self.upper_bound - initial_population\n        return np.vstack((initial_population, opposition_population))[:self.population_size]\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def select_parents(self, idx):\n        indices = [i for i in range(self.population_size) if i != idx]\n        return np.random.choice(indices, 3, replace=False)\n\n    def differential_evolution_mutation(self, idx, scale_factor):\n        a, b, c = self.select_parents(idx)\n        mutant_vector = self.population[a] + scale_factor * (self.population[b] - self.population[c])\n        return np.clip(mutant_vector, self.lower_bound, self.upper_bound)\n\n    def crossover(self, target_vector, mutant_vector, crossover_prob):\n        crossover_mask = np.random.rand(self.dim) < crossover_prob\n        return np.where(crossover_mask, mutant_vector, target_vector)\n\n    def local_search(self, vector):\n        perturbation = np.random.normal(0, self.learning_rate, self.dim)\n        candidate_vector = vector + perturbation\n        return np.clip(candidate_vector, self.lower_bound, self.upper_bound)\n\n    def __call__(self, func):\n        self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                # Dynamic adjustment of parameters\n                scale_factor = self.initial_scale_factor * (1 - self.evaluations / self.budget)\n                crossover_prob = self.initial_crossover_prob + 0.2 * np.random.rand()\n\n                mutant_vector = self.differential_evolution_mutation(i, scale_factor)\n                trial_vector = self.crossover(self.population[i], mutant_vector, crossover_prob)\n                trial_fitness = func(trial_vector)\n                self.evaluations += 1\n\n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial_vector\n                    self.fitness[i] = trial_fitness\n\n                if np.random.rand() < 0.2:\n                    candidate_vector = self.local_search(self.population[i])\n                    candidate_fitness = func(candidate_vector)\n                    self.evaluations += 1\n                    if candidate_fitness < self.fitness[i]:\n                        self.population[i] = candidate_vector\n                        self.fitness[i] = candidate_fitness\n\n                # Update learning rate adaptively\n                self.learning_rate = max(0.01, self.learning_rate * (1 - 0.05 * np.random.rand()))\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx]", "name": "HybridStochasticOptimizer", "description": "Hybrid optimizer with adaptive learning rate and opposition-based initialization for faster convergence.", "configspace": "", "generation": 32, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "c9aafe6b-332b-4283-9cc1-4b2e50c44e27", "metadata": {"aucs": [0.8614318631710612, 0.8615612395806681, 0.8648092636329068, 0.8836823589578061, 0.8809302815417128, 0.8537149049105373, 0.8649927264859298, 0.8661719092889808, 0.8732297035052712, 0.7784313840335078, 0.7934709027220181, 0.7709929153086685, 0.7666377215322294, 0.7792354193630129, 0.7955283567004858, 0.7874175449423783, 0.7934505999095464, 0.7963508686605465, 0.25196533963541234, 0.603323462921044, 0.31189512717187695, 0.45254508955474515, 0.3855834591332875, 0.6324217482611867, 0.38087210189123466, 0.37101210401952667, 0.37832843410229366, 0.3374576510463305, 0.1599578735010866, 0.25181667196560353, 0.15598715927116646, 0.15926218502651568, 0.14742944278799264, 0.33123888835847937, 0.3066524419836607, 0.16048855283955032, 0.9738501285938751, 0.9538242295949432, 0.9811516406082638, 0.9649564447290745, 0.9726531518905077, 0.9814481843774895, 0.9732999615150277, 0.9721568744354393, 0.9772487896256388, 0.5602477359628759, 0.5166699154516793, 0.49955145811133295, 0.4607762689339385, 0.5168840480364095, 0.5837268635955544, 0.5848178673193583, 0.44902566155463985, 0.6535189332618503, 0.8108564290388902, 0.7472995471422044, 0.7832292739258102, 0.780298104186407, 0.7997326258786164, 0.8307049265401217, 0.7670254946306878, 0.7966436314344594, 0.7737581975614084, 0.26793793612173356, 0.24955593945589905, 0.19834760026021148, 0.22530649729727037, 0.21576791524353967, 0.22803418538233733, 0.23532474250416835, 0.20160318462124072, 0.1920099262564383, 0.19613605298744063, 0.23556007360063813, 0.11440376677945807, 0.22544642163615658, 0.24862040742714708, 0.1874776391649946, 0.19655907028592323, 0.19284064583165672, 0.19577311722397894, 0.3108121261595491, 0.2566448530161354, 0.36431074059207347, 0.24025452278021453, 0.4802140919950957, 0.0007248673300417341, 0.35401012029935763, 0.3692632979788284, 0.276018593628536, 0.3623824055587753, 0.3924469974919602, 0.23556071143414548, 0.35272611497922335, 0.44565667864537895, 0.39468163509167276, 0.4061903350584134, 0.36215287040133537, 0.3973933722095969, 0.11732286633565558, 0.20172957542003978, 0.1498188877382276, 0.24772296138908012, 0.1546062088760296, 0.10282719367721671, 0.20924019897046064, 0.1183165114034116, 0.20632727477440505, 0.2809986328144657, 0.29684536408175155, 0.3175185646986213, 0.3737401167683222, 0.33517695108836454, 0.3271148862228106, 0.36693310045283345, 0.3166293273038533, 0.24146562201228117, 0.6055800258234042, 0.7234496228479592, 0.6438958422770305, 0.7507872332161957, 0.7324506280759618, 0.6589215501985713, 0.7496261142206224, 0.7209822579527148, 0.6855782937737074, 0.1371651233838116, 0.12075132229211483, 0.13780801853641733, 0.12584800341409697, 0.13850225460242616, 0.11675557226216027, 0.12863432580056944, 0.1273098808292441, 0.14268148704984884, 0.19934365172377466, 0.18594466654988895, 0.32832255345101913, 0.23469770515038102, 0.14866333116079655, 0.20267106519038025, 0.20152641892425394, 0.23576091839252133, 0.21421383347767264, 0.5391071467719948, 0.5042943805711715, 0.4612246809606788, 0.47676447358121665, 0.5291640446387131, 0.602638537448555, 0.6246427601161761, 0.5714078963023623, 0.6225968922190244, 0.33304162231889356, 0.3304783841194173, 0.4141875902408757, 0.354629285337982, 0.33990446568261656, 0.27919480653858797, 0.41214607585167573, 0.4488101082230267, 0.4070679191988382, 0.2075439556387948, 0.21303390663090027, 0.21635748499617935, 0.19827783015642464, 0.20259798106785998, 0.2337489300339528, 0.1995085629275789, 0.23000732022010184, 0.20258936164228614, 0.3583455868837565, 0.5974983189015708, 0.6315004735669381, 0.48375114306220435, 0.5228073851654269, 0.6765804181616237, 0.44806218567436396, 0.4405846418933689, 0.2231537031442834, 0.15177066878967904, 0.19069875851639195, 0.8538083672200958, 0.8489568076127689, 0.8369418101551436, 0.7959958487406582, 0.1809055777063855, 0.19441884874194382, 0.6735733685324677, 0.653629308683124, 0.16602485673297018, 0.20335804099962917, 0.20133806574003343, 0.20063819684592765, 0.4267483879660453, 0.203500441501264, 0.12468842409948466, 0.7601496747285006, 0.21347654253114523, 0.20557946271286787, 0.20946166959342294, 0.2037521178560513, 0.19899119990835357, 0.20502188897743512, 0.21185012528730773, 0.23029761548989025, 0.2226957927771256, 0.10106476573254897, 0.08282313409598063, 0.0927463241901183, 0.0892673049309205, 0.09187005772305634, 0.11808457473018663, 0.09136297465966581, 0.09200662746312827, 0.09667808533660249]}, "mutation_prompt": null}
{"id": "e4b2d04e-7874-42e1-b7f3-26185a5d30cb", "solution": "import numpy as np\n\nclass HybridStochasticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.initial_scale_factor = 0.8\n        self.initial_crossover_prob = 0.7\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population = self.opposition_based_initialization()\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.learning_rate = 0.1\n\n    def opposition_based_initialization(self):\n        initial_population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        opposition_population = self.lower_bound + self.upper_bound - initial_population\n        return np.vstack((initial_population, opposition_population))[:self.population_size]\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def select_parents(self, idx):\n        indices = [i for i in range(self.population_size) if i != idx]\n        return np.random.choice(indices, 3, replace=False)\n\n    def differential_evolution_mutation(self, idx, scale_factor):\n        a, b, c = self.select_parents(idx)\n        mutant_vector = self.population[a] + scale_factor * (self.population[b] - self.population[c])\n        return np.clip(mutant_vector, self.lower_bound, self.upper_bound)\n\n    def crossover(self, target_vector, mutant_vector, crossover_prob):\n        crossover_mask = np.random.rand(self.dim) < crossover_prob\n        return np.where(crossover_mask, mutant_vector, target_vector)\n\n    def local_search(self, vector):\n        perturbation = np.random.normal(0, self.learning_rate, self.dim)\n        candidate_vector = vector + perturbation\n        return np.clip(candidate_vector, self.lower_bound, self.upper_bound)\n\n    def __call__(self, func):\n        self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                # Dynamic adjustment of parameters\n                scale_factor = self.initial_scale_factor * (1 - self.evaluations / self.budget)\n                crossover_prob = self.initial_crossover_prob + 0.2 * np.random.rand()\n\n                mutant_vector = self.differential_evolution_mutation(i, scale_factor)\n                trial_vector = self.crossover(self.population[i], mutant_vector, crossover_prob)\n                trial_fitness = func(trial_vector)\n                self.evaluations += 1\n\n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial_vector\n                    self.fitness[i] = trial_fitness\n\n                if np.random.rand() < 0.2:\n                    candidate_vector = self.local_search(self.population[i])\n                    candidate_fitness = func(candidate_vector)\n                    self.evaluations += 1\n                    if candidate_fitness < self.fitness[i]:\n                        self.population[i] = candidate_vector\n                        self.fitness[i] = candidate_fitness\n\n                # Update learning rate adaptively\n                self.learning_rate = max(0.01, self.learning_rate * (1 - 0.05 * np.random.rand()))\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx]", "name": "HybridStochasticOptimizer", "description": "Hybrid optimizer with adaptive learning rate and opposition-based initialization for faster convergence.", "configspace": "", "generation": 32, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "c9aafe6b-332b-4283-9cc1-4b2e50c44e27", "metadata": {"aucs": [0.8614318631710612, 0.8615612395806681, 0.8648092636329068, 0.8836823589578061, 0.8809302815417128, 0.8537149049105373, 0.8649927264859298, 0.8661719092889808, 0.8732297035052712, 0.7784313840335078, 0.7934709027220181, 0.7709929153086685, 0.7666377215322294, 0.7792354193630129, 0.7955283567004858, 0.7874175449423783, 0.7934505999095464, 0.7963508686605465, 0.25196533963541234, 0.603323462921044, 0.31189512717187695, 0.45254508955474515, 0.3855834591332875, 0.6324217482611867, 0.38087210189123466, 0.37101210401952667, 0.37832843410229366, 0.3374576510463305, 0.1599578735010866, 0.25181667196560353, 0.15598715927116646, 0.15926218502651568, 0.14742944278799264, 0.33123888835847937, 0.3066524419836607, 0.16048855283955032, 0.9738501285938751, 0.9538242295949432, 0.9811516406082638, 0.9649564447290745, 0.9726531518905077, 0.9814481843774895, 0.9732999615150277, 0.9721568744354393, 0.9772487896256388, 0.5602477359628759, 0.5166699154516793, 0.49955145811133295, 0.4607762689339385, 0.5168840480364095, 0.5837268635955544, 0.5848178673193583, 0.44902566155463985, 0.6535189332618503, 0.8108564290388902, 0.7472995471422044, 0.7832292739258102, 0.780298104186407, 0.7997326258786164, 0.8307049265401217, 0.7670254946306878, 0.7966436314344594, 0.7737581975614084, 0.26793793612173356, 0.24955593945589905, 0.19834760026021148, 0.22530649729727037, 0.21576791524353967, 0.22803418538233733, 0.23532474250416835, 0.20160318462124072, 0.1920099262564383, 0.19613605298744063, 0.23556007360063813, 0.11440376677945807, 0.22544642163615658, 0.24862040742714708, 0.1874776391649946, 0.19655907028592323, 0.19284064583165672, 0.19577311722397894, 0.3108121261595491, 0.2566448530161354, 0.36431074059207347, 0.24025452278021453, 0.4802140919950957, 0.0007248673300417341, 0.35401012029935763, 0.3692632979788284, 0.276018593628536, 0.3623824055587753, 0.3924469974919602, 0.23556071143414548, 0.35272611497922335, 0.44565667864537895, 0.39468163509167276, 0.4061903350584134, 0.36215287040133537, 0.3973933722095969, 0.11732286633565558, 0.20172957542003978, 0.1498188877382276, 0.24772296138908012, 0.1546062088760296, 0.10282719367721671, 0.20924019897046064, 0.1183165114034116, 0.20632727477440505, 0.2809986328144657, 0.29684536408175155, 0.3175185646986213, 0.3737401167683222, 0.33517695108836454, 0.3271148862228106, 0.36693310045283345, 0.3166293273038533, 0.24146562201228117, 0.6055800258234042, 0.7234496228479592, 0.6438958422770305, 0.7507872332161957, 0.7324506280759618, 0.6589215501985713, 0.7496261142206224, 0.7209822579527148, 0.6855782937737074, 0.1371651233838116, 0.12075132229211483, 0.13780801853641733, 0.12584800341409697, 0.13850225460242616, 0.11675557226216027, 0.12863432580056944, 0.1273098808292441, 0.14268148704984884, 0.19934365172377466, 0.18594466654988895, 0.32832255345101913, 0.23469770515038102, 0.14866333116079655, 0.20267106519038025, 0.20152641892425394, 0.23576091839252133, 0.21421383347767264, 0.5391071467719948, 0.5042943805711715, 0.4612246809606788, 0.47676447358121665, 0.5291640446387131, 0.602638537448555, 0.6246427601161761, 0.5714078963023623, 0.6225968922190244, 0.33304162231889356, 0.3304783841194173, 0.4141875902408757, 0.354629285337982, 0.33990446568261656, 0.27919480653858797, 0.41214607585167573, 0.4488101082230267, 0.4070679191988382, 0.2075439556387948, 0.21303390663090027, 0.21635748499617935, 0.19827783015642464, 0.20259798106785998, 0.2337489300339528, 0.1995085629275789, 0.23000732022010184, 0.20258936164228614, 0.3583455868837565, 0.5974983189015708, 0.6315004735669381, 0.48375114306220435, 0.5228073851654269, 0.6765804181616237, 0.44806218567436396, 0.4405846418933689, 0.2231537031442834, 0.15177066878967904, 0.19069875851639195, 0.8538083672200958, 0.8489568076127689, 0.8369418101551436, 0.7959958487406582, 0.1809055777063855, 0.19441884874194382, 0.6735733685324677, 0.653629308683124, 0.16602485673297018, 0.20335804099962917, 0.20133806574003343, 0.20063819684592765, 0.4267483879660453, 0.203500441501264, 0.12468842409948466, 0.7601496747285006, 0.21347654253114523, 0.20557946271286787, 0.20946166959342294, 0.2037521178560513, 0.19899119990835357, 0.20502188897743512, 0.21185012528730773, 0.23029761548989025, 0.2226957927771256, 0.10106476573254897, 0.08282313409598063, 0.0927463241901183, 0.0892673049309205, 0.09187005772305634, 0.11808457473018663, 0.09136297465966581, 0.09200662746312827, 0.09667808533660249]}, "mutation_prompt": null}
{"id": "23ab8242-ccde-4e48-ad2c-a2ababc6fd8a", "solution": "import numpy as np\n\nclass EnhancedHybridStochasticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.initial_scale_factor = 0.9  # Adjusted for more exploration\n        self.initial_crossover_prob = 0.75  # Slight increase for diversity\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population = self.opposition_based_initialization()\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.learning_rate = 0.1\n        self.elite_fraction = 0.1  # New parameter for elitism\n\n    def opposition_based_initialization(self):\n        initial_population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        opposition_population = self.lower_bound + self.upper_bound - initial_population\n        return np.vstack((initial_population, opposition_population))[:self.population_size]\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def select_parents(self, idx):\n        indices = [i for i in range(self.population_size) if i != idx]\n        return np.random.choice(indices, 3, replace=False)\n\n    def differential_evolution_mutation(self, idx, scale_factor):\n        a, b, c = self.select_parents(idx)\n        mutant_vector = self.population[a] + scale_factor * (self.population[b] - self.population[c])\n        return np.clip(mutant_vector, self.lower_bound, self.upper_bound)\n\n    def crossover(self, target_vector, mutant_vector, crossover_prob):\n        crossover_mask = np.random.rand(self.dim) < crossover_prob\n        return np.where(crossover_mask, mutant_vector, target_vector)\n\n    def local_search(self, vector):\n        perturbation = np.random.normal(0, self.learning_rate, self.dim)\n        candidate_vector = vector + perturbation\n        return np.clip(candidate_vector, self.lower_bound, self.upper_bound)\n\n    def __call__(self, func):\n        self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                # Dynamic adjustment of parameters with adaptive scaling\n                scale_factor = self.initial_scale_factor * (1 - self.evaluations / self.budget) * (0.5 + 0.5 * np.random.rand())\n                crossover_prob = self.initial_crossover_prob + 0.1 * np.random.rand()\n\n                mutant_vector = self.differential_evolution_mutation(i, scale_factor)\n                trial_vector = self.crossover(self.population[i], mutant_vector, crossover_prob)\n                trial_fitness = func(trial_vector)\n                self.evaluations += 1\n\n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial_vector\n                    self.fitness[i] = trial_fitness\n\n                if np.random.rand() < 0.2:\n                    candidate_vector = self.local_search(self.population[i])\n                    candidate_fitness = func(candidate_vector)\n                    self.evaluations += 1\n                    if candidate_fitness < self.fitness[i]:\n                        self.population[i] = candidate_vector\n                        self.fitness[i] = candidate_fitness\n\n                # Update learning rate adaptively\n                self.learning_rate = max(0.01, self.learning_rate * (1 - 0.05 * np.random.rand()))\n\n            # Elitist selection to retain a fraction of the best solutions\n            num_elite = int(self.elite_fraction * self.population_size)\n            elite_indices = np.argsort(self.fitness)[:num_elite]\n            self.population[:num_elite] = self.population[elite_indices]\n            self.fitness[:num_elite] = self.fitness[elite_indices]\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx]", "name": "EnhancedHybridStochasticOptimizer", "description": "Enhanced hybrid optimizer using adaptive mutation scaling and elitist selection for improved convergence.", "configspace": "", "generation": 52, "fitness": 0.3950306213153771, "feedback": "The algorithm EnhancedHybridStochasticOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.40 with standard deviation 0.28.", "error": "", "parent_id": "c9aafe6b-332b-4283-9cc1-4b2e50c44e27", "metadata": {"aucs": [0.9056427816591162, 0.9042458597045941, 0.9111391806040539, 0.9148367724068625, 0.9037439831261, 0.9054642983624702, 0.9134282247113845, 0.9146312869162002, 0.9169115399707252, 0.8416111001327229, 0.8422208060199978, 0.8333769058365226, 0.8386174567562036, 0.8379008897524454, 0.8444596391245534, 0.8511209762905434, 0.8062328061265037, 0.8546810931886284, 0.14109327997593923, 0.5673165538998646, 0.6709062469581997, 0.7316060752585911, 0.7156303856539447, 0.6980477942300101, 0.1584232768392192, 0.7049193467683759, 0.47140219979713804, 0.15580127370120367, 0.11927145682780915, 0.4345926648482349, 0.15448003218487372, 0.1781175910955386, 0.13927296381913912, 0.15302864505541525, 0.143908343584764, 0.17224212787936632, 0.9718752783037344, 0.988852802198224, 0.9652309544025051, 0.9868108180883112, 0.9845677535362845, 0.9729610733954887, 0.9629810784693568, 0.971490320371229, 0.9865874753531959, 0.6211174438384688, 0.4335276305885377, 0.5990427779480676, 0.6026888174184495, 0.6847533395352904, 0.5061758301812722, 0.7147973170571964, 0.6869858493783718, 0.6170328857212726, 0.35807550108630926, 0.37110836338639297, 0.22700587843425046, 0.21055777728930747, 0.8952449845483504, 0.8476728327550471, 0.1769249494518137, 0.8645569103344248, 0.2328664626720648, 0.2219017276504638, 0.22121541150922852, 0.1820481624289424, 0.2774921637477621, 0.19036459766488167, 0.2751147127962399, 0.22863804436105306, 0.13172240813581904, 0.21309751310469827, 0.19612474975964844, 0.2085417866294813, 0.22903948899149917, 0.3429671085990954, 0.24980693340637106, 0.20444146922551887, 0.26440372047929783, 0.12650932463709919, 0.2687687239214537, 0.0808487502380123, 0.15542251176525257, 0.138938307204728, 0.042265144641987384, 0.12109111138321771, 0.13942312618848973, 0.2759110688914974, 0.4440541126382147, 0.1478585480144482, 0.17437389810938297, 0.41068227188518824, 0.26970645279625227, 0.1927074591882909, 0.2187514356696002, 0.49084391259278304, 0.17778179004808392, 0.2873689196975868, 0.17158404708274555, 0.09438285700027349, 0.10564955356186334, 0.2362793311564304, 0.261232780355256, 0.09638090848635528, 0.16358807324107516, 0.26605109951107264, 0.16029283614762846, 0.09417997137795686, 0.34263689140914244, 0.3467468864945783, 0.27686764378847795, 0.21970884714448868, 0.3514794240779143, 0.3856628241933122, 0.27468840084259316, 0.18109279963855562, 0.42324490012909977, 0.6917233686100468, 0.6123893522423844, 0.6514337350043484, 0.6947982822480578, 0.6549786981893673, 0.6788563505504602, 0.6120669471293163, 0.7079986724785927, 0.6686131773354512, 0.11321896897898265, 0.15190851409061346, 0.12151450007638398, 0.09483557262971953, 0.1465425503333092, 0.14333757789569668, 0.12788938341824252, 0.11179666396808319, 0.14812895559744343, 0.15613681189757722, 0.18123874402183837, 0.14860189731967677, 0.1428965866499191, 0.19677115697287695, 0.2630003568544679, 0.17153474732877327, 0.1954900709043238, 0.1432419979547387, 0.6642308018541068, 0.5458541987633125, 0.5224153243100329, 0.5619797675557964, 0.45296355992677595, 0.5736570709733362, 0.640877817721946, 0.6211154061607388, 0.5799066399890322, 0.37397826013099233, 0.3842986835102965, 0.4271955824998166, 0.4949919141468938, 0.40116705239736494, 0.497426031256161, 0.45623134811762034, 0.4840747267301072, 0.561540359693721, 0.1899195971014337, 0.2479154555475317, 0.21524267274894537, 0.23686773598633082, 0.20076505784136722, 0.22935092647220967, 0.22241731134731546, 0.27494651303334594, 0.23203513569074308, 0.24035041460905215, 0.7521625285945365, 0.24859127974336714, 0.2210290868714473, 0.19824084800714403, 0.20726194096177264, 0.24700591799882898, 0.22380961626165718, 0.7617078384404334, 0.16285941236644152, 0.18438413270527898, 0.1530492375391186, 0.7925646874786423, 0.19831486395620213, 0.19252521514519927, 0.8753163917154108, 0.17603481665977527, 0.7965295638581512, 0.2080343864281431, 0.20866048254638858, 0.12657955100838858, 0.7996793118024942, 0.16038468999487798, 0.5774777241415057, 0.1568238113846554, 0.21086549402047183, 0.15333739783203149, 0.2404516259237376, 0.22168481419006225, 0.2350766620219934, 0.3653350729088921, 0.24103351425491593, 0.2051014328038433, 0.23730902934753872, 0.2308252530662528, 0.2590119718148788, 0.09682318332181206, 0.09795017276954499, 0.09955737711795687, 0.11178967639846937, 0.09847159316805443, 0.10230351725280284, 0.08960575556762562, 0.0942109963283081, 0.09838741315451549]}, "mutation_prompt": null}
{"id": "3b0bf209-ed41-492c-86cf-1da39380d223", "solution": "import numpy as np\n\nclass HybridStochasticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.initial_scale_factor = 0.8\n        self.initial_crossover_prob = 0.7\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population = self.opposition_based_initialization()\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.learning_rate = 0.1\n\n    def opposition_based_initialization(self):\n        initial_population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        opposition_population = self.lower_bound + self.upper_bound - initial_population\n        return np.vstack((initial_population, opposition_population))[:self.population_size]\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def select_parents(self, idx):\n        indices = [i for i in range(self.population_size) if i != idx]\n        return np.random.choice(indices, 3, replace=False)\n\n    def differential_evolution_mutation(self, idx, scale_factor):\n        a, b, c = self.select_parents(idx)\n        mutant_vector = self.population[a] + scale_factor * (self.population[b] - self.population[c])\n        return np.clip(mutant_vector, self.lower_bound, self.upper_bound)\n\n    def crossover(self, target_vector, mutant_vector, crossover_prob):\n        crossover_mask = np.random.rand(self.dim) < crossover_prob\n        return np.where(crossover_mask, mutant_vector, target_vector)\n\n    def local_search(self, vector):\n        perturbation = np.random.normal(0, self.learning_rate, self.dim)\n        candidate_vector = vector + perturbation\n        return np.clip(candidate_vector, self.lower_bound, self.upper_bound)\n\n    def __call__(self, func):\n        self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                # Dynamic adjustment of parameters\n                scale_factor = self.initial_scale_factor * (1 - self.evaluations / self.budget)\n                crossover_prob = self.initial_crossover_prob + 0.2 * np.random.rand()\n\n                mutant_vector = self.differential_evolution_mutation(i, scale_factor)\n                trial_vector = self.crossover(self.population[i], mutant_vector, crossover_prob)\n                trial_fitness = func(trial_vector)\n                self.evaluations += 1\n\n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial_vector\n                    self.fitness[i] = trial_fitness\n\n                if np.random.rand() < 0.2:\n                    candidate_vector = self.local_search(self.population[i])\n                    candidate_fitness = func(candidate_vector)\n                    self.evaluations += 1\n                    if candidate_fitness < self.fitness[i]:\n                        self.population[i] = candidate_vector\n                        self.fitness[i] = candidate_fitness\n\n                # Update learning rate adaptively\n                self.learning_rate = max(0.01, self.learning_rate * (1 - 0.05 * np.random.rand()))\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx]", "name": "HybridStochasticOptimizer", "description": "Hybrid optimizer with adaptive learning rate and opposition-based initialization for faster convergence.", "configspace": "", "generation": 32, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "c9aafe6b-332b-4283-9cc1-4b2e50c44e27", "metadata": {"aucs": [0.8614318631710612, 0.8615612395806681, 0.8648092636329068, 0.8836823589578061, 0.8809302815417128, 0.8537149049105373, 0.8649927264859298, 0.8661719092889808, 0.8732297035052712, 0.7784313840335078, 0.7934709027220181, 0.7709929153086685, 0.7666377215322294, 0.7792354193630129, 0.7955283567004858, 0.7874175449423783, 0.7934505999095464, 0.7963508686605465, 0.25196533963541234, 0.603323462921044, 0.31189512717187695, 0.45254508955474515, 0.3855834591332875, 0.6324217482611867, 0.38087210189123466, 0.37101210401952667, 0.37832843410229366, 0.3374576510463305, 0.1599578735010866, 0.25181667196560353, 0.15598715927116646, 0.15926218502651568, 0.14742944278799264, 0.33123888835847937, 0.3066524419836607, 0.16048855283955032, 0.9738501285938751, 0.9538242295949432, 0.9811516406082638, 0.9649564447290745, 0.9726531518905077, 0.9814481843774895, 0.9732999615150277, 0.9721568744354393, 0.9772487896256388, 0.5602477359628759, 0.5166699154516793, 0.49955145811133295, 0.4607762689339385, 0.5168840480364095, 0.5837268635955544, 0.5848178673193583, 0.44902566155463985, 0.6535189332618503, 0.8108564290388902, 0.7472995471422044, 0.7832292739258102, 0.780298104186407, 0.7997326258786164, 0.8307049265401217, 0.7670254946306878, 0.7966436314344594, 0.7737581975614084, 0.26793793612173356, 0.24955593945589905, 0.19834760026021148, 0.22530649729727037, 0.21576791524353967, 0.22803418538233733, 0.23532474250416835, 0.20160318462124072, 0.1920099262564383, 0.19613605298744063, 0.23556007360063813, 0.11440376677945807, 0.22544642163615658, 0.24862040742714708, 0.1874776391649946, 0.19655907028592323, 0.19284064583165672, 0.19577311722397894, 0.3108121261595491, 0.2566448530161354, 0.36431074059207347, 0.24025452278021453, 0.4802140919950957, 0.0007248673300417341, 0.35401012029935763, 0.3692632979788284, 0.276018593628536, 0.3623824055587753, 0.3924469974919602, 0.23556071143414548, 0.35272611497922335, 0.44565667864537895, 0.39468163509167276, 0.4061903350584134, 0.36215287040133537, 0.3973933722095969, 0.11732286633565558, 0.20172957542003978, 0.1498188877382276, 0.24772296138908012, 0.1546062088760296, 0.10282719367721671, 0.20924019897046064, 0.1183165114034116, 0.20632727477440505, 0.2809986328144657, 0.29684536408175155, 0.3175185646986213, 0.3737401167683222, 0.33517695108836454, 0.3271148862228106, 0.36693310045283345, 0.3166293273038533, 0.24146562201228117, 0.6055800258234042, 0.7234496228479592, 0.6438958422770305, 0.7507872332161957, 0.7324506280759618, 0.6589215501985713, 0.7496261142206224, 0.7209822579527148, 0.6855782937737074, 0.1371651233838116, 0.12075132229211483, 0.13780801853641733, 0.12584800341409697, 0.13850225460242616, 0.11675557226216027, 0.12863432580056944, 0.1273098808292441, 0.14268148704984884, 0.19934365172377466, 0.18594466654988895, 0.32832255345101913, 0.23469770515038102, 0.14866333116079655, 0.20267106519038025, 0.20152641892425394, 0.23576091839252133, 0.21421383347767264, 0.5391071467719948, 0.5042943805711715, 0.4612246809606788, 0.47676447358121665, 0.5291640446387131, 0.602638537448555, 0.6246427601161761, 0.5714078963023623, 0.6225968922190244, 0.33304162231889356, 0.3304783841194173, 0.4141875902408757, 0.354629285337982, 0.33990446568261656, 0.27919480653858797, 0.41214607585167573, 0.4488101082230267, 0.4070679191988382, 0.2075439556387948, 0.21303390663090027, 0.21635748499617935, 0.19827783015642464, 0.20259798106785998, 0.2337489300339528, 0.1995085629275789, 0.23000732022010184, 0.20258936164228614, 0.3583455868837565, 0.5974983189015708, 0.6315004735669381, 0.48375114306220435, 0.5228073851654269, 0.6765804181616237, 0.44806218567436396, 0.4405846418933689, 0.2231537031442834, 0.15177066878967904, 0.19069875851639195, 0.8538083672200958, 0.8489568076127689, 0.8369418101551436, 0.7959958487406582, 0.1809055777063855, 0.19441884874194382, 0.6735733685324677, 0.653629308683124, 0.16602485673297018, 0.20335804099962917, 0.20133806574003343, 0.20063819684592765, 0.4267483879660453, 0.203500441501264, 0.12468842409948466, 0.7601496747285006, 0.21347654253114523, 0.20557946271286787, 0.20946166959342294, 0.2037521178560513, 0.19899119990835357, 0.20502188897743512, 0.21185012528730773, 0.23029761548989025, 0.2226957927771256, 0.10106476573254897, 0.08282313409598063, 0.0927463241901183, 0.0892673049309205, 0.09187005772305634, 0.11808457473018663, 0.09136297465966581, 0.09200662746312827, 0.09667808533660249]}, "mutation_prompt": null}
{"id": "eae5c627-f96d-487d-b226-d285a97a64ae", "solution": "import numpy as np\n\nclass HybridStochasticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.initial_scale_factor = 0.8\n        self.initial_crossover_prob = 0.7\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population = self.opposition_based_initialization()\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.learning_rate = 0.1\n\n    def opposition_based_initialization(self):\n        initial_population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        opposition_population = self.lower_bound + self.upper_bound - initial_population\n        return np.vstack((initial_population, opposition_population))[:self.population_size]\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def select_parents(self, idx):\n        indices = [i for i in range(self.population_size) if i != idx]\n        return np.random.choice(indices, 3, replace=False)\n\n    def differential_evolution_mutation(self, idx, scale_factor):\n        a, b, c = self.select_parents(idx)\n        mutant_vector = self.population[a] + scale_factor * (self.population[b] - self.population[c])\n        return np.clip(mutant_vector, self.lower_bound, self.upper_bound)\n\n    def crossover(self, target_vector, mutant_vector, crossover_prob):\n        crossover_mask = np.random.rand(self.dim) < crossover_prob\n        return np.where(crossover_mask, mutant_vector, target_vector)\n\n    def local_search(self, vector):\n        perturbation = np.random.normal(0, self.learning_rate, self.dim)\n        candidate_vector = vector + perturbation\n        return np.clip(candidate_vector, self.lower_bound, self.upper_bound)\n\n    def __call__(self, func):\n        self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                # Dynamic adjustment of parameters\n                scale_factor = self.initial_scale_factor * (1 - self.evaluations / self.budget)\n                crossover_prob = self.initial_crossover_prob + 0.2 * np.random.rand()\n\n                mutant_vector = self.differential_evolution_mutation(i, scale_factor)\n                trial_vector = self.crossover(self.population[i], mutant_vector, crossover_prob)\n                trial_fitness = func(trial_vector)\n                self.evaluations += 1\n\n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial_vector\n                    self.fitness[i] = trial_fitness\n\n                if np.random.rand() < 0.2:\n                    candidate_vector = self.local_search(self.population[i])\n                    candidate_fitness = func(candidate_vector)\n                    self.evaluations += 1\n                    if candidate_fitness < self.fitness[i]:\n                        self.population[i] = candidate_vector\n                        self.fitness[i] = candidate_fitness\n\n                # Update learning rate adaptively\n                self.learning_rate = max(0.01, self.learning_rate * (1 - 0.05 * np.random.rand()))\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx]", "name": "HybridStochasticOptimizer", "description": "Hybrid optimizer with adaptive learning rate and opposition-based initialization for faster convergence.", "configspace": "", "generation": 32, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "c9aafe6b-332b-4283-9cc1-4b2e50c44e27", "metadata": {"aucs": [0.8614318631710612, 0.8615612395806681, 0.8648092636329068, 0.8836823589578061, 0.8809302815417128, 0.8537149049105373, 0.8649927264859298, 0.8661719092889808, 0.8732297035052712, 0.7784313840335078, 0.7934709027220181, 0.7709929153086685, 0.7666377215322294, 0.7792354193630129, 0.7955283567004858, 0.7874175449423783, 0.7934505999095464, 0.7963508686605465, 0.25196533963541234, 0.603323462921044, 0.31189512717187695, 0.45254508955474515, 0.3855834591332875, 0.6324217482611867, 0.38087210189123466, 0.37101210401952667, 0.37832843410229366, 0.3374576510463305, 0.1599578735010866, 0.25181667196560353, 0.15598715927116646, 0.15926218502651568, 0.14742944278799264, 0.33123888835847937, 0.3066524419836607, 0.16048855283955032, 0.9738501285938751, 0.9538242295949432, 0.9811516406082638, 0.9649564447290745, 0.9726531518905077, 0.9814481843774895, 0.9732999615150277, 0.9721568744354393, 0.9772487896256388, 0.5602477359628759, 0.5166699154516793, 0.49955145811133295, 0.4607762689339385, 0.5168840480364095, 0.5837268635955544, 0.5848178673193583, 0.44902566155463985, 0.6535189332618503, 0.8108564290388902, 0.7472995471422044, 0.7832292739258102, 0.780298104186407, 0.7997326258786164, 0.8307049265401217, 0.7670254946306878, 0.7966436314344594, 0.7737581975614084, 0.26793793612173356, 0.24955593945589905, 0.19834760026021148, 0.22530649729727037, 0.21576791524353967, 0.22803418538233733, 0.23532474250416835, 0.20160318462124072, 0.1920099262564383, 0.19613605298744063, 0.23556007360063813, 0.11440376677945807, 0.22544642163615658, 0.24862040742714708, 0.1874776391649946, 0.19655907028592323, 0.19284064583165672, 0.19577311722397894, 0.3108121261595491, 0.2566448530161354, 0.36431074059207347, 0.24025452278021453, 0.4802140919950957, 0.0007248673300417341, 0.35401012029935763, 0.3692632979788284, 0.276018593628536, 0.3623824055587753, 0.3924469974919602, 0.23556071143414548, 0.35272611497922335, 0.44565667864537895, 0.39468163509167276, 0.4061903350584134, 0.36215287040133537, 0.3973933722095969, 0.11732286633565558, 0.20172957542003978, 0.1498188877382276, 0.24772296138908012, 0.1546062088760296, 0.10282719367721671, 0.20924019897046064, 0.1183165114034116, 0.20632727477440505, 0.2809986328144657, 0.29684536408175155, 0.3175185646986213, 0.3737401167683222, 0.33517695108836454, 0.3271148862228106, 0.36693310045283345, 0.3166293273038533, 0.24146562201228117, 0.6055800258234042, 0.7234496228479592, 0.6438958422770305, 0.7507872332161957, 0.7324506280759618, 0.6589215501985713, 0.7496261142206224, 0.7209822579527148, 0.6855782937737074, 0.1371651233838116, 0.12075132229211483, 0.13780801853641733, 0.12584800341409697, 0.13850225460242616, 0.11675557226216027, 0.12863432580056944, 0.1273098808292441, 0.14268148704984884, 0.19934365172377466, 0.18594466654988895, 0.32832255345101913, 0.23469770515038102, 0.14866333116079655, 0.20267106519038025, 0.20152641892425394, 0.23576091839252133, 0.21421383347767264, 0.5391071467719948, 0.5042943805711715, 0.4612246809606788, 0.47676447358121665, 0.5291640446387131, 0.602638537448555, 0.6246427601161761, 0.5714078963023623, 0.6225968922190244, 0.33304162231889356, 0.3304783841194173, 0.4141875902408757, 0.354629285337982, 0.33990446568261656, 0.27919480653858797, 0.41214607585167573, 0.4488101082230267, 0.4070679191988382, 0.2075439556387948, 0.21303390663090027, 0.21635748499617935, 0.19827783015642464, 0.20259798106785998, 0.2337489300339528, 0.1995085629275789, 0.23000732022010184, 0.20258936164228614, 0.3583455868837565, 0.5974983189015708, 0.6315004735669381, 0.48375114306220435, 0.5228073851654269, 0.6765804181616237, 0.44806218567436396, 0.4405846418933689, 0.2231537031442834, 0.15177066878967904, 0.19069875851639195, 0.8538083672200958, 0.8489568076127689, 0.8369418101551436, 0.7959958487406582, 0.1809055777063855, 0.19441884874194382, 0.6735733685324677, 0.653629308683124, 0.16602485673297018, 0.20335804099962917, 0.20133806574003343, 0.20063819684592765, 0.4267483879660453, 0.203500441501264, 0.12468842409948466, 0.7601496747285006, 0.21347654253114523, 0.20557946271286787, 0.20946166959342294, 0.2037521178560513, 0.19899119990835357, 0.20502188897743512, 0.21185012528730773, 0.23029761548989025, 0.2226957927771256, 0.10106476573254897, 0.08282313409598063, 0.0927463241901183, 0.0892673049309205, 0.09187005772305634, 0.11808457473018663, 0.09136297465966581, 0.09200662746312827, 0.09667808533660249]}, "mutation_prompt": null}
{"id": "b4a22ef3-d924-4a84-8667-8409804e898a", "solution": "import numpy as np\n\nclass HybridStochasticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.initial_scale_factor = 0.8\n        self.initial_crossover_prob = 0.7\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population = self.opposition_based_initialization()\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.learning_rate = 0.1\n\n    def opposition_based_initialization(self):\n        initial_population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        opposition_population = self.lower_bound + self.upper_bound - initial_population\n        return np.vstack((initial_population, opposition_population))[:self.population_size]\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def select_parents(self, idx):\n        indices = [i for i in range(self.population_size) if i != idx]\n        return np.random.choice(indices, 3, replace=False)\n\n    def differential_evolution_mutation(self, idx, scale_factor):\n        a, b, c = self.select_parents(idx)\n        mutant_vector = self.population[a] + scale_factor * (self.population[b] - self.population[c])\n        return np.clip(mutant_vector, self.lower_bound, self.upper_bound)\n\n    def crossover(self, target_vector, mutant_vector, crossover_prob):\n        crossover_mask = np.random.rand(self.dim) < crossover_prob\n        return np.where(crossover_mask, mutant_vector, target_vector)\n\n    def local_search(self, vector):\n        perturbation = np.random.normal(0, self.learning_rate, self.dim)\n        candidate_vector = vector + perturbation\n        return np.clip(candidate_vector, self.lower_bound, self.upper_bound)\n\n    def __call__(self, func):\n        self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                # Dynamic adjustment of parameters\n                scale_factor = self.initial_scale_factor * (1 - self.evaluations / self.budget)\n                crossover_prob = self.initial_crossover_prob + 0.2 * np.random.rand()\n\n                mutant_vector = self.differential_evolution_mutation(i, scale_factor)\n                trial_vector = self.crossover(self.population[i], mutant_vector, crossover_prob)\n                trial_fitness = func(trial_vector)\n                self.evaluations += 1\n\n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial_vector\n                    self.fitness[i] = trial_fitness\n\n                if np.random.rand() < 0.2:\n                    candidate_vector = self.local_search(self.population[i])\n                    candidate_fitness = func(candidate_vector)\n                    self.evaluations += 1\n                    if candidate_fitness < self.fitness[i]:\n                        self.population[i] = candidate_vector\n                        self.fitness[i] = candidate_fitness\n\n                # Update learning rate adaptively\n                self.learning_rate = max(0.01, self.learning_rate * (1 - 0.05 * np.random.rand()))\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx]", "name": "HybridStochasticOptimizer", "description": "Hybrid optimizer with adaptive learning rate and opposition-based initialization for faster convergence.", "configspace": "", "generation": 32, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "c9aafe6b-332b-4283-9cc1-4b2e50c44e27", "metadata": {"aucs": [0.8614318631710612, 0.8615612395806681, 0.8648092636329068, 0.8836823589578061, 0.8809302815417128, 0.8537149049105373, 0.8649927264859298, 0.8661719092889808, 0.8732297035052712, 0.7784313840335078, 0.7934709027220181, 0.7709929153086685, 0.7666377215322294, 0.7792354193630129, 0.7955283567004858, 0.7874175449423783, 0.7934505999095464, 0.7963508686605465, 0.25196533963541234, 0.603323462921044, 0.31189512717187695, 0.45254508955474515, 0.3855834591332875, 0.6324217482611867, 0.38087210189123466, 0.37101210401952667, 0.37832843410229366, 0.3374576510463305, 0.1599578735010866, 0.25181667196560353, 0.15598715927116646, 0.15926218502651568, 0.14742944278799264, 0.33123888835847937, 0.3066524419836607, 0.16048855283955032, 0.9738501285938751, 0.9538242295949432, 0.9811516406082638, 0.9649564447290745, 0.9726531518905077, 0.9814481843774895, 0.9732999615150277, 0.9721568744354393, 0.9772487896256388, 0.5602477359628759, 0.5166699154516793, 0.49955145811133295, 0.4607762689339385, 0.5168840480364095, 0.5837268635955544, 0.5848178673193583, 0.44902566155463985, 0.6535189332618503, 0.8108564290388902, 0.7472995471422044, 0.7832292739258102, 0.780298104186407, 0.7997326258786164, 0.8307049265401217, 0.7670254946306878, 0.7966436314344594, 0.7737581975614084, 0.26793793612173356, 0.24955593945589905, 0.19834760026021148, 0.22530649729727037, 0.21576791524353967, 0.22803418538233733, 0.23532474250416835, 0.20160318462124072, 0.1920099262564383, 0.19613605298744063, 0.23556007360063813, 0.11440376677945807, 0.22544642163615658, 0.24862040742714708, 0.1874776391649946, 0.19655907028592323, 0.19284064583165672, 0.19577311722397894, 0.3108121261595491, 0.2566448530161354, 0.36431074059207347, 0.24025452278021453, 0.4802140919950957, 0.0007248673300417341, 0.35401012029935763, 0.3692632979788284, 0.276018593628536, 0.3623824055587753, 0.3924469974919602, 0.23556071143414548, 0.35272611497922335, 0.44565667864537895, 0.39468163509167276, 0.4061903350584134, 0.36215287040133537, 0.3973933722095969, 0.11732286633565558, 0.20172957542003978, 0.1498188877382276, 0.24772296138908012, 0.1546062088760296, 0.10282719367721671, 0.20924019897046064, 0.1183165114034116, 0.20632727477440505, 0.2809986328144657, 0.29684536408175155, 0.3175185646986213, 0.3737401167683222, 0.33517695108836454, 0.3271148862228106, 0.36693310045283345, 0.3166293273038533, 0.24146562201228117, 0.6055800258234042, 0.7234496228479592, 0.6438958422770305, 0.7507872332161957, 0.7324506280759618, 0.6589215501985713, 0.7496261142206224, 0.7209822579527148, 0.6855782937737074, 0.1371651233838116, 0.12075132229211483, 0.13780801853641733, 0.12584800341409697, 0.13850225460242616, 0.11675557226216027, 0.12863432580056944, 0.1273098808292441, 0.14268148704984884, 0.19934365172377466, 0.18594466654988895, 0.32832255345101913, 0.23469770515038102, 0.14866333116079655, 0.20267106519038025, 0.20152641892425394, 0.23576091839252133, 0.21421383347767264, 0.5391071467719948, 0.5042943805711715, 0.4612246809606788, 0.47676447358121665, 0.5291640446387131, 0.602638537448555, 0.6246427601161761, 0.5714078963023623, 0.6225968922190244, 0.33304162231889356, 0.3304783841194173, 0.4141875902408757, 0.354629285337982, 0.33990446568261656, 0.27919480653858797, 0.41214607585167573, 0.4488101082230267, 0.4070679191988382, 0.2075439556387948, 0.21303390663090027, 0.21635748499617935, 0.19827783015642464, 0.20259798106785998, 0.2337489300339528, 0.1995085629275789, 0.23000732022010184, 0.20258936164228614, 0.3583455868837565, 0.5974983189015708, 0.6315004735669381, 0.48375114306220435, 0.5228073851654269, 0.6765804181616237, 0.44806218567436396, 0.4405846418933689, 0.2231537031442834, 0.15177066878967904, 0.19069875851639195, 0.8538083672200958, 0.8489568076127689, 0.8369418101551436, 0.7959958487406582, 0.1809055777063855, 0.19441884874194382, 0.6735733685324677, 0.653629308683124, 0.16602485673297018, 0.20335804099962917, 0.20133806574003343, 0.20063819684592765, 0.4267483879660453, 0.203500441501264, 0.12468842409948466, 0.7601496747285006, 0.21347654253114523, 0.20557946271286787, 0.20946166959342294, 0.2037521178560513, 0.19899119990835357, 0.20502188897743512, 0.21185012528730773, 0.23029761548989025, 0.2226957927771256, 0.10106476573254897, 0.08282313409598063, 0.0927463241901183, 0.0892673049309205, 0.09187005772305634, 0.11808457473018663, 0.09136297465966581, 0.09200662746312827, 0.09667808533660249]}, "mutation_prompt": null}
{"id": "694680cb-bfe5-474e-a131-f1e51a4fe8bf", "solution": "import numpy as np\n\nclass EnhancedHybridStochasticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.scale_factor_min = 0.4\n        self.scale_factor_max = 0.9\n        self.initial_crossover_prob = 0.7\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population = self.opposition_based_initialization()\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.base_learning_rate = 0.05\n        self.momentum = 0.9\n\n    def opposition_based_initialization(self):\n        initial_population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        opposition_population = self.lower_bound + self.upper_bound - initial_population\n        return np.vstack((initial_population, opposition_population))[:self.population_size]\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def select_parents(self, idx):\n        indices = [i for i in range(self.population_size) if i != idx]\n        return np.random.choice(indices, 3, replace=False)\n\n    def differential_evolution_mutation(self, idx, scale_factor):\n        a, b, c = self.select_parents(idx)\n        mutant_vector = self.population[a] + scale_factor * (self.population[b] - self.population[c])\n        return np.clip(mutant_vector, self.lower_bound, self.upper_bound)\n\n    def crossover(self, target_vector, mutant_vector, crossover_prob):\n        crossover_mask = np.random.rand(self.dim) < crossover_prob\n        return np.where(crossover_mask, mutant_vector, target_vector)\n\n    def local_search(self, vector):\n        perturbation = np.random.normal(0, self.base_learning_rate, self.dim)\n        candidate_vector = vector + self.momentum * perturbation\n        return np.clip(candidate_vector, self.lower_bound, self.upper_bound)\n\n    def __call__(self, func):\n        self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                # Adaptively adjust mutation scale factor\n                scale_factor = self.scale_factor_min + (self.scale_factor_max - self.scale_factor_min) * (1 - self.evaluations / self.budget)\n                crossover_prob = self.initial_crossover_prob + 0.1 * np.random.rand()\n\n                mutant_vector = self.differential_evolution_mutation(i, scale_factor)\n                trial_vector = self.crossover(self.population[i], mutant_vector, crossover_prob)\n                trial_fitness = func(trial_vector)\n                self.evaluations += 1\n\n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial_vector\n                    self.fitness[i] = trial_fitness\n\n                if np.random.rand() < 0.25:\n                    candidate_vector = self.local_search(self.population[i])\n                    candidate_fitness = func(candidate_vector)\n                    self.evaluations += 1\n                    if candidate_fitness < self.fitness[i]:\n                        self.population[i] = candidate_vector\n                        self.fitness[i] = candidate_fitness\n\n                # Update momentum adaptively\n                self.momentum = max(0.5, self.momentum * (1 - 0.03 * np.random.rand()))\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx]", "name": "EnhancedHybridStochasticOptimizer", "description": "Enhanced Hybrid Optimizer with adaptive mutation scaling and refined local search for improved convergence.", "configspace": "", "generation": 56, "fitness": 0.38487029109771714, "feedback": "The algorithm EnhancedHybridStochasticOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.38 with standard deviation 0.24.", "error": "", "parent_id": "c9aafe6b-332b-4283-9cc1-4b2e50c44e27", "metadata": {"aucs": [0.8316476204700164, 0.8286111046223771, 0.8318065381461994, 0.8474533804550513, 0.8241179731067093, 0.8396997882655283, 0.8187127817677694, 0.8466088912269187, 0.8224709514439056, 0.7371335982830756, 0.7273545479147558, 0.7238557318872582, 0.6966510509862194, 0.7428452845701713, 0.7151316423624783, 0.7358121434588225, 0.7299186004294671, 0.7091982865773987, 0.46844868566595055, 0.41639254800599435, 0.42014863311999895, 0.5356850039646885, 0.2916023629659059, 0.38195106509301735, 0.5127406526283793, 0.3784795589726786, 0.481512013530769, 0.33331975838465167, 0.35148080951180205, 0.3851156030164764, 0.26813883077965917, 0.16034262127813237, 0.15120606825101202, 0.4907969498356135, 0.4095229890923022, 0.4075895550772055, 0.9545903030438733, 0.9738284499272435, 0.9588638337092308, 0.9588356413387651, 0.9651372698696346, 0.9790369053161754, 0.9663785922377637, 0.9620539947815175, 0.9823134867881882, 0.5430882055019838, 0.5388652142771608, 0.5172438896828649, 0.5545972742545997, 0.5371033347997087, 0.5905952174446762, 0.4741125151330444, 0.6041879225656004, 0.5544351937911692, 0.6550134832868182, 0.6833080476024653, 0.7287702955296635, 0.699371790473651, 0.6711380935197373, 0.6680277440412852, 0.7150882246045193, 0.6763070776797183, 0.6929018284086846, 0.3232976704173429, 0.46243270719839025, 0.2605575458135442, 0.3203148223945985, 0.3647872949785932, 0.29766640940809197, 0.3215831259595515, 0.21954595494971996, 0.37784551476542094, 0.16123621640869423, 0.12329695920287032, 0.38091602182451745, 0.39872142214039963, 0.2339677226492517, 0.3325737388122516, 0.37131515387158875, 0.22507246308810658, 0.4138359159099525, 0.1992099678719721, 0.229578890000681, 0.23196953982113677, 0.22009481115607965, 0.13653700527222734, 0.23101033227830559, 0.2403964267030797, 0.21085886468785586, 0.24203564672705624, 0.29492436758941687, 0.26468919956824033, 0.36874548000635954, 0.3598483287638842, 0.3694710742406472, 0.3575759072283895, 0.3724452237503423, 0.40389261056489645, 0.4143758072671846, 0.08953567362594916, 0.1197833258300316, 0.05880721016765489, 0.10275799051887768, 0.08298570569277619, 0.12648928539285276, 0.11519628162935602, 0.12003394548694069, 0.11101804896907819, 0.2477615033692332, 0.23407495226565422, 0.24056499369070927, 0.2896536586956532, 0.2813997873009805, 0.2754926608224746, 0.2464170822124725, 0.253952194592598, 0.2538469971346644, 0.6540300958718539, 0.6077670823449624, 0.677851520156899, 0.6766201278219537, 0.6310253738981685, 0.667649480033425, 0.6499497701058286, 0.6284068890778862, 0.6604232529347416, 0.12090346152535492, 0.12793610066790384, 0.11789238436205474, 0.10762949005122624, 0.13502679480193858, 0.15553013129267745, 0.11793128786891371, 0.1334882133914269, 0.11599939672284831, 0.18095306182312965, 0.16417229984988424, 0.15089976438648212, 0.21382452894132142, 0.19801901875465466, 0.19255676567272617, 0.21988800983954582, 0.1987440782373695, 0.1799239995820604, 0.4012502703480224, 0.3973306093435305, 0.3871354170611282, 0.36535433826989017, 0.3558279362820056, 0.3308122732163402, 0.44870199056942106, 0.38104581229231416, 0.5270725447959888, 0.25934322133914667, 0.27643176696201377, 0.24254183240825145, 0.25793613686972083, 0.2214903278432574, 0.26939908775065946, 0.2996365529420665, 0.24059456228427234, 0.3278034319053963, 0.178560555254188, 0.1903157655698533, 0.21800029317374248, 0.20355983124635568, 0.18091052599781055, 0.17814792516594624, 0.18659474065092074, 0.20067799385212592, 0.1806483497030542, 0.4425581352275243, 0.38807162444440524, 0.49881609255399306, 0.598754921595309, 0.31024882715835034, 0.2132449940062502, 0.20235910835497095, 0.26029467738698375, 0.41607549742855354, 0.6908186335774673, 0.7209212411924009, 0.1491226687299091, 0.679211019888907, 0.18059706504964124, 0.18463102848875756, 0.1864973668160088, 0.5832664475640148, 0.1531325288700489, 0.16596600461295963, 0.20740639924041626, 0.6604921361030367, 0.19921551181814, 0.1606472567847712, 0.15694316828314236, 0.20871370102865472, 0.6865075462007331, 0.21075708509134006, 0.2078057353218682, 0.19313623385777434, 0.1781283166820623, 0.24245710369159346, 0.191027273086808, 0.20683106802232498, 0.1884185225545275, 0.20636928462879112, 0.19648623101669627, 0.08475012581527686, 0.09000568085888627, 0.0804933785921299, 0.09207905995255383, 0.08589315577393353, 0.07996135316289787, 0.08254932083903199, 0.08357133086986401, 0.08948764632182626]}, "mutation_prompt": null}
{"id": "fa43ea62-9578-4011-b48c-939fe7c8c16d", "solution": "import numpy as np\n\nclass HybridStochasticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.initial_scale_factor = 0.8\n        self.initial_crossover_prob = 0.7\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population = self.opposition_based_initialization()\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.learning_rate = 0.1\n\n    def opposition_based_initialization(self):\n        initial_population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        opposition_population = self.lower_bound + self.upper_bound - initial_population\n        return np.vstack((initial_population, opposition_population))[:self.population_size]\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def select_parents(self, idx):\n        indices = [i for i in range(self.population_size) if i != idx]\n        return np.random.choice(indices, 3, replace=False)\n\n    def differential_evolution_mutation(self, idx, scale_factor):\n        a, b, c = self.select_parents(idx)\n        mutant_vector = self.population[a] + scale_factor * (self.population[b] - self.population[c])\n        return np.clip(mutant_vector, self.lower_bound, self.upper_bound)\n\n    def crossover(self, target_vector, mutant_vector, crossover_prob):\n        crossover_mask = np.random.rand(self.dim) < crossover_prob\n        return np.where(crossover_mask, mutant_vector, target_vector)\n\n    def local_search(self, vector):\n        perturbation = np.random.normal(0, self.learning_rate, self.dim)\n        candidate_vector = vector + perturbation\n        return np.clip(candidate_vector, self.lower_bound, self.upper_bound)\n\n    def __call__(self, func):\n        self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                # Dynamic adjustment of parameters\n                scale_factor = self.initial_scale_factor * (1 - self.evaluations / self.budget)\n                crossover_prob = self.initial_crossover_prob + 0.2 * np.random.rand()\n\n                mutant_vector = self.differential_evolution_mutation(i, scale_factor)\n                trial_vector = self.crossover(self.population[i], mutant_vector, crossover_prob)\n                trial_fitness = func(trial_vector)\n                self.evaluations += 1\n\n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial_vector\n                    self.fitness[i] = trial_fitness\n\n                if np.random.rand() < 0.2:\n                    candidate_vector = self.local_search(self.population[i])\n                    candidate_fitness = func(candidate_vector)\n                    self.evaluations += 1\n                    if candidate_fitness < self.fitness[i]:\n                        self.population[i] = candidate_vector\n                        self.fitness[i] = candidate_fitness\n\n                # Update learning rate adaptively\n                self.learning_rate = max(0.01, self.learning_rate * (1 - 0.05 * np.random.rand()))\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx]", "name": "HybridStochasticOptimizer", "description": "Hybrid optimizer with adaptive learning rate and opposition-based initialization for faster convergence.", "configspace": "", "generation": 32, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "c9aafe6b-332b-4283-9cc1-4b2e50c44e27", "metadata": {"aucs": [0.8614318631710612, 0.8615612395806681, 0.8648092636329068, 0.8836823589578061, 0.8809302815417128, 0.8537149049105373, 0.8649927264859298, 0.8661719092889808, 0.8732297035052712, 0.7784313840335078, 0.7934709027220181, 0.7709929153086685, 0.7666377215322294, 0.7792354193630129, 0.7955283567004858, 0.7874175449423783, 0.7934505999095464, 0.7963508686605465, 0.25196533963541234, 0.603323462921044, 0.31189512717187695, 0.45254508955474515, 0.3855834591332875, 0.6324217482611867, 0.38087210189123466, 0.37101210401952667, 0.37832843410229366, 0.3374576510463305, 0.1599578735010866, 0.25181667196560353, 0.15598715927116646, 0.15926218502651568, 0.14742944278799264, 0.33123888835847937, 0.3066524419836607, 0.16048855283955032, 0.9738501285938751, 0.9538242295949432, 0.9811516406082638, 0.9649564447290745, 0.9726531518905077, 0.9814481843774895, 0.9732999615150277, 0.9721568744354393, 0.9772487896256388, 0.5602477359628759, 0.5166699154516793, 0.49955145811133295, 0.4607762689339385, 0.5168840480364095, 0.5837268635955544, 0.5848178673193583, 0.44902566155463985, 0.6535189332618503, 0.8108564290388902, 0.7472995471422044, 0.7832292739258102, 0.780298104186407, 0.7997326258786164, 0.8307049265401217, 0.7670254946306878, 0.7966436314344594, 0.7737581975614084, 0.26793793612173356, 0.24955593945589905, 0.19834760026021148, 0.22530649729727037, 0.21576791524353967, 0.22803418538233733, 0.23532474250416835, 0.20160318462124072, 0.1920099262564383, 0.19613605298744063, 0.23556007360063813, 0.11440376677945807, 0.22544642163615658, 0.24862040742714708, 0.1874776391649946, 0.19655907028592323, 0.19284064583165672, 0.19577311722397894, 0.3108121261595491, 0.2566448530161354, 0.36431074059207347, 0.24025452278021453, 0.4802140919950957, 0.0007248673300417341, 0.35401012029935763, 0.3692632979788284, 0.276018593628536, 0.3623824055587753, 0.3924469974919602, 0.23556071143414548, 0.35272611497922335, 0.44565667864537895, 0.39468163509167276, 0.4061903350584134, 0.36215287040133537, 0.3973933722095969, 0.11732286633565558, 0.20172957542003978, 0.1498188877382276, 0.24772296138908012, 0.1546062088760296, 0.10282719367721671, 0.20924019897046064, 0.1183165114034116, 0.20632727477440505, 0.2809986328144657, 0.29684536408175155, 0.3175185646986213, 0.3737401167683222, 0.33517695108836454, 0.3271148862228106, 0.36693310045283345, 0.3166293273038533, 0.24146562201228117, 0.6055800258234042, 0.7234496228479592, 0.6438958422770305, 0.7507872332161957, 0.7324506280759618, 0.6589215501985713, 0.7496261142206224, 0.7209822579527148, 0.6855782937737074, 0.1371651233838116, 0.12075132229211483, 0.13780801853641733, 0.12584800341409697, 0.13850225460242616, 0.11675557226216027, 0.12863432580056944, 0.1273098808292441, 0.14268148704984884, 0.19934365172377466, 0.18594466654988895, 0.32832255345101913, 0.23469770515038102, 0.14866333116079655, 0.20267106519038025, 0.20152641892425394, 0.23576091839252133, 0.21421383347767264, 0.5391071467719948, 0.5042943805711715, 0.4612246809606788, 0.47676447358121665, 0.5291640446387131, 0.602638537448555, 0.6246427601161761, 0.5714078963023623, 0.6225968922190244, 0.33304162231889356, 0.3304783841194173, 0.4141875902408757, 0.354629285337982, 0.33990446568261656, 0.27919480653858797, 0.41214607585167573, 0.4488101082230267, 0.4070679191988382, 0.2075439556387948, 0.21303390663090027, 0.21635748499617935, 0.19827783015642464, 0.20259798106785998, 0.2337489300339528, 0.1995085629275789, 0.23000732022010184, 0.20258936164228614, 0.3583455868837565, 0.5974983189015708, 0.6315004735669381, 0.48375114306220435, 0.5228073851654269, 0.6765804181616237, 0.44806218567436396, 0.4405846418933689, 0.2231537031442834, 0.15177066878967904, 0.19069875851639195, 0.8538083672200958, 0.8489568076127689, 0.8369418101551436, 0.7959958487406582, 0.1809055777063855, 0.19441884874194382, 0.6735733685324677, 0.653629308683124, 0.16602485673297018, 0.20335804099962917, 0.20133806574003343, 0.20063819684592765, 0.4267483879660453, 0.203500441501264, 0.12468842409948466, 0.7601496747285006, 0.21347654253114523, 0.20557946271286787, 0.20946166959342294, 0.2037521178560513, 0.19899119990835357, 0.20502188897743512, 0.21185012528730773, 0.23029761548989025, 0.2226957927771256, 0.10106476573254897, 0.08282313409598063, 0.0927463241901183, 0.0892673049309205, 0.09187005772305634, 0.11808457473018663, 0.09136297465966581, 0.09200662746312827, 0.09667808533660249]}, "mutation_prompt": null}
{"id": "b2d3a186-17a4-43b9-adb0-e93fc1ab23c4", "solution": "import numpy as np\n\nclass HybridStochasticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.initial_scale_factor = 0.8\n        self.initial_crossover_prob = 0.7\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population = self.opposition_based_initialization()\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.learning_rate = 0.1\n\n    def opposition_based_initialization(self):\n        initial_population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        opposition_population = self.lower_bound + self.upper_bound - initial_population\n        return np.vstack((initial_population, opposition_population))[:self.population_size]\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def select_parents(self, idx):\n        indices = [i for i in range(self.population_size) if i != idx]\n        return np.random.choice(indices, 3, replace=False)\n\n    def differential_evolution_mutation(self, idx, scale_factor):\n        a, b, c = self.select_parents(idx)\n        mutant_vector = self.population[a] + scale_factor * (self.population[b] - self.population[c])\n        return np.clip(mutant_vector, self.lower_bound, self.upper_bound)\n\n    def crossover(self, target_vector, mutant_vector, crossover_prob):\n        crossover_mask = np.random.rand(self.dim) < crossover_prob\n        return np.where(crossover_mask, mutant_vector, target_vector)\n\n    def local_search(self, vector):\n        perturbation = np.random.normal(0, self.learning_rate, self.dim)\n        candidate_vector = vector + perturbation\n        return np.clip(candidate_vector, self.lower_bound, self.upper_bound)\n\n    def __call__(self, func):\n        self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                # Dynamic adjustment of parameters\n                scale_factor = self.initial_scale_factor * (1 - self.evaluations / self.budget)\n                crossover_prob = self.initial_crossover_prob + 0.2 * np.random.rand()\n\n                mutant_vector = self.differential_evolution_mutation(i, scale_factor)\n                trial_vector = self.crossover(self.population[i], mutant_vector, crossover_prob)\n                trial_fitness = func(trial_vector)\n                self.evaluations += 1\n\n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial_vector\n                    self.fitness[i] = trial_fitness\n\n                if np.random.rand() < 0.2:\n                    candidate_vector = self.local_search(self.population[i])\n                    candidate_fitness = func(candidate_vector)\n                    self.evaluations += 1\n                    if candidate_fitness < self.fitness[i]:\n                        self.population[i] = candidate_vector\n                        self.fitness[i] = candidate_fitness\n\n                # Update learning rate adaptively\n                self.learning_rate = max(0.01, self.learning_rate * (1 - 0.05 * np.random.rand()))\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx]", "name": "HybridStochasticOptimizer", "description": "Hybrid optimizer with adaptive learning rate and opposition-based initialization for faster convergence.", "configspace": "", "generation": 32, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "c9aafe6b-332b-4283-9cc1-4b2e50c44e27", "metadata": {"aucs": [0.8614318631710612, 0.8615612395806681, 0.8648092636329068, 0.8836823589578061, 0.8809302815417128, 0.8537149049105373, 0.8649927264859298, 0.8661719092889808, 0.8732297035052712, 0.7784313840335078, 0.7934709027220181, 0.7709929153086685, 0.7666377215322294, 0.7792354193630129, 0.7955283567004858, 0.7874175449423783, 0.7934505999095464, 0.7963508686605465, 0.25196533963541234, 0.603323462921044, 0.31189512717187695, 0.45254508955474515, 0.3855834591332875, 0.6324217482611867, 0.38087210189123466, 0.37101210401952667, 0.37832843410229366, 0.3374576510463305, 0.1599578735010866, 0.25181667196560353, 0.15598715927116646, 0.15926218502651568, 0.14742944278799264, 0.33123888835847937, 0.3066524419836607, 0.16048855283955032, 0.9738501285938751, 0.9538242295949432, 0.9811516406082638, 0.9649564447290745, 0.9726531518905077, 0.9814481843774895, 0.9732999615150277, 0.9721568744354393, 0.9772487896256388, 0.5602477359628759, 0.5166699154516793, 0.49955145811133295, 0.4607762689339385, 0.5168840480364095, 0.5837268635955544, 0.5848178673193583, 0.44902566155463985, 0.6535189332618503, 0.8108564290388902, 0.7472995471422044, 0.7832292739258102, 0.780298104186407, 0.7997326258786164, 0.8307049265401217, 0.7670254946306878, 0.7966436314344594, 0.7737581975614084, 0.26793793612173356, 0.24955593945589905, 0.19834760026021148, 0.22530649729727037, 0.21576791524353967, 0.22803418538233733, 0.23532474250416835, 0.20160318462124072, 0.1920099262564383, 0.19613605298744063, 0.23556007360063813, 0.11440376677945807, 0.22544642163615658, 0.24862040742714708, 0.1874776391649946, 0.19655907028592323, 0.19284064583165672, 0.19577311722397894, 0.3108121261595491, 0.2566448530161354, 0.36431074059207347, 0.24025452278021453, 0.4802140919950957, 0.0007248673300417341, 0.35401012029935763, 0.3692632979788284, 0.276018593628536, 0.3623824055587753, 0.3924469974919602, 0.23556071143414548, 0.35272611497922335, 0.44565667864537895, 0.39468163509167276, 0.4061903350584134, 0.36215287040133537, 0.3973933722095969, 0.11732286633565558, 0.20172957542003978, 0.1498188877382276, 0.24772296138908012, 0.1546062088760296, 0.10282719367721671, 0.20924019897046064, 0.1183165114034116, 0.20632727477440505, 0.2809986328144657, 0.29684536408175155, 0.3175185646986213, 0.3737401167683222, 0.33517695108836454, 0.3271148862228106, 0.36693310045283345, 0.3166293273038533, 0.24146562201228117, 0.6055800258234042, 0.7234496228479592, 0.6438958422770305, 0.7507872332161957, 0.7324506280759618, 0.6589215501985713, 0.7496261142206224, 0.7209822579527148, 0.6855782937737074, 0.1371651233838116, 0.12075132229211483, 0.13780801853641733, 0.12584800341409697, 0.13850225460242616, 0.11675557226216027, 0.12863432580056944, 0.1273098808292441, 0.14268148704984884, 0.19934365172377466, 0.18594466654988895, 0.32832255345101913, 0.23469770515038102, 0.14866333116079655, 0.20267106519038025, 0.20152641892425394, 0.23576091839252133, 0.21421383347767264, 0.5391071467719948, 0.5042943805711715, 0.4612246809606788, 0.47676447358121665, 0.5291640446387131, 0.602638537448555, 0.6246427601161761, 0.5714078963023623, 0.6225968922190244, 0.33304162231889356, 0.3304783841194173, 0.4141875902408757, 0.354629285337982, 0.33990446568261656, 0.27919480653858797, 0.41214607585167573, 0.4488101082230267, 0.4070679191988382, 0.2075439556387948, 0.21303390663090027, 0.21635748499617935, 0.19827783015642464, 0.20259798106785998, 0.2337489300339528, 0.1995085629275789, 0.23000732022010184, 0.20258936164228614, 0.3583455868837565, 0.5974983189015708, 0.6315004735669381, 0.48375114306220435, 0.5228073851654269, 0.6765804181616237, 0.44806218567436396, 0.4405846418933689, 0.2231537031442834, 0.15177066878967904, 0.19069875851639195, 0.8538083672200958, 0.8489568076127689, 0.8369418101551436, 0.7959958487406582, 0.1809055777063855, 0.19441884874194382, 0.6735733685324677, 0.653629308683124, 0.16602485673297018, 0.20335804099962917, 0.20133806574003343, 0.20063819684592765, 0.4267483879660453, 0.203500441501264, 0.12468842409948466, 0.7601496747285006, 0.21347654253114523, 0.20557946271286787, 0.20946166959342294, 0.2037521178560513, 0.19899119990835357, 0.20502188897743512, 0.21185012528730773, 0.23029761548989025, 0.2226957927771256, 0.10106476573254897, 0.08282313409598063, 0.0927463241901183, 0.0892673049309205, 0.09187005772305634, 0.11808457473018663, 0.09136297465966581, 0.09200662746312827, 0.09667808533660249]}, "mutation_prompt": null}
{"id": "fcad6aab-ac8f-4889-aeed-d4a558b7d8d3", "solution": "import numpy as np\n\nclass HybridStochasticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.initial_scale_factor = 0.8\n        self.initial_crossover_prob = 0.7\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population = self.opposition_based_initialization()\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.learning_rate = 0.1\n\n    def opposition_based_initialization(self):\n        initial_population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        opposition_population = self.lower_bound + self.upper_bound - initial_population\n        return np.vstack((initial_population, opposition_population))[:self.population_size]\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def select_parents(self, idx):\n        indices = [i for i in range(self.population_size) if i != idx]\n        return np.random.choice(indices, 3, replace=False)\n\n    def differential_evolution_mutation(self, idx, scale_factor):\n        a, b, c = self.select_parents(idx)\n        mutant_vector = self.population[a] + scale_factor * (self.population[b] - self.population[c])\n        return np.clip(mutant_vector, self.lower_bound, self.upper_bound)\n\n    def crossover(self, target_vector, mutant_vector, crossover_prob):\n        crossover_mask = np.random.rand(self.dim) < crossover_prob\n        return np.where(crossover_mask, mutant_vector, target_vector)\n\n    def local_search(self, vector):\n        perturbation = np.random.normal(0, self.learning_rate, self.dim)\n        candidate_vector = vector + perturbation\n        return np.clip(candidate_vector, self.lower_bound, self.upper_bound)\n\n    def __call__(self, func):\n        self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                # Dynamic adjustment of parameters\n                scale_factor = self.initial_scale_factor * (1 - self.evaluations / self.budget)\n                crossover_prob = self.initial_crossover_prob + 0.2 * np.random.rand()\n\n                mutant_vector = self.differential_evolution_mutation(i, scale_factor)\n                trial_vector = self.crossover(self.population[i], mutant_vector, crossover_prob)\n                trial_fitness = func(trial_vector)\n                self.evaluations += 1\n\n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial_vector\n                    self.fitness[i] = trial_fitness\n\n                if np.random.rand() < 0.2:\n                    candidate_vector = self.local_search(self.population[i])\n                    candidate_fitness = func(candidate_vector)\n                    self.evaluations += 1\n                    if candidate_fitness < self.fitness[i]:\n                        self.population[i] = candidate_vector\n                        self.fitness[i] = candidate_fitness\n\n                # Update learning rate adaptively\n                self.learning_rate = max(0.01, self.learning_rate * (1 - 0.05 * np.random.rand()))\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx]", "name": "HybridStochasticOptimizer", "description": "Hybrid optimizer with adaptive learning rate and opposition-based initialization for faster convergence.", "configspace": "", "generation": 32, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "c9aafe6b-332b-4283-9cc1-4b2e50c44e27", "metadata": {"aucs": [0.8614318631710612, 0.8615612395806681, 0.8648092636329068, 0.8836823589578061, 0.8809302815417128, 0.8537149049105373, 0.8649927264859298, 0.8661719092889808, 0.8732297035052712, 0.7784313840335078, 0.7934709027220181, 0.7709929153086685, 0.7666377215322294, 0.7792354193630129, 0.7955283567004858, 0.7874175449423783, 0.7934505999095464, 0.7963508686605465, 0.25196533963541234, 0.603323462921044, 0.31189512717187695, 0.45254508955474515, 0.3855834591332875, 0.6324217482611867, 0.38087210189123466, 0.37101210401952667, 0.37832843410229366, 0.3374576510463305, 0.1599578735010866, 0.25181667196560353, 0.15598715927116646, 0.15926218502651568, 0.14742944278799264, 0.33123888835847937, 0.3066524419836607, 0.16048855283955032, 0.9738501285938751, 0.9538242295949432, 0.9811516406082638, 0.9649564447290745, 0.9726531518905077, 0.9814481843774895, 0.9732999615150277, 0.9721568744354393, 0.9772487896256388, 0.5602477359628759, 0.5166699154516793, 0.49955145811133295, 0.4607762689339385, 0.5168840480364095, 0.5837268635955544, 0.5848178673193583, 0.44902566155463985, 0.6535189332618503, 0.8108564290388902, 0.7472995471422044, 0.7832292739258102, 0.780298104186407, 0.7997326258786164, 0.8307049265401217, 0.7670254946306878, 0.7966436314344594, 0.7737581975614084, 0.26793793612173356, 0.24955593945589905, 0.19834760026021148, 0.22530649729727037, 0.21576791524353967, 0.22803418538233733, 0.23532474250416835, 0.20160318462124072, 0.1920099262564383, 0.19613605298744063, 0.23556007360063813, 0.11440376677945807, 0.22544642163615658, 0.24862040742714708, 0.1874776391649946, 0.19655907028592323, 0.19284064583165672, 0.19577311722397894, 0.3108121261595491, 0.2566448530161354, 0.36431074059207347, 0.24025452278021453, 0.4802140919950957, 0.0007248673300417341, 0.35401012029935763, 0.3692632979788284, 0.276018593628536, 0.3623824055587753, 0.3924469974919602, 0.23556071143414548, 0.35272611497922335, 0.44565667864537895, 0.39468163509167276, 0.4061903350584134, 0.36215287040133537, 0.3973933722095969, 0.11732286633565558, 0.20172957542003978, 0.1498188877382276, 0.24772296138908012, 0.1546062088760296, 0.10282719367721671, 0.20924019897046064, 0.1183165114034116, 0.20632727477440505, 0.2809986328144657, 0.29684536408175155, 0.3175185646986213, 0.3737401167683222, 0.33517695108836454, 0.3271148862228106, 0.36693310045283345, 0.3166293273038533, 0.24146562201228117, 0.6055800258234042, 0.7234496228479592, 0.6438958422770305, 0.7507872332161957, 0.7324506280759618, 0.6589215501985713, 0.7496261142206224, 0.7209822579527148, 0.6855782937737074, 0.1371651233838116, 0.12075132229211483, 0.13780801853641733, 0.12584800341409697, 0.13850225460242616, 0.11675557226216027, 0.12863432580056944, 0.1273098808292441, 0.14268148704984884, 0.19934365172377466, 0.18594466654988895, 0.32832255345101913, 0.23469770515038102, 0.14866333116079655, 0.20267106519038025, 0.20152641892425394, 0.23576091839252133, 0.21421383347767264, 0.5391071467719948, 0.5042943805711715, 0.4612246809606788, 0.47676447358121665, 0.5291640446387131, 0.602638537448555, 0.6246427601161761, 0.5714078963023623, 0.6225968922190244, 0.33304162231889356, 0.3304783841194173, 0.4141875902408757, 0.354629285337982, 0.33990446568261656, 0.27919480653858797, 0.41214607585167573, 0.4488101082230267, 0.4070679191988382, 0.2075439556387948, 0.21303390663090027, 0.21635748499617935, 0.19827783015642464, 0.20259798106785998, 0.2337489300339528, 0.1995085629275789, 0.23000732022010184, 0.20258936164228614, 0.3583455868837565, 0.5974983189015708, 0.6315004735669381, 0.48375114306220435, 0.5228073851654269, 0.6765804181616237, 0.44806218567436396, 0.4405846418933689, 0.2231537031442834, 0.15177066878967904, 0.19069875851639195, 0.8538083672200958, 0.8489568076127689, 0.8369418101551436, 0.7959958487406582, 0.1809055777063855, 0.19441884874194382, 0.6735733685324677, 0.653629308683124, 0.16602485673297018, 0.20335804099962917, 0.20133806574003343, 0.20063819684592765, 0.4267483879660453, 0.203500441501264, 0.12468842409948466, 0.7601496747285006, 0.21347654253114523, 0.20557946271286787, 0.20946166959342294, 0.2037521178560513, 0.19899119990835357, 0.20502188897743512, 0.21185012528730773, 0.23029761548989025, 0.2226957927771256, 0.10106476573254897, 0.08282313409598063, 0.0927463241901183, 0.0892673049309205, 0.09187005772305634, 0.11808457473018663, 0.09136297465966581, 0.09200662746312827, 0.09667808533660249]}, "mutation_prompt": null}
{"id": "3c502fa8-a794-47c0-8ef1-b5d98ed8007c", "solution": "import numpy as np\n\nclass HybridStochasticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.initial_scale_factor = 0.8\n        self.initial_crossover_prob = 0.7\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population = self.opposition_based_initialization()\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.learning_rate = 0.1\n\n    def opposition_based_initialization(self):\n        initial_population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        opposition_population = self.lower_bound + self.upper_bound - initial_population\n        return np.vstack((initial_population, opposition_population))[:self.population_size]\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def select_parents(self, idx):\n        indices = [i for i in range(self.population_size) if i != idx]\n        return np.random.choice(indices, 3, replace=False)\n\n    def differential_evolution_mutation(self, idx, scale_factor):\n        a, b, c = self.select_parents(idx)\n        mutant_vector = self.population[a] + scale_factor * (self.population[b] - self.population[c])\n        return np.clip(mutant_vector, self.lower_bound, self.upper_bound)\n\n    def crossover(self, target_vector, mutant_vector, crossover_prob):\n        crossover_mask = np.random.rand(self.dim) < crossover_prob\n        return np.where(crossover_mask, mutant_vector, target_vector)\n\n    def local_search(self, vector):\n        perturbation = np.random.normal(0, self.learning_rate, self.dim)\n        candidate_vector = vector + perturbation\n        return np.clip(candidate_vector, self.lower_bound, self.upper_bound)\n\n    def __call__(self, func):\n        self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                # Dynamic adjustment of parameters\n                scale_factor = self.initial_scale_factor * (1 - self.evaluations / self.budget)\n                crossover_prob = self.initial_crossover_prob + 0.2 * np.random.rand()\n\n                mutant_vector = self.differential_evolution_mutation(i, scale_factor)\n                trial_vector = self.crossover(self.population[i], mutant_vector, crossover_prob)\n                trial_fitness = func(trial_vector)\n                self.evaluations += 1\n\n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial_vector\n                    self.fitness[i] = trial_fitness\n\n                if np.random.rand() < 0.2:\n                    candidate_vector = self.local_search(self.population[i])\n                    candidate_fitness = func(candidate_vector)\n                    self.evaluations += 1\n                    if candidate_fitness < self.fitness[i]:\n                        self.population[i] = candidate_vector\n                        self.fitness[i] = candidate_fitness\n\n                # Update learning rate adaptively\n                self.learning_rate = max(0.01, self.learning_rate * (1 - 0.05 * np.random.rand()))\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx]", "name": "HybridStochasticOptimizer", "description": "Hybrid optimizer with adaptive learning rate and opposition-based initialization for faster convergence.", "configspace": "", "generation": 32, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "c9aafe6b-332b-4283-9cc1-4b2e50c44e27", "metadata": {"aucs": [0.8614318631710612, 0.8615612395806681, 0.8648092636329068, 0.8836823589578061, 0.8809302815417128, 0.8537149049105373, 0.8649927264859298, 0.8661719092889808, 0.8732297035052712, 0.7784313840335078, 0.7934709027220181, 0.7709929153086685, 0.7666377215322294, 0.7792354193630129, 0.7955283567004858, 0.7874175449423783, 0.7934505999095464, 0.7963508686605465, 0.25196533963541234, 0.603323462921044, 0.31189512717187695, 0.45254508955474515, 0.3855834591332875, 0.6324217482611867, 0.38087210189123466, 0.37101210401952667, 0.37832843410229366, 0.3374576510463305, 0.1599578735010866, 0.25181667196560353, 0.15598715927116646, 0.15926218502651568, 0.14742944278799264, 0.33123888835847937, 0.3066524419836607, 0.16048855283955032, 0.9738501285938751, 0.9538242295949432, 0.9811516406082638, 0.9649564447290745, 0.9726531518905077, 0.9814481843774895, 0.9732999615150277, 0.9721568744354393, 0.9772487896256388, 0.5602477359628759, 0.5166699154516793, 0.49955145811133295, 0.4607762689339385, 0.5168840480364095, 0.5837268635955544, 0.5848178673193583, 0.44902566155463985, 0.6535189332618503, 0.8108564290388902, 0.7472995471422044, 0.7832292739258102, 0.780298104186407, 0.7997326258786164, 0.8307049265401217, 0.7670254946306878, 0.7966436314344594, 0.7737581975614084, 0.26793793612173356, 0.24955593945589905, 0.19834760026021148, 0.22530649729727037, 0.21576791524353967, 0.22803418538233733, 0.23532474250416835, 0.20160318462124072, 0.1920099262564383, 0.19613605298744063, 0.23556007360063813, 0.11440376677945807, 0.22544642163615658, 0.24862040742714708, 0.1874776391649946, 0.19655907028592323, 0.19284064583165672, 0.19577311722397894, 0.3108121261595491, 0.2566448530161354, 0.36431074059207347, 0.24025452278021453, 0.4802140919950957, 0.0007248673300417341, 0.35401012029935763, 0.3692632979788284, 0.276018593628536, 0.3623824055587753, 0.3924469974919602, 0.23556071143414548, 0.35272611497922335, 0.44565667864537895, 0.39468163509167276, 0.4061903350584134, 0.36215287040133537, 0.3973933722095969, 0.11732286633565558, 0.20172957542003978, 0.1498188877382276, 0.24772296138908012, 0.1546062088760296, 0.10282719367721671, 0.20924019897046064, 0.1183165114034116, 0.20632727477440505, 0.2809986328144657, 0.29684536408175155, 0.3175185646986213, 0.3737401167683222, 0.33517695108836454, 0.3271148862228106, 0.36693310045283345, 0.3166293273038533, 0.24146562201228117, 0.6055800258234042, 0.7234496228479592, 0.6438958422770305, 0.7507872332161957, 0.7324506280759618, 0.6589215501985713, 0.7496261142206224, 0.7209822579527148, 0.6855782937737074, 0.1371651233838116, 0.12075132229211483, 0.13780801853641733, 0.12584800341409697, 0.13850225460242616, 0.11675557226216027, 0.12863432580056944, 0.1273098808292441, 0.14268148704984884, 0.19934365172377466, 0.18594466654988895, 0.32832255345101913, 0.23469770515038102, 0.14866333116079655, 0.20267106519038025, 0.20152641892425394, 0.23576091839252133, 0.21421383347767264, 0.5391071467719948, 0.5042943805711715, 0.4612246809606788, 0.47676447358121665, 0.5291640446387131, 0.602638537448555, 0.6246427601161761, 0.5714078963023623, 0.6225968922190244, 0.33304162231889356, 0.3304783841194173, 0.4141875902408757, 0.354629285337982, 0.33990446568261656, 0.27919480653858797, 0.41214607585167573, 0.4488101082230267, 0.4070679191988382, 0.2075439556387948, 0.21303390663090027, 0.21635748499617935, 0.19827783015642464, 0.20259798106785998, 0.2337489300339528, 0.1995085629275789, 0.23000732022010184, 0.20258936164228614, 0.3583455868837565, 0.5974983189015708, 0.6315004735669381, 0.48375114306220435, 0.5228073851654269, 0.6765804181616237, 0.44806218567436396, 0.4405846418933689, 0.2231537031442834, 0.15177066878967904, 0.19069875851639195, 0.8538083672200958, 0.8489568076127689, 0.8369418101551436, 0.7959958487406582, 0.1809055777063855, 0.19441884874194382, 0.6735733685324677, 0.653629308683124, 0.16602485673297018, 0.20335804099962917, 0.20133806574003343, 0.20063819684592765, 0.4267483879660453, 0.203500441501264, 0.12468842409948466, 0.7601496747285006, 0.21347654253114523, 0.20557946271286787, 0.20946166959342294, 0.2037521178560513, 0.19899119990835357, 0.20502188897743512, 0.21185012528730773, 0.23029761548989025, 0.2226957927771256, 0.10106476573254897, 0.08282313409598063, 0.0927463241901183, 0.0892673049309205, 0.09187005772305634, 0.11808457473018663, 0.09136297465966581, 0.09200662746312827, 0.09667808533660249]}, "mutation_prompt": null}
{"id": "6e510982-d190-4cfe-b5af-eb109564b14c", "solution": "import numpy as np\n\nclass HybridStochasticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.initial_scale_factor = 0.8\n        self.initial_crossover_prob = 0.7\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population = self.opposition_based_initialization()\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.learning_rate = 0.1\n\n    def opposition_based_initialization(self):\n        initial_population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        opposition_population = self.lower_bound + self.upper_bound - initial_population\n        return np.vstack((initial_population, opposition_population))[:self.population_size]\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def select_parents(self, idx):\n        indices = [i for i in range(self.population_size) if i != idx]\n        return np.random.choice(indices, 3, replace=False)\n\n    def differential_evolution_mutation(self, idx, scale_factor):\n        a, b, c = self.select_parents(idx)\n        mutant_vector = self.population[a] + scale_factor * (self.population[b] - self.population[c])\n        return np.clip(mutant_vector, self.lower_bound, self.upper_bound)\n\n    def crossover(self, target_vector, mutant_vector, crossover_prob):\n        crossover_mask = np.random.rand(self.dim) < crossover_prob\n        return np.where(crossover_mask, mutant_vector, target_vector)\n\n    def local_search(self, vector):\n        perturbation = np.random.normal(0, self.learning_rate, self.dim)\n        candidate_vector = vector + perturbation\n        return np.clip(candidate_vector, self.lower_bound, self.upper_bound)\n\n    def __call__(self, func):\n        self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                # Dynamic adjustment of parameters\n                scale_factor = self.initial_scale_factor * (1 - self.evaluations / self.budget)\n                crossover_prob = self.initial_crossover_prob + 0.2 * np.random.rand()\n\n                mutant_vector = self.differential_evolution_mutation(i, scale_factor)\n                trial_vector = self.crossover(self.population[i], mutant_vector, crossover_prob)\n                trial_fitness = func(trial_vector)\n                self.evaluations += 1\n\n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial_vector\n                    self.fitness[i] = trial_fitness\n\n                if np.random.rand() < 0.2:\n                    candidate_vector = self.local_search(self.population[i])\n                    candidate_fitness = func(candidate_vector)\n                    self.evaluations += 1\n                    if candidate_fitness < self.fitness[i]:\n                        self.population[i] = candidate_vector\n                        self.fitness[i] = candidate_fitness\n\n                # Update learning rate adaptively\n                self.learning_rate = max(0.01, self.learning_rate * (1 - 0.05 * np.random.rand()))\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx]", "name": "HybridStochasticOptimizer", "description": "Hybrid optimizer with adaptive learning rate and opposition-based initialization for faster convergence.", "configspace": "", "generation": 32, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "c9aafe6b-332b-4283-9cc1-4b2e50c44e27", "metadata": {"aucs": [0.8614318631710612, 0.8615612395806681, 0.8648092636329068, 0.8836823589578061, 0.8809302815417128, 0.8537149049105373, 0.8649927264859298, 0.8661719092889808, 0.8732297035052712, 0.7784313840335078, 0.7934709027220181, 0.7709929153086685, 0.7666377215322294, 0.7792354193630129, 0.7955283567004858, 0.7874175449423783, 0.7934505999095464, 0.7963508686605465, 0.25196533963541234, 0.603323462921044, 0.31189512717187695, 0.45254508955474515, 0.3855834591332875, 0.6324217482611867, 0.38087210189123466, 0.37101210401952667, 0.37832843410229366, 0.3374576510463305, 0.1599578735010866, 0.25181667196560353, 0.15598715927116646, 0.15926218502651568, 0.14742944278799264, 0.33123888835847937, 0.3066524419836607, 0.16048855283955032, 0.9738501285938751, 0.9538242295949432, 0.9811516406082638, 0.9649564447290745, 0.9726531518905077, 0.9814481843774895, 0.9732999615150277, 0.9721568744354393, 0.9772487896256388, 0.5602477359628759, 0.5166699154516793, 0.49955145811133295, 0.4607762689339385, 0.5168840480364095, 0.5837268635955544, 0.5848178673193583, 0.44902566155463985, 0.6535189332618503, 0.8108564290388902, 0.7472995471422044, 0.7832292739258102, 0.780298104186407, 0.7997326258786164, 0.8307049265401217, 0.7670254946306878, 0.7966436314344594, 0.7737581975614084, 0.26793793612173356, 0.24955593945589905, 0.19834760026021148, 0.22530649729727037, 0.21576791524353967, 0.22803418538233733, 0.23532474250416835, 0.20160318462124072, 0.1920099262564383, 0.19613605298744063, 0.23556007360063813, 0.11440376677945807, 0.22544642163615658, 0.24862040742714708, 0.1874776391649946, 0.19655907028592323, 0.19284064583165672, 0.19577311722397894, 0.3108121261595491, 0.2566448530161354, 0.36431074059207347, 0.24025452278021453, 0.4802140919950957, 0.0007248673300417341, 0.35401012029935763, 0.3692632979788284, 0.276018593628536, 0.3623824055587753, 0.3924469974919602, 0.23556071143414548, 0.35272611497922335, 0.44565667864537895, 0.39468163509167276, 0.4061903350584134, 0.36215287040133537, 0.3973933722095969, 0.11732286633565558, 0.20172957542003978, 0.1498188877382276, 0.24772296138908012, 0.1546062088760296, 0.10282719367721671, 0.20924019897046064, 0.1183165114034116, 0.20632727477440505, 0.2809986328144657, 0.29684536408175155, 0.3175185646986213, 0.3737401167683222, 0.33517695108836454, 0.3271148862228106, 0.36693310045283345, 0.3166293273038533, 0.24146562201228117, 0.6055800258234042, 0.7234496228479592, 0.6438958422770305, 0.7507872332161957, 0.7324506280759618, 0.6589215501985713, 0.7496261142206224, 0.7209822579527148, 0.6855782937737074, 0.1371651233838116, 0.12075132229211483, 0.13780801853641733, 0.12584800341409697, 0.13850225460242616, 0.11675557226216027, 0.12863432580056944, 0.1273098808292441, 0.14268148704984884, 0.19934365172377466, 0.18594466654988895, 0.32832255345101913, 0.23469770515038102, 0.14866333116079655, 0.20267106519038025, 0.20152641892425394, 0.23576091839252133, 0.21421383347767264, 0.5391071467719948, 0.5042943805711715, 0.4612246809606788, 0.47676447358121665, 0.5291640446387131, 0.602638537448555, 0.6246427601161761, 0.5714078963023623, 0.6225968922190244, 0.33304162231889356, 0.3304783841194173, 0.4141875902408757, 0.354629285337982, 0.33990446568261656, 0.27919480653858797, 0.41214607585167573, 0.4488101082230267, 0.4070679191988382, 0.2075439556387948, 0.21303390663090027, 0.21635748499617935, 0.19827783015642464, 0.20259798106785998, 0.2337489300339528, 0.1995085629275789, 0.23000732022010184, 0.20258936164228614, 0.3583455868837565, 0.5974983189015708, 0.6315004735669381, 0.48375114306220435, 0.5228073851654269, 0.6765804181616237, 0.44806218567436396, 0.4405846418933689, 0.2231537031442834, 0.15177066878967904, 0.19069875851639195, 0.8538083672200958, 0.8489568076127689, 0.8369418101551436, 0.7959958487406582, 0.1809055777063855, 0.19441884874194382, 0.6735733685324677, 0.653629308683124, 0.16602485673297018, 0.20335804099962917, 0.20133806574003343, 0.20063819684592765, 0.4267483879660453, 0.203500441501264, 0.12468842409948466, 0.7601496747285006, 0.21347654253114523, 0.20557946271286787, 0.20946166959342294, 0.2037521178560513, 0.19899119990835357, 0.20502188897743512, 0.21185012528730773, 0.23029761548989025, 0.2226957927771256, 0.10106476573254897, 0.08282313409598063, 0.0927463241901183, 0.0892673049309205, 0.09187005772305634, 0.11808457473018663, 0.09136297465966581, 0.09200662746312827, 0.09667808533660249]}, "mutation_prompt": null}
{"id": "7d7cff7a-a5f5-4115-b7fd-8a6201289b3a", "solution": "import numpy as np\n\nclass EnhancedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.initial_scale_factor = 0.9  # Slightly increased for faster exploration\n        self.initial_crossover_prob = 0.8  # Slightly increased to maintain diversity\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.opposition_rate = 0.1  # New parameter for dynamic opposition\n        self.population = self.opposition_based_initialization()\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.learning_rate = 0.1\n\n    def opposition_based_initialization(self):\n        initial_population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        opposition_population = self.lower_bound + self.upper_bound - initial_population\n        return self.multi_level_opposition(initial_population, opposition_population)\n    \n    def multi_level_opposition(self, population, opposition_population):\n        combined_population = np.vstack((population, opposition_population))\n        multi_opposition_population = self.lower_bound + self.upper_bound - combined_population\n        all_population = np.vstack((combined_population, multi_opposition_population))\n        return all_population[:self.population_size]\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def select_parents(self, idx):\n        indices = [i for i in range(self.population_size) if i != idx]\n        return np.random.choice(indices, 3, replace=False)\n\n    def differential_evolution_mutation(self, idx, scale_factor):\n        a, b, c = self.select_parents(idx)\n        mutant_vector = self.population[a] + scale_factor * (self.population[b] - self.population[c])\n        return np.clip(mutant_vector, self.lower_bound, self.upper_bound)\n\n    def crossover(self, target_vector, mutant_vector, crossover_prob):\n        crossover_mask = np.random.rand(self.dim) < crossover_prob\n        return np.where(crossover_mask, mutant_vector, target_vector)\n\n    def local_search(self, vector):\n        perturbation = np.random.normal(0, self.learning_rate, self.dim)\n        candidate_vector = vector + perturbation\n        return np.clip(candidate_vector, self.lower_bound, self.upper_bound)\n\n    def __call__(self, func):\n        self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                # Dynamic adjustment of parameters\n                scale_factor = self.initial_scale_factor * np.random.uniform(0.8, 1.2)  # Added randomness\n                crossover_prob = self.initial_crossover_prob * np.random.uniform(0.8, 1.2) \n\n                mutant_vector = self.differential_evolution_mutation(i, scale_factor)\n                trial_vector = self.crossover(self.population[i], mutant_vector, crossover_prob)\n                trial_fitness = func(trial_vector)\n                self.evaluations += 1\n\n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial_vector\n                    self.fitness[i] = trial_fitness\n\n                if np.random.rand() < self.opposition_rate:  # Conditional opposition-based learning\n                    candidate_vector = self.opposition_based_local_search(self.population[i])\n                    candidate_fitness = func(candidate_vector)\n                    self.evaluations += 1\n                    if candidate_fitness < self.fitness[i]:\n                        self.population[i] = candidate_vector\n                        self.fitness[i] = candidate_fitness\n\n                # Update learning rate adaptively\n                self.learning_rate = max(0.01, self.learning_rate * (1 - 0.05 * np.random.rand()))\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx]\n\n    def opposition_based_local_search(self, vector):\n        opposite_vector = self.lower_bound + self.upper_bound - vector\n        perturbation = np.random.normal(0, self.learning_rate, self.dim)\n        candidate_vector = opposite_vector + perturbation\n        return np.clip(candidate_vector, self.lower_bound, self.upper_bound)", "name": "EnhancedHybridOptimizer", "description": "Enhanced hybrid optimizer using adaptive strategy parameters and multi-level opposition-based learning for improved convergence.", "configspace": "", "generation": 62, "fitness": 0.36400894283367796, "feedback": "The algorithm EnhancedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.36 with standard deviation 0.25.", "error": "", "parent_id": "c9aafe6b-332b-4283-9cc1-4b2e50c44e27", "metadata": {"aucs": [0.8314365729437193, 0.7830018328327985, 0.8153376360446659, 0.8414332789866635, 0.8141140328148296, 0.829470091889477, 0.8157456436772745, 0.8212995392429215, 0.8041723193400381, 0.6692226603400565, 0.678138149779298, 0.6854777202652904, 0.7010950165456984, 0.6941009891089969, 0.6919367095106255, 0.6774357088596968, 0.7008132311199664, 0.7445118650299252, 0.3329773733540269, 0.32519441049250075, 0.25100882458158613, 0.4719429698906461, 0.3019747109837073, 0.4361649005653808, 0.3748774897508169, 0.2087757972265949, 0.3187988473827963, 0.1954235801604094, 0.27486895532051026, 0.14036224440391365, 0.18369931490537705, 0.3845854293238674, 0.15491672125361, 0.2344301119443909, 0.15042902376955158, 0.15652667112927154, 0.9616394756386274, 0.9793136086730083, 0.9477871123486801, 0.9745517631869206, 0.9854489871550779, 0.9659798064808486, 0.963361200007848, 0.9749280141249386, 0.9835631888393392, 0.4805198791756953, 0.38246637419423357, 0.47203824619524604, 0.5199968814025009, 0.4942033559290744, 0.4367890052963922, 0.5238013074608072, 0.4839953253661412, 0.0872167169075958, 0.6592920770048454, 0.6619719221410261, 0.6866260580197736, 0.6193720017282842, 0.6797170129587516, 0.7339302716425375, 0.6572369908140119, 0.6345533862973648, 0.6490385648446417, 0.3249047704793847, 0.31684252780240474, 0.38505535362523857, 0.3861749245315286, 0.32781408204237605, 0.37888141590352475, 0.46067394727390143, 0.3619940475418967, 0.3517391407843534, 0.4049646302364208, 0.0222052922614403, 0.12046861601463277, 0.3759954899851162, 0.3300802674282953, 0.5345622843270881, 0.3244094594610213, 0.3304654595695563, 0.381048001934985, 0.20975319859606056, 0.15797844407259487, 0.19754534319254224, 0.18531525936141957, 0.14413493297853752, 0.240166669355804, 0.1461255797151867, 0.24795832369516346, 0.1799278480872052, 0.3885368053204832, 0.33528598571855905, 0.3387446571306264, 0.32998772682574495, 0.3173264953076257, 0.3019165355621152, 0.4053832746422831, 0.3850731235048026, 0.43025793247417043, 0.11886504551142696, 0.06650416432755135, 0.06692978580632536, 0.10572702351994157, 0.12329102774418277, 0.12983060450661632, 0.08912676497715799, 0.124056635034134, 0.0692839127814937, 0.2337838903584154, 0.20288069745581627, 0.22447777846352457, 0.26785414004176133, 0.26677962731139593, 0.2433857906295087, 0.22295732685092995, 0.24205139714848078, 0.21235021257492126, 0.6083159545302794, 0.5939690063267802, 0.6042264270916031, 0.6006870758331873, 0.6320254643539966, 0.626279772871507, 0.6263632913868628, 0.6100337154382629, 0.6222468288164422, 0.09606399364633944, 0.10969989392596002, 0.11277596489445674, 0.11303348465057883, 0.11060333313837345, 0.0789345126708706, 0.10097728045552268, 0.09797825388216808, 0.11634371305988844, 0.14133779693031534, 0.2333899177308123, 0.13961928137436697, 0.22940540957690214, 0.19011266592910792, 0.22123109987854472, 0.23697415499593688, 0.47372446482474595, 0.19384850913247065, 0.3408922768708108, 0.31797053110780316, 0.33492815159747336, 0.321842613244983, 0.339981402837219, 0.3304559088224117, 0.3991181940843097, 0.3698499003266462, 0.3553970597100192, 0.24762783554810353, 0.25945848035869656, 0.2487447169305912, 0.26039545571117784, 0.22287317466046153, 0.24512931381624814, 0.22300174697583985, 0.2568333564541987, 0.27078322810348165, 0.18205007369463622, 0.18555284065560695, 0.1804095294619774, 0.1845477639067804, 0.1757450568275446, 0.20433514761903104, 0.19181696979806406, 0.19349904072640634, 0.20253369764457752, 0.19538114792759553, 0.29234414292238264, 0.3618109280709785, 0.3003438032107336, 0.1959172066885725, 0.20532932442952956, 0.24328924225961368, 0.2045936678785102, 0.17906137040899905, 0.16504596740386135, 0.7655421375004946, 0.16854162271943718, 0.7555974170117421, 0.18904359647424573, 0.1816340667162203, 0.7951119511962781, 0.6953254258962658, 0.7065913349200186, 0.7254772377574901, 0.16538537149491328, 0.19880876384598078, 0.5512723134268693, 0.5777681826835493, 0.19321448236148886, 0.19540814253375616, 0.1500534618374304, 0.20861023883187724, 0.17672731325861113, 0.17945782407751398, 0.18815392437942247, 0.2097176029244341, 0.17842318284522352, 0.18689168932642297, 0.1856891458116382, 0.18646814569488102, 0.19824026197156008, 0.06451630772518946, 0.0673291382199217, 0.08237618217132314, 0.0689625129152368, 0.07087309571840184, 0.08292430219530145, 0.06646303776608453, 0.06865783823450289, 0.06671028319577121]}, "mutation_prompt": null}
{"id": "dd3562b2-08dc-4dd7-ba3d-3929d2feb184", "solution": "import numpy as np\n\nclass HybridStochasticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.initial_scale_factor = 0.8\n        self.initial_crossover_prob = 0.7\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population = self.opposition_based_initialization()\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.learning_rate = 0.1\n\n    def opposition_based_initialization(self):\n        initial_population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        opposition_population = self.lower_bound + self.upper_bound - initial_population\n        return np.vstack((initial_population, opposition_population))[:self.population_size]\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def select_parents(self, idx):\n        indices = [i for i in range(self.population_size) if i != idx]\n        return np.random.choice(indices, 3, replace=False)\n\n    def differential_evolution_mutation(self, idx, scale_factor):\n        a, b, c = self.select_parents(idx)\n        mutant_vector = self.population[a] + scale_factor * (self.population[b] - self.population[c])\n        return np.clip(mutant_vector, self.lower_bound, self.upper_bound)\n\n    def crossover(self, target_vector, mutant_vector, crossover_prob):\n        crossover_mask = np.random.rand(self.dim) < crossover_prob\n        return np.where(crossover_mask, mutant_vector, target_vector)\n\n    def local_search(self, vector):\n        perturbation = np.random.normal(0, self.learning_rate, self.dim)\n        candidate_vector = vector + perturbation\n        return np.clip(candidate_vector, self.lower_bound, self.upper_bound)\n\n    def __call__(self, func):\n        self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                # Dynamic adjustment of parameters\n                scale_factor = self.initial_scale_factor * (1 - self.evaluations / self.budget)\n                crossover_prob = self.initial_crossover_prob + 0.2 * np.random.rand()\n\n                mutant_vector = self.differential_evolution_mutation(i, scale_factor)\n                trial_vector = self.crossover(self.population[i], mutant_vector, crossover_prob)\n                trial_fitness = func(trial_vector)\n                self.evaluations += 1\n\n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial_vector\n                    self.fitness[i] = trial_fitness\n\n                if np.random.rand() < 0.2:\n                    candidate_vector = self.local_search(self.population[i])\n                    candidate_fitness = func(candidate_vector)\n                    self.evaluations += 1\n                    if candidate_fitness < self.fitness[i]:\n                        self.population[i] = candidate_vector\n                        self.fitness[i] = candidate_fitness\n\n                # Update learning rate adaptively\n                self.learning_rate = max(0.01, self.learning_rate * (1 - 0.05 * np.random.rand()))\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx]", "name": "HybridStochasticOptimizer", "description": "Hybrid optimizer with adaptive learning rate and opposition-based initialization for faster convergence.", "configspace": "", "generation": 32, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "c9aafe6b-332b-4283-9cc1-4b2e50c44e27", "metadata": {"aucs": [0.8614318631710612, 0.8615612395806681, 0.8648092636329068, 0.8836823589578061, 0.8809302815417128, 0.8537149049105373, 0.8649927264859298, 0.8661719092889808, 0.8732297035052712, 0.7784313840335078, 0.7934709027220181, 0.7709929153086685, 0.7666377215322294, 0.7792354193630129, 0.7955283567004858, 0.7874175449423783, 0.7934505999095464, 0.7963508686605465, 0.25196533963541234, 0.603323462921044, 0.31189512717187695, 0.45254508955474515, 0.3855834591332875, 0.6324217482611867, 0.38087210189123466, 0.37101210401952667, 0.37832843410229366, 0.3374576510463305, 0.1599578735010866, 0.25181667196560353, 0.15598715927116646, 0.15926218502651568, 0.14742944278799264, 0.33123888835847937, 0.3066524419836607, 0.16048855283955032, 0.9738501285938751, 0.9538242295949432, 0.9811516406082638, 0.9649564447290745, 0.9726531518905077, 0.9814481843774895, 0.9732999615150277, 0.9721568744354393, 0.9772487896256388, 0.5602477359628759, 0.5166699154516793, 0.49955145811133295, 0.4607762689339385, 0.5168840480364095, 0.5837268635955544, 0.5848178673193583, 0.44902566155463985, 0.6535189332618503, 0.8108564290388902, 0.7472995471422044, 0.7832292739258102, 0.780298104186407, 0.7997326258786164, 0.8307049265401217, 0.7670254946306878, 0.7966436314344594, 0.7737581975614084, 0.26793793612173356, 0.24955593945589905, 0.19834760026021148, 0.22530649729727037, 0.21576791524353967, 0.22803418538233733, 0.23532474250416835, 0.20160318462124072, 0.1920099262564383, 0.19613605298744063, 0.23556007360063813, 0.11440376677945807, 0.22544642163615658, 0.24862040742714708, 0.1874776391649946, 0.19655907028592323, 0.19284064583165672, 0.19577311722397894, 0.3108121261595491, 0.2566448530161354, 0.36431074059207347, 0.24025452278021453, 0.4802140919950957, 0.0007248673300417341, 0.35401012029935763, 0.3692632979788284, 0.276018593628536, 0.3623824055587753, 0.3924469974919602, 0.23556071143414548, 0.35272611497922335, 0.44565667864537895, 0.39468163509167276, 0.4061903350584134, 0.36215287040133537, 0.3973933722095969, 0.11732286633565558, 0.20172957542003978, 0.1498188877382276, 0.24772296138908012, 0.1546062088760296, 0.10282719367721671, 0.20924019897046064, 0.1183165114034116, 0.20632727477440505, 0.2809986328144657, 0.29684536408175155, 0.3175185646986213, 0.3737401167683222, 0.33517695108836454, 0.3271148862228106, 0.36693310045283345, 0.3166293273038533, 0.24146562201228117, 0.6055800258234042, 0.7234496228479592, 0.6438958422770305, 0.7507872332161957, 0.7324506280759618, 0.6589215501985713, 0.7496261142206224, 0.7209822579527148, 0.6855782937737074, 0.1371651233838116, 0.12075132229211483, 0.13780801853641733, 0.12584800341409697, 0.13850225460242616, 0.11675557226216027, 0.12863432580056944, 0.1273098808292441, 0.14268148704984884, 0.19934365172377466, 0.18594466654988895, 0.32832255345101913, 0.23469770515038102, 0.14866333116079655, 0.20267106519038025, 0.20152641892425394, 0.23576091839252133, 0.21421383347767264, 0.5391071467719948, 0.5042943805711715, 0.4612246809606788, 0.47676447358121665, 0.5291640446387131, 0.602638537448555, 0.6246427601161761, 0.5714078963023623, 0.6225968922190244, 0.33304162231889356, 0.3304783841194173, 0.4141875902408757, 0.354629285337982, 0.33990446568261656, 0.27919480653858797, 0.41214607585167573, 0.4488101082230267, 0.4070679191988382, 0.2075439556387948, 0.21303390663090027, 0.21635748499617935, 0.19827783015642464, 0.20259798106785998, 0.2337489300339528, 0.1995085629275789, 0.23000732022010184, 0.20258936164228614, 0.3583455868837565, 0.5974983189015708, 0.6315004735669381, 0.48375114306220435, 0.5228073851654269, 0.6765804181616237, 0.44806218567436396, 0.4405846418933689, 0.2231537031442834, 0.15177066878967904, 0.19069875851639195, 0.8538083672200958, 0.8489568076127689, 0.8369418101551436, 0.7959958487406582, 0.1809055777063855, 0.19441884874194382, 0.6735733685324677, 0.653629308683124, 0.16602485673297018, 0.20335804099962917, 0.20133806574003343, 0.20063819684592765, 0.4267483879660453, 0.203500441501264, 0.12468842409948466, 0.7601496747285006, 0.21347654253114523, 0.20557946271286787, 0.20946166959342294, 0.2037521178560513, 0.19899119990835357, 0.20502188897743512, 0.21185012528730773, 0.23029761548989025, 0.2226957927771256, 0.10106476573254897, 0.08282313409598063, 0.0927463241901183, 0.0892673049309205, 0.09187005772305634, 0.11808457473018663, 0.09136297465966581, 0.09200662746312827, 0.09667808533660249]}, "mutation_prompt": null}
{"id": "d463cbee-edea-4a60-8294-ae30f42057f8", "solution": "import numpy as np\n\nclass HybridStochasticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.initial_scale_factor = 0.8\n        self.initial_crossover_prob = 0.7\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population = self.opposition_based_initialization()\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.learning_rate = 0.1\n\n    def opposition_based_initialization(self):\n        initial_population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        opposition_population = self.lower_bound + self.upper_bound - initial_population\n        return np.vstack((initial_population, opposition_population))[:self.population_size]\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def select_parents(self, idx):\n        indices = [i for i in range(self.population_size) if i != idx]\n        return np.random.choice(indices, 3, replace=False)\n\n    def differential_evolution_mutation(self, idx, scale_factor):\n        a, b, c = self.select_parents(idx)\n        mutant_vector = self.population[a] + scale_factor * (self.population[b] - self.population[c])\n        return np.clip(mutant_vector, self.lower_bound, self.upper_bound)\n\n    def crossover(self, target_vector, mutant_vector, crossover_prob):\n        crossover_mask = np.random.rand(self.dim) < crossover_prob\n        return np.where(crossover_mask, mutant_vector, target_vector)\n\n    def local_search(self, vector):\n        perturbation = np.random.normal(0, self.learning_rate, self.dim)\n        candidate_vector = vector + perturbation\n        return np.clip(candidate_vector, self.lower_bound, self.upper_bound)\n\n    def __call__(self, func):\n        self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                # Dynamic adjustment of parameters\n                scale_factor = self.initial_scale_factor * (1 - self.evaluations / self.budget)\n                crossover_prob = self.initial_crossover_prob + 0.2 * np.random.rand()\n\n                mutant_vector = self.differential_evolution_mutation(i, scale_factor)\n                trial_vector = self.crossover(self.population[i], mutant_vector, crossover_prob)\n                trial_fitness = func(trial_vector)\n                self.evaluations += 1\n\n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial_vector\n                    self.fitness[i] = trial_fitness\n\n                if np.random.rand() < 0.2:\n                    candidate_vector = self.local_search(self.population[i])\n                    candidate_fitness = func(candidate_vector)\n                    self.evaluations += 1\n                    if candidate_fitness < self.fitness[i]:\n                        self.population[i] = candidate_vector\n                        self.fitness[i] = candidate_fitness\n\n                # Update learning rate adaptively\n                self.learning_rate = max(0.01, self.learning_rate * (1 - 0.05 * np.random.rand()))\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx]", "name": "HybridStochasticOptimizer", "description": "Hybrid optimizer with adaptive learning rate and opposition-based initialization for faster convergence.", "configspace": "", "generation": 32, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "c9aafe6b-332b-4283-9cc1-4b2e50c44e27", "metadata": {"aucs": [0.8614318631710612, 0.8615612395806681, 0.8648092636329068, 0.8836823589578061, 0.8809302815417128, 0.8537149049105373, 0.8649927264859298, 0.8661719092889808, 0.8732297035052712, 0.7784313840335078, 0.7934709027220181, 0.7709929153086685, 0.7666377215322294, 0.7792354193630129, 0.7955283567004858, 0.7874175449423783, 0.7934505999095464, 0.7963508686605465, 0.25196533963541234, 0.603323462921044, 0.31189512717187695, 0.45254508955474515, 0.3855834591332875, 0.6324217482611867, 0.38087210189123466, 0.37101210401952667, 0.37832843410229366, 0.3374576510463305, 0.1599578735010866, 0.25181667196560353, 0.15598715927116646, 0.15926218502651568, 0.14742944278799264, 0.33123888835847937, 0.3066524419836607, 0.16048855283955032, 0.9738501285938751, 0.9538242295949432, 0.9811516406082638, 0.9649564447290745, 0.9726531518905077, 0.9814481843774895, 0.9732999615150277, 0.9721568744354393, 0.9772487896256388, 0.5602477359628759, 0.5166699154516793, 0.49955145811133295, 0.4607762689339385, 0.5168840480364095, 0.5837268635955544, 0.5848178673193583, 0.44902566155463985, 0.6535189332618503, 0.8108564290388902, 0.7472995471422044, 0.7832292739258102, 0.780298104186407, 0.7997326258786164, 0.8307049265401217, 0.7670254946306878, 0.7966436314344594, 0.7737581975614084, 0.26793793612173356, 0.24955593945589905, 0.19834760026021148, 0.22530649729727037, 0.21576791524353967, 0.22803418538233733, 0.23532474250416835, 0.20160318462124072, 0.1920099262564383, 0.19613605298744063, 0.23556007360063813, 0.11440376677945807, 0.22544642163615658, 0.24862040742714708, 0.1874776391649946, 0.19655907028592323, 0.19284064583165672, 0.19577311722397894, 0.3108121261595491, 0.2566448530161354, 0.36431074059207347, 0.24025452278021453, 0.4802140919950957, 0.0007248673300417341, 0.35401012029935763, 0.3692632979788284, 0.276018593628536, 0.3623824055587753, 0.3924469974919602, 0.23556071143414548, 0.35272611497922335, 0.44565667864537895, 0.39468163509167276, 0.4061903350584134, 0.36215287040133537, 0.3973933722095969, 0.11732286633565558, 0.20172957542003978, 0.1498188877382276, 0.24772296138908012, 0.1546062088760296, 0.10282719367721671, 0.20924019897046064, 0.1183165114034116, 0.20632727477440505, 0.2809986328144657, 0.29684536408175155, 0.3175185646986213, 0.3737401167683222, 0.33517695108836454, 0.3271148862228106, 0.36693310045283345, 0.3166293273038533, 0.24146562201228117, 0.6055800258234042, 0.7234496228479592, 0.6438958422770305, 0.7507872332161957, 0.7324506280759618, 0.6589215501985713, 0.7496261142206224, 0.7209822579527148, 0.6855782937737074, 0.1371651233838116, 0.12075132229211483, 0.13780801853641733, 0.12584800341409697, 0.13850225460242616, 0.11675557226216027, 0.12863432580056944, 0.1273098808292441, 0.14268148704984884, 0.19934365172377466, 0.18594466654988895, 0.32832255345101913, 0.23469770515038102, 0.14866333116079655, 0.20267106519038025, 0.20152641892425394, 0.23576091839252133, 0.21421383347767264, 0.5391071467719948, 0.5042943805711715, 0.4612246809606788, 0.47676447358121665, 0.5291640446387131, 0.602638537448555, 0.6246427601161761, 0.5714078963023623, 0.6225968922190244, 0.33304162231889356, 0.3304783841194173, 0.4141875902408757, 0.354629285337982, 0.33990446568261656, 0.27919480653858797, 0.41214607585167573, 0.4488101082230267, 0.4070679191988382, 0.2075439556387948, 0.21303390663090027, 0.21635748499617935, 0.19827783015642464, 0.20259798106785998, 0.2337489300339528, 0.1995085629275789, 0.23000732022010184, 0.20258936164228614, 0.3583455868837565, 0.5974983189015708, 0.6315004735669381, 0.48375114306220435, 0.5228073851654269, 0.6765804181616237, 0.44806218567436396, 0.4405846418933689, 0.2231537031442834, 0.15177066878967904, 0.19069875851639195, 0.8538083672200958, 0.8489568076127689, 0.8369418101551436, 0.7959958487406582, 0.1809055777063855, 0.19441884874194382, 0.6735733685324677, 0.653629308683124, 0.16602485673297018, 0.20335804099962917, 0.20133806574003343, 0.20063819684592765, 0.4267483879660453, 0.203500441501264, 0.12468842409948466, 0.7601496747285006, 0.21347654253114523, 0.20557946271286787, 0.20946166959342294, 0.2037521178560513, 0.19899119990835357, 0.20502188897743512, 0.21185012528730773, 0.23029761548989025, 0.2226957927771256, 0.10106476573254897, 0.08282313409598063, 0.0927463241901183, 0.0892673049309205, 0.09187005772305634, 0.11808457473018663, 0.09136297465966581, 0.09200662746312827, 0.09667808533660249]}, "mutation_prompt": null}
{"id": "40ae7f1d-6e6f-4ae7-b365-25430647038c", "solution": "import numpy as np\n\nclass HybridStochasticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.initial_scale_factor = 0.8\n        self.initial_crossover_prob = 0.7\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population = self.opposition_based_initialization()\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.learning_rate = 0.1\n\n    def opposition_based_initialization(self):\n        initial_population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        opposition_population = self.lower_bound + self.upper_bound - initial_population\n        return np.vstack((initial_population, opposition_population))[:self.population_size]\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def select_parents(self, idx):\n        indices = [i for i in range(self.population_size) if i != idx]\n        return np.random.choice(indices, 3, replace=False)\n\n    def differential_evolution_mutation(self, idx, scale_factor):\n        a, b, c = self.select_parents(idx)\n        mutant_vector = self.population[a] + scale_factor * (self.population[b] - self.population[c])\n        return np.clip(mutant_vector, self.lower_bound, self.upper_bound)\n\n    def crossover(self, target_vector, mutant_vector, crossover_prob):\n        crossover_mask = np.random.rand(self.dim) < crossover_prob\n        return np.where(crossover_mask, mutant_vector, target_vector)\n\n    def local_search(self, vector):\n        perturbation = np.random.normal(0, self.learning_rate, self.dim)\n        candidate_vector = vector + perturbation\n        return np.clip(candidate_vector, self.lower_bound, self.upper_bound)\n\n    def __call__(self, func):\n        self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                # Dynamic adjustment of parameters\n                scale_factor = self.initial_scale_factor * (1 - self.evaluations / self.budget)\n                crossover_prob = self.initial_crossover_prob + 0.2 * np.random.rand()\n\n                mutant_vector = self.differential_evolution_mutation(i, scale_factor)\n                trial_vector = self.crossover(self.population[i], mutant_vector, crossover_prob)\n                trial_fitness = func(trial_vector)\n                self.evaluations += 1\n\n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial_vector\n                    self.fitness[i] = trial_fitness\n\n                if np.random.rand() < 0.2:\n                    candidate_vector = self.local_search(self.population[i])\n                    candidate_fitness = func(candidate_vector)\n                    self.evaluations += 1\n                    if candidate_fitness < self.fitness[i]:\n                        self.population[i] = candidate_vector\n                        self.fitness[i] = candidate_fitness\n\n                # Update learning rate adaptively\n                self.learning_rate = max(0.01, self.learning_rate * (1 - 0.05 * np.random.rand()))\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx]", "name": "HybridStochasticOptimizer", "description": "Hybrid optimizer with adaptive learning rate and opposition-based initialization for faster convergence.", "configspace": "", "generation": 32, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "c9aafe6b-332b-4283-9cc1-4b2e50c44e27", "metadata": {"aucs": [0.8614318631710612, 0.8615612395806681, 0.8648092636329068, 0.8836823589578061, 0.8809302815417128, 0.8537149049105373, 0.8649927264859298, 0.8661719092889808, 0.8732297035052712, 0.7784313840335078, 0.7934709027220181, 0.7709929153086685, 0.7666377215322294, 0.7792354193630129, 0.7955283567004858, 0.7874175449423783, 0.7934505999095464, 0.7963508686605465, 0.25196533963541234, 0.603323462921044, 0.31189512717187695, 0.45254508955474515, 0.3855834591332875, 0.6324217482611867, 0.38087210189123466, 0.37101210401952667, 0.37832843410229366, 0.3374576510463305, 0.1599578735010866, 0.25181667196560353, 0.15598715927116646, 0.15926218502651568, 0.14742944278799264, 0.33123888835847937, 0.3066524419836607, 0.16048855283955032, 0.9738501285938751, 0.9538242295949432, 0.9811516406082638, 0.9649564447290745, 0.9726531518905077, 0.9814481843774895, 0.9732999615150277, 0.9721568744354393, 0.9772487896256388, 0.5602477359628759, 0.5166699154516793, 0.49955145811133295, 0.4607762689339385, 0.5168840480364095, 0.5837268635955544, 0.5848178673193583, 0.44902566155463985, 0.6535189332618503, 0.8108564290388902, 0.7472995471422044, 0.7832292739258102, 0.780298104186407, 0.7997326258786164, 0.8307049265401217, 0.7670254946306878, 0.7966436314344594, 0.7737581975614084, 0.26793793612173356, 0.24955593945589905, 0.19834760026021148, 0.22530649729727037, 0.21576791524353967, 0.22803418538233733, 0.23532474250416835, 0.20160318462124072, 0.1920099262564383, 0.19613605298744063, 0.23556007360063813, 0.11440376677945807, 0.22544642163615658, 0.24862040742714708, 0.1874776391649946, 0.19655907028592323, 0.19284064583165672, 0.19577311722397894, 0.3108121261595491, 0.2566448530161354, 0.36431074059207347, 0.24025452278021453, 0.4802140919950957, 0.0007248673300417341, 0.35401012029935763, 0.3692632979788284, 0.276018593628536, 0.3623824055587753, 0.3924469974919602, 0.23556071143414548, 0.35272611497922335, 0.44565667864537895, 0.39468163509167276, 0.4061903350584134, 0.36215287040133537, 0.3973933722095969, 0.11732286633565558, 0.20172957542003978, 0.1498188877382276, 0.24772296138908012, 0.1546062088760296, 0.10282719367721671, 0.20924019897046064, 0.1183165114034116, 0.20632727477440505, 0.2809986328144657, 0.29684536408175155, 0.3175185646986213, 0.3737401167683222, 0.33517695108836454, 0.3271148862228106, 0.36693310045283345, 0.3166293273038533, 0.24146562201228117, 0.6055800258234042, 0.7234496228479592, 0.6438958422770305, 0.7507872332161957, 0.7324506280759618, 0.6589215501985713, 0.7496261142206224, 0.7209822579527148, 0.6855782937737074, 0.1371651233838116, 0.12075132229211483, 0.13780801853641733, 0.12584800341409697, 0.13850225460242616, 0.11675557226216027, 0.12863432580056944, 0.1273098808292441, 0.14268148704984884, 0.19934365172377466, 0.18594466654988895, 0.32832255345101913, 0.23469770515038102, 0.14866333116079655, 0.20267106519038025, 0.20152641892425394, 0.23576091839252133, 0.21421383347767264, 0.5391071467719948, 0.5042943805711715, 0.4612246809606788, 0.47676447358121665, 0.5291640446387131, 0.602638537448555, 0.6246427601161761, 0.5714078963023623, 0.6225968922190244, 0.33304162231889356, 0.3304783841194173, 0.4141875902408757, 0.354629285337982, 0.33990446568261656, 0.27919480653858797, 0.41214607585167573, 0.4488101082230267, 0.4070679191988382, 0.2075439556387948, 0.21303390663090027, 0.21635748499617935, 0.19827783015642464, 0.20259798106785998, 0.2337489300339528, 0.1995085629275789, 0.23000732022010184, 0.20258936164228614, 0.3583455868837565, 0.5974983189015708, 0.6315004735669381, 0.48375114306220435, 0.5228073851654269, 0.6765804181616237, 0.44806218567436396, 0.4405846418933689, 0.2231537031442834, 0.15177066878967904, 0.19069875851639195, 0.8538083672200958, 0.8489568076127689, 0.8369418101551436, 0.7959958487406582, 0.1809055777063855, 0.19441884874194382, 0.6735733685324677, 0.653629308683124, 0.16602485673297018, 0.20335804099962917, 0.20133806574003343, 0.20063819684592765, 0.4267483879660453, 0.203500441501264, 0.12468842409948466, 0.7601496747285006, 0.21347654253114523, 0.20557946271286787, 0.20946166959342294, 0.2037521178560513, 0.19899119990835357, 0.20502188897743512, 0.21185012528730773, 0.23029761548989025, 0.2226957927771256, 0.10106476573254897, 0.08282313409598063, 0.0927463241901183, 0.0892673049309205, 0.09187005772305634, 0.11808457473018663, 0.09136297465966581, 0.09200662746312827, 0.09667808533660249]}, "mutation_prompt": null}
{"id": "cb330be0-339f-4657-bb36-fdc828b42e7a", "solution": "import numpy as np\n\nclass HybridStochasticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.initial_scale_factor = 0.8\n        self.initial_crossover_prob = 0.7\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population = self.opposition_based_initialization()\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.learning_rate = 0.1\n\n    def opposition_based_initialization(self):\n        initial_population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        opposition_population = self.lower_bound + self.upper_bound - initial_population\n        return np.vstack((initial_population, opposition_population))[:self.population_size]\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def select_parents(self, idx):\n        indices = [i for i in range(self.population_size) if i != idx]\n        return np.random.choice(indices, 3, replace=False)\n\n    def differential_evolution_mutation(self, idx, scale_factor):\n        a, b, c = self.select_parents(idx)\n        mutant_vector = self.population[a] + scale_factor * (self.population[b] - self.population[c])\n        return np.clip(mutant_vector, self.lower_bound, self.upper_bound)\n\n    def crossover(self, target_vector, mutant_vector, crossover_prob):\n        crossover_mask = np.random.rand(self.dim) < crossover_prob\n        return np.where(crossover_mask, mutant_vector, target_vector)\n\n    def local_search(self, vector):\n        perturbation = np.random.normal(0, self.learning_rate, self.dim)\n        candidate_vector = vector + perturbation\n        return np.clip(candidate_vector, self.lower_bound, self.upper_bound)\n\n    def __call__(self, func):\n        self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                # Dynamic adjustment of parameters\n                scale_factor = self.initial_scale_factor * (1 - self.evaluations / self.budget)\n                crossover_prob = self.initial_crossover_prob + 0.2 * np.random.rand()\n\n                mutant_vector = self.differential_evolution_mutation(i, scale_factor)\n                trial_vector = self.crossover(self.population[i], mutant_vector, crossover_prob)\n                trial_fitness = func(trial_vector)\n                self.evaluations += 1\n\n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial_vector\n                    self.fitness[i] = trial_fitness\n\n                if np.random.rand() < 0.2:\n                    candidate_vector = self.local_search(self.population[i])\n                    candidate_fitness = func(candidate_vector)\n                    self.evaluations += 1\n                    if candidate_fitness < self.fitness[i]:\n                        self.population[i] = candidate_vector\n                        self.fitness[i] = candidate_fitness\n\n                # Update learning rate adaptively\n                self.learning_rate = max(0.01, self.learning_rate * (1 - 0.05 * np.random.rand()))\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx]", "name": "HybridStochasticOptimizer", "description": "Hybrid optimizer with adaptive learning rate and opposition-based initialization for faster convergence.", "configspace": "", "generation": 32, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "c9aafe6b-332b-4283-9cc1-4b2e50c44e27", "metadata": {"aucs": [0.8614318631710612, 0.8615612395806681, 0.8648092636329068, 0.8836823589578061, 0.8809302815417128, 0.8537149049105373, 0.8649927264859298, 0.8661719092889808, 0.8732297035052712, 0.7784313840335078, 0.7934709027220181, 0.7709929153086685, 0.7666377215322294, 0.7792354193630129, 0.7955283567004858, 0.7874175449423783, 0.7934505999095464, 0.7963508686605465, 0.25196533963541234, 0.603323462921044, 0.31189512717187695, 0.45254508955474515, 0.3855834591332875, 0.6324217482611867, 0.38087210189123466, 0.37101210401952667, 0.37832843410229366, 0.3374576510463305, 0.1599578735010866, 0.25181667196560353, 0.15598715927116646, 0.15926218502651568, 0.14742944278799264, 0.33123888835847937, 0.3066524419836607, 0.16048855283955032, 0.9738501285938751, 0.9538242295949432, 0.9811516406082638, 0.9649564447290745, 0.9726531518905077, 0.9814481843774895, 0.9732999615150277, 0.9721568744354393, 0.9772487896256388, 0.5602477359628759, 0.5166699154516793, 0.49955145811133295, 0.4607762689339385, 0.5168840480364095, 0.5837268635955544, 0.5848178673193583, 0.44902566155463985, 0.6535189332618503, 0.8108564290388902, 0.7472995471422044, 0.7832292739258102, 0.780298104186407, 0.7997326258786164, 0.8307049265401217, 0.7670254946306878, 0.7966436314344594, 0.7737581975614084, 0.26793793612173356, 0.24955593945589905, 0.19834760026021148, 0.22530649729727037, 0.21576791524353967, 0.22803418538233733, 0.23532474250416835, 0.20160318462124072, 0.1920099262564383, 0.19613605298744063, 0.23556007360063813, 0.11440376677945807, 0.22544642163615658, 0.24862040742714708, 0.1874776391649946, 0.19655907028592323, 0.19284064583165672, 0.19577311722397894, 0.3108121261595491, 0.2566448530161354, 0.36431074059207347, 0.24025452278021453, 0.4802140919950957, 0.0007248673300417341, 0.35401012029935763, 0.3692632979788284, 0.276018593628536, 0.3623824055587753, 0.3924469974919602, 0.23556071143414548, 0.35272611497922335, 0.44565667864537895, 0.39468163509167276, 0.4061903350584134, 0.36215287040133537, 0.3973933722095969, 0.11732286633565558, 0.20172957542003978, 0.1498188877382276, 0.24772296138908012, 0.1546062088760296, 0.10282719367721671, 0.20924019897046064, 0.1183165114034116, 0.20632727477440505, 0.2809986328144657, 0.29684536408175155, 0.3175185646986213, 0.3737401167683222, 0.33517695108836454, 0.3271148862228106, 0.36693310045283345, 0.3166293273038533, 0.24146562201228117, 0.6055800258234042, 0.7234496228479592, 0.6438958422770305, 0.7507872332161957, 0.7324506280759618, 0.6589215501985713, 0.7496261142206224, 0.7209822579527148, 0.6855782937737074, 0.1371651233838116, 0.12075132229211483, 0.13780801853641733, 0.12584800341409697, 0.13850225460242616, 0.11675557226216027, 0.12863432580056944, 0.1273098808292441, 0.14268148704984884, 0.19934365172377466, 0.18594466654988895, 0.32832255345101913, 0.23469770515038102, 0.14866333116079655, 0.20267106519038025, 0.20152641892425394, 0.23576091839252133, 0.21421383347767264, 0.5391071467719948, 0.5042943805711715, 0.4612246809606788, 0.47676447358121665, 0.5291640446387131, 0.602638537448555, 0.6246427601161761, 0.5714078963023623, 0.6225968922190244, 0.33304162231889356, 0.3304783841194173, 0.4141875902408757, 0.354629285337982, 0.33990446568261656, 0.27919480653858797, 0.41214607585167573, 0.4488101082230267, 0.4070679191988382, 0.2075439556387948, 0.21303390663090027, 0.21635748499617935, 0.19827783015642464, 0.20259798106785998, 0.2337489300339528, 0.1995085629275789, 0.23000732022010184, 0.20258936164228614, 0.3583455868837565, 0.5974983189015708, 0.6315004735669381, 0.48375114306220435, 0.5228073851654269, 0.6765804181616237, 0.44806218567436396, 0.4405846418933689, 0.2231537031442834, 0.15177066878967904, 0.19069875851639195, 0.8538083672200958, 0.8489568076127689, 0.8369418101551436, 0.7959958487406582, 0.1809055777063855, 0.19441884874194382, 0.6735733685324677, 0.653629308683124, 0.16602485673297018, 0.20335804099962917, 0.20133806574003343, 0.20063819684592765, 0.4267483879660453, 0.203500441501264, 0.12468842409948466, 0.7601496747285006, 0.21347654253114523, 0.20557946271286787, 0.20946166959342294, 0.2037521178560513, 0.19899119990835357, 0.20502188897743512, 0.21185012528730773, 0.23029761548989025, 0.2226957927771256, 0.10106476573254897, 0.08282313409598063, 0.0927463241901183, 0.0892673049309205, 0.09187005772305634, 0.11808457473018663, 0.09136297465966581, 0.09200662746312827, 0.09667808533660249]}, "mutation_prompt": null}
{"id": "907abdad-0f15-4c8b-89da-bcdfa5448156", "solution": "import numpy as np\n\nclass HybridStochasticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.initial_scale_factor = 0.8\n        self.initial_crossover_prob = 0.7\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population = self.opposition_based_initialization()\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.learning_rate = 0.1\n\n    def opposition_based_initialization(self):\n        initial_population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        opposition_population = self.lower_bound + self.upper_bound - initial_population\n        return np.vstack((initial_population, opposition_population))[:self.population_size]\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def select_parents(self, idx):\n        indices = [i for i in range(self.population_size) if i != idx]\n        return np.random.choice(indices, 3, replace=False)\n\n    def differential_evolution_mutation(self, idx, scale_factor):\n        a, b, c = self.select_parents(idx)\n        mutant_vector = self.population[a] + scale_factor * (self.population[b] - self.population[c])\n        return np.clip(mutant_vector, self.lower_bound, self.upper_bound)\n\n    def crossover(self, target_vector, mutant_vector, crossover_prob):\n        crossover_mask = np.random.rand(self.dim) < crossover_prob\n        return np.where(crossover_mask, mutant_vector, target_vector)\n\n    def local_search(self, vector):\n        perturbation = np.random.normal(0, self.learning_rate, self.dim)\n        candidate_vector = vector + perturbation\n        return np.clip(candidate_vector, self.lower_bound, self.upper_bound)\n\n    def __call__(self, func):\n        self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                # Dynamic adjustment of parameters\n                scale_factor = self.initial_scale_factor * (1 - self.evaluations / self.budget)\n                crossover_prob = self.initial_crossover_prob + 0.2 * np.random.rand()\n\n                mutant_vector = self.differential_evolution_mutation(i, scale_factor)\n                trial_vector = self.crossover(self.population[i], mutant_vector, crossover_prob)\n                trial_fitness = func(trial_vector)\n                self.evaluations += 1\n\n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial_vector\n                    self.fitness[i] = trial_fitness\n\n                if np.random.rand() < 0.2:\n                    candidate_vector = self.local_search(self.population[i])\n                    candidate_fitness = func(candidate_vector)\n                    self.evaluations += 1\n                    if candidate_fitness < self.fitness[i]:\n                        self.population[i] = candidate_vector\n                        self.fitness[i] = candidate_fitness\n\n                # Update learning rate adaptively\n                self.learning_rate = max(0.01, self.learning_rate * (1 - 0.05 * np.random.rand()))\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx]", "name": "HybridStochasticOptimizer", "description": "Hybrid optimizer with adaptive learning rate and opposition-based initialization for faster convergence.", "configspace": "", "generation": 32, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "c9aafe6b-332b-4283-9cc1-4b2e50c44e27", "metadata": {"aucs": [0.8614318631710612, 0.8615612395806681, 0.8648092636329068, 0.8836823589578061, 0.8809302815417128, 0.8537149049105373, 0.8649927264859298, 0.8661719092889808, 0.8732297035052712, 0.7784313840335078, 0.7934709027220181, 0.7709929153086685, 0.7666377215322294, 0.7792354193630129, 0.7955283567004858, 0.7874175449423783, 0.7934505999095464, 0.7963508686605465, 0.25196533963541234, 0.603323462921044, 0.31189512717187695, 0.45254508955474515, 0.3855834591332875, 0.6324217482611867, 0.38087210189123466, 0.37101210401952667, 0.37832843410229366, 0.3374576510463305, 0.1599578735010866, 0.25181667196560353, 0.15598715927116646, 0.15926218502651568, 0.14742944278799264, 0.33123888835847937, 0.3066524419836607, 0.16048855283955032, 0.9738501285938751, 0.9538242295949432, 0.9811516406082638, 0.9649564447290745, 0.9726531518905077, 0.9814481843774895, 0.9732999615150277, 0.9721568744354393, 0.9772487896256388, 0.5602477359628759, 0.5166699154516793, 0.49955145811133295, 0.4607762689339385, 0.5168840480364095, 0.5837268635955544, 0.5848178673193583, 0.44902566155463985, 0.6535189332618503, 0.8108564290388902, 0.7472995471422044, 0.7832292739258102, 0.780298104186407, 0.7997326258786164, 0.8307049265401217, 0.7670254946306878, 0.7966436314344594, 0.7737581975614084, 0.26793793612173356, 0.24955593945589905, 0.19834760026021148, 0.22530649729727037, 0.21576791524353967, 0.22803418538233733, 0.23532474250416835, 0.20160318462124072, 0.1920099262564383, 0.19613605298744063, 0.23556007360063813, 0.11440376677945807, 0.22544642163615658, 0.24862040742714708, 0.1874776391649946, 0.19655907028592323, 0.19284064583165672, 0.19577311722397894, 0.3108121261595491, 0.2566448530161354, 0.36431074059207347, 0.24025452278021453, 0.4802140919950957, 0.0007248673300417341, 0.35401012029935763, 0.3692632979788284, 0.276018593628536, 0.3623824055587753, 0.3924469974919602, 0.23556071143414548, 0.35272611497922335, 0.44565667864537895, 0.39468163509167276, 0.4061903350584134, 0.36215287040133537, 0.3973933722095969, 0.11732286633565558, 0.20172957542003978, 0.1498188877382276, 0.24772296138908012, 0.1546062088760296, 0.10282719367721671, 0.20924019897046064, 0.1183165114034116, 0.20632727477440505, 0.2809986328144657, 0.29684536408175155, 0.3175185646986213, 0.3737401167683222, 0.33517695108836454, 0.3271148862228106, 0.36693310045283345, 0.3166293273038533, 0.24146562201228117, 0.6055800258234042, 0.7234496228479592, 0.6438958422770305, 0.7507872332161957, 0.7324506280759618, 0.6589215501985713, 0.7496261142206224, 0.7209822579527148, 0.6855782937737074, 0.1371651233838116, 0.12075132229211483, 0.13780801853641733, 0.12584800341409697, 0.13850225460242616, 0.11675557226216027, 0.12863432580056944, 0.1273098808292441, 0.14268148704984884, 0.19934365172377466, 0.18594466654988895, 0.32832255345101913, 0.23469770515038102, 0.14866333116079655, 0.20267106519038025, 0.20152641892425394, 0.23576091839252133, 0.21421383347767264, 0.5391071467719948, 0.5042943805711715, 0.4612246809606788, 0.47676447358121665, 0.5291640446387131, 0.602638537448555, 0.6246427601161761, 0.5714078963023623, 0.6225968922190244, 0.33304162231889356, 0.3304783841194173, 0.4141875902408757, 0.354629285337982, 0.33990446568261656, 0.27919480653858797, 0.41214607585167573, 0.4488101082230267, 0.4070679191988382, 0.2075439556387948, 0.21303390663090027, 0.21635748499617935, 0.19827783015642464, 0.20259798106785998, 0.2337489300339528, 0.1995085629275789, 0.23000732022010184, 0.20258936164228614, 0.3583455868837565, 0.5974983189015708, 0.6315004735669381, 0.48375114306220435, 0.5228073851654269, 0.6765804181616237, 0.44806218567436396, 0.4405846418933689, 0.2231537031442834, 0.15177066878967904, 0.19069875851639195, 0.8538083672200958, 0.8489568076127689, 0.8369418101551436, 0.7959958487406582, 0.1809055777063855, 0.19441884874194382, 0.6735733685324677, 0.653629308683124, 0.16602485673297018, 0.20335804099962917, 0.20133806574003343, 0.20063819684592765, 0.4267483879660453, 0.203500441501264, 0.12468842409948466, 0.7601496747285006, 0.21347654253114523, 0.20557946271286787, 0.20946166959342294, 0.2037521178560513, 0.19899119990835357, 0.20502188897743512, 0.21185012528730773, 0.23029761548989025, 0.2226957927771256, 0.10106476573254897, 0.08282313409598063, 0.0927463241901183, 0.0892673049309205, 0.09187005772305634, 0.11808457473018663, 0.09136297465966581, 0.09200662746312827, 0.09667808533660249]}, "mutation_prompt": null}
{"id": "99e23635-984d-41b7-ad3f-30662cc8415f", "solution": "import numpy as np\n\nclass EnhancedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 24  # Slightly increased for better exploration\n        self.initial_scale_factor = 0.85  # Adjusted for better exploration/exploitation balance\n        self.initial_crossover_prob = 0.75  # Adjusted to improve diversity\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population = self.opposition_based_initialization()\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.learning_rate = 0.1\n\n    def opposition_based_initialization(self):\n        initial_population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        opposition_population = self.lower_bound + self.upper_bound - initial_population\n        return np.vstack((initial_population, opposition_population))[:self.population_size]\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def select_parents(self, idx):\n        indices = [i for i in range(self.population_size) if i != idx]\n        return np.random.choice(indices, 3, replace=False)\n\n    def differential_evolution_mutation(self, idx, scale_factor):\n        a, b, c = self.select_parents(idx)\n        mutant_vector = self.population[a] + scale_factor * (self.population[b] - self.population[c])\n        return np.clip(mutant_vector, self.lower_bound, self.upper_bound)\n\n    def crossover(self, target_vector, mutant_vector, crossover_prob):\n        crossover_mask = np.random.rand(self.dim) < crossover_prob\n        return np.where(crossover_mask, mutant_vector, target_vector)\n\n    def local_search(self, vector):\n        perturbation = np.random.normal(0, self.learning_rate, self.dim)\n        candidate_vector = vector + perturbation\n        return np.clip(candidate_vector, self.lower_bound, self.upper_bound)\n\n    def adaptive_dimension_reduction(self, vector):\n        reduced_vector = vector * (1 - 0.1 * np.random.rand(self.dim))\n        return np.clip(reduced_vector, self.lower_bound, self.upper_bound)\n\n    def __call__(self, func):\n        self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                # Dynamic adjustment of parameters\n                scale_factor = self.initial_scale_factor * (0.9 + 0.1 * (1 - self.evaluations / self.budget))\n                crossover_prob = self.initial_crossover_prob * (1 + 0.1 * np.random.rand())\n\n                mutant_vector = self.differential_evolution_mutation(i, scale_factor)\n                trial_vector = self.crossover(self.population[i], mutant_vector, crossover_prob)\n                trial_fitness = func(trial_vector)\n                self.evaluations += 1\n\n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial_vector\n                    self.fitness[i] = trial_fitness\n\n                if np.random.rand() < 0.25:  # Increased chance for local search\n                    candidate_vector = self.local_search(self.population[i])\n                    candidate_fitness = func(candidate_vector)\n                    self.evaluations += 1\n                    if candidate_fitness < self.fitness[i]:\n                        self.population[i] = candidate_vector\n                        self.fitness[i] = candidate_fitness\n\n                if np.random.rand() < 0.15:  # New dimension reduction strategy\n                    reduced_vector = self.adaptive_dimension_reduction(self.population[i])\n                    reduced_fitness = func(reduced_vector)\n                    self.evaluations += 1\n                    if reduced_fitness < self.fitness[i]:\n                        self.population[i] = reduced_vector\n                        self.fitness[i] = reduced_fitness\n\n                # Update learning rate adaptively\n                self.learning_rate = max(0.01, self.learning_rate * (1 - 0.03 * np.random.rand()))\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx]", "name": "EnhancedHybridOptimizer", "description": "Enhanced hybrid optimizer with adaptive dimension reduction and dynamic learning rate for accelerated convergence.", "configspace": "", "generation": 68, "fitness": 0.3387248591626186, "feedback": "The algorithm EnhancedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.34 with standard deviation 0.23.", "error": "", "parent_id": "c9aafe6b-332b-4283-9cc1-4b2e50c44e27", "metadata": {"aucs": [0.7879583751611103, 0.7568360632055132, 0.7681753122398254, 0.7807985021770296, 0.7611632904327962, 0.7884638408633443, 0.7707710060118662, 0.7781497454031194, 0.768942122054044, 0.616276883750188, 0.6142118690065728, 0.6266818532015956, 0.6557175743555088, 0.6036756326699648, 0.5992922005245149, 0.609780783378008, 0.6214396433278595, 0.6322195542662528, 0.22618485129797872, 0.32005550801002614, 0.19375163188069777, 0.42381039783050933, 0.16612329703262918, 0.22682981598084873, 0.30530252417885784, 0.1505547635983694, 0.1811678217881043, 0.1536028818792643, 0.19779752404276796, 0.2511329870651331, 0.14598166221858444, 0.1399340339375027, 0.18049963337104002, 0.1447685411419286, 0.12341199713641382, 0.13410122984701134, 0.959905472947495, 0.9579462384409508, 0.955522477414855, 0.969167763097999, 0.9848492590142611, 0.9535816191727534, 0.9739308894007076, 0.9397025530406169, 0.9757050589292603, 0.3662736879189289, 0.4177059488568733, 0.4166192102781152, 0.36986546521093056, 0.35924505286639463, 0.4164033621440113, 0.40901190839178203, 0.40468210215647027, 0.39950423744504926, 0.6285181094798935, 0.5273977425351275, 0.6861106795094967, 0.6619158869673368, 0.573847014150692, 0.6186111792449747, 0.6363430085918343, 0.6553391346411501, 0.5746466664966438, 0.22628555815272644, 0.3479250395099611, 0.2538870272582282, 0.27006735055802733, 0.24849776001920265, 0.27165111401625186, 0.34104924273252757, 0.29012998577851623, 0.33970377589405754, 0.25181582346945663, 0.2475273012935767, 0.2837709871875982, 0.2815330769068851, 0.2684998429753065, 0.29777745581863513, 0.2626200042588491, 0.23178750043064933, 0.3546380966917304, 0.15012730697769316, 0.1278200573870547, 0.13360914877524355, 0.14319924457201616, 0.13816624932427546, 0.1608332166201344, 0.1532349643048927, 0.15941227877548048, 0.18590232130711593, 0.3096683689071359, 0.2798333285052943, 0.25763948087736166, 0.2845605032648375, 0.2511452114250333, 0.2886992215994324, 0.3240709548788544, 0.272965165366986, 0.294021372937586, 0.08173523055948284, 0.05092999131334286, 0.06966241222484437, 0.11336313577805646, 0.05023413920267017, 0.10178535647412024, 0.07325615670895369, 0.06462605898827478, 0.1073476431484871, 0.18790323420933996, 0.18824182941941392, 0.20575118708418771, 0.20139992197519263, 0.20328184170303853, 0.204453279191659, 0.18653313762131063, 0.19217149517458465, 0.1937690021001982, 0.5778538065753, 0.5738855569404802, 0.5458969900686197, 0.5526859975335745, 0.5605407453081188, 0.5747931517920082, 0.5486774502878559, 0.5446845816330479, 0.5576905889641182, 0.1117494585696146, 0.12006955347948789, 0.11754984324887918, 0.10046888148322508, 0.11798385845963899, 0.1201424009775034, 0.11437894281246042, 0.10474955864391655, 0.10526527802920882, 0.18205075719965103, 0.172982137683253, 0.1569056636903703, 0.16052541435410017, 0.20626761666586357, 0.1672535558488365, 0.24149443794235625, 0.16767632827250178, 0.15715133469633524, 0.3121963165307706, 0.310726774691929, 0.28803041400759644, 0.277656435721815, 0.3085041722474665, 0.2939430932900803, 0.3220916321813003, 0.3269031009547776, 0.35918523518505263, 0.19913559419796345, 0.20333285653191457, 0.2035807315961381, 0.23720930718634414, 0.21243813745113382, 0.2121295814655917, 0.23274003432261314, 0.2621869584388774, 0.19730708655717644, 0.20336827919010725, 0.2384277039736563, 0.19738318078457306, 0.1788143896279738, 0.19942351931667657, 0.20919762223100302, 0.20514930947917032, 0.21656603260524232, 0.22663645349746786, 0.19539287702794306, 0.20533042585588424, 0.2293615335442648, 0.27164396137806623, 0.19714045256363955, 0.24077615713032818, 0.20666660823237581, 0.1973464280702577, 0.30924434362340014, 0.8116271962668409, 0.7975549204546362, 0.6676377345518776, 0.5374307487003449, 0.17714689064585998, 0.18425702322420523, 0.5406418883474706, 0.5243604084216531, 0.7220239426020169, 0.6252385649491818, 0.5259773788995239, 0.47304171364937875, 0.530012466824219, 0.49030118553423885, 0.44615497896739564, 0.20965635725290277, 0.21034199226928996, 0.2098229999001865, 0.20842305382657067, 0.19747737987511693, 0.19729613312569216, 0.2030867056017277, 0.21368811368399587, 0.20719867592401042, 0.20755574073968797, 0.20657278679498925, 0.21739982128072688, 0.0940397602361005, 0.09036180169380659, 0.08464138556243028, 0.0872073178373386, 0.10197898959529994, 0.07783895102358895, 0.08977558875758607, 0.09255501805103172, 0.0972658579566299]}, "mutation_prompt": null}
{"id": "c55104b7-482a-4023-855e-fe7e59a8b8ea", "solution": "import numpy as np\n\nclass HybridStochasticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.initial_scale_factor = 0.8\n        self.initial_crossover_prob = 0.7\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population = self.opposition_based_initialization()\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.learning_rate = 0.1\n\n    def opposition_based_initialization(self):\n        initial_population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        opposition_population = self.lower_bound + self.upper_bound - initial_population\n        return np.vstack((initial_population, opposition_population))[:self.population_size]\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def select_parents(self, idx):\n        indices = [i for i in range(self.population_size) if i != idx]\n        return np.random.choice(indices, 3, replace=False)\n\n    def differential_evolution_mutation(self, idx, scale_factor):\n        a, b, c = self.select_parents(idx)\n        mutant_vector = self.population[a] + scale_factor * (self.population[b] - self.population[c])\n        return np.clip(mutant_vector, self.lower_bound, self.upper_bound)\n\n    def crossover(self, target_vector, mutant_vector, crossover_prob):\n        crossover_mask = np.random.rand(self.dim) < crossover_prob\n        return np.where(crossover_mask, mutant_vector, target_vector)\n\n    def local_search(self, vector):\n        perturbation = np.random.normal(0, self.learning_rate, self.dim)\n        candidate_vector = vector + perturbation\n        return np.clip(candidate_vector, self.lower_bound, self.upper_bound)\n\n    def __call__(self, func):\n        self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                # Dynamic adjustment of parameters\n                scale_factor = self.initial_scale_factor * (1 - self.evaluations / self.budget)\n                crossover_prob = self.initial_crossover_prob + 0.2 * np.random.rand()\n\n                mutant_vector = self.differential_evolution_mutation(i, scale_factor)\n                trial_vector = self.crossover(self.population[i], mutant_vector, crossover_prob)\n                trial_fitness = func(trial_vector)\n                self.evaluations += 1\n\n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial_vector\n                    self.fitness[i] = trial_fitness\n\n                if np.random.rand() < 0.2:\n                    candidate_vector = self.local_search(self.population[i])\n                    candidate_fitness = func(candidate_vector)\n                    self.evaluations += 1\n                    if candidate_fitness < self.fitness[i]:\n                        self.population[i] = candidate_vector\n                        self.fitness[i] = candidate_fitness\n\n                # Update learning rate adaptively\n                self.learning_rate = max(0.01, self.learning_rate * (1 - 0.05 * np.random.rand()))\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx]", "name": "HybridStochasticOptimizer", "description": "Hybrid optimizer with adaptive learning rate and opposition-based initialization for faster convergence.", "configspace": "", "generation": 32, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "c9aafe6b-332b-4283-9cc1-4b2e50c44e27", "metadata": {"aucs": [0.8614318631710612, 0.8615612395806681, 0.8648092636329068, 0.8836823589578061, 0.8809302815417128, 0.8537149049105373, 0.8649927264859298, 0.8661719092889808, 0.8732297035052712, 0.7784313840335078, 0.7934709027220181, 0.7709929153086685, 0.7666377215322294, 0.7792354193630129, 0.7955283567004858, 0.7874175449423783, 0.7934505999095464, 0.7963508686605465, 0.25196533963541234, 0.603323462921044, 0.31189512717187695, 0.45254508955474515, 0.3855834591332875, 0.6324217482611867, 0.38087210189123466, 0.37101210401952667, 0.37832843410229366, 0.3374576510463305, 0.1599578735010866, 0.25181667196560353, 0.15598715927116646, 0.15926218502651568, 0.14742944278799264, 0.33123888835847937, 0.3066524419836607, 0.16048855283955032, 0.9738501285938751, 0.9538242295949432, 0.9811516406082638, 0.9649564447290745, 0.9726531518905077, 0.9814481843774895, 0.9732999615150277, 0.9721568744354393, 0.9772487896256388, 0.5602477359628759, 0.5166699154516793, 0.49955145811133295, 0.4607762689339385, 0.5168840480364095, 0.5837268635955544, 0.5848178673193583, 0.44902566155463985, 0.6535189332618503, 0.8108564290388902, 0.7472995471422044, 0.7832292739258102, 0.780298104186407, 0.7997326258786164, 0.8307049265401217, 0.7670254946306878, 0.7966436314344594, 0.7737581975614084, 0.26793793612173356, 0.24955593945589905, 0.19834760026021148, 0.22530649729727037, 0.21576791524353967, 0.22803418538233733, 0.23532474250416835, 0.20160318462124072, 0.1920099262564383, 0.19613605298744063, 0.23556007360063813, 0.11440376677945807, 0.22544642163615658, 0.24862040742714708, 0.1874776391649946, 0.19655907028592323, 0.19284064583165672, 0.19577311722397894, 0.3108121261595491, 0.2566448530161354, 0.36431074059207347, 0.24025452278021453, 0.4802140919950957, 0.0007248673300417341, 0.35401012029935763, 0.3692632979788284, 0.276018593628536, 0.3623824055587753, 0.3924469974919602, 0.23556071143414548, 0.35272611497922335, 0.44565667864537895, 0.39468163509167276, 0.4061903350584134, 0.36215287040133537, 0.3973933722095969, 0.11732286633565558, 0.20172957542003978, 0.1498188877382276, 0.24772296138908012, 0.1546062088760296, 0.10282719367721671, 0.20924019897046064, 0.1183165114034116, 0.20632727477440505, 0.2809986328144657, 0.29684536408175155, 0.3175185646986213, 0.3737401167683222, 0.33517695108836454, 0.3271148862228106, 0.36693310045283345, 0.3166293273038533, 0.24146562201228117, 0.6055800258234042, 0.7234496228479592, 0.6438958422770305, 0.7507872332161957, 0.7324506280759618, 0.6589215501985713, 0.7496261142206224, 0.7209822579527148, 0.6855782937737074, 0.1371651233838116, 0.12075132229211483, 0.13780801853641733, 0.12584800341409697, 0.13850225460242616, 0.11675557226216027, 0.12863432580056944, 0.1273098808292441, 0.14268148704984884, 0.19934365172377466, 0.18594466654988895, 0.32832255345101913, 0.23469770515038102, 0.14866333116079655, 0.20267106519038025, 0.20152641892425394, 0.23576091839252133, 0.21421383347767264, 0.5391071467719948, 0.5042943805711715, 0.4612246809606788, 0.47676447358121665, 0.5291640446387131, 0.602638537448555, 0.6246427601161761, 0.5714078963023623, 0.6225968922190244, 0.33304162231889356, 0.3304783841194173, 0.4141875902408757, 0.354629285337982, 0.33990446568261656, 0.27919480653858797, 0.41214607585167573, 0.4488101082230267, 0.4070679191988382, 0.2075439556387948, 0.21303390663090027, 0.21635748499617935, 0.19827783015642464, 0.20259798106785998, 0.2337489300339528, 0.1995085629275789, 0.23000732022010184, 0.20258936164228614, 0.3583455868837565, 0.5974983189015708, 0.6315004735669381, 0.48375114306220435, 0.5228073851654269, 0.6765804181616237, 0.44806218567436396, 0.4405846418933689, 0.2231537031442834, 0.15177066878967904, 0.19069875851639195, 0.8538083672200958, 0.8489568076127689, 0.8369418101551436, 0.7959958487406582, 0.1809055777063855, 0.19441884874194382, 0.6735733685324677, 0.653629308683124, 0.16602485673297018, 0.20335804099962917, 0.20133806574003343, 0.20063819684592765, 0.4267483879660453, 0.203500441501264, 0.12468842409948466, 0.7601496747285006, 0.21347654253114523, 0.20557946271286787, 0.20946166959342294, 0.2037521178560513, 0.19899119990835357, 0.20502188897743512, 0.21185012528730773, 0.23029761548989025, 0.2226957927771256, 0.10106476573254897, 0.08282313409598063, 0.0927463241901183, 0.0892673049309205, 0.09187005772305634, 0.11808457473018663, 0.09136297465966581, 0.09200662746312827, 0.09667808533660249]}, "mutation_prompt": null}
{"id": "085fc4fd-6092-49b6-946c-e88881c5d87c", "solution": "import numpy as np\n\nclass EnhancedHybridStochasticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.initial_scale_factor = 0.8\n        self.initial_crossover_prob = 0.7\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population = self.opposition_based_initialization()\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.learning_rate = 0.1\n        self.elitism_rate = 0.1  # New elitism parameter\n\n    def opposition_based_initialization(self):\n        initial_population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        opposition_population = self.lower_bound + self.upper_bound - initial_population\n        return np.vstack((initial_population, opposition_population))[:self.population_size]\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def select_parents(self, idx):\n        indices = [i for i in range(self.population_size) if i != idx]\n        return np.random.choice(indices, 3, replace=False)\n\n    def differential_evolution_mutation(self, idx, scale_factor):\n        a, b, c = self.select_parents(idx)\n        mutant_vector = self.population[a] + scale_factor * (self.population[b] - self.population[c])\n        return np.clip(mutant_vector, self.lower_bound, self.upper_bound)\n\n    def crossover(self, target_vector, mutant_vector, crossover_prob):\n        crossover_mask = np.random.rand(self.dim) < crossover_prob\n        return np.where(crossover_mask, mutant_vector, target_vector)\n\n    def local_search(self, vector):\n        perturbation = np.random.normal(0, self.learning_rate, self.dim)\n        candidate_vector = vector + perturbation\n        return np.clip(candidate_vector, self.lower_bound, self.upper_bound)\n\n    def elitism_selection(self):\n        num_elites = max(1, int(self.elitism_rate * self.population_size))\n        elite_indices = np.argsort(self.fitness)[:num_elites]\n        return self.population[elite_indices], self.fitness[elite_indices]\n\n    def __call__(self, func):\n        self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            elites, elite_fitness = self.elitism_selection()\n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                scale_factor = self.initial_scale_factor * (1 - self.evaluations / self.budget)\n                crossover_prob = self.initial_crossover_prob + 0.2 * np.random.rand()\n                \n                if i < len(elites):  # Use elite individuals\n                    candidate_vector = self.local_search(elites[i])\n                else:\n                    mutant_vector = self.differential_evolution_mutation(i, scale_factor)\n                    candidate_vector = self.crossover(self.population[i], mutant_vector, crossover_prob)\n\n                candidate_fitness = func(candidate_vector)\n                self.evaluations += 1\n\n                if candidate_fitness < self.fitness[i]:\n                    self.population[i] = candidate_vector\n                    self.fitness[i] = candidate_fitness\n\n                self.learning_rate = max(0.01, self.learning_rate * (1 - 0.05 * np.random.rand()))\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx]", "name": "EnhancedHybridStochasticOptimizer", "description": "Enhanced hybrid optimizer with adaptive elitism and dynamic mutation for improved convergence speed.", "configspace": "", "generation": 70, "fitness": 0.42803888818792124, "feedback": "The algorithm EnhancedHybridStochasticOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.43 with standard deviation 0.28.", "error": "", "parent_id": "c9aafe6b-332b-4283-9cc1-4b2e50c44e27", "metadata": {"aucs": [0.8887079975314532, 0.894012818670537, 0.8991705287696641, 0.8978847561824669, 0.8879257415736936, 0.8940642586100076, 0.9007105063480211, 0.8958392425350181, 0.8969457975943529, 0.8044175229050868, 0.7853180510418849, 0.7770538765325782, 0.7824751042832793, 0.7991480344536278, 0.7859582873813178, 0.7829457001324069, 0.8073918603513864, 0.794782193201033, 0.6808600123402386, 0.1750883946278604, 0.14531540322259429, 0.1442044729706906, 0.18425100518796977, 0.6818733473800802, 0.6535836728714721, 0.18149935269683826, 0.18178474171291303, 0.17985980474169194, 0.15402982831947865, 0.15226864312698196, 0.173967907678645, 0.14621090429947348, 0.47097234153094814, 0.1410752174129266, 0.6696335857850447, 0.14569775079321112, 0.9826172320377552, 0.9666035752060869, 0.9829618007032487, 0.976950051717164, 0.9827548155568644, 0.9852053707001819, 0.9767881737976671, 0.9804489105964168, 0.9669130107270508, 0.6261081652649766, 0.6510922594882195, 0.6255531916869328, 0.6774833282904631, 0.5975973805993315, 0.5910302179938389, 0.6678501570106004, 0.6330163811357998, 0.5508126220258189, 0.8531774442531348, 0.8389810559862521, 0.8880414902745725, 0.8289173802432317, 0.8953502885124923, 0.8397824108196726, 0.8273523214295622, 0.8057815752417086, 0.8580295054264129, 0.4016755801839059, 0.26933131272005106, 0.22925181461595256, 0.13204420434091646, 0.39317336231664235, 0.2284734447615212, 0.3105063171146114, 0.27055107832358327, 0.2917673882193722, 0.38502853820393, 0.2123706550707669, 0.4034652264328532, 0.568732561497391, 0.36500406246735106, 0.19551716008178943, 0.24638934001327484, 0.5225532750228102, 0.3624738677767353, 0.2666496944974961, 0.3046806810096032, 0.13668233147297137, 0.1931747108259616, 0.37752645472722535, 0.31618154708059465, 0.31835813584843375, 0.3070681730831166, 0.12402679428341545, 0.26495981942516666, 0.25188898472987253, 0.5385819495844237, 0.40165450812704007, 0.23932881991640842, 0.11965141563213433, 0.34184071432514285, 0.3160838217897127, 0.5338472984904024, 0.047572984733449286, 0.09039073281130205, 0.1537274380235698, 0.1289551537399951, 0.2604941529185929, 0.25706237822972466, 0.1845051531279127, 0.10652716324087819, 0.2708006776187757, 0.3414923847942186, 0.4123837225375905, 0.3117320514759382, 0.24748938072863658, 0.39906933520462307, 0.38488430703266197, 0.30616649005219543, 0.35800448698393594, 0.2925584450216504, 0.691927567575801, 0.6852837437185479, 0.7713371776842154, 0.6980978655290662, 0.766579905272989, 0.6382172121539862, 0.6996949068837017, 0.6411455169887086, 0.6813004968647007, 0.13780851187498355, 0.14510698635437513, 0.0893582233783049, 0.09991541502631718, 0.13054731281561616, 0.11683523087565595, 0.171925554807697, 0.12998368305350327, 0.1543780872863041, 0.23778094004152495, 0.19740299269447714, 0.19419993189472884, 0.6711147357652977, 0.3381930059883864, 0.16150820454378223, 0.2054840485150662, 0.16698240295904943, 0.2369198920033001, 0.5378973437045065, 0.5086990285195376, 0.4430302848859211, 0.5529629454086893, 0.5198127341548278, 0.5279513604494719, 0.5500605824796765, 0.6079135827937183, 0.6101637594254216, 0.39824080369013304, 0.4370748660527193, 0.3653791923313585, 0.4762702225417462, 0.462984548459363, 0.360263740881579, 0.47399412193575075, 0.4281155485172986, 0.4598926486463012, 0.2699804326748404, 0.20417958046377094, 0.19642411996639997, 0.2090269531837965, 0.22596432344098072, 0.21190214578886424, 0.2627536333413878, 0.22132815692849062, 0.2608405310025357, 0.2363027529318048, 0.23551312504404354, 0.243560947128306, 0.6850969382534209, 0.18536012491528941, 0.2458619905947247, 0.24381288780284605, 0.24263341472021616, 0.2027686971008097, 0.18286924219413314, 0.20516327697756553, 0.18575367079274585, 0.9095286514961691, 0.8528250005346223, 0.8450929708093164, 0.14315397106139793, 0.19657384656366028, 0.8524027151110926, 0.16612350803193054, 0.8531453508001282, 0.8783942091238557, 0.8155303269827223, 0.16507174910354772, 0.20270816914068102, 0.12735536938542336, 0.15526468014730144, 0.20877673505561833, 0.22523905812434164, 0.256192055388863, 0.2035935227993826, 0.2536636617248401, 0.3606807596988413, 0.2505651780761303, 0.22899606852656185, 0.2425765551210327, 0.2562714338840041, 0.09867543144495539, 0.1077639754617119, 0.10622902939542522, 0.08139414847363335, 0.1006091149598467, 0.10781530238189518, 0.10050249366479436, 0.09691080533906204, 0.08149992429255548]}, "mutation_prompt": null}
{"id": "cbd220ec-0a93-4b20-bd84-fc12b6133239", "solution": "import numpy as np\n\nclass EnhancedHybridStochasticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.initial_scale_factor = 0.8\n        self.initial_crossover_prob = 0.7\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population = self.opposition_based_initialization()\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.learning_rate = 0.1\n        self.elitism_rate = 0.1  # New elitism parameter\n\n    def opposition_based_initialization(self):\n        initial_population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        opposition_population = self.lower_bound + self.upper_bound - initial_population\n        return np.vstack((initial_population, opposition_population))[:self.population_size]\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def select_parents(self, idx):\n        indices = [i for i in range(self.population_size) if i != idx]\n        return np.random.choice(indices, 3, replace=False)\n\n    def differential_evolution_mutation(self, idx, scale_factor):\n        a, b, c = self.select_parents(idx)\n        mutant_vector = self.population[a] + scale_factor * (self.population[b] - self.population[c])\n        return np.clip(mutant_vector, self.lower_bound, self.upper_bound)\n\n    def crossover(self, target_vector, mutant_vector, crossover_prob):\n        crossover_mask = np.random.rand(self.dim) < crossover_prob\n        return np.where(crossover_mask, mutant_vector, target_vector)\n\n    def local_search(self, vector):\n        perturbation = np.random.normal(0, self.learning_rate, self.dim)\n        candidate_vector = vector + perturbation\n        return np.clip(candidate_vector, self.lower_bound, self.upper_bound)\n\n    def elitism_selection(self):\n        num_elites = max(1, int(self.elitism_rate * self.population_size))\n        elite_indices = np.argsort(self.fitness)[:num_elites]\n        return self.population[elite_indices], self.fitness[elite_indices]\n\n    def __call__(self, func):\n        self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            elites, elite_fitness = self.elitism_selection()\n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                scale_factor = self.initial_scale_factor * (1 - self.evaluations / self.budget)\n                crossover_prob = self.initial_crossover_prob + 0.2 * np.random.rand()\n                \n                if i < len(elites):  # Use elite individuals\n                    candidate_vector = self.local_search(elites[i])\n                else:\n                    mutant_vector = self.differential_evolution_mutation(i, scale_factor)\n                    candidate_vector = self.crossover(self.population[i], mutant_vector, crossover_prob)\n\n                candidate_fitness = func(candidate_vector)\n                self.evaluations += 1\n\n                if candidate_fitness < self.fitness[i]:\n                    self.population[i] = candidate_vector\n                    self.fitness[i] = candidate_fitness\n\n                self.learning_rate = max(0.01, self.learning_rate * (1 - 0.05 * np.random.rand()))\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx]", "name": "EnhancedHybridStochasticOptimizer", "description": "Enhanced hybrid optimizer with adaptive elitism and dynamic mutation for improved convergence speed.", "configspace": "", "generation": 71, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "085fc4fd-6092-49b6-946c-e88881c5d87c", "metadata": {"aucs": [0.8887079975314532, 0.894012818670537, 0.8991705287696641, 0.8978847561824669, 0.8879257415736936, 0.8940642586100076, 0.9007105063480211, 0.8958392425350181, 0.8969457975943529, 0.8044175229050868, 0.7853180510418849, 0.7770538765325782, 0.7824751042832793, 0.7991480344536278, 0.7859582873813178, 0.7829457001324069, 0.8073918603513864, 0.794782193201033, 0.6808600123402386, 0.1750883946278604, 0.14531540322259429, 0.1442044729706906, 0.18425100518796977, 0.6818733473800802, 0.6535836728714721, 0.18149935269683826, 0.18178474171291303, 0.17985980474169194, 0.15402982831947865, 0.15226864312698196, 0.173967907678645, 0.14621090429947348, 0.47097234153094814, 0.1410752174129266, 0.6696335857850447, 0.14569775079321112, 0.9826172320377552, 0.9666035752060869, 0.9829618007032487, 0.976950051717164, 0.9827548155568644, 0.9852053707001819, 0.9767881737976671, 0.9804489105964168, 0.9669130107270508, 0.6261081652649766, 0.6510922594882195, 0.6255531916869328, 0.6774833282904631, 0.5975973805993315, 0.5910302179938389, 0.6678501570106004, 0.6330163811357998, 0.5508126220258189, 0.8531774442531348, 0.8389810559862521, 0.8880414902745725, 0.8289173802432317, 0.8953502885124923, 0.8397824108196726, 0.8273523214295622, 0.8057815752417086, 0.8580295054264129, 0.4016755801839059, 0.26933131272005106, 0.22925181461595256, 0.13204420434091646, 0.39317336231664235, 0.2284734447615212, 0.3105063171146114, 0.27055107832358327, 0.2917673882193722, 0.38502853820393, 0.2123706550707669, 0.4034652264328532, 0.568732561497391, 0.36500406246735106, 0.19551716008178943, 0.24638934001327484, 0.5225532750228102, 0.3624738677767353, 0.2666496944974961, 0.3046806810096032, 0.13668233147297137, 0.1931747108259616, 0.37752645472722535, 0.31618154708059465, 0.31835813584843375, 0.3070681730831166, 0.12402679428341545, 0.26495981942516666, 0.25188898472987253, 0.5385819495844237, 0.40165450812704007, 0.23932881991640842, 0.11965141563213433, 0.34184071432514285, 0.3160838217897127, 0.5338472984904024, 0.047572984733449286, 0.09039073281130205, 0.1537274380235698, 0.1289551537399951, 0.2604941529185929, 0.25706237822972466, 0.1845051531279127, 0.10652716324087819, 0.2708006776187757, 0.3414923847942186, 0.4123837225375905, 0.3117320514759382, 0.24748938072863658, 0.39906933520462307, 0.38488430703266197, 0.30616649005219543, 0.35800448698393594, 0.2925584450216504, 0.691927567575801, 0.6852837437185479, 0.7713371776842154, 0.6980978655290662, 0.766579905272989, 0.6382172121539862, 0.6996949068837017, 0.6411455169887086, 0.6813004968647007, 0.13780851187498355, 0.14510698635437513, 0.0893582233783049, 0.09991541502631718, 0.13054731281561616, 0.11683523087565595, 0.171925554807697, 0.12998368305350327, 0.1543780872863041, 0.23778094004152495, 0.19740299269447714, 0.19419993189472884, 0.6711147357652977, 0.3381930059883864, 0.16150820454378223, 0.2054840485150662, 0.16698240295904943, 0.2369198920033001, 0.5378973437045065, 0.5086990285195376, 0.4430302848859211, 0.5529629454086893, 0.5198127341548278, 0.5279513604494719, 0.5500605824796765, 0.6079135827937183, 0.6101637594254216, 0.39824080369013304, 0.4370748660527193, 0.3653791923313585, 0.4762702225417462, 0.462984548459363, 0.360263740881579, 0.47399412193575075, 0.4281155485172986, 0.4598926486463012, 0.2699804326748404, 0.20417958046377094, 0.19642411996639997, 0.2090269531837965, 0.22596432344098072, 0.21190214578886424, 0.2627536333413878, 0.22132815692849062, 0.2608405310025357, 0.2363027529318048, 0.23551312504404354, 0.243560947128306, 0.6850969382534209, 0.18536012491528941, 0.2458619905947247, 0.24381288780284605, 0.24263341472021616, 0.2027686971008097, 0.18286924219413314, 0.20516327697756553, 0.18575367079274585, 0.9095286514961691, 0.8528250005346223, 0.8450929708093164, 0.14315397106139793, 0.19657384656366028, 0.8524027151110926, 0.16612350803193054, 0.8531453508001282, 0.8783942091238557, 0.8155303269827223, 0.16507174910354772, 0.20270816914068102, 0.12735536938542336, 0.15526468014730144, 0.20877673505561833, 0.22523905812434164, 0.256192055388863, 0.2035935227993826, 0.2536636617248401, 0.3606807596988413, 0.2505651780761303, 0.22899606852656185, 0.2425765551210327, 0.2562714338840041, 0.09867543144495539, 0.1077639754617119, 0.10622902939542522, 0.08139414847363335, 0.1006091149598467, 0.10781530238189518, 0.10050249366479436, 0.09691080533906204, 0.08149992429255548]}, "mutation_prompt": null}
{"id": "737f3193-b8d0-4734-ae36-c7679ce85645", "solution": "import numpy as np\n\nclass EnhancedHybridStochasticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.initial_scale_factor = 0.85  # Adjusted to increase exploration\n        self.initial_crossover_prob = 0.75  # Slightly increased for better diversity\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population = self.opposition_based_initialization()\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.learning_rate = 0.1\n        self.elitism_rate = 0.15  # Increased to consider more elite solutions\n\n    def opposition_based_initialization(self):\n        initial_population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        opposition_population = self.lower_bound + self.upper_bound - initial_population\n        return np.vstack((initial_population, opposition_population))[:self.population_size]\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def select_parents(self, idx):\n        indices = [i for i in range(self.population_size) if i != idx]\n        return np.random.choice(indices, 3, replace=False)\n\n    def differential_evolution_mutation(self, idx, scale_factor):\n        a, b, c = self.select_parents(idx)\n        mutant_vector = self.population[a] + scale_factor * (self.population[b] - self.population[c])\n        return np.clip(mutant_vector, self.lower_bound, self.upper_bound)\n\n    def crossover(self, target_vector, mutant_vector, crossover_prob):\n        crossover_mask = np.random.rand(self.dim) < crossover_prob\n        return np.where(crossover_mask, mutant_vector, target_vector)\n\n    def local_search(self, vector):\n        perturbation = np.random.normal(0, self.learning_rate, self.dim)\n        candidate_vector = vector + perturbation\n        return np.clip(candidate_vector, self.lower_bound, self.upper_bound)\n\n    def elitism_selection(self):\n        num_elites = max(1, int(self.elitism_rate * self.population_size))\n        elite_indices = np.argsort(self.fitness)[:num_elites]\n        return self.population[elite_indices], self.fitness[elite_indices]\n\n    def __call__(self, func):\n        self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            elites, elite_fitness = self.elitism_selection()\n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                scale_factor = self.initial_scale_factor * (1 - self.evaluations / self.budget)\n                crossover_prob = self.initial_crossover_prob + 0.1 * np.random.rand()\n\n                if i < len(elites):  # Use elite individuals\n                    candidate_vector = self.local_search(elites[i])\n                else:\n                    mutant_vector = self.differential_evolution_mutation(i, scale_factor)\n                    candidate_vector = self.crossover(self.population[i], mutant_vector, crossover_prob)\n\n                candidate_fitness = func(candidate_vector)\n                self.evaluations += 1\n\n                if candidate_fitness < self.fitness[i]:\n                    self.population[i] = candidate_vector\n                    self.fitness[i] = candidate_fitness\n\n                self.learning_rate = max(0.01, self.learning_rate * (1 - 0.05 * np.random.rand()))\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx]", "name": "EnhancedHybridStochasticOptimizer", "description": "Enhanced hybrid optimizer with adaptive elitism, dynamic mutation, and energy-efficient step size for improved convergence speed and exploration.", "configspace": "", "generation": 72, "fitness": 0.4022621807107765, "feedback": "The algorithm EnhancedHybridStochasticOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.40 with standard deviation 0.27.", "error": "", "parent_id": "085fc4fd-6092-49b6-946c-e88881c5d87c", "metadata": {"aucs": [0.8981873531070977, 0.8856839214617668, 0.8913768565644348, 0.8897259671612643, 0.8769624890372131, 0.8933942544381991, 0.8890944245914307, 0.879457018898699, 0.8889513082857478, 0.8045403877581424, 0.7571250465587824, 0.7627015298896471, 0.7587427100031012, 0.7677619451656602, 0.7601473062189242, 0.7656488433457358, 0.7786726068472294, 0.7609563804224639, 0.18462816277927974, 0.7436062611414311, 0.7930186447186277, 0.16092259172043577, 0.11553921628836816, 0.15703776375791134, 0.15580076434704682, 0.14400997809478155, 0.14207164518415916, 0.15404597401974207, 0.15238232426537723, 0.15813161945326681, 0.17930477126017064, 0.1783381432969543, 0.14140405037968462, 0.15004438832690614, 0.6684164776775028, 0.1548955233180943, 0.9758564390859089, 0.9711564551564769, 0.9917785794092839, 0.9929270697346151, 0.993773128736597, 0.9880926909272743, 0.9818932211910814, 0.9902288892333709, 0.9698995164598064, 0.6495164279020769, 0.6109045789302332, 0.6258882410687507, 0.6764590497151747, 0.6627793962564024, 0.6286766470446166, 0.5815287261273129, 0.5701796206332186, 0.631510908602921, 0.37620333506360837, 0.224673528378176, 0.22521461452542968, 0.8841753699354504, 0.8713437865368694, 0.855330651703945, 0.8409011502538937, 0.39613626056103257, 0.379754572420943, 0.4275100714377559, 0.1311932706404234, 0.24800382886437589, 0.23682342618024455, 0.1330636624862478, 0.49249895943115496, 0.2932938769504172, 0.2782963120000108, 0.6106652978091494, 0.3576085830502672, 0.18404588881628803, 0.18832921256748214, 0.25223648873158755, 0.2064798897328678, 0.40583366109670915, 0.47380340690502554, 0.25399301957278286, 0.23639747205992967, 0.25257997423527745, 0.08507817209722512, 0.2737694414537679, 0.048562636491779054, 0.34495577005920197, 0.07013468368081144, 0.3077085564163835, 0.26306076560830016, 0.33108572052997576, 0.2836694458089879, 0.4851498162713368, 0.2915301998568619, 0.24990465006293816, 0.22107397439978704, 0.1877805084815064, 0.28210674589103935, 0.2991899298037499, 0.24733704848993643, 0.14431467339220982, 0.09220044344513478, 0.08370241929342848, 0.2527459702309802, 0.13693500961599925, 0.20060072512516758, 0.2528545565158954, 0.1817846400756632, 0.10443568153945348, 0.34908181385381, 0.2951461372365587, 0.3193745510333972, 0.35650954281270786, 0.3520462266453932, 0.3270214698548878, 0.35689830057082983, 0.27992673410989855, 0.3352015694642191, 0.6338409281712134, 0.6304395442747845, 0.6554076017825377, 0.6856928294400892, 0.6275634178183778, 0.6239875904865979, 0.6408491985054825, 0.6912401941877921, 0.7296477565713193, 0.11370654975248562, 0.12411383215064908, 0.10993320716058474, 0.13276292300988612, 0.12411077896431433, 0.12383478459873676, 0.11189295301801505, 0.11719817601312588, 0.6646116725273936, 0.16171993599109014, 0.16188383908091386, 0.1850216163131123, 0.34021731029244906, 0.23999407128913108, 0.2373697075884169, 0.19084177752462406, 0.14832330886618872, 0.5714794124287473, 0.48695107527258175, 0.5249869330345436, 0.46762047426466014, 0.5048400646634446, 0.48742467180711924, 0.4640936494895097, 0.6186596494384367, 0.5165942278829364, 0.5474285744185992, 0.426044362742398, 0.4060903385533349, 0.4163176691131588, 0.29671588581229724, 0.3738747746163853, 0.3477863764004008, 0.43369544169595275, 0.43554843965042456, 0.4631720870980286, 0.21795395698314934, 0.208934475282886, 0.19826569547720974, 0.2572544381976608, 0.23727927692649797, 0.20756037531161342, 0.20752300146614056, 0.29452204336375787, 0.24398987339518174, 0.24442071011260535, 0.22572171933791785, 0.2022800842092466, 0.7910820886891625, 0.1996567585423924, 0.8242813867037083, 0.2213038850656952, 0.20509399309138676, 0.20240160311397104, 0.20556275424415438, 0.8826365193547903, 0.18679510830769797, 0.8875388216915279, 0.19907851450496628, 0.1463329944394225, 0.169313803263195, 0.7798453130679013, 0.1962422480561663, 0.1550773854060702, 0.16566009999679476, 0.7985059589121186, 0.20550261724116692, 0.164895917284968, 0.16891684388732953, 0.21120681279663844, 0.8733282659418534, 0.12786230547261823, 0.3524728829448598, 0.23233965789282163, 0.38392548875238897, 0.30625995635268677, 0.4104814853288926, 0.21779336546000883, 0.23777556324113214, 0.47205374678928835, 0.3428921512255869, 0.09964068123191805, 0.10411851274785622, 0.10100941466599456, 0.09805236167598619, 0.09070601212597318, 0.11425755969670448, 0.07800647730994081, 0.08514622509867809, 0.10729588991820638]}, "mutation_prompt": null}
{"id": "70e1fbf7-5a7d-41b5-aaab-3c7abbe49c8f", "solution": "import numpy as np\n\nclass EnhancedHybridStochasticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.initial_scale_factor = 0.85  # Adjusted initial scale factor\n        self.initial_crossover_prob = 0.75  # Adjusted initial crossover probability\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population = self.opposition_based_initialization()\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.learning_rate = 0.12  # Adjusted learning rate\n        self.elitism_rate = 0.15  # Adjusted elitism parameter\n\n    def opposition_based_initialization(self):\n        initial_population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        opposition_population = self.lower_bound + self.upper_bound - initial_population\n        return np.vstack((initial_population, opposition_population))[:self.population_size]\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def select_parents(self, idx):\n        indices = [i for i in range(self.population_size) if i != idx]\n        return np.random.choice(indices, 3, replace=False)\n\n    def differential_evolution_mutation(self, idx, scale_factor):\n        a, b, c = self.select_parents(idx)\n        mutant_vector = self.population[a] + scale_factor * (self.population[b] - self.population[c])\n        return np.clip(mutant_vector, self.lower_bound, self.upper_bound)\n\n    def crossover(self, target_vector, mutant_vector, crossover_prob):\n        crossover_mask = np.random.rand(self.dim) < crossover_prob\n        return np.where(crossover_mask, mutant_vector, target_vector)\n\n    def local_search(self, vector):\n        perturbation = np.random.normal(0, self.learning_rate, self.dim)\n        candidate_vector = vector + perturbation\n        return np.clip(candidate_vector, self.lower_bound, self.upper_bound)\n\n    def elitism_selection(self):\n        num_elites = max(1, int(self.elitism_rate * self.population_size))\n        elite_indices = np.argsort(self.fitness)[:num_elites]\n        return self.population[elite_indices], self.fitness[elite_indices]\n\n    def adaptive_crossover(self, target_vector, mutant_vector):\n        adap_crossover_prob = self.initial_crossover_prob + 0.3 * np.random.rand()\n        return self.crossover(target_vector, mutant_vector, adap_crossover_prob)\n\n    def __call__(self, func):\n        self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            elites, elite_fitness = self.elitism_selection()\n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                scale_factor = self.initial_scale_factor * (1 - 0.8 * self.evaluations / self.budget)\n                \n                if i < len(elites):  # Use elite individuals\n                    candidate_vector = self.local_search(elites[i])\n                else:\n                    mutant_vector = self.differential_evolution_mutation(i, scale_factor)\n                    candidate_vector = self.adaptive_crossover(self.population[i], mutant_vector)\n\n                candidate_fitness = func(candidate_vector)\n                self.evaluations += 1\n\n                if candidate_fitness < self.fitness[i]:\n                    self.population[i] = candidate_vector\n                    self.fitness[i] = candidate_fitness\n\n                self.learning_rate = max(0.01, self.learning_rate * (1 - 0.03 * np.random.rand()))\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx]", "name": "EnhancedHybridStochasticOptimizer", "description": "Improved hybrid optimizer with adaptive scaling factors and diverse genetic operators for enhanced convergence speed.", "configspace": "", "generation": 73, "fitness": 0.4185364251982713, "feedback": "The algorithm EnhancedHybridStochasticOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.42 with standard deviation 0.28.", "error": "", "parent_id": "085fc4fd-6092-49b6-946c-e88881c5d87c", "metadata": {"aucs": [0.8697832553904019, 0.881434627001956, 0.8767147123093004, 0.8847722435763111, 0.8856234492787147, 0.8809771089045898, 0.8899577294394249, 0.8914369118405192, 0.8942951363977706, 0.7131519916201217, 0.7611118642779549, 0.7683449409601697, 0.7374446752068626, 0.7640062005644452, 0.7599019048720898, 0.7624681951348565, 0.6851760389939107, 0.781791749021677, 0.15129607217096785, 0.14286905909958403, 0.1823621391180511, 0.1553784193201453, 0.18177221993276926, 0.15477351312653131, 0.12420212011152121, 0.14487625052001096, 0.11265813422621018, 0.1751308176979961, 0.11444824893615146, 0.151161235812298, 0.11111076417178778, 0.15579757565275665, 0.12082898028023192, 0.11675182690809394, 0.11507812689502117, 0.11475004651766008, 0.9852062512299572, 0.9301411927147476, 0.9709156353418325, 0.9566367853677047, 0.9657807849678784, 0.9758071380929134, 0.9717082230984063, 0.9797496019863757, 0.9502401240616334, 0.7204640834540862, 0.662706308150468, 0.724443535193755, 0.6612622882792439, 0.6732979584741896, 0.6037436729411381, 0.7229148487321667, 0.5806195906492322, 0.5903059655454715, 0.24124162010088213, 0.8362802260638592, 0.889467713324161, 0.8831955796783559, 0.21213898261274078, 0.21140892919165644, 0.8889684380387961, 0.24489628881417824, 0.8495744127408382, 0.12767690601059145, 0.6319458801122129, 0.13340717140505987, 0.3455780994019285, 0.42388452241560304, 0.7514941695847521, 0.13246444260331502, 0.6879690784294679, 0.43876266657185714, 0.6068079472547009, 0.1300596158880618, 0.7247439247455087, 0.6929526958247745, 0.35834125504216974, 0.6502865027787827, 0.47327553842009296, 0.3101636407911813, 0.7304923593887813, 0.40940816115048084, 0.19667824514817067, 0.22381133273800802, 0.5092622430358952, 0.3578506973862764, 0.09456127419076077, 0.4277916196771361, 0.4660873520091122, 0.3336750632343479, 0.19900424159456676, 0.28041065375074337, 0.2229481110821696, 0.7309454835792404, 0.49413602005349244, 0.527280026929691, 0.49248483832708057, 0.7339091555551196, 0.40634651559658275, 0.10457498377054164, 0.041321561564121256, 0.343973999474661, 0.29178498273740083, 0.08014375847186872, 0.3136764719649494, 0.1640566589461323, 0.08928459834747338, 0.12069853276598419, 0.3793129888037382, 0.3516111878646564, 0.3019112643122309, 0.42397628417778976, 0.3998521569712027, 0.4134083300021012, 0.3691075032579091, 0.3805020292575545, 0.3535561690446659, 0.7268237805820886, 0.8302460122411877, 0.8187938404678399, 0.7386817109222701, 0.8158954229125973, 0.7564743591125608, 0.8125136283768019, 0.8212432425623012, 0.7854620762620255, 0.14116237214276894, 0.11701643369336778, 0.11127217894782604, 0.1876782315298423, 0.08817416406059675, 0.1317128584775974, 0.0990763494342144, 0.11758609710715118, 0.1804505434513226, 0.1403594447780484, 0.7235792448313179, 0.2756183447630146, 0.7493507645676915, 0.22862163394732882, 0.19239286896165642, 0.27667163020817076, 0.2123270298519364, 0.2271511300300052, 0.3631581287497424, 0.3493343930678231, 0.4990461768906719, 0.5499823021487432, 0.5327069674846887, 0.46238890785118414, 0.647931066412357, 0.5443589745636346, 0.5720390597331622, 0.4531825622176894, 0.17370529618315578, 0.2434910019869576, 0.4409978666954807, 0.21005346819969928, 0.3629556369473813, 0.19454114827280922, 0.4899209115693738, 0.4576367475009946, 0.2226895089034313, 0.21786648776942052, 0.21780406972825594, 0.2230992327847956, 0.19027082408519436, 0.27800230576315155, 0.1855875571410115, 0.18818862262293534, 0.19906369019899006, 0.21876426302194873, 0.2520674699373472, 0.2269971010096986, 0.24863179051331907, 0.20841844033620405, 0.17783255847212653, 0.2255801471956348, 0.17918275801306094, 0.7438868684170346, 0.8932049828846043, 0.17153328135732826, 0.15365868760490753, 0.16728466548318632, 0.19911303306590356, 0.19528090284445487, 0.14099013361811763, 0.1661278581366742, 0.15662638089231007, 0.154351411479432, 0.16803919611472884, 0.88167729918539, 0.8832702433358421, 0.16803874326245727, 0.20818138960161403, 0.2125758319468064, 0.21275444409623467, 0.1553865479500831, 0.2583685971452252, 0.3701108334767238, 0.2641451920462212, 0.476446605457291, 0.4024455140460753, 0.32381229346065643, 0.4827501010091775, 0.42361976294500436, 0.4865313142888965, 0.10751625992439728, 0.10907735950455077, 0.10090764996963375, 0.10309559968026971, 0.11812816755161015, 0.09986381534048883, 0.10164259035134182, 0.07550005731516085, 0.07770601226293805]}, "mutation_prompt": null}
{"id": "c436d3da-0b59-4bb8-9f28-18dbd610aea9", "solution": "import numpy as np\n\nclass ImprovedStochasticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 22  # Slightly increased population size\n        self.initial_scale_factor = 0.85  # Adjusted scale factor\n        self.initial_crossover_prob = 0.75  # Adjusted crossover probability\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population = self.opposition_based_initialization()\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.learning_rate = 0.12  # Adjusted learning rate\n        self.elitism_rate = 0.15  # Adjusted elitism rate\n\n    def opposition_based_initialization(self):\n        initial_population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        opposition_population = self.lower_bound + self.upper_bound - initial_population\n        return np.vstack((initial_population, opposition_population))[:self.population_size]\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def select_parents(self, idx):\n        indices = [i for i in range(self.population_size) if i != idx]\n        return np.random.choice(indices, 3, replace=False)\n\n    def differential_evolution_mutation(self, idx, scale_factor):\n        a, b, c = self.select_parents(idx)\n        mutant_vector = self.population[a] + scale_factor * (self.population[b] - self.population[c])\n        return np.clip(mutant_vector, self.lower_bound, self.upper_bound)\n\n    def crossover(self, target_vector, mutant_vector, crossover_prob):\n        crossover_mask = np.random.rand(self.dim) < crossover_prob\n        return np.where(crossover_mask, mutant_vector, target_vector)\n\n    def local_search(self, vector):\n        perturbation = np.random.normal(0, self.learning_rate, self.dim)\n        candidate_vector = vector + perturbation\n        return np.clip(candidate_vector, self.lower_bound, self.upper_bound)\n\n    def elitism_selection(self):\n        num_elites = max(1, int(self.elitism_rate * self.population_size))\n        elite_indices = np.argsort(self.fitness)[:num_elites]\n        return self.population[elite_indices], self.fitness[elite_indices]\n\n    def __call__(self, func):\n        self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            elites, elite_fitness = self.elitism_selection()\n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                scale_factor = self.initial_scale_factor * (1 - self.evaluations / self.budget)\n                crossover_prob = self.initial_crossover_prob + 0.15 * np.random.rand()  # Slightly reduced randomness\n\n                if i < len(elites):\n                    candidate_vector = self.local_search(elites[i])\n                else:\n                    mutant_vector = self.differential_evolution_mutation(i, scale_factor)\n                    candidate_vector = self.crossover(self.population[i], mutant_vector, crossover_prob)\n\n                candidate_fitness = func(candidate_vector)\n                self.evaluations += 1\n\n                if candidate_fitness < self.fitness[i]:\n                    self.population[i] = candidate_vector\n                    self.fitness[i] = candidate_fitness\n\n                self.learning_rate = max(0.01, self.learning_rate * (1 - 0.03 * np.random.rand()))  # Adjusted decay\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx]", "name": "ImprovedStochasticOptimizer", "description": "Improved adaptive elitism and dynamic learning rate adjustment for faster convergence in stochastic optimization.", "configspace": "", "generation": 74, "fitness": 0.4017800156270126, "feedback": "The algorithm ImprovedStochasticOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.40 with standard deviation 0.27.", "error": "", "parent_id": "085fc4fd-6092-49b6-946c-e88881c5d87c", "metadata": {"aucs": [0.8699592938530898, 0.8830566743656805, 0.8741772806711373, 0.8843145766053552, 0.8852547534252593, 0.8709976501842557, 0.8800227359094464, 0.8753040827718008, 0.8789355715186581, 0.7589498308257073, 0.7576439906399224, 0.7633172414614136, 0.7512799734549905, 0.7415585090900607, 0.7511672858564904, 0.7530905638856289, 0.7555350319401504, 0.7526369495734656, 0.1572964380402756, 0.18002047414445022, 0.16130011768983565, 0.7492620172536666, 0.18000936575659066, 0.6961402339164364, 0.13838044120555282, 0.15143743561449263, 0.15607417146699631, 0.17117562607295078, 0.1610730690540404, 0.1469755265348276, 0.17809629825178286, 0.15848590156110554, 0.15438612000878837, 0.10873630849173155, 0.1785734134227427, 0.1567762773100645, 0.9751978824072496, 0.9733521986730215, 0.9627966357426418, 0.9783766802886295, 0.9833885281019948, 0.9810789281889959, 0.9649227427002608, 0.976919609408511, 0.9834320097245461, 0.5848080786622932, 0.5266445974950272, 0.6766936449195949, 0.615881896476967, 0.6259013285940438, 0.6258119718078088, 0.5799552489938047, 0.5785115489959569, 0.5757634791402438, 0.8786504022843505, 0.8377487231967882, 0.8564098409997511, 0.20731573508268086, 0.8543649526237754, 0.19211702625956972, 0.8388593521756307, 0.3887426544571878, 0.23438799317801073, 0.5407053809875779, 0.43700405534832076, 0.13119742839663628, 0.45160455704745806, 0.3006337666403327, 0.23496453539847528, 0.27603401537682115, 0.44833842708202665, 0.23140215290771482, 0.30593304107843355, 0.13256581492506903, 0.3571397463615126, 0.36126658118368726, 0.2522281578530192, 0.2645321666625323, 0.23676924516685793, 0.3844582510325114, 0.2652866053731686, 0.37143999484448587, 0.1303216228091576, 0.129883812478913, 0.2244247365145815, 0.20308884698249152, 0.3557353718890268, 0.4288190723860327, 0.19260756521007705, 0.43898425476832326, 0.13218853075983628, 0.22675025307011143, 0.14333796326997417, 0.3461459169211788, 0.3279057541318351, 0.1541276887548032, 0.4815523920270335, 0.6551624487041587, 0.5498170084014706, 0.05493491593926736, 0.110996557977048, 0.07822358383790728, 0.1370296167311994, 0.1378250414757386, 0.18041831962573962, 0.2496374169334885, 0.16121449250323827, 0.11162848818821436, 0.36868108268395494, 0.3358104917676089, 0.39731037183741447, 0.3595664866145386, 0.38258194215162633, 0.37343727541797866, 0.31163261177859236, 0.3314642923836918, 0.3113306658393499, 0.7920158974840502, 0.6858125187473519, 0.7540584522841203, 0.6957542863817326, 0.6634611894490345, 0.7029267819099332, 0.6619917460556136, 0.7469473780944988, 0.7666974360049247, 0.16668114958615787, 0.12322984177051499, 0.12321123374851961, 0.18739470355030285, 0.13903445720459207, 0.13176352050113327, 0.15007705869831622, 0.10690709458022152, 0.13231034458611024, 0.23711598295464487, 0.15547502688732728, 0.3063016635296387, 0.6578286887115944, 0.15018847768982746, 0.22207661188346817, 0.27186591340596933, 0.2186874997793301, 0.2849241057597218, 0.47589239599914246, 0.5729741019497979, 0.4844313476475598, 0.5104432812919609, 0.6152984267540433, 0.528243840063229, 0.5504245481222748, 0.5389270608866708, 0.5274184690015243, 0.3159086662501699, 0.39278655474184765, 0.3594461483450595, 0.43386139249561073, 0.4103513868429206, 0.4616525707274324, 0.4200551972312183, 0.4503686753354781, 0.22211737528182074, 0.20455013068369665, 0.22338199396260594, 0.23240462975776444, 0.2864691635538982, 0.22447441379928834, 0.21751012495176114, 0.2614814928261211, 0.21312896781492086, 0.20452026400847922, 0.21699192763418695, 0.2450429860986486, 0.24678974647822616, 0.23711042734297916, 0.24266798904917264, 0.7456553665163288, 0.7482088570463019, 0.22071191026623538, 0.6794371954932144, 0.16644833165637696, 0.18226746200802368, 0.15305828886816053, 0.17633804715326895, 0.1983820092650057, 0.8628331717887705, 0.16210942198016076, 0.1677394824678955, 0.14085239380759762, 0.1683168962730549, 0.21070428606217595, 0.20718040724968445, 0.20429160394601886, 0.20873135016007138, 0.9095051713083269, 0.8675882856231135, 0.21104253194611555, 0.212243360866482, 0.2042093972304574, 0.38197314444855046, 0.2750059465646968, 0.3660884005318693, 0.23439737850894493, 0.25677348646948284, 0.25283780550373414, 0.44347963869473817, 0.25820221771661045, 0.0988275258161162, 0.11543410486109773, 0.09765544277598714, 0.11288508505506456, 0.11220056636091724, 0.08203858291452304, 0.10708504292978782, 0.08954431567020638, 0.10281896462094464]}, "mutation_prompt": null}
{"id": "7a284f48-3ce5-42b2-bc53-82d61767a3da", "solution": "import numpy as np\n\nclass EnhancedHybridStochasticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.initial_scale_factor = 0.8\n        self.initial_crossover_prob = 0.7\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population = self.opposition_based_initialization()\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.learning_rate = 0.1\n        self.elitism_rate = 0.1  # New elitism parameter\n\n    def opposition_based_initialization(self):\n        initial_population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        opposition_population = self.lower_bound + self.upper_bound - initial_population\n        return np.vstack((initial_population, opposition_population))[:self.population_size]\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def select_parents(self, idx):\n        indices = [i for i in range(self.population_size) if i != idx]\n        return np.random.choice(indices, 3, replace=False)\n\n    def differential_evolution_mutation(self, idx, scale_factor):\n        a, b, c = self.select_parents(idx)\n        mutant_vector = self.population[a] + scale_factor * (self.population[b] - self.population[c])\n        return np.clip(mutant_vector, self.lower_bound, self.upper_bound)\n\n    def crossover(self, target_vector, mutant_vector, crossover_prob):\n        crossover_mask = np.random.rand(self.dim) < crossover_prob\n        return np.where(crossover_mask, mutant_vector, target_vector)\n\n    def local_search(self, vector):\n        perturbation = np.random.normal(0, self.learning_rate, self.dim)\n        candidate_vector = vector + perturbation\n        return np.clip(candidate_vector, self.lower_bound, self.upper_bound)\n\n    def elitism_selection(self):\n        num_elites = max(1, int(self.elitism_rate * self.population_size))\n        elite_indices = np.argsort(self.fitness)[:num_elites]\n        return self.population[elite_indices], self.fitness[elite_indices]\n\n    def __call__(self, func):\n        self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            elites, elite_fitness = self.elitism_selection()\n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                scale_factor = self.initial_scale_factor * (1 - self.evaluations / self.budget)\n                crossover_prob = self.initial_crossover_prob + 0.2 * np.random.rand()\n                \n                if i < len(elites):  # Use elite individuals\n                    candidate_vector = self.local_search(elites[i])\n                else:\n                    mutant_vector = self.differential_evolution_mutation(i, scale_factor)\n                    candidate_vector = self.crossover(self.population[i], mutant_vector, crossover_prob)\n\n                candidate_fitness = func(candidate_vector)\n                self.evaluations += 1\n\n                if candidate_fitness < self.fitness[i]:\n                    self.population[i] = candidate_vector\n                    self.fitness[i] = candidate_fitness\n\n                self.learning_rate = max(0.01, self.learning_rate * (1 - 0.05 * np.random.rand()))\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx]", "name": "EnhancedHybridStochasticOptimizer", "description": "Enhanced hybrid optimizer with adaptive elitism and dynamic mutation for improved convergence speed.", "configspace": "", "generation": 71, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "085fc4fd-6092-49b6-946c-e88881c5d87c", "metadata": {"aucs": [0.8887079975314532, 0.894012818670537, 0.8991705287696641, 0.8978847561824669, 0.8879257415736936, 0.8940642586100076, 0.9007105063480211, 0.8958392425350181, 0.8969457975943529, 0.8044175229050868, 0.7853180510418849, 0.7770538765325782, 0.7824751042832793, 0.7991480344536278, 0.7859582873813178, 0.7829457001324069, 0.8073918603513864, 0.794782193201033, 0.6808600123402386, 0.1750883946278604, 0.14531540322259429, 0.1442044729706906, 0.18425100518796977, 0.6818733473800802, 0.6535836728714721, 0.18149935269683826, 0.18178474171291303, 0.17985980474169194, 0.15402982831947865, 0.15226864312698196, 0.173967907678645, 0.14621090429947348, 0.47097234153094814, 0.1410752174129266, 0.6696335857850447, 0.14569775079321112, 0.9826172320377552, 0.9666035752060869, 0.9829618007032487, 0.976950051717164, 0.9827548155568644, 0.9852053707001819, 0.9767881737976671, 0.9804489105964168, 0.9669130107270508, 0.6261081652649766, 0.6510922594882195, 0.6255531916869328, 0.6774833282904631, 0.5975973805993315, 0.5910302179938389, 0.6678501570106004, 0.6330163811357998, 0.5508126220258189, 0.8531774442531348, 0.8389810559862521, 0.8880414902745725, 0.8289173802432317, 0.8953502885124923, 0.8397824108196726, 0.8273523214295622, 0.8057815752417086, 0.8580295054264129, 0.4016755801839059, 0.26933131272005106, 0.22925181461595256, 0.13204420434091646, 0.39317336231664235, 0.2284734447615212, 0.3105063171146114, 0.27055107832358327, 0.2917673882193722, 0.38502853820393, 0.2123706550707669, 0.4034652264328532, 0.568732561497391, 0.36500406246735106, 0.19551716008178943, 0.24638934001327484, 0.5225532750228102, 0.3624738677767353, 0.2666496944974961, 0.3046806810096032, 0.13668233147297137, 0.1931747108259616, 0.37752645472722535, 0.31618154708059465, 0.31835813584843375, 0.3070681730831166, 0.12402679428341545, 0.26495981942516666, 0.25188898472987253, 0.5385819495844237, 0.40165450812704007, 0.23932881991640842, 0.11965141563213433, 0.34184071432514285, 0.3160838217897127, 0.5338472984904024, 0.047572984733449286, 0.09039073281130205, 0.1537274380235698, 0.1289551537399951, 0.2604941529185929, 0.25706237822972466, 0.1845051531279127, 0.10652716324087819, 0.2708006776187757, 0.3414923847942186, 0.4123837225375905, 0.3117320514759382, 0.24748938072863658, 0.39906933520462307, 0.38488430703266197, 0.30616649005219543, 0.35800448698393594, 0.2925584450216504, 0.691927567575801, 0.6852837437185479, 0.7713371776842154, 0.6980978655290662, 0.766579905272989, 0.6382172121539862, 0.6996949068837017, 0.6411455169887086, 0.6813004968647007, 0.13780851187498355, 0.14510698635437513, 0.0893582233783049, 0.09991541502631718, 0.13054731281561616, 0.11683523087565595, 0.171925554807697, 0.12998368305350327, 0.1543780872863041, 0.23778094004152495, 0.19740299269447714, 0.19419993189472884, 0.6711147357652977, 0.3381930059883864, 0.16150820454378223, 0.2054840485150662, 0.16698240295904943, 0.2369198920033001, 0.5378973437045065, 0.5086990285195376, 0.4430302848859211, 0.5529629454086893, 0.5198127341548278, 0.5279513604494719, 0.5500605824796765, 0.6079135827937183, 0.6101637594254216, 0.39824080369013304, 0.4370748660527193, 0.3653791923313585, 0.4762702225417462, 0.462984548459363, 0.360263740881579, 0.47399412193575075, 0.4281155485172986, 0.4598926486463012, 0.2699804326748404, 0.20417958046377094, 0.19642411996639997, 0.2090269531837965, 0.22596432344098072, 0.21190214578886424, 0.2627536333413878, 0.22132815692849062, 0.2608405310025357, 0.2363027529318048, 0.23551312504404354, 0.243560947128306, 0.6850969382534209, 0.18536012491528941, 0.2458619905947247, 0.24381288780284605, 0.24263341472021616, 0.2027686971008097, 0.18286924219413314, 0.20516327697756553, 0.18575367079274585, 0.9095286514961691, 0.8528250005346223, 0.8450929708093164, 0.14315397106139793, 0.19657384656366028, 0.8524027151110926, 0.16612350803193054, 0.8531453508001282, 0.8783942091238557, 0.8155303269827223, 0.16507174910354772, 0.20270816914068102, 0.12735536938542336, 0.15526468014730144, 0.20877673505561833, 0.22523905812434164, 0.256192055388863, 0.2035935227993826, 0.2536636617248401, 0.3606807596988413, 0.2505651780761303, 0.22899606852656185, 0.2425765551210327, 0.2562714338840041, 0.09867543144495539, 0.1077639754617119, 0.10622902939542522, 0.08139414847363335, 0.1006091149598467, 0.10781530238189518, 0.10050249366479436, 0.09691080533906204, 0.08149992429255548]}, "mutation_prompt": null}
{"id": "9f4cf437-4084-4bc5-a1a4-9a0a17046404", "solution": "import numpy as np\n\nclass EnhancedHybridStochasticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.initial_scale_factor = 0.8\n        self.initial_crossover_prob = 0.7\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population = self.opposition_based_initialization()\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.learning_rate = 0.1\n        self.elitism_rate = 0.1  # New elitism parameter\n\n    def opposition_based_initialization(self):\n        initial_population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        opposition_population = self.lower_bound + self.upper_bound - initial_population\n        return np.vstack((initial_population, opposition_population))[:self.population_size]\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def select_parents(self, idx):\n        indices = [i for i in range(self.population_size) if i != idx]\n        return np.random.choice(indices, 3, replace=False)\n\n    def differential_evolution_mutation(self, idx, scale_factor):\n        a, b, c = self.select_parents(idx)\n        mutant_vector = self.population[a] + scale_factor * (self.population[b] - self.population[c])\n        return np.clip(mutant_vector, self.lower_bound, self.upper_bound)\n\n    def crossover(self, target_vector, mutant_vector, crossover_prob):\n        crossover_mask = np.random.rand(self.dim) < crossover_prob\n        return np.where(crossover_mask, mutant_vector, target_vector)\n\n    def local_search(self, vector):\n        perturbation = np.random.normal(0, self.learning_rate, self.dim)\n        candidate_vector = vector + perturbation\n        return np.clip(candidate_vector, self.lower_bound, self.upper_bound)\n\n    def elitism_selection(self):\n        num_elites = max(1, int(self.elitism_rate * self.population_size))\n        elite_indices = np.argsort(self.fitness)[:num_elites]\n        return self.population[elite_indices], self.fitness[elite_indices]\n\n    def __call__(self, func):\n        self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            elites, elite_fitness = self.elitism_selection()\n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                scale_factor = self.initial_scale_factor * (1 - self.evaluations / self.budget)\n                crossover_prob = self.initial_crossover_prob + 0.2 * np.random.rand()\n                \n                if i < len(elites):  # Use elite individuals\n                    candidate_vector = self.local_search(elites[i])\n                else:\n                    mutant_vector = self.differential_evolution_mutation(i, scale_factor)\n                    candidate_vector = self.crossover(self.population[i], mutant_vector, crossover_prob)\n\n                candidate_fitness = func(candidate_vector)\n                self.evaluations += 1\n\n                if candidate_fitness < self.fitness[i]:\n                    self.population[i] = candidate_vector\n                    self.fitness[i] = candidate_fitness\n\n                self.learning_rate = max(0.01, self.learning_rate * (1 - 0.05 * np.random.rand()))\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx]", "name": "EnhancedHybridStochasticOptimizer", "description": "Enhanced hybrid optimizer with adaptive elitism and dynamic mutation for improved convergence speed.", "configspace": "", "generation": 71, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "085fc4fd-6092-49b6-946c-e88881c5d87c", "metadata": {"aucs": [0.8887079975314532, 0.894012818670537, 0.8991705287696641, 0.8978847561824669, 0.8879257415736936, 0.8940642586100076, 0.9007105063480211, 0.8958392425350181, 0.8969457975943529, 0.8044175229050868, 0.7853180510418849, 0.7770538765325782, 0.7824751042832793, 0.7991480344536278, 0.7859582873813178, 0.7829457001324069, 0.8073918603513864, 0.794782193201033, 0.6808600123402386, 0.1750883946278604, 0.14531540322259429, 0.1442044729706906, 0.18425100518796977, 0.6818733473800802, 0.6535836728714721, 0.18149935269683826, 0.18178474171291303, 0.17985980474169194, 0.15402982831947865, 0.15226864312698196, 0.173967907678645, 0.14621090429947348, 0.47097234153094814, 0.1410752174129266, 0.6696335857850447, 0.14569775079321112, 0.9826172320377552, 0.9666035752060869, 0.9829618007032487, 0.976950051717164, 0.9827548155568644, 0.9852053707001819, 0.9767881737976671, 0.9804489105964168, 0.9669130107270508, 0.6261081652649766, 0.6510922594882195, 0.6255531916869328, 0.6774833282904631, 0.5975973805993315, 0.5910302179938389, 0.6678501570106004, 0.6330163811357998, 0.5508126220258189, 0.8531774442531348, 0.8389810559862521, 0.8880414902745725, 0.8289173802432317, 0.8953502885124923, 0.8397824108196726, 0.8273523214295622, 0.8057815752417086, 0.8580295054264129, 0.4016755801839059, 0.26933131272005106, 0.22925181461595256, 0.13204420434091646, 0.39317336231664235, 0.2284734447615212, 0.3105063171146114, 0.27055107832358327, 0.2917673882193722, 0.38502853820393, 0.2123706550707669, 0.4034652264328532, 0.568732561497391, 0.36500406246735106, 0.19551716008178943, 0.24638934001327484, 0.5225532750228102, 0.3624738677767353, 0.2666496944974961, 0.3046806810096032, 0.13668233147297137, 0.1931747108259616, 0.37752645472722535, 0.31618154708059465, 0.31835813584843375, 0.3070681730831166, 0.12402679428341545, 0.26495981942516666, 0.25188898472987253, 0.5385819495844237, 0.40165450812704007, 0.23932881991640842, 0.11965141563213433, 0.34184071432514285, 0.3160838217897127, 0.5338472984904024, 0.047572984733449286, 0.09039073281130205, 0.1537274380235698, 0.1289551537399951, 0.2604941529185929, 0.25706237822972466, 0.1845051531279127, 0.10652716324087819, 0.2708006776187757, 0.3414923847942186, 0.4123837225375905, 0.3117320514759382, 0.24748938072863658, 0.39906933520462307, 0.38488430703266197, 0.30616649005219543, 0.35800448698393594, 0.2925584450216504, 0.691927567575801, 0.6852837437185479, 0.7713371776842154, 0.6980978655290662, 0.766579905272989, 0.6382172121539862, 0.6996949068837017, 0.6411455169887086, 0.6813004968647007, 0.13780851187498355, 0.14510698635437513, 0.0893582233783049, 0.09991541502631718, 0.13054731281561616, 0.11683523087565595, 0.171925554807697, 0.12998368305350327, 0.1543780872863041, 0.23778094004152495, 0.19740299269447714, 0.19419993189472884, 0.6711147357652977, 0.3381930059883864, 0.16150820454378223, 0.2054840485150662, 0.16698240295904943, 0.2369198920033001, 0.5378973437045065, 0.5086990285195376, 0.4430302848859211, 0.5529629454086893, 0.5198127341548278, 0.5279513604494719, 0.5500605824796765, 0.6079135827937183, 0.6101637594254216, 0.39824080369013304, 0.4370748660527193, 0.3653791923313585, 0.4762702225417462, 0.462984548459363, 0.360263740881579, 0.47399412193575075, 0.4281155485172986, 0.4598926486463012, 0.2699804326748404, 0.20417958046377094, 0.19642411996639997, 0.2090269531837965, 0.22596432344098072, 0.21190214578886424, 0.2627536333413878, 0.22132815692849062, 0.2608405310025357, 0.2363027529318048, 0.23551312504404354, 0.243560947128306, 0.6850969382534209, 0.18536012491528941, 0.2458619905947247, 0.24381288780284605, 0.24263341472021616, 0.2027686971008097, 0.18286924219413314, 0.20516327697756553, 0.18575367079274585, 0.9095286514961691, 0.8528250005346223, 0.8450929708093164, 0.14315397106139793, 0.19657384656366028, 0.8524027151110926, 0.16612350803193054, 0.8531453508001282, 0.8783942091238557, 0.8155303269827223, 0.16507174910354772, 0.20270816914068102, 0.12735536938542336, 0.15526468014730144, 0.20877673505561833, 0.22523905812434164, 0.256192055388863, 0.2035935227993826, 0.2536636617248401, 0.3606807596988413, 0.2505651780761303, 0.22899606852656185, 0.2425765551210327, 0.2562714338840041, 0.09867543144495539, 0.1077639754617119, 0.10622902939542522, 0.08139414847363335, 0.1006091149598467, 0.10781530238189518, 0.10050249366479436, 0.09691080533906204, 0.08149992429255548]}, "mutation_prompt": null}
{"id": "bc6201d9-1564-46d3-91cd-f1f3801056bb", "solution": "import numpy as np\n\nclass EnhancedHybridStochasticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.initial_scale_factor = 0.8\n        self.initial_crossover_prob = 0.7\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population = self.opposition_based_initialization()\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.learning_rate = 0.1\n        self.elitism_rate = 0.1  # New elitism parameter\n\n    def opposition_based_initialization(self):\n        initial_population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        opposition_population = self.lower_bound + self.upper_bound - initial_population\n        return np.vstack((initial_population, opposition_population))[:self.population_size]\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def select_parents(self, idx):\n        indices = [i for i in range(self.population_size) if i != idx]\n        return np.random.choice(indices, 3, replace=False)\n\n    def differential_evolution_mutation(self, idx, scale_factor):\n        a, b, c = self.select_parents(idx)\n        mutant_vector = self.population[a] + scale_factor * (self.population[b] - self.population[c])\n        return np.clip(mutant_vector, self.lower_bound, self.upper_bound)\n\n    def crossover(self, target_vector, mutant_vector, crossover_prob):\n        crossover_mask = np.random.rand(self.dim) < crossover_prob\n        return np.where(crossover_mask, mutant_vector, target_vector)\n\n    def local_search(self, vector):\n        perturbation = np.random.normal(0, self.learning_rate, self.dim)\n        candidate_vector = vector + perturbation\n        return np.clip(candidate_vector, self.lower_bound, self.upper_bound)\n\n    def elitism_selection(self):\n        num_elites = max(1, int(self.elitism_rate * self.population_size))\n        elite_indices = np.argsort(self.fitness)[:num_elites]\n        return self.population[elite_indices], self.fitness[elite_indices]\n\n    def __call__(self, func):\n        self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            elites, elite_fitness = self.elitism_selection()\n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                scale_factor = self.initial_scale_factor * (1 - self.evaluations / self.budget)\n                crossover_prob = self.initial_crossover_prob + 0.2 * np.random.rand()\n                \n                if i < len(elites):  # Use elite individuals\n                    candidate_vector = self.local_search(elites[i])\n                else:\n                    mutant_vector = self.differential_evolution_mutation(i, scale_factor)\n                    candidate_vector = self.crossover(self.population[i], mutant_vector, crossover_prob)\n\n                candidate_fitness = func(candidate_vector)\n                self.evaluations += 1\n\n                if candidate_fitness < self.fitness[i]:\n                    self.population[i] = candidate_vector\n                    self.fitness[i] = candidate_fitness\n\n                self.learning_rate = max(0.01, self.learning_rate * (1 - 0.05 * np.random.rand()))\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx]", "name": "EnhancedHybridStochasticOptimizer", "description": "Enhanced hybrid optimizer with adaptive elitism and dynamic mutation for improved convergence speed.", "configspace": "", "generation": 71, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "085fc4fd-6092-49b6-946c-e88881c5d87c", "metadata": {"aucs": [0.8887079975314532, 0.894012818670537, 0.8991705287696641, 0.8978847561824669, 0.8879257415736936, 0.8940642586100076, 0.9007105063480211, 0.8958392425350181, 0.8969457975943529, 0.8044175229050868, 0.7853180510418849, 0.7770538765325782, 0.7824751042832793, 0.7991480344536278, 0.7859582873813178, 0.7829457001324069, 0.8073918603513864, 0.794782193201033, 0.6808600123402386, 0.1750883946278604, 0.14531540322259429, 0.1442044729706906, 0.18425100518796977, 0.6818733473800802, 0.6535836728714721, 0.18149935269683826, 0.18178474171291303, 0.17985980474169194, 0.15402982831947865, 0.15226864312698196, 0.173967907678645, 0.14621090429947348, 0.47097234153094814, 0.1410752174129266, 0.6696335857850447, 0.14569775079321112, 0.9826172320377552, 0.9666035752060869, 0.9829618007032487, 0.976950051717164, 0.9827548155568644, 0.9852053707001819, 0.9767881737976671, 0.9804489105964168, 0.9669130107270508, 0.6261081652649766, 0.6510922594882195, 0.6255531916869328, 0.6774833282904631, 0.5975973805993315, 0.5910302179938389, 0.6678501570106004, 0.6330163811357998, 0.5508126220258189, 0.8531774442531348, 0.8389810559862521, 0.8880414902745725, 0.8289173802432317, 0.8953502885124923, 0.8397824108196726, 0.8273523214295622, 0.8057815752417086, 0.8580295054264129, 0.4016755801839059, 0.26933131272005106, 0.22925181461595256, 0.13204420434091646, 0.39317336231664235, 0.2284734447615212, 0.3105063171146114, 0.27055107832358327, 0.2917673882193722, 0.38502853820393, 0.2123706550707669, 0.4034652264328532, 0.568732561497391, 0.36500406246735106, 0.19551716008178943, 0.24638934001327484, 0.5225532750228102, 0.3624738677767353, 0.2666496944974961, 0.3046806810096032, 0.13668233147297137, 0.1931747108259616, 0.37752645472722535, 0.31618154708059465, 0.31835813584843375, 0.3070681730831166, 0.12402679428341545, 0.26495981942516666, 0.25188898472987253, 0.5385819495844237, 0.40165450812704007, 0.23932881991640842, 0.11965141563213433, 0.34184071432514285, 0.3160838217897127, 0.5338472984904024, 0.047572984733449286, 0.09039073281130205, 0.1537274380235698, 0.1289551537399951, 0.2604941529185929, 0.25706237822972466, 0.1845051531279127, 0.10652716324087819, 0.2708006776187757, 0.3414923847942186, 0.4123837225375905, 0.3117320514759382, 0.24748938072863658, 0.39906933520462307, 0.38488430703266197, 0.30616649005219543, 0.35800448698393594, 0.2925584450216504, 0.691927567575801, 0.6852837437185479, 0.7713371776842154, 0.6980978655290662, 0.766579905272989, 0.6382172121539862, 0.6996949068837017, 0.6411455169887086, 0.6813004968647007, 0.13780851187498355, 0.14510698635437513, 0.0893582233783049, 0.09991541502631718, 0.13054731281561616, 0.11683523087565595, 0.171925554807697, 0.12998368305350327, 0.1543780872863041, 0.23778094004152495, 0.19740299269447714, 0.19419993189472884, 0.6711147357652977, 0.3381930059883864, 0.16150820454378223, 0.2054840485150662, 0.16698240295904943, 0.2369198920033001, 0.5378973437045065, 0.5086990285195376, 0.4430302848859211, 0.5529629454086893, 0.5198127341548278, 0.5279513604494719, 0.5500605824796765, 0.6079135827937183, 0.6101637594254216, 0.39824080369013304, 0.4370748660527193, 0.3653791923313585, 0.4762702225417462, 0.462984548459363, 0.360263740881579, 0.47399412193575075, 0.4281155485172986, 0.4598926486463012, 0.2699804326748404, 0.20417958046377094, 0.19642411996639997, 0.2090269531837965, 0.22596432344098072, 0.21190214578886424, 0.2627536333413878, 0.22132815692849062, 0.2608405310025357, 0.2363027529318048, 0.23551312504404354, 0.243560947128306, 0.6850969382534209, 0.18536012491528941, 0.2458619905947247, 0.24381288780284605, 0.24263341472021616, 0.2027686971008097, 0.18286924219413314, 0.20516327697756553, 0.18575367079274585, 0.9095286514961691, 0.8528250005346223, 0.8450929708093164, 0.14315397106139793, 0.19657384656366028, 0.8524027151110926, 0.16612350803193054, 0.8531453508001282, 0.8783942091238557, 0.8155303269827223, 0.16507174910354772, 0.20270816914068102, 0.12735536938542336, 0.15526468014730144, 0.20877673505561833, 0.22523905812434164, 0.256192055388863, 0.2035935227993826, 0.2536636617248401, 0.3606807596988413, 0.2505651780761303, 0.22899606852656185, 0.2425765551210327, 0.2562714338840041, 0.09867543144495539, 0.1077639754617119, 0.10622902939542522, 0.08139414847363335, 0.1006091149598467, 0.10781530238189518, 0.10050249366479436, 0.09691080533906204, 0.08149992429255548]}, "mutation_prompt": null}
{"id": "ece51cbc-76db-4e5e-8cb0-b81f367172de", "solution": "import numpy as np\n\nclass EnhancedHybridStochasticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.initial_scale_factor = 0.7  # Adjusted scale factor\n        self.initial_crossover_prob = 0.8  # Adjusted crossover probability\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population = self.opposition_based_initialization()\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.learning_rate = 0.15  # Adjusted learning rate\n        self.elitism_rate = 0.15  # Adjusted elitism parameter\n\n    def opposition_based_initialization(self):\n        initial_population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        opposition_population = self.lower_bound + self.upper_bound - initial_population\n        return np.vstack((initial_population, opposition_population))[:self.population_size]\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def select_parents(self, idx):\n        indices = [i for i in range(self.population_size) if i != idx]\n        return np.random.choice(indices, 3, replace=False)\n\n    def differential_evolution_mutation(self, idx, scale_factor):\n        a, b, c = self.select_parents(idx)\n        mutant_vector = self.population[a] + scale_factor * (self.population[b] - self.population[c])\n        return np.clip(mutant_vector, self.lower_bound, self.upper_bound)\n\n    def crossover(self, target_vector, mutant_vector, crossover_prob):\n        crossover_mask = np.random.rand(self.dim) < crossover_prob\n        return np.where(crossover_mask, mutant_vector, target_vector)\n\n    def local_search(self, vector):\n        perturbation = np.random.normal(0, self.learning_rate, self.dim)\n        candidate_vector = vector + perturbation\n        return np.clip(candidate_vector, self.lower_bound, self.upper_bound)\n\n    def elitism_selection(self):\n        num_elites = max(1, int(self.elitism_rate * self.population_size))\n        elite_indices = np.argsort(self.fitness)[:num_elites]\n        return self.population[elite_indices], self.fitness[elite_indices]\n\n    def __call__(self, func):\n        self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            elites, elite_fitness = self.elitism_selection()\n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                scale_factor = self.initial_scale_factor * (1 - self.evaluations / self.budget)\n                crossover_prob = self.initial_crossover_prob + 0.1 * np.random.rand()\n                \n                if i < len(elites):  # Use elite individuals selectively\n                    candidate_vector = self.local_search(elites[i])\n                else:\n                    mutant_vector = self.differential_evolution_mutation(i, scale_factor)\n                    candidate_vector = self.crossover(self.population[i], mutant_vector, crossover_prob)\n\n                candidate_fitness = func(candidate_vector)\n                self.evaluations += 1\n\n                if candidate_fitness < self.fitness[i]:\n                    self.population[i] = candidate_vector\n                    self.fitness[i] = candidate_fitness\n\n                # More dynamic learning rate adjustment\n                self.learning_rate = max(0.01, self.learning_rate * (1 - 0.03 * np.random.rand()))\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx]", "name": "EnhancedHybridStochasticOptimizer", "description": "Enhanced hybrid optimizer with adaptive learning rates and selective elitism for improved convergence speed.", "configspace": "", "generation": 78, "fitness": 0.37470596536089784, "feedback": "The algorithm EnhancedHybridStochasticOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.37 with standard deviation 0.29.", "error": "", "parent_id": "085fc4fd-6092-49b6-946c-e88881c5d87c", "metadata": {"aucs": [0.9115663563061961, 0.9083966732530152, 0.9069220351620946, 0.9152786192807711, 0.9035707832027582, 0.8794675150962628, 0.9150984638276153, 0.9191996725834611, 0.9159253433901289, 0.7922500589666079, 0.8206300255574914, 0.7867586893359271, 0.7123917231436214, 0.7989224556692409, 0.817244542800227, 0.8070941382318705, 0.8183127986085827, 0.8242248209480061, 0.1457879143246804, 0.7253869443678874, 0.16372845928370572, 0.14537091913054523, 0.16048710716207681, 0.7304475980413025, 0.12399753861132268, 0.12421343761760295, 0.185861997245793, 0.14505222473274793, 0.10990725797773615, 0.11574742530170046, 0.11729053134238254, 0.11544599803204691, 0.12243905589686077, 0.13109617072769064, 0.11567487185895864, 0.18231335092925982, 0.9658280629291267, 0.9692953484350043, 0.9846362252429741, 0.9726719553259753, 0.9915997028249128, 0.9912097202379355, 0.9893807269257532, 0.9809317814063994, 0.9700375958473657, 0.5880347312982489, 0.6475149383008683, 0.6697618396883311, 0.6646309368415106, 0.5606008692827396, 0.5872498390259007, 0.6176322258872419, 0.609805675981746, 0.5744422962604443, 0.37644928082871343, 0.9179586014860152, 0.17185025150583133, 0.8967088984975595, 0.27673127347068094, 0.9299377510130166, 0.40154831156030946, 0.9024148748899401, 0.2488334286742211, 0.6335216009150229, 0.1914006966371652, 0.24234038456234153, 0.20512662064279397, 0.19395443984591343, 0.20662033006649871, 0.21343267019570622, 0.23602167572745203, 0.2659223149731751, 0.2295891845113598, 0.18295888130979432, 0.2260725009734701, 0.21079353084114172, 0.19305915624449133, 0.37760934040431693, 0.19673517493077874, 0.13055187386445855, 0.2247679126618497, 9.999999999998899e-05, 0.09753994365711438, 0.42674540002513905, 0.4385051818749828, 0.24231396723102716, 0.2981004077942162, 0.05479909102692726, 0.013631215643225225, 0.5219209547349579, 0.12221879405773672, 0.3168448177192107, 0.23037969359276433, 0.15152379701746044, 0.19855126724079875, 0.09324249184920097, 0.37437580462420084, 0.1775262985365751, 0.20563401510100643, 0.1809773854315232, 0.15142338386982102, 0.2662219856706247, 0.1534733251698852, 0.08493751593690191, 0.13436618471901562, 0.1139987871597512, 0.11621828486348496, 0.2316041633386945, 0.29239373825957293, 0.18714207612954337, 0.28799404689286956, 0.3777321772941795, 0.30401126471837636, 0.37017304449387267, 0.19410398470204326, 0.21847086697261386, 0.3798854602302141, 0.7210742663067469, 0.7903738860223427, 0.7382165607887534, 0.8263497534328679, 0.691302000992799, 0.6157625862874918, 0.7026187906465617, 0.6714546120749916, 0.8397160442976386, 0.1248055121344136, 0.0792453168126167, 0.08665293847525424, 0.09712606672500224, 0.11203079995170595, 0.07153259602248996, 0.16295046008215142, 0.15863577034675314, 0.1567321021342204, 0.13954646693854533, 0.19948274935283894, 0.1445020481370528, 0.7932374386528899, 0.17941955885767114, 0.17649294260632087, 0.26748793913460767, 0.146829597851588, 0.2071990401111875, 0.5447054206790146, 0.5097937388160225, 0.1553200055238365, 0.5296335656108642, 0.3014103574154263, 0.5004835428119888, 0.656933249051112, 0.5442601439881477, 0.6447642342527597, 0.2686175319698144, 0.317931945288221, 0.4083014936729341, 0.2482182218493919, 0.21221427244401025, 0.4326143643039826, 0.4648712554532308, 0.4340299707313606, 0.11072245542878778, 0.23020313678590643, 0.24691856290481984, 0.28242842461584783, 0.24105652341232686, 0.2349132613688556, 0.20746337804017745, 0.24700770790634852, 0.26062763500624797, 0.31332876621398986, 0.8187827389102986, 0.19470014583896622, 0.20057137631739952, 0.19873101780527147, 0.19995182651537258, 0.18423626490151335, 0.1996662020101706, 0.2243543914495041, 0.19537768398945643, 0.9327234782737859, 0.16485458236384132, 0.1528972943995719, 0.18790821799898638, 0.1994456359798621, 0.16769411438361426, 0.15382809714486556, 0.1674107134782098, 0.16674080693734117, 0.9195565046663727, 0.21079173829353381, 0.128229850066245, 0.20571394096550488, 0.5424829741674644, 0.19847925971605962, 0.10439370232331202, 0.213052431151891, 0.2110875748757277, 0.32133732953651384, 0.3650431868664207, 0.23525428570863827, 0.26589683083904914, 0.17906847716117824, 0.22751905998911792, 0.25950337091839715, 0.2788463026574063, 0.2494120624784648, 0.08573338963698784, 0.09197005544896275, 0.07065442784468157, 0.09471203557403496, 0.09264188611310387, 0.1025961378161494, 0.07893045851508684, 0.09632182020285562, 0.10707405649561375]}, "mutation_prompt": null}
{"id": "7df4cd78-809f-41e7-8458-4cfb165dd349", "solution": "import numpy as np\n\nclass EnhancedHybridStochasticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.initial_scale_factor = 0.9  # Increased scale factor for more exploration\n        self.initial_crossover_prob = 0.8  # Slightly increased crossover probability\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population = self.opposition_based_initialization()\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.learning_rate = 0.05  # Reduced learning rate for better local search precision\n        self.elitism_rate = 0.2  # Increased elitism rate to preserve more top solutions\n\n    def opposition_based_initialization(self):\n        initial_population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        opposition_population = self.lower_bound + self.upper_bound - initial_population\n        return np.vstack((initial_population, opposition_population))[:self.population_size]\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def select_parents(self, idx):\n        indices = [i for i in range(self.population_size) if i != idx]\n        return np.random.choice(indices, 3, replace=False)\n\n    def differential_evolution_mutation(self, idx, scale_factor):\n        a, b, c = self.select_parents(idx)\n        mutant_vector = self.population[a] + scale_factor * (self.population[b] - self.population[c])\n        return np.clip(mutant_vector, self.lower_bound, self.upper_bound)\n\n    def crossover(self, target_vector, mutant_vector, crossover_prob):\n        crossover_mask = np.random.rand(self.dim) < crossover_prob\n        return np.where(crossover_mask, mutant_vector, target_vector)\n\n    def local_search(self, vector):\n        perturbation = np.random.normal(0, self.learning_rate, self.dim)\n        candidate_vector = vector + perturbation\n        return np.clip(candidate_vector, self.lower_bound, self.upper_bound)\n\n    def elitism_selection(self):\n        num_elites = max(1, int(self.elitism_rate * self.population_size))\n        elite_indices = np.argsort(self.fitness)[:num_elites]\n        return self.population[elite_indices], self.fitness[elite_indices]\n\n    def __call__(self, func):\n        self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            elites, elite_fitness = self.elitism_selection()\n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                scale_factor = self.initial_scale_factor * (1 - self.evaluations / self.budget)\n                crossover_prob = self.initial_crossover_prob - 0.1 * np.random.rand()  # Increase consistency\n                \n                if i < len(elites):  # Use elite individuals\n                    candidate_vector = self.local_search(elites[i])\n                else:\n                    mutant_vector = self.differential_evolution_mutation(i, scale_factor)\n                    candidate_vector = self.crossover(self.population[i], mutant_vector, crossover_prob)\n\n                candidate_fitness = func(candidate_vector)\n                self.evaluations += 1\n\n                if candidate_fitness < self.fitness[i]:\n                    self.population[i] = candidate_vector\n                    self.fitness[i] = candidate_fitness\n\n                self.learning_rate = max(0.02, self.learning_rate * (1 - 0.05 * np.random.rand()))  # Adjust learning rate bounds\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx]", "name": "EnhancedHybridStochasticOptimizer", "description": "Hybrid optimizer with adaptive exploration-exploitation balance and diversity maintenance for accelerated convergence.", "configspace": "", "generation": 79, "fitness": 0.3868849320861044, "feedback": "The algorithm EnhancedHybridStochasticOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.39 with standard deviation 0.28.", "error": "", "parent_id": "085fc4fd-6092-49b6-946c-e88881c5d87c", "metadata": {"aucs": [0.8638561400672309, 0.8713811823854569, 0.8782291325329002, 0.8777876649144294, 0.8980123439765584, 0.868261471251745, 0.8624739768779002, 0.8659537015707308, 0.8688659152253905, 0.7221893370262864, 0.6343705382324583, 0.6234597976581837, 0.7141466960035363, 0.6958002907047554, 0.7243497359391797, 0.7401729241636286, 0.6479839911517604, 0.7301912822321466, 0.1783139932623048, 0.18669984575041743, 0.12384201643705706, 0.7012653149627279, 0.18371697206520776, 0.7241478391234101, 0.15801479587305844, 0.7370950248508636, 0.7209413576006873, 0.1519200171389251, 0.14197978903165986, 0.12271796280460112, 0.6187175007980412, 0.15816945550241057, 0.13291556932901383, 0.1574625741247765, 0.11785238179042479, 0.14342058021708348, 0.9908878279141445, 0.9805579684629636, 0.9820187050103268, 0.9690840322313544, 0.9852616634802391, 0.975118471320504, 0.9842824235862578, 0.9882626607940889, 0.9810113719046104, 0.60506247341931, 0.5904549049168994, 0.5961543281751327, 0.569204736294179, 0.5422711337469537, 0.5494660455064767, 0.6121945809515217, 0.5183894577047266, 0.545539570361683, 0.17161307351865285, 0.8655530928475837, 0.7848253275393697, 0.21183979070310865, 0.8552068601032896, 0.8450717261365589, 0.24260961365643785, 0.2352062164594788, 0.8318329097800556, 0.12418450548791748, 0.4153868291928662, 0.6238924579749121, 0.3214757639091762, 0.28696949685876694, 0.39140116847275186, 0.13194554830669514, 0.33800286319733097, 0.312924963011765, 0.20719250150477087, 0.36578252600820693, 0.3910212220309929, 0.1314967645014512, 0.308371190013192, 0.3627340310403736, 0.1302014384466358, 0.17508057993995296, 0.12818358723842194, 0.09143981189295591, 0.06751342513083769, 0.2448758154716264, 0.06288092293987724, 0.3004849387077887, 0.036966783253350344, 0.022802907180558996, 0.02427033633797604, 0.16003357064582013, 0.2761247805393998, 0.16771589975006007, 0.14648281963559362, 0.3335735880981643, 0.347651715503445, 0.06637695858245551, 0.4160013942765919, 0.09934137576117608, 0.36153837407712885, 0.05849985105647426, 0.0705566440193034, 0.0744706354123651, 0.09300141563087672, 0.11347727139097463, 0.1119108848815984, 0.1699406547260096, 0.09959467311076642, 0.10030652650305527, 0.30331603870484514, 0.30273992769303715, 0.25375538685912724, 0.2634032194364626, 0.27018980661423175, 0.33319888588624746, 0.2459183614603878, 0.2723822297156203, 0.19395368546534186, 0.6468609984487432, 0.664905343362681, 0.6175488181428359, 0.6561269080880474, 0.6487240401094261, 0.6057653917379278, 0.705639056147537, 0.6052675885944783, 0.6508776219501617, 0.12345397506148381, 0.14530810001530936, 0.10124200988694754, 0.15446145472929607, 0.05951208456486745, 0.12258135596312547, 0.11505193284956716, 0.13639202635520997, 0.0923377651221714, 0.21781548440572496, 0.14084014130029443, 0.7127320157741623, 0.6852233158419817, 0.666657691628522, 0.4988911018993524, 0.23729177148062985, 0.6453549702800163, 0.6385509171466301, 0.4918510753423144, 0.28615589965370514, 0.19250615520792835, 0.4662172158341281, 0.48369664736077045, 0.4716351255560699, 0.5094093208633992, 0.5645313735765279, 0.5034187700297201, 0.35983894987404574, 0.3277729492946104, 0.36706135345652224, 0.3662913245066509, 0.3214737148372586, 0.3663752863766796, 0.2263143487366096, 0.403095931947441, 0.3721492506218128, 0.2237254000798874, 0.1924096698420823, 0.20422830154192384, 0.2304028976603466, 0.20220676267873028, 0.28890478179861157, 0.18634240402043611, 0.20370418060313766, 0.22739009363464813, 0.2247676996880723, 0.7664643797151325, 0.1979104500982255, 0.7426772949070606, 0.1958119696761621, 0.7152949021449588, 0.19572551192872167, 0.25027397656906303, 0.7982819358887521, 0.16919085140166368, 0.20522093553557152, 0.15333952200801992, 0.18747564636212244, 0.88190462880929, 0.19618060935259596, 0.13012964099062396, 0.16861019824606516, 0.8349361078528922, 0.16703373079879724, 0.1679617566816609, 0.8697141870713876, 0.20962717349603055, 0.11093979582917113, 0.8787472007882837, 0.21240729079649023, 0.15369962444706964, 0.164336076964669, 0.21971470544596827, 0.2507721567301805, 0.26337155276405644, 0.2705973834740273, 0.2013290386042822, 0.21090694971883994, 0.22924081636555094, 0.25245731589489784, 0.24277704932194832, 0.09889200226206918, 0.11271176736534039, 0.09709490721384895, 0.08778074057293073, 0.0811264508524242, 0.10569099709821927, 0.10474362897708123, 0.1004198213309575, 0.09573285014366628]}, "mutation_prompt": null}
{"id": "71a3ae4b-c3e2-4fff-83c9-892ae639c083", "solution": "import numpy as np\n\nclass EnhancedHybridStochasticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.initial_scale_factor = 0.8\n        self.initial_crossover_prob = 0.7\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population = self.opposition_based_initialization()\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.learning_rate = 0.1\n        self.elitism_rate = 0.1  # New elitism parameter\n\n    def opposition_based_initialization(self):\n        initial_population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        opposition_population = self.lower_bound + self.upper_bound - initial_population\n        return np.vstack((initial_population, opposition_population))[:self.population_size]\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def select_parents(self, idx):\n        indices = [i for i in range(self.population_size) if i != idx]\n        return np.random.choice(indices, 3, replace=False)\n\n    def differential_evolution_mutation(self, idx, scale_factor):\n        a, b, c = self.select_parents(idx)\n        mutant_vector = self.population[a] + scale_factor * (self.population[b] - self.population[c])\n        return np.clip(mutant_vector, self.lower_bound, self.upper_bound)\n\n    def crossover(self, target_vector, mutant_vector, crossover_prob):\n        crossover_mask = np.random.rand(self.dim) < crossover_prob\n        return np.where(crossover_mask, mutant_vector, target_vector)\n\n    def local_search(self, vector):\n        perturbation = np.random.normal(0, self.learning_rate, self.dim)\n        candidate_vector = vector + perturbation\n        return np.clip(candidate_vector, self.lower_bound, self.upper_bound)\n\n    def elitism_selection(self):\n        num_elites = max(1, int(self.elitism_rate * self.population_size))\n        elite_indices = np.argsort(self.fitness)[:num_elites]\n        return self.population[elite_indices], self.fitness[elite_indices]\n\n    def __call__(self, func):\n        self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            elites, elite_fitness = self.elitism_selection()\n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                scale_factor = self.initial_scale_factor * (1 - self.evaluations / self.budget)\n                crossover_prob = self.initial_crossover_prob + 0.2 * np.random.rand()\n                \n                if i < len(elites):  # Use elite individuals\n                    candidate_vector = self.local_search(elites[i])\n                else:\n                    mutant_vector = self.differential_evolution_mutation(i, scale_factor)\n                    candidate_vector = self.crossover(self.population[i], mutant_vector, crossover_prob)\n\n                candidate_fitness = func(candidate_vector)\n                self.evaluations += 1\n\n                if candidate_fitness < self.fitness[i]:\n                    self.population[i] = candidate_vector\n                    self.fitness[i] = candidate_fitness\n\n                self.learning_rate = max(0.01, self.learning_rate * (1 - 0.05 * np.random.rand()))\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx]", "name": "EnhancedHybridStochasticOptimizer", "description": "Enhanced hybrid optimizer with adaptive elitism and dynamic mutation for improved convergence speed.", "configspace": "", "generation": 71, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "085fc4fd-6092-49b6-946c-e88881c5d87c", "metadata": {"aucs": [0.8887079975314532, 0.894012818670537, 0.8991705287696641, 0.8978847561824669, 0.8879257415736936, 0.8940642586100076, 0.9007105063480211, 0.8958392425350181, 0.8969457975943529, 0.8044175229050868, 0.7853180510418849, 0.7770538765325782, 0.7824751042832793, 0.7991480344536278, 0.7859582873813178, 0.7829457001324069, 0.8073918603513864, 0.794782193201033, 0.6808600123402386, 0.1750883946278604, 0.14531540322259429, 0.1442044729706906, 0.18425100518796977, 0.6818733473800802, 0.6535836728714721, 0.18149935269683826, 0.18178474171291303, 0.17985980474169194, 0.15402982831947865, 0.15226864312698196, 0.173967907678645, 0.14621090429947348, 0.47097234153094814, 0.1410752174129266, 0.6696335857850447, 0.14569775079321112, 0.9826172320377552, 0.9666035752060869, 0.9829618007032487, 0.976950051717164, 0.9827548155568644, 0.9852053707001819, 0.9767881737976671, 0.9804489105964168, 0.9669130107270508, 0.6261081652649766, 0.6510922594882195, 0.6255531916869328, 0.6774833282904631, 0.5975973805993315, 0.5910302179938389, 0.6678501570106004, 0.6330163811357998, 0.5508126220258189, 0.8531774442531348, 0.8389810559862521, 0.8880414902745725, 0.8289173802432317, 0.8953502885124923, 0.8397824108196726, 0.8273523214295622, 0.8057815752417086, 0.8580295054264129, 0.4016755801839059, 0.26933131272005106, 0.22925181461595256, 0.13204420434091646, 0.39317336231664235, 0.2284734447615212, 0.3105063171146114, 0.27055107832358327, 0.2917673882193722, 0.38502853820393, 0.2123706550707669, 0.4034652264328532, 0.568732561497391, 0.36500406246735106, 0.19551716008178943, 0.24638934001327484, 0.5225532750228102, 0.3624738677767353, 0.2666496944974961, 0.3046806810096032, 0.13668233147297137, 0.1931747108259616, 0.37752645472722535, 0.31618154708059465, 0.31835813584843375, 0.3070681730831166, 0.12402679428341545, 0.26495981942516666, 0.25188898472987253, 0.5385819495844237, 0.40165450812704007, 0.23932881991640842, 0.11965141563213433, 0.34184071432514285, 0.3160838217897127, 0.5338472984904024, 0.047572984733449286, 0.09039073281130205, 0.1537274380235698, 0.1289551537399951, 0.2604941529185929, 0.25706237822972466, 0.1845051531279127, 0.10652716324087819, 0.2708006776187757, 0.3414923847942186, 0.4123837225375905, 0.3117320514759382, 0.24748938072863658, 0.39906933520462307, 0.38488430703266197, 0.30616649005219543, 0.35800448698393594, 0.2925584450216504, 0.691927567575801, 0.6852837437185479, 0.7713371776842154, 0.6980978655290662, 0.766579905272989, 0.6382172121539862, 0.6996949068837017, 0.6411455169887086, 0.6813004968647007, 0.13780851187498355, 0.14510698635437513, 0.0893582233783049, 0.09991541502631718, 0.13054731281561616, 0.11683523087565595, 0.171925554807697, 0.12998368305350327, 0.1543780872863041, 0.23778094004152495, 0.19740299269447714, 0.19419993189472884, 0.6711147357652977, 0.3381930059883864, 0.16150820454378223, 0.2054840485150662, 0.16698240295904943, 0.2369198920033001, 0.5378973437045065, 0.5086990285195376, 0.4430302848859211, 0.5529629454086893, 0.5198127341548278, 0.5279513604494719, 0.5500605824796765, 0.6079135827937183, 0.6101637594254216, 0.39824080369013304, 0.4370748660527193, 0.3653791923313585, 0.4762702225417462, 0.462984548459363, 0.360263740881579, 0.47399412193575075, 0.4281155485172986, 0.4598926486463012, 0.2699804326748404, 0.20417958046377094, 0.19642411996639997, 0.2090269531837965, 0.22596432344098072, 0.21190214578886424, 0.2627536333413878, 0.22132815692849062, 0.2608405310025357, 0.2363027529318048, 0.23551312504404354, 0.243560947128306, 0.6850969382534209, 0.18536012491528941, 0.2458619905947247, 0.24381288780284605, 0.24263341472021616, 0.2027686971008097, 0.18286924219413314, 0.20516327697756553, 0.18575367079274585, 0.9095286514961691, 0.8528250005346223, 0.8450929708093164, 0.14315397106139793, 0.19657384656366028, 0.8524027151110926, 0.16612350803193054, 0.8531453508001282, 0.8783942091238557, 0.8155303269827223, 0.16507174910354772, 0.20270816914068102, 0.12735536938542336, 0.15526468014730144, 0.20877673505561833, 0.22523905812434164, 0.256192055388863, 0.2035935227993826, 0.2536636617248401, 0.3606807596988413, 0.2505651780761303, 0.22899606852656185, 0.2425765551210327, 0.2562714338840041, 0.09867543144495539, 0.1077639754617119, 0.10622902939542522, 0.08139414847363335, 0.1006091149598467, 0.10781530238189518, 0.10050249366479436, 0.09691080533906204, 0.08149992429255548]}, "mutation_prompt": null}
{"id": "83757a60-b551-4508-8caa-fb70721a676a", "solution": "import numpy as np\n\nclass EnhancedHybridStochasticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.initial_scale_factor = 0.8\n        self.initial_crossover_prob = 0.7\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population = self.opposition_based_initialization()\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.learning_rate = 0.1\n        self.elitism_rate = 0.1  # New elitism parameter\n\n    def opposition_based_initialization(self):\n        initial_population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        opposition_population = self.lower_bound + self.upper_bound - initial_population\n        return np.vstack((initial_population, opposition_population))[:self.population_size]\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def select_parents(self, idx):\n        indices = [i for i in range(self.population_size) if i != idx]\n        return np.random.choice(indices, 3, replace=False)\n\n    def differential_evolution_mutation(self, idx, scale_factor):\n        a, b, c = self.select_parents(idx)\n        mutant_vector = self.population[a] + scale_factor * (self.population[b] - self.population[c])\n        return np.clip(mutant_vector, self.lower_bound, self.upper_bound)\n\n    def crossover(self, target_vector, mutant_vector, crossover_prob):\n        crossover_mask = np.random.rand(self.dim) < crossover_prob\n        return np.where(crossover_mask, mutant_vector, target_vector)\n\n    def local_search(self, vector):\n        perturbation = np.random.normal(0, self.learning_rate, self.dim)\n        candidate_vector = vector + perturbation\n        return np.clip(candidate_vector, self.lower_bound, self.upper_bound)\n\n    def elitism_selection(self):\n        num_elites = max(1, int(self.elitism_rate * self.population_size))\n        elite_indices = np.argsort(self.fitness)[:num_elites]\n        return self.population[elite_indices], self.fitness[elite_indices]\n\n    def __call__(self, func):\n        self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            elites, elite_fitness = self.elitism_selection()\n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                scale_factor = self.initial_scale_factor * (1 - self.evaluations / self.budget)\n                crossover_prob = self.initial_crossover_prob + 0.2 * np.random.rand()\n                \n                if i < len(elites):  # Use elite individuals\n                    candidate_vector = self.local_search(elites[i])\n                else:\n                    mutant_vector = self.differential_evolution_mutation(i, scale_factor)\n                    candidate_vector = self.crossover(self.population[i], mutant_vector, crossover_prob)\n\n                candidate_fitness = func(candidate_vector)\n                self.evaluations += 1\n\n                if candidate_fitness < self.fitness[i]:\n                    self.population[i] = candidate_vector\n                    self.fitness[i] = candidate_fitness\n\n                self.learning_rate = max(0.01, self.learning_rate * (1 - 0.05 * np.random.rand()))\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx]", "name": "EnhancedHybridStochasticOptimizer", "description": "Enhanced hybrid optimizer with adaptive elitism and dynamic mutation for improved convergence speed.", "configspace": "", "generation": 71, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "085fc4fd-6092-49b6-946c-e88881c5d87c", "metadata": {"aucs": [0.8887079975314532, 0.894012818670537, 0.8991705287696641, 0.8978847561824669, 0.8879257415736936, 0.8940642586100076, 0.9007105063480211, 0.8958392425350181, 0.8969457975943529, 0.8044175229050868, 0.7853180510418849, 0.7770538765325782, 0.7824751042832793, 0.7991480344536278, 0.7859582873813178, 0.7829457001324069, 0.8073918603513864, 0.794782193201033, 0.6808600123402386, 0.1750883946278604, 0.14531540322259429, 0.1442044729706906, 0.18425100518796977, 0.6818733473800802, 0.6535836728714721, 0.18149935269683826, 0.18178474171291303, 0.17985980474169194, 0.15402982831947865, 0.15226864312698196, 0.173967907678645, 0.14621090429947348, 0.47097234153094814, 0.1410752174129266, 0.6696335857850447, 0.14569775079321112, 0.9826172320377552, 0.9666035752060869, 0.9829618007032487, 0.976950051717164, 0.9827548155568644, 0.9852053707001819, 0.9767881737976671, 0.9804489105964168, 0.9669130107270508, 0.6261081652649766, 0.6510922594882195, 0.6255531916869328, 0.6774833282904631, 0.5975973805993315, 0.5910302179938389, 0.6678501570106004, 0.6330163811357998, 0.5508126220258189, 0.8531774442531348, 0.8389810559862521, 0.8880414902745725, 0.8289173802432317, 0.8953502885124923, 0.8397824108196726, 0.8273523214295622, 0.8057815752417086, 0.8580295054264129, 0.4016755801839059, 0.26933131272005106, 0.22925181461595256, 0.13204420434091646, 0.39317336231664235, 0.2284734447615212, 0.3105063171146114, 0.27055107832358327, 0.2917673882193722, 0.38502853820393, 0.2123706550707669, 0.4034652264328532, 0.568732561497391, 0.36500406246735106, 0.19551716008178943, 0.24638934001327484, 0.5225532750228102, 0.3624738677767353, 0.2666496944974961, 0.3046806810096032, 0.13668233147297137, 0.1931747108259616, 0.37752645472722535, 0.31618154708059465, 0.31835813584843375, 0.3070681730831166, 0.12402679428341545, 0.26495981942516666, 0.25188898472987253, 0.5385819495844237, 0.40165450812704007, 0.23932881991640842, 0.11965141563213433, 0.34184071432514285, 0.3160838217897127, 0.5338472984904024, 0.047572984733449286, 0.09039073281130205, 0.1537274380235698, 0.1289551537399951, 0.2604941529185929, 0.25706237822972466, 0.1845051531279127, 0.10652716324087819, 0.2708006776187757, 0.3414923847942186, 0.4123837225375905, 0.3117320514759382, 0.24748938072863658, 0.39906933520462307, 0.38488430703266197, 0.30616649005219543, 0.35800448698393594, 0.2925584450216504, 0.691927567575801, 0.6852837437185479, 0.7713371776842154, 0.6980978655290662, 0.766579905272989, 0.6382172121539862, 0.6996949068837017, 0.6411455169887086, 0.6813004968647007, 0.13780851187498355, 0.14510698635437513, 0.0893582233783049, 0.09991541502631718, 0.13054731281561616, 0.11683523087565595, 0.171925554807697, 0.12998368305350327, 0.1543780872863041, 0.23778094004152495, 0.19740299269447714, 0.19419993189472884, 0.6711147357652977, 0.3381930059883864, 0.16150820454378223, 0.2054840485150662, 0.16698240295904943, 0.2369198920033001, 0.5378973437045065, 0.5086990285195376, 0.4430302848859211, 0.5529629454086893, 0.5198127341548278, 0.5279513604494719, 0.5500605824796765, 0.6079135827937183, 0.6101637594254216, 0.39824080369013304, 0.4370748660527193, 0.3653791923313585, 0.4762702225417462, 0.462984548459363, 0.360263740881579, 0.47399412193575075, 0.4281155485172986, 0.4598926486463012, 0.2699804326748404, 0.20417958046377094, 0.19642411996639997, 0.2090269531837965, 0.22596432344098072, 0.21190214578886424, 0.2627536333413878, 0.22132815692849062, 0.2608405310025357, 0.2363027529318048, 0.23551312504404354, 0.243560947128306, 0.6850969382534209, 0.18536012491528941, 0.2458619905947247, 0.24381288780284605, 0.24263341472021616, 0.2027686971008097, 0.18286924219413314, 0.20516327697756553, 0.18575367079274585, 0.9095286514961691, 0.8528250005346223, 0.8450929708093164, 0.14315397106139793, 0.19657384656366028, 0.8524027151110926, 0.16612350803193054, 0.8531453508001282, 0.8783942091238557, 0.8155303269827223, 0.16507174910354772, 0.20270816914068102, 0.12735536938542336, 0.15526468014730144, 0.20877673505561833, 0.22523905812434164, 0.256192055388863, 0.2035935227993826, 0.2536636617248401, 0.3606807596988413, 0.2505651780761303, 0.22899606852656185, 0.2425765551210327, 0.2562714338840041, 0.09867543144495539, 0.1077639754617119, 0.10622902939542522, 0.08139414847363335, 0.1006091149598467, 0.10781530238189518, 0.10050249366479436, 0.09691080533906204, 0.08149992429255548]}, "mutation_prompt": null}
{"id": "81daebf0-1499-40a8-bc4d-829afb3d029f", "solution": "import numpy as np\n\nclass EnhancedHybridStochasticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.initial_scale_factor = 0.8\n        self.initial_crossover_prob = 0.7\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population = self.opposition_based_initialization()\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.learning_rate = 0.1\n        self.elitism_rate = 0.1  # New elitism parameter\n\n    def opposition_based_initialization(self):\n        initial_population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        opposition_population = self.lower_bound + self.upper_bound - initial_population\n        return np.vstack((initial_population, opposition_population))[:self.population_size]\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def select_parents(self, idx):\n        indices = [i for i in range(self.population_size) if i != idx]\n        return np.random.choice(indices, 3, replace=False)\n\n    def differential_evolution_mutation(self, idx, scale_factor):\n        a, b, c = self.select_parents(idx)\n        mutant_vector = self.population[a] + scale_factor * (self.population[b] - self.population[c])\n        return np.clip(mutant_vector, self.lower_bound, self.upper_bound)\n\n    def crossover(self, target_vector, mutant_vector, crossover_prob):\n        crossover_mask = np.random.rand(self.dim) < crossover_prob\n        return np.where(crossover_mask, mutant_vector, target_vector)\n\n    def local_search(self, vector):\n        perturbation = np.random.normal(0, self.learning_rate, self.dim)\n        candidate_vector = vector + perturbation\n        return np.clip(candidate_vector, self.lower_bound, self.upper_bound)\n\n    def elitism_selection(self):\n        num_elites = max(1, int(self.elitism_rate * self.population_size))\n        elite_indices = np.argsort(self.fitness)[:num_elites]\n        return self.population[elite_indices], self.fitness[elite_indices]\n\n    def __call__(self, func):\n        self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            elites, elite_fitness = self.elitism_selection()\n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                scale_factor = self.initial_scale_factor * (1 - self.evaluations / self.budget)\n                crossover_prob = self.initial_crossover_prob + 0.2 * np.random.rand()\n                \n                if i < len(elites):  # Use elite individuals\n                    candidate_vector = self.local_search(elites[i])\n                else:\n                    mutant_vector = self.differential_evolution_mutation(i, scale_factor)\n                    candidate_vector = self.crossover(self.population[i], mutant_vector, crossover_prob)\n\n                candidate_fitness = func(candidate_vector)\n                self.evaluations += 1\n\n                if candidate_fitness < self.fitness[i]:\n                    self.population[i] = candidate_vector\n                    self.fitness[i] = candidate_fitness\n\n                self.learning_rate = max(0.01, self.learning_rate * (1 - 0.05 * np.random.rand()))\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx]", "name": "EnhancedHybridStochasticOptimizer", "description": "Enhanced hybrid optimizer with adaptive elitism and dynamic mutation for improved convergence speed.", "configspace": "", "generation": 71, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "085fc4fd-6092-49b6-946c-e88881c5d87c", "metadata": {"aucs": [0.8887079975314532, 0.894012818670537, 0.8991705287696641, 0.8978847561824669, 0.8879257415736936, 0.8940642586100076, 0.9007105063480211, 0.8958392425350181, 0.8969457975943529, 0.8044175229050868, 0.7853180510418849, 0.7770538765325782, 0.7824751042832793, 0.7991480344536278, 0.7859582873813178, 0.7829457001324069, 0.8073918603513864, 0.794782193201033, 0.6808600123402386, 0.1750883946278604, 0.14531540322259429, 0.1442044729706906, 0.18425100518796977, 0.6818733473800802, 0.6535836728714721, 0.18149935269683826, 0.18178474171291303, 0.17985980474169194, 0.15402982831947865, 0.15226864312698196, 0.173967907678645, 0.14621090429947348, 0.47097234153094814, 0.1410752174129266, 0.6696335857850447, 0.14569775079321112, 0.9826172320377552, 0.9666035752060869, 0.9829618007032487, 0.976950051717164, 0.9827548155568644, 0.9852053707001819, 0.9767881737976671, 0.9804489105964168, 0.9669130107270508, 0.6261081652649766, 0.6510922594882195, 0.6255531916869328, 0.6774833282904631, 0.5975973805993315, 0.5910302179938389, 0.6678501570106004, 0.6330163811357998, 0.5508126220258189, 0.8531774442531348, 0.8389810559862521, 0.8880414902745725, 0.8289173802432317, 0.8953502885124923, 0.8397824108196726, 0.8273523214295622, 0.8057815752417086, 0.8580295054264129, 0.4016755801839059, 0.26933131272005106, 0.22925181461595256, 0.13204420434091646, 0.39317336231664235, 0.2284734447615212, 0.3105063171146114, 0.27055107832358327, 0.2917673882193722, 0.38502853820393, 0.2123706550707669, 0.4034652264328532, 0.568732561497391, 0.36500406246735106, 0.19551716008178943, 0.24638934001327484, 0.5225532750228102, 0.3624738677767353, 0.2666496944974961, 0.3046806810096032, 0.13668233147297137, 0.1931747108259616, 0.37752645472722535, 0.31618154708059465, 0.31835813584843375, 0.3070681730831166, 0.12402679428341545, 0.26495981942516666, 0.25188898472987253, 0.5385819495844237, 0.40165450812704007, 0.23932881991640842, 0.11965141563213433, 0.34184071432514285, 0.3160838217897127, 0.5338472984904024, 0.047572984733449286, 0.09039073281130205, 0.1537274380235698, 0.1289551537399951, 0.2604941529185929, 0.25706237822972466, 0.1845051531279127, 0.10652716324087819, 0.2708006776187757, 0.3414923847942186, 0.4123837225375905, 0.3117320514759382, 0.24748938072863658, 0.39906933520462307, 0.38488430703266197, 0.30616649005219543, 0.35800448698393594, 0.2925584450216504, 0.691927567575801, 0.6852837437185479, 0.7713371776842154, 0.6980978655290662, 0.766579905272989, 0.6382172121539862, 0.6996949068837017, 0.6411455169887086, 0.6813004968647007, 0.13780851187498355, 0.14510698635437513, 0.0893582233783049, 0.09991541502631718, 0.13054731281561616, 0.11683523087565595, 0.171925554807697, 0.12998368305350327, 0.1543780872863041, 0.23778094004152495, 0.19740299269447714, 0.19419993189472884, 0.6711147357652977, 0.3381930059883864, 0.16150820454378223, 0.2054840485150662, 0.16698240295904943, 0.2369198920033001, 0.5378973437045065, 0.5086990285195376, 0.4430302848859211, 0.5529629454086893, 0.5198127341548278, 0.5279513604494719, 0.5500605824796765, 0.6079135827937183, 0.6101637594254216, 0.39824080369013304, 0.4370748660527193, 0.3653791923313585, 0.4762702225417462, 0.462984548459363, 0.360263740881579, 0.47399412193575075, 0.4281155485172986, 0.4598926486463012, 0.2699804326748404, 0.20417958046377094, 0.19642411996639997, 0.2090269531837965, 0.22596432344098072, 0.21190214578886424, 0.2627536333413878, 0.22132815692849062, 0.2608405310025357, 0.2363027529318048, 0.23551312504404354, 0.243560947128306, 0.6850969382534209, 0.18536012491528941, 0.2458619905947247, 0.24381288780284605, 0.24263341472021616, 0.2027686971008097, 0.18286924219413314, 0.20516327697756553, 0.18575367079274585, 0.9095286514961691, 0.8528250005346223, 0.8450929708093164, 0.14315397106139793, 0.19657384656366028, 0.8524027151110926, 0.16612350803193054, 0.8531453508001282, 0.8783942091238557, 0.8155303269827223, 0.16507174910354772, 0.20270816914068102, 0.12735536938542336, 0.15526468014730144, 0.20877673505561833, 0.22523905812434164, 0.256192055388863, 0.2035935227993826, 0.2536636617248401, 0.3606807596988413, 0.2505651780761303, 0.22899606852656185, 0.2425765551210327, 0.2562714338840041, 0.09867543144495539, 0.1077639754617119, 0.10622902939542522, 0.08139414847363335, 0.1006091149598467, 0.10781530238189518, 0.10050249366479436, 0.09691080533906204, 0.08149992429255548]}, "mutation_prompt": null}
{"id": "1c0827db-af6a-44d6-a32c-ddfb22d0afde", "solution": "import numpy as np\n\nclass EnhancedDiversityInspiredOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 22  # Increased slightly for diversity\n        self.initial_scale_factor = 0.85  # Tweaked for enhanced exploration\n        self.initial_crossover_prob = 0.75  # Adjusted for balanced exploration-exploitation\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population = self.opposition_based_initialization()\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.learning_rate = 0.1\n        self.elitism_rate = 0.15  # Increased elitism for better exploitation\n\n    def opposition_based_initialization(self):\n        initial_population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        opposition_population = self.lower_bound + self.upper_bound - initial_population\n        return np.vstack((initial_population, opposition_population))[:self.population_size]\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def select_parents(self, idx):\n        indices = [i for i in range(self.population_size) if i != idx]\n        return np.random.choice(indices, 3, replace=False)\n\n    def differential_evolution_mutation(self, idx, scale_factor):\n        a, b, c = self.select_parents(idx)\n        mutant_vector = self.population[a] + scale_factor * (self.population[b] - self.population[c])\n        return np.clip(mutant_vector, self.lower_bound, self.upper_bound)\n\n    def crossover(self, target_vector, mutant_vector, crossover_prob):\n        crossover_mask = np.random.rand(self.dim) < crossover_prob\n        return np.where(crossover_mask, mutant_vector, target_vector)\n\n    def local_search(self, vector):\n        perturbation = np.random.normal(0, self.learning_rate, self.dim)\n        candidate_vector = vector + perturbation\n        return np.clip(candidate_vector, self.lower_bound, self.upper_bound)\n\n    def elitism_selection(self):\n        num_elites = max(1, int(self.elitism_rate * self.population_size))\n        elite_indices = np.argsort(self.fitness)[:num_elites]\n        return self.population[elite_indices], self.fitness[elite_indices]\n\n    def adaptive_opposition_learning(self):\n        oppositional_population = self.lower_bound + self.upper_bound - self.population\n        for i in range(self.population_size):\n            opposite_fitness = func(oppositional_population[i])\n            if opposite_fitness < self.fitness[i]:\n                self.population[i] = oppositional_population[i]\n                self.fitness[i] = opposite_fitness\n            self.evaluations += 1\n            if self.evaluations >= self.budget:\n                break\n\n    def __call__(self, func):\n        self.evaluate_population(func)\n        self.adaptive_opposition_learning()\n        while self.evaluations < self.budget:\n            elites, elite_fitness = self.elitism_selection()\n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                scale_factor = self.initial_scale_factor * (1 - self.evaluations / self.budget)\n                crossover_prob = self.initial_crossover_prob + 0.2 * np.random.rand()\n                \n                if i < len(elites):  # Use elite individuals\n                    candidate_vector = self.local_search(elites[i])\n                else:\n                    mutant_vector = self.differential_evolution_mutation(i, scale_factor)\n                    candidate_vector = self.crossover(self.population[i], mutant_vector, crossover_prob)\n\n                candidate_fitness = func(candidate_vector)\n                self.evaluations += 1\n\n                if candidate_fitness < self.fitness[i]:\n                    self.population[i] = candidate_vector\n                    self.fitness[i] = candidate_fitness\n\n                self.learning_rate = max(0.01, self.learning_rate * (1 - 0.05 * np.random.rand()))\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx]", "name": "EnhancedDiversityInspiredOptimizer", "description": "Enhanced Diversity-Inspired Stochastic Optimizer with adaptive opposition-based learning and elitism-driven mutation control for improved convergence efficiency.", "configspace": "", "generation": 83, "fitness": -Infinity, "feedback": "An exception occurred: NameError(\"name 'func' is not defined\").", "error": "NameError(\"name 'func' is not defined\")", "parent_id": "085fc4fd-6092-49b6-946c-e88881c5d87c", "metadata": {}, "mutation_prompt": null}
{"id": "611e17be-2543-4b24-a742-9175f71fb862", "solution": "import numpy as np\n\nclass RefinedHybridStochasticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.initial_scale_factor = 0.85  # Adjusted for slightly more aggressive mutations\n        self.initial_crossover_prob = 0.75  # Increase crossover probability to enhance diversity\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population = self.opposition_based_initialization()\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.learning_rate = 0.12  # Slightly increased learning rate for faster local search\n        self.elitism_rate = 0.12  # Slightly increased elitism for preserving more top candidates\n\n    def opposition_based_initialization(self):\n        initial_population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        opposition_population = self.lower_bound + self.upper_bound - initial_population\n        combined_population = np.vstack((initial_population, opposition_population))\n        fitness = np.array([np.inf] * len(combined_population))\n        for i in range(len(combined_population)):\n            fitness[i] = np.random.rand()  # Introduce randomness for better initial diversity\n        best_indices = np.argsort(fitness)[:self.population_size]\n        return combined_population[best_indices]\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def select_parents(self, idx):\n        indices = [i for i in range(self.population_size) if i != idx]\n        return np.random.choice(indices, 3, replace=False)\n\n    def differential_evolution_mutation(self, idx, scale_factor):\n        a, b, c = self.select_parents(idx)\n        mutant_vector = self.population[a] + scale_factor * (self.population[b] - self.population[c])\n        return np.clip(mutant_vector, self.lower_bound, self.upper_bound)\n\n    def crossover(self, target_vector, mutant_vector, crossover_prob):\n        crossover_mask = np.random.rand(self.dim) < crossover_prob\n        return np.where(crossover_mask, mutant_vector, target_vector)\n\n    def local_search(self, vector):\n        perturbation = np.random.normal(0, self.learning_rate, self.dim)\n        candidate_vector = vector + perturbation\n        return np.clip(candidate_vector, self.lower_bound, self.upper_bound)\n\n    def elitism_selection(self):\n        num_elites = max(1, int(self.elitism_rate * self.population_size))\n        elite_indices = np.argsort(self.fitness)[:num_elites]\n        return self.population[elite_indices], self.fitness[elite_indices]\n\n    def __call__(self, func):\n        self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            elites, elite_fitness = self.elitism_selection()\n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                scale_factor = self.initial_scale_factor * (1 - self.evaluations / self.budget)\n                crossover_prob = self.initial_crossover_prob + 0.15 * np.random.rand()  # Narrow crossover randomness\n\n                if i < len(elites):  # Use elite individuals\n                    candidate_vector = self.local_search(elites[i])\n                else:\n                    mutant_vector = self.differential_evolution_mutation(i, scale_factor)\n                    candidate_vector = self.crossover(self.population[i], mutant_vector, crossover_prob)\n\n                candidate_fitness = func(candidate_vector)\n                self.evaluations += 1\n\n                if candidate_fitness < self.fitness[i]:\n                    self.population[i] = candidate_vector\n                    self.fitness[i] = candidate_fitness\n\n                self.learning_rate = max(0.01, self.learning_rate * (1 - 0.04 * np.random.rand()))  # Adjusted learning decay\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx]", "name": "RefinedHybridStochasticOptimizer", "description": "A refined hybrid optimizer with adaptive opposition strategy and dynamic exploration-exploitation balance for enhanced convergence speed.", "configspace": "", "generation": 84, "fitness": 0.42885360425599933, "feedback": "The algorithm RefinedHybridStochasticOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.43 with standard deviation 0.28.", "error": "", "parent_id": "085fc4fd-6092-49b6-946c-e88881c5d87c", "metadata": {"aucs": [0.8766756452394434, 0.8780259518892735, 0.8679732870259016, 0.8851321249681726, 0.8834248924740489, 0.8917330111634347, 0.8915409061661912, 0.8625223086831446, 0.8855384988765383, 0.7702063528242651, 0.764605999911898, 0.7866633313683472, 0.788225311474078, 0.7768513285041114, 0.783710568385616, 0.7710553972167912, 0.7826320564567563, 0.7747122369896489, 0.7207065189713453, 0.6772268580219241, 0.17524904499097194, 0.1545419816557405, 0.12287270778105774, 0.689897670488521, 0.17408711892974027, 0.7264644514688223, 0.17782525894103152, 0.5540661022978465, 0.1718555788393058, 0.17432731451169114, 0.1401812381721591, 0.181817031126965, 0.17084893094232578, 0.14077243420807695, 0.11782626428419385, 0.1411557885190976, 0.9937933542659784, 0.9824818277808939, 0.980707506224134, 0.9544651636952174, 0.9824601842623015, 0.9786103071199913, 0.9917495387625039, 0.9773550272665161, 0.9767648277870316, 0.7186425059203514, 0.7072341755938936, 0.67619418713103, 0.5500313926892695, 0.6353964019006688, 0.5655276295816347, 0.6087248017767631, 0.7250088325007087, 0.5509231661245519, 0.8306251087247614, 0.3722435473015898, 0.8654864869334986, 0.8863503903735646, 0.8760746636972028, 0.8458760719743885, 0.8486549335282704, 0.8432469041054449, 0.3860328241903793, 0.3132235440659491, 0.4899210907813042, 0.2895826691181592, 0.19744367076628966, 0.4358144265843764, 0.19907439487724254, 0.36614638947632094, 0.4673958269450197, 0.4412371012727202, 0.1285886837615413, 0.23403231590111162, 0.36109192393377976, 0.22664878079649053, 0.2540072344222718, 0.4265671714982343, 0.2827316237596058, 0.6431777309769313, 0.3732167479133396, 0.12116522298168053, 0.1678990673445524, 0.3052731722642458, 0.04944125032055546, 0.42013758686785785, 0.4675757890099749, 0.34909871477201915, 0.44848393730635916, 0.4121936894642598, 0.2572264031099728, 0.14782089879620763, 0.45511882842056595, 0.3614252035664588, 0.12576950129727194, 0.19368490486653112, 0.5599530644881539, 0.4930337585131278, 0.48359551057535666, 0.31457189059998514, 0.12229189958524489, 0.1343438096316968, 0.1129583796588105, 0.17065635226036613, 0.21121710331876486, 0.17960579905235896, 0.13080473597692177, 0.17281085533956853, 0.3785105716917646, 0.32356504637548955, 0.2930510023224221, 0.4263055259587192, 0.3758837171163727, 0.3487130962919658, 0.34061488164921405, 0.38468365570003926, 0.36601498696637014, 0.684964500616335, 0.7393164733752317, 0.7681352739007481, 0.7580447340486791, 0.6512200131325806, 0.7160469344263029, 0.5965650033008165, 0.7695202275323955, 0.7091180301825857, 0.13027852195339762, 0.1752862344756313, 0.14725999438068593, 0.10963074171804343, 0.12196205006310135, 0.12842695335615995, 0.1535536314706123, 0.09324353688930664, 0.11756250657323708, 0.1583195481084767, 0.17804157548667843, 0.27489139187246203, 0.21745895839396734, 0.667095829932591, 0.20628860250021674, 0.5805893447980215, 0.2351413331273987, 0.23368786575655498, 0.5245002013696674, 0.5809814491680333, 0.4604348637764978, 0.5487855436976408, 0.5109761909197574, 0.5670142492913299, 0.5523316647904777, 0.6211521395155045, 0.5065222018957187, 0.43913043730592705, 0.4209119941181332, 0.24723244458163163, 0.36854984117381473, 0.4714349416601522, 0.4087858118863441, 0.4445023749465481, 0.47953063462436707, 0.4056768097816228, 0.20187088443067747, 0.28194838553846535, 0.22079563605800234, 0.2193518224022324, 0.18672017534619767, 0.21755812838823685, 0.21888893291732603, 0.20535976400015987, 0.24766721582344342, 0.7731611289963906, 0.22212714398023892, 0.22683569749128651, 0.2219774539530932, 0.2038861631536516, 0.22700750845132922, 0.19396010038295208, 0.19566429190218382, 0.24739215490626576, 0.16341617726849111, 0.8639742256032173, 0.18598438634511893, 0.19365704699243147, 0.8973800514282001, 0.8634132304309223, 0.8472812455205514, 0.1985001542527517, 0.1585146185947931, 0.8337654020690852, 0.16815553765976654, 0.16767777920264038, 0.8145817251939893, 0.1661887704123024, 0.2002384045022092, 0.21270385229000333, 0.15502895016859852, 0.15581584216296274, 0.44112899395539285, 0.2704890640209542, 0.317178982997638, 0.2965929127827681, 0.4314839426197882, 0.2443844071739082, 0.2272806115444309, 0.24705198287063868, 0.2919163957284867, 0.10742674978866673, 0.09615090976507867, 0.08932957373600692, 0.10039145184892406, 0.07777519619601359, 0.09346825340867582, 0.09191978538549628, 0.10051502779803445, 0.09032627828645579]}, "mutation_prompt": null}
{"id": "665441bd-9b3d-421a-abf9-02d9b35fb42b", "solution": "import numpy as np\n\nclass RefinedHybridStochasticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.initial_scale_factor = 0.85  # Adjusted for slightly more aggressive mutations\n        self.initial_crossover_prob = 0.75  # Increase crossover probability to enhance diversity\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population = self.opposition_based_initialization()\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.learning_rate = 0.12  # Slightly increased learning rate for faster local search\n        self.elitism_rate = 0.12  # Slightly increased elitism for preserving more top candidates\n\n    def opposition_based_initialization(self):\n        initial_population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        opposition_population = self.lower_bound + self.upper_bound - initial_population\n        combined_population = np.vstack((initial_population, opposition_population))\n        fitness = np.array([np.inf] * len(combined_population))\n        for i in range(len(combined_population)):\n            fitness[i] = np.random.rand()  # Introduce randomness for better initial diversity\n        best_indices = np.argsort(fitness)[:self.population_size]\n        return combined_population[best_indices]\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def select_parents(self, idx):\n        indices = [i for i in range(self.population_size) if i != idx]\n        return np.random.choice(indices, 3, replace=False)\n\n    def differential_evolution_mutation(self, idx, scale_factor):\n        a, b, c = self.select_parents(idx)\n        mutant_vector = self.population[a] + scale_factor * (self.population[b] - self.population[c])\n        return np.clip(mutant_vector, self.lower_bound, self.upper_bound)\n\n    def crossover(self, target_vector, mutant_vector, crossover_prob):\n        crossover_mask = np.random.rand(self.dim) < crossover_prob\n        return np.where(crossover_mask, mutant_vector, target_vector)\n\n    def local_search(self, vector):\n        perturbation = np.random.normal(0, self.learning_rate, self.dim)\n        candidate_vector = vector + perturbation\n        return np.clip(candidate_vector, self.lower_bound, self.upper_bound)\n\n    def elitism_selection(self):\n        num_elites = max(1, int(self.elitism_rate * self.population_size))\n        elite_indices = np.argsort(self.fitness)[:num_elites]\n        return self.population[elite_indices], self.fitness[elite_indices]\n\n    def __call__(self, func):\n        self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            elites, elite_fitness = self.elitism_selection()\n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                scale_factor = self.initial_scale_factor * (1 - self.evaluations / self.budget)\n                crossover_prob = self.initial_crossover_prob + 0.15 * np.random.rand()  # Narrow crossover randomness\n\n                if i < len(elites):  # Use elite individuals\n                    candidate_vector = self.local_search(elites[i])\n                else:\n                    mutant_vector = self.differential_evolution_mutation(i, scale_factor)\n                    candidate_vector = self.crossover(self.population[i], mutant_vector, crossover_prob)\n\n                candidate_fitness = func(candidate_vector)\n                self.evaluations += 1\n\n                if candidate_fitness < self.fitness[i]:\n                    self.population[i] = candidate_vector\n                    self.fitness[i] = candidate_fitness\n\n                self.learning_rate = max(0.01, self.learning_rate * (1 - 0.04 * np.random.rand()))  # Adjusted learning decay\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx]", "name": "RefinedHybridStochasticOptimizer", "description": "A refined hybrid optimizer with adaptive opposition strategy and dynamic exploration-exploitation balance for enhanced convergence speed.", "configspace": "", "generation": 85, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "611e17be-2543-4b24-a742-9175f71fb862", "metadata": {"aucs": [0.8766756452394434, 0.8780259518892735, 0.8679732870259016, 0.8851321249681726, 0.8834248924740489, 0.8917330111634347, 0.8915409061661912, 0.8625223086831446, 0.8855384988765383, 0.7702063528242651, 0.764605999911898, 0.7866633313683472, 0.788225311474078, 0.7768513285041114, 0.783710568385616, 0.7710553972167912, 0.7826320564567563, 0.7747122369896489, 0.7207065189713453, 0.6772268580219241, 0.17524904499097194, 0.1545419816557405, 0.12287270778105774, 0.689897670488521, 0.17408711892974027, 0.7264644514688223, 0.17782525894103152, 0.5540661022978465, 0.1718555788393058, 0.17432731451169114, 0.1401812381721591, 0.181817031126965, 0.17084893094232578, 0.14077243420807695, 0.11782626428419385, 0.1411557885190976, 0.9937933542659784, 0.9824818277808939, 0.980707506224134, 0.9544651636952174, 0.9824601842623015, 0.9786103071199913, 0.9917495387625039, 0.9773550272665161, 0.9767648277870316, 0.7186425059203514, 0.7072341755938936, 0.67619418713103, 0.5500313926892695, 0.6353964019006688, 0.5655276295816347, 0.6087248017767631, 0.7250088325007087, 0.5509231661245519, 0.8306251087247614, 0.3722435473015898, 0.8654864869334986, 0.8863503903735646, 0.8760746636972028, 0.8458760719743885, 0.8486549335282704, 0.8432469041054449, 0.3860328241903793, 0.3132235440659491, 0.4899210907813042, 0.2895826691181592, 0.19744367076628966, 0.4358144265843764, 0.19907439487724254, 0.36614638947632094, 0.4673958269450197, 0.4412371012727202, 0.1285886837615413, 0.23403231590111162, 0.36109192393377976, 0.22664878079649053, 0.2540072344222718, 0.4265671714982343, 0.2827316237596058, 0.6431777309769313, 0.3732167479133396, 0.12116522298168053, 0.1678990673445524, 0.3052731722642458, 0.04944125032055546, 0.42013758686785785, 0.4675757890099749, 0.34909871477201915, 0.44848393730635916, 0.4121936894642598, 0.2572264031099728, 0.14782089879620763, 0.45511882842056595, 0.3614252035664588, 0.12576950129727194, 0.19368490486653112, 0.5599530644881539, 0.4930337585131278, 0.48359551057535666, 0.31457189059998514, 0.12229189958524489, 0.1343438096316968, 0.1129583796588105, 0.17065635226036613, 0.21121710331876486, 0.17960579905235896, 0.13080473597692177, 0.17281085533956853, 0.3785105716917646, 0.32356504637548955, 0.2930510023224221, 0.4263055259587192, 0.3758837171163727, 0.3487130962919658, 0.34061488164921405, 0.38468365570003926, 0.36601498696637014, 0.684964500616335, 0.7393164733752317, 0.7681352739007481, 0.7580447340486791, 0.6512200131325806, 0.7160469344263029, 0.5965650033008165, 0.7695202275323955, 0.7091180301825857, 0.13027852195339762, 0.1752862344756313, 0.14725999438068593, 0.10963074171804343, 0.12196205006310135, 0.12842695335615995, 0.1535536314706123, 0.09324353688930664, 0.11756250657323708, 0.1583195481084767, 0.17804157548667843, 0.27489139187246203, 0.21745895839396734, 0.667095829932591, 0.20628860250021674, 0.5805893447980215, 0.2351413331273987, 0.23368786575655498, 0.5245002013696674, 0.5809814491680333, 0.4604348637764978, 0.5487855436976408, 0.5109761909197574, 0.5670142492913299, 0.5523316647904777, 0.6211521395155045, 0.5065222018957187, 0.43913043730592705, 0.4209119941181332, 0.24723244458163163, 0.36854984117381473, 0.4714349416601522, 0.4087858118863441, 0.4445023749465481, 0.47953063462436707, 0.4056768097816228, 0.20187088443067747, 0.28194838553846535, 0.22079563605800234, 0.2193518224022324, 0.18672017534619767, 0.21755812838823685, 0.21888893291732603, 0.20535976400015987, 0.24766721582344342, 0.7731611289963906, 0.22212714398023892, 0.22683569749128651, 0.2219774539530932, 0.2038861631536516, 0.22700750845132922, 0.19396010038295208, 0.19566429190218382, 0.24739215490626576, 0.16341617726849111, 0.8639742256032173, 0.18598438634511893, 0.19365704699243147, 0.8973800514282001, 0.8634132304309223, 0.8472812455205514, 0.1985001542527517, 0.1585146185947931, 0.8337654020690852, 0.16815553765976654, 0.16767777920264038, 0.8145817251939893, 0.1661887704123024, 0.2002384045022092, 0.21270385229000333, 0.15502895016859852, 0.15581584216296274, 0.44112899395539285, 0.2704890640209542, 0.317178982997638, 0.2965929127827681, 0.4314839426197882, 0.2443844071739082, 0.2272806115444309, 0.24705198287063868, 0.2919163957284867, 0.10742674978866673, 0.09615090976507867, 0.08932957373600692, 0.10039145184892406, 0.07777519619601359, 0.09346825340867582, 0.09191978538549628, 0.10051502779803445, 0.09032627828645579]}, "mutation_prompt": null}
{"id": "cf225eb1-ab2a-4256-8798-1d5d46da4838", "solution": "import numpy as np\n\nclass RefinedHybridStochasticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.initial_scale_factor = 0.85  # Adjusted for slightly more aggressive mutations\n        self.initial_crossover_prob = 0.75  # Increase crossover probability to enhance diversity\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population = self.opposition_based_initialization()\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.learning_rate = 0.12  # Slightly increased learning rate for faster local search\n        self.elitism_rate = 0.12  # Slightly increased elitism for preserving more top candidates\n\n    def opposition_based_initialization(self):\n        initial_population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        opposition_population = self.lower_bound + self.upper_bound - initial_population\n        combined_population = np.vstack((initial_population, opposition_population))\n        fitness = np.array([np.inf] * len(combined_population))\n        for i in range(len(combined_population)):\n            fitness[i] = np.random.rand()  # Introduce randomness for better initial diversity\n        best_indices = np.argsort(fitness)[:self.population_size]\n        return combined_population[best_indices]\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def select_parents(self, idx):\n        indices = [i for i in range(self.population_size) if i != idx]\n        return np.random.choice(indices, 3, replace=False)\n\n    def differential_evolution_mutation(self, idx, scale_factor):\n        a, b, c = self.select_parents(idx)\n        mutant_vector = self.population[a] + scale_factor * (self.population[b] - self.population[c])\n        return np.clip(mutant_vector, self.lower_bound, self.upper_bound)\n\n    def crossover(self, target_vector, mutant_vector, crossover_prob):\n        crossover_mask = np.random.rand(self.dim) < crossover_prob\n        return np.where(crossover_mask, mutant_vector, target_vector)\n\n    def local_search(self, vector):\n        perturbation = np.random.normal(0, self.learning_rate, self.dim)\n        candidate_vector = vector + perturbation\n        return np.clip(candidate_vector, self.lower_bound, self.upper_bound)\n\n    def elitism_selection(self):\n        num_elites = max(1, int(self.elitism_rate * self.population_size))\n        elite_indices = np.argsort(self.fitness)[:num_elites]\n        return self.population[elite_indices], self.fitness[elite_indices]\n\n    def __call__(self, func):\n        self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            elites, elite_fitness = self.elitism_selection()\n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                scale_factor = self.initial_scale_factor * (1 - self.evaluations / self.budget)\n                crossover_prob = self.initial_crossover_prob + 0.15 * np.random.rand()  # Narrow crossover randomness\n\n                if i < len(elites):  # Use elite individuals\n                    candidate_vector = self.local_search(elites[i])\n                else:\n                    mutant_vector = self.differential_evolution_mutation(i, scale_factor)\n                    candidate_vector = self.crossover(self.population[i], mutant_vector, crossover_prob)\n\n                candidate_fitness = func(candidate_vector)\n                self.evaluations += 1\n\n                if candidate_fitness < self.fitness[i]:\n                    self.population[i] = candidate_vector\n                    self.fitness[i] = candidate_fitness\n\n                self.learning_rate = max(0.01, self.learning_rate * (1 - 0.04 * np.random.rand()))  # Adjusted learning decay\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx]", "name": "RefinedHybridStochasticOptimizer", "description": "A refined hybrid optimizer with adaptive opposition strategy and dynamic exploration-exploitation balance for enhanced convergence speed.", "configspace": "", "generation": 85, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "611e17be-2543-4b24-a742-9175f71fb862", "metadata": {"aucs": [0.8766756452394434, 0.8780259518892735, 0.8679732870259016, 0.8851321249681726, 0.8834248924740489, 0.8917330111634347, 0.8915409061661912, 0.8625223086831446, 0.8855384988765383, 0.7702063528242651, 0.764605999911898, 0.7866633313683472, 0.788225311474078, 0.7768513285041114, 0.783710568385616, 0.7710553972167912, 0.7826320564567563, 0.7747122369896489, 0.7207065189713453, 0.6772268580219241, 0.17524904499097194, 0.1545419816557405, 0.12287270778105774, 0.689897670488521, 0.17408711892974027, 0.7264644514688223, 0.17782525894103152, 0.5540661022978465, 0.1718555788393058, 0.17432731451169114, 0.1401812381721591, 0.181817031126965, 0.17084893094232578, 0.14077243420807695, 0.11782626428419385, 0.1411557885190976, 0.9937933542659784, 0.9824818277808939, 0.980707506224134, 0.9544651636952174, 0.9824601842623015, 0.9786103071199913, 0.9917495387625039, 0.9773550272665161, 0.9767648277870316, 0.7186425059203514, 0.7072341755938936, 0.67619418713103, 0.5500313926892695, 0.6353964019006688, 0.5655276295816347, 0.6087248017767631, 0.7250088325007087, 0.5509231661245519, 0.8306251087247614, 0.3722435473015898, 0.8654864869334986, 0.8863503903735646, 0.8760746636972028, 0.8458760719743885, 0.8486549335282704, 0.8432469041054449, 0.3860328241903793, 0.3132235440659491, 0.4899210907813042, 0.2895826691181592, 0.19744367076628966, 0.4358144265843764, 0.19907439487724254, 0.36614638947632094, 0.4673958269450197, 0.4412371012727202, 0.1285886837615413, 0.23403231590111162, 0.36109192393377976, 0.22664878079649053, 0.2540072344222718, 0.4265671714982343, 0.2827316237596058, 0.6431777309769313, 0.3732167479133396, 0.12116522298168053, 0.1678990673445524, 0.3052731722642458, 0.04944125032055546, 0.42013758686785785, 0.4675757890099749, 0.34909871477201915, 0.44848393730635916, 0.4121936894642598, 0.2572264031099728, 0.14782089879620763, 0.45511882842056595, 0.3614252035664588, 0.12576950129727194, 0.19368490486653112, 0.5599530644881539, 0.4930337585131278, 0.48359551057535666, 0.31457189059998514, 0.12229189958524489, 0.1343438096316968, 0.1129583796588105, 0.17065635226036613, 0.21121710331876486, 0.17960579905235896, 0.13080473597692177, 0.17281085533956853, 0.3785105716917646, 0.32356504637548955, 0.2930510023224221, 0.4263055259587192, 0.3758837171163727, 0.3487130962919658, 0.34061488164921405, 0.38468365570003926, 0.36601498696637014, 0.684964500616335, 0.7393164733752317, 0.7681352739007481, 0.7580447340486791, 0.6512200131325806, 0.7160469344263029, 0.5965650033008165, 0.7695202275323955, 0.7091180301825857, 0.13027852195339762, 0.1752862344756313, 0.14725999438068593, 0.10963074171804343, 0.12196205006310135, 0.12842695335615995, 0.1535536314706123, 0.09324353688930664, 0.11756250657323708, 0.1583195481084767, 0.17804157548667843, 0.27489139187246203, 0.21745895839396734, 0.667095829932591, 0.20628860250021674, 0.5805893447980215, 0.2351413331273987, 0.23368786575655498, 0.5245002013696674, 0.5809814491680333, 0.4604348637764978, 0.5487855436976408, 0.5109761909197574, 0.5670142492913299, 0.5523316647904777, 0.6211521395155045, 0.5065222018957187, 0.43913043730592705, 0.4209119941181332, 0.24723244458163163, 0.36854984117381473, 0.4714349416601522, 0.4087858118863441, 0.4445023749465481, 0.47953063462436707, 0.4056768097816228, 0.20187088443067747, 0.28194838553846535, 0.22079563605800234, 0.2193518224022324, 0.18672017534619767, 0.21755812838823685, 0.21888893291732603, 0.20535976400015987, 0.24766721582344342, 0.7731611289963906, 0.22212714398023892, 0.22683569749128651, 0.2219774539530932, 0.2038861631536516, 0.22700750845132922, 0.19396010038295208, 0.19566429190218382, 0.24739215490626576, 0.16341617726849111, 0.8639742256032173, 0.18598438634511893, 0.19365704699243147, 0.8973800514282001, 0.8634132304309223, 0.8472812455205514, 0.1985001542527517, 0.1585146185947931, 0.8337654020690852, 0.16815553765976654, 0.16767777920264038, 0.8145817251939893, 0.1661887704123024, 0.2002384045022092, 0.21270385229000333, 0.15502895016859852, 0.15581584216296274, 0.44112899395539285, 0.2704890640209542, 0.317178982997638, 0.2965929127827681, 0.4314839426197882, 0.2443844071739082, 0.2272806115444309, 0.24705198287063868, 0.2919163957284867, 0.10742674978866673, 0.09615090976507867, 0.08932957373600692, 0.10039145184892406, 0.07777519619601359, 0.09346825340867582, 0.09191978538549628, 0.10051502779803445, 0.09032627828645579]}, "mutation_prompt": null}
{"id": "8bd13218-da10-4f1f-8640-aa867f4aeaec", "solution": "import numpy as np\n\nclass EnhancedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 22  # Slightly increased population size for diversity\n        self.scale_factor = 0.9  # Increased scale factor for more aggressive mutations\n        self.crossover_prob = 0.8  # Increased crossover probability to enhance mixture\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population = self.initialization()\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.learning_rate = 0.15  # Increased learning rate for more significant local search\n        self.elitism_rate = 0.15  # Adjusted elitism to maintain competitive diversity\n\n    def initialization(self):\n        initial_population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        fitness = np.array([np.inf] * len(initial_population))\n        for i in range(len(initial_population)):\n            fitness[i] = np.random.rand()\n        best_indices = np.argsort(fitness)[:self.population_size]\n        return initial_population[best_indices]\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def select_parents(self, idx):\n        indices = [i for i in range(self.population_size) if i != idx]\n        return np.random.choice(indices, 3, replace=False)\n\n    def differential_evolution_mutation(self, idx):\n        a, b, c = self.select_parents(idx)\n        mutant_vector = self.population[a] + self.scale_factor * (self.population[b] - self.population[c])\n        return np.clip(mutant_vector, self.lower_bound, self.upper_bound)\n\n    def crossover(self, target_vector, mutant_vector):\n        crossover_mask = np.random.rand(self.dim) < self.crossover_prob\n        return np.where(crossover_mask, mutant_vector, target_vector)\n\n    def local_search(self, vector):\n        perturbation = np.random.normal(0, self.learning_rate, self.dim)\n        candidate_vector = vector + perturbation\n        return np.clip(candidate_vector, self.lower_bound, self.upper_bound)\n\n    def elitism_selection(self):\n        num_elites = max(1, int(self.elitism_rate * self.population_size))\n        elite_indices = np.argsort(self.fitness)[:num_elites]\n        return self.population[elite_indices], self.fitness[elite_indices]\n\n    def __call__(self, func):\n        self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            elites, elite_fitness = self.elitism_selection()\n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                if i < len(elites):\n                    candidate_vector = self.local_search(elites[i])\n                else:\n                    mutant_vector = self.differential_evolution_mutation(i)\n                    candidate_vector = self.crossover(self.population[i], mutant_vector)\n\n                candidate_fitness = func(candidate_vector)\n                self.evaluations += 1\n\n                if candidate_fitness < self.fitness[i]:\n                    self.population[i] = candidate_vector\n                    self.fitness[i] = candidate_fitness\n\n                self.learning_rate = max(0.01, self.learning_rate * (1 - 0.03 * np.random.rand()))\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx]", "name": "EnhancedHybridOptimizer", "description": "Enhanced Hybrid Optimizer with dynamic adaptive strategies and refined local search for accelerated convergence.", "configspace": "", "generation": 87, "fitness": 0.3953176806700475, "feedback": "The algorithm EnhancedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.40 with standard deviation 0.26.", "error": "", "parent_id": "611e17be-2543-4b24-a742-9175f71fb862", "metadata": {"aucs": [0.808343570759986, 0.8245828224878968, 0.8225111127012231, 0.8481921823554263, 0.8272874839188539, 0.8175216209644943, 0.8396750662853323, 0.8367946933945751, 0.8421070724953386, 0.6578328676591205, 0.6444592218575679, 0.6418369900646248, 0.6099224521651987, 0.6474984273410049, 0.6273713121605851, 0.6213017892398139, 0.6037000633227816, 0.6531597658613317, 0.18070843730107078, 0.5547959031093757, 0.6211263141458693, 0.13437131503312572, 0.6011697603799341, 0.17089372720511586, 0.1584116533837756, 0.5508990753519749, 0.563933674196756, 0.12038667034255235, 0.5188927358458173, 0.5327545630688617, 0.1729562748607777, 0.13866713766104022, 0.4780887050788395, 0.1755696102662203, 0.14078757601824143, 0.1836081014636981, 0.9854989412851825, 0.9716028493841492, 0.989297025205313, 0.9636035011612646, 0.9785346257897646, 0.9819531111110068, 0.9877024164196505, 0.9700926112899667, 0.9649655530439021, 0.51166172247864, 0.5318955640710573, 0.5466831941481911, 0.4889045985332521, 0.47874925459340223, 0.4915771750258723, 0.5145760559829138, 0.5119129797370489, 0.515960477317299, 0.7890447473654997, 0.16824087209492644, 0.7625374422113682, 0.20698193823231448, 0.763216156024068, 0.7720212985282815, 0.8018062501570395, 0.8029365728159696, 0.8287598830843074, 0.13026955631045534, 0.408166416248119, 0.36446512857198954, 0.1250345467942674, 0.4070205679609461, 0.42709609856812747, 0.406039309909071, 0.3821100694241448, 0.369864040025478, 0.26977808375246703, 0.1321693730721044, 0.37392683500789403, 0.3565415613705658, 0.3793459282398284, 0.3557990200424175, 0.3881187574592212, 0.37127131728049967, 0.34924515789991606, 0.10621136371275275, 0.26713522313905314, 0.22438079944496325, 0.24064723866380144, 0.15791800316275206, 0.14859294872308892, 0.1186819319815704, 0.20228044151636837, 0.21424733487426606, 0.34540389447927466, 0.30197347715195877, 0.336694183878972, 0.34925578349106334, 0.2903186255995652, 0.22742070726507058, 0.22075623326527283, 0.37387414297387744, 0.3422201065967092, 0.1502545860341965, 0.047808668845785385, 0.145489761477255, 0.18896133817694138, 0.1038025119106154, 0.14098341369584655, 0.18429153274594579, 0.06603846815681158, 0.0731741133539755, 0.21715979486296733, 0.2335076325833656, 0.2309349684628187, 0.2636757906059183, 0.24224645696609914, 0.24263892534411746, 0.26819256108746214, 0.21253483859167288, 0.21246746431841634, 0.6439149636656208, 0.6112161054275103, 0.6240884892282608, 0.6106954302803724, 0.6228731550030531, 0.6139792205871804, 0.6405001604728031, 0.5698658153501939, 0.614720048871588, 0.15022446496584285, 0.11768106466581207, 0.14072960416025526, 0.1570467126861106, 0.12912597806278003, 0.15456228800744753, 0.1357294440412823, 0.1281673015556395, 0.1048064007615761, 0.26211258988408204, 0.35133253769016604, 0.17605007161710362, 0.45327649064173714, 0.2298671724365875, 0.23201022269985783, 0.20934447243631138, 0.16744789223926304, 0.2103609797824263, 0.39917233950792597, 0.39759176748347147, 0.3829423951614468, 0.377762601506596, 0.3920653161079043, 0.40178635039940924, 0.4048886935054907, 0.4432699745002233, 0.41535989139463003, 0.3057611149357642, 0.2667227560352755, 0.29699319471547136, 0.31706750133903483, 0.31240696500521914, 0.28944492026452684, 0.25086383266777446, 0.29268908168897034, 0.30566484039006614, 0.2168216941642268, 0.22775579445690952, 0.24645995253159714, 0.19047675782966011, 0.1786406939585249, 0.224162230327974, 0.2818122149970288, 0.22188423372856914, 0.19068108023402375, 0.1990701604853674, 0.21755358488765875, 0.6830970318254832, 0.6195108210253302, 0.20495405131843647, 0.18323490990112257, 0.22085186096195442, 0.62948800089563, 0.24690934387225405, 0.8240458910615129, 0.8845107088067572, 0.20939793652121952, 0.8632279487211659, 0.863918822313356, 0.798730071776074, 0.7928835124102388, 0.20463745993601623, 0.8374382492135881, 0.7902793981282401, 0.1674957713333317, 0.8061606942489522, 0.20888983872801847, 0.20685490055546008, 0.20390015861136968, 0.12693279217995312, 0.21157143507289278, 0.7967779316378156, 0.2172757563409683, 0.24167860623267312, 0.23184685625568113, 0.20481766674537671, 0.222942414581087, 0.24628672499388604, 0.2404962682850187, 0.24925472891539635, 0.2676902895924218, 0.08716080927896941, 0.07659052457452309, 0.09138364904600216, 0.07930107166640121, 0.1079108111401198, 0.09444777189122178, 0.11423001563936286, 0.09948280672425203, 0.07934809842115631]}, "mutation_prompt": null}
{"id": "1162789e-0762-450d-8ca9-66462e7d6754", "solution": "import numpy as np\n\nclass RefinedHybridStochasticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.initial_scale_factor = 0.85  # Adjusted for slightly more aggressive mutations\n        self.initial_crossover_prob = 0.75  # Increase crossover probability to enhance diversity\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population = self.opposition_based_initialization()\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.learning_rate = 0.12  # Slightly increased learning rate for faster local search\n        self.elitism_rate = 0.12  # Slightly increased elitism for preserving more top candidates\n\n    def opposition_based_initialization(self):\n        initial_population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        opposition_population = self.lower_bound + self.upper_bound - initial_population\n        combined_population = np.vstack((initial_population, opposition_population))\n        fitness = np.array([np.inf] * len(combined_population))\n        for i in range(len(combined_population)):\n            fitness[i] = np.random.rand()  # Introduce randomness for better initial diversity\n        best_indices = np.argsort(fitness)[:self.population_size]\n        return combined_population[best_indices]\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def select_parents(self, idx):\n        indices = [i for i in range(self.population_size) if i != idx]\n        return np.random.choice(indices, 3, replace=False)\n\n    def differential_evolution_mutation(self, idx, scale_factor):\n        a, b, c = self.select_parents(idx)\n        mutant_vector = self.population[a] + scale_factor * (self.population[b] - self.population[c])\n        return np.clip(mutant_vector, self.lower_bound, self.upper_bound)\n\n    def crossover(self, target_vector, mutant_vector, crossover_prob):\n        crossover_mask = np.random.rand(self.dim) < crossover_prob\n        return np.where(crossover_mask, mutant_vector, target_vector)\n\n    def local_search(self, vector):\n        perturbation = np.random.normal(0, self.learning_rate, self.dim)\n        candidate_vector = vector + perturbation\n        return np.clip(candidate_vector, self.lower_bound, self.upper_bound)\n\n    def elitism_selection(self):\n        num_elites = max(1, int(self.elitism_rate * self.population_size))\n        elite_indices = np.argsort(self.fitness)[:num_elites]\n        return self.population[elite_indices], self.fitness[elite_indices]\n\n    def __call__(self, func):\n        self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            elites, elite_fitness = self.elitism_selection()\n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                scale_factor = self.initial_scale_factor * (1 - self.evaluations / self.budget)\n                crossover_prob = self.initial_crossover_prob + 0.15 * np.random.rand()  # Narrow crossover randomness\n\n                if i < len(elites):  # Use elite individuals\n                    candidate_vector = self.local_search(elites[i])\n                else:\n                    mutant_vector = self.differential_evolution_mutation(i, scale_factor)\n                    candidate_vector = self.crossover(self.population[i], mutant_vector, crossover_prob)\n\n                candidate_fitness = func(candidate_vector)\n                self.evaluations += 1\n\n                if candidate_fitness < self.fitness[i]:\n                    self.population[i] = candidate_vector\n                    self.fitness[i] = candidate_fitness\n\n                self.learning_rate = max(0.01, self.learning_rate * (1 - 0.04 * np.random.rand()))  # Adjusted learning decay\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx]", "name": "RefinedHybridStochasticOptimizer", "description": "A refined hybrid optimizer with adaptive opposition strategy and dynamic exploration-exploitation balance for enhanced convergence speed.", "configspace": "", "generation": 85, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "611e17be-2543-4b24-a742-9175f71fb862", "metadata": {"aucs": [0.8766756452394434, 0.8780259518892735, 0.8679732870259016, 0.8851321249681726, 0.8834248924740489, 0.8917330111634347, 0.8915409061661912, 0.8625223086831446, 0.8855384988765383, 0.7702063528242651, 0.764605999911898, 0.7866633313683472, 0.788225311474078, 0.7768513285041114, 0.783710568385616, 0.7710553972167912, 0.7826320564567563, 0.7747122369896489, 0.7207065189713453, 0.6772268580219241, 0.17524904499097194, 0.1545419816557405, 0.12287270778105774, 0.689897670488521, 0.17408711892974027, 0.7264644514688223, 0.17782525894103152, 0.5540661022978465, 0.1718555788393058, 0.17432731451169114, 0.1401812381721591, 0.181817031126965, 0.17084893094232578, 0.14077243420807695, 0.11782626428419385, 0.1411557885190976, 0.9937933542659784, 0.9824818277808939, 0.980707506224134, 0.9544651636952174, 0.9824601842623015, 0.9786103071199913, 0.9917495387625039, 0.9773550272665161, 0.9767648277870316, 0.7186425059203514, 0.7072341755938936, 0.67619418713103, 0.5500313926892695, 0.6353964019006688, 0.5655276295816347, 0.6087248017767631, 0.7250088325007087, 0.5509231661245519, 0.8306251087247614, 0.3722435473015898, 0.8654864869334986, 0.8863503903735646, 0.8760746636972028, 0.8458760719743885, 0.8486549335282704, 0.8432469041054449, 0.3860328241903793, 0.3132235440659491, 0.4899210907813042, 0.2895826691181592, 0.19744367076628966, 0.4358144265843764, 0.19907439487724254, 0.36614638947632094, 0.4673958269450197, 0.4412371012727202, 0.1285886837615413, 0.23403231590111162, 0.36109192393377976, 0.22664878079649053, 0.2540072344222718, 0.4265671714982343, 0.2827316237596058, 0.6431777309769313, 0.3732167479133396, 0.12116522298168053, 0.1678990673445524, 0.3052731722642458, 0.04944125032055546, 0.42013758686785785, 0.4675757890099749, 0.34909871477201915, 0.44848393730635916, 0.4121936894642598, 0.2572264031099728, 0.14782089879620763, 0.45511882842056595, 0.3614252035664588, 0.12576950129727194, 0.19368490486653112, 0.5599530644881539, 0.4930337585131278, 0.48359551057535666, 0.31457189059998514, 0.12229189958524489, 0.1343438096316968, 0.1129583796588105, 0.17065635226036613, 0.21121710331876486, 0.17960579905235896, 0.13080473597692177, 0.17281085533956853, 0.3785105716917646, 0.32356504637548955, 0.2930510023224221, 0.4263055259587192, 0.3758837171163727, 0.3487130962919658, 0.34061488164921405, 0.38468365570003926, 0.36601498696637014, 0.684964500616335, 0.7393164733752317, 0.7681352739007481, 0.7580447340486791, 0.6512200131325806, 0.7160469344263029, 0.5965650033008165, 0.7695202275323955, 0.7091180301825857, 0.13027852195339762, 0.1752862344756313, 0.14725999438068593, 0.10963074171804343, 0.12196205006310135, 0.12842695335615995, 0.1535536314706123, 0.09324353688930664, 0.11756250657323708, 0.1583195481084767, 0.17804157548667843, 0.27489139187246203, 0.21745895839396734, 0.667095829932591, 0.20628860250021674, 0.5805893447980215, 0.2351413331273987, 0.23368786575655498, 0.5245002013696674, 0.5809814491680333, 0.4604348637764978, 0.5487855436976408, 0.5109761909197574, 0.5670142492913299, 0.5523316647904777, 0.6211521395155045, 0.5065222018957187, 0.43913043730592705, 0.4209119941181332, 0.24723244458163163, 0.36854984117381473, 0.4714349416601522, 0.4087858118863441, 0.4445023749465481, 0.47953063462436707, 0.4056768097816228, 0.20187088443067747, 0.28194838553846535, 0.22079563605800234, 0.2193518224022324, 0.18672017534619767, 0.21755812838823685, 0.21888893291732603, 0.20535976400015987, 0.24766721582344342, 0.7731611289963906, 0.22212714398023892, 0.22683569749128651, 0.2219774539530932, 0.2038861631536516, 0.22700750845132922, 0.19396010038295208, 0.19566429190218382, 0.24739215490626576, 0.16341617726849111, 0.8639742256032173, 0.18598438634511893, 0.19365704699243147, 0.8973800514282001, 0.8634132304309223, 0.8472812455205514, 0.1985001542527517, 0.1585146185947931, 0.8337654020690852, 0.16815553765976654, 0.16767777920264038, 0.8145817251939893, 0.1661887704123024, 0.2002384045022092, 0.21270385229000333, 0.15502895016859852, 0.15581584216296274, 0.44112899395539285, 0.2704890640209542, 0.317178982997638, 0.2965929127827681, 0.4314839426197882, 0.2443844071739082, 0.2272806115444309, 0.24705198287063868, 0.2919163957284867, 0.10742674978866673, 0.09615090976507867, 0.08932957373600692, 0.10039145184892406, 0.07777519619601359, 0.09346825340867582, 0.09191978538549628, 0.10051502779803445, 0.09032627828645579]}, "mutation_prompt": null}
{"id": "a836fb74-ccf8-49cd-8f26-89df6729db81", "solution": "import numpy as np\n\nclass RefinedHybridStochasticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.initial_scale_factor = 0.85  # Adjusted for slightly more aggressive mutations\n        self.initial_crossover_prob = 0.75  # Increase crossover probability to enhance diversity\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population = self.opposition_based_initialization()\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.learning_rate = 0.12  # Slightly increased learning rate for faster local search\n        self.elitism_rate = 0.12  # Slightly increased elitism for preserving more top candidates\n\n    def opposition_based_initialization(self):\n        initial_population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        opposition_population = self.lower_bound + self.upper_bound - initial_population\n        combined_population = np.vstack((initial_population, opposition_population))\n        fitness = np.array([np.inf] * len(combined_population))\n        for i in range(len(combined_population)):\n            fitness[i] = np.random.rand()  # Introduce randomness for better initial diversity\n        best_indices = np.argsort(fitness)[:self.population_size]\n        return combined_population[best_indices]\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def select_parents(self, idx):\n        indices = [i for i in range(self.population_size) if i != idx]\n        return np.random.choice(indices, 3, replace=False)\n\n    def differential_evolution_mutation(self, idx, scale_factor):\n        a, b, c = self.select_parents(idx)\n        mutant_vector = self.population[a] + scale_factor * (self.population[b] - self.population[c])\n        return np.clip(mutant_vector, self.lower_bound, self.upper_bound)\n\n    def crossover(self, target_vector, mutant_vector, crossover_prob):\n        crossover_mask = np.random.rand(self.dim) < crossover_prob\n        return np.where(crossover_mask, mutant_vector, target_vector)\n\n    def local_search(self, vector):\n        perturbation = np.random.normal(0, self.learning_rate, self.dim)\n        candidate_vector = vector + perturbation\n        return np.clip(candidate_vector, self.lower_bound, self.upper_bound)\n\n    def elitism_selection(self):\n        num_elites = max(1, int(self.elitism_rate * self.population_size))\n        elite_indices = np.argsort(self.fitness)[:num_elites]\n        return self.population[elite_indices], self.fitness[elite_indices]\n\n    def __call__(self, func):\n        self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            elites, elite_fitness = self.elitism_selection()\n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                scale_factor = self.initial_scale_factor * (1 - self.evaluations / self.budget)\n                crossover_prob = self.initial_crossover_prob + 0.15 * np.random.rand()  # Narrow crossover randomness\n\n                if i < len(elites):  # Use elite individuals\n                    candidate_vector = self.local_search(elites[i])\n                else:\n                    mutant_vector = self.differential_evolution_mutation(i, scale_factor)\n                    candidate_vector = self.crossover(self.population[i], mutant_vector, crossover_prob)\n\n                candidate_fitness = func(candidate_vector)\n                self.evaluations += 1\n\n                if candidate_fitness < self.fitness[i]:\n                    self.population[i] = candidate_vector\n                    self.fitness[i] = candidate_fitness\n\n                self.learning_rate = max(0.01, self.learning_rate * (1 - 0.04 * np.random.rand()))  # Adjusted learning decay\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx]", "name": "RefinedHybridStochasticOptimizer", "description": "A refined hybrid optimizer with adaptive opposition strategy and dynamic exploration-exploitation balance for enhanced convergence speed.", "configspace": "", "generation": 85, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "611e17be-2543-4b24-a742-9175f71fb862", "metadata": {"aucs": [0.8766756452394434, 0.8780259518892735, 0.8679732870259016, 0.8851321249681726, 0.8834248924740489, 0.8917330111634347, 0.8915409061661912, 0.8625223086831446, 0.8855384988765383, 0.7702063528242651, 0.764605999911898, 0.7866633313683472, 0.788225311474078, 0.7768513285041114, 0.783710568385616, 0.7710553972167912, 0.7826320564567563, 0.7747122369896489, 0.7207065189713453, 0.6772268580219241, 0.17524904499097194, 0.1545419816557405, 0.12287270778105774, 0.689897670488521, 0.17408711892974027, 0.7264644514688223, 0.17782525894103152, 0.5540661022978465, 0.1718555788393058, 0.17432731451169114, 0.1401812381721591, 0.181817031126965, 0.17084893094232578, 0.14077243420807695, 0.11782626428419385, 0.1411557885190976, 0.9937933542659784, 0.9824818277808939, 0.980707506224134, 0.9544651636952174, 0.9824601842623015, 0.9786103071199913, 0.9917495387625039, 0.9773550272665161, 0.9767648277870316, 0.7186425059203514, 0.7072341755938936, 0.67619418713103, 0.5500313926892695, 0.6353964019006688, 0.5655276295816347, 0.6087248017767631, 0.7250088325007087, 0.5509231661245519, 0.8306251087247614, 0.3722435473015898, 0.8654864869334986, 0.8863503903735646, 0.8760746636972028, 0.8458760719743885, 0.8486549335282704, 0.8432469041054449, 0.3860328241903793, 0.3132235440659491, 0.4899210907813042, 0.2895826691181592, 0.19744367076628966, 0.4358144265843764, 0.19907439487724254, 0.36614638947632094, 0.4673958269450197, 0.4412371012727202, 0.1285886837615413, 0.23403231590111162, 0.36109192393377976, 0.22664878079649053, 0.2540072344222718, 0.4265671714982343, 0.2827316237596058, 0.6431777309769313, 0.3732167479133396, 0.12116522298168053, 0.1678990673445524, 0.3052731722642458, 0.04944125032055546, 0.42013758686785785, 0.4675757890099749, 0.34909871477201915, 0.44848393730635916, 0.4121936894642598, 0.2572264031099728, 0.14782089879620763, 0.45511882842056595, 0.3614252035664588, 0.12576950129727194, 0.19368490486653112, 0.5599530644881539, 0.4930337585131278, 0.48359551057535666, 0.31457189059998514, 0.12229189958524489, 0.1343438096316968, 0.1129583796588105, 0.17065635226036613, 0.21121710331876486, 0.17960579905235896, 0.13080473597692177, 0.17281085533956853, 0.3785105716917646, 0.32356504637548955, 0.2930510023224221, 0.4263055259587192, 0.3758837171163727, 0.3487130962919658, 0.34061488164921405, 0.38468365570003926, 0.36601498696637014, 0.684964500616335, 0.7393164733752317, 0.7681352739007481, 0.7580447340486791, 0.6512200131325806, 0.7160469344263029, 0.5965650033008165, 0.7695202275323955, 0.7091180301825857, 0.13027852195339762, 0.1752862344756313, 0.14725999438068593, 0.10963074171804343, 0.12196205006310135, 0.12842695335615995, 0.1535536314706123, 0.09324353688930664, 0.11756250657323708, 0.1583195481084767, 0.17804157548667843, 0.27489139187246203, 0.21745895839396734, 0.667095829932591, 0.20628860250021674, 0.5805893447980215, 0.2351413331273987, 0.23368786575655498, 0.5245002013696674, 0.5809814491680333, 0.4604348637764978, 0.5487855436976408, 0.5109761909197574, 0.5670142492913299, 0.5523316647904777, 0.6211521395155045, 0.5065222018957187, 0.43913043730592705, 0.4209119941181332, 0.24723244458163163, 0.36854984117381473, 0.4714349416601522, 0.4087858118863441, 0.4445023749465481, 0.47953063462436707, 0.4056768097816228, 0.20187088443067747, 0.28194838553846535, 0.22079563605800234, 0.2193518224022324, 0.18672017534619767, 0.21755812838823685, 0.21888893291732603, 0.20535976400015987, 0.24766721582344342, 0.7731611289963906, 0.22212714398023892, 0.22683569749128651, 0.2219774539530932, 0.2038861631536516, 0.22700750845132922, 0.19396010038295208, 0.19566429190218382, 0.24739215490626576, 0.16341617726849111, 0.8639742256032173, 0.18598438634511893, 0.19365704699243147, 0.8973800514282001, 0.8634132304309223, 0.8472812455205514, 0.1985001542527517, 0.1585146185947931, 0.8337654020690852, 0.16815553765976654, 0.16767777920264038, 0.8145817251939893, 0.1661887704123024, 0.2002384045022092, 0.21270385229000333, 0.15502895016859852, 0.15581584216296274, 0.44112899395539285, 0.2704890640209542, 0.317178982997638, 0.2965929127827681, 0.4314839426197882, 0.2443844071739082, 0.2272806115444309, 0.24705198287063868, 0.2919163957284867, 0.10742674978866673, 0.09615090976507867, 0.08932957373600692, 0.10039145184892406, 0.07777519619601359, 0.09346825340867582, 0.09191978538549628, 0.10051502779803445, 0.09032627828645579]}, "mutation_prompt": null}
{"id": "6431f471-0c37-4684-9b67-6aeaa2189ee3", "solution": "import numpy as np\n\nclass RefinedHybridStochasticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.initial_scale_factor = 0.85  # Adjusted for slightly more aggressive mutations\n        self.initial_crossover_prob = 0.75  # Increase crossover probability to enhance diversity\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population = self.opposition_based_initialization()\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.learning_rate = 0.12  # Slightly increased learning rate for faster local search\n        self.elitism_rate = 0.12  # Slightly increased elitism for preserving more top candidates\n\n    def opposition_based_initialization(self):\n        initial_population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        opposition_population = self.lower_bound + self.upper_bound - initial_population\n        combined_population = np.vstack((initial_population, opposition_population))\n        fitness = np.array([np.inf] * len(combined_population))\n        for i in range(len(combined_population)):\n            fitness[i] = np.random.rand()  # Introduce randomness for better initial diversity\n        best_indices = np.argsort(fitness)[:self.population_size]\n        return combined_population[best_indices]\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def select_parents(self, idx):\n        indices = [i for i in range(self.population_size) if i != idx]\n        return np.random.choice(indices, 3, replace=False)\n\n    def differential_evolution_mutation(self, idx, scale_factor):\n        a, b, c = self.select_parents(idx)\n        mutant_vector = self.population[a] + scale_factor * (self.population[b] - self.population[c])\n        return np.clip(mutant_vector, self.lower_bound, self.upper_bound)\n\n    def crossover(self, target_vector, mutant_vector, crossover_prob):\n        crossover_mask = np.random.rand(self.dim) < crossover_prob\n        return np.where(crossover_mask, mutant_vector, target_vector)\n\n    def local_search(self, vector):\n        perturbation = np.random.normal(0, self.learning_rate, self.dim)\n        candidate_vector = vector + perturbation\n        return np.clip(candidate_vector, self.lower_bound, self.upper_bound)\n\n    def elitism_selection(self):\n        num_elites = max(1, int(self.elitism_rate * self.population_size))\n        elite_indices = np.argsort(self.fitness)[:num_elites]\n        return self.population[elite_indices], self.fitness[elite_indices]\n\n    def __call__(self, func):\n        self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            elites, elite_fitness = self.elitism_selection()\n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                scale_factor = self.initial_scale_factor * (1 - self.evaluations / self.budget)\n                crossover_prob = self.initial_crossover_prob + 0.15 * np.random.rand()  # Narrow crossover randomness\n\n                if i < len(elites):  # Use elite individuals\n                    candidate_vector = self.local_search(elites[i])\n                else:\n                    mutant_vector = self.differential_evolution_mutation(i, scale_factor)\n                    candidate_vector = self.crossover(self.population[i], mutant_vector, crossover_prob)\n\n                candidate_fitness = func(candidate_vector)\n                self.evaluations += 1\n\n                if candidate_fitness < self.fitness[i]:\n                    self.population[i] = candidate_vector\n                    self.fitness[i] = candidate_fitness\n\n                self.learning_rate = max(0.01, self.learning_rate * (1 - 0.04 * np.random.rand()))  # Adjusted learning decay\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx]", "name": "RefinedHybridStochasticOptimizer", "description": "A refined hybrid optimizer with adaptive opposition strategy and dynamic exploration-exploitation balance for enhanced convergence speed.", "configspace": "", "generation": 85, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "611e17be-2543-4b24-a742-9175f71fb862", "metadata": {"aucs": [0.8766756452394434, 0.8780259518892735, 0.8679732870259016, 0.8851321249681726, 0.8834248924740489, 0.8917330111634347, 0.8915409061661912, 0.8625223086831446, 0.8855384988765383, 0.7702063528242651, 0.764605999911898, 0.7866633313683472, 0.788225311474078, 0.7768513285041114, 0.783710568385616, 0.7710553972167912, 0.7826320564567563, 0.7747122369896489, 0.7207065189713453, 0.6772268580219241, 0.17524904499097194, 0.1545419816557405, 0.12287270778105774, 0.689897670488521, 0.17408711892974027, 0.7264644514688223, 0.17782525894103152, 0.5540661022978465, 0.1718555788393058, 0.17432731451169114, 0.1401812381721591, 0.181817031126965, 0.17084893094232578, 0.14077243420807695, 0.11782626428419385, 0.1411557885190976, 0.9937933542659784, 0.9824818277808939, 0.980707506224134, 0.9544651636952174, 0.9824601842623015, 0.9786103071199913, 0.9917495387625039, 0.9773550272665161, 0.9767648277870316, 0.7186425059203514, 0.7072341755938936, 0.67619418713103, 0.5500313926892695, 0.6353964019006688, 0.5655276295816347, 0.6087248017767631, 0.7250088325007087, 0.5509231661245519, 0.8306251087247614, 0.3722435473015898, 0.8654864869334986, 0.8863503903735646, 0.8760746636972028, 0.8458760719743885, 0.8486549335282704, 0.8432469041054449, 0.3860328241903793, 0.3132235440659491, 0.4899210907813042, 0.2895826691181592, 0.19744367076628966, 0.4358144265843764, 0.19907439487724254, 0.36614638947632094, 0.4673958269450197, 0.4412371012727202, 0.1285886837615413, 0.23403231590111162, 0.36109192393377976, 0.22664878079649053, 0.2540072344222718, 0.4265671714982343, 0.2827316237596058, 0.6431777309769313, 0.3732167479133396, 0.12116522298168053, 0.1678990673445524, 0.3052731722642458, 0.04944125032055546, 0.42013758686785785, 0.4675757890099749, 0.34909871477201915, 0.44848393730635916, 0.4121936894642598, 0.2572264031099728, 0.14782089879620763, 0.45511882842056595, 0.3614252035664588, 0.12576950129727194, 0.19368490486653112, 0.5599530644881539, 0.4930337585131278, 0.48359551057535666, 0.31457189059998514, 0.12229189958524489, 0.1343438096316968, 0.1129583796588105, 0.17065635226036613, 0.21121710331876486, 0.17960579905235896, 0.13080473597692177, 0.17281085533956853, 0.3785105716917646, 0.32356504637548955, 0.2930510023224221, 0.4263055259587192, 0.3758837171163727, 0.3487130962919658, 0.34061488164921405, 0.38468365570003926, 0.36601498696637014, 0.684964500616335, 0.7393164733752317, 0.7681352739007481, 0.7580447340486791, 0.6512200131325806, 0.7160469344263029, 0.5965650033008165, 0.7695202275323955, 0.7091180301825857, 0.13027852195339762, 0.1752862344756313, 0.14725999438068593, 0.10963074171804343, 0.12196205006310135, 0.12842695335615995, 0.1535536314706123, 0.09324353688930664, 0.11756250657323708, 0.1583195481084767, 0.17804157548667843, 0.27489139187246203, 0.21745895839396734, 0.667095829932591, 0.20628860250021674, 0.5805893447980215, 0.2351413331273987, 0.23368786575655498, 0.5245002013696674, 0.5809814491680333, 0.4604348637764978, 0.5487855436976408, 0.5109761909197574, 0.5670142492913299, 0.5523316647904777, 0.6211521395155045, 0.5065222018957187, 0.43913043730592705, 0.4209119941181332, 0.24723244458163163, 0.36854984117381473, 0.4714349416601522, 0.4087858118863441, 0.4445023749465481, 0.47953063462436707, 0.4056768097816228, 0.20187088443067747, 0.28194838553846535, 0.22079563605800234, 0.2193518224022324, 0.18672017534619767, 0.21755812838823685, 0.21888893291732603, 0.20535976400015987, 0.24766721582344342, 0.7731611289963906, 0.22212714398023892, 0.22683569749128651, 0.2219774539530932, 0.2038861631536516, 0.22700750845132922, 0.19396010038295208, 0.19566429190218382, 0.24739215490626576, 0.16341617726849111, 0.8639742256032173, 0.18598438634511893, 0.19365704699243147, 0.8973800514282001, 0.8634132304309223, 0.8472812455205514, 0.1985001542527517, 0.1585146185947931, 0.8337654020690852, 0.16815553765976654, 0.16767777920264038, 0.8145817251939893, 0.1661887704123024, 0.2002384045022092, 0.21270385229000333, 0.15502895016859852, 0.15581584216296274, 0.44112899395539285, 0.2704890640209542, 0.317178982997638, 0.2965929127827681, 0.4314839426197882, 0.2443844071739082, 0.2272806115444309, 0.24705198287063868, 0.2919163957284867, 0.10742674978866673, 0.09615090976507867, 0.08932957373600692, 0.10039145184892406, 0.07777519619601359, 0.09346825340867582, 0.09191978538549628, 0.10051502779803445, 0.09032627828645579]}, "mutation_prompt": null}
{"id": "ed5ecbd3-784c-4eb2-84eb-adedf7b33da6", "solution": "import numpy as np\n\nclass RefinedHybridStochasticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.initial_scale_factor = 0.85  # Adjusted for slightly more aggressive mutations\n        self.initial_crossover_prob = 0.75  # Increase crossover probability to enhance diversity\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population = self.opposition_based_initialization()\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.learning_rate = 0.12  # Slightly increased learning rate for faster local search\n        self.elitism_rate = 0.12  # Slightly increased elitism for preserving more top candidates\n\n    def opposition_based_initialization(self):\n        initial_population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        opposition_population = self.lower_bound + self.upper_bound - initial_population\n        combined_population = np.vstack((initial_population, opposition_population))\n        fitness = np.array([np.inf] * len(combined_population))\n        for i in range(len(combined_population)):\n            fitness[i] = np.random.rand()  # Introduce randomness for better initial diversity\n        best_indices = np.argsort(fitness)[:self.population_size]\n        return combined_population[best_indices]\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def select_parents(self, idx):\n        indices = [i for i in range(self.population_size) if i != idx]\n        return np.random.choice(indices, 3, replace=False)\n\n    def differential_evolution_mutation(self, idx, scale_factor):\n        a, b, c = self.select_parents(idx)\n        mutant_vector = self.population[a] + scale_factor * (self.population[b] - self.population[c])\n        return np.clip(mutant_vector, self.lower_bound, self.upper_bound)\n\n    def crossover(self, target_vector, mutant_vector, crossover_prob):\n        crossover_mask = np.random.rand(self.dim) < crossover_prob\n        return np.where(crossover_mask, mutant_vector, target_vector)\n\n    def local_search(self, vector):\n        perturbation = np.random.normal(0, self.learning_rate, self.dim)\n        candidate_vector = vector + perturbation\n        return np.clip(candidate_vector, self.lower_bound, self.upper_bound)\n\n    def elitism_selection(self):\n        num_elites = max(1, int(self.elitism_rate * self.population_size))\n        elite_indices = np.argsort(self.fitness)[:num_elites]\n        return self.population[elite_indices], self.fitness[elite_indices]\n\n    def __call__(self, func):\n        self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            elites, elite_fitness = self.elitism_selection()\n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                scale_factor = self.initial_scale_factor * (1 - self.evaluations / self.budget)\n                crossover_prob = self.initial_crossover_prob + 0.15 * np.random.rand()  # Narrow crossover randomness\n\n                if i < len(elites):  # Use elite individuals\n                    candidate_vector = self.local_search(elites[i])\n                else:\n                    mutant_vector = self.differential_evolution_mutation(i, scale_factor)\n                    candidate_vector = self.crossover(self.population[i], mutant_vector, crossover_prob)\n\n                candidate_fitness = func(candidate_vector)\n                self.evaluations += 1\n\n                if candidate_fitness < self.fitness[i]:\n                    self.population[i] = candidate_vector\n                    self.fitness[i] = candidate_fitness\n\n                self.learning_rate = max(0.01, self.learning_rate * (1 - 0.04 * np.random.rand()))  # Adjusted learning decay\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx]", "name": "RefinedHybridStochasticOptimizer", "description": "A refined hybrid optimizer with adaptive opposition strategy and dynamic exploration-exploitation balance for enhanced convergence speed.", "configspace": "", "generation": 85, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "611e17be-2543-4b24-a742-9175f71fb862", "metadata": {"aucs": [0.8766756452394434, 0.8780259518892735, 0.8679732870259016, 0.8851321249681726, 0.8834248924740489, 0.8917330111634347, 0.8915409061661912, 0.8625223086831446, 0.8855384988765383, 0.7702063528242651, 0.764605999911898, 0.7866633313683472, 0.788225311474078, 0.7768513285041114, 0.783710568385616, 0.7710553972167912, 0.7826320564567563, 0.7747122369896489, 0.7207065189713453, 0.6772268580219241, 0.17524904499097194, 0.1545419816557405, 0.12287270778105774, 0.689897670488521, 0.17408711892974027, 0.7264644514688223, 0.17782525894103152, 0.5540661022978465, 0.1718555788393058, 0.17432731451169114, 0.1401812381721591, 0.181817031126965, 0.17084893094232578, 0.14077243420807695, 0.11782626428419385, 0.1411557885190976, 0.9937933542659784, 0.9824818277808939, 0.980707506224134, 0.9544651636952174, 0.9824601842623015, 0.9786103071199913, 0.9917495387625039, 0.9773550272665161, 0.9767648277870316, 0.7186425059203514, 0.7072341755938936, 0.67619418713103, 0.5500313926892695, 0.6353964019006688, 0.5655276295816347, 0.6087248017767631, 0.7250088325007087, 0.5509231661245519, 0.8306251087247614, 0.3722435473015898, 0.8654864869334986, 0.8863503903735646, 0.8760746636972028, 0.8458760719743885, 0.8486549335282704, 0.8432469041054449, 0.3860328241903793, 0.3132235440659491, 0.4899210907813042, 0.2895826691181592, 0.19744367076628966, 0.4358144265843764, 0.19907439487724254, 0.36614638947632094, 0.4673958269450197, 0.4412371012727202, 0.1285886837615413, 0.23403231590111162, 0.36109192393377976, 0.22664878079649053, 0.2540072344222718, 0.4265671714982343, 0.2827316237596058, 0.6431777309769313, 0.3732167479133396, 0.12116522298168053, 0.1678990673445524, 0.3052731722642458, 0.04944125032055546, 0.42013758686785785, 0.4675757890099749, 0.34909871477201915, 0.44848393730635916, 0.4121936894642598, 0.2572264031099728, 0.14782089879620763, 0.45511882842056595, 0.3614252035664588, 0.12576950129727194, 0.19368490486653112, 0.5599530644881539, 0.4930337585131278, 0.48359551057535666, 0.31457189059998514, 0.12229189958524489, 0.1343438096316968, 0.1129583796588105, 0.17065635226036613, 0.21121710331876486, 0.17960579905235896, 0.13080473597692177, 0.17281085533956853, 0.3785105716917646, 0.32356504637548955, 0.2930510023224221, 0.4263055259587192, 0.3758837171163727, 0.3487130962919658, 0.34061488164921405, 0.38468365570003926, 0.36601498696637014, 0.684964500616335, 0.7393164733752317, 0.7681352739007481, 0.7580447340486791, 0.6512200131325806, 0.7160469344263029, 0.5965650033008165, 0.7695202275323955, 0.7091180301825857, 0.13027852195339762, 0.1752862344756313, 0.14725999438068593, 0.10963074171804343, 0.12196205006310135, 0.12842695335615995, 0.1535536314706123, 0.09324353688930664, 0.11756250657323708, 0.1583195481084767, 0.17804157548667843, 0.27489139187246203, 0.21745895839396734, 0.667095829932591, 0.20628860250021674, 0.5805893447980215, 0.2351413331273987, 0.23368786575655498, 0.5245002013696674, 0.5809814491680333, 0.4604348637764978, 0.5487855436976408, 0.5109761909197574, 0.5670142492913299, 0.5523316647904777, 0.6211521395155045, 0.5065222018957187, 0.43913043730592705, 0.4209119941181332, 0.24723244458163163, 0.36854984117381473, 0.4714349416601522, 0.4087858118863441, 0.4445023749465481, 0.47953063462436707, 0.4056768097816228, 0.20187088443067747, 0.28194838553846535, 0.22079563605800234, 0.2193518224022324, 0.18672017534619767, 0.21755812838823685, 0.21888893291732603, 0.20535976400015987, 0.24766721582344342, 0.7731611289963906, 0.22212714398023892, 0.22683569749128651, 0.2219774539530932, 0.2038861631536516, 0.22700750845132922, 0.19396010038295208, 0.19566429190218382, 0.24739215490626576, 0.16341617726849111, 0.8639742256032173, 0.18598438634511893, 0.19365704699243147, 0.8973800514282001, 0.8634132304309223, 0.8472812455205514, 0.1985001542527517, 0.1585146185947931, 0.8337654020690852, 0.16815553765976654, 0.16767777920264038, 0.8145817251939893, 0.1661887704123024, 0.2002384045022092, 0.21270385229000333, 0.15502895016859852, 0.15581584216296274, 0.44112899395539285, 0.2704890640209542, 0.317178982997638, 0.2965929127827681, 0.4314839426197882, 0.2443844071739082, 0.2272806115444309, 0.24705198287063868, 0.2919163957284867, 0.10742674978866673, 0.09615090976507867, 0.08932957373600692, 0.10039145184892406, 0.07777519619601359, 0.09346825340867582, 0.09191978538549628, 0.10051502779803445, 0.09032627828645579]}, "mutation_prompt": null}
{"id": "12140e1b-8d0a-4653-b585-b04b53069757", "solution": "import numpy as np\n\nclass RefinedHybridStochasticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.initial_scale_factor = 0.85  # Adjusted for slightly more aggressive mutations\n        self.initial_crossover_prob = 0.75  # Increase crossover probability to enhance diversity\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population = self.opposition_based_initialization()\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.learning_rate = 0.12  # Slightly increased learning rate for faster local search\n        self.elitism_rate = 0.12  # Slightly increased elitism for preserving more top candidates\n\n    def opposition_based_initialization(self):\n        initial_population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        opposition_population = self.lower_bound + self.upper_bound - initial_population\n        combined_population = np.vstack((initial_population, opposition_population))\n        fitness = np.array([np.inf] * len(combined_population))\n        for i in range(len(combined_population)):\n            fitness[i] = np.random.rand()  # Introduce randomness for better initial diversity\n        best_indices = np.argsort(fitness)[:self.population_size]\n        return combined_population[best_indices]\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def select_parents(self, idx):\n        indices = [i for i in range(self.population_size) if i != idx]\n        return np.random.choice(indices, 3, replace=False)\n\n    def differential_evolution_mutation(self, idx, scale_factor):\n        a, b, c = self.select_parents(idx)\n        mutant_vector = self.population[a] + scale_factor * (self.population[b] - self.population[c])\n        return np.clip(mutant_vector, self.lower_bound, self.upper_bound)\n\n    def crossover(self, target_vector, mutant_vector, crossover_prob):\n        crossover_mask = np.random.rand(self.dim) < crossover_prob\n        return np.where(crossover_mask, mutant_vector, target_vector)\n\n    def local_search(self, vector):\n        perturbation = np.random.normal(0, self.learning_rate, self.dim)\n        candidate_vector = vector + perturbation\n        return np.clip(candidate_vector, self.lower_bound, self.upper_bound)\n\n    def elitism_selection(self):\n        num_elites = max(1, int(self.elitism_rate * self.population_size))\n        elite_indices = np.argsort(self.fitness)[:num_elites]\n        return self.population[elite_indices], self.fitness[elite_indices]\n\n    def __call__(self, func):\n        self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            elites, elite_fitness = self.elitism_selection()\n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                scale_factor = self.initial_scale_factor * (1 - self.evaluations / self.budget)\n                crossover_prob = self.initial_crossover_prob + 0.15 * np.random.rand()  # Narrow crossover randomness\n\n                if i < len(elites):  # Use elite individuals\n                    candidate_vector = self.local_search(elites[i])\n                else:\n                    mutant_vector = self.differential_evolution_mutation(i, scale_factor)\n                    candidate_vector = self.crossover(self.population[i], mutant_vector, crossover_prob)\n\n                candidate_fitness = func(candidate_vector)\n                self.evaluations += 1\n\n                if candidate_fitness < self.fitness[i]:\n                    self.population[i] = candidate_vector\n                    self.fitness[i] = candidate_fitness\n\n                self.learning_rate = max(0.01, self.learning_rate * (1 - 0.04 * np.random.rand()))  # Adjusted learning decay\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx]", "name": "RefinedHybridStochasticOptimizer", "description": "A refined hybrid optimizer with adaptive opposition strategy and dynamic exploration-exploitation balance for enhanced convergence speed.", "configspace": "", "generation": 85, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "611e17be-2543-4b24-a742-9175f71fb862", "metadata": {"aucs": [0.8766756452394434, 0.8780259518892735, 0.8679732870259016, 0.8851321249681726, 0.8834248924740489, 0.8917330111634347, 0.8915409061661912, 0.8625223086831446, 0.8855384988765383, 0.7702063528242651, 0.764605999911898, 0.7866633313683472, 0.788225311474078, 0.7768513285041114, 0.783710568385616, 0.7710553972167912, 0.7826320564567563, 0.7747122369896489, 0.7207065189713453, 0.6772268580219241, 0.17524904499097194, 0.1545419816557405, 0.12287270778105774, 0.689897670488521, 0.17408711892974027, 0.7264644514688223, 0.17782525894103152, 0.5540661022978465, 0.1718555788393058, 0.17432731451169114, 0.1401812381721591, 0.181817031126965, 0.17084893094232578, 0.14077243420807695, 0.11782626428419385, 0.1411557885190976, 0.9937933542659784, 0.9824818277808939, 0.980707506224134, 0.9544651636952174, 0.9824601842623015, 0.9786103071199913, 0.9917495387625039, 0.9773550272665161, 0.9767648277870316, 0.7186425059203514, 0.7072341755938936, 0.67619418713103, 0.5500313926892695, 0.6353964019006688, 0.5655276295816347, 0.6087248017767631, 0.7250088325007087, 0.5509231661245519, 0.8306251087247614, 0.3722435473015898, 0.8654864869334986, 0.8863503903735646, 0.8760746636972028, 0.8458760719743885, 0.8486549335282704, 0.8432469041054449, 0.3860328241903793, 0.3132235440659491, 0.4899210907813042, 0.2895826691181592, 0.19744367076628966, 0.4358144265843764, 0.19907439487724254, 0.36614638947632094, 0.4673958269450197, 0.4412371012727202, 0.1285886837615413, 0.23403231590111162, 0.36109192393377976, 0.22664878079649053, 0.2540072344222718, 0.4265671714982343, 0.2827316237596058, 0.6431777309769313, 0.3732167479133396, 0.12116522298168053, 0.1678990673445524, 0.3052731722642458, 0.04944125032055546, 0.42013758686785785, 0.4675757890099749, 0.34909871477201915, 0.44848393730635916, 0.4121936894642598, 0.2572264031099728, 0.14782089879620763, 0.45511882842056595, 0.3614252035664588, 0.12576950129727194, 0.19368490486653112, 0.5599530644881539, 0.4930337585131278, 0.48359551057535666, 0.31457189059998514, 0.12229189958524489, 0.1343438096316968, 0.1129583796588105, 0.17065635226036613, 0.21121710331876486, 0.17960579905235896, 0.13080473597692177, 0.17281085533956853, 0.3785105716917646, 0.32356504637548955, 0.2930510023224221, 0.4263055259587192, 0.3758837171163727, 0.3487130962919658, 0.34061488164921405, 0.38468365570003926, 0.36601498696637014, 0.684964500616335, 0.7393164733752317, 0.7681352739007481, 0.7580447340486791, 0.6512200131325806, 0.7160469344263029, 0.5965650033008165, 0.7695202275323955, 0.7091180301825857, 0.13027852195339762, 0.1752862344756313, 0.14725999438068593, 0.10963074171804343, 0.12196205006310135, 0.12842695335615995, 0.1535536314706123, 0.09324353688930664, 0.11756250657323708, 0.1583195481084767, 0.17804157548667843, 0.27489139187246203, 0.21745895839396734, 0.667095829932591, 0.20628860250021674, 0.5805893447980215, 0.2351413331273987, 0.23368786575655498, 0.5245002013696674, 0.5809814491680333, 0.4604348637764978, 0.5487855436976408, 0.5109761909197574, 0.5670142492913299, 0.5523316647904777, 0.6211521395155045, 0.5065222018957187, 0.43913043730592705, 0.4209119941181332, 0.24723244458163163, 0.36854984117381473, 0.4714349416601522, 0.4087858118863441, 0.4445023749465481, 0.47953063462436707, 0.4056768097816228, 0.20187088443067747, 0.28194838553846535, 0.22079563605800234, 0.2193518224022324, 0.18672017534619767, 0.21755812838823685, 0.21888893291732603, 0.20535976400015987, 0.24766721582344342, 0.7731611289963906, 0.22212714398023892, 0.22683569749128651, 0.2219774539530932, 0.2038861631536516, 0.22700750845132922, 0.19396010038295208, 0.19566429190218382, 0.24739215490626576, 0.16341617726849111, 0.8639742256032173, 0.18598438634511893, 0.19365704699243147, 0.8973800514282001, 0.8634132304309223, 0.8472812455205514, 0.1985001542527517, 0.1585146185947931, 0.8337654020690852, 0.16815553765976654, 0.16767777920264038, 0.8145817251939893, 0.1661887704123024, 0.2002384045022092, 0.21270385229000333, 0.15502895016859852, 0.15581584216296274, 0.44112899395539285, 0.2704890640209542, 0.317178982997638, 0.2965929127827681, 0.4314839426197882, 0.2443844071739082, 0.2272806115444309, 0.24705198287063868, 0.2919163957284867, 0.10742674978866673, 0.09615090976507867, 0.08932957373600692, 0.10039145184892406, 0.07777519619601359, 0.09346825340867582, 0.09191978538549628, 0.10051502779803445, 0.09032627828645579]}, "mutation_prompt": null}
{"id": "5a72891b-327a-4bdb-93f7-0ad622d980cd", "solution": "import numpy as np\n\nclass RefinedHybridStochasticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.initial_scale_factor = 0.85  # Adjusted for slightly more aggressive mutations\n        self.initial_crossover_prob = 0.75  # Increase crossover probability to enhance diversity\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population = self.opposition_based_initialization()\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.learning_rate = 0.12  # Slightly increased learning rate for faster local search\n        self.elitism_rate = 0.12  # Slightly increased elitism for preserving more top candidates\n\n    def opposition_based_initialization(self):\n        initial_population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        opposition_population = self.lower_bound + self.upper_bound - initial_population\n        combined_population = np.vstack((initial_population, opposition_population))\n        fitness = np.array([np.inf] * len(combined_population))\n        for i in range(len(combined_population)):\n            fitness[i] = np.random.rand()  # Introduce randomness for better initial diversity\n        best_indices = np.argsort(fitness)[:self.population_size]\n        return combined_population[best_indices]\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def select_parents(self, idx):\n        indices = [i for i in range(self.population_size) if i != idx]\n        return np.random.choice(indices, 3, replace=False)\n\n    def differential_evolution_mutation(self, idx, scale_factor):\n        a, b, c = self.select_parents(idx)\n        mutant_vector = self.population[a] + scale_factor * (self.population[b] - self.population[c])\n        return np.clip(mutant_vector, self.lower_bound, self.upper_bound)\n\n    def crossover(self, target_vector, mutant_vector, crossover_prob):\n        crossover_mask = np.random.rand(self.dim) < crossover_prob\n        return np.where(crossover_mask, mutant_vector, target_vector)\n\n    def local_search(self, vector):\n        perturbation = np.random.normal(0, self.learning_rate, self.dim)\n        candidate_vector = vector + perturbation\n        return np.clip(candidate_vector, self.lower_bound, self.upper_bound)\n\n    def elitism_selection(self):\n        num_elites = max(1, int(self.elitism_rate * self.population_size))\n        elite_indices = np.argsort(self.fitness)[:num_elites]\n        return self.population[elite_indices], self.fitness[elite_indices]\n\n    def __call__(self, func):\n        self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            elites, elite_fitness = self.elitism_selection()\n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                scale_factor = self.initial_scale_factor * (1 - self.evaluations / self.budget)\n                crossover_prob = self.initial_crossover_prob + 0.15 * np.random.rand()  # Narrow crossover randomness\n\n                if i < len(elites):  # Use elite individuals\n                    candidate_vector = self.local_search(elites[i])\n                else:\n                    mutant_vector = self.differential_evolution_mutation(i, scale_factor)\n                    candidate_vector = self.crossover(self.population[i], mutant_vector, crossover_prob)\n\n                candidate_fitness = func(candidate_vector)\n                self.evaluations += 1\n\n                if candidate_fitness < self.fitness[i]:\n                    self.population[i] = candidate_vector\n                    self.fitness[i] = candidate_fitness\n\n                self.learning_rate = max(0.01, self.learning_rate * (1 - 0.04 * np.random.rand()))  # Adjusted learning decay\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx]", "name": "RefinedHybridStochasticOptimizer", "description": "A refined hybrid optimizer with adaptive opposition strategy and dynamic exploration-exploitation balance for enhanced convergence speed.", "configspace": "", "generation": 85, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "611e17be-2543-4b24-a742-9175f71fb862", "metadata": {"aucs": [0.8766756452394434, 0.8780259518892735, 0.8679732870259016, 0.8851321249681726, 0.8834248924740489, 0.8917330111634347, 0.8915409061661912, 0.8625223086831446, 0.8855384988765383, 0.7702063528242651, 0.764605999911898, 0.7866633313683472, 0.788225311474078, 0.7768513285041114, 0.783710568385616, 0.7710553972167912, 0.7826320564567563, 0.7747122369896489, 0.7207065189713453, 0.6772268580219241, 0.17524904499097194, 0.1545419816557405, 0.12287270778105774, 0.689897670488521, 0.17408711892974027, 0.7264644514688223, 0.17782525894103152, 0.5540661022978465, 0.1718555788393058, 0.17432731451169114, 0.1401812381721591, 0.181817031126965, 0.17084893094232578, 0.14077243420807695, 0.11782626428419385, 0.1411557885190976, 0.9937933542659784, 0.9824818277808939, 0.980707506224134, 0.9544651636952174, 0.9824601842623015, 0.9786103071199913, 0.9917495387625039, 0.9773550272665161, 0.9767648277870316, 0.7186425059203514, 0.7072341755938936, 0.67619418713103, 0.5500313926892695, 0.6353964019006688, 0.5655276295816347, 0.6087248017767631, 0.7250088325007087, 0.5509231661245519, 0.8306251087247614, 0.3722435473015898, 0.8654864869334986, 0.8863503903735646, 0.8760746636972028, 0.8458760719743885, 0.8486549335282704, 0.8432469041054449, 0.3860328241903793, 0.3132235440659491, 0.4899210907813042, 0.2895826691181592, 0.19744367076628966, 0.4358144265843764, 0.19907439487724254, 0.36614638947632094, 0.4673958269450197, 0.4412371012727202, 0.1285886837615413, 0.23403231590111162, 0.36109192393377976, 0.22664878079649053, 0.2540072344222718, 0.4265671714982343, 0.2827316237596058, 0.6431777309769313, 0.3732167479133396, 0.12116522298168053, 0.1678990673445524, 0.3052731722642458, 0.04944125032055546, 0.42013758686785785, 0.4675757890099749, 0.34909871477201915, 0.44848393730635916, 0.4121936894642598, 0.2572264031099728, 0.14782089879620763, 0.45511882842056595, 0.3614252035664588, 0.12576950129727194, 0.19368490486653112, 0.5599530644881539, 0.4930337585131278, 0.48359551057535666, 0.31457189059998514, 0.12229189958524489, 0.1343438096316968, 0.1129583796588105, 0.17065635226036613, 0.21121710331876486, 0.17960579905235896, 0.13080473597692177, 0.17281085533956853, 0.3785105716917646, 0.32356504637548955, 0.2930510023224221, 0.4263055259587192, 0.3758837171163727, 0.3487130962919658, 0.34061488164921405, 0.38468365570003926, 0.36601498696637014, 0.684964500616335, 0.7393164733752317, 0.7681352739007481, 0.7580447340486791, 0.6512200131325806, 0.7160469344263029, 0.5965650033008165, 0.7695202275323955, 0.7091180301825857, 0.13027852195339762, 0.1752862344756313, 0.14725999438068593, 0.10963074171804343, 0.12196205006310135, 0.12842695335615995, 0.1535536314706123, 0.09324353688930664, 0.11756250657323708, 0.1583195481084767, 0.17804157548667843, 0.27489139187246203, 0.21745895839396734, 0.667095829932591, 0.20628860250021674, 0.5805893447980215, 0.2351413331273987, 0.23368786575655498, 0.5245002013696674, 0.5809814491680333, 0.4604348637764978, 0.5487855436976408, 0.5109761909197574, 0.5670142492913299, 0.5523316647904777, 0.6211521395155045, 0.5065222018957187, 0.43913043730592705, 0.4209119941181332, 0.24723244458163163, 0.36854984117381473, 0.4714349416601522, 0.4087858118863441, 0.4445023749465481, 0.47953063462436707, 0.4056768097816228, 0.20187088443067747, 0.28194838553846535, 0.22079563605800234, 0.2193518224022324, 0.18672017534619767, 0.21755812838823685, 0.21888893291732603, 0.20535976400015987, 0.24766721582344342, 0.7731611289963906, 0.22212714398023892, 0.22683569749128651, 0.2219774539530932, 0.2038861631536516, 0.22700750845132922, 0.19396010038295208, 0.19566429190218382, 0.24739215490626576, 0.16341617726849111, 0.8639742256032173, 0.18598438634511893, 0.19365704699243147, 0.8973800514282001, 0.8634132304309223, 0.8472812455205514, 0.1985001542527517, 0.1585146185947931, 0.8337654020690852, 0.16815553765976654, 0.16767777920264038, 0.8145817251939893, 0.1661887704123024, 0.2002384045022092, 0.21270385229000333, 0.15502895016859852, 0.15581584216296274, 0.44112899395539285, 0.2704890640209542, 0.317178982997638, 0.2965929127827681, 0.4314839426197882, 0.2443844071739082, 0.2272806115444309, 0.24705198287063868, 0.2919163957284867, 0.10742674978866673, 0.09615090976507867, 0.08932957373600692, 0.10039145184892406, 0.07777519619601359, 0.09346825340867582, 0.09191978538549628, 0.10051502779803445, 0.09032627828645579]}, "mutation_prompt": null}
{"id": "7e5eb359-b9c3-4885-acff-791e8a710059", "solution": "import numpy as np\n\nclass EnhancedHybridStochasticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.scale_factor_base = 0.85\n        self.crossover_prob_base = 0.75\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population = self.opposition_based_initialization()\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.learning_rate = 0.15\n        self.elitism_rate = 0.15\n\n    def opposition_based_initialization(self):\n        initial_population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        opposition_population = self.lower_bound + self.upper_bound - initial_population\n        combined_population = np.vstack((initial_population, opposition_population))\n        fitness = np.array([np.inf] * len(combined_population))\n        for i in range(len(combined_population)):\n            fitness[i] = np.sum(combined_population[i]**2)\n        best_indices = np.argsort(fitness)[:self.population_size]\n        return combined_population[best_indices]\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def select_parents(self, idx):\n        indices = [i for i in range(self.population_size) if i != idx]\n        return np.random.choice(indices, 3, replace=False)\n\n    def differential_evolution_mutation(self, idx, scale_factor):\n        a, b, c = self.select_parents(idx)\n        mutant_vector = self.population[a] + scale_factor * (self.population[b] - self.population[c])\n        return np.clip(mutant_vector, self.lower_bound, self.upper_bound)\n\n    def crossover(self, target_vector, mutant_vector, crossover_prob):\n        crossover_mask = np.random.rand(self.dim) < crossover_prob\n        return np.where(crossover_mask, mutant_vector, target_vector)\n\n    def local_search(self, vector):\n        perturbation = np.random.normal(0, self.learning_rate, self.dim)\n        candidate_vector = vector + perturbation\n        return np.clip(candidate_vector, self.lower_bound, self.upper_bound)\n\n    def elitism_selection(self):\n        num_elites = max(1, int(self.elitism_rate * self.population_size))\n        elite_indices = np.argsort(self.fitness)[:num_elites]\n        return self.population[elite_indices], self.fitness[elite_indices]\n\n    def adaptive_parameters(self):\n        scale_factor = self.scale_factor_base * (1 + 0.2 * np.sin(self.evaluations / self.budget * np.pi))\n        crossover_prob = self.crossover_prob_base + 0.1 * np.cos(self.evaluations / self.budget * np.pi)\n        return scale_factor, crossover_prob\n\n    def __call__(self, func):\n        self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            elites, elite_fitness = self.elitism_selection()\n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                scale_factor, crossover_prob = self.adaptive_parameters()\n\n                if i < len(elites):\n                    candidate_vector = self.local_search(elites[i])\n                else:\n                    mutant_vector = self.differential_evolution_mutation(i, scale_factor)\n                    candidate_vector = self.crossover(self.population[i], mutant_vector, crossover_prob)\n\n                candidate_fitness = func(candidate_vector)\n                self.evaluations += 1\n\n                if candidate_fitness < self.fitness[i]:\n                    self.population[i] = candidate_vector\n                    self.fitness[i] = candidate_fitness\n\n                self.learning_rate = max(0.01, self.learning_rate * (1 - 0.03 * np.random.rand()))\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx]", "name": "EnhancedHybridStochasticOptimizer", "description": "Enhanced hybrid optimizer with strategic adaptive scaling, dynamic convergence control, and improved elite preservation to accelerate convergence.", "configspace": "", "generation": 94, "fitness": 0.36968713049560464, "feedback": "The algorithm EnhancedHybridStochasticOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.37 with standard deviation 0.25.", "error": "", "parent_id": "611e17be-2543-4b24-a742-9175f71fb862", "metadata": {"aucs": [0.8386815577687412, 0.8188196256900071, 0.8433942203480411, 0.8355849636559125, 0.8231311251558875, 0.8586687377689669, 0.8394724522177672, 0.8521417557260852, 0.8495021662778636, 0.5588692799921875, 0.6472373126247013, 0.6426148519912156, 0.675898347553397, 0.580812035054299, 0.606488982034244, 0.6595733534996926, 0.5653228808422821, 0.6324203955956806, 0.14647623221266037, 0.1614956940176282, 0.6599621194690044, 0.1439436885017552, 0.15246069522072214, 0.16158273865717931, 0.15698921446461667, 0.15665180395790534, 0.14252206146861557, 0.5231324445503952, 0.11629057426586153, 0.1782934507673164, 0.1782968017155696, 0.15538968015252352, 0.18481481237198627, 0.14184118051378214, 0.17956388607633655, 0.14576241373632182, 0.9902660931913864, 0.9895253671632557, 0.9922075291256625, 0.9729013905667985, 0.987028376884923, 0.9947374424359393, 0.976530918420106, 0.9725824825213976, 0.9752685221545752, 0.5237538970499374, 0.5112837282100958, 0.5028579889802445, 0.5284821615023971, 0.5169578985296879, 0.5154944536232752, 0.4791891811302704, 0.5129781786648524, 0.49185814815887774, 0.22391372361782558, 0.21779506154027028, 0.22246807679801206, 0.8409258945788329, 0.20593338547489015, 0.6919792689745783, 0.8362550924247963, 0.2336169972855633, 0.8171317984394867, 0.36419531899557034, 0.40498958836023113, 0.4025188020164062, 0.4104437413937432, 0.13342419241347192, 0.3665845220734517, 0.13047419353409972, 0.37133819166311555, 0.3922716923412103, 0.42862486715847437, 0.4137380975858064, 0.32583748103549803, 0.4093216767541249, 0.4079753397810527, 0.4156751500357152, 0.35370129099194747, 0.39202278245307165, 0.43910016562893983, 0.21583279250080745, 0.18755583809097875, 0.2957477470784481, 0.24460990766943247, 0.24370246934585638, 0.26216696120764826, 0.2445677768711294, 0.2601021211757193, 0.2719574033984259, 0.2974833211400605, 0.1622487887733507, 0.28538790987358886, 0.3064548086914043, 0.26627580603344203, 0.26269424075566106, 0.35378221769283213, 0.3443966321749614, 0.39630511820170666, 0.08869991925066245, 0.05136386061170195, 0.0669923733775144, 0.15149153527960968, 0.17107114756803732, 0.13591010233782785, 0.15387124808931207, 0.11023575656003703, 0.16567092399900007, 0.24970295680284804, 0.23469035942941452, 0.2705879259202454, 0.24859573258605328, 0.232101264787416, 0.24202503559840527, 0.20640734184884968, 0.2803288070142731, 0.23499926848917707, 0.6049648255067124, 0.6085537295402084, 0.6302873773166559, 0.6095582957505283, 0.6040909348010863, 0.5977023520222751, 0.6275631138044069, 0.6377507228637245, 0.623501864389614, 0.0958204286986073, 0.10348808409886256, 0.16020173808563398, 0.1198764567384476, 0.13545913840328772, 0.1444418282997183, 0.15913119649022933, 0.15282937893245474, 0.16203556904600624, 0.13177513583333944, 0.3581116257662358, 0.2557613003600412, 0.43988810725606964, 0.4944175559933426, 0.14490053719881513, 0.18968229730226271, 0.18028312029481075, 0.2103218701423517, 0.4225027817888386, 0.3990968609687554, 0.41141364123380775, 0.39377539872917644, 0.30015014451453936, 0.4159872012014222, 0.449877394210192, 0.4138548587824882, 0.2504708970264401, 0.2015718046775843, 0.2865217596926868, 0.33030900058655466, 0.314844316475124, 0.2924382500435261, 0.2801622232558081, 0.2905354124918872, 0.34857275121617726, 0.321676647854951, 0.2018410454915287, 0.19758484671285415, 0.1745989696401008, 0.23974598750708487, 0.19800766631780697, 0.2075949598011515, 0.19573874155251347, 0.1864400269640657, 0.19731816949531267, 0.2007039061386362, 0.7088460752689607, 0.7399436681542358, 0.19879806312155657, 0.2222001815961271, 0.244757371070202, 0.2503867599270476, 0.25009901009057434, 0.20198231454002225, 0.1468417954735951, 0.15335283125206411, 0.1639551564057936, 0.16968825582302438, 0.19829324848963836, 0.9038920646165093, 0.15509442441503674, 0.8852977369447718, 0.19486570314929186, 0.16423373131032237, 0.853271676510522, 0.7880391133466134, 0.20637437122810298, 0.8404349557393236, 0.16774808008821362, 0.7939080693455985, 0.2100588899611293, 0.16887850799696358, 0.25623201375873905, 0.21809941351486628, 0.28276906249513356, 0.24863497099647058, 0.260851573475382, 0.21386597755762782, 0.29932210718727414, 0.25365304363566243, 0.3431031865553419, 0.10703844334546775, 0.09439649054760302, 0.1015066915812406, 0.09653318380355702, 0.1149587604639748, 0.08465305472412255, 0.1085901094493602, 0.10223612852685637, 0.08271856245306464]}, "mutation_prompt": null}
{"id": "08b5e37f-c9aa-4991-a8fd-1b1838c95348", "solution": "import numpy as np\n\nclass EnhancedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.initial_scale_factor = 0.90  # Slightly more aggressive to explore the search space\n        self.initial_crossover_prob = 0.8  # Further increased to promote diversity\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population = self.opposition_based_initialization()\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.learning_rate = 0.14  # Increased for faster adaptation during local search\n        self.elitism_rate = 0.15  # More elites to maintain variety and retain quality solutions\n\n    def opposition_based_initialization(self):\n        initial_population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        opposition_population = self.lower_bound + self.upper_bound - initial_population\n        combined_population = np.vstack((initial_population, opposition_population))\n        fitness = np.array([np.inf] * len(combined_population))\n        for i in range(len(combined_population)):\n            fitness[i] = np.random.uniform(0, 1)  # Slightly more structured randomness\n        best_indices = np.argsort(fitness)[:self.population_size]\n        return combined_population[best_indices]\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def select_parents(self, idx):\n        indices = [i for i in range(self.population_size) if i != idx]\n        return np.random.choice(indices, 3, replace=False)\n\n    def differential_evolution_mutation(self, idx, scale_factor):\n        a, b, c = self.select_parents(idx)\n        mutant_vector = self.population[a] + scale_factor * (self.population[b] - self.population[c])\n        return np.clip(mutant_vector, self.lower_bound, self.upper_bound)\n\n    def crossover(self, target_vector, mutant_vector, crossover_prob):\n        crossover_mask = np.random.rand(self.dim) < crossover_prob\n        return np.where(crossover_mask, mutant_vector, target_vector)\n\n    def local_search(self, vector):\n        perturbation = np.random.normal(0, self.learning_rate, self.dim)\n        candidate_vector = vector + perturbation\n        return np.clip(candidate_vector, self.lower_bound, self.upper_bound)\n\n    def elitism_selection(self):\n        num_elites = max(1, int(self.elitism_rate * self.population_size))\n        elite_indices = np.argsort(self.fitness)[:num_elites]\n        return self.population[elite_indices], self.fitness[elite_indices]\n\n    def __call__(self, func):\n        self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            elites, elite_fitness = self.elitism_selection()\n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                scale_factor = self.initial_scale_factor * (1 - 0.8 * self.evaluations / self.budget)  # More adaptive scaling\n                crossover_prob = self.initial_crossover_prob + 0.1 * np.random.rand()  # Subtle randomization\n\n                if i < len(elites):  # Use elite individuals\n                    candidate_vector = self.local_search(elites[i])\n                else:\n                    mutant_vector = self.differential_evolution_mutation(i, scale_factor)\n                    candidate_vector = self.crossover(self.population[i], mutant_vector, crossover_prob)\n\n                candidate_fitness = func(candidate_vector)\n                self.evaluations += 1\n\n                if candidate_fitness < self.fitness[i]:\n                    self.population[i] = candidate_vector\n                    self.fitness[i] = candidate_fitness\n\n                self.learning_rate = max(0.02, self.learning_rate * (1 - 0.05 * np.random.rand()))  # Enhanced learning decay\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx]", "name": "EnhancedHybridOptimizer", "description": "Enhanced hybrid optimizer with adaptive learning rate and balance between exploration and exploitation for improved convergence efficiency.", "configspace": "", "generation": 95, "fitness": 0.4185652441997817, "feedback": "The algorithm EnhancedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.42 with standard deviation 0.27.", "error": "", "parent_id": "611e17be-2543-4b24-a742-9175f71fb862", "metadata": {"aucs": [0.8553941522248028, 0.8699682472221094, 0.8577424307020431, 0.8889632014314348, 0.877447361414685, 0.8723171967303739, 0.861568031062717, 0.8734039813318454, 0.8740589666571668, 0.698976639192313, 0.7001792350686197, 0.7343289450782329, 0.7307237150881634, 0.7341023447528198, 0.7411194826016624, 0.7176162504073413, 0.7084367546153063, 0.7262190409254619, 0.15726594124110327, 0.15990791071777588, 0.14073995495844205, 0.1625138151714609, 0.7030792667396895, 0.6798050383583956, 0.14458177559058472, 0.18387390489800615, 0.12678733111801266, 0.13700511867956355, 0.12269720912841642, 0.12260202667800513, 0.1575011922785794, 0.6633282385793904, 0.16060469819728185, 0.1675599773614882, 0.14404499428049133, 0.13784473021858323, 0.9785721611307306, 0.9701022834504855, 0.9694977118825908, 0.9705121576571691, 0.9797785492075727, 0.9934196325554131, 0.9812712168941777, 0.9825982226422101, 0.9853944076817275, 0.6109124614337631, 0.6490980793993513, 0.655145960141061, 0.5982029436379266, 0.604088430120379, 0.6270465194297092, 0.6801982567605079, 0.6639012031030944, 0.6694182388934281, 0.23062772194372172, 0.22499757514387453, 0.1703536249332256, 0.9122638003935042, 0.8411364790721678, 0.27672599388519725, 0.8516968973952084, 0.9125868694436358, 0.859463844892025, 0.38194579304444753, 0.3429072313117091, 0.5180002156315096, 0.659251525609003, 0.13169506497444838, 0.4899761488991057, 0.3696589104913769, 0.6587269306742938, 0.6175126649703548, 0.2555299649474606, 0.37603729688492116, 0.24190073034031234, 0.5204667246382576, 0.6340290338813911, 0.33356403348941677, 0.5255170606733175, 0.6193622926605324, 0.49845704543569713, 0.06756620845852501, 0.4112244362269656, 0.360884528778752, 0.05477129495005906, 0.4508111280513193, 0.33275078733977215, 0.31650216867461134, 0.36867555022876963, 0.4804110475016229, 0.22425270067014735, 0.4968527019965472, 0.3980602803174814, 0.24726372737925817, 0.3615766333604349, 0.20137221207716194, 0.636458264384903, 0.32406173695322393, 0.624297282528031, 0.06250254105019049, 0.13498442944106515, 0.09961231305945983, 0.11187842137807658, 0.14700334818993654, 0.14670899945314086, 0.18106068426944022, 0.14111376802775866, 0.14477869419998113, 0.3743507662678711, 0.33320287016203176, 0.350222695337187, 0.3585844856855248, 0.34783037037743103, 0.3167984233149438, 0.31747434330734536, 0.3282922769324459, 0.3518061282203365, 0.7316251897090935, 0.7098347038580456, 0.7316374603814999, 0.7680844475312425, 0.7875446264030395, 0.761287192101711, 0.6232227675123639, 0.6933583325814168, 0.6767192502723596, 0.17932840232921987, 0.10102809506887189, 0.13293002337772852, 0.15924837898603583, 0.14524235339988356, 0.09722003039326044, 0.11800459209358682, 0.10296702799492907, 0.14022670521424607, 0.20775133591448558, 0.133322643738348, 0.2790460178317783, 0.6715223350179662, 0.7072322775988445, 0.22979522628866422, 0.19470045245237033, 0.3127715108285837, 0.2165904057449921, 0.4699289649548569, 0.5333445235279527, 0.4936877051338664, 0.5261556605879112, 0.2177993299245713, 0.5289205901573584, 0.4895338410118023, 0.48501949151168555, 0.49915919645089546, 0.36451262751542, 0.39144908721663485, 0.4004647577689552, 0.3965830021698633, 0.22032293996274332, 0.3824000264301426, 0.4035072169133255, 0.4740305928101787, 0.39932773084341544, 0.2423845704004155, 0.22148717750346325, 0.1919023829850155, 0.24863418492678402, 0.248769918995575, 0.20098709060590358, 0.21821303876454534, 0.21929147695857143, 0.2073326443738468, 0.7483348209977194, 0.7847240194583254, 0.18956489406083132, 0.24530975325390791, 0.2472525082529965, 0.20144531602646443, 0.18903113436380037, 0.22583293030356955, 0.7081300069783495, 0.16503487595437183, 0.19907701629405206, 0.15318426691817333, 0.1863722826277192, 0.19948404025132227, 0.8673837488949441, 0.16200173241905946, 0.1851107239068085, 0.1758418066377051, 0.12603338776763684, 0.20708576846933502, 0.8290308970028585, 0.16643080720968595, 0.15310078680293226, 0.8822754293942993, 0.21215911145737576, 0.8501011667396555, 0.21158699133412506, 0.2163418331796716, 0.23509955305440622, 0.3314008640130687, 0.2406187646075415, 0.3563638715028806, 0.2255763017286717, 0.37307823155586617, 0.4186502823252617, 0.23530477819097106, 0.09790290849233119, 0.09599615985586463, 0.09367077443048688, 0.09957761116486818, 0.0712657810566456, 0.07954232629418989, 0.09609308959989471, 0.0895078549265278, 0.07193272142988494]}, "mutation_prompt": null}
{"id": "752aaa82-d6c2-4a92-bd14-a6d42f199464", "solution": "import numpy as np\n\nclass EnhancedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.initial_scale_factor = 0.9  # Increased initial mutation factor for exploration\n        self.initial_crossover_prob = 0.7  # Reduced initial crossover to maintain diversity\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population = self.opposition_based_initialization()\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.learning_rate = 0.15  # Increased learning rate for faster local search adjustments\n        self.elitism_rate = 0.15  # Increased elitism to retain more high-quality solutions\n\n    def opposition_based_initialization(self):\n        initial_population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        opposition_population = self.lower_bound + self.upper_bound - initial_population\n        combined_population = np.vstack((initial_population, opposition_population))\n        fitness = np.array([np.inf] * len(combined_population))\n        for i in range(len(combined_population)):\n            fitness[i] = np.random.rand() \n        best_indices = np.argsort(fitness)[:self.population_size]\n        return combined_population[best_indices]\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def select_parents(self, idx):\n        indices = [i for i in range(self.population_size) if i != idx]\n        return np.random.choice(indices, 3, replace=False)\n\n    def differential_evolution_mutation(self, idx, scale_factor):\n        a, b, c = self.select_parents(idx)\n        mutant_vector = self.population[a] + scale_factor * (self.population[b] - self.population[c])\n        return np.clip(mutant_vector, self.lower_bound, self.upper_bound)\n\n    def crossover(self, target_vector, mutant_vector, crossover_prob):\n        crossover_mask = np.random.rand(self.dim) < crossover_prob\n        random_index = np.random.randint(self.dim)  # Ensure at least one element is crossed\n        crossover_mask[random_index] = True\n        return np.where(crossover_mask, mutant_vector, target_vector)\n\n    def local_search(self, vector):\n        perturbation = np.random.normal(0, self.learning_rate, self.dim)\n        candidate_vector = vector + perturbation\n        return np.clip(candidate_vector, self.lower_bound, self.upper_bound)\n\n    def elitism_selection(self):\n        num_elites = max(1, int(self.elitism_rate * self.population_size))\n        elite_indices = np.argsort(self.fitness)[:num_elites]\n        return self.population[elite_indices], self.fitness[elite_indices]\n\n    def __call__(self, func):\n        self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            elites, elite_fitness = self.elitism_selection()\n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                scale_factor = self.initial_scale_factor * (1 - self.evaluations / self.budget)\n                crossover_prob = self.initial_crossover_prob + 0.15 * np.random.rand()\n\n                if i < len(elites):  # Utilize elite individuals\n                    candidate_vector = self.local_search(elites[i])\n                else:\n                    mutant_vector = self.differential_evolution_mutation(i, scale_factor)\n                    candidate_vector = self.crossover(self.population[i], mutant_vector, crossover_prob)\n\n                candidate_fitness = func(candidate_vector)\n                self.evaluations += 1\n\n                if candidate_fitness < self.fitness[i]:\n                    self.population[i] = candidate_vector\n                    self.fitness[i] = candidate_fitness\n\n                self.learning_rate = max(0.01, self.learning_rate * (1 - 0.03 * np.random.rand()))  # Adjust decay rate\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx]", "name": "EnhancedHybridOptimizer", "description": "An enhanced hybrid optimizer with dynamic mutation, adaptive crossover, and intensified local search for accelerated convergence.", "configspace": "", "generation": 96, "fitness": 0.40773626416105174, "feedback": "The algorithm EnhancedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.41 with standard deviation 0.28.", "error": "", "parent_id": "611e17be-2543-4b24-a742-9175f71fb862", "metadata": {"aucs": [0.8797817543517282, 0.8847111834400215, 0.8702932879316931, 0.8848764758618594, 0.8913033751613602, 0.8854580657462394, 0.8707556720693043, 0.8927850414604935, 0.8737870347586807, 0.7544156247215134, 0.7399132361708354, 0.7519178987283897, 0.750956905493166, 0.7658386843383309, 0.7405416754539325, 0.7499736364582946, 0.7416074369436927, 0.7740175406332813, 0.15758890559885808, 0.18325695509171702, 0.15386431980651272, 0.1580833202064057, 0.131616759481483, 0.16074530375938845, 0.18840734839378215, 0.16046509907645412, 0.15396024525075358, 0.10452071723695866, 0.14118738159456246, 0.13878986008612626, 0.1426042039902784, 0.09997521650564467, 0.15624436636798078, 0.11831802857168694, 0.15346972136633963, 0.14570859066594022, 0.9890151715360054, 0.9816482070380307, 0.9748821065481196, 0.9678500221406494, 0.961104094856297, 0.9902172204594999, 0.9767117195841053, 0.9733560230797987, 0.9668536858856311, 0.5854601735652454, 0.6659489402427222, 0.6322487099130316, 0.657112514175028, 0.6124384068631321, 0.6262356832054032, 0.5548933554004432, 0.5910022466914707, 0.5792017544243495, 0.22538357123440278, 0.38909724824023273, 0.8465305643126653, 0.8565071512446976, 0.847194245367645, 0.8790422667474423, 0.17409195422263546, 0.8292812137715366, 0.8458179275552776, 0.494346640984339, 0.305940909760703, 0.504787363781051, 0.45717641040197365, 0.38928798216363447, 0.3622117958815565, 0.37334443177857124, 0.48421889460555356, 0.45137246992456115, 0.16290752771611894, 0.1279200446344685, 0.4513319521637602, 0.5393605639169196, 0.494739474655018, 0.378049220851882, 0.24020609072106425, 0.6578935493930391, 0.4167061994123018, 0.14830806681866493, 0.08828590977959427, 0.4368031961453265, 0.20240457982529914, 0.06847886973229833, 0.17889008120368166, 0.1087686582870725, 0.43340777212742765, 0.22780577638043187, 0.42394971032438755, 0.1971913898845411, 0.20181001043542357, 0.3640413114196607, 0.17759152610326034, 0.3227594651533706, 0.18354224005628827, 0.5599490804102478, 0.4415220262598588, 0.10881815626312896, 0.05250736852969895, 0.16433699414990055, 0.16996785835899464, 0.17062348673410233, 0.18996470859847825, 0.10807925287538578, 0.19839170842250686, 0.1335372410610025, 0.3426327683856133, 0.27045135581651014, 0.3156741387039723, 0.3067276158293746, 0.3949355633172633, 0.35566726139859894, 0.3552645988106282, 0.342582702129154, 0.2822619568610385, 0.6603220263948845, 0.8123613443000562, 0.7463383580604261, 0.7428858071020615, 0.7536375198789864, 0.6298440278314201, 0.7283194769743849, 0.6310749988812538, 0.6623214742878165, 0.1224184178975074, 0.14176972141572475, 0.09155906710201067, 0.09429869090172638, 0.14218899021568598, 0.1404403555740248, 0.1276176773629235, 0.09470584768945067, 0.11622008881173718, 0.17682909261175483, 0.22647302933907298, 0.2066491378564531, 0.7134152381428215, 0.17341169556226688, 0.1556793187043919, 0.20779220573168777, 0.18787837590467105, 0.6433482927164459, 0.5644175929142365, 0.5243475122786936, 0.293708974773212, 0.5664465475071008, 0.21712173484323494, 0.4977337032010196, 0.5364296111548297, 0.6298327917762027, 0.5447032857257756, 0.3656035255616308, 0.4035599718343911, 0.35679452901614395, 0.4645093022432528, 0.3539607527333787, 0.39200014568730346, 0.36674679217588224, 0.24790237435384, 0.38412446789593424, 0.18457053860506845, 0.24016207157421843, 0.24620754153952618, 0.22820808918505486, 0.26717346941545916, 0.21279453481051036, 0.24393962459034635, 0.22934804941375408, 0.24079234174073172, 0.22196499658711277, 0.2420927964796501, 0.19580810883217714, 0.2106729361413331, 0.7871746346918017, 0.20241337638197188, 0.21912474309459273, 0.24966323704310478, 0.17330169522625605, 0.8586686641004121, 0.1623684909381694, 0.18318034058767108, 0.8973171119208435, 0.8653679473876894, 0.8877785585055258, 0.1420976145398447, 0.17799653561108353, 0.16184889268020308, 0.8075499956293067, 0.16566286505874195, 0.8555429470661904, 0.20779061191467063, 0.21047924274622076, 0.127399877617556, 0.9021150647668923, 0.20932090032884676, 0.21143017133108089, 0.39518426447555444, 0.3582755969355439, 0.2672569318973884, 0.313712107452804, 0.22504055151904112, 0.3992934481241813, 0.2807211520999747, 0.25167793548194317, 0.4309422453800851, 0.095438886096468, 0.0901659685999312, 0.09502831031719416, 0.10482554475021277, 0.07252603241786115, 0.08788430189934915, 0.09372784881305252, 0.07896718890276266, 0.11068925219461012]}, "mutation_prompt": null}
{"id": "e066b819-3da5-4a88-ab8d-1b60939a1753", "solution": "import numpy as np\n\nclass RefinedHybridStochasticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.initial_scale_factor = 0.85  # Adjusted for slightly more aggressive mutations\n        self.initial_crossover_prob = 0.75  # Increase crossover probability to enhance diversity\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population = self.opposition_based_initialization()\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.learning_rate = 0.12  # Slightly increased learning rate for faster local search\n        self.elitism_rate = 0.12  # Slightly increased elitism for preserving more top candidates\n\n    def opposition_based_initialization(self):\n        initial_population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        opposition_population = self.lower_bound + self.upper_bound - initial_population\n        combined_population = np.vstack((initial_population, opposition_population))\n        fitness = np.array([np.inf] * len(combined_population))\n        for i in range(len(combined_population)):\n            fitness[i] = np.random.rand()  # Introduce randomness for better initial diversity\n        best_indices = np.argsort(fitness)[:self.population_size]\n        return combined_population[best_indices]\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def select_parents(self, idx):\n        indices = [i for i in range(self.population_size) if i != idx]\n        return np.random.choice(indices, 3, replace=False)\n\n    def differential_evolution_mutation(self, idx, scale_factor):\n        a, b, c = self.select_parents(idx)\n        mutant_vector = self.population[a] + scale_factor * (self.population[b] - self.population[c])\n        return np.clip(mutant_vector, self.lower_bound, self.upper_bound)\n\n    def crossover(self, target_vector, mutant_vector, crossover_prob):\n        crossover_mask = np.random.rand(self.dim) < crossover_prob\n        return np.where(crossover_mask, mutant_vector, target_vector)\n\n    def local_search(self, vector):\n        perturbation = np.random.normal(0, self.learning_rate, self.dim)\n        candidate_vector = vector + perturbation\n        return np.clip(candidate_vector, self.lower_bound, self.upper_bound)\n\n    def elitism_selection(self):\n        num_elites = max(1, int(self.elitism_rate * self.population_size))\n        elite_indices = np.argsort(self.fitness)[:num_elites]\n        return self.population[elite_indices], self.fitness[elite_indices]\n\n    def __call__(self, func):\n        self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            elites, elite_fitness = self.elitism_selection()\n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                scale_factor = self.initial_scale_factor * (1 - self.evaluations / self.budget)\n                crossover_prob = self.initial_crossover_prob + 0.15 * np.random.rand()  # Narrow crossover randomness\n\n                if i < len(elites):  # Use elite individuals\n                    candidate_vector = self.local_search(elites[i])\n                else:\n                    mutant_vector = self.differential_evolution_mutation(i, scale_factor)\n                    candidate_vector = self.crossover(self.population[i], mutant_vector, crossover_prob)\n\n                candidate_fitness = func(candidate_vector)\n                self.evaluations += 1\n\n                if candidate_fitness < self.fitness[i]:\n                    self.population[i] = candidate_vector\n                    self.fitness[i] = candidate_fitness\n\n                self.learning_rate = max(0.01, self.learning_rate * (1 - 0.04 * np.random.rand()))  # Adjusted learning decay\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx]", "name": "RefinedHybridStochasticOptimizer", "description": "A refined hybrid optimizer with adaptive opposition strategy and dynamic exploration-exploitation balance for enhanced convergence speed.", "configspace": "", "generation": 85, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "611e17be-2543-4b24-a742-9175f71fb862", "metadata": {"aucs": [0.8766756452394434, 0.8780259518892735, 0.8679732870259016, 0.8851321249681726, 0.8834248924740489, 0.8917330111634347, 0.8915409061661912, 0.8625223086831446, 0.8855384988765383, 0.7702063528242651, 0.764605999911898, 0.7866633313683472, 0.788225311474078, 0.7768513285041114, 0.783710568385616, 0.7710553972167912, 0.7826320564567563, 0.7747122369896489, 0.7207065189713453, 0.6772268580219241, 0.17524904499097194, 0.1545419816557405, 0.12287270778105774, 0.689897670488521, 0.17408711892974027, 0.7264644514688223, 0.17782525894103152, 0.5540661022978465, 0.1718555788393058, 0.17432731451169114, 0.1401812381721591, 0.181817031126965, 0.17084893094232578, 0.14077243420807695, 0.11782626428419385, 0.1411557885190976, 0.9937933542659784, 0.9824818277808939, 0.980707506224134, 0.9544651636952174, 0.9824601842623015, 0.9786103071199913, 0.9917495387625039, 0.9773550272665161, 0.9767648277870316, 0.7186425059203514, 0.7072341755938936, 0.67619418713103, 0.5500313926892695, 0.6353964019006688, 0.5655276295816347, 0.6087248017767631, 0.7250088325007087, 0.5509231661245519, 0.8306251087247614, 0.3722435473015898, 0.8654864869334986, 0.8863503903735646, 0.8760746636972028, 0.8458760719743885, 0.8486549335282704, 0.8432469041054449, 0.3860328241903793, 0.3132235440659491, 0.4899210907813042, 0.2895826691181592, 0.19744367076628966, 0.4358144265843764, 0.19907439487724254, 0.36614638947632094, 0.4673958269450197, 0.4412371012727202, 0.1285886837615413, 0.23403231590111162, 0.36109192393377976, 0.22664878079649053, 0.2540072344222718, 0.4265671714982343, 0.2827316237596058, 0.6431777309769313, 0.3732167479133396, 0.12116522298168053, 0.1678990673445524, 0.3052731722642458, 0.04944125032055546, 0.42013758686785785, 0.4675757890099749, 0.34909871477201915, 0.44848393730635916, 0.4121936894642598, 0.2572264031099728, 0.14782089879620763, 0.45511882842056595, 0.3614252035664588, 0.12576950129727194, 0.19368490486653112, 0.5599530644881539, 0.4930337585131278, 0.48359551057535666, 0.31457189059998514, 0.12229189958524489, 0.1343438096316968, 0.1129583796588105, 0.17065635226036613, 0.21121710331876486, 0.17960579905235896, 0.13080473597692177, 0.17281085533956853, 0.3785105716917646, 0.32356504637548955, 0.2930510023224221, 0.4263055259587192, 0.3758837171163727, 0.3487130962919658, 0.34061488164921405, 0.38468365570003926, 0.36601498696637014, 0.684964500616335, 0.7393164733752317, 0.7681352739007481, 0.7580447340486791, 0.6512200131325806, 0.7160469344263029, 0.5965650033008165, 0.7695202275323955, 0.7091180301825857, 0.13027852195339762, 0.1752862344756313, 0.14725999438068593, 0.10963074171804343, 0.12196205006310135, 0.12842695335615995, 0.1535536314706123, 0.09324353688930664, 0.11756250657323708, 0.1583195481084767, 0.17804157548667843, 0.27489139187246203, 0.21745895839396734, 0.667095829932591, 0.20628860250021674, 0.5805893447980215, 0.2351413331273987, 0.23368786575655498, 0.5245002013696674, 0.5809814491680333, 0.4604348637764978, 0.5487855436976408, 0.5109761909197574, 0.5670142492913299, 0.5523316647904777, 0.6211521395155045, 0.5065222018957187, 0.43913043730592705, 0.4209119941181332, 0.24723244458163163, 0.36854984117381473, 0.4714349416601522, 0.4087858118863441, 0.4445023749465481, 0.47953063462436707, 0.4056768097816228, 0.20187088443067747, 0.28194838553846535, 0.22079563605800234, 0.2193518224022324, 0.18672017534619767, 0.21755812838823685, 0.21888893291732603, 0.20535976400015987, 0.24766721582344342, 0.7731611289963906, 0.22212714398023892, 0.22683569749128651, 0.2219774539530932, 0.2038861631536516, 0.22700750845132922, 0.19396010038295208, 0.19566429190218382, 0.24739215490626576, 0.16341617726849111, 0.8639742256032173, 0.18598438634511893, 0.19365704699243147, 0.8973800514282001, 0.8634132304309223, 0.8472812455205514, 0.1985001542527517, 0.1585146185947931, 0.8337654020690852, 0.16815553765976654, 0.16767777920264038, 0.8145817251939893, 0.1661887704123024, 0.2002384045022092, 0.21270385229000333, 0.15502895016859852, 0.15581584216296274, 0.44112899395539285, 0.2704890640209542, 0.317178982997638, 0.2965929127827681, 0.4314839426197882, 0.2443844071739082, 0.2272806115444309, 0.24705198287063868, 0.2919163957284867, 0.10742674978866673, 0.09615090976507867, 0.08932957373600692, 0.10039145184892406, 0.07777519619601359, 0.09346825340867582, 0.09191978538549628, 0.10051502779803445, 0.09032627828645579]}, "mutation_prompt": null}
{"id": "f309fa58-6d65-4b75-bc5a-a291b8f919a2", "solution": "import numpy as np\n\nclass EnhancedHybridStochasticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 22  # Increased population size slightly for better diversity\n        self.initial_scale_factor = 0.9  # More aggressive mutations\n        self.initial_crossover_prob = 0.8  # Further increase to enhance diversity\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population = self.opposition_based_initialization()\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.learning_rate = 0.15  # Increased learning rate for faster local search\n        self.elitism_rate = 0.15  # Increased elitism for preserving more top candidates\n\n    def opposition_based_initialization(self):\n        initial_population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        opposition_population = self.lower_bound + self.upper_bound - initial_population\n        combined_population = np.vstack((initial_population, opposition_population))\n        fitness = np.array([np.inf] * len(combined_population))\n        for i in range(len(combined_population)):\n            fitness[i] = np.random.rand()\n        best_indices = np.argsort(fitness)[:self.population_size]\n        return combined_population[best_indices]\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def select_parents(self, idx):\n        indices = [i for i in range(self.population_size) if i != idx]\n        return np.random.choice(indices, 3, replace=False)\n\n    def differential_evolution_mutation(self, idx, scale_factor):\n        a, b, c = self.select_parents(idx)\n        mutant_vector = self.population[a] + scale_factor * (self.population[b] - self.population[c])\n        return np.clip(mutant_vector, self.lower_bound, self.upper_bound)\n\n    def crossover(self, target_vector, mutant_vector, crossover_prob):\n        crossover_mask = np.random.rand(self.dim) < crossover_prob\n        return np.where(crossover_mask, mutant_vector, target_vector)\n\n    def local_search(self, vector):\n        perturbation = np.random.normal(0, self.learning_rate, self.dim)\n        candidate_vector = vector + perturbation\n        return np.clip(candidate_vector, self.lower_bound, self.upper_bound)\n\n    def elitism_selection(self):\n        num_elites = max(1, int(self.elitism_rate * self.population_size))\n        elite_indices = np.argsort(self.fitness)[:num_elites]\n        return self.population[elite_indices], self.fitness[elite_indices]\n\n    def __call__(self, func):\n        self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            elites, elite_fitness = self.elitism_selection()\n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                scale_factor = self.initial_scale_factor * (1 - self.evaluations / self.budget)\n                crossover_prob = self.initial_crossover_prob + 0.1 * np.random.rand()  # Reduce randomness for stability\n\n                if i < len(elites):\n                    candidate_vector = self.local_search(elites[i])\n                else:\n                    mutant_vector = self.differential_evolution_mutation(i, scale_factor)\n                    candidate_vector = self.crossover(self.population[i], mutant_vector, crossover_prob)\n\n                candidate_fitness = func(candidate_vector)\n                self.evaluations += 1\n\n                if candidate_fitness < self.fitness[i]:\n                    self.population[i] = candidate_vector\n                    self.fitness[i] = candidate_fitness\n\n                self.learning_rate = max(0.01, self.learning_rate * (1 - 0.03 * np.random.rand()))  # Slower learning decay\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx]", "name": "EnhancedHybridStochasticOptimizer", "description": "Enhanced Hybrid Stochastic Optimizer with improved adaptive learning and dynamic diversity mechanisms for faster convergence.", "configspace": "", "generation": 98, "fitness": 0.4087565691528016, "feedback": "The algorithm EnhancedHybridStochasticOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.41 with standard deviation 0.27.", "error": "", "parent_id": "611e17be-2543-4b24-a742-9175f71fb862", "metadata": {"aucs": [0.8601013073824898, 0.8428390112754793, 0.869302836687836, 0.8833215116617323, 0.8625159405829468, 0.872160117302418, 0.8657273763605866, 0.879585289050161, 0.8523140951490619, 0.7371463344475234, 0.7339923177598413, 0.7497127609439107, 0.7279266165455223, 0.692347064797822, 0.7483764245911323, 0.7467221769249854, 0.7429601984671208, 0.7440514830689702, 0.7050311512896994, 0.15252415702743405, 0.18333315939510708, 0.6738921966363252, 0.18091236724744575, 0.14334663733871889, 0.18190340629094282, 0.15724032243040653, 0.18326493598800453, 0.17842693028019385, 0.1377741058183597, 0.6334210712584321, 0.11567805176408219, 0.17771443660469455, 0.6609163147952806, 0.1392847662899097, 0.11394414377214324, 0.18183980550904744, 0.9699263485664347, 0.9640236627436924, 0.9768984045030714, 0.9775426747564945, 0.983207305037671, 0.9575840736108766, 0.9697754598725232, 0.9778492216344526, 0.9767897251981567, 0.6837754666528435, 0.5289393179131251, 0.5652112651059125, 0.654075151958292, 0.566892436094027, 0.60103966541397, 0.5602333646657949, 0.5796249506415376, 0.6423376775954894, 0.8387737141611902, 0.837885954560992, 0.8121932149410622, 0.2086432122937304, 0.8101516883557823, 0.8675728659620163, 0.8785777539985797, 0.24085505926252704, 0.21983342284487095, 0.5038772679980879, 0.6964927084976219, 0.3981873849273857, 0.23858257219424295, 0.33873949198865916, 0.21141858711501094, 0.6682020765079759, 0.5438154058070809, 0.5612787767761985, 0.13462038138204335, 0.1958254180325233, 0.20964931285475086, 0.6545796840711137, 0.4891099842811891, 0.30716666594589237, 0.3093478703138133, 0.2849778232495752, 0.1249534773144495, 0.44417115081071057, 0.10673598596789813, 0.11923546970539878, 0.2561560186734385, 0.03389595192964501, 0.17065671544181893, 0.4229495556530405, 0.4995270851954031, 0.17298869446478982, 0.5489498777838666, 0.4866053418569296, 0.20774972582132156, 0.2686869277288896, 0.5544153314954875, 0.2869246023553026, 0.4836134757649988, 0.6390138392333432, 0.526636603704502, 0.0675627869639931, 0.0941452378263864, 0.115165440577644, 0.17009759957698278, 0.2572269497358396, 0.25297540425698284, 0.11522227027411247, 0.1816811182097814, 0.1264972808769811, 0.3230697530696225, 0.37338988135401674, 0.40723043463550634, 0.3881156671766657, 0.3780011753513559, 0.36217794603059583, 0.35335515771316084, 0.3223985770310629, 0.3312293263810818, 0.6775402749731962, 0.7379730124691701, 0.7748909185566101, 0.6475342804646995, 0.7488699010364057, 0.7645654229766077, 0.7449675025954927, 0.6990686134351163, 0.709139909705339, 0.1607386181826168, 0.07766352271939814, 0.10875315399644736, 0.1458718109455749, 0.1419633374985222, 0.12105298377789897, 0.12270403271707198, 0.14750747993635993, 0.1261261482025211, 0.15742880583774244, 0.17742668468901446, 0.15723062989463943, 0.17867645951946143, 0.6642520765315387, 0.2211657612817981, 0.2900062662874814, 0.17175803576428916, 0.15023700436198373, 0.5137537919538464, 0.4966317781765066, 0.5206144300840027, 0.5289034048090959, 0.5270828280310091, 0.5846749833376639, 0.5365571041067279, 0.5290950718459881, 0.4917876160806862, 0.39805483522443696, 0.2595994909746091, 0.4103988488081515, 0.36055685315933095, 0.4665895245745243, 0.2176492120191278, 0.43844665707988384, 0.25090072746454584, 0.4265793776885577, 0.25022081507498295, 0.21742422010849105, 0.22617095018849054, 0.26838083234966026, 0.2336189159534683, 0.22503841793350887, 0.2498255709279611, 0.2068825211084494, 0.2719249606197627, 0.22470903861656966, 0.20672997735056908, 0.21771378085163418, 0.2001149852029761, 0.22316787525325477, 0.74884910387021, 0.24519416966906749, 0.20976749697635655, 0.7371095727541548, 0.16418820708530002, 0.1492850018035644, 0.20884587566872137, 0.18604165031750175, 0.17006201500716056, 0.8888444996068846, 0.1840308335261024, 0.8436024122137097, 0.18500426474703113, 0.16769528347948237, 0.7663300671491373, 0.11267207166685145, 0.12545666115651, 0.16775256750569612, 0.16763521689911232, 0.21094413952170932, 0.21122061293135508, 0.20969753580013528, 0.3266478193765193, 0.20775522285165293, 0.21853866004235512, 0.34822912606143097, 0.23153849039828112, 0.23175793664337063, 0.2855674241200171, 0.35427498102369603, 0.23191090694156125, 0.09809641813498782, 0.09752100564275057, 0.11592267960032165, 0.09719287072879013, 0.10562065861118031, 0.09002186933712697, 0.09744942599020812, 0.09829210736578531, 0.10889689849682238]}, "mutation_prompt": null}
{"id": "6e52b346-ca38-43bc-b366-6673d03c1bc9", "solution": "import numpy as np\n\nclass EnhancedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 22  # Slightly increased for broader coverage\n        self.initial_scale_factor = 0.9  # Enhanced mutation aggressiveness\n        self.initial_crossover_prob = 0.8  # Increased diversity through crossover\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population = self.opposition_based_initialization()\n        self.fitness = np.full(self.population_size, np.inf)\n        self.evaluations = 0\n        self.learning_rate = 0.15  # Enhanced local search efficacy\n        self.elitism_rate = 0.15  # More elites preserved for robustness\n\n    def opposition_based_initialization(self):\n        initial_population = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dim))\n        opposition_population = self.lower_bound + self.upper_bound - initial_population\n        combined_population = np.vstack((initial_population, opposition_population))\n        fitness = np.array([np.inf] * len(combined_population))\n        for i in range(len(combined_population)):\n            fitness[i] = np.random.uniform(0.5, 1.5)  # Random scaling for initial diversity\n        best_indices = np.argsort(fitness)[:self.population_size]\n        return combined_population[best_indices]\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def select_parents(self, idx):\n        indices = [i for i in range(self.population_size) if i != idx]\n        return np.random.choice(indices, 3, replace=False)\n\n    def differential_evolution_mutation(self, idx, scale_factor):\n        a, b, c = self.select_parents(idx)\n        mutant_vector = self.population[a] + scale_factor * (self.population[b] - self.population[c])\n        return np.clip(mutant_vector, self.lower_bound, self.upper_bound)\n\n    def crossover(self, target_vector, mutant_vector, crossover_prob):\n        crossover_mask = np.random.rand(self.dim) < crossover_prob\n        return np.where(crossover_mask, mutant_vector, target_vector)\n\n    def local_search(self, vector):\n        perturbation = np.random.normal(0, self.learning_rate, self.dim)\n        candidate_vector = vector + perturbation\n        return np.clip(candidate_vector, self.lower_bound, self.upper_bound)\n\n    def elitism_selection(self):\n        num_elites = max(1, int(self.elitism_rate * self.population_size))\n        elite_indices = np.argsort(self.fitness)[:num_elites]\n        return self.population[elite_indices], self.fitness[elite_indices]\n\n    def __call__(self, func):\n        self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            elites, elite_fitness = self.elitism_selection()\n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                scale_factor = self.initial_scale_factor * (1 - self.evaluations / self.budget)\n                crossover_prob = self.initial_crossover_prob + 0.10 * np.random.rand()  # Slightly reduced randomness\n\n                if i < len(elites):  # Use elite individuals\n                    candidate_vector = self.local_search(elites[i])\n                else:\n                    mutant_vector = self.differential_evolution_mutation(i, scale_factor)\n                    candidate_vector = self.crossover(self.population[i], mutant_vector, crossover_prob)\n\n                candidate_fitness = func(candidate_vector)\n                self.evaluations += 1\n\n                if candidate_fitness < self.fitness[i]:\n                    self.population[i] = candidate_vector\n                    self.fitness[i] = candidate_fitness\n\n                self.learning_rate = max(0.01, self.learning_rate * (1 - 0.03 * np.random.rand()))  # Slightly adjusted decay\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx]", "name": "EnhancedHybridOptimizer", "description": "Enhanced Hybrid Optimizer with tuned mutation strategies and adaptive learning for faster convergence.", "configspace": "", "generation": 99, "fitness": 0.4087565691528016, "feedback": "The algorithm EnhancedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.41 with standard deviation 0.27.", "error": "", "parent_id": "611e17be-2543-4b24-a742-9175f71fb862", "metadata": {"aucs": [0.8601013073824898, 0.8428390112754793, 0.869302836687836, 0.8833215116617323, 0.8625159405829468, 0.872160117302418, 0.8657273763605866, 0.879585289050161, 0.8523140951490619, 0.7371463344475234, 0.7339923177598413, 0.7497127609439107, 0.7279266165455223, 0.692347064797822, 0.7483764245911323, 0.7467221769249854, 0.7429601984671208, 0.7440514830689702, 0.7050311512896994, 0.15252415702743405, 0.18333315939510708, 0.6738921966363252, 0.18091236724744575, 0.14334663733871889, 0.18190340629094282, 0.15724032243040653, 0.18326493598800453, 0.17842693028019385, 0.1377741058183597, 0.6334210712584321, 0.11567805176408219, 0.17771443660469455, 0.6609163147952806, 0.1392847662899097, 0.11394414377214324, 0.18183980550904744, 0.9699263485664347, 0.9640236627436924, 0.9768984045030714, 0.9775426747564945, 0.983207305037671, 0.9575840736108766, 0.9697754598725232, 0.9778492216344526, 0.9767897251981567, 0.6837754666528435, 0.5289393179131251, 0.5652112651059125, 0.654075151958292, 0.566892436094027, 0.60103966541397, 0.5602333646657949, 0.5796249506415376, 0.6423376775954894, 0.8387737141611902, 0.837885954560992, 0.8121932149410622, 0.2086432122937304, 0.8101516883557823, 0.8675728659620163, 0.8785777539985797, 0.24085505926252704, 0.21983342284487095, 0.5038772679980879, 0.6964927084976219, 0.3981873849273857, 0.23858257219424295, 0.33873949198865916, 0.21141858711501094, 0.6682020765079759, 0.5438154058070809, 0.5612787767761985, 0.13462038138204335, 0.1958254180325233, 0.20964931285475086, 0.6545796840711137, 0.4891099842811891, 0.30716666594589237, 0.3093478703138133, 0.2849778232495752, 0.1249534773144495, 0.44417115081071057, 0.10673598596789813, 0.11923546970539878, 0.2561560186734385, 0.03389595192964501, 0.17065671544181893, 0.4229495556530405, 0.4995270851954031, 0.17298869446478982, 0.5489498777838666, 0.4866053418569296, 0.20774972582132156, 0.2686869277288896, 0.5544153314954875, 0.2869246023553026, 0.4836134757649988, 0.6390138392333432, 0.526636603704502, 0.0675627869639931, 0.0941452378263864, 0.115165440577644, 0.17009759957698278, 0.2572269497358396, 0.25297540425698284, 0.11522227027411247, 0.1816811182097814, 0.1264972808769811, 0.3230697530696225, 0.37338988135401674, 0.40723043463550634, 0.3881156671766657, 0.3780011753513559, 0.36217794603059583, 0.35335515771316084, 0.3223985770310629, 0.3312293263810818, 0.6775402749731962, 0.7379730124691701, 0.7748909185566101, 0.6475342804646995, 0.7488699010364057, 0.7645654229766077, 0.7449675025954927, 0.6990686134351163, 0.709139909705339, 0.1607386181826168, 0.07766352271939814, 0.10875315399644736, 0.1458718109455749, 0.1419633374985222, 0.12105298377789897, 0.12270403271707198, 0.14750747993635993, 0.1261261482025211, 0.15742880583774244, 0.17742668468901446, 0.15723062989463943, 0.17867645951946143, 0.6642520765315387, 0.2211657612817981, 0.2900062662874814, 0.17175803576428916, 0.15023700436198373, 0.5137537919538464, 0.4966317781765066, 0.5206144300840027, 0.5289034048090959, 0.5270828280310091, 0.5846749833376639, 0.5365571041067279, 0.5290950718459881, 0.4917876160806862, 0.39805483522443696, 0.2595994909746091, 0.4103988488081515, 0.36055685315933095, 0.4665895245745243, 0.2176492120191278, 0.43844665707988384, 0.25090072746454584, 0.4265793776885577, 0.25022081507498295, 0.21742422010849105, 0.22617095018849054, 0.26838083234966026, 0.2336189159534683, 0.22503841793350887, 0.2498255709279611, 0.2068825211084494, 0.2719249606197627, 0.22470903861656966, 0.20672997735056908, 0.21771378085163418, 0.2001149852029761, 0.22316787525325477, 0.74884910387021, 0.24519416966906749, 0.20976749697635655, 0.7371095727541548, 0.16418820708530002, 0.1492850018035644, 0.20884587566872137, 0.18604165031750175, 0.17006201500716056, 0.8888444996068846, 0.1840308335261024, 0.8436024122137097, 0.18500426474703113, 0.16769528347948237, 0.7663300671491373, 0.11267207166685145, 0.12545666115651, 0.16775256750569612, 0.16763521689911232, 0.21094413952170932, 0.21122061293135508, 0.20969753580013528, 0.3266478193765193, 0.20775522285165293, 0.21853866004235512, 0.34822912606143097, 0.23153849039828112, 0.23175793664337063, 0.2855674241200171, 0.35427498102369603, 0.23191090694156125, 0.09809641813498782, 0.09752100564275057, 0.11592267960032165, 0.09719287072879013, 0.10562065861118031, 0.09002186933712697, 0.09744942599020812, 0.09829210736578531, 0.10889689849682238]}, "mutation_prompt": null}
