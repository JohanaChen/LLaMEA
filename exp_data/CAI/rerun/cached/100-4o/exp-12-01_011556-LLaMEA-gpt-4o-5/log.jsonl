{"id": "898f0209-2d25-4550-a127-503a4f955d5b", "solution": "import numpy as np\n\nclass EvolutionaryGradientSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        \n    def __call__(self, func):\n        np.random.seed(42)\n        population_size = 20\n        step_size = 0.1\n        \n        population = np.random.uniform(self.lower_bound, self.upper_bound, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = population_size\n        \n        while evaluations < self.budget:\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n                # Gradient-based local search\n                grad = self.estimate_gradient(func, population[i], func(population[i]), step_size)\n                new_solution = population[i] - step_size * grad\n                new_solution = np.clip(new_solution, self.lower_bound, self.upper_bound)\n                new_fitness = func(new_solution)\n                evaluations += 1\n                \n                # Replace if new solution is better\n                if new_fitness < fitness[i]:\n                    population[i] = new_solution\n                    fitness[i] = new_fitness\n            \n            # Evolutionary step: crossover and mutation\n            if evaluations < self.budget:\n                parents_indices = np.random.choice(population_size, size=2, replace=False)\n                parent1, parent2 = population[parents_indices]\n                \n                crossover_point = np.random.randint(1, self.dim)\n                child = np.concatenate((parent1[:crossover_point], parent2[crossover_point:]))\n                \n                # Mutation\n                mutation_vector = np.random.normal(0, 0.1, self.dim)\n                child += mutation_vector\n                child = np.clip(child, self.lower_bound, self.upper_bound)\n                \n                child_fitness = func(child)\n                evaluations += 1\n                \n                # Replace worst in population if child is better\n                worst_idx = np.argmax(fitness)\n                if child_fitness < fitness[worst_idx]:\n                    population[worst_idx] = child\n                    fitness[worst_idx] = child_fitness\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]\n    \n    def estimate_gradient(self, func, x, fx, step_size):\n        grad = np.zeros(self.dim)\n        for i in range(self.dim):\n            x_plus = np.array(x)\n            x_plus[i] += step_size\n            grad[i] = (func(x_plus) - fx) / step_size\n        return grad", "name": "EvolutionaryGradientSearch", "description": "Evolutionary Gradient Search (EGS) combines gradient-based refinement with evolutionary exploration for efficient optimization.", "configspace": "", "generation": 0, "fitness": 0.11796594770197577, "feedback": "", "error": "", "parent_id": null, "metadata": {"aucs": [0.4558393258243778, 0.4558393258243778, 0.4558393258243778, 0.4077948206485026, 0.4077948206485026, 0.4077948206485026, 0.4369001782530133, 0.4369001782530133, 0.4369001782530133, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.04392363021188461, 0.04392363021188461, 0.04392363021188461, 0.023264068281378947, 0.023264068281378947, 0.023264068281378947, 0.018924089410861766, 0.018924089410861766, 0.018924089410861766, 0.001502744752593932, 0.001502744752593932, 0.001502744752593932, 0.012211248092099614, 0.012211248092099614, 0.012211248092099614, 0.0028868345583047406, 0.0028868345583047406, 0.0028868345583047406, 0.8464450483531298, 0.8464450483531298, 0.8464450483531298, 0.7721120266655545, 0.7721120266655545, 0.7721120266655545, 0.7775494983259195, 0.7775494983259195, 0.7775494983259195, 0.11150209677262224, 0.11150209677262224, 0.11150209677262224, 0.10864933875948768, 0.10864933875948768, 0.10864933875948768, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.06910747741452794, 0.06910747741452794, 0.06910747741452794, 0.08168107019036619, 0.08168107019036619, 0.08168107019036619, 0.025591964285575197, 0.025591964285575197, 0.025591964285575197, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.04609344090653289, 0.04609344090653289, 0.04609344090653289, 0.016734448157654902, 0.016734448157654902, 0.016734448157654902, 0.03751318418901628, 0.03751318418901628, 0.03751318418901628, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.33595690032065495, 0.33595690032065495, 0.33595690032065495, 0.32687116181380815, 0.32687116181380815, 0.32687116181380815, 0.3201843818864458, 0.3201843818864458, 0.3201843818864458, 0.028407476522943176, 0.028407476522943176, 0.028407476522943176, 0.015145889240966937, 0.015145889240966937, 0.015145889240966937, 0.035502855895028884, 0.035502855895028884, 0.035502855895028884, 0.14985502622022195, 0.14985502622022195, 0.14985502622022195, 0.07872700132791977, 0.07872700132791977, 0.07872700132791977, 0.10408423573730075, 0.10408423573730075, 0.10408423573730075, 0.15458285722263176, 0.15458285722263176, 0.15458285722263176, 0.16297440134425123, 0.16297440134425123, 0.16297440134425123, 0.13957369636393746, 0.13957369636393746, 0.13957369636393746, 0.05760248693648151, 0.05760248693648151, 0.05760248693648151, 0.06059688725349788, 0.06059688725349788, 0.06059688725349788, 0.09293065478701978, 0.09293065478701978, 0.09293065478701978, 0.11997341643698545, 0.11997341643698545, 0.11997341643698545, 0.12450523296988525, 0.12450523296988525, 0.12450523296988525, 0.13136861466886107, 0.13136861466886107, 0.13136861466886107, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.08410095421871266, 0.08410095421871266, 0.08410095421871266, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.16739178083962147, 0.16739178083962147, 0.16739178083962147, 0.19967099215270323, 0.19967099215270323, 0.19967099215270323, 0.1578645902551018, 0.1578645902551018, 0.1578645902551018, 0.17005727822519967, 0.17005727822519967, 0.17005727822519967, 0.1760285816124546, 0.1760285816124546, 0.1760285816124546, 0.20997494002881745, 0.20997494002881745, 0.20997494002881745, 0.15379211352217148, 0.15379211352217148, 0.15379211352217148, 0.16041351420838001, 0.16041351420838001, 0.16041351420838001, 0.15475201731047805, 0.15475201731047805, 0.15475201731047805, 0.0346742611023948, 0.0346742611023948, 0.0346742611023948, 0.037460325169347386, 0.037460325169347386, 0.037460325169347386, 0.050197174894627494, 0.050197174894627494, 0.050197174894627494]}, "mutation_prompt": null}
{"id": "d4847e2c-dab2-44e9-b377-c974977adf74", "solution": "import numpy as np\n\nclass EvolutionaryGradientSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        \n    def __call__(self, func):\n        np.random.seed(42)\n        population_size = 20\n        step_size = 0.1\n        \n        population = np.random.uniform(self.lower_bound, self.upper_bound, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = population_size\n        \n        while evaluations < self.budget:\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n                # Gradient-based local search\n                grad = self.estimate_gradient(func, population[i], func(population[i]), step_size)\n                new_solution = population[i] - step_size * grad\n                new_solution = np.clip(new_solution, self.lower_bound, self.upper_bound)\n                new_fitness = func(new_solution)\n                evaluations += 1\n                \n                # Replace if new solution is better\n                if new_fitness < fitness[i]:\n                    population[i] = new_solution\n                    fitness[i] = new_fitness\n            \n            # Evolutionary step: crossover and mutation\n            if evaluations < self.budget:\n                parents_indices = np.random.choice(population_size, size=2, replace=False)\n                parent1, parent2 = population[parents_indices]\n                \n                crossover_point = np.random.randint(1, self.dim)\n                child = np.concatenate((parent1[:crossover_point], parent2[crossover_point:]))\n                \n                # Mutation\n                mutation_vector = np.random.normal(0, 0.1, self.dim)\n                child += mutation_vector\n                child = np.clip(child, self.lower_bound, self.upper_bound)\n                \n                child_fitness = func(child)\n                evaluations += 1\n                \n                # Replace worst in population if child is better\n                worst_idx = np.argmax(fitness)\n                if child_fitness < fitness[worst_idx]:\n                    population[worst_idx] = child\n                    fitness[worst_idx] = child_fitness\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]\n    \n    def estimate_gradient(self, func, x, fx, step_size):\n        grad = np.zeros(self.dim)\n        for i in range(self.dim):\n            x_plus = np.array(x)\n            x_plus[i] += step_size\n            grad[i] = (func(x_plus) - fx) / step_size\n        return grad", "name": "EvolutionaryGradientSearch", "description": "Evolutionary Gradient Search (EGS) combines gradient-based refinement with evolutionary exploration for efficient optimization.", "configspace": "", "generation": 1, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "898f0209-2d25-4550-a127-503a4f955d5b", "metadata": {"aucs": [0.4558393258243778, 0.4558393258243778, 0.4558393258243778, 0.4077948206485026, 0.4077948206485026, 0.4077948206485026, 0.4369001782530133, 0.4369001782530133, 0.4369001782530133, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.04392363021188461, 0.04392363021188461, 0.04392363021188461, 0.023264068281378947, 0.023264068281378947, 0.023264068281378947, 0.018924089410861766, 0.018924089410861766, 0.018924089410861766, 0.001502744752593932, 0.001502744752593932, 0.001502744752593932, 0.012211248092099614, 0.012211248092099614, 0.012211248092099614, 0.0028868345583047406, 0.0028868345583047406, 0.0028868345583047406, 0.8464450483531298, 0.8464450483531298, 0.8464450483531298, 0.7721120266655545, 0.7721120266655545, 0.7721120266655545, 0.7775494983259195, 0.7775494983259195, 0.7775494983259195, 0.11150209677262224, 0.11150209677262224, 0.11150209677262224, 0.10864933875948768, 0.10864933875948768, 0.10864933875948768, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.06910747741452794, 0.06910747741452794, 0.06910747741452794, 0.08168107019036619, 0.08168107019036619, 0.08168107019036619, 0.025591964285575197, 0.025591964285575197, 0.025591964285575197, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.04609344090653289, 0.04609344090653289, 0.04609344090653289, 0.016734448157654902, 0.016734448157654902, 0.016734448157654902, 0.03751318418901628, 0.03751318418901628, 0.03751318418901628, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.33595690032065495, 0.33595690032065495, 0.33595690032065495, 0.32687116181380815, 0.32687116181380815, 0.32687116181380815, 0.3201843818864458, 0.3201843818864458, 0.3201843818864458, 0.028407476522943176, 0.028407476522943176, 0.028407476522943176, 0.015145889240966937, 0.015145889240966937, 0.015145889240966937, 0.035502855895028884, 0.035502855895028884, 0.035502855895028884, 0.14985502622022195, 0.14985502622022195, 0.14985502622022195, 0.07872700132791977, 0.07872700132791977, 0.07872700132791977, 0.10408423573730075, 0.10408423573730075, 0.10408423573730075, 0.15458285722263176, 0.15458285722263176, 0.15458285722263176, 0.16297440134425123, 0.16297440134425123, 0.16297440134425123, 0.13957369636393746, 0.13957369636393746, 0.13957369636393746, 0.05760248693648151, 0.05760248693648151, 0.05760248693648151, 0.06059688725349788, 0.06059688725349788, 0.06059688725349788, 0.09293065478701978, 0.09293065478701978, 0.09293065478701978, 0.11997341643698545, 0.11997341643698545, 0.11997341643698545, 0.12450523296988525, 0.12450523296988525, 0.12450523296988525, 0.13136861466886107, 0.13136861466886107, 0.13136861466886107, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.08410095421871266, 0.08410095421871266, 0.08410095421871266, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.16739178083962147, 0.16739178083962147, 0.16739178083962147, 0.19967099215270323, 0.19967099215270323, 0.19967099215270323, 0.1578645902551018, 0.1578645902551018, 0.1578645902551018, 0.17005727822519967, 0.17005727822519967, 0.17005727822519967, 0.1760285816124546, 0.1760285816124546, 0.1760285816124546, 0.20997494002881745, 0.20997494002881745, 0.20997494002881745, 0.15379211352217148, 0.15379211352217148, 0.15379211352217148, 0.16041351420838001, 0.16041351420838001, 0.16041351420838001, 0.15475201731047805, 0.15475201731047805, 0.15475201731047805, 0.0346742611023948, 0.0346742611023948, 0.0346742611023948, 0.037460325169347386, 0.037460325169347386, 0.037460325169347386, 0.050197174894627494, 0.050197174894627494, 0.050197174894627494]}, "mutation_prompt": null}
