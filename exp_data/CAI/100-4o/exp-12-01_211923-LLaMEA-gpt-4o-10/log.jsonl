{"id": "a9c43e68-ad2a-48a8-a6e8-6fc66e64d0cd", "solution": "import numpy as np\n\nclass ACSGI:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.sigma = 0.3  # Initial step size\n        self.learning_rate = 0.1  # Learning rate for covariance adaptation\n\n    def __call__(self, func):\n        # Initialize variables\n        x_best = np.random.uniform(self.lower_bound, self.upper_bound, self.dim)\n        f_best = func(x_best)\n        eval_count = 1\n        \n        # Covariance matrix initialized to identity\n        cov_matrix = np.eye(self.dim)\n        \n        while eval_count < self.budget:\n            # Sample a set of candidate solutions\n            num_samples = min(10, self.budget - eval_count)  # Adjust number of samples based on remaining budget\n            samples = np.random.multivariate_normal(x_best, self.sigma**2 * cov_matrix, num_samples)\n            samples = np.clip(samples, self.lower_bound, self.upper_bound)\n            \n            # Evaluate all samples\n            f_values = np.array([func(x) for x in samples])\n            eval_count += num_samples\n            \n            # Select the best sample\n            idx_best = np.argmin(f_values)\n            if f_values[idx_best] < f_best:\n                x_best = samples[idx_best]\n                f_best = f_values[idx_best]\n\n            # Calculate approximate gradient\n            gradient = np.mean((samples - x_best) * (f_values[:, np.newaxis] - f_best), axis=0)\n\n            # Update covariance matrix\n            cov_matrix = (1 - self.learning_rate) * cov_matrix + self.learning_rate * np.outer(gradient, gradient)\n            \n            # Adjust step size based on performance\n            self.sigma *= 0.85 if f_values[idx_best] >= f_best else 1.05\n\n        return x_best, f_best", "name": "ACSGI", "description": "Adaptive Covariance Sampling with Gradient Information (ACSGI) integrates adaptive sampling with gradient estimation to efficiently explore and exploit the search space.", "configspace": "", "generation": 0, "fitness": 0.08352106821603555, "feedback": "", "error": "", "parent_id": null, "metadata": {"aucs": [0.1581570220145011, 0.11648953379965132, 0.1429335133515084, 0.061633914158253145, 0.054387390124523205, 0.14742666921438363, 0.18253051302728562, 0.10006768172651115, 0.08057120439061072, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.052963717795878384, 0.09185601855895786, 0.01113105763109179, 0.04550999233498265, 0.051371169989202325, 0.06213515401207259, 0.047531381381353044, 0.05884969745590585, 0.01321020800705286, 0.07672009286008397, 0.06197529277101277, 0.07272050561856014, 0.05423097744099703, 0.052346928132461445, 0.05358385016277356, 0.025655842746850066, 0.005949129323673197, 0.024156027533995594, 0.09556989763868629, 0.991804382828801, 0.13106301574639778, 0.0636408968188229, 0.06192558829695849, 0.06699541224915606, 0.03807238521807199, 0.18509203475848102, 0.07718662266139342, 0.0038785082321186604, 9.999999999998899e-05, 9.999999999998899e-05, 0.012900309545561539, 0.018171033378098556, 0.016631189714978567, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.13604587167230653, 0.1993177428008578, 0.06850641412504133, 0.10416726205126325, 0.04891255637932135, 0.09580467127670578, 0.04725909176210752, 0.017167148652123765, 0.07845583894415287, 0.1296604884289252, 0.0112430008159361, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.017096309084223105, 9.999999999998899e-05, 0.06732791859615883, 0.1673998364605206, 0.12835284025727123, 0.2491100793309713, 0.12716787170259936, 9.999999999998899e-05, 9.999999999998899e-05, 0.019019174335539568, 0.2324016736619935, 0.10694670765543635, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.0310060934455747, 9.999999999998899e-05, 0.05113022692819502, 0.048465723062902755, 0.021027571076034146, 0.013314161556469806, 0.10632961210408087, 0.021653044696842927, 0.011668502212737075, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.025972933465114312, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.045838330499838764, 9.999999999998899e-05, 0.21980144812351854, 0.1421312258043257, 0.1652201090515264, 0.09557929215189376, 0.0932236941911162, 0.11309432287985854, 0.13787925367305265, 0.12392916412621457, 0.334331673031545, 0.09960295021657317, 0.09760006982561442, 0.055100882155622144, 0.059744893814799416, 0.06622155448809941, 0.07526853729014238, 9.999999999998899e-05, 0.05890466450826792, 0.019380709715182798, 0.1238241734031833, 0.20468515255906006, 0.2439179417681746, 0.11444195938164192, 0.14236345828469443, 0.2050040211845191, 0.11139177827487745, 0.0923020652523111, 0.12236367060011621, 0.1440347332448475, 0.09424319598669262, 0.10205299671240775, 0.14405164664960646, 0.0746318241216466, 0.11108726451928941, 0.1515409535156823, 0.07799499982670022, 0.09487532433994716, 0.0670431300725759, 0.03177632302419042, 0.07522011073900625, 0.11891231373097089, 0.08072461501526906, 0.07580000794770292, 0.14987471027978028, 0.07796935712691688, 0.08107770484343346, 0.19771568745535428, 0.17599035311991518, 0.09639537946261623, 0.2056419533379561, 0.19272040070920438, 0.08209582710164065, 0.1750036535039433, 0.16905853407687121, 0.1356191017840067, 0.1432335485709384, 0.14291491908515486, 0.16241451419656183, 0.1303953383888793, 0.15752581773673446, 0.1502489447645079, 0.15412398704189179, 0.1433625734512498, 0.14864294429476255, 0.12226344132034805, 0.12788431723103255, 0.23367111479204827, 0.15893827745874622, 0.07719292862662153, 0.05762428601772407, 0.09924133215539166, 0.10833998940102252, 0.15174782781346496, 0.2725823227493033, 0.08286527149043976, 0.01817249880257732, 0.014889525324099484, 0.07320594883563714, 0.06297455785820805, 0.03021684819758541, 0.026431895971296915, 0.11045605092758082, 0.21386686270979993, 0.21140598553441836, 0.21187943511684604, 0.18210832294187707, 0.16663249237213817, 0.1887499036145407, 0.1857809225903143, 0.23455923059346173, 0.18164319752543523, 0.034473469977346904, 0.07482414887433153, 0.12447615271473811, 0.038286748995449704, 0.015405750854442735, 0.059888048296766305, 0.06383294332473666, 0.01170153162358134, 0.07741933680263557]}, "mutation_prompt": null}
{"id": "951a123c-c262-4e40-88db-f27e4400b95a", "solution": "import numpy as np\n\nclass ACSGI:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.sigma = 0.3  # Initial step size\n        self.learning_rate = 0.1  # Learning rate for covariance adaptation\n\n    def __call__(self, func):\n        # Initialize variables\n        x_best = np.random.uniform(self.lower_bound, self.upper_bound, self.dim)\n        f_best = func(x_best)\n        eval_count = 1\n        \n        # Covariance matrix initialized to identity\n        cov_matrix = np.eye(self.dim)\n        \n        while eval_count < self.budget:\n            # Sample a set of candidate solutions\n            num_samples = min(10, self.budget - eval_count)  # Adjust number of samples based on remaining budget\n            samples = np.random.multivariate_normal(x_best, self.sigma**2 * cov_matrix, num_samples)\n            samples = np.clip(samples, self.lower_bound, self.upper_bound)\n            \n            # Evaluate all samples\n            f_values = np.array([func(x) for x in samples])\n            eval_count += num_samples\n            \n            # Select the best sample\n            idx_best = np.argmin(f_values)\n            if f_values[idx_best] < f_best:\n                x_best = samples[idx_best]\n                f_best = f_values[idx_best]\n\n            # Calculate approximate gradient\n            gradient = np.mean((samples - x_best) * (f_values[:, np.newaxis] - f_best), axis=0)\n\n            # Update covariance matrix\n            cov_matrix = (1 - self.learning_rate) * cov_matrix + self.learning_rate * np.outer(gradient, gradient)\n            \n            # Adjust step size and learning rate based on performance\n            self.sigma *= 0.9 if f_values[idx_best] >= f_best else 1.1\n            self.learning_rate *= 0.9 if f_values[idx_best] >= f_best else 1.1\n\n        return x_best, f_best", "name": "ACSGI", "description": "Optimized ACSGI with adaptive learning rate to enhance exploration and convergence.", "configspace": "", "generation": 1, "fitness": 0.10827207621315071, "feedback": "", "error": "", "parent_id": "a9c43e68-ad2a-48a8-a6e8-6fc66e64d0cd", "metadata": {"aucs": [0.30336434326122164, 0.16538603546232133, 0.21456490387504523, 0.2631245599187527, 0.07969413044096274, 0.22710818817084977, 0.14359082093308684, 0.14118258652337667, 0.2485844922008753, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.060867556935182754, 0.07918513827578555, 0.012579604901466146, 0.04865603676459229, 0.0783710506351355, 0.09589033668765623, 0.03138119469630507, 0.08364339984856395, 0.06108823588315149, 0.11349455390057916, 0.038551112956278666, 0.06503349148829618, 0.060113035087092204, 0.05794308162261097, 0.08188440557031185, 0.04195818787449912, 0.03848543444052266, 0.027436701079055048, 0.07790829632958052, 0.9788520136199884, 0.9929514632393434, 0.1476887695577569, 0.111882540400984, 0.09962656742262499, 0.06356406718087271, 0.9916989134091816, 0.23275717274018048, 0.10580263124044353, 9.999999999998899e-05, 9.999999999998899e-05, 0.012973359134724194, 0.05449080551164576, 0.022864005630751638, 0.017798571394688767, 9.999999999998899e-05, 9.999999999998899e-05, 0.1303604404836235, 0.0788569343525356, 0.1299463541503052, 0.17212148515764514, 0.09566121948430051, 0.11349090752343272, 0.1603642382386743, 0.1432923115240604, 0.10518160638402896, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.19245217139219672, 9.999999999998899e-05, 0.09007223530424713, 0.11749929438654128, 0.10948264250761819, 9.999999999998899e-05, 0.1357163916601496, 0.18337016378419124, 0.033225885704823654, 0.22587347873466002, 0.14880442808487804, 0.12382381290079192, 0.11952420904633398, 0.051292000066788246, 0.2012006501516862, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.0078115744077839055, 0.03459071572640038, 0.026614289710372163, 0.048826891304765696, 9.999999999998899e-05, 0.017316300349924707, 0.09748607003596732, 0.012633789079870783, 0.005716605385986617, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.016737523655932507, 9.999999999998899e-05, 0.0496533397099953, 0.28091257545778103, 9.999999999998899e-05, 9.999999999998899e-05, 0.030922852696502412, 9.999999999998899e-05, 9.999999999998899e-05, 0.25664126717656066, 0.14574820975301883, 0.2145114541256503, 0.11078634455025671, 0.1868229314110743, 0.11902051749032994, 0.1722997596991237, 0.1342761731925204, 0.23447598031864647, 0.06190290325847558, 0.06841989684484828, 9.999999999998899e-05, 0.03584680846352384, 0.09554843598218488, 0.06843032175469033, 0.013773364390386789, 0.04751397546397351, 0.06259115014741112, 0.11261330275977743, 0.13376307737110238, 0.2647228041695787, 0.12933921720194308, 0.18269269922089992, 0.13853497909874768, 0.23438958022148415, 0.1295145008619305, 0.12187237273833595, 0.15020027110556788, 0.10254556319549901, 0.1091033603166921, 0.1412561461694407, 0.08722567802371362, 0.10265237149435869, 0.13844587758764826, 0.0766983906114036, 0.09495085941913184, 0.08680217210089014, 0.03830968210779795, 0.06123313695054444, 0.12157281443393875, 0.11725586841704261, 0.08969396803380747, 0.07076122282625219, 0.13177789403276652, 0.056557158253239814, 0.2501537229927566, 0.19564894206562722, 0.09605081157163164, 0.21385085188723285, 0.21561049676246036, 0.12446761619317714, 0.22693620849713514, 0.2205164347554529, 0.1371550674389349, 0.15616442961492394, 0.14003046723734425, 0.14900183451487226, 0.16697758891238568, 0.14694340151414764, 0.17178869819444098, 0.16946697465971372, 0.1670232017149892, 0.16260064141110997, 0.1233855497638795, 0.13302274244977574, 0.0571080004978225, 0.17407064367461822, 0.11194392753999638, 0.06107158417226222, 0.10799297384602635, 0.11506178769836406, 0.13911389305533717, 0.40817597281565954, 0.09240198929036503, 0.07950639744456622, 0.015369824845477664, 0.09949640029152762, 0.16704122780397157, 0.178625435528052, 0.04690459922257817, 0.15464491341547926, 0.20777674328337403, 0.23866869339373364, 0.20452659273379192, 0.2661534355246875, 0.15625160251883696, 0.1950488545070188, 0.21607287768936878, 0.16694072598232268, 0.26194005044445023, 0.07323049934782377, 0.07183128948739392, 0.051705480033251816, 0.05591139786839361, 0.04168318878133481, 0.06006654894900143, 0.07642686534592302, 0.04570166232714601, 0.09925505299125348]}, "mutation_prompt": null}
