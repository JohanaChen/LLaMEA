{"id": "75c582b6-0659-40b4-808f-ab9f0b357dcf", "solution": "import numpy as np\n\nclass AASOOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_agents = 5 + int(0.1 * self.dim)\n        exploration_factor = 0.9\n        exploitation_factor = 0.1\n        inertia_weight = 0.5\n        alpha = 0.5  # Coefficient for dynamic adaptation\n\n        # Initialize position and velocity for each agent\n        lb, ub = func.bounds.lb, func.bounds.ub\n        positions = np.random.uniform(lb, ub, (num_agents, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_agents, self.dim))\n\n        # Evaluate initial positions\n        fitness = np.apply_along_axis(func, 1, positions)\n        self.eval_count += num_agents\n\n        best_agent_idx = np.argmin(fitness)\n        global_best_position = positions[best_agent_idx].copy()\n        global_best_value = fitness[best_agent_idx]\n\n        personal_best_positions = positions.copy()\n        personal_best_values = fitness.copy()\n\n        while self.eval_count < self.budget:\n            for i in range(num_agents):\n                # Update velocity\n                r1, r2 = np.random.rand(2)\n                cognitive_component = exploration_factor * r1 * (personal_best_positions[i] - positions[i])\n                social_component = exploitation_factor * r2 * (global_best_position - positions[i])\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n\n                # Update position\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], lb, ub)  # ensure within bounds\n\n                # Evaluate current position\n                current_fitness = func(positions[i])\n                self.eval_count += 1\n\n                # Update personal bests\n                if current_fitness < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_fitness\n\n                # Update global best\n                if current_fitness < global_best_value:\n                    global_best_position = positions[i]\n                    global_best_value = current_fitness\n\n                if self.eval_count >= self.budget:\n                    break\n\n            # Dynamic adjustment of exploration and exploitation\n            exploration_factor = alpha * (1 - self.eval_count / self.budget)\n            exploitation_factor = 1 - exploration_factor\n            inertia_weight = 0.5 + 0.5 * (self.eval_count / self.budget)\n\n        return global_best_position", "name": "AASOOptimizer", "description": "Novel Adaptive Agent Swarm Optimization (AASO) algorithm that dynamically adjusts exploration and exploitation using agent-based strategies for enhanced global search efficiency.", "configspace": "", "generation": 0, "fitness": 0.07711896430691671, "feedback": "The algorithm AASOOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.077 with standard deviation 0.004. And the mean value of best solutions found was 0.239 (0. is the best).", "error": "", "parent_id": null, "metadata": {"aucs": [0.07197251071500999, 0.07746884877817362, 0.08191553342756652], "final_y": [0.26558772250199647, 0.23187378989766405, 0.22083115312310664]}, "mutation_prompt": null}
{"id": "6b72a79f-dfbf-4da1-96bd-8b7d8b6a8f22", "solution": "import numpy as np\n\nclass EDASOOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_agents = 5 + int(0.1 * self.dim)\n        exploration_factor = 0.9\n        exploitation_factor = 0.1\n        inertia_weight = 0.5\n        alpha = 0.5  # Coefficient for dynamic adaptation\n\n        # Initialize position and velocity for each agent\n        lb, ub = func.bounds.lb, func.bounds.ub\n        positions = np.random.uniform(lb, ub, (num_agents, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_agents, self.dim))\n\n        # Evaluate initial positions\n        fitness = np.apply_along_axis(func, 1, positions)\n        self.eval_count += num_agents\n\n        best_agent_idx = np.argmin(fitness)\n        global_best_position = positions[best_agent_idx].copy()\n        global_best_value = fitness[best_agent_idx]\n\n        personal_best_positions = positions.copy()\n        personal_best_values = fitness.copy()\n\n        while self.eval_count < self.budget:\n            for i in range(num_agents):\n                # Update velocity\n                r1, r2 = np.random.rand(2)\n                cognitive_component = exploration_factor * r1 * (personal_best_positions[i] - positions[i])\n                social_component = exploitation_factor * r2 * (global_best_position - positions[i])\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n\n                # Introduce opposition-based learning\n                opposite_position = lb + ub - positions[i]\n                opposite_fitness = func(opposite_position)\n                self.eval_count += 1\n\n                if opposite_fitness < personal_best_values[i]:\n                    positions[i] = opposite_position\n                    personal_best_values[i] = opposite_fitness\n\n                # Update position\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], lb, ub)  # ensure within bounds\n\n                # Evaluate current position\n                current_fitness = func(positions[i])\n                self.eval_count += 1\n\n                # Update personal bests\n                if current_fitness < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_fitness\n\n                # Update global best\n                if current_fitness < global_best_value:\n                    global_best_position = positions[i]\n                    global_best_value = current_fitness\n\n                if self.eval_count >= self.budget:\n                    break\n\n            # Adaptive parameters\n            exploration_factor = alpha * (1 - self.eval_count / self.budget)\n            exploitation_factor = 1 - exploration_factor\n            inertia_weight = 0.4 + 0.5 * (self.eval_count / self.budget)\n\n        return global_best_position", "name": "EDASOOptimizer", "description": "Enhanced Dynamic Agent Swarm Optimization (EDASO) using adaptive inertia and opposition-based learning for improved convergence and diversity.", "configspace": "", "generation": 1, "fitness": 0.08893753594197717, "feedback": "The algorithm EDASOOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.089 with standard deviation 0.001. And the mean value of best solutions found was 0.194 (0. is the best).", "error": "", "parent_id": "75c582b6-0659-40b4-808f-ab9f0b357dcf", "metadata": {"aucs": [0.08779471157257901, 0.08969911936924213, 0.08931877688411038], "final_y": [0.19803880724952605, 0.1904732566894507, 0.19231754738291607]}, "mutation_prompt": null}
{"id": "0a5837af-cdbc-4534-a5d8-372ced4c82ae", "solution": "import numpy as np\n\nclass EDASOOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_agents = 5 + int(0.1 * self.dim)\n        exploration_factor = 0.9\n        exploitation_factor = 0.1\n        inertia_weight = 0.5\n        alpha = 0.5  # Coefficient for dynamic adaptation\n\n        # Initialize position and velocity for each agent\n        lb, ub = func.bounds.lb, func.bounds.ub\n        positions = np.random.uniform(lb, ub, (num_agents, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_agents, self.dim))\n\n        # Evaluate initial positions\n        fitness = np.apply_along_axis(func, 1, positions)\n        self.eval_count += num_agents\n\n        best_agent_idx = np.argmin(fitness)\n        global_best_position = positions[best_agent_idx].copy()\n        global_best_value = fitness[best_agent_idx]\n\n        personal_best_positions = positions.copy()\n        personal_best_values = fitness.copy()\n\n        while self.eval_count < self.budget:\n            for i in range(num_agents):\n                # Update velocity\n                r1, r2 = np.random.rand(2)\n                cognitive_component = exploration_factor * r1 * (personal_best_positions[i] - positions[i])\n                social_component = exploitation_factor * r2 * (global_best_position - positions[i])\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n\n                # Introduce opposition-based learning\n                opposite_position = lb + ub - positions[i]\n                opposite_fitness = func(opposite_position)\n                self.eval_count += 1\n\n                if opposite_fitness < personal_best_values[i]:\n                    positions[i] = opposite_position\n                    personal_best_values[i] = opposite_fitness\n\n                # Update position\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], lb, ub)  # ensure within bounds\n\n                # Evaluate current position\n                current_fitness = func(positions[i])\n                self.eval_count += 1\n\n                # Update personal bests\n                if current_fitness < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_fitness\n\n                # Update global best\n                if current_fitness < global_best_value:\n                    global_best_position = positions[i]\n                    global_best_value = current_fitness\n\n                if self.eval_count >= self.budget:\n                    break\n\n            # Adaptive parameters\n            exploration_factor = alpha * (1 - self.eval_count / self.budget)\n            exploitation_factor = 1 - exploration_factor\n            inertia_weight = 0.9 - 0.5 * (self.eval_count / self.budget)  # Adjusted formula\n\n        return global_best_position", "name": "EDASOOptimizer", "description": "Enhanced EDASOOptimizer by adjusting the inertia weight formula for improved convergence dynamics.", "configspace": "", "generation": 2, "fitness": 0.09128743740098506, "feedback": "The algorithm EDASOOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.091 with standard deviation 0.003. And the mean value of best solutions found was 0.184 (0. is the best).", "error": "", "parent_id": "6b72a79f-dfbf-4da1-96bd-8b7d8b6a8f22", "metadata": {"aucs": [0.08686832265606603, 0.09423201607004739, 0.09276197347684179], "final_y": [0.20013030292235257, 0.17340277776490065, 0.1790153701538617]}, "mutation_prompt": null}
{"id": "4962f0ee-4a84-4dba-8369-ccc76cf59e87", "solution": "import numpy as np\n\nclass ImprovedEDASOOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_agents = 5 + int(0.1 * self.dim)\n        exploration_factor = 0.9\n        exploitation_factor = 0.1\n        inertia_weight_min = 0.4\n        inertia_weight_max = 0.9\n        alpha = 0.5  # Coefficient for dynamic adaptation\n\n        # Initialize position and velocity for each agent\n        lb, ub = func.bounds.lb, func.bounds.ub\n        positions = np.random.uniform(lb, ub, (num_agents, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_agents, self.dim))\n\n        # Evaluate initial positions\n        fitness = np.apply_along_axis(func, 1, positions)\n        self.eval_count += num_agents\n\n        best_agent_idx = np.argmin(fitness)\n        global_best_position = positions[best_agent_idx].copy()\n        global_best_value = fitness[best_agent_idx]\n\n        personal_best_positions = positions.copy()\n        personal_best_values = fitness.copy()\n\n        while self.eval_count < self.budget:\n            # Update inertia weight dynamically\n            inertia_weight = inertia_weight_max - (inertia_weight_max - inertia_weight_min) * (self.eval_count / self.budget)\n\n            for i in range(num_agents):\n                # Update velocity\n                r1, r2 = np.random.rand(2)\n                cognitive_component = exploration_factor * r1 * (personal_best_positions[i] - positions[i])\n                social_component = exploitation_factor * r2 * (global_best_position - positions[i])\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n\n                # Introduce opposition-based learning\n                opposite_position = lb + ub - positions[i]\n                opposite_fitness = func(opposite_position)\n                self.eval_count += 1\n\n                if opposite_fitness < personal_best_values[i]:\n                    positions[i] = opposite_position\n                    personal_best_values[i] = opposite_fitness\n\n                # Update position\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], lb, ub)  # ensure within bounds\n\n                # Evaluate current position\n                current_fitness = func(positions[i])\n                self.eval_count += 1\n\n                # Update personal bests\n                if current_fitness < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_fitness\n\n                # Update global best\n                if current_fitness < global_best_value:\n                    global_best_position = positions[i]\n                    global_best_value = current_fitness\n\n                if self.eval_count >= self.budget:\n                    break\n\n            # Cooperative learning using average of personal bests\n            average_personal_best = np.mean(personal_best_positions, axis=0)\n            for i in range(num_agents):\n                if np.random.rand() < 0.1:  # 10% chance to use the average strategy\n                    r3 = np.random.rand()\n                    positions[i] = r3 * average_personal_best + (1 - r3) * positions[i]\n                    positions[i] = np.clip(positions[i], lb, ub)\n                    current_fitness = func(positions[i])\n                    self.eval_count += 1\n                    if current_fitness < personal_best_values[i]:\n                        personal_best_positions[i] = positions[i]\n                        personal_best_values[i] = current_fitness\n                    if current_fitness < global_best_value:\n                        global_best_position = positions[i]\n                        global_best_value = current_fitness\n\n            # Adaptive parameters\n            exploration_factor = alpha * (1 - self.eval_count / self.budget)\n            exploitation_factor = 1 - exploration_factor\n\n        return global_best_position", "name": "ImprovedEDASOOptimizer", "description": "Introduce a self-adaptive inertia weight and cooperative learning strategy to EDASOOptimizer for enhanced convergence and diversity management.", "configspace": "", "generation": 3, "fitness": 0.09269875381592167, "feedback": "The algorithm ImprovedEDASOOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.093 with standard deviation 0.001. And the mean value of best solutions found was 0.180 (0. is the best).", "error": "", "parent_id": "0a5837af-cdbc-4534-a5d8-372ced4c82ae", "metadata": {"aucs": [0.09348406741828663, 0.09061053101909589, 0.09400166301038249], "final_y": [0.17710774714723443, 0.18723097536975775, 0.17541143361590894]}, "mutation_prompt": null}
{"id": "e2226140-f426-4d6b-b48a-b01ff98cbd1e", "solution": "import numpy as np\n\nclass ImprovedEDASOOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_agents = 5 + int(0.1 * self.dim)\n        exploration_factor = 0.9\n        exploitation_factor = 0.1\n        inertia_weight_min = 0.4\n        inertia_weight_max = 0.9\n        alpha = 0.5  # Coefficient for dynamic adaptation\n\n        # Initialize position and velocity for each agent\n        lb, ub = func.bounds.lb, func.bounds.ub\n        positions = np.random.uniform(lb, ub, (num_agents, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_agents, self.dim))\n\n        # Evaluate initial positions\n        fitness = np.apply_along_axis(func, 1, positions)\n        self.eval_count += num_agents\n\n        best_agent_idx = np.argmin(fitness)\n        global_best_position = positions[best_agent_idx].copy()\n        global_best_value = fitness[best_agent_idx]\n\n        personal_best_positions = positions.copy()\n        personal_best_values = fitness.copy()\n\n        while self.eval_count < self.budget:\n            # Update inertia weight dynamically\n            inertia_weight = inertia_weight_max - (inertia_weight_max - inertia_weight_min) * ((self.eval_count + 1) / self.budget)  # Changed line\n\n            for i in range(num_agents):\n                # Update velocity\n                r1, r2 = np.random.rand(2)\n                cognitive_component = exploration_factor * r1 * (personal_best_positions[i] - positions[i])\n                social_component = exploitation_factor * r2 * (global_best_position - positions[i])\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n\n                # Introduce opposition-based learning\n                opposite_position = lb + ub - positions[i]\n                opposite_fitness = func(opposite_position)\n                self.eval_count += 1\n\n                if opposite_fitness < personal_best_values[i]:\n                    positions[i] = opposite_position\n                    personal_best_values[i] = opposite_fitness\n\n                # Update position\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], lb, ub)  # ensure within bounds\n\n                # Evaluate current position\n                current_fitness = func(positions[i])\n                self.eval_count += 1\n\n                # Update personal bests\n                if current_fitness < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_fitness\n\n                # Update global best\n                if current_fitness < global_best_value:\n                    global_best_position = positions[i]\n                    global_best_value = current_fitness\n\n                if self.eval_count >= self.budget:\n                    break\n\n            # Cooperative learning using average of personal bests\n            average_personal_best = np.mean(personal_best_positions, axis=0)\n            for i in range(num_agents):\n                if np.random.rand() < 0.1:  # 10% chance to use the average strategy\n                    r3 = np.random.rand()\n                    positions[i] = r3 * average_personal_best + (1 - r3) * positions[i]\n                    positions[i] = np.clip(positions[i], lb, ub)\n                    current_fitness = func(positions[i])\n                    self.eval_count += 1\n                    if current_fitness < personal_best_values[i]:\n                        personal_best_positions[i] = positions[i]\n                        personal_best_values[i] = current_fitness\n                    if current_fitness < global_best_value:\n                        global_best_position = positions[i]\n                        global_best_value = current_fitness\n\n            # Adaptive parameters\n            exploration_factor = alpha * (1 - self.eval_count / self.budget)\n            exploitation_factor = 1 - exploration_factor\n\n        return global_best_position", "name": "ImprovedEDASOOptimizer", "description": "Introduce a slight adjustment to inertia weight calculation for improved exploration-exploitation balance.", "configspace": "", "generation": 4, "fitness": 0.09279742658092753, "feedback": "The algorithm ImprovedEDASOOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.093 with standard deviation 0.001. And the mean value of best solutions found was 0.180 (0. is the best).", "error": "", "parent_id": "4962f0ee-4a84-4dba-8369-ccc76cf59e87", "metadata": {"aucs": [0.09346939516522512, 0.09092235498371248, 0.09400052959384497], "final_y": [0.1771756742606807, 0.18595524706029665, 0.1754149228323284]}, "mutation_prompt": null}
{"id": "252c45b0-efb8-4ed2-876e-e2da27fae50f", "solution": "import numpy as np\n\nclass ImprovedEDASOOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_agents = 5 + int(0.1 * self.dim)\n        exploration_factor = 0.9\n        exploitation_factor = 0.1\n        inertia_weight_min = 0.4\n        inertia_weight_max = 0.9\n        alpha = 0.5  # Coefficient for dynamic adaptation\n\n        # Initialize position and velocity for each agent\n        lb, ub = func.bounds.lb, func.bounds.ub\n        positions = np.random.uniform(lb, ub, (num_agents, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_agents, self.dim))\n\n        # Evaluate initial positions\n        fitness = np.apply_along_axis(func, 1, positions)\n        self.eval_count += num_agents\n\n        best_agent_idx = np.argmin(fitness)\n        global_best_position = positions[best_agent_idx].copy()\n        global_best_value = fitness[best_agent_idx]\n\n        personal_best_positions = positions.copy()\n        personal_best_values = fitness.copy()\n\n        while self.eval_count < self.budget:\n            # Update inertia weight dynamically\n            inertia_weight = inertia_weight_max - (inertia_weight_max - inertia_weight_min) * ((self.eval_count + 1) / self.budget)  # Changed line\n\n            for i in range(num_agents):\n                # Update velocity\n                r1, r2 = np.random.rand(2)\n                cognitive_component = exploration_factor * r1 * (personal_best_positions[i] - positions[i])\n                social_component = exploitation_factor * r2 * (global_best_position - positions[i])\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n\n                # Introduce enhanced opposition-based learning\n                opposite_position = lb + ub - positions[i]\n                opposite_velocity = -velocities[i]  # Update velocity as well\n                opposite_fitness = func(opposite_position)\n                self.eval_count += 1\n\n                if opposite_fitness < personal_best_values[i]:\n                    positions[i] = opposite_position\n                    velocities[i] = opposite_velocity  # Apply velocity update\n                    personal_best_values[i] = opposite_fitness\n\n                # Update position\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], lb, ub)  # ensure within bounds\n\n                # Evaluate current position\n                current_fitness = func(positions[i])\n                self.eval_count += 1\n\n                # Update personal bests\n                if current_fitness < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_fitness\n\n                # Update global best\n                if current_fitness < global_best_value:\n                    global_best_position = positions[i]\n                    global_best_value = current_fitness\n\n                if self.eval_count >= self.budget:\n                    break\n\n            # Cooperative learning using average of personal bests\n            average_personal_best = np.mean(personal_best_positions, axis=0)\n            for i in range(num_agents):\n                if np.random.rand() < 0.1:  # 10% chance to use the average strategy\n                    r3 = np.random.rand()\n                    positions[i] = r3 * average_personal_best + (1 - r3) * positions[i]\n                    positions[i] = np.clip(positions[i], lb, ub)\n                    current_fitness = func(positions[i])\n                    self.eval_count += 1\n                    if current_fitness < personal_best_values[i]:\n                        personal_best_positions[i] = positions[i]\n                        personal_best_values[i] = current_fitness\n                    if current_fitness < global_best_value:\n                        global_best_position = positions[i]\n                        global_best_value = current_fitness\n\n            # Adaptive parameters\n            exploration_factor = alpha * (1 - self.eval_count / self.budget)\n            exploitation_factor = 1 - exploration_factor\n\n        return global_best_position", "name": "ImprovedEDASOOptimizer", "description": "Implement enhanced opposition-based learning by updating both position and velocity to improve convergence speed.", "configspace": "", "generation": 5, "fitness": 0.09118326268054895, "feedback": "The algorithm ImprovedEDASOOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.091 with standard deviation 0.003. And the mean value of best solutions found was 0.185 (0. is the best).", "error": "", "parent_id": "e2226140-f426-4d6b-b48a-b01ff98cbd1e", "metadata": {"aucs": [0.08705623886926128, 0.09442941152248763, 0.09206413764989796], "final_y": [0.19957509712154187, 0.17438272734046312, 0.18221634459528047]}, "mutation_prompt": null}
{"id": "f9f0af87-034d-4614-bbee-9113913745bf", "solution": "import numpy as np\n\nclass ImprovedEDASOOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_agents = 5 + int(0.1 * self.dim)\n        exploration_factor = 0.9\n        exploitation_factor = 0.1\n        inertia_weight_min = 0.4\n        inertia_weight_max = 0.9\n        alpha = 0.5  # Coefficient for dynamic adaptation\n\n        # Initialize position and velocity for each agent\n        lb, ub = func.bounds.lb, func.bounds.ub\n        positions = np.random.uniform(lb, ub, (num_agents, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_agents, self.dim))\n\n        # Evaluate initial positions\n        fitness = np.apply_along_axis(func, 1, positions)\n        self.eval_count += num_agents\n\n        best_agent_idx = np.argmin(fitness)\n        global_best_position = positions[best_agent_idx].copy()\n        global_best_value = fitness[best_agent_idx]\n\n        personal_best_positions = positions.copy()\n        personal_best_values = fitness.copy()\n\n        while self.eval_count < self.budget:\n            # Update inertia weight dynamically\n            inertia_weight = inertia_weight_max - (inertia_weight_max - inertia_weight_min) * ((self.eval_count + 1) / self.budget)  # Changed line\n\n            for i in range(num_agents):\n                # Update velocity\n                r1, r2 = np.random.rand(2)\n                cognitive_component = exploration_factor * r1 * (personal_best_positions[i] - positions[i])\n                social_component = exploitation_factor * r2 * (global_best_position - positions[i])\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n\n                # Introduce adaptive velocity scaling\n                velocity_scaling = 1 - self.eval_count / self.budget  # New line\n                velocities[i] *= velocity_scaling  # New line\n\n                # Introduce opposition-based learning\n                opposite_position = lb + ub - positions[i]\n                opposite_fitness = func(opposite_position)\n                self.eval_count += 1\n\n                if opposite_fitness < personal_best_values[i]:\n                    positions[i] = opposite_position\n                    personal_best_values[i] = opposite_fitness\n\n                # Update position\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], lb, ub)  # ensure within bounds\n\n                # Evaluate current position\n                current_fitness = func(positions[i])\n                self.eval_count += 1\n\n                # Update personal bests\n                if current_fitness < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_fitness\n\n                # Update global best\n                if current_fitness < global_best_value:\n                    global_best_position = positions[i]\n                    global_best_value = current_fitness\n\n                if self.eval_count >= self.budget:\n                    break\n\n            # Cooperative learning using average of personal bests\n            average_personal_best = np.mean(personal_best_positions, axis=0)\n            for i in range(num_agents):\n                if np.random.rand() < 0.1:  # 10% chance to use the average strategy\n                    r3 = np.random.rand()\n                    positions[i] = r3 * average_personal_best + (1 - r3) * positions[i]\n                    positions[i] = np.clip(positions[i], lb, ub)\n                    current_fitness = func(positions[i])\n                    self.eval_count += 1\n                    if current_fitness < personal_best_values[i]:\n                        personal_best_positions[i] = positions[i]\n                        personal_best_values[i] = current_fitness\n                    if current_fitness < global_best_value:\n                        global_best_position = positions[i]\n                        global_best_value = current_fitness\n\n            # Adaptive parameters\n            exploration_factor = alpha * (1 - self.eval_count / self.budget)\n            exploitation_factor = 1 - exploration_factor\n\n        return global_best_position", "name": "ImprovedEDASOOptimizer", "description": "Introduce adaptive velocity scaling for enhanced exploitation while maintaining exploration robustness.", "configspace": "", "generation": 6, "fitness": 0.09119945678136514, "feedback": "The algorithm ImprovedEDASOOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.091 with standard deviation 0.002. And the mean value of best solutions found was 0.185 (0. is the best).", "error": "", "parent_id": "e2226140-f426-4d6b-b48a-b01ff98cbd1e", "metadata": {"aucs": [0.09126937900616561, 0.09348827718843444, 0.08884071414949535], "final_y": [0.185242585191967, 0.1772015191512275, 0.19279240464941638]}, "mutation_prompt": null}
{"id": "3bdc0570-63ef-4246-8a56-84cffafc5d45", "solution": "import numpy as np\n\nclass ImprovedEDASOOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_agents = 5 + int(0.1 * self.dim)\n        exploration_factor = 0.85  # Changed line\n        exploitation_factor = 0.1\n        inertia_weight_min = 0.4\n        inertia_weight_max = 0.9\n        alpha = 0.5  # Coefficient for dynamic adaptation\n\n        # Initialize position and velocity for each agent\n        lb, ub = func.bounds.lb, func.bounds.ub\n        positions = np.random.uniform(lb, ub, (num_agents, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_agents, self.dim))\n\n        # Evaluate initial positions\n        fitness = np.apply_along_axis(func, 1, positions)\n        self.eval_count += num_agents\n\n        best_agent_idx = np.argmin(fitness)\n        global_best_position = positions[best_agent_idx].copy()\n        global_best_value = fitness[best_agent_idx]\n\n        personal_best_positions = positions.copy()\n        personal_best_values = fitness.copy()\n\n        while self.eval_count < self.budget:\n            # Update inertia weight dynamically\n            inertia_weight = inertia_weight_max - (inertia_weight_max - inertia_weight_min) * ((self.eval_count + 1) / self.budget)  # Changed line\n\n            for i in range(num_agents):\n                # Update velocity\n                r1, r2 = np.random.rand(2)\n                cognitive_component = exploration_factor * r1 * (personal_best_positions[i] - positions[i])\n                social_component = exploitation_factor * r2 * (global_best_position - positions[i])\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n\n                # Introduce opposition-based learning\n                opposite_position = lb + ub - positions[i]\n                opposite_fitness = func(opposite_position)\n                self.eval_count += 1\n\n                if opposite_fitness < personal_best_values[i]:\n                    positions[i] = opposite_position\n                    personal_best_values[i] = opposite_fitness\n\n                # Update position\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], lb, ub)  # ensure within bounds\n\n                # Evaluate current position\n                current_fitness = func(positions[i])\n                self.eval_count += 1\n\n                # Update personal bests\n                if current_fitness < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_fitness\n\n                # Update global best\n                if current_fitness < global_best_value:\n                    global_best_position = positions[i]\n                    global_best_value = current_fitness\n\n                if self.eval_count >= self.budget:\n                    break\n\n            # Cooperative learning using average of personal bests\n            average_personal_best = np.mean(personal_best_positions, axis=0)\n            for i in range(num_agents):\n                if np.random.rand() < 0.1:  # 10% chance to use the average strategy\n                    r3 = np.random.rand()\n                    positions[i] = r3 * average_personal_best + (1 - r3) * positions[i]\n                    positions[i] = np.clip(positions[i], lb, ub)\n                    current_fitness = func(positions[i])\n                    self.eval_count += 1\n                    if current_fitness < personal_best_values[i]:\n                        personal_best_positions[i] = positions[i]\n                        personal_best_values[i] = current_fitness\n                    if current_fitness < global_best_value:\n                        global_best_position = positions[i]\n                        global_best_value = current_fitness\n\n            # Adaptive parameters\n            exploration_factor = alpha * (1 - self.eval_count / self.budget)\n            exploitation_factor = 1 - exploration_factor\n\n        return global_best_position", "name": "ImprovedEDASOOptimizer", "description": "Slightly adjust the exploration factor to enhance the balance between exploration and exploitation.", "configspace": "", "generation": 7, "fitness": 0.09279742658092753, "feedback": "The algorithm ImprovedEDASOOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.093 with standard deviation 0.001. And the mean value of best solutions found was 0.180 (0. is the best).", "error": "", "parent_id": "e2226140-f426-4d6b-b48a-b01ff98cbd1e", "metadata": {"aucs": [0.09346939516522512, 0.09092235498371248, 0.09400052959384497], "final_y": [0.1771756742606807, 0.18595524706029665, 0.1754149228323284]}, "mutation_prompt": null}
{"id": "a57a8c32-2c69-45b1-b287-e88f5f890f48", "solution": "import numpy as np\n\nclass ImprovedEDASOOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_agents = 5 + int(0.1 * self.dim)\n        exploration_factor = 0.9\n        exploitation_factor = 0.1\n        inertia_weight_min = 0.4\n        inertia_weight_max = 0.9\n        alpha = 0.5  # Coefficient for dynamic adaptation\n\n        # Initialize position and velocity for each agent\n        lb, ub = func.bounds.lb, func.bounds.ub\n        positions = np.random.uniform(lb, ub, (num_agents, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_agents, self.dim))\n\n        # Evaluate initial positions\n        fitness = np.apply_along_axis(func, 1, positions)\n        self.eval_count += num_agents\n\n        best_agent_idx = np.argmin(fitness)\n        global_best_position = positions[best_agent_idx].copy()\n        global_best_value = fitness[best_agent_idx]\n\n        personal_best_positions = positions.copy()\n        personal_best_values = fitness.copy()\n\n        while self.eval_count < self.budget:\n            # Update inertia weight dynamically\n            inertia_weight = inertia_weight_max - (inertia_weight_max - inertia_weight_min) * ((self.eval_count + 1) / self.budget)**0.95  # Changed line\n\n            for i in range(num_agents):\n                # Update velocity\n                r1, r2 = np.random.rand(2)\n                cognitive_component = exploration_factor * r1 * (personal_best_positions[i] - positions[i])\n                social_component = exploitation_factor * r2 * (global_best_position - positions[i])\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n\n                # Introduce opposition-based learning\n                opposite_position = lb + ub - positions[i]\n                opposite_fitness = func(opposite_position)\n                self.eval_count += 1\n\n                if opposite_fitness < personal_best_values[i]:\n                    positions[i] = opposite_position\n                    personal_best_values[i] = opposite_fitness\n\n                # Update position\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], lb, ub)  # ensure within bounds\n\n                # Evaluate current position\n                current_fitness = func(positions[i])\n                self.eval_count += 1\n\n                # Update personal bests\n                if current_fitness < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_fitness\n\n                # Update global best\n                if current_fitness < global_best_value:\n                    global_best_position = positions[i]\n                    global_best_value = current_fitness\n\n                if self.eval_count >= self.budget:\n                    break\n\n            # Cooperative learning using average of personal bests\n            average_personal_best = np.mean(personal_best_positions, axis=0)\n            for i in range(num_agents):\n                if np.random.rand() < 0.1:  # 10% chance to use the average strategy\n                    r3 = np.random.rand()\n                    positions[i] = r3 * average_personal_best + (1 - r3) * positions[i]\n                    positions[i] = np.clip(positions[i], lb, ub)\n                    current_fitness = func(positions[i])\n                    self.eval_count += 1\n                    if current_fitness < personal_best_values[i]:\n                        personal_best_positions[i] = positions[i]\n                        personal_best_values[i] = current_fitness\n                    if current_fitness < global_best_value:\n                        global_best_position = positions[i]\n                        global_best_value = current_fitness\n\n            # Adaptive parameters\n            exploration_factor = alpha * (1 - self.eval_count / self.budget)\n            exploitation_factor = 1 - exploration_factor\n\n        return global_best_position", "name": "ImprovedEDASOOptimizer", "description": "Enhanced exploration-exploitation balance by tweaking inertia weight decay for dynamic adaptation.", "configspace": "", "generation": 8, "fitness": 0.09237490935087948, "feedback": "The algorithm ImprovedEDASOOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.092 with standard deviation 0.001. And the mean value of best solutions found was 0.181 (0. is the best).", "error": "", "parent_id": "e2226140-f426-4d6b-b48a-b01ff98cbd1e", "metadata": {"aucs": [0.09104134021458687, 0.09212424566750577, 0.0939591421705458], "final_y": [0.18591464573854422, 0.18177596008001096, 0.17558582658653]}, "mutation_prompt": null}
{"id": "90bfe44c-d1d1-4130-9527-6610df6130a5", "solution": "import numpy as np\n\nclass ImprovedEDASOOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_agents = 5 + int(0.1 * self.dim)\n        exploration_factor = 0.9\n        exploitation_factor = 0.1\n        inertia_weight_min = 0.4\n        inertia_weight_max = 0.9\n        alpha = 0.5  # Coefficient for dynamic adaptation\n\n        # Initialize position and velocity for each agent\n        lb, ub = func.bounds.lb, func.bounds.ub\n        positions = np.random.uniform(lb, ub, (num_agents, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_agents, self.dim))\n\n        # Evaluate initial positions\n        fitness = np.apply_along_axis(func, 1, positions)\n        self.eval_count += num_agents\n\n        best_agent_idx = np.argmin(fitness)\n        global_best_position = positions[best_agent_idx].copy()\n        global_best_value = fitness[best_agent_idx]\n\n        personal_best_positions = positions.copy()\n        personal_best_values = fitness.copy()\n\n        while self.eval_count < self.budget:\n            # Update inertia weight dynamically\n            inertia_weight = inertia_weight_max - (inertia_weight_max - inertia_weight_min) * ((self.eval_count + 1) / self.budget)  # Changed line\n\n            for i in range(num_agents):\n                # Update velocity\n                r1, r2 = np.random.rand(2)\n                cognitive_component = exploration_factor * r1 * (personal_best_positions[i] - positions[i])\n                social_component = exploitation_factor * r2 * (global_best_position - positions[i])\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n\n                # Introduce opposition-based learning\n                opposite_position = lb + ub - positions[i]\n                opposite_fitness = func(opposite_position)\n                self.eval_count += 1\n\n                if opposite_fitness < personal_best_values[i]:\n                    positions[i] = opposite_position\n                    personal_best_values[i] = opposite_fitness\n\n                # Update position\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], lb, ub)  # ensure within bounds\n\n                # Evaluate current position\n                current_fitness = func(positions[i])\n                self.eval_count += 1\n\n                # Update personal bests\n                if current_fitness < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_fitness\n\n                # Update global best\n                if current_fitness < global_best_value:\n                    global_best_position = positions[i]\n                    global_best_value = current_fitness\n\n                if self.eval_count >= self.budget:\n                    break\n\n            # Cooperative learning using average of personal bests\n            average_personal_best = np.mean(personal_best_positions, axis=0)\n            for i in range(num_agents):\n                if np.random.rand() < 0.1:  # 10% chance to use the average strategy\n                    r3 = np.random.rand()\n                    positions[i] = r3 * average_personal_best + (1 - r3) * positions[i]\n                    positions[i] = np.clip(positions[i], lb, ub)\n                    current_fitness = func(positions[i])\n                    self.eval_count += 1\n                    if current_fitness < personal_best_values[i]:\n                        personal_best_positions[i] = positions[i]\n                        personal_best_values[i] = current_fitness\n                    if current_fitness < global_best_value:\n                        global_best_position = positions[i]\n                        global_best_value = current_fitness\n\n            # Adaptive parameters\n            exploration_factor = alpha * (1 - (self.eval_count / self.budget) ** 1.1)  # Changed line\n            exploitation_factor = 1 - exploration_factor\n\n        return global_best_position", "name": "ImprovedEDASOOptimizer", "description": "Introducing a slight adjustment to exploration factor's dynamic adaptation for enhanced search efficiency.", "configspace": "", "generation": 9, "fitness": 0.09225115629744118, "feedback": "The algorithm ImprovedEDASOOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.092 with standard deviation 0.001. And the mean value of best solutions found was 0.182 (0. is the best).", "error": "", "parent_id": "e2226140-f426-4d6b-b48a-b01ff98cbd1e", "metadata": {"aucs": [0.09226883760498583, 0.09051974167406462, 0.0939648896132731], "final_y": [0.18136058609893946, 0.18758234848815847, 0.17556039409484603]}, "mutation_prompt": null}
{"id": "e39be0f1-025e-4ea7-a984-89ca770a43e4", "solution": "import numpy as np\n\nclass ImprovedEDASOOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_agents = 5 + int(0.1 * self.dim)\n        exploration_factor = 0.9\n        exploitation_factor = 0.1\n        inertia_weight_min = 0.4\n        inertia_weight_max = 0.9\n        alpha = 0.55  # Coefficient for dynamic adaptation, changed from 0.5 to 0.55\n\n        # Initialize position and velocity for each agent\n        lb, ub = func.bounds.lb, func.bounds.ub\n        positions = np.random.uniform(lb, ub, (num_agents, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_agents, self.dim))\n\n        # Evaluate initial positions\n        fitness = np.apply_along_axis(func, 1, positions)\n        self.eval_count += num_agents\n\n        best_agent_idx = np.argmin(fitness)\n        global_best_position = positions[best_agent_idx].copy()\n        global_best_value = fitness[best_agent_idx]\n\n        personal_best_positions = positions.copy()\n        personal_best_values = fitness.copy()\n\n        while self.eval_count < self.budget:\n            # Update inertia weight dynamically\n            inertia_weight = inertia_weight_max - (inertia_weight_max - inertia_weight_min) * ((self.eval_count + 1) / self.budget)\n\n            for i in range(num_agents):\n                # Update velocity\n                r1, r2 = np.random.rand(2)\n                cognitive_component = exploration_factor * r1 * (personal_best_positions[i] - positions[i])\n                social_component = exploitation_factor * r2 * (global_best_position - positions[i])\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n\n                # Introduce opposition-based learning\n                opposite_position = lb + ub - positions[i]\n                opposite_fitness = func(opposite_position)\n                self.eval_count += 1\n\n                if opposite_fitness < personal_best_values[i]:\n                    positions[i] = opposite_position\n                    personal_best_values[i] = opposite_fitness\n\n                # Update position\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], lb, ub)  # ensure within bounds\n\n                # Evaluate current position\n                current_fitness = func(positions[i])\n                self.eval_count += 1\n\n                # Update personal bests\n                if current_fitness < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_fitness\n\n                # Update global best\n                if current_fitness < global_best_value:\n                    global_best_position = positions[i]\n                    global_best_value = current_fitness\n\n                if self.eval_count >= self.budget:\n                    break\n\n            # Cooperative learning using average of personal bests\n            average_personal_best = np.mean(personal_best_positions, axis=0)\n            for i in range(num_agents):\n                if np.random.rand() < 0.1:  # 10% chance to use the average strategy\n                    r3 = np.random.rand()\n                    positions[i] = r3 * average_personal_best + (1 - r3) * positions[i]\n                    positions[i] = np.clip(positions[i], lb, ub)\n                    current_fitness = func(positions[i])\n                    self.eval_count += 1\n                    if current_fitness < personal_best_values[i]:\n                        personal_best_positions[i] = positions[i]\n                        personal_best_values[i] = current_fitness\n                    if current_fitness < global_best_value:\n                        global_best_position = positions[i]\n                        global_best_value = current_fitness\n\n            # Adaptive parameters\n            exploration_factor = alpha * (1 - self.eval_count / self.budget)\n            exploitation_factor = 1 - exploration_factor\n\n        return global_best_position", "name": "ImprovedEDASOOptimizer", "description": "Fine-tune the balance between exploration and exploitation by slightly adjusting the adaptive parameters for more effective convergence.", "configspace": "", "generation": 10, "fitness": 0.09029408603507565, "feedback": "The algorithm ImprovedEDASOOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.090 with standard deviation 0.002. And the mean value of best solutions found was 0.187 (0. is the best).", "error": "", "parent_id": "e2226140-f426-4d6b-b48a-b01ff98cbd1e", "metadata": {"aucs": [0.09230473076695944, 0.09107682101538539, 0.0875007063228821], "final_y": [0.18158714245765095, 0.185172023166697, 0.1953369117363455]}, "mutation_prompt": null}
{"id": "59e6fa27-a221-45ac-9012-821cdddd82ca", "solution": "import numpy as np\n\nclass ImprovedEDASOOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_agents = 5 + int(0.1 * self.dim)\n        exploration_factor = 0.9\n        exploitation_factor = 0.1\n        inertia_weight_min = 0.4\n        inertia_weight_max = 0.9\n        alpha = 0.5  # Coefficient for dynamic adaptation\n\n        # Initialize position and velocity for each agent\n        lb, ub = func.bounds.lb, func.bounds.ub\n        positions = np.random.uniform(lb, ub, (num_agents, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_agents, self.dim))\n\n        # Evaluate initial positions\n        fitness = np.apply_along_axis(func, 1, positions)\n        self.eval_count += num_agents\n\n        best_agent_idx = np.argmin(fitness)\n        global_best_position = positions[best_agent_idx].copy()\n        global_best_value = fitness[best_agent_idx]\n\n        personal_best_positions = positions.copy()\n        personal_best_values = fitness.copy()\n\n        while self.eval_count < self.budget:\n            # Update inertia weight dynamically\n            inertia_weight = inertia_weight_max - (inertia_weight_max - inertia_weight_min) * ((self.eval_count + 1) / self.budget)  # Changed line\n\n            for i in range(num_agents):\n                # Update velocity\n                r1, r2 = np.random.rand(2) * 2  # Adjusted line for extended exploration-exploitation balance\n                cognitive_component = exploration_factor * r1 * (personal_best_positions[i] - positions[i])\n                social_component = exploitation_factor * r2 * (global_best_position - positions[i])\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n\n                # Introduce opposition-based learning\n                opposite_position = lb + ub - positions[i]\n                opposite_fitness = func(opposite_position)\n                self.eval_count += 1\n\n                if opposite_fitness < personal_best_values[i]:\n                    positions[i] = opposite_position\n                    personal_best_values[i] = opposite_fitness\n\n                # Update position\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], lb, ub)  # ensure within bounds\n\n                # Evaluate current position\n                current_fitness = func(positions[i])\n                self.eval_count += 1\n\n                # Update personal bests\n                if current_fitness < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_fitness\n\n                # Update global best\n                if current_fitness < global_best_value:\n                    global_best_position = positions[i]\n                    global_best_value = current_fitness\n\n                if self.eval_count >= self.budget:\n                    break\n\n            # Cooperative learning using average of personal bests\n            average_personal_best = np.mean(personal_best_positions, axis=0)\n            for i in range(num_agents):\n                if np.random.rand() < 0.1:  # 10% chance to use the average strategy\n                    r3 = np.random.rand()\n                    positions[i] = r3 * average_personal_best + (1 - r3) * positions[i]\n                    positions[i] = np.clip(positions[i], lb, ub)\n                    current_fitness = func(positions[i])\n                    self.eval_count += 1\n                    if current_fitness < personal_best_values[i]:\n                        personal_best_positions[i] = positions[i]\n                        personal_best_values[i] = current_fitness\n                    if current_fitness < global_best_value:\n                        global_best_position = positions[i]\n                        global_best_value = current_fitness\n\n            # Adaptive parameters\n            exploration_factor = alpha * (1 - self.eval_count / self.budget)\n            exploitation_factor = 1 - exploration_factor\n\n        return global_best_position", "name": "ImprovedEDASOOptimizer", "description": "Enhance exploration-exploitation balance by adjusting the random factor in velocity update.", "configspace": "", "generation": 11, "fitness": 0.09116292518818148, "feedback": "The algorithm ImprovedEDASOOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.091 with standard deviation 0.002. And the mean value of best solutions found was 0.183 (0. is the best).", "error": "", "parent_id": "e2226140-f426-4d6b-b48a-b01ff98cbd1e", "metadata": {"aucs": [0.09345222376273998, 0.09188901224694235, 0.0881475395548621], "final_y": [0.17742622657094664, 0.18192911752961927, 0.19106002916955722]}, "mutation_prompt": null}
{"id": "8ae1ae03-3cd0-4b04-a335-2429a8454d71", "solution": "import numpy as np\n\nclass QuantumInspiredOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_agents = 5 + int(0.1 * self.dim)\n        exploration_factor = 0.9\n        exploitation_factor = 0.1\n        inertia_weight_min = 0.4\n        inertia_weight_max = 0.9\n        alpha = 0.5\n\n        # Initialize position and velocity for each agent\n        lb, ub = func.bounds.lb, func.bounds.ub\n        positions = np.random.uniform(lb, ub, (num_agents, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_agents, self.dim))\n\n        # Evaluate initial positions\n        fitness = np.apply_along_axis(func, 1, positions)\n        self.eval_count += num_agents\n\n        best_agent_idx = np.argmin(fitness)\n        global_best_position = positions[best_agent_idx].copy()\n        global_best_value = fitness[best_agent_idx]\n\n        personal_best_positions = positions.copy()\n        personal_best_values = fitness.copy()\n\n        while self.eval_count < self.budget:\n            # Update inertia weight dynamically\n            inertia_weight = inertia_weight_max - (inertia_weight_max - inertia_weight_min) * ((self.eval_count + 1) / self.budget)\n\n            for i in range(num_agents):\n                # Update velocity\n                r1, r2 = np.random.rand(2)\n                cognitive_component = exploration_factor * r1 * (personal_best_positions[i] - positions[i])\n                social_component = exploitation_factor * r2 * (global_best_position - positions[i])\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n\n                # Quantum-inspired superposition update\n                quantum_superposition = np.random.uniform(lb, ub, self.dim)\n                superposed_position = (positions[i] + quantum_superposition) / 2\n\n                # Evaluate superposed position\n                superposed_fitness = func(superposed_position)\n                self.eval_count += 1\n\n                if superposed_fitness < personal_best_values[i]:\n                    positions[i] = superposed_position\n                    personal_best_values[i] = superposed_fitness\n\n                # Update position\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], lb, ub)\n\n                # Evaluate current position\n                current_fitness = func(positions[i])\n                self.eval_count += 1\n\n                # Update personal bests\n                if current_fitness < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_fitness\n\n                # Update global best\n                if current_fitness < global_best_value:\n                    global_best_position = positions[i]\n                    global_best_value = current_fitness\n\n                if self.eval_count >= self.budget:\n                    break\n\n            # Cooperative learning using average of personal bests\n            average_personal_best = np.mean(personal_best_positions, axis=0)\n            for i in range(num_agents):\n                if np.random.rand() < 0.1:\n                    r3 = np.random.rand()\n                    positions[i] = r3 * average_personal_best + (1 - r3) * positions[i]\n                    positions[i] = np.clip(positions[i], lb, ub)\n                    current_fitness = func(positions[i])\n                    self.eval_count += 1\n                    if current_fitness < personal_best_values[i]:\n                        personal_best_positions[i] = positions[i]\n                        personal_best_values[i] = current_fitness\n                    if current_fitness < global_best_value:\n                        global_best_position = positions[i]\n                        global_best_value = current_fitness\n\n            # Adaptive parameters\n            exploration_factor = alpha * (1 - self.eval_count / self.budget)\n            exploitation_factor = 1 - exploration_factor\n\n        return global_best_position", "name": "QuantumInspiredOptimizer", "description": "Introduce quantum-inspired superposition states for enhanced diversity and convergence in dynamic evolutionary search.", "configspace": "", "generation": 12, "fitness": 0.09064028921018763, "feedback": "The algorithm QuantumInspiredOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.091 with standard deviation 0.002. And the mean value of best solutions found was 0.186 (0. is the best).", "error": "", "parent_id": "e2226140-f426-4d6b-b48a-b01ff98cbd1e", "metadata": {"aucs": [0.08739038286948653, 0.09328925141141431, 0.09124123334966205], "final_y": [0.196272577813698, 0.17815275672885955, 0.18450988153313508]}, "mutation_prompt": null}
{"id": "e95ba8ed-501a-4823-9597-7ecf5abc9930", "solution": "import numpy as np\n\nclass ImprovedEDASOOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_agents = 5 + int(0.1 * self.dim)\n        exploration_factor = 0.9\n        exploitation_factor = 0.1\n        inertia_weight_min = 0.4\n        inertia_weight_max = 0.9\n        alpha = 0.5  # Coefficient for dynamic adaptation\n\n        # Initialize position and velocity for each agent\n        lb, ub = func.bounds.lb, func.bounds.ub\n        positions = np.random.uniform(lb, ub, (num_agents, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_agents, self.dim))\n\n        # Evaluate initial positions\n        fitness = np.apply_along_axis(func, 1, positions)\n        self.eval_count += num_agents\n\n        best_agent_idx = np.argmin(fitness)\n        global_best_position = positions[best_agent_idx].copy()\n        global_best_value = fitness[best_agent_idx]\n\n        personal_best_positions = positions.copy()\n        personal_best_values = fitness.copy()\n\n        while self.eval_count < self.budget:\n            # Update inertia weight dynamically\n            inertia_weight = inertia_weight_max - (inertia_weight_max - inertia_weight_min) * ((self.eval_count + 1) / self.budget)\n\n            for i in range(num_agents):\n                # Update velocity\n                r1, r2 = np.random.rand(2)\n                cognitive_component = (exploration_factor + 0.05) * r1 * (personal_best_positions[i] - positions[i])  # Changed line\n                social_component = exploitation_factor * r2 * (global_best_position - positions[i])\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n\n                # Introduce opposition-based learning\n                opposite_position = lb + ub - positions[i]\n                opposite_fitness = func(opposite_position)\n                self.eval_count += 1\n\n                if opposite_fitness < personal_best_values[i]:\n                    positions[i] = opposite_position\n                    personal_best_values[i] = opposite_fitness\n\n                # Update position\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], lb, ub)  # ensure within bounds\n\n                # Evaluate current position\n                current_fitness = func(positions[i])\n                self.eval_count += 1\n\n                # Update personal bests\n                if current_fitness < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_fitness\n\n                # Update global best\n                if current_fitness < global_best_value:\n                    global_best_position = positions[i]\n                    global_best_value = current_fitness\n\n                if self.eval_count >= self.budget:\n                    break\n\n            # Cooperative learning using average of personal bests\n            average_personal_best = np.mean(personal_best_positions, axis=0)\n            for i in range(num_agents):\n                if np.random.rand() < 0.1:  # 10% chance to use the average strategy\n                    r3 = np.random.rand()\n                    positions[i] = r3 * average_personal_best + (1 - r3) * positions[i]\n                    positions[i] = np.clip(positions[i], lb, ub)\n                    current_fitness = func(positions[i])\n                    self.eval_count += 1\n                    if current_fitness < personal_best_values[i]:\n                        personal_best_positions[i] = positions[i]\n                        personal_best_values[i] = current_fitness\n                    if current_fitness < global_best_value:\n                        global_best_position = positions[i]\n                        global_best_value = current_fitness\n\n            # Adaptive parameters\n            exploration_factor = alpha * (1 - self.eval_count / self.budget)\n            exploitation_factor = 1 - exploration_factor\n\n        return global_best_position", "name": "ImprovedEDASOOptimizer", "description": "Introduce a slight increase in cognitive component for enhanced personal exploration.", "configspace": "", "generation": 13, "fitness": 0.08996544941740574, "feedback": "The algorithm ImprovedEDASOOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.090 with standard deviation 0.001. And the mean value of best solutions found was 0.189 (0. is the best).", "error": "", "parent_id": "e2226140-f426-4d6b-b48a-b01ff98cbd1e", "metadata": {"aucs": [0.08949474358996756, 0.09151665280271659, 0.08888495185953305], "final_y": [0.1913756509746256, 0.18349969748899997, 0.19177032898245594]}, "mutation_prompt": null}
{"id": "d598b6b6-f547-4b30-ba2b-bced33ffe018", "solution": "import numpy as np\n\nclass EnhancedEDASOOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_agents = 5 + int(0.1 * self.dim)\n        exploration_factor = 0.9\n        exploitation_factor = 0.1\n        inertia_weight_min = 0.4\n        inertia_weight_max = 0.9\n        alpha = 0.5  # Coefficient for dynamic adaptation\n\n        # Initialize position and velocity for each agent\n        lb, ub = func.bounds.lb, func.bounds.ub\n        positions = np.random.uniform(lb, ub, (num_agents, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_agents, self.dim))\n\n        # Evaluate initial positions\n        fitness = np.apply_along_axis(func, 1, positions)\n        self.eval_count += num_agents\n\n        best_agent_idx = np.argmin(fitness)\n        global_best_position = positions[best_agent_idx].copy()\n        global_best_value = fitness[best_agent_idx]\n\n        personal_best_positions = positions.copy()\n        personal_best_values = fitness.copy()\n\n        while self.eval_count < self.budget:\n            # Update inertia weight dynamically\n            inertia_weight = inertia_weight_max - (inertia_weight_max - inertia_weight_min) * np.sin(np.pi * (self.eval_count + 1) / self.budget)\n\n            for i in range(num_agents):\n                # Update velocity\n                r1, r2 = np.random.rand(2)\n                cognitive_component = exploration_factor * r1 * (personal_best_positions[i] - positions[i])\n                social_component = exploitation_factor * r2 * (global_best_position - positions[i])\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n\n                # Introduce dynamic opposition-based learning with a higher probability for worse solutions\n                opposite_position = lb + ub - positions[i]\n                opposite_fitness = func(opposite_position)\n                self.eval_count += 1\n\n                if opposite_fitness < personal_best_values[i] or np.random.rand() < 0.2:\n                    positions[i] = opposite_position\n                    personal_best_values[i] = opposite_fitness\n\n                # Update position\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], lb, ub)  # ensure within bounds\n\n                # Evaluate current position\n                current_fitness = func(positions[i])\n                self.eval_count += 1\n\n                # Update personal bests\n                if current_fitness < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_fitness\n\n                # Update global best\n                if current_fitness < global_best_value:\n                    global_best_position = positions[i]\n                    global_best_value = current_fitness\n\n                if self.eval_count >= self.budget:\n                    break\n\n            # Cooperative learning using dynamic leader selection\n            leader_idx = np.random.choice(num_agents, p=1-personal_best_values/np.sum(personal_best_values))\n            leader_position = personal_best_positions[leader_idx]\n            for i in range(num_agents):\n                if np.random.rand() < 0.15:  # 15% chance to use the leader strategy\n                    r3 = np.random.rand()\n                    positions[i] = r3 * leader_position + (1 - r3) * positions[i]\n                    positions[i] = np.clip(positions[i], lb, ub)\n                    current_fitness = func(positions[i])\n                    self.eval_count += 1\n                    if current_fitness < personal_best_values[i]:\n                        personal_best_positions[i] = positions[i]\n                        personal_best_values[i] = current_fitness\n                    if current_fitness < global_best_value:\n                        global_best_position = positions[i]\n                        global_best_value = current_fitness\n\n            # Adaptive parameters\n            exploration_factor = alpha * (1 - self.eval_count / self.budget)\n            exploitation_factor = 1 - exploration_factor\n\n        return global_best_position", "name": "EnhancedEDASOOptimizer", "description": "Implement a dynamic opposition-based learning and leader selection to enhance convergence speed and solution accuracy.", "configspace": "", "generation": 14, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('probabilities do not sum to 1').", "error": "ValueError('probabilities do not sum to 1')", "parent_id": "e2226140-f426-4d6b-b48a-b01ff98cbd1e", "metadata": {}, "mutation_prompt": null}
{"id": "3e01c57a-c35f-4230-a469-1311a05db7ea", "solution": "import numpy as np\n\nclass ImprovedEDASOOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_agents = 5 + int(0.1 * self.dim)\n        exploration_factor = 0.9\n        exploitation_factor = 0.1\n        inertia_weight_min = 0.4\n        inertia_weight_max = 0.9\n        alpha = 0.5  # Coefficient for dynamic adaptation\n\n        # Initialize position and velocity for each agent\n        lb, ub = func.bounds.lb, func.bounds.ub\n        positions = np.random.uniform(lb, ub, (num_agents, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_agents, self.dim))\n\n        # Evaluate initial positions\n        fitness = np.apply_along_axis(func, 1, positions)\n        self.eval_count += num_agents\n\n        best_agent_idx = np.argmin(fitness)\n        global_best_position = positions[best_agent_idx].copy()\n        global_best_value = fitness[best_agent_idx]\n\n        personal_best_positions = positions.copy()\n        personal_best_values = fitness.copy()\n\n        while self.eval_count < self.budget:\n            # Update inertia weight dynamically\n            inertia_weight = inertia_weight_max - (inertia_weight_max - inertia_weight_min) * ((self.eval_count + 1) / self.budget)  # Changed line\n\n            for i in range(num_agents):\n                # Update velocity\n                r1, r2 = np.random.rand(2)\n                cognitive_component = exploration_factor * r1 * (personal_best_positions[i] - positions[i])\n                social_component = (exploitation_factor + alpha * (1 - self.eval_count / self.budget)) * r2 * (global_best_position - positions[i])  # Changed line\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n\n                # Introduce opposition-based learning\n                opposite_position = lb + ub - positions[i]\n                opposite_fitness = func(opposite_position)\n                self.eval_count += 1\n\n                if opposite_fitness < personal_best_values[i]:\n                    positions[i] = opposite_position\n                    personal_best_values[i] = opposite_fitness\n\n                # Update position\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], lb, ub)  # ensure within bounds\n\n                # Evaluate current position\n                current_fitness = func(positions[i])\n                self.eval_count += 1\n\n                # Update personal bests\n                if current_fitness < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_fitness\n\n                # Update global best\n                if current_fitness < global_best_value:\n                    global_best_position = positions[i]\n                    global_best_value = current_fitness\n\n                if self.eval_count >= self.budget:\n                    break\n\n            # Cooperative learning using average of personal bests\n            average_personal_best = np.mean(personal_best_positions, axis=0)\n            for i in range(num_agents):\n                if np.random.rand() < 0.1:  # 10% chance to use the average strategy\n                    r3 = np.random.rand()\n                    positions[i] = r3 * average_personal_best + (1 - r3) * positions[i]\n                    positions[i] = np.clip(positions[i], lb, ub)\n                    current_fitness = func(positions[i])\n                    self.eval_count += 1\n                    if current_fitness < personal_best_values[i]:\n                        personal_best_positions[i] = positions[i]\n                        personal_best_values[i] = current_fitness\n                    if current_fitness < global_best_value:\n                        global_best_position = positions[i]\n                        global_best_value = current_fitness\n\n            # Adaptive parameters\n            exploration_factor = alpha * (1 - self.eval_count / self.budget)\n            exploitation_factor = 1 - exploration_factor\n\n        return global_best_position", "name": "ImprovedEDASOOptimizer", "description": "Improved exploration through dynamic adaptation of social component factor.", "configspace": "", "generation": 15, "fitness": 0.08551611329952032, "feedback": "The algorithm ImprovedEDASOOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.086 with standard deviation 0.004. And the mean value of best solutions found was 0.204 (0. is the best).", "error": "", "parent_id": "e2226140-f426-4d6b-b48a-b01ff98cbd1e", "metadata": {"aucs": [0.09140784616123931, 0.08067275615948843, 0.08446773757783321], "final_y": [0.18408082333842357, 0.21981689038364305, 0.20799404237162766]}, "mutation_prompt": null}
{"id": "0a579f15-49a9-4b5d-8753-47b529ef7933", "solution": "import numpy as np\n\nclass HybridDEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def __call__(self, func):\n        # Initialize parameters\n        pop_size = 10 + int(0.2 * self.dim)\n        F = 0.5  # Mutation factor\n        CR = 0.9  # Crossover probability\n        adapt_rate = 0.2  # Adjustment rate for F and CR\n\n        # Initialize population\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (pop_size, self.dim))\n\n        # Evaluate initial population\n        fitness = np.apply_along_axis(func, 1, population)\n        self.eval_count += pop_size\n\n        best_idx = np.argmin(fitness)\n        global_best_position = population[best_idx].copy()\n        global_best_value = fitness[best_idx]\n\n        while self.eval_count < self.budget:\n            for i in range(pop_size):\n                # Mutation\n                indices = np.random.choice(np.delete(np.arange(pop_size), i), 3, replace=False)\n                x1, x2, x3 = population[indices]\n                mutant_vector = x1 + F * (x2 - x3)\n                mutant_vector = np.clip(mutant_vector, lb, ub)\n\n                # Crossover\n                trial_vector = np.where(np.random.rand(self.dim) < CR, mutant_vector, population[i])\n\n                # Evaluate trial vector\n                trial_fitness = func(trial_vector)\n                self.eval_count += 1\n\n                # Selection\n                if trial_fitness < fitness[i]:\n                    population[i] = trial_vector\n                    fitness[i] = trial_fitness\n\n                    # Update global best\n                    if trial_fitness < global_best_value:\n                        global_best_position = trial_vector\n                        global_best_value = trial_fitness\n\n                # Dynamic parameter adaptation\n                if np.random.rand() < adapt_rate:\n                    F = 0.4 + 0.5 * np.random.rand()\n                    CR = 0.8 + 0.2 * np.random.rand()\n\n                # Opposition-Based Learning\n                if np.random.rand() < 0.1:\n                    opposite_vector = lb + ub - population[i]\n                    opposite_fitness = func(opposite_vector)\n                    self.eval_count += 1\n                    if opposite_fitness < fitness[i]:\n                        population[i] = opposite_vector\n                        fitness[i] = opposite_fitness\n                        if opposite_fitness < global_best_value:\n                            global_best_position = opposite_vector\n                            global_best_value = opposite_fitness\n\n                if self.eval_count >= self.budget:\n                    break\n\n        return global_best_position", "name": "HybridDEOptimizer", "description": "Hybrid Differential Evolution with Dynamic Parameter Adaptation and Opposition-Based Learning for enhanced exploration and exploitation balance.", "configspace": "", "generation": 16, "fitness": 0.08643681014649973, "feedback": "The algorithm HybridDEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.086 with standard deviation 0.013. And the mean value of best solutions found was 0.204 (0. is the best).", "error": "", "parent_id": "e2226140-f426-4d6b-b48a-b01ff98cbd1e", "metadata": {"aucs": [0.06763121686116624, 0.09620968458527168, 0.09546952899306127], "final_y": [0.2827530912305001, 0.16490549377974395, 0.165013485135163]}, "mutation_prompt": null}
{"id": "da807e55-ff42-4d43-888e-f80cee23973a", "solution": "import numpy as np\n\nclass HybridDynamicVelocityOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_agents = 5 + int(0.1 * self.dim)\n        inertia_weight_min = 0.3\n        inertia_weight_max = 0.9\n        personal_coef = 2.0\n        social_coef = 2.0\n        adaptation_rate = 0.01\n\n        # Initialize positions and velocities\n        lb, ub = func.bounds.lb, func.bounds.ub\n        positions = np.random.uniform(lb, ub, (num_agents, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_agents, self.dim))\n\n        # Evaluate initial positions\n        fitness = np.apply_along_axis(func, 1, positions)\n        self.eval_count += num_agents\n\n        best_global_idx = np.argmin(fitness)\n        global_best_position = positions[best_global_idx].copy()\n        global_best_value = fitness[best_global_idx]\n\n        personal_best_positions = positions.copy()\n        personal_best_values = fitness.copy()\n\n        while self.eval_count < self.budget:\n            # Dynamically adjust inertia weight\n            inertia_weight = inertia_weight_max - (inertia_weight_max - inertia_weight_min) * (self.eval_count / self.budget)\n\n            for i in range(num_agents):\n                # Update velocity\n                r1, r2 = np.random.rand(2)\n                cognitive_component = personal_coef * r1 * (personal_best_positions[i] - positions[i])\n                social_component = social_coef * r2 * (global_best_position - positions[i])\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n\n                # Dynamic velocity adjustment\n                velocities[i] *= (1 - adaptation_rate) + 2 * adaptation_rate * np.random.rand()\n\n                # Update position\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], lb, ub)\n\n                # Evaluate current position\n                current_fitness = func(positions[i])\n                self.eval_count += 1\n\n                # Update personal bests\n                if current_fitness < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_fitness\n\n                # Update global best if necessary\n                if current_fitness < global_best_value:\n                    global_best_position = positions[i]\n                    global_best_value = current_fitness\n\n                # Adaptive opposition-based learning\n                if np.random.rand() < 0.2:  # 20% probability to use opposition-based strategy\n                    opposite_position = lb + ub - positions[i]\n                    opposite_fitness = func(opposite_position)\n                    self.eval_count += 1\n\n                    if opposite_fitness < personal_best_values[i]:\n                        personal_best_positions[i] = opposite_position\n                        personal_best_values[i] = opposite_fitness\n                        if opposite_fitness < global_best_value:\n                            global_best_position = opposite_position\n                            global_best_value = opposite_fitness\n\n                if self.eval_count >= self.budget:\n                    break\n\n        return global_best_position", "name": "HybridDynamicVelocityOptimizer", "description": "Introduce a hybrid dynamic velocity adjustment and adaptive opposition-based learning for enhanced convergence.", "configspace": "", "generation": 17, "fitness": 0.08329239764272545, "feedback": "The algorithm HybridDynamicVelocityOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.083 with standard deviation 0.013. And the mean value of best solutions found was 0.211 (0. is the best).", "error": "", "parent_id": "e2226140-f426-4d6b-b48a-b01ff98cbd1e", "metadata": {"aucs": [0.06585435179656762, 0.09519414559215689, 0.08882869553945183], "final_y": [0.2851318081864823, 0.17020656097228226, 0.1766239652988182]}, "mutation_prompt": null}
{"id": "d6c22a1d-c43d-4322-b8ae-8fbd7259e6f4", "solution": "import numpy as np\n\nclass ImprovedEDASOOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_agents = 5 + int(0.1 * self.dim)\n        exploration_factor = 0.9\n        exploitation_factor = 0.1\n        inertia_weight_min = 0.4\n        inertia_weight_max = 0.9\n        alpha = 0.5  # Coefficient for dynamic adaptation\n\n        # Initialize position and velocity for each agent\n        lb, ub = func.bounds.lb, func.bounds.ub\n        positions = np.random.uniform(lb, ub, (num_agents, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_agents, self.dim))\n\n        # Evaluate initial positions\n        fitness = np.apply_along_axis(func, 1, positions)\n        self.eval_count += num_agents\n\n        best_agent_idx = np.argmin(fitness)\n        global_best_position = positions[best_agent_idx].copy()\n        global_best_value = fitness[best_agent_idx]\n\n        personal_best_positions = positions.copy()\n        personal_best_values = fitness.copy()\n\n        while self.eval_count < self.budget:\n            # Update inertia weight dynamically\n            inertia_weight = inertia_weight_max - (inertia_weight_max - inertia_weight_min) * ((self.eval_count + 1) / self.budget)  # Changed line\n\n            for i in range(num_agents):\n                # Update velocity\n                r1, r2 = np.random.rand(2)\n                adaptive_learning_rate = 0.5 + 0.5 * np.sin(np.pi * self.eval_count / self.budget)  # Changed line\n                cognitive_component = exploration_factor * r1 * (personal_best_positions[i] - positions[i])\n                social_component = exploitation_factor * r2 * (global_best_position - positions[i])\n                velocities[i] = inertia_weight * velocities[i] + adaptive_learning_rate * (cognitive_component + social_component)\n\n                # Introduce opposition-based learning\n                opposite_position = lb + ub - positions[i]\n                opposite_fitness = func(opposite_position)\n                self.eval_count += 1\n\n                if opposite_fitness < personal_best_values[i]:\n                    positions[i] = opposite_position\n                    personal_best_values[i] = opposite_fitness\n\n                # Update position\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], lb, ub)  # ensure within bounds\n\n                # Evaluate current position\n                current_fitness = func(positions[i])\n                self.eval_count += 1\n\n                # Update personal bests\n                if current_fitness < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_fitness\n\n                # Update global best\n                if current_fitness < global_best_value:\n                    global_best_position = positions[i]\n                    global_best_value = current_fitness\n\n                if self.eval_count >= self.budget:\n                    break\n\n            # Cooperative learning using average of personal bests\n            average_personal_best = np.mean(personal_best_positions, axis=0)\n            for i in range(num_agents):\n                if np.random.rand() < 0.1:  # 10% chance to use the average strategy\n                    r3 = np.random.rand()\n                    positions[i] = r3 * average_personal_best + (1 - r3) * positions[i]\n                    positions[i] = np.clip(positions[i], lb, ub)\n                    current_fitness = func(positions[i])\n                    self.eval_count += 1\n                    if current_fitness < personal_best_values[i]:\n                        personal_best_positions[i] = positions[i]\n                        personal_best_values[i] = current_fitness\n                    if current_fitness < global_best_value:\n                        global_best_position = positions[i]\n                        global_best_value = current_fitness\n\n            # Adaptive parameters\n            exploration_factor = alpha * (1 - self.eval_count / self.budget)\n            exploitation_factor = 1 - exploration_factor\n\n        return global_best_position", "name": "ImprovedEDASOOptimizer", "description": "Introduce adaptive learning rate for velocity update to enhance convergence speed.", "configspace": "", "generation": 18, "fitness": 0.0923280944838178, "feedback": "The algorithm ImprovedEDASOOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.092 with standard deviation 0.002. And the mean value of best solutions found was 0.181 (0. is the best).", "error": "", "parent_id": "e2226140-f426-4d6b-b48a-b01ff98cbd1e", "metadata": {"aucs": [0.08998433390122496, 0.09542739827096725, 0.09157255127926123], "final_y": [0.190023775977064, 0.16962450584721445, 0.18383400705034492]}, "mutation_prompt": null}
{"id": "2705d250-54ab-4d5a-854d-b8b213c98169", "solution": "import numpy as np\n\nclass AQPSOOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_agents = 10 + int(0.15 * self.dim)\n        inertia_weight = 0.7\n        cognitive_coeff = 2.0\n        social_coeff = 2.0\n        quantum_factor = 0.5\n\n        # Initialize position and velocity for each agent\n        lb, ub = func.bounds.lb, func.bounds.ub\n        positions = np.random.uniform(lb, ub, (num_agents, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_agents, self.dim))\n\n        # Evaluate initial positions\n        fitness = np.apply_along_axis(func, 1, positions)\n        self.eval_count += num_agents\n\n        best_agent_idx = np.argmin(fitness)\n        global_best_position = positions[best_agent_idx].copy()\n        global_best_value = fitness[best_agent_idx]\n\n        personal_best_positions = positions.copy()\n        personal_best_values = fitness.copy()\n\n        while self.eval_count < self.budget:\n            for i in range(num_agents):\n                # Update velocity using quantum-inspired approach\n                r1, r2 = np.random.rand(2)\n                cognitive_component = cognitive_coeff * r1 * (personal_best_positions[i] - positions[i])\n                social_component = social_coeff * r2 * (global_best_position - positions[i])\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n                \n                # Implement quantum-inspired probability adaptation\n                if np.random.rand() < quantum_factor:\n                    quantum_position = lb[i] + np.random.rand() * (ub[i] - lb[i])\n                    quantum_fitness = func(quantum_position)\n                    self.eval_count += 1\n                    if quantum_fitness < personal_best_values[i]:\n                        positions[i] = quantum_position\n                        personal_best_values[i] = quantum_fitness\n\n                # Update position with hypercube strategy\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], lb, ub)\n\n                # Evaluate current position\n                current_fitness = func(positions[i])\n                self.eval_count += 1\n\n                # Update personal bests\n                if current_fitness < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_fitness\n\n                # Update global best\n                if current_fitness < global_best_value:\n                    global_best_position = positions[i]\n                    global_best_value = current_fitness\n\n                if self.eval_count >= self.budget:\n                    break\n\n            # Adaptive quantum factor adjustment\n            quantum_factor = 0.5 * (1 - self.eval_count / self.budget)\n\n        return global_best_position", "name": "AQPSOOptimizer", "description": "Adaptive Quantum-inspired Particle Swarm Optimization (AQPSO) with dynamic hypercube-based search and quantum probability adaptation for enhanced exploration and exploitation.", "configspace": "", "generation": 19, "fitness": -Infinity, "feedback": "An exception occurred: TypeError('__call__(): incompatible function arguments. The following argument types are supported:\\n    1. (self: ioh.iohcpp.problem.RealSingleObjective, arg0: List[float]) -> float\\n    2. (self: ioh.iohcpp.problem.RealSingleObjective, arg0: List[List[float]]) -> List[float]\\n\\nInvoked with: <RealSingleObjectiveProblem 1121. brag_mirror (iid=1 dim=10)>, 171.01003583840654').", "error": "TypeError('__call__(): incompatible function arguments. The following argument types are supported:\\n    1. (self: ioh.iohcpp.problem.RealSingleObjective, arg0: List[float]) -> float\\n    2. (self: ioh.iohcpp.problem.RealSingleObjective, arg0: List[List[float]]) -> List[float]\\n\\nInvoked with: <RealSingleObjectiveProblem 1121. brag_mirror (iid=1 dim=10)>, 171.01003583840654')", "parent_id": "e2226140-f426-4d6b-b48a-b01ff98cbd1e", "metadata": {}, "mutation_prompt": null}
{"id": "25603311-b207-48a2-b0e4-7ccfa55f7a1b", "solution": "import numpy as np\n\nclass ImprovedEDASOOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_agents = 5 + int(0.1 * self.dim)\n        exploration_factor = 0.9\n        exploitation_factor = 0.1\n        inertia_weight_min = 0.4\n        inertia_weight_max = 0.9\n        alpha = 0.5  # Coefficient for dynamic adaptation\n\n        # Initialize position and velocity for each agent\n        lb, ub = func.bounds.lb, func.bounds.ub\n        positions = np.random.uniform(lb, ub, (num_agents, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_agents, self.dim))\n\n        # Evaluate initial positions\n        fitness = np.apply_along_axis(func, 1, positions)\n        self.eval_count += num_agents\n\n        best_agent_idx = np.argmin(fitness)\n        global_best_position = positions[best_agent_idx].copy()\n        global_best_value = fitness[best_agent_idx]\n\n        personal_best_positions = positions.copy()\n        personal_best_values = fitness.copy()\n\n        while self.eval_count < self.budget:\n            # Update inertia weight dynamically\n            inertia_weight = inertia_weight_max - (inertia_weight_max - inertia_weight_min) * ((self.eval_count + 1) / self.budget)\n\n            for i in range(num_agents):\n                # Update velocity with an adaptive approach\n                r1, r2 = np.random.rand(2)\n                cognitive_component = exploration_factor * r1 * (personal_best_positions[i] - positions[i])\n                social_component = exploitation_factor * r2 * (global_best_position - positions[i])\n                adaptive_velocity = 0.1 * ((global_best_value - personal_best_values[i]) / (global_best_value + 1e-10)) * velocities[i]  # Changed line\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component + adaptive_velocity\n\n                # Introduce opposition-based learning\n                opposite_position = lb + ub - positions[i]\n                opposite_fitness = func(opposite_position)\n                self.eval_count += 1\n\n                if opposite_fitness < personal_best_values[i]:\n                    positions[i] = opposite_position\n                    personal_best_values[i] = opposite_fitness\n\n                # Update position\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], lb, ub)  # ensure within bounds\n\n                # Evaluate current position\n                current_fitness = func(positions[i])\n                self.eval_count += 1\n\n                # Update personal bests\n                if current_fitness < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_fitness\n\n                # Update global best\n                if current_fitness < global_best_value:\n                    global_best_position = positions[i]\n                    global_best_value = current_fitness\n\n                if self.eval_count >= self.budget:\n                    break\n\n            # Cooperative learning using average of personal bests\n            average_personal_best = np.mean(personal_best_positions, axis=0)\n            for i in range(num_agents):\n                if np.random.rand() < 0.1:  # 10% chance to use the average strategy\n                    r3 = np.random.rand()\n                    positions[i] = r3 * average_personal_best + (1 - r3) * positions[i]\n                    positions[i] = np.clip(positions[i], lb, ub)\n                    current_fitness = func(positions[i])\n                    self.eval_count += 1\n                    if current_fitness < personal_best_values[i]:\n                        personal_best_positions[i] = positions[i]\n                        personal_best_values[i] = current_fitness\n                    if current_fitness < global_best_value:\n                        global_best_position = positions[i]\n                        global_best_value = current_fitness\n\n            # Adaptive parameters\n            exploration_factor = alpha * (1 - self.eval_count / self.budget)\n            exploitation_factor = 1 - exploration_factor\n\n        return global_best_position", "name": "ImprovedEDASOOptimizer", "description": "Introduce an adaptive adjustment to the velocity update rule to enhance convergence speed near the optimum.", "configspace": "", "generation": 20, "fitness": 0.09241322319147267, "feedback": "The algorithm ImprovedEDASOOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.092 with standard deviation 0.001. And the mean value of best solutions found was 0.181 (0. is the best).", "error": "", "parent_id": "e2226140-f426-4d6b-b48a-b01ff98cbd1e", "metadata": {"aucs": [0.09036186038867833, 0.09344551053990946, 0.09343229864583025], "final_y": [0.1884599263924288, 0.1767403623762781, 0.1773019456527235]}, "mutation_prompt": null}
{"id": "6f7e1cc8-2d70-4d1c-ac6b-ed65b9e1865b", "solution": "import numpy as np\n\nclass EnhancedEDASOOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def __call__(self, func):\n        num_agents = 5 + int(0.1 * self.dim)\n        exploration_factor = 0.9\n        exploitation_factor = 0.1\n        inertia_weight_min = 0.4\n        inertia_weight_max = 0.9\n        alpha = 0.5\n\n        lb, ub = func.bounds.lb, func.bounds.ub\n        positions = np.random.uniform(lb, ub, (num_agents, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_agents, self.dim))\n        velocity_scaling_factor = 0.5\n\n        fitness = np.apply_along_axis(func, 1, positions)\n        self.eval_count += num_agents\n\n        best_agent_idx = np.argmin(fitness)\n        global_best_position = positions[best_agent_idx].copy()\n        global_best_value = fitness[best_agent_idx]\n\n        personal_best_positions = positions.copy()\n        personal_best_values = fitness.copy()\n\n        while self.eval_count < self.budget:\n            inertia_weight = inertia_weight_max - (inertia_weight_max - inertia_weight_min) * ((self.eval_count + 1) / self.budget)\n\n            for i in range(num_agents):\n                r1, r2 = np.random.rand(2)\n                cognitive_component = exploration_factor * r1 * (personal_best_positions[i] - positions[i])\n                social_component = exploitation_factor * r2 * (global_best_position - positions[i])\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n\n                velocities[i] *= velocity_scaling_factor\n\n                opposite_position = lb + ub - positions[i]\n                opposite_fitness = func(opposite_position)\n                self.eval_count += 1\n\n                if opposite_fitness < personal_best_values[i]:\n                    positions[i] = opposite_position\n                    personal_best_values[i] = opposite_fitness\n\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], lb, ub)\n\n                current_fitness = func(positions[i])\n                self.eval_count += 1\n\n                if current_fitness < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_fitness\n\n                if current_fitness < global_best_value:\n                    global_best_position = positions[i]\n                    global_best_value = current_fitness\n\n                if self.eval_count >= self.budget:\n                    break\n\n            tournament_size = max(2, num_agents // 3)\n            selected_indices = np.random.choice(num_agents, tournament_size, replace=False)\n            tournament_best_idx = min(selected_indices, key=lambda idx: personal_best_values[idx])\n            global_best_position = personal_best_positions[tournament_best_idx]\n            global_best_value = personal_best_values[tournament_best_idx]\n\n            exploration_factor = alpha * (1 - self.eval_count / self.budget)\n            exploitation_factor = 1 - exploration_factor\n\n        return global_best_position", "name": "EnhancedEDASOOptimizer", "description": "Enhance exploration-exploitation dynamics by incorporating adaptive velocity scaling and tournament selection for diversity preservation.", "configspace": "", "generation": 21, "fitness": 0.08497951080505199, "feedback": "The algorithm EnhancedEDASOOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.085 with standard deviation 0.005. And the mean value of best solutions found was 0.209 (0. is the best).", "error": "", "parent_id": "e2226140-f426-4d6b-b48a-b01ff98cbd1e", "metadata": {"aucs": [0.07844374679031518, 0.0866264435433326, 0.08986834208150818], "final_y": [0.23472209718972414, 0.20151537324329127, 0.1900165931765314]}, "mutation_prompt": null}
{"id": "b8da6caa-d591-4782-9d5c-6e8a21857f17", "solution": "import numpy as np\n\nclass HybridPSODEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_agents = 5 + int(0.1 * self.dim)\n        inertia_weight = 0.7\n        cognitive_coefficient = 1.5\n        social_coefficient = 1.5\n        F = 0.5  # Differential evolution scaling factor\n        CR = 0.9  # Crossover probability\n\n        # Initialize position and velocity for each agent\n        lb, ub = func.bounds.lb, func.bounds.ub\n        positions = np.random.uniform(lb, ub, (num_agents, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_agents, self.dim))\n\n        # Evaluate initial positions\n        fitness = np.apply_along_axis(func, 1, positions)\n        self.eval_count += num_agents\n\n        best_agent_idx = np.argmin(fitness)\n        global_best_position = positions[best_agent_idx].copy()\n        global_best_value = fitness[best_agent_idx]\n\n        personal_best_positions = positions.copy()\n        personal_best_values = fitness.copy()\n\n        while self.eval_count < self.budget:\n            for i in range(num_agents):\n                # Update velocity using PSO\n                r1, r2 = np.random.rand(2)\n                cognitive_component = cognitive_coefficient * r1 * (personal_best_positions[i] - positions[i])\n                social_component = social_coefficient * r2 * (global_best_position - positions[i])\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n\n                # Update position\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], lb, ub)  # ensure within bounds\n\n                # Evaluate current position\n                current_fitness = func(positions[i])\n                self.eval_count += 1\n\n                # Update personal bests\n                if current_fitness < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_fitness\n\n                # Update global best\n                if current_fitness < global_best_value:\n                    global_best_position = positions[i]\n                    global_best_value = current_fitness\n\n                if self.eval_count >= self.budget:\n                    break\n\n            # Differential Evolution-inspired mutation\n            for i in range(num_agents):\n                if np.random.rand() < CR:\n                    indices = np.random.choice(num_agents, 3, replace=False)\n                    x1, x2, x3 = positions[indices]\n                    mutant_vector = x1 + F * (x2 - x3)\n                    trial_vector = np.where(np.random.rand(self.dim) < CR, mutant_vector, positions[i])\n                    trial_vector = np.clip(trial_vector, lb, ub)\n\n                    trial_fitness = func(trial_vector)\n                    self.eval_count += 1\n\n                    # Update personal bests and global best for trial vector\n                    if trial_fitness < personal_best_values[i]:\n                        personal_best_positions[i] = trial_vector\n                        personal_best_values[i] = trial_fitness\n\n                    if trial_fitness < global_best_value:\n                        global_best_position = trial_vector\n                        global_best_value = trial_fitness\n\n                    if self.eval_count >= self.budget:\n                        break\n\n        return global_best_position", "name": "HybridPSODEOptimizer", "description": "Hybrid Particle Swarm Optimization with Differential Evolution-inspired mutation for enhanced exploration and exploitation.", "configspace": "", "generation": 22, "fitness": 0.0952559490663023, "feedback": "The algorithm HybridPSODEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.095 with standard deviation 0.001. And the mean value of best solutions found was 0.165 (0. is the best).", "error": "", "parent_id": "e2226140-f426-4d6b-b48a-b01ff98cbd1e", "metadata": {"aucs": [0.09679594813442471, 0.09375159467263161, 0.09522030439185059], "final_y": [0.16485596312949702, 0.16485578841773818, 0.1648557807891119]}, "mutation_prompt": null}
{"id": "e20c12e6-4561-4bae-a5db-fa52f69a93d5", "solution": "import numpy as np\n\nclass HybridPSODEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_agents = 5 + int(0.1 * self.dim)\n        inertia_weight = 0.9  # Adjusted from 0.7 to 0.9 for improved performance\n        cognitive_coefficient = 1.5\n        social_coefficient = 1.5\n        F = 0.5  # Differential evolution scaling factor\n        CR = 0.9  # Crossover probability\n\n        # Initialize position and velocity for each agent\n        lb, ub = func.bounds.lb, func.bounds.ub\n        positions = np.random.uniform(lb, ub, (num_agents, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_agents, self.dim))\n\n        # Evaluate initial positions\n        fitness = np.apply_along_axis(func, 1, positions)\n        self.eval_count += num_agents\n\n        best_agent_idx = np.argmin(fitness)\n        global_best_position = positions[best_agent_idx].copy()\n        global_best_value = fitness[best_agent_idx]\n\n        personal_best_positions = positions.copy()\n        personal_best_values = fitness.copy()\n\n        while self.eval_count < self.budget:\n            for i in range(num_agents):\n                # Update velocity using PSO\n                r1, r2 = np.random.rand(2)\n                cognitive_component = cognitive_coefficient * r1 * (personal_best_positions[i] - positions[i])\n                social_component = social_coefficient * r2 * (global_best_position - positions[i])\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n\n                # Update position\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], lb, ub)  # ensure within bounds\n\n                # Evaluate current position\n                current_fitness = func(positions[i])\n                self.eval_count += 1\n\n                # Update personal bests\n                if current_fitness < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_fitness\n\n                # Update global best\n                if current_fitness < global_best_value:\n                    global_best_position = positions[i]\n                    global_best_value = current_fitness\n\n                if self.eval_count >= self.budget:\n                    break\n\n            # Differential Evolution-inspired mutation\n            for i in range(num_agents):\n                if np.random.rand() < CR:\n                    indices = np.random.choice(num_agents, 3, replace=False)\n                    x1, x2, x3 = positions[indices]\n                    mutant_vector = x1 + F * (x2 - x3)\n                    trial_vector = np.where(np.random.rand(self.dim) < CR, mutant_vector, positions[i])\n                    trial_vector = np.clip(trial_vector, lb, ub)\n\n                    trial_fitness = func(trial_vector)\n                    self.eval_count += 1\n\n                    # Update personal bests and global best for trial vector\n                    if trial_fitness < personal_best_values[i]:\n                        personal_best_positions[i] = trial_vector\n                        personal_best_values[i] = trial_fitness\n\n                    if trial_fitness < global_best_value:\n                        global_best_position = trial_vector\n                        global_best_value = trial_fitness\n\n                    if self.eval_count >= self.budget:\n                        break\n\n        return global_best_position", "name": "HybridPSODEOptimizer", "description": "Enhanced inertia weight for improved exploration-exploitation balance in hybrid PSO-DE.", "configspace": "", "generation": 23, "fitness": 0.09144204270843548, "feedback": "The algorithm HybridPSODEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.091 with standard deviation 0.001. And the mean value of best solutions found was 0.169 (0. is the best).", "error": "", "parent_id": "b8da6caa-d591-4782-9d5c-6e8a21857f17", "metadata": {"aucs": [0.09229503336017209, 0.08974499923572166, 0.09228609552941269], "final_y": [0.16668006564448634, 0.1685749638659887, 0.17040204035852569]}, "mutation_prompt": null}
{"id": "67f2abaa-b1b7-4deb-a35f-5aa48c422da4", "solution": "import numpy as np\n\nclass HybridPSODEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_agents = 5 + int(0.1 * self.dim)\n        inertia_weight = 0.9 - (self.eval_count / self.budget) * 0.5  # Adaptive inertia weight\n        cognitive_coefficient = 1.5\n        social_coefficient = 1.5\n        F = 0.5  # Differential evolution scaling factor\n        CR = 0.9  # Crossover probability\n\n        # Initialize position and velocity for each agent\n        lb, ub = func.bounds.lb, func.bounds.ub\n        positions = np.random.uniform(lb, ub, (num_agents, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_agents, self.dim))\n\n        # Evaluate initial positions\n        fitness = np.apply_along_axis(func, 1, positions)\n        self.eval_count += num_agents\n\n        best_agent_idx = np.argmin(fitness)\n        global_best_position = positions[best_agent_idx].copy()\n        global_best_value = fitness[best_agent_idx]\n\n        personal_best_positions = positions.copy()\n        personal_best_values = fitness.copy()\n\n        while self.eval_count < self.budget:\n            for i in range(num_agents):\n                # Update velocity using PSO\n                r1, r2 = np.random.rand(2)\n                cognitive_component = cognitive_coefficient * r1 * (personal_best_positions[i] - positions[i])\n                social_component = social_coefficient * r2 * (global_best_position - positions[i])\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n\n                # Update position\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], lb, ub)  # ensure within bounds\n\n                # Evaluate current position\n                current_fitness = func(positions[i])\n                self.eval_count += 1\n\n                # Update personal bests\n                if current_fitness < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_fitness\n\n                # Update global best\n                if current_fitness < global_best_value:\n                    global_best_position = positions[i]\n                    global_best_value = current_fitness\n\n                if self.eval_count >= self.budget:\n                    break\n\n            # Differential Evolution-inspired mutation\n            for i in range(num_agents):\n                if np.random.rand() < CR:\n                    indices = np.random.choice(num_agents, 3, replace=False)\n                    x1, x2, x3 = positions[indices]\n                    mutant_vector = x1 + F * (x2 - x3)\n                    trial_vector = np.where(np.random.rand(self.dim) < CR, mutant_vector, positions[i])\n                    trial_vector = np.clip(trial_vector, lb, ub)\n\n                    trial_fitness = func(trial_vector)\n                    self.eval_count += 1\n\n                    # Update personal bests and global best for trial vector\n                    if trial_fitness < personal_best_values[i]:\n                        personal_best_positions[i] = trial_vector\n                        personal_best_values[i] = trial_fitness\n\n                    if trial_fitness < global_best_value:\n                        global_best_position = trial_vector\n                        global_best_value = trial_fitness\n\n                    if self.eval_count >= self.budget:\n                        break\n\n        return global_best_position", "name": "HybridPSODEOptimizer", "description": "Introduce adaptive inertia weight to the Hybrid Particle Swarm Optimization with Differential Evolution-inspired mutation for enhanced convergence.", "configspace": "", "generation": 24, "fitness": 0.09144204270843548, "feedback": "The algorithm HybridPSODEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.091 with standard deviation 0.001. And the mean value of best solutions found was 0.169 (0. is the best).", "error": "", "parent_id": "b8da6caa-d591-4782-9d5c-6e8a21857f17", "metadata": {"aucs": [0.09229503336017209, 0.08974499923572166, 0.09228609552941269], "final_y": [0.16668006564448634, 0.1685749638659887, 0.17040204035852569]}, "mutation_prompt": null}
{"id": "d614598f-5630-4a11-b266-ec21d2f91ac7", "solution": "import numpy as np\n\nclass HybridPSODEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_agents = 5 + int(0.1 * self.dim)\n        inertia_weight = 0.9  # Increased for exploration\n        cognitive_coefficient = 1.5\n        social_coefficient = 1.5\n        F = 0.6  # Increased DE scaling factor\n        CR = 0.9  # Crossover probability\n\n        # Initialize position and velocity for each agent\n        lb, ub = func.bounds.lb, func.bounds.ub\n        positions = np.random.uniform(lb, ub, (num_agents, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_agents, self.dim))\n\n        # Evaluate initial positions\n        fitness = np.apply_along_axis(func, 1, positions)\n        self.eval_count += num_agents\n\n        best_agent_idx = np.argmin(fitness)\n        global_best_position = positions[best_agent_idx].copy()\n        global_best_value = fitness[best_agent_idx]\n\n        personal_best_positions = positions.copy()\n        personal_best_values = fitness.copy()\n\n        while self.eval_count < self.budget:\n            for i in range(num_agents):\n                # Update velocity using PSO\n                r1, r2 = np.random.rand(2)\n                cognitive_component = cognitive_coefficient * r1 * (personal_best_positions[i] - positions[i])\n                social_component = social_coefficient * r2 * (global_best_position - positions[i])\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n\n                # Update position\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], lb, ub)  # ensure within bounds\n\n                # Evaluate current position\n                current_fitness = func(positions[i])\n                self.eval_count += 1\n\n                # Update personal bests\n                if current_fitness < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_fitness\n\n                # Update global best\n                if current_fitness < global_best_value:\n                    global_best_position = positions[i]\n                    global_best_value = current_fitness\n\n                if self.eval_count >= self.budget:\n                    break\n\n            # Differential Evolution-inspired mutation\n            for i in range(num_agents):\n                if np.random.rand() < CR:\n                    indices = np.random.choice(num_agents, 3, replace=False)\n                    x1, x2, x3 = positions[indices]\n                    mutant_vector = x1 + F * (x2 - x3)\n                    trial_vector = np.where(np.random.rand(self.dim) < CR, mutant_vector, positions[i])\n                    trial_vector = np.clip(trial_vector, lb, ub)\n\n                    trial_fitness = func(trial_vector)\n                    self.eval_count += 1\n\n                    # Update personal bests and global best for trial vector\n                    if trial_fitness < personal_best_values[i]:\n                        personal_best_positions[i] = trial_vector\n                        personal_best_values[i] = trial_fitness\n\n                    if trial_fitness < global_best_value:\n                        global_best_position = trial_vector\n                        global_best_value = trial_fitness\n\n                    if self.eval_count >= self.budget:\n                        break\n\n            # Adaptive inertia weight to balance exploration and exploitation\n            inertia_weight = 0.9 - (0.7 * (self.eval_count / self.budget))\n\n        return global_best_position", "name": "HybridPSODEOptimizer", "description": "Enhanced Hybrid PSO-DE with adaptive inertia and mutation parameters for improved convergence efficiency.", "configspace": "", "generation": 25, "fitness": 0.0796565162823381, "feedback": "The algorithm HybridPSODEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.080 with standard deviation 0.021. And the mean value of best solutions found was 0.240 (0. is the best).", "error": "", "parent_id": "b8da6caa-d591-4782-9d5c-6e8a21857f17", "metadata": {"aucs": [0.09501416355482484, 0.05044110678191449, 0.09351427851027494], "final_y": [0.1648560288945926, 0.3900954547030716, 0.16485594647981006]}, "mutation_prompt": null}
{"id": "d8884b84-ae89-4e51-8736-90d22cb35f5f", "solution": "import numpy as np\n\nclass HybridPSODEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_agents = 5 + int(0.1 * self.dim)\n        inertia_weight = 0.9  # Start with a higher inertia weight\n        cognitive_coefficient = 1.5\n        social_coefficient = 1.5\n        F = 0.5  # Differential evolution scaling factor\n        CR = 0.8  # Reduced crossover probability\n\n        # Initialize position and velocity for each agent\n        lb, ub = func.bounds.lb, func.bounds.ub\n        positions = np.random.uniform(lb, ub, (num_agents, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_agents, self.dim))\n\n        # Evaluate initial positions\n        fitness = np.apply_along_axis(func, 1, positions)\n        self.eval_count += num_agents\n\n        best_agent_idx = np.argmin(fitness)\n        global_best_position = positions[best_agent_idx].copy()\n        global_best_value = fitness[best_agent_idx]\n\n        personal_best_positions = positions.copy()\n        personal_best_values = fitness.copy()\n\n        while self.eval_count < self.budget:\n            inertia_weight = 0.4 + 0.5 * (self.budget - self.eval_count) / self.budget  # Adaptive inertia weight\n            for i in range(num_agents):\n                # Update velocity using PSO\n                r1, r2 = np.random.rand(2)\n                cognitive_component = cognitive_coefficient * r1 * (personal_best_positions[i] - positions[i])\n                social_component = social_coefficient * r2 * (global_best_position - positions[i])\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n\n                # Update position\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], lb, ub)  # ensure within bounds\n\n                # Evaluate current position\n                current_fitness = func(positions[i])\n                self.eval_count += 1\n\n                # Update personal bests\n                if current_fitness < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_fitness\n\n                # Update global best\n                if current_fitness < global_best_value:\n                    global_best_position = positions[i]\n                    global_best_value = current_fitness\n\n                if self.eval_count >= self.budget:\n                    break\n\n            # Differential Evolution-inspired mutation\n            for i in range(num_agents):\n                if np.random.rand() < CR:\n                    indices = np.random.choice(num_agents, 3, replace=False)\n                    x1, x2, x3 = positions[indices]\n                    mutant_vector = x1 + F * (x2 - x3)\n                    trial_vector = np.where(np.random.rand(self.dim) < CR, mutant_vector, positions[i])\n                    trial_vector = np.clip(trial_vector, lb, ub)\n\n                    trial_fitness = func(trial_vector)\n                    self.eval_count += 1\n\n                    # Update personal bests and global best for trial vector\n                    if trial_fitness < personal_best_values[i]:\n                        personal_best_positions[i] = trial_vector\n                        personal_best_values[i] = trial_fitness\n\n                    if trial_fitness < global_best_value:\n                        global_best_position = trial_vector\n                        global_best_value = trial_fitness\n\n                    if self.eval_count >= self.budget:\n                        break\n\n        return global_best_position", "name": "HybridPSODEOptimizer", "description": "Hybrid PSO with adaptive inertia weight and randomized differential evolution crossover for enhanced optimization efficiency.", "configspace": "", "generation": 26, "fitness": 0.07983187698450682, "feedback": "The algorithm HybridPSODEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.080 with standard deviation 0.021. And the mean value of best solutions found was 0.240 (0. is the best).", "error": "", "parent_id": "b8da6caa-d591-4782-9d5c-6e8a21857f17", "metadata": {"aucs": [0.09554892850045527, 0.050353305135455995, 0.09359339731760918], "final_y": [0.16485577235563476, 0.39009545470303986, 0.16485687675079963]}, "mutation_prompt": null}
{"id": "1a3b456b-565a-41ab-8000-aac887275fc5", "solution": "import numpy as np\n\nclass HybridPSODEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_agents = 5 + int(0.1 * self.dim)\n        inertia_weight = 0.7\n        cognitive_coefficient = 1.5\n        social_coefficient = 1.5\n        F = 0.5  # Differential evolution scaling factor\n        CR = 0.9  # Crossover probability\n\n        # Initialize position and velocity for each agent\n        lb, ub = func.bounds.lb, func.bounds.ub\n        positions = np.random.uniform(lb, ub, (num_agents, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_agents, self.dim))\n\n        # Evaluate initial positions\n        fitness = np.apply_along_axis(func, 1, positions)\n        self.eval_count += num_agents\n\n        best_agent_idx = np.argmin(fitness)\n        global_best_position = positions[best_agent_idx].copy()\n        global_best_value = fitness[best_agent_idx]\n\n        personal_best_positions = positions.copy()\n        personal_best_values = fitness.copy()\n\n        while self.eval_count < self.budget:\n            inertia_weight = 0.9 - 0.5 * (self.eval_count / self.budget)  # Adaptive inertia weight\n            \n            for i in range(num_agents):\n                # Update velocity using PSO\n                r1, r2 = np.random.rand(2)\n                cognitive_component = cognitive_coefficient * r1 * (personal_best_positions[i] - positions[i])\n                social_component = social_coefficient * r2 * (global_best_position - positions[i])\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n\n                # Update position\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], lb, ub)  # ensure within bounds\n\n                # Evaluate current position\n                current_fitness = func(positions[i])\n                self.eval_count += 1\n\n                # Update personal bests\n                if current_fitness < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_fitness\n\n                # Update global best\n                if current_fitness < global_best_value:\n                    global_best_position = positions[i]\n                    global_best_value = current_fitness\n\n                if self.eval_count >= self.budget:\n                    break\n\n            # Differential Evolution-inspired mutation\n            for i in range(num_agents):\n                if np.random.rand() < CR:\n                    indices = np.random.choice(num_agents, 3, replace=False)\n                    x1, x2, x3 = positions[indices]\n                    mutant_vector = x1 + F * (x2 - x3)\n                    trial_vector = np.where(np.random.rand(self.dim) < CR, mutant_vector, positions[i])\n                    trial_vector = np.clip(trial_vector, lb, ub)\n\n                    trial_fitness = func(trial_vector)\n                    self.eval_count += 1\n\n                    # Update personal bests and global best for trial vector\n                    if trial_fitness < personal_best_values[i]:\n                        personal_best_positions[i] = trial_vector\n                        personal_best_values[i] = trial_fitness\n\n                    if trial_fitness < global_best_value:\n                        global_best_position = trial_vector\n                        global_best_value = trial_fitness\n\n                    if self.eval_count >= self.budget:\n                        break\n\n        return global_best_position", "name": "HybridPSODEOptimizer", "description": "Introduce adaptive inertia weight to improve convergence speed and avoid premature convergence.", "configspace": "", "generation": 27, "fitness": 0.0875227615136206, "feedback": "The algorithm HybridPSODEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.088 with standard deviation 0.011. And the mean value of best solutions found was 0.196 (0. is the best).", "error": "", "parent_id": "b8da6caa-d591-4782-9d5c-6e8a21857f17", "metadata": {"aucs": [0.09471387659501163, 0.0723065974631576, 0.09554781048269256], "final_y": [0.16485585897587363, 0.2578100821104141, 0.16485586847342926]}, "mutation_prompt": null}
{"id": "dcd431f4-3894-4abe-aba8-a08e26fce3b8", "solution": "import numpy as np\n\nclass HybridPSODEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_agents = 5 + int(0.1 * self.dim)\n        inertia_weight = 0.7\n        cognitive_coefficient = 1.5\n        social_coefficient = 1.5\n        F = 0.5  # Differential evolution scaling factor\n        CR = 0.9  # Crossover probability\n\n        # Initialize position and velocity for each agent\n        lb, ub = func.bounds.lb, func.bounds.ub\n        positions = np.random.uniform(lb, ub, (num_agents, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_agents, self.dim))\n\n        # Evaluate initial positions\n        fitness = np.apply_along_axis(func, 1, positions)\n        self.eval_count += num_agents\n\n        best_agent_idx = np.argmin(fitness)\n        global_best_position = positions[best_agent_idx].copy()\n        global_best_value = fitness[best_agent_idx]\n\n        personal_best_positions = positions.copy()\n        personal_best_values = fitness.copy()\n\n        while self.eval_count < self.budget:\n            for i in range(num_agents):\n                # Adaptive inertia weight\n                inertia_weight = 0.9 - (0.5 * (self.eval_count / self.budget)) \n\n                # Update velocity using PSO\n                r1, r2 = np.random.rand(2)\n                cognitive_component = cognitive_coefficient * r1 * (personal_best_positions[i] - positions[i])\n                social_component = social_coefficient * r2 * (global_best_position - positions[i])\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n\n                # Update position\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], lb, ub)  # ensure within bounds\n\n                # Evaluate current position\n                current_fitness = func(positions[i])\n                self.eval_count += 1\n\n                # Update personal bests\n                if current_fitness < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_fitness\n\n                # Update global best\n                if current_fitness < global_best_value:\n                    global_best_position = positions[i]\n                    global_best_value = current_fitness\n\n                if self.eval_count >= self.budget:\n                    break\n\n            # Differential Evolution-inspired mutation with adaptive F\n            for i in range(num_agents):\n                if np.random.rand() < CR:\n                    indices = np.random.choice(num_agents, 3, replace=False)\n                    x1, x2, x3 = positions[indices]\n                    F = 0.4 + 0.1 * (self.eval_count / self.budget)  # Adaptive scaling factor\n                    mutant_vector = x1 + F * (x2 - x3)\n                    trial_vector = np.where(np.random.rand(self.dim) < CR, mutant_vector, positions[i])\n                    trial_vector = np.clip(trial_vector, lb, ub)\n\n                    trial_fitness = func(trial_vector)\n                    self.eval_count += 1\n\n                    # Update personal bests and global best for trial vector\n                    if trial_fitness < personal_best_values[i]:\n                        personal_best_positions[i] = trial_vector\n                        personal_best_values[i] = trial_fitness\n\n                    if trial_fitness < global_best_value:\n                        global_best_position = trial_vector\n                        global_best_value = trial_fitness\n\n                    if self.eval_count >= self.budget:\n                        break\n\n        return global_best_position", "name": "HybridPSODEOptimizer", "description": "Enhanced HybridPSODEOptimizer with adaptive inertia and mutation for improved convergence.", "configspace": "", "generation": 28, "fitness": 0.08730165562354497, "feedback": "The algorithm HybridPSODEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.087 with standard deviation 0.011. And the mean value of best solutions found was 0.196 (0. is the best).", "error": "", "parent_id": "b8da6caa-d591-4782-9d5c-6e8a21857f17", "metadata": {"aucs": [0.09404289602667393, 0.07220396942084972, 0.09565810142311126], "final_y": [0.16485607491928045, 0.25781008208797684, 0.16485577447192357]}, "mutation_prompt": null}
{"id": "fdb58f33-2028-465e-83ea-0a0e32197b5c", "solution": "import numpy as np\n\nclass HybridPSODEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_agents = 5 + int(0.1 * self.dim)\n        inertia_weight = 0.9  # Adjusted initial inertia weight for better exploration\n        cognitive_coefficient = 1.5\n        social_coefficient = 1.5\n        F = 0.5  # Differential evolution scaling factor\n        CR = 0.9  # Crossover probability\n\n        # Initialize position and velocity for each agent\n        lb, ub = func.bounds.lb, func.bounds.ub\n        positions = np.random.uniform(lb, ub, (num_agents, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_agents, self.dim))\n\n        # Evaluate initial positions\n        fitness = np.apply_along_axis(func, 1, positions)\n        self.eval_count += num_agents\n\n        best_agent_idx = np.argmin(fitness)\n        global_best_position = positions[best_agent_idx].copy()\n        global_best_value = fitness[best_agent_idx]\n\n        personal_best_positions = positions.copy()\n        personal_best_values = fitness.copy()\n\n        while self.eval_count < self.budget:\n            for i in range(num_agents):\n                # Update velocity using PSO\n                r1, r2 = np.random.rand(2)\n                cognitive_component = cognitive_coefficient * r1 * (personal_best_positions[i] - positions[i])\n                social_component = social_coefficient * r2 * (global_best_position - positions[i])\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n\n                # Update position\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], lb, ub)  # ensure within bounds\n\n                # Evaluate current position\n                current_fitness = func(positions[i])\n                self.eval_count += 1\n\n                # Update personal bests\n                if current_fitness < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_fitness\n\n                # Update global best\n                if current_fitness < global_best_value:\n                    global_best_position = positions[i]\n                    global_best_value = current_fitness\n\n                if self.eval_count >= self.budget:\n                    break\n\n            # Differential Evolution-inspired mutation\n            for i in range(num_agents):\n                if np.random.rand() < CR:\n                    indices = np.random.choice(num_agents, 3, replace=False)\n                    x1, x2, x3 = positions[indices]\n                    mutant_vector = x1 + F * (x2 - x3)\n                    trial_vector = np.where(np.random.rand(self.dim) < CR, mutant_vector, positions[i])\n                    trial_vector = np.clip(trial_vector, lb, ub)\n\n                    trial_fitness = func(trial_vector)\n                    self.eval_count += 1\n\n                    # Update personal bests and global best for trial vector\n                    if trial_fitness < personal_best_values[i]:\n                        personal_best_positions[i] = trial_vector\n                        personal_best_values[i] = trial_fitness\n\n                    if trial_fitness < global_best_value:\n                        global_best_position = trial_vector\n                        global_best_value = trial_fitness\n\n                    if self.eval_count >= self.budget:\n                        break\n\n        return global_best_position", "name": "HybridPSODEOptimizer", "description": "Improved exploration by introducing adaptive inertia weight in PSO to enhance global search capability.", "configspace": "", "generation": 29, "fitness": 0.09144204270843548, "feedback": "The algorithm HybridPSODEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.091 with standard deviation 0.001. And the mean value of best solutions found was 0.169 (0. is the best).", "error": "", "parent_id": "b8da6caa-d591-4782-9d5c-6e8a21857f17", "metadata": {"aucs": [0.09229503336017209, 0.08974499923572166, 0.09228609552941269], "final_y": [0.16668006564448634, 0.1685749638659887, 0.17040204035852569]}, "mutation_prompt": null}
{"id": "3bf2ef8d-2362-467f-81f7-6de48386bf0c", "solution": "import numpy as np\n\nclass HybridPSODEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_agents = 5 + int(0.1 * self.dim)\n        inertia_weight = 0.9  # Start higher for exploration\n        inertia_weight_decay = 0.99  # Decaying inertia\n        cognitive_coefficient = 1.5\n        social_coefficient = 1.5\n        F = 0.8  # Increased DE scaling factor\n        CR = 0.7  # Reduced crossover probability for exploration\n\n        # Initialize position and velocity for each agent\n        lb, ub = func.bounds.lb, func.bounds.ub\n        positions = np.random.uniform(lb, ub, (num_agents, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_agents, self.dim))\n\n        # Evaluate initial positions\n        fitness = np.apply_along_axis(func, 1, positions)\n        self.eval_count += num_agents\n\n        best_agent_idx = np.argmin(fitness)\n        global_best_position = positions[best_agent_idx].copy()\n        global_best_value = fitness[best_agent_idx]\n\n        personal_best_positions = positions.copy()\n        personal_best_values = fitness.copy()\n\n        while self.eval_count < self.budget:\n            for i in range(num_agents):\n                # Update velocity using PSO\n                r1, r2 = np.random.rand(2)\n                cognitive_component = cognitive_coefficient * r1 * (personal_best_positions[i] - positions[i])\n                social_component = social_coefficient * r2 * (global_best_position - positions[i])\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n\n                # Update position\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], lb, ub)  # ensure within bounds\n\n                # Evaluate current position\n                current_fitness = func(positions[i])\n                self.eval_count += 1\n\n                # Update personal bests\n                if current_fitness < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_fitness\n\n                # Update global best\n                if current_fitness < global_best_value:\n                    global_best_position = positions[i]\n                    global_best_value = current_fitness\n\n                if self.eval_count >= self.budget:\n                    break\n\n            # Differential Evolution-inspired mutation\n            for i in range(num_agents):\n                if np.random.rand() < CR:\n                    indices = np.random.choice(num_agents, 3, replace=False)\n                    x1, x2, x3 = positions[indices]\n                    mutant_vector = x1 + F * (x2 - x3)\n                    trial_vector = np.where(np.random.rand(self.dim) < CR, mutant_vector, positions[i])\n                    trial_vector = np.clip(trial_vector, lb, ub)\n\n                    trial_fitness = func(trial_vector)\n                    self.eval_count += 1\n\n                    # Update personal bests and global best for trial vector\n                    if trial_fitness < personal_best_values[i]:\n                        personal_best_positions[i] = trial_vector\n                        personal_best_values[i] = trial_fitness\n\n                    if trial_fitness < global_best_value:\n                        global_best_position = trial_vector\n                        global_best_value = trial_fitness\n\n                    if self.eval_count >= self.budget:\n                        break\n\n            inertia_weight *= inertia_weight_decay  # Decay inertia weight\n\n        return global_best_position", "name": "HybridPSODEOptimizer", "description": "Enhanced Hybrid PSO with adaptive inertia and dynamic DE parameters for improved convergence.", "configspace": "", "generation": 30, "fitness": 0.08647394766657741, "feedback": "The algorithm HybridPSODEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.086 with standard deviation 0.012. And the mean value of best solutions found was 0.201 (0. is the best).", "error": "", "parent_id": "b8da6caa-d591-4782-9d5c-6e8a21857f17", "metadata": {"aucs": [0.09321577382833734, 0.0702779222569605, 0.09592814691443441], "final_y": [0.17452640344582493, 0.2623243556464858, 0.16529341684128607]}, "mutation_prompt": null}
{"id": "b53c54b4-9eef-49e4-9253-9c4d3cafe582", "solution": "import numpy as np\n\nclass HybridPSODEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_agents = 5 + int(0.1 * self.dim)\n        inertia_weight_initial = 0.9\n        inertia_weight_final = 0.4\n        cognitive_coefficient = 1.5\n        social_coefficient = 1.5\n        F = 0.5  # Differential evolution scaling factor\n        CR = 0.9  # Crossover probability\n\n        # Initialize position and velocity for each agent\n        lb, ub = func.bounds.lb, func.bounds.ub\n        positions = np.random.uniform(lb, ub, (num_agents, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_agents, self.dim))\n\n        # Evaluate initial positions\n        fitness = np.apply_along_axis(func, 1, positions)\n        self.eval_count += num_agents\n\n        best_agent_idx = np.argmin(fitness)\n        global_best_position = positions[best_agent_idx].copy()\n        global_best_value = fitness[best_agent_idx]\n\n        personal_best_positions = positions.copy()\n        personal_best_values = fitness.copy()\n\n        while self.eval_count < self.budget:\n            # Dynamic inertia weight\n            inertia_weight = inertia_weight_initial - (inertia_weight_initial - inertia_weight_final) * \\\n                             (self.eval_count / self.budget)\n            \n            for i in range(num_agents):\n                # Update velocity using PSO\n                r1, r2 = np.random.rand(2)\n                cognitive_component = cognitive_coefficient * r1 * (personal_best_positions[i] - positions[i])\n                social_component = social_coefficient * r2 * (global_best_position - positions[i])\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n\n                # Update position\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], lb, ub)  # ensure within bounds\n\n                # Evaluate current position\n                current_fitness = func(positions[i])\n                self.eval_count += 1\n\n                # Update personal bests\n                if current_fitness < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_fitness\n\n                # Update global best\n                if current_fitness < global_best_value:\n                    global_best_position = positions[i]\n                    global_best_value = current_fitness\n\n                if self.eval_count >= self.budget:\n                    break\n\n            # Differential Evolution-inspired mutation\n            for i in range(num_agents):\n                if np.random.rand() < CR:\n                    indices = np.random.choice(num_agents, 3, replace=False)\n                    x1, x2, x3 = positions[indices]\n                    mutant_vector = x1 + F * (x2 - x3)\n                    trial_vector = np.where(np.random.rand(self.dim) < CR, mutant_vector, positions[i])\n                    trial_vector = np.clip(trial_vector, lb, ub)\n\n                    trial_fitness = func(trial_vector)\n                    self.eval_count += 1\n\n                    # Update personal bests and global best for trial vector\n                    if trial_fitness < personal_best_values[i]:\n                        personal_best_positions[i] = trial_vector\n                        personal_best_values[i] = trial_fitness\n\n                    if trial_fitness < global_best_value:\n                        global_best_position = trial_vector\n                        global_best_value = trial_fitness\n\n                    if self.eval_count >= self.budget:\n                        break\n\n        return global_best_position", "name": "HybridPSODEOptimizer", "description": "Enhanced HybridPSODEOptimizer with adaptive inertia weight for improved convergence performance.", "configspace": "", "generation": 31, "fitness": 0.0875227615136206, "feedback": "The algorithm HybridPSODEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.088 with standard deviation 0.011. And the mean value of best solutions found was 0.196 (0. is the best).", "error": "", "parent_id": "b8da6caa-d591-4782-9d5c-6e8a21857f17", "metadata": {"aucs": [0.09471387659501163, 0.0723065974631576, 0.09554781048269256], "final_y": [0.16485585897587363, 0.2578100821104141, 0.16485586847342926]}, "mutation_prompt": null}
{"id": "8ac1adcb-fef3-4eeb-b8ee-cb65926a283a", "solution": "import numpy as np\n\nclass HybridPSODEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_agents = 5 + int(0.1 * self.dim)\n        inertia_weight = 0.9  # Modified for adaptive inertia\n        inertia_weight_min = 0.4  # Added for adaptive computation\n        inertia_weight_max = 0.9  # Added for adaptive computation\n        cognitive_coefficient = 1.5\n        social_coefficient = 1.5\n        F = 0.5  # Differential evolution scaling factor\n        CR = 0.9  # Crossover probability\n\n        # Initialize position and velocity for each agent\n        lb, ub = func.bounds.lb, func.bounds.ub\n        positions = np.random.uniform(lb, ub, (num_agents, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_agents, self.dim))\n\n        # Evaluate initial positions\n        fitness = np.apply_along_axis(func, 1, positions)\n        self.eval_count += num_agents\n\n        best_agent_idx = np.argmin(fitness)\n        global_best_position = positions[best_agent_idx].copy()\n        global_best_value = fitness[best_agent_idx]\n\n        personal_best_positions = positions.copy()\n        personal_best_values = fitness.copy()\n\n        while self.eval_count < self.budget:\n            for i in range(num_agents):\n                # Update velocity using PSO\n                r1, r2 = np.random.rand(2)\n                inertia_weight = inertia_weight_max - (inertia_weight_max - inertia_weight_min) * (self.eval_count / self.budget) # Adaptive Inertia\n                cognitive_component = cognitive_coefficient * r1 * (personal_best_positions[i] - positions[i])\n                social_component = social_coefficient * r2 * (global_best_position - positions[i])\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n\n                # Update position\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], lb, ub)  # ensure within bounds\n\n                # Evaluate current position\n                current_fitness = func(positions[i])\n                self.eval_count += 1\n\n                # Update personal bests\n                if current_fitness < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_fitness\n\n                # Update global best\n                if current_fitness < global_best_value:\n                    global_best_position = positions[i]\n                    global_best_value = current_fitness\n\n                if self.eval_count >= self.budget:\n                    break\n\n            # Differential Evolution-inspired mutation\n            for i in range(num_agents):\n                if np.random.rand() < CR:\n                    indices = np.random.choice(num_agents, 3, replace=False)\n                    x1, x2, x3 = positions[indices]\n                    mutant_vector = x1 + F * (x2 - x3)\n                    trial_vector = np.where(np.random.rand(self.dim) < CR, mutant_vector, positions[i])\n                    trial_vector = np.clip(trial_vector, lb, ub)\n\n                    trial_fitness = func(trial_vector)\n                    self.eval_count += 1\n\n                    # Local search around successful trial vector\n                    if trial_fitness < personal_best_values[i]:\n                        local_search_vector = trial_vector + np.random.uniform(-0.1, 0.1, self.dim)  # Add local search\n                        local_search_vector = np.clip(local_search_vector, lb, ub)\n                        local_search_fitness = func(local_search_vector)\n                        self.eval_count += 1\n                        if local_search_fitness < trial_fitness:\n                            trial_vector = local_search_vector\n                            trial_fitness = local_search_fitness\n\n                    # Update personal bests and global best for trial vector\n                    if trial_fitness < personal_best_values[i]:\n                        personal_best_positions[i] = trial_vector\n                        personal_best_values[i] = trial_fitness\n\n                    if trial_fitness < global_best_value:\n                        global_best_position = trial_vector\n                        global_best_value = trial_fitness\n\n                    if self.eval_count >= self.budget:\n                        break\n\n        return global_best_position", "name": "HybridPSODEOptimizer", "description": "Enhanced Hybrid PSO with Adaptive Inertia Weight and Local Search to improve convergence and solution quality.", "configspace": "", "generation": 32, "fitness": 0.08312717970694161, "feedback": "The algorithm HybridPSODEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.083 with standard deviation 0.014. And the mean value of best solutions found was 0.196 (0. is the best).", "error": "", "parent_id": "b8da6caa-d591-4782-9d5c-6e8a21857f17", "metadata": {"aucs": [0.09479706129842236, 0.06310090707769611, 0.09148357074470637], "final_y": [0.16485578559942093, 0.2578100867093799, 0.16485577310559563]}, "mutation_prompt": null}
{"id": "cd8dd040-83f4-4b00-b0f8-c3822b77a2d5", "solution": "import numpy as np\n\nclass HybridPSODEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_agents = 5 + int(0.1 * self.dim)\n        inertia_weight = 0.9  # Changed from 0.7 to 0.9\n        cognitive_coefficient = 1.5\n        social_coefficient = 1.5\n        F = 0.5  # Differential evolution scaling factor\n        CR = 0.9  # Crossover probability\n\n        # Initialize position and velocity for each agent\n        lb, ub = func.bounds.lb, func.bounds.ub\n        positions = np.random.uniform(lb, ub, (num_agents, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_agents, self.dim))\n\n        # Evaluate initial positions\n        fitness = np.apply_along_axis(func, 1, positions)\n        self.eval_count += num_agents\n\n        best_agent_idx = np.argmin(fitness)\n        global_best_position = positions[best_agent_idx].copy()\n        global_best_value = fitness[best_agent_idx]\n\n        personal_best_positions = positions.copy()\n        personal_best_values = fitness.copy()\n\n        while self.eval_count < self.budget:\n            for i in range(num_agents):\n                # Update velocity using PSO\n                r1, r2 = np.random.rand(2)\n                cognitive_component = cognitive_coefficient * r1 * (personal_best_positions[i] - positions[i])\n                social_component = social_coefficient * r2 * (global_best_position - positions[i])\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n\n                # Update position\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], lb, ub)  # ensure within bounds\n\n                # Evaluate current position\n                current_fitness = func(positions[i])\n                self.eval_count += 1\n\n                # Update personal bests\n                if current_fitness < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_fitness\n\n                # Update global best\n                if current_fitness < global_best_value:\n                    global_best_position = positions[i]\n                    global_best_value = current_fitness\n\n                if self.eval_count >= self.budget:\n                    break\n\n            # Differential Evolution-inspired mutation\n            for i in range(num_agents):\n                if np.random.rand() < CR:\n                    indices = np.random.choice(num_agents, 3, replace=False)\n                    x1, x2, x3 = positions[indices]\n                    mutant_vector = x1 + F * (x2 - x3)\n                    trial_vector = np.where(np.random.rand(self.dim) < CR, mutant_vector, positions[i])\n                    trial_vector = np.clip(trial_vector, lb, ub)\n\n                    trial_fitness = func(trial_vector)\n                    self.eval_count += 1\n\n                    # Update personal bests and global best for trial vector\n                    if trial_fitness < personal_best_values[i]:\n                        personal_best_positions[i] = trial_vector\n                        personal_best_values[i] = trial_fitness\n\n                    if trial_fitness < global_best_value:\n                        global_best_position = trial_vector\n                        global_best_value = trial_fitness\n\n                    if self.eval_count >= self.budget:\n                        break\n\n        return global_best_position", "name": "HybridPSODEOptimizer", "description": "Adjusted the inertia weight for dynamic control to enhance the algorithm's adaptability and convergence rate.", "configspace": "", "generation": 33, "fitness": 0.09144204270843548, "feedback": "The algorithm HybridPSODEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.091 with standard deviation 0.001. And the mean value of best solutions found was 0.169 (0. is the best).", "error": "", "parent_id": "b8da6caa-d591-4782-9d5c-6e8a21857f17", "metadata": {"aucs": [0.09229503336017209, 0.08974499923572166, 0.09228609552941269], "final_y": [0.16668006564448634, 0.1685749638659887, 0.17040204035852569]}, "mutation_prompt": null}
{"id": "aea81924-c28e-4389-b6c4-d92e8d868f20", "solution": "import numpy as np\n\nclass HybridPSODEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_agents = 5 + int(0.1 * self.dim)\n        inertia_weight = 0.9  # Adaptive inertia weight\n        cognitive_coefficient = 1.5\n        social_coefficient = 1.5\n        F = 0.5  # Differential evolution scaling factor\n        CR = 0.9  # Crossover probability\n\n        # Initialize position and velocity for each agent\n        lb, ub = func.bounds.lb, func.bounds.ub\n        positions = np.random.uniform(lb, ub, (num_agents, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_agents, self.dim))\n\n        # Evaluate initial positions\n        fitness = np.apply_along_axis(func, 1, positions)\n        self.eval_count += num_agents\n\n        best_agent_idx = np.argmin(fitness)\n        global_best_position = positions[best_agent_idx].copy()\n        global_best_value = fitness[best_agent_idx]\n\n        personal_best_positions = positions.copy()\n        personal_best_values = fitness.copy()\n\n        while self.eval_count < self.budget:\n            inertia_weight = 0.9 - 0.7 * (self.eval_count / self.budget)  # Adaptive inertia\n            for i in range(num_agents):\n                # Update velocity using PSO\n                r1, r2 = np.random.rand(2)\n                cognitive_component = cognitive_coefficient * r1 * (personal_best_positions[i] - positions[i])\n                social_component = social_coefficient * r2 * (global_best_position - positions[i])\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n\n                # Update position\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], lb, ub)  # ensure within bounds\n\n                # Evaluate current position\n                current_fitness = func(positions[i])\n                self.eval_count += 1\n\n                # Update personal bests\n                if current_fitness < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_fitness\n\n                # Update global best\n                if current_fitness < global_best_value:\n                    global_best_position = positions[i]\n                    global_best_value = current_fitness\n\n                if self.eval_count >= self.budget:\n                    break\n\n            # Differential Evolution-inspired mutation\n            for i in range(num_agents):\n                if np.random.rand() < CR:\n                    indices = np.random.choice(num_agents, 3, replace=False)\n                    x1, x2, x3 = positions[indices]\n                    mutant_vector = x1 + F * (x2 - x3)\n                    trial_vector = np.where(np.random.rand(self.dim) < CR, mutant_vector, positions[i])\n                    trial_vector = np.clip(trial_vector, lb, ub)\n\n                    trial_fitness = func(trial_vector)\n                    self.eval_count += 1\n\n                    # Update personal bests and global best for trial vector\n                    if trial_fitness < personal_best_values[i]:\n                        personal_best_positions[i] = trial_vector\n                        personal_best_values[i] = trial_fitness\n\n                    if trial_fitness < global_best_value:\n                        global_best_position = trial_vector\n                        global_best_value = trial_fitness\n\n                    if self.eval_count >= self.budget:\n                        break\n\n            # Introduce dynamic agent restart\n            if self.eval_count % (self.budget // 5) == 0:\n                worst_agent_idx = np.argmax(personal_best_values)\n                positions[worst_agent_idx] = np.random.uniform(lb, ub, self.dim)\n                velocities[worst_agent_idx] = np.random.uniform(-1, 1, self.dim)\n                personal_best_values[worst_agent_idx] = func(positions[worst_agent_idx])\n                self.eval_count += 1\n\n        return global_best_position", "name": "HybridPSODEOptimizer", "description": "Enhanced Hybrid PSO with adaptive inertia and dynamic agent restart for improved convergence.", "configspace": "", "generation": 34, "fitness": 0.09405474257851976, "feedback": "The algorithm HybridPSODEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.094 with standard deviation 0.002. And the mean value of best solutions found was 0.165 (0. is the best).", "error": "", "parent_id": "b8da6caa-d591-4782-9d5c-6e8a21857f17", "metadata": {"aucs": [0.09480654415485457, 0.09181083598393891, 0.0955468475967658], "final_y": [0.16485606077049053, 0.16485591088299822, 0.1648557965519658]}, "mutation_prompt": null}
{"id": "0ba69d57-2b3c-4391-847f-129f8d0b93bf", "solution": "import numpy as np\n\nclass HybridPSODEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_agents = 5 + int(0.1 * self.dim)\n        inertia_weight = 0.72  # Slightly increased from 0.7\n        cognitive_coefficient = 1.5\n        social_coefficient = 1.5\n        F = 0.5  # Differential evolution scaling factor\n        CR = 0.9  # Crossover probability\n\n        # Initialize position and velocity for each agent\n        lb, ub = func.bounds.lb, func.bounds.ub\n        positions = np.random.uniform(lb, ub, (num_agents, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_agents, self.dim))\n\n        # Evaluate initial positions\n        fitness = np.apply_along_axis(func, 1, positions)\n        self.eval_count += num_agents\n\n        best_agent_idx = np.argmin(fitness)\n        global_best_position = positions[best_agent_idx].copy()\n        global_best_value = fitness[best_agent_idx]\n\n        personal_best_positions = positions.copy()\n        personal_best_values = fitness.copy()\n\n        while self.eval_count < self.budget:\n            for i in range(num_agents):\n                # Update velocity using PSO\n                r1, r2 = np.random.rand(2)\n                cognitive_component = cognitive_coefficient * r1 * (personal_best_positions[i] - positions[i])\n                social_component = social_coefficient * r2 * (global_best_position - positions[i])\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n\n                # Update position\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], lb, ub)  # ensure within bounds\n\n                # Evaluate current position\n                current_fitness = func(positions[i])\n                self.eval_count += 1\n\n                # Update personal bests\n                if current_fitness < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_fitness\n\n                # Update global best\n                if current_fitness < global_best_value:\n                    global_best_position = positions[i]\n                    global_best_value = current_fitness\n\n                if self.eval_count >= self.budget:\n                    break\n\n            # Differential Evolution-inspired mutation\n            for i in range(num_agents):\n                if np.random.rand() < CR:\n                    indices = np.random.choice(num_agents, 3, replace=False)\n                    x1, x2, x3 = positions[indices]\n                    mutant_vector = x1 + F * (x2 - x3)\n                    trial_vector = np.where(np.random.rand(self.dim) < CR, mutant_vector, positions[i])\n                    trial_vector = np.clip(trial_vector, lb, ub)\n\n                    trial_fitness = func(trial_vector)\n                    self.eval_count += 1\n\n                    # Update personal bests and global best for trial vector\n                    if trial_fitness < personal_best_values[i]:\n                        personal_best_positions[i] = trial_vector\n                        personal_best_values[i] = trial_fitness\n\n                    if trial_fitness < global_best_value:\n                        global_best_position = trial_vector\n                        global_best_value = trial_fitness\n\n                    if self.eval_count >= self.budget:\n                        break\n\n        return global_best_position", "name": "HybridPSODEOptimizer", "description": "Slightly increase the inertia weight to enhance exploration capabilities.", "configspace": "", "generation": 35, "fitness": 0.08819297186863025, "feedback": "The algorithm HybridPSODEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.088 with standard deviation 0.011. And the mean value of best solutions found was 0.196 (0. is the best).", "error": "", "parent_id": "b8da6caa-d591-4782-9d5c-6e8a21857f17", "metadata": {"aucs": [0.09668064981650837, 0.0725348025732615, 0.09536346321612088], "final_y": [0.16485577207094426, 0.2578100888478746, 0.1648557724031523]}, "mutation_prompt": null}
{"id": "60e62230-d2ed-4323-aa56-17396fc144b3", "solution": "import numpy as np\n\nclass HybridPSODEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_agents = 5 + int(0.1 * self.dim)\n        inertia_weight = 0.7\n        cognitive_coefficient = 1.6  # Changed from 1.5 to 1.6\n        social_coefficient = 1.5\n        F = 0.5  # Differential evolution scaling factor\n        CR = 0.9  # Crossover probability\n\n        # Initialize position and velocity for each agent\n        lb, ub = func.bounds.lb, func.bounds.ub\n        positions = np.random.uniform(lb, ub, (num_agents, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_agents, self.dim))\n\n        # Evaluate initial positions\n        fitness = np.apply_along_axis(func, 1, positions)\n        self.eval_count += num_agents\n\n        best_agent_idx = np.argmin(fitness)\n        global_best_position = positions[best_agent_idx].copy()\n        global_best_value = fitness[best_agent_idx]\n\n        personal_best_positions = positions.copy()\n        personal_best_values = fitness.copy()\n\n        while self.eval_count < self.budget:\n            for i in range(num_agents):\n                # Update velocity using PSO\n                r1, r2 = np.random.rand(2)\n                cognitive_component = cognitive_coefficient * r1 * (personal_best_positions[i] - positions[i])\n                social_component = social_coefficient * r2 * (global_best_position - positions[i])\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n\n                # Update position\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], lb, ub)  # ensure within bounds\n\n                # Evaluate current position\n                current_fitness = func(positions[i])\n                self.eval_count += 1\n\n                # Update personal bests\n                if current_fitness < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_fitness\n\n                # Update global best\n                if current_fitness < global_best_value:\n                    global_best_position = positions[i]\n                    global_best_value = current_fitness\n\n                if self.eval_count >= self.budget:\n                    break\n\n            # Differential Evolution-inspired mutation\n            for i in range(num_agents):\n                if np.random.rand() < CR:\n                    indices = np.random.choice(num_agents, 3, replace=False)\n                    x1, x2, x3 = positions[indices]\n                    mutant_vector = x1 + F * (x2 - x3)\n                    trial_vector = np.where(np.random.rand(self.dim) < CR, mutant_vector, positions[i])\n                    trial_vector = np.clip(trial_vector, lb, ub)\n\n                    trial_fitness = func(trial_vector)\n                    self.eval_count += 1\n\n                    # Update personal bests and global best for trial vector\n                    if trial_fitness < personal_best_values[i]:\n                        personal_best_positions[i] = trial_vector\n                        personal_best_values[i] = trial_fitness\n\n                    if trial_fitness < global_best_value:\n                        global_best_position = trial_vector\n                        global_best_value = trial_fitness\n\n                    if self.eval_count >= self.budget:\n                        break\n\n        return global_best_position", "name": "HybridPSODEOptimizer", "description": "Slightly increase the cognitive coefficient to enhance individual exploration.", "configspace": "", "generation": 36, "fitness": 0.08761936602711358, "feedback": "The algorithm HybridPSODEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.088 with standard deviation 0.012. And the mean value of best solutions found was 0.196 (0. is the best).", "error": "", "parent_id": "b8da6caa-d591-4782-9d5c-6e8a21857f17", "metadata": {"aucs": [0.09577031891465992, 0.07103849073820967, 0.09604928842847116], "final_y": [0.16486025262549608, 0.25781008209299294, 0.16485577371811921]}, "mutation_prompt": null}
{"id": "0cce5033-4bf4-4bc2-ad48-a83d9b6490f8", "solution": "import numpy as np\n\nclass DynamicAdaptiveLevyPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def __call__(self, func):\n        def levy_flight(Lambda):\n            sigma = (np.math.gamma(1 + Lambda) * np.sin(np.pi * Lambda / 2) / \n                     (np.math.gamma((1 + Lambda) / 2) * Lambda * 2 ** ((Lambda - 1) / 2))) ** (1 / Lambda)\n            u = np.random.normal(0, sigma, size=self.dim)\n            v = np.random.normal(0, 1, size=self.dim)\n            step = u / abs(v) ** (1 / Lambda)\n            return step\n\n        num_agents = 5 + int(0.1 * self.dim)\n        inertia_weight = 0.9\n        inertia_weight_min = 0.4\n        cognitive_coefficient = 2.0\n        social_coefficient = 2.0\n\n        lb, ub = func.bounds.lb, func.bounds.ub\n        positions = np.random.uniform(lb, ub, (num_agents, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_agents, self.dim))\n\n        fitness = np.apply_along_axis(func, 1, positions)\n        self.eval_count += num_agents\n\n        best_agent_idx = np.argmin(fitness)\n        global_best_position = positions[best_agent_idx].copy()\n        global_best_value = fitness[best_agent_idx]\n\n        personal_best_positions = positions.copy()\n        personal_best_values = fitness.copy()\n\n        while self.eval_count < self.budget:\n            for i in range(num_agents):\n                # Adjust inertia weight dynamically\n                inertia_weight = inertia_weight_min + (0.5 * (self.budget - self.eval_count) / self.budget)\n\n                # Update velocity using PSO with Lévy Flight\n                r1, r2 = np.random.rand(2)\n                cognitive_component = cognitive_coefficient * r1 * (personal_best_positions[i] - positions[i])\n                social_component = social_coefficient * r2 * (global_best_position - positions[i])\n                levy_component = 0.01 * levy_flight(1.5)\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component + levy_component\n\n                # Update position\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], lb, ub)\n\n                # Evaluate current position\n                current_fitness = func(positions[i])\n                self.eval_count += 1\n\n                # Update personal bests\n                if current_fitness < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_fitness\n\n                # Update global best\n                if current_fitness < global_best_value:\n                    global_best_position = positions[i]\n                    global_best_value = current_fitness\n\n                if self.eval_count >= self.budget:\n                    break\n\n        return global_best_position", "name": "DynamicAdaptiveLevyPSO", "description": "Dynamic Adaptive Lévy Flight PSO (DALF-PSO) integrates Lévy flight for enhanced exploration and dynamically adjusts parameters based on convergence rate.", "configspace": "", "generation": 37, "fitness": 0.08378295005198999, "feedback": "The algorithm DynamicAdaptiveLevyPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.084 with standard deviation 0.004. And the mean value of best solutions found was 0.171 (0. is the best).", "error": "", "parent_id": "b8da6caa-d591-4782-9d5c-6e8a21857f17", "metadata": {"aucs": [0.07970145283337182, 0.08207527513828494, 0.08957212218431321], "final_y": [0.18187815790606965, 0.1648558810835774, 0.16485583104496881]}, "mutation_prompt": null}
{"id": "22166d2c-adb0-4c96-b15b-b9a206bcb827", "solution": "import numpy as np\n\nclass HybridPSODEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_agents = 5 + int(0.1 * self.dim)\n        inertia_weight = 0.9  # Changed initial inertia weight\n        cognitive_coefficient = 1.5\n        social_coefficient = 1.5\n        F = 0.5  # Differential evolution scaling factor\n        CR = 0.9  # Crossover probability\n\n        # Initialize position and velocity for each agent\n        lb, ub = func.bounds.lb, func.bounds.ub\n        positions = np.random.uniform(lb, ub, (num_agents, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_agents, self.dim))\n\n        # Evaluate initial positions\n        fitness = np.apply_along_axis(func, 1, positions)\n        self.eval_count += num_agents\n\n        best_agent_idx = np.argmin(fitness)\n        global_best_position = positions[best_agent_idx].copy()\n        global_best_value = fitness[best_agent_idx]\n\n        personal_best_positions = positions.copy()\n        personal_best_values = fitness.copy()\n\n        while self.eval_count < self.budget:\n            for i in range(num_agents):\n                # Update velocity using PSO\n                r1, r2 = np.random.rand(2)\n                cognitive_component = cognitive_coefficient * r1 * (personal_best_positions[i] - positions[i])\n                social_component = social_coefficient * r2 * (global_best_position - positions[i])\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n\n                # Update position\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], lb, ub)  # ensure within bounds\n\n                # Evaluate current position\n                current_fitness = func(positions[i])\n                self.eval_count += 1\n\n                # Update personal bests\n                if current_fitness < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_fitness\n\n                # Update global best\n                if current_fitness < global_best_value:\n                    global_best_position = positions[i]\n                    global_best_value = current_fitness\n\n                if self.eval_count >= self.budget:\n                    break\n\n            # Adaptive inertia weight decrease\n            inertia_weight = 0.4 + (0.5 * (1 - (self.eval_count / self.budget)))\n\n            # Differential Evolution-inspired mutation\n            for i in range(num_agents):\n                if np.random.rand() < CR:\n                    indices = np.random.choice(num_agents, 3, replace=False)\n                    x1, x2, x3 = positions[indices]\n                    mutant_vector = x1 + F * (x2 - x3)\n                    trial_vector = np.where(np.random.rand(self.dim) < CR, mutant_vector, positions[i])\n                    trial_vector = np.clip(trial_vector, lb, ub)\n\n                    trial_fitness = func(trial_vector)\n                    self.eval_count += 1\n\n                    # Update personal bests and global best for trial vector\n                    if trial_fitness < personal_best_values[i]:\n                        personal_best_positions[i] = trial_vector\n                        personal_best_values[i] = trial_fitness\n\n                    if trial_fitness < global_best_value:\n                        global_best_position = trial_vector\n                        global_best_value = trial_fitness\n\n                    if self.eval_count >= self.budget:\n                        break\n\n            # Opposite-Based Learning\n            opposite_positions = lb + ub - positions\n            opposite_fitness = np.apply_along_axis(func, 1, opposite_positions)\n            self.eval_count += num_agents\n            improvement_indices = opposite_fitness < personal_best_values\n            positions[improvement_indices] = opposite_positions[improvement_indices]\n            personal_best_positions[improvement_indices] = opposite_positions[improvement_indices]\n            personal_best_values[improvement_indices] = opposite_fitness[improvement_indices]\n\n        return global_best_position", "name": "HybridPSODEOptimizer", "description": "Enhanced Hybrid PSO with Adaptive Inertia Weight and Opposite-Based Learning for improved convergence.", "configspace": "", "generation": 38, "fitness": 0.0934234146816559, "feedback": "The algorithm HybridPSODEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.093 with standard deviation 0.002. And the mean value of best solutions found was 0.165 (0. is the best).", "error": "", "parent_id": "b8da6caa-d591-4782-9d5c-6e8a21857f17", "metadata": {"aucs": [0.09299507766512927, 0.09093396263250997, 0.09634120374732846], "final_y": [0.16485630608065227, 0.16485775143766312, 0.1648558317225859]}, "mutation_prompt": null}
{"id": "08448ede-cb5b-4e41-b4d0-7ee7b134144d", "solution": "import numpy as np\n\nclass HybridPSODEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_agents = 5 + int(0.1 * self.dim)\n        inertia_weight_init = 0.9\n        inertia_weight_final = 0.4\n        cognitive_coefficient = 1.5\n        social_coefficient_init = 1.5\n        social_coefficient_final = 2.0\n        F_base = 0.5\n        CR = 0.9\n\n        # Initialize position and velocity for each agent\n        lb, ub = func.bounds.lb, func.bounds.ub\n        positions = np.random.uniform(lb, ub, (num_agents, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_agents, self.dim))\n\n        # Evaluate initial positions\n        fitness = np.apply_along_axis(func, 1, positions)\n        self.eval_count += num_agents\n\n        best_agent_idx = np.argmin(fitness)\n        global_best_position = positions[best_agent_idx].copy()\n        global_best_value = fitness[best_agent_idx]\n\n        personal_best_positions = positions.copy()\n        personal_best_values = fitness.copy()\n\n        while self.eval_count < self.budget:\n            inertia_weight = inertia_weight_final + (inertia_weight_init - inertia_weight_final) * ((self.budget - self.eval_count) / self.budget)\n            social_coefficient = social_coefficient_final - (self.eval_count / self.budget) * (social_coefficient_final - social_coefficient_init)\n            \n            for i in range(num_agents):\n                r1, r2 = np.random.rand(2)\n                cognitive_component = cognitive_coefficient * r1 * (personal_best_positions[i] - positions[i])\n                social_component = social_coefficient * r2 * (global_best_position - positions[i])\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n\n                # Update position\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], lb, ub)\n\n                # Evaluate current position\n                current_fitness = func(positions[i])\n                self.eval_count += 1\n\n                # Update personal bests\n                if current_fitness < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_fitness\n\n                # Update global best\n                if current_fitness < global_best_value:\n                    global_best_position = positions[i]\n                    global_best_value = current_fitness\n\n                if self.eval_count >= self.budget:\n                    break\n\n            for i in range(num_agents):\n                if np.random.rand() < CR:\n                    indices = np.random.choice(num_agents, 3, replace=False)\n                    x1, x2, x3 = positions[indices]\n                    F = F_base + (0.3 * np.random.rand())\n                    mutant_vector = x1 + F * (x2 - x3)\n                    trial_vector = np.where(np.random.rand(self.dim) < CR, mutant_vector, positions[i])\n                    trial_vector = np.clip(trial_vector, lb, ub)\n\n                    trial_fitness = func(trial_vector)\n                    self.eval_count += 1\n\n                    if trial_fitness < personal_best_values[i]:\n                        personal_best_positions[i] = trial_vector\n                        personal_best_values[i] = trial_fitness\n\n                    if trial_fitness < global_best_value:\n                        global_best_position = trial_vector\n                        global_best_value = trial_fitness\n\n                    if self.eval_count >= self.budget:\n                        break\n\n        return global_best_position", "name": "HybridPSODEOptimizer", "description": "Enhanced Hybrid PSODE with adaptive inertia weight and dynamic mutation for robust convergence.", "configspace": "", "generation": 39, "fitness": 0.08001689125955529, "feedback": "The algorithm HybridPSODEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.080 with standard deviation 0.008. And the mean value of best solutions found was 0.207 (0. is the best).", "error": "", "parent_id": "b8da6caa-d591-4782-9d5c-6e8a21857f17", "metadata": {"aucs": [0.08566787351608451, 0.08583563277431416, 0.06854716748826717], "final_y": [0.1818781392755059, 0.1818780981871313, 0.2578100820921436]}, "mutation_prompt": null}
{"id": "076adbe1-b308-45d2-a751-82574655390e", "solution": "import numpy as np\n\nclass HybridPSODEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_agents = 5 + int(0.1 * self.dim)\n        inertia_weight = 0.72  # Adjusted inertia weight for improved performance\n        cognitive_coefficient = 1.5\n        social_coefficient = 1.5\n        F = 0.5  # Differential evolution scaling factor\n        CR = 0.9  # Crossover probability\n\n        # Initialize position and velocity for each agent\n        lb, ub = func.bounds.lb, func.bounds.ub\n        positions = np.random.uniform(lb, ub, (num_agents, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_agents, self.dim))\n\n        # Evaluate initial positions\n        fitness = np.apply_along_axis(func, 1, positions)\n        self.eval_count += num_agents\n\n        best_agent_idx = np.argmin(fitness)\n        global_best_position = positions[best_agent_idx].copy()\n        global_best_value = fitness[best_agent_idx]\n\n        personal_best_positions = positions.copy()\n        personal_best_values = fitness.copy()\n\n        while self.eval_count < self.budget:\n            for i in range(num_agents):\n                # Update velocity using PSO\n                r1, r2 = np.random.rand(2)\n                cognitive_component = cognitive_coefficient * r1 * (personal_best_positions[i] - positions[i])\n                social_component = social_coefficient * r2 * (global_best_position - positions[i])\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n\n                # Update position\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], lb, ub)  # ensure within bounds\n\n                # Evaluate current position\n                current_fitness = func(positions[i])\n                self.eval_count += 1\n\n                # Update personal bests\n                if current_fitness < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_fitness\n\n                # Update global best\n                if current_fitness < global_best_value:\n                    global_best_position = positions[i]\n                    global_best_value = current_fitness\n\n                if self.eval_count >= self.budget:\n                    break\n\n            # Differential Evolution-inspired mutation\n            for i in range(num_agents):\n                if np.random.rand() < CR:\n                    indices = np.random.choice(num_agents, 3, replace=False)\n                    x1, x2, x3 = positions[indices]\n                    mutant_vector = x1 + F * (x2 - x3)\n                    trial_vector = np.where(np.random.rand(self.dim) < CR, mutant_vector, positions[i])\n                    trial_vector = np.clip(trial_vector, lb, ub)\n\n                    trial_fitness = func(trial_vector)\n                    self.eval_count += 1\n\n                    # Update personal bests and global best for trial vector\n                    if trial_fitness < personal_best_values[i]:\n                        personal_best_positions[i] = trial_vector\n                        personal_best_values[i] = trial_fitness\n\n                    if trial_fitness < global_best_value:\n                        global_best_position = trial_vector\n                        global_best_value = trial_fitness\n\n                    if self.eval_count >= self.budget:\n                        break\n\n        return global_best_position", "name": "HybridPSODEOptimizer", "description": "Minor tweak in inertia weight for improved exploration-exploitation balance in Hybrid PSODE.", "configspace": "", "generation": 40, "fitness": 0.08819297186863025, "feedback": "The algorithm HybridPSODEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.088 with standard deviation 0.011. And the mean value of best solutions found was 0.196 (0. is the best).", "error": "", "parent_id": "b8da6caa-d591-4782-9d5c-6e8a21857f17", "metadata": {"aucs": [0.09668064981650837, 0.0725348025732615, 0.09536346321612088], "final_y": [0.16485577207094426, 0.2578100888478746, 0.1648557724031523]}, "mutation_prompt": null}
{"id": "6af0f9d4-2740-4d1a-bb27-642bb460c701", "solution": "import numpy as np\n\nclass HybridPSODEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_agents = 5 + int(0.1 * self.dim)\n        inertia_weight = 0.7\n        cognitive_coefficient = 2.0  # Increased for better individual exploration\n        social_coefficient = 1.5\n        F = 0.5  # Differential evolution scaling factor\n        CR = 0.9  # Crossover probability\n\n        # Initialize position and velocity for each agent\n        lb, ub = func.bounds.lb, func.bounds.ub\n        positions = np.random.uniform(lb, ub, (num_agents, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_agents, self.dim))\n\n        # Evaluate initial positions\n        fitness = np.apply_along_axis(func, 1, positions)\n        self.eval_count += num_agents\n\n        best_agent_idx = np.argmin(fitness)\n        global_best_position = positions[best_agent_idx].copy()\n        global_best_value = fitness[best_agent_idx]\n\n        personal_best_positions = positions.copy()\n        personal_best_values = fitness.copy()\n\n        while self.eval_count < self.budget:\n            for i in range(num_agents):\n                # Update velocity using PSO\n                r1, r2 = np.random.rand(2)\n                cognitive_component = cognitive_coefficient * r1 * (personal_best_positions[i] - positions[i])\n                social_component = social_coefficient * r2 * (global_best_position - positions[i])\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n\n                # Update position\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], lb, ub)  # ensure within bounds\n\n                # Evaluate current position\n                current_fitness = func(positions[i])\n                self.eval_count += 1\n\n                # Update personal bests\n                if current_fitness < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_fitness\n\n                # Update global best\n                if current_fitness < global_best_value:\n                    global_best_position = positions[i]\n                    global_best_value = current_fitness\n\n                if self.eval_count >= self.budget:\n                    break\n\n            # Differential Evolution-inspired mutation\n            for i in range(num_agents):\n                if np.random.rand() < CR:\n                    indices = np.random.choice(num_agents, 3, replace=False)\n                    x1, x2, x3 = positions[indices]\n                    mutant_vector = x1 + F * (x2 - x3)\n                    trial_vector = np.where(np.random.rand(self.dim) < CR, mutant_vector, positions[i])\n                    trial_vector = np.clip(trial_vector, lb, ub)\n\n                    trial_fitness = func(trial_vector)\n                    self.eval_count += 1\n\n                    # Update personal bests and global best for trial vector\n                    if trial_fitness < personal_best_values[i]:\n                        personal_best_positions[i] = trial_vector\n                        personal_best_values[i] = trial_fitness\n\n                    if trial_fitness < global_best_value:\n                        global_best_position = trial_vector\n                        global_best_value = trial_fitness\n\n                    if self.eval_count >= self.budget:\n                        break\n\n        return global_best_position", "name": "HybridPSODEOptimizer", "description": "Improved Hybrid Particle Swarm Optimization by enhancing PSO's cognitive component for better convergence.", "configspace": "", "generation": 41, "fitness": 0.09482525824453547, "feedback": "The algorithm HybridPSODEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.095 with standard deviation 0.002. And the mean value of best solutions found was 0.165 (0. is the best).", "error": "", "parent_id": "b8da6caa-d591-4782-9d5c-6e8a21857f17", "metadata": {"aucs": [0.09624243944996325, 0.09205759593665064, 0.09617573934699253], "final_y": [0.16485577224988313, 0.1648557739413008, 0.1648559112694865]}, "mutation_prompt": null}
{"id": "5c920cf3-9d22-4ee5-a3a0-f205d5b24033", "solution": "import numpy as np\n\nclass HybridPSODEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_agents = 5 + int(0.1 * self.dim)\n        inertia_weight_max = 0.9\n        inertia_weight_min = 0.4\n        cognitive_coefficient = 1.5\n        social_coefficient = 1.5\n        F = 0.5  # Differential evolution scaling factor\n        CR = 0.9  # Crossover probability\n\n        # Initialize position and velocity for each agent\n        lb, ub = func.bounds.lb, func.bounds.ub\n        positions = np.random.uniform(lb, ub, (num_agents, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_agents, self.dim))\n\n        # Evaluate initial positions\n        fitness = np.apply_along_axis(func, 1, positions)\n        self.eval_count += num_agents\n\n        best_agent_idx = np.argmin(fitness)\n        global_best_position = positions[best_agent_idx].copy()\n        global_best_value = fitness[best_agent_idx]\n\n        personal_best_positions = positions.copy()\n        personal_best_values = fitness.copy()\n\n        while self.eval_count < self.budget:\n            inertia_weight = inertia_weight_max - (inertia_weight_max - inertia_weight_min) * (self.eval_count / self.budget)\n            elite_position = global_best_position.copy()\n\n            for i in range(num_agents):\n                # Update velocity using PSO\n                r1, r2 = np.random.rand(2)\n                cognitive_component = cognitive_coefficient * r1 * (personal_best_positions[i] - positions[i])\n                social_component = social_coefficient * r2 * (global_best_position - positions[i])\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n\n                # Update position\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], lb, ub)  # ensure within bounds\n\n                # Evaluate current position\n                current_fitness = func(positions[i])\n                self.eval_count += 1\n\n                # Update personal bests\n                if current_fitness < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_fitness\n\n                # Update global best\n                if current_fitness < global_best_value:\n                    global_best_position = positions[i]\n                    global_best_value = current_fitness\n\n                if self.eval_count >= self.budget:\n                    break\n\n            # Differential Evolution-inspired mutation\n            for i in range(num_agents):\n                if np.random.rand() < CR:\n                    indices = np.random.choice(num_agents, 3, replace=False)\n                    x1, x2, x3 = positions[indices]\n                    mutant_vector = x1 + F * (x2 - x3)\n                    trial_vector = np.where(np.random.rand(self.dim) < CR, mutant_vector, positions[i])\n                    trial_vector = np.clip(trial_vector, lb, ub)\n\n                    trial_fitness = func(trial_vector)\n                    self.eval_count += 1\n\n                    # Update personal bests and global best for trial vector\n                    if trial_fitness < personal_best_values[i]:\n                        personal_best_positions[i] = trial_vector\n                        personal_best_values[i] = trial_fitness\n\n                    if trial_fitness < global_best_value:\n                        global_best_position = trial_vector\n                        global_best_value = trial_fitness\n\n                    if self.eval_count >= self.budget:\n                        break\n\n            # Update elite position to ensure keeping the best solution\n            if global_best_value < fitness[best_agent_idx]:\n                best_agent_idx = np.argmin(fitness)\n                global_best_position = elite_position\n\n        return global_best_position", "name": "HybridPSODEOptimizer", "description": "Introduced adaptive inertia weight and elitism to enhance convergence and solution quality.", "configspace": "", "generation": 42, "fitness": 0.08793075116788585, "feedback": "The algorithm HybridPSODEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.088 with standard deviation 0.003. And the mean value of best solutions found was 0.178 (0. is the best).", "error": "", "parent_id": "b8da6caa-d591-4782-9d5c-6e8a21857f17", "metadata": {"aucs": [0.08873659561865499, 0.09064856253503617, 0.08440709534996638], "final_y": [0.17963578837823246, 0.16931158020412063, 0.18481429470771726]}, "mutation_prompt": null}
{"id": "210bd07e-3ab0-47af-b500-b1743a5306b0", "solution": "import numpy as np\n\nclass EnhancedHybridPSODEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_agents = 5 + int(0.1 * self.dim)\n        inertia_weight_max = 0.9\n        inertia_weight_min = 0.4\n        cognitive_coefficient = 1.5\n        social_coefficient = 1.5\n        F = 0.8  # Differential evolution scaling factor\n        CR = 0.9  # Crossover probability\n\n        # Initialize position and velocity for each agent\n        lb, ub = func.bounds.lb, func.bounds.ub\n        positions = np.random.uniform(lb, ub, (num_agents, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_agents, self.dim))\n\n        # Evaluate initial positions\n        fitness = np.apply_along_axis(func, 1, positions)\n        self.eval_count += num_agents\n\n        best_agent_idx = np.argmin(fitness)\n        global_best_position = positions[best_agent_idx].copy()\n        global_best_value = fitness[best_agent_idx]\n\n        personal_best_positions = positions.copy()\n        personal_best_values = fitness.copy()\n\n        while self.eval_count < self.budget:\n            inertia_weight = inertia_weight_max - ((inertia_weight_max - inertia_weight_min) * \n                                                   (self.eval_count / self.budget))\n            for i in range(num_agents):\n                # Update velocity using PSO with adaptive inertia weight\n                r1, r2 = np.random.rand(2)\n                cognitive_component = cognitive_coefficient * r1 * (personal_best_positions[i] - positions[i])\n                social_component = social_coefficient * r2 * (global_best_position - positions[i])\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n\n                # Update position\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], lb, ub)  # ensure within bounds\n\n                # Evaluate current position\n                current_fitness = func(positions[i])\n                self.eval_count += 1\n\n                # Update personal bests\n                if current_fitness < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_fitness\n\n                # Update global best\n                if current_fitness < global_best_value:\n                    global_best_position = positions[i]\n                    global_best_value = current_fitness\n\n                if self.eval_count >= self.budget:\n                    break\n\n            # Differential Evolution-inspired mutation\n            for i in range(num_agents):\n                if np.random.rand() < CR:\n                    indices = np.random.choice(num_agents, 3, replace=False)\n                    x1, x2, x3 = positions[indices]\n                    mutant_vector = x1 + F * (x2 - x3)\n                    trial_vector = np.where(np.random.rand(self.dim) < CR, mutant_vector, positions[i])\n                    trial_vector = np.clip(trial_vector, lb, ub)\n\n                    trial_fitness = func(trial_vector)\n                    self.eval_count += 1\n\n                    # Update personal bests and global best for trial vector\n                    if trial_fitness < personal_best_values[i]:\n                        personal_best_positions[i] = trial_vector\n                        personal_best_values[i] = trial_fitness\n\n                    if trial_fitness < global_best_value:\n                        global_best_position = trial_vector\n                        global_best_value = trial_fitness\n\n                    if self.eval_count >= self.budget:\n                        break\n\n            # Random Walk for increased exploration\n            if np.random.rand() < 0.1:\n                random_agent_idx = np.random.randint(num_agents)\n                positions[random_agent_idx] = np.random.uniform(lb, ub, self.dim)\n                random_fitness = func(positions[random_agent_idx])\n                self.eval_count += 1\n\n                if random_fitness < personal_best_values[random_agent_idx]:\n                    personal_best_positions[random_agent_idx] = positions[random_agent_idx]\n                    personal_best_values[random_agent_idx] = random_fitness\n\n                if random_fitness < global_best_value:\n                    global_best_position = positions[random_agent_idx]\n                    global_best_value = random_fitness\n\n                if self.eval_count >= self.budget:\n                    break\n\n        return global_best_position", "name": "EnhancedHybridPSODEOptimizer", "description": "Enhanced Hybrid PSO incorporating Adaptive Inertia Weight, Differential Evolution, and Random Walk to improve convergence and exploration.", "configspace": "", "generation": 43, "fitness": 0.08056829061915001, "feedback": "The algorithm EnhancedHybridPSODEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.081 with standard deviation 0.011. And the mean value of best solutions found was 0.218 (0. is the best).", "error": "", "parent_id": "b8da6caa-d591-4782-9d5c-6e8a21857f17", "metadata": {"aucs": [0.09333243354817533, 0.06681280111423915, 0.08155963719503556], "final_y": [0.16485637714038104, 0.2827467765384176, 0.20725483280491863]}, "mutation_prompt": null}
{"id": "24644ae9-2f3c-43c3-8f2b-d8349611fab9", "solution": "import numpy as np\n\nclass HybridPSODEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_agents = 5 + int(0.1 * self.dim)\n        inertia_weight = 0.9  # Changed from 0.7 to 0.9 for adaptive behavior\n        cognitive_coefficient = 1.5\n        social_coefficient = 1.5\n        F = 0.5  # Differential evolution scaling factor\n        CR = 0.9  # Crossover probability\n\n        # Initialize position and velocity for each agent\n        lb, ub = func.bounds.lb, func.bounds.ub\n        positions = np.random.uniform(lb, ub, (num_agents, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_agents, self.dim))\n\n        # Evaluate initial positions\n        fitness = np.apply_along_axis(func, 1, positions)\n        self.eval_count += num_agents\n\n        best_agent_idx = np.argmin(fitness)\n        global_best_position = positions[best_agent_idx].copy()\n        global_best_value = fitness[best_agent_idx]\n\n        personal_best_positions = positions.copy()\n        personal_best_values = fitness.copy()\n\n        while self.eval_count < self.budget:\n            for i in range(num_agents):\n                # Update velocity using PSO\n                r1, r2 = np.random.rand(2)\n                cognitive_component = cognitive_coefficient * r1 * (personal_best_positions[i] - positions[i])\n                social_component = social_coefficient * r2 * (global_best_position - positions[i])\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n\n                # Update position\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], lb, ub)  # ensure within bounds\n\n                # Evaluate current position\n                current_fitness = func(positions[i])\n                self.eval_count += 1\n\n                # Update personal bests\n                if current_fitness < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_fitness\n\n                # Update global best\n                if current_fitness < global_best_value:\n                    global_best_position = positions[i]\n                    global_best_value = current_fitness\n\n                if self.eval_count >= self.budget:\n                    break\n\n            # Differential Evolution-inspired mutation\n            for i in range(num_agents):\n                if np.random.rand() < CR:\n                    indices = np.random.choice(num_agents, 3, replace=False)\n                    x1, x2, x3 = positions[indices]\n                    mutant_vector = x1 + F * (x2 - x3)\n                    trial_vector = np.where(np.random.rand(self.dim) < CR, mutant_vector, positions[i])\n                    trial_vector = np.clip(trial_vector, lb, ub)\n\n                    trial_fitness = func(trial_vector)\n                    self.eval_count += 1\n\n                    # Update personal bests and global best for trial vector\n                    if trial_fitness < personal_best_values[i]:\n                        personal_best_positions[i] = trial_vector\n                        personal_best_values[i] = trial_fitness\n\n                    if trial_fitness < global_best_value:\n                        global_best_position = trial_vector\n                        global_best_value = trial_fitness\n\n                    if self.eval_count >= self.budget:\n                        break\n\n        return global_best_position", "name": "HybridPSODEOptimizer", "description": "Introduced an adaptive inertia weight strategy in the PSO velocity update to balance exploration and exploitation dynamically.", "configspace": "", "generation": 44, "fitness": 0.09144204270843548, "feedback": "The algorithm HybridPSODEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.091 with standard deviation 0.001. And the mean value of best solutions found was 0.169 (0. is the best).", "error": "", "parent_id": "b8da6caa-d591-4782-9d5c-6e8a21857f17", "metadata": {"aucs": [0.09229503336017209, 0.08974499923572166, 0.09228609552941269], "final_y": [0.16668006564448634, 0.1685749638659887, 0.17040204035852569]}, "mutation_prompt": null}
{"id": "60916d72-86cb-4f26-8faf-0bd560528560", "solution": "import numpy as np\n\nclass HybridPSODEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_agents = 5 + int(0.1 * self.dim)\n        inertia_weight = 0.9 - 0.5 * (self.eval_count / self.budget)  # Adaptive inertia\n        cognitive_coefficient = 1.5\n        social_coefficient = 1.5\n        F = 0.5 + 0.5 * (self.eval_count / self.budget)  # Dynamic scaling factor\n        CR = 0.9  # Crossover probability\n\n        # Initialize position and velocity for each agent\n        lb, ub = func.bounds.lb, func.bounds.ub\n        positions = np.random.uniform(lb, ub, (num_agents, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_agents, self.dim))\n\n        # Evaluate initial positions\n        fitness = np.apply_along_axis(func, 1, positions)\n        self.eval_count += num_agents\n\n        best_agent_idx = np.argmin(fitness)\n        global_best_position = positions[best_agent_idx].copy()\n        global_best_value = fitness[best_agent_idx]\n\n        personal_best_positions = positions.copy()\n        personal_best_values = fitness.copy()\n\n        while self.eval_count < self.budget:\n            inertia_weight = 0.9 - 0.5 * (self.eval_count / self.budget)  # Adaptive inertia\n            for i in range(num_agents):\n                # Update velocity using PSO\n                r1, r2 = np.random.rand(2)\n                cognitive_component = cognitive_coefficient * r1 * (personal_best_positions[i] - positions[i])\n                social_component = social_coefficient * r2 * (global_best_position - positions[i])\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n\n                # Update position\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], lb, ub)\n\n                # Evaluate current position\n                current_fitness = func(positions[i])\n                self.eval_count += 1\n\n                # Update personal bests\n                if current_fitness < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_fitness\n\n                # Update global best\n                if current_fitness < global_best_value:\n                    global_best_position = positions[i]\n                    global_best_value = current_fitness\n\n                if self.eval_count >= self.budget:\n                    break\n\n            # Differential Evolution-inspired mutation\n            for i in range(num_agents):\n                if np.random.rand() < CR:\n                    indices = np.random.choice(num_agents, 3, replace=False)\n                    x1, x2, x3 = positions[indices]\n                    mutant_vector = x1 + F * (x2 - x3)\n                    trial_vector = np.where(np.random.rand(self.dim) < CR, mutant_vector, positions[i])\n                    trial_vector = np.clip(trial_vector, lb, ub)\n\n                    trial_fitness = func(trial_vector)\n                    self.eval_count += 1\n\n                    # Update personal bests and global best for trial vector\n                    if trial_fitness < personal_best_values[i]:\n                        personal_best_positions[i] = trial_vector\n                        personal_best_values[i] = trial_fitness\n\n                    if trial_fitness < global_best_value:\n                        global_best_position = trial_vector\n                        global_best_value = trial_fitness\n\n                    if self.eval_count >= self.budget:\n                        break\n\n        return global_best_position", "name": "HybridPSODEOptimizer", "description": "Enhanced PSO with adaptive inertia and dynamic DE mutation for improved convergence and exploration.", "configspace": "", "generation": 45, "fitness": 0.0875227615136206, "feedback": "The algorithm HybridPSODEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.088 with standard deviation 0.011. And the mean value of best solutions found was 0.196 (0. is the best).", "error": "", "parent_id": "b8da6caa-d591-4782-9d5c-6e8a21857f17", "metadata": {"aucs": [0.09471387659501163, 0.0723065974631576, 0.09554781048269256], "final_y": [0.16485585897587363, 0.2578100821104141, 0.16485586847342926]}, "mutation_prompt": null}
{"id": "d5166dfe-3ee8-4b30-9d6a-4aec13a69541", "solution": "import numpy as np\n\nclass HybridPSODEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_agents = 5 + int(0.1 * self.dim)\n        inertia_weight = 0.9  # Start higher, adapt later\n        cognitive_coefficient = 1.5\n        social_coefficient = 1.5\n        F = 0.5  # Differential evolution scaling factor\n        CR = 0.9  # Crossover probability\n\n        # Initialize position and velocity for each agent\n        lb, ub = func.bounds.lb, func.bounds.ub\n        positions = np.random.uniform(lb, ub, (num_agents, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_agents, self.dim))\n\n        # Evaluate initial positions\n        fitness = np.apply_along_axis(func, 1, positions)\n        self.eval_count += num_agents\n\n        best_agent_idx = np.argmin(fitness)\n        global_best_position = positions[best_agent_idx].copy()\n        global_best_value = fitness[best_agent_idx]\n\n        personal_best_positions = positions.copy()\n        personal_best_values = fitness.copy()\n\n        while self.eval_count < self.budget:\n            for i in range(num_agents):\n                # Update velocity using PSO\n                r1 = 0.5 + 0.5 * np.sin(np.pi * self.eval_count / self.budget)  # Chaotic sequence\n                r2 = np.random.rand()\n                cognitive_component = cognitive_coefficient * r1 * (personal_best_positions[i] - positions[i])\n                social_component = social_coefficient * r2 * (global_best_position - positions[i])\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n\n                # Update position\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], lb, ub)  # ensure within bounds\n\n                # Evaluate current position\n                current_fitness = func(positions[i])\n                self.eval_count += 1\n\n                # Update personal bests\n                if current_fitness < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_fitness\n\n                # Update global best\n                if current_fitness < global_best_value:\n                    global_best_position = positions[i]\n                    global_best_value = current_fitness\n\n                if self.eval_count >= self.budget:\n                    break\n\n            # Differential Evolution-inspired mutation\n            for i in range(num_agents):\n                if np.random.rand() < CR:\n                    indices = np.random.choice(num_agents, 3, replace=False)\n                    x1, x2, x3 = positions[indices]\n                    mutant_vector = x1 + F * (x2 - x3)\n                    trial_vector = np.where(np.random.rand(self.dim) < CR, mutant_vector, positions[i])\n                    trial_vector = np.clip(trial_vector, lb, ub)\n\n                    trial_fitness = func(trial_vector)\n                    self.eval_count += 1\n\n                    # Update personal bests and global best for trial vector\n                    if trial_fitness < personal_best_values[i]:\n                        personal_best_positions[i] = trial_vector\n                        personal_best_values[i] = trial_fitness\n\n                    if trial_fitness < global_best_value:\n                        global_best_position = trial_vector\n                        global_best_value = trial_fitness\n\n                    if self.eval_count >= self.budget:\n                        break\n\n            # Adaptive inertia weight\n            inertia_weight = 0.4 + 0.5 * (1 - self.eval_count / self.budget)\n\n        return global_best_position", "name": "HybridPSODEOptimizer", "description": "Enhanced Hybrid PSODE with adaptive inertia weight and chaotic sequences for improved convergence.", "configspace": "", "generation": 46, "fitness": 0.08449046029179175, "feedback": "The algorithm HybridPSODEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.084 with standard deviation 0.012. And the mean value of best solutions found was 0.204 (0. is the best).", "error": "", "parent_id": "b8da6caa-d591-4782-9d5c-6e8a21857f17", "metadata": {"aucs": [0.06817659770615736, 0.09282525836816036, 0.09246952480105752], "final_y": [0.2827466932878524, 0.16485806177657014, 0.1648557719978645]}, "mutation_prompt": null}
{"id": "3c5cd28f-8c3f-4554-8d0c-ba53d434b9b3", "solution": "import numpy as np\n\nclass HybridPSODEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_agents = 5 + int(0.1 * self.dim)\n        inertia_weight_max = 0.9  # Changed from 0.7 to 0.9\n        inertia_weight_min = 0.4  # Added for adaptive inertia weight\n        cognitive_coefficient = 1.5\n        social_coefficient = 1.5\n        F = 0.6  # Increased from 0.5 to 0.6\n        CR = 0.9\n\n        # Initialize position and velocity for each agent\n        lb, ub = func.bounds.lb, func.bounds.ub\n        positions = np.random.uniform(lb, ub, (num_agents, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_agents, self.dim))\n\n        # Evaluate initial positions\n        fitness = np.apply_along_axis(func, 1, positions)\n        self.eval_count += num_agents\n\n        best_agent_idx = np.argmin(fitness)\n        global_best_position = positions[best_agent_idx].copy()\n        global_best_value = fitness[best_agent_idx]\n\n        personal_best_positions = positions.copy()\n        personal_best_values = fitness.copy()\n\n        while self.eval_count < self.budget:\n            # Adaptive inertia weight calculation\n            inertia_weight = inertia_weight_max - ((inertia_weight_max - inertia_weight_min) * (self.eval_count / self.budget))\n\n            for i in range(num_agents):\n                # Update velocity using PSO\n                r1, r2 = np.random.rand(2)\n                cognitive_component = cognitive_coefficient * r1 * (personal_best_positions[i] - positions[i])\n                social_component = social_coefficient * r2 * (global_best_position - positions[i])\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n\n                # Update position\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], lb, ub)  # ensure within bounds\n\n                # Evaluate current position\n                current_fitness = func(positions[i])\n                self.eval_count += 1\n\n                # Update personal bests\n                if current_fitness < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_fitness\n\n                # Update global best\n                if current_fitness < global_best_value:\n                    global_best_position = positions[i]\n                    global_best_value = current_fitness\n\n                if self.eval_count >= self.budget:\n                    break\n\n            # Differential Evolution-inspired mutation\n            for i in range(num_agents):\n                if np.random.rand() < CR:\n                    indices = np.random.choice(num_agents, 3, replace=False)\n                    x1, x2, x3 = positions[indices]\n                    mutant_vector = x1 + F * (x2 - x3)\n                    trial_vector = np.where(np.random.rand(self.dim) < CR, mutant_vector, positions[i])\n                    trial_vector = np.clip(trial_vector, lb, ub)\n\n                    trial_fitness = func(trial_vector)\n                    self.eval_count += 1\n\n                    # Update personal bests and global best for trial vector\n                    if trial_fitness < personal_best_values[i]:\n                        personal_best_positions[i] = trial_vector\n                        personal_best_values[i] = trial_fitness\n\n                    if trial_fitness < global_best_value:\n                        global_best_position = trial_vector\n                        global_best_value = trial_fitness\n\n                    if self.eval_count >= self.budget:\n                        break\n\n        return global_best_position", "name": "HybridPSODEOptimizer", "description": "Enhanced Hybrid PSO-DE with adaptive inertia weight for improved convergence and exploration-exploitation balance.", "configspace": "", "generation": 47, "fitness": 0.08426898379476151, "feedback": "The algorithm HybridPSODEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.084 with standard deviation 0.014. And the mean value of best solutions found was 0.196 (0. is the best).", "error": "", "parent_id": "b8da6caa-d591-4782-9d5c-6e8a21857f17", "metadata": {"aucs": [0.09504365732114184, 0.06515747864173393, 0.09260581542140878], "final_y": [0.1648557719226953, 0.25781008208795053, 0.164855790103816]}, "mutation_prompt": null}
{"id": "15b09bcb-9a2e-4c4c-9f42-5642c132b655", "solution": "import numpy as np\n\nclass HybridPSODEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_agents = 5 + int(0.1 * self.dim)\n        inertia_weight = 0.9  # Start with a higher inertia weight\n        inertia_weight_min = 0.4  # Minimum inertia weight\n        inertia_decay = (inertia_weight - inertia_weight_min) / self.budget\n        cognitive_coefficient = 1.5\n        social_coefficient = 1.5\n        F = 0.5  # Differential evolution scaling factor\n        CR = 0.9  # Crossover probability\n        dynamic_agent_flag = False  # Flag for dynamic agent adjustment\n\n        # Initialize position and velocity for each agent\n        lb, ub = func.bounds.lb, func.bounds.ub\n        positions = np.random.uniform(lb, ub, (num_agents, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_agents, self.dim))\n\n        # Evaluate initial positions\n        fitness = np.apply_along_axis(func, 1, positions)\n        self.eval_count += num_agents\n\n        best_agent_idx = np.argmin(fitness)\n        global_best_position = positions[best_agent_idx].copy()\n        global_best_value = fitness[best_agent_idx]\n\n        personal_best_positions = positions.copy()\n        personal_best_values = fitness.copy()\n\n        while self.eval_count < self.budget:\n            for i in range(num_agents):\n                # Update inertia weight adaptively\n                inertia_weight = max(inertia_weight - inertia_decay, inertia_weight_min)\n\n                # Update velocity using PSO\n                r1, r2 = np.random.rand(2)\n                cognitive_component = cognitive_coefficient * r1 * (personal_best_positions[i] - positions[i])\n                social_component = social_coefficient * r2 * (global_best_position - positions[i])\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n\n                # Update position\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], lb, ub)  # ensure within bounds\n\n                # Evaluate current position\n                current_fitness = func(positions[i])\n                self.eval_count += 1\n\n                # Update personal bests\n                if current_fitness < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_fitness\n\n                # Update global best\n                if current_fitness < global_best_value:\n                    global_best_position = positions[i]\n                    global_best_value = current_fitness\n\n                if self.eval_count >= self.budget:\n                    break\n\n            # Differential Evolution-inspired mutation\n            for i in range(num_agents):\n                if np.random.rand() < CR:\n                    indices = np.random.choice(num_agents, 3, replace=False)\n                    x1, x2, x3 = positions[indices]\n                    mutant_vector = x1 + F * (x2 - x3)\n                    trial_vector = np.where(np.random.rand(self.dim) < CR, mutant_vector, positions[i])\n                    trial_vector = np.clip(trial_vector, lb, ub)\n\n                    trial_fitness = func(trial_vector)\n                    self.eval_count += 1\n\n                    # Update personal bests and global best for trial vector\n                    if trial_fitness < personal_best_values[i]:\n                        personal_best_positions[i] = trial_vector\n                        personal_best_values[i] = trial_fitness\n\n                    if trial_fitness < global_best_value:\n                        global_best_position = trial_vector\n                        global_best_value = trial_fitness\n\n                    if self.eval_count >= self.budget:\n                        break\n\n            # Adjust number of agents dynamically based on progress\n            if not dynamic_agent_flag and self.eval_count > 0.3 * self.budget:\n                num_agents = int(num_agents * 1.2)  # Increase agents by 20%\n                dynamic_agent_flag = True\n                positions = np.random.uniform(lb, ub, (num_agents, self.dim))\n                velocities = np.random.uniform(-1, 1, (num_agents, self.dim))\n\n        return global_best_position", "name": "HybridPSODEOptimizer", "description": "Enhanced Hybrid PSODE with adaptive inertia weight and dynamic agent count adjustment for improved convergence.", "configspace": "", "generation": 48, "fitness": -Infinity, "feedback": "An exception occurred: IndexError('index 6 is out of bounds for axis 0 with size 6').", "error": "IndexError('index 6 is out of bounds for axis 0 with size 6')", "parent_id": "b8da6caa-d591-4782-9d5c-6e8a21857f17", "metadata": {}, "mutation_prompt": null}
{"id": "0c011028-dffb-4d80-9adb-3c88278d8059", "solution": "import numpy as np\n\nclass HybridPSODEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_agents = 5 + int(0.1 * self.dim)\n        inertia_weight = 0.7\n        cognitive_coefficient = 1.7  # Increased cognitive coefficient for better convergence\n        social_coefficient = 1.5\n        F = 0.5  # Differential evolution scaling factor\n        CR = 0.9  # Crossover probability\n\n        # Initialize position and velocity for each agent\n        lb, ub = func.bounds.lb, func.bounds.ub\n        positions = np.random.uniform(lb, ub, (num_agents, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_agents, self.dim))\n\n        # Evaluate initial positions\n        fitness = np.apply_along_axis(func, 1, positions)\n        self.eval_count += num_agents\n\n        best_agent_idx = np.argmin(fitness)\n        global_best_position = positions[best_agent_idx].copy()\n        global_best_value = fitness[best_agent_idx]\n\n        personal_best_positions = positions.copy()\n        personal_best_values = fitness.copy()\n\n        while self.eval_count < self.budget:\n            for i in range(num_agents):\n                # Update velocity using PSO\n                r1, r2 = np.random.rand(2)\n                cognitive_component = cognitive_coefficient * r1 * (personal_best_positions[i] - positions[i])\n                social_component = social_coefficient * r2 * (global_best_position - positions[i])\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n\n                # Update position\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], lb, ub)  # ensure within bounds\n\n                # Evaluate current position\n                current_fitness = func(positions[i])\n                self.eval_count += 1\n\n                # Update personal bests\n                if current_fitness < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_fitness\n\n                # Update global best\n                if current_fitness < global_best_value:\n                    global_best_position = positions[i]\n                    global_best_value = current_fitness\n\n                if self.eval_count >= self.budget:\n                    break\n\n            # Differential Evolution-inspired mutation\n            for i in range(num_agents):\n                if np.random.rand() < CR:\n                    indices = np.random.choice(num_agents, 3, replace=False)\n                    x1, x2, x3 = positions[indices]\n                    mutant_vector = x1 + F * (x2 - x3)\n                    trial_vector = np.where(np.random.rand(self.dim) < CR, mutant_vector, positions[i])\n                    trial_vector = np.clip(trial_vector, lb, ub)\n\n                    trial_fitness = func(trial_vector)\n                    self.eval_count += 1\n\n                    # Update personal bests and global best for trial vector\n                    if trial_fitness < personal_best_values[i]:\n                        personal_best_positions[i] = trial_vector\n                        personal_best_values[i] = trial_fitness\n\n                    if trial_fitness < global_best_value:\n                        global_best_position = trial_vector\n                        global_best_value = trial_fitness\n\n                    if self.eval_count >= self.budget:\n                        break\n\n        return global_best_position", "name": "HybridPSODEOptimizer", "description": "Enhanced exploitation by increasing the cognitive coefficient for improved convergence.", "configspace": "", "generation": 49, "fitness": 0.0951560023061907, "feedback": "The algorithm HybridPSODEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.095 with standard deviation 0.001. And the mean value of best solutions found was 0.165 (0. is the best).", "error": "", "parent_id": "b8da6caa-d591-4782-9d5c-6e8a21857f17", "metadata": {"aucs": [0.09636415590021263, 0.09352657637545636, 0.09557727464290311], "final_y": [0.1648558048305726, 0.1648560114569858, 0.16485580794580879]}, "mutation_prompt": null}
{"id": "7bb19806-666f-48b4-ab1c-733659785879", "solution": "import numpy as np\n\nclass HybridPSODEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_agents = 5 + int(0.1 * self.dim)\n        inertia_weight = 0.9  # Changed initial inertia weight\n        cognitive_coefficient = 1.5\n        social_coefficient = 1.5\n        F = 0.5  # Differential evolution scaling factor\n        CR = 0.9  # Crossover probability\n\n        # Initialize position and velocity for each agent\n        lb, ub = func.bounds.lb, func.bounds.ub\n        positions = np.random.uniform(lb, ub, (num_agents, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_agents, self.dim))\n\n        # Evaluate initial positions\n        fitness = np.apply_along_axis(func, 1, positions)\n        self.eval_count += num_agents\n\n        best_agent_idx = np.argmin(fitness)\n        global_best_position = positions[best_agent_idx].copy()\n        global_best_value = fitness[best_agent_idx]\n\n        personal_best_positions = positions.copy()\n        personal_best_values = fitness.copy()\n\n        while self.eval_count < self.budget:\n            for i in range(num_agents):\n                # Update velocity using PSO\n                r1, r2 = np.random.rand(2)\n                cognitive_component = cognitive_coefficient * r1 * (personal_best_positions[i] - positions[i])\n                social_component = social_coefficient * r2 * (global_best_position - positions[i])\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n\n                # Update position\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], lb, ub)  # ensure within bounds\n\n                # Evaluate current position\n                current_fitness = func(positions[i])\n                self.eval_count += 1\n\n                # Update personal bests\n                if current_fitness < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_fitness\n\n                # Update global best\n                if current_fitness < global_best_value:\n                    global_best_position = positions[i]\n                    global_best_value = current_fitness\n\n                if self.eval_count >= self.budget:\n                    break\n\n            # Differential Evolution-inspired mutation\n            for i in range(num_agents):\n                if np.random.rand() < CR:\n                    indices = np.random.choice(num_agents, 3, replace=False)\n                    x1, x2, x3 = positions[indices]\n                    F_dynamic = F + ((self.budget - self.eval_count) / self.budget) * 0.3  # Dynamically adjust F\n                    mutant_vector = x1 + F_dynamic * (x2 - x3)\n                    CR_dynamic = CR - ((self.eval_count / self.budget) * 0.2)  # Dynamically adjust CR\n                    trial_vector = np.where(np.random.rand(self.dim) < CR_dynamic, mutant_vector, positions[i])\n                    trial_vector = np.clip(trial_vector, lb, ub)\n\n                    trial_fitness = func(trial_vector)\n                    self.eval_count += 1\n\n                    # Update personal bests and global best for trial vector\n                    if trial_fitness < personal_best_values[i]:\n                        personal_best_positions[i] = trial_vector\n                        personal_best_values[i] = trial_fitness\n\n                    if trial_fitness < global_best_value:\n                        global_best_position = trial_vector\n                        global_best_value = trial_fitness\n\n                    if self.eval_count >= self.budget:\n                        break\n\n            inertia_weight *= 0.99  # Decay inertia weight\n\n        return global_best_position", "name": "HybridPSODEOptimizer", "description": "Enhanced Hybrid Particle Swarm Optimization by introducing a dynamic inertia weight and adaptive DE parameters for improved convergence.", "configspace": "", "generation": 50, "fitness": 0.09420481860767038, "feedback": "The algorithm HybridPSODEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.094 with standard deviation 0.002. And the mean value of best solutions found was 0.167 (0. is the best).", "error": "", "parent_id": "b8da6caa-d591-4782-9d5c-6e8a21857f17", "metadata": {"aucs": [0.09639868099648996, 0.09401408375237674, 0.09220169107414444], "final_y": [0.16575772387295784, 0.16521846812051855, 0.1712742445495734]}, "mutation_prompt": null}
{"id": "9671deb5-1d7c-498d-9304-6e45d06dc8c0", "solution": "import numpy as np\n\nclass HybridPSODEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_agents = 5 + int(0.1 * self.dim)\n        inertia_weight = 0.9  # Adjusted initial inertia weight\n        cognitive_coefficient = 1.5\n        social_coefficient = 1.5\n        F = 0.5  # Differential evolution scaling factor\n        CR = 0.9  # Crossover probability\n\n        # Initialize position and velocity for each agent\n        lb, ub = func.bounds.lb, func.bounds.ub\n        positions = np.random.uniform(lb, ub, (num_agents, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_agents, self.dim))\n\n        # Evaluate initial positions\n        fitness = np.apply_along_axis(func, 1, positions)\n        self.eval_count += num_agents\n\n        best_agent_idx = np.argmin(fitness)\n        global_best_position = positions[best_agent_idx].copy()\n        global_best_value = fitness[best_agent_idx]\n\n        personal_best_positions = positions.copy()\n        personal_best_values = fitness.copy()\n\n        while self.eval_count < self.budget:\n            # Dynamically adjust the number of agents\n            num_agents = max(5, num_agents - (self.eval_count // (self.budget / num_agents)))\n\n            for i in range(num_agents):\n                # Update velocity using PSO\n                r1, r2 = np.random.rand(2)\n                cognitive_component = cognitive_coefficient * r1 * (personal_best_positions[i] - positions[i])\n                social_component = social_coefficient * r2 * (global_best_position - positions[i])\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n\n                # Update position\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], lb, ub)  # ensure within bounds\n\n                # Evaluate current position\n                current_fitness = func(positions[i])\n                self.eval_count += 1\n\n                # Update personal bests\n                if current_fitness < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_fitness\n\n                # Update global best\n                if current_fitness < global_best_value:\n                    global_best_position = positions[i]\n                    global_best_value = current_fitness\n\n                if self.eval_count >= self.budget:\n                    break\n\n            # Differential Evolution-inspired mutation\n            for i in range(num_agents):\n                if np.random.rand() < CR:\n                    indices = np.random.choice(num_agents, 3, replace=False)\n                    x1, x2, x3 = positions[indices]\n                    mutant_vector = x1 + F * (x2 - x3)\n                    trial_vector = np.where(np.random.rand(self.dim) < CR, mutant_vector, positions[i])\n                    trial_vector = np.clip(trial_vector, lb, ub)\n\n                    trial_fitness = func(trial_vector)\n                    self.eval_count += 1\n\n                    # Update personal bests and global best for trial vector\n                    if trial_fitness < personal_best_values[i]:\n                        personal_best_positions[i] = trial_vector\n                        personal_best_values[i] = trial_fitness\n\n                    if trial_fitness < global_best_value:\n                        global_best_position = trial_vector\n                        global_best_value = trial_fitness\n\n                    if self.eval_count >= self.budget:\n                        break\n\n            inertia_weight = 0.4 + (0.5 * (self.budget - self.eval_count) / self.budget)  # Adapt inertia weight\n\n        return global_best_position", "name": "HybridPSODEOptimizer", "description": "Enhanced Hybrid PSODE with adaptive inertia and dynamic population size for better convergence.", "configspace": "", "generation": 51, "fitness": -Infinity, "feedback": "An exception occurred: TypeError(\"'float' object cannot be interpreted as an integer\").", "error": "TypeError(\"'float' object cannot be interpreted as an integer\")", "parent_id": "b8da6caa-d591-4782-9d5c-6e8a21857f17", "metadata": {}, "mutation_prompt": null}
{"id": "f6fe125d-98e8-4530-8f6a-3eaa934793dd", "solution": "import numpy as np\n\nclass HybridPSODEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_agents = 5 + int(0.1 * self.dim)\n        inertia_weight = 0.7\n        cognitive_coefficient = 1.7  # Increased from 1.5 to 1.7\n        social_coefficient = 1.5\n        F = 0.6  # Increased from 0.5 to 0.6\n        CR = 0.9  # Crossover probability\n\n        # Initialize position and velocity for each agent\n        lb, ub = func.bounds.lb, func.bounds.ub\n        positions = np.random.uniform(lb, ub, (num_agents, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_agents, self.dim))\n\n        # Evaluate initial positions\n        fitness = np.apply_along_axis(func, 1, positions)\n        self.eval_count += num_agents\n\n        best_agent_idx = np.argmin(fitness)\n        global_best_position = positions[best_agent_idx].copy()\n        global_best_value = fitness[best_agent_idx]\n\n        personal_best_positions = positions.copy()\n        personal_best_values = fitness.copy()\n\n        while self.eval_count < self.budget:\n            for i in range(num_agents):\n                # Update velocity using PSO\n                r1, r2 = np.random.rand(2)\n                cognitive_component = cognitive_coefficient * r1 * (personal_best_positions[i] - positions[i])\n                social_component = social_coefficient * r2 * (global_best_position - positions[i])\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n\n                # Update position\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], lb, ub)  # ensure within bounds\n\n                # Evaluate current position\n                current_fitness = func(positions[i])\n                self.eval_count += 1\n\n                # Update personal bests\n                if current_fitness < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_fitness\n\n                # Update global best\n                if current_fitness < global_best_value:\n                    global_best_position = positions[i]\n                    global_best_value = current_fitness\n\n                if self.eval_count >= self.budget:\n                    break\n\n            # Differential Evolution-inspired mutation\n            for i in range(num_agents):\n                if np.random.rand() < CR:\n                    indices = np.random.choice(num_agents, 3, replace=False)\n                    x1, x2, x3 = positions[indices]\n                    mutant_vector = x1 + F * (x2 - x3)\n                    trial_vector = np.where(np.random.rand(self.dim) < CR, mutant_vector, positions[i])\n                    trial_vector = np.clip(trial_vector, lb, ub)\n\n                    trial_fitness = func(trial_vector)\n                    self.eval_count += 1\n\n                    # Update personal bests and global best for trial vector\n                    if trial_fitness < personal_best_values[i]:\n                        personal_best_positions[i] = trial_vector\n                        personal_best_values[i] = trial_fitness\n\n                    if trial_fitness < global_best_value:\n                        global_best_position = trial_vector\n                        global_best_value = trial_fitness\n\n                    if self.eval_count >= self.budget:\n                        break\n\n        return global_best_position", "name": "HybridPSODEOptimizer", "description": "A slight increase in cognitive coefficient and mutation scaling factor to enhance convergence speed.", "configspace": "", "generation": 52, "fitness": 0.09490432572642175, "feedback": "The algorithm HybridPSODEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.095 with standard deviation 0.002. And the mean value of best solutions found was 0.165 (0. is the best).", "error": "", "parent_id": "b8da6caa-d591-4782-9d5c-6e8a21857f17", "metadata": {"aucs": [0.09652166372515625, 0.09282715728717872, 0.09536415616693028], "final_y": [0.1648557734072934, 0.1648557931230844, 0.16485600134144018]}, "mutation_prompt": null}
{"id": "05687cfc-c281-4a13-96c9-12ed10cd9ecd", "solution": "import numpy as np\n\nclass HybridPSODEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_agents = 5 + int(0.1 * self.dim)\n        inertia_weight = 0.75  # increased from 0.7\n        cognitive_coefficient = 1.5\n        social_coefficient = 1.5\n        F = 0.5  # Differential evolution scaling factor\n        CR = 0.9  # Crossover probability\n\n        # Initialize position and velocity for each agent\n        lb, ub = func.bounds.lb, func.bounds.ub\n        positions = np.random.uniform(lb, ub, (num_agents, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_agents, self.dim))\n\n        # Evaluate initial positions\n        fitness = np.apply_along_axis(func, 1, positions)\n        self.eval_count += num_agents\n\n        best_agent_idx = np.argmin(fitness)\n        global_best_position = positions[best_agent_idx].copy()\n        global_best_value = fitness[best_agent_idx]\n\n        personal_best_positions = positions.copy()\n        personal_best_values = fitness.copy()\n\n        while self.eval_count < self.budget:\n            for i in range(num_agents):\n                # Update velocity using PSO\n                r1, r2 = np.random.rand(2)\n                cognitive_component = cognitive_coefficient * r1 * (personal_best_positions[i] - positions[i])\n                social_component = social_coefficient * r2 * (global_best_position - positions[i])\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n\n                # Update position\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], lb, ub)  # ensure within bounds\n\n                # Evaluate current position\n                current_fitness = func(positions[i])\n                self.eval_count += 1\n\n                # Update personal bests\n                if current_fitness < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_fitness\n\n                # Update global best\n                if current_fitness < global_best_value:\n                    global_best_position = positions[i]\n                    global_best_value = current_fitness\n\n                if self.eval_count >= self.budget:\n                    break\n\n            # Differential Evolution-inspired mutation\n            for i in range(num_agents):\n                if np.random.rand() < CR:\n                    indices = np.random.choice(num_agents, 3, replace=False)\n                    x1, x2, x3 = positions[indices]\n                    mutant_vector = x1 + F * (x2 - x3)\n                    trial_vector = np.where(np.random.rand(self.dim) < CR, mutant_vector, positions[i])\n                    trial_vector = np.clip(trial_vector, lb, ub)\n\n                    trial_fitness = func(trial_vector)\n                    self.eval_count += 1\n\n                    # Update personal bests and global best for trial vector\n                    if trial_fitness < personal_best_values[i]:\n                        personal_best_positions[i] = trial_vector\n                        personal_best_values[i] = trial_fitness\n\n                    if trial_fitness < global_best_value:\n                        global_best_position = trial_vector\n                        global_best_value = trial_fitness\n\n                    if self.eval_count >= self.budget:\n                        break\n\n        return global_best_position", "name": "HybridPSODEOptimizer", "description": "Incrementally boost exploration by slightly increasing the inertia weight to improve convergence.", "configspace": "", "generation": 53, "fitness": 0.08785266913952457, "feedback": "The algorithm HybridPSODEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.088 with standard deviation 0.011. And the mean value of best solutions found was 0.196 (0. is the best).", "error": "", "parent_id": "b8da6caa-d591-4782-9d5c-6e8a21857f17", "metadata": {"aucs": [0.09537037287611516, 0.07240095689382764, 0.09578667764863091], "final_y": [0.16485577520304218, 0.2578100834860172, 0.16485621933931482]}, "mutation_prompt": null}
{"id": "a2592f23-3d12-49ca-b22c-0b604ea7812b", "solution": "import numpy as np\n\nclass HybridPSODEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_agents = 5 + int(0.1 * self.dim)\n        inertia_weight = 0.9 - (0.5 * (self.eval_count / self.budget))  # Adaptive inertia weight\n        cognitive_coefficient = 1.5\n        social_coefficient = 1.5\n        F = 0.5  # Differential evolution scaling factor\n        CR = 0.9  # Crossover probability\n\n        # Initialize position and velocity for each agent\n        lb, ub = func.bounds.lb, func.bounds.ub\n        positions = np.random.uniform(lb, ub, (num_agents, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_agents, self.dim))\n\n        # Evaluate initial positions\n        fitness = np.apply_along_axis(func, 1, positions)\n        self.eval_count += num_agents\n\n        best_agent_idx = np.argmin(fitness)\n        global_best_position = positions[best_agent_idx].copy()\n        global_best_value = fitness[best_agent_idx]\n\n        personal_best_positions = positions.copy()\n        personal_best_values = fitness.copy()\n\n        while self.eval_count < self.budget:\n            for i in range(num_agents):\n                # Update velocity using PSO\n                r1, r2 = np.random.rand(2)\n                cognitive_component = cognitive_coefficient * r1 * (personal_best_positions[i] - positions[i])\n                social_component = social_coefficient * r2 * (global_best_position - positions[i])\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n\n                # Update position\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], lb, ub)  # ensure within bounds\n\n                # Evaluate current position\n                current_fitness = func(positions[i])\n                self.eval_count += 1\n\n                # Update personal bests\n                if current_fitness < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_fitness\n\n                # Update global best\n                if current_fitness < global_best_value:\n                    global_best_position = positions[i]\n                    global_best_value = current_fitness\n\n                if self.eval_count >= self.budget:\n                    break\n\n            # Differential Evolution-inspired mutation\n            for i in range(num_agents):\n                if np.random.rand() < CR:\n                    indices = np.random.choice(num_agents, 3, replace=False)\n                    x1, x2, x3 = positions[indices]\n                    mutant_vector = x1 + F * (x2 - x3)\n                    trial_vector = np.where(np.random.rand(self.dim) < CR, mutant_vector, positions[i])\n                    trial_vector = np.clip(trial_vector, lb, ub)\n\n                    trial_fitness = func(trial_vector)\n                    self.eval_count += 1\n\n                    # Update personal bests and global best for trial vector\n                    if trial_fitness < personal_best_values[i]:\n                        personal_best_positions[i] = trial_vector\n                        personal_best_values[i] = trial_fitness\n\n                    if trial_fitness < global_best_value:\n                        global_best_position = trial_vector\n                        global_best_value = trial_fitness\n\n                    if self.eval_count >= self.budget:\n                        break\n\n        return global_best_position", "name": "HybridPSODEOptimizer", "description": "Hybrid PSO with adaptive inertia weight for improved balance between exploration and exploitation.", "configspace": "", "generation": 54, "fitness": 0.09144204270843548, "feedback": "The algorithm HybridPSODEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.091 with standard deviation 0.001. And the mean value of best solutions found was 0.169 (0. is the best).", "error": "", "parent_id": "b8da6caa-d591-4782-9d5c-6e8a21857f17", "metadata": {"aucs": [0.09229503336017209, 0.08974499923572166, 0.09228609552941269], "final_y": [0.16668006564448634, 0.1685749638659887, 0.17040204035852569]}, "mutation_prompt": null}
{"id": "bbd48cf3-cf20-4e61-842f-91b4961854bd", "solution": "import numpy as np\n\nclass HybridPSODEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_agents = 5 + int(0.1 * self.dim)\n        inertia_weight = 0.8  # Changed from 0.7 to 0.8 to improve exploration\n        cognitive_coefficient = 1.5\n        social_coefficient = 1.5\n        F = 0.5  # Differential evolution scaling factor\n        CR = 0.9  # Crossover probability\n\n        # Initialize position and velocity for each agent\n        lb, ub = func.bounds.lb, func.bounds.ub\n        positions = np.random.uniform(lb, ub, (num_agents, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_agents, self.dim))\n\n        # Evaluate initial positions\n        fitness = np.apply_along_axis(func, 1, positions)\n        self.eval_count += num_agents\n\n        best_agent_idx = np.argmin(fitness)\n        global_best_position = positions[best_agent_idx].copy()\n        global_best_value = fitness[best_agent_idx]\n\n        personal_best_positions = positions.copy()\n        personal_best_values = fitness.copy()\n\n        while self.eval_count < self.budget:\n            for i in range(num_agents):\n                # Update velocity using PSO\n                r1, r2 = np.random.rand(2)\n                cognitive_component = cognitive_coefficient * r1 * (personal_best_positions[i] - positions[i])\n                social_component = social_coefficient * r2 * (global_best_position - positions[i])\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n\n                # Update position\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], lb, ub)  # ensure within bounds\n\n                # Evaluate current position\n                current_fitness = func(positions[i])\n                self.eval_count += 1\n\n                # Update personal bests\n                if current_fitness < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_fitness\n\n                # Update global best\n                if current_fitness < global_best_value:\n                    global_best_position = positions[i]\n                    global_best_value = current_fitness\n\n                if self.eval_count >= self.budget:\n                    break\n\n            # Differential Evolution-inspired mutation\n            for i in range(num_agents):\n                if np.random.rand() < CR:\n                    indices = np.random.choice(num_agents, 3, replace=False)\n                    x1, x2, x3 = positions[indices]\n                    mutant_vector = x1 + F * (x2 - x3)\n                    trial_vector = np.where(np.random.rand(self.dim) < CR, mutant_vector, positions[i])\n                    trial_vector = np.clip(trial_vector, lb, ub)\n\n                    trial_fitness = func(trial_vector)\n                    self.eval_count += 1\n\n                    # Update personal bests and global best for trial vector\n                    if trial_fitness < personal_best_values[i]:\n                        personal_best_positions[i] = trial_vector\n                        personal_best_values[i] = trial_fitness\n\n                    if trial_fitness < global_best_value:\n                        global_best_position = trial_vector\n                        global_best_value = trial_fitness\n\n                    if self.eval_count >= self.budget:\n                        break\n\n        return global_best_position", "name": "HybridPSODEOptimizer", "description": "Hybrid Particle Swarm Optimization with increased inertia weight for better exploration.", "configspace": "", "generation": 55, "fitness": 0.09289271509353318, "feedback": "The algorithm HybridPSODEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.093 with standard deviation 0.004. And the mean value of best solutions found was 0.165 (0. is the best).", "error": "", "parent_id": "b8da6caa-d591-4782-9d5c-6e8a21857f17", "metadata": {"aucs": [0.0951392305216221, 0.08733161946018775, 0.09620729529878969], "final_y": [0.16496556736268864, 0.1648917249580676, 0.16486031736594553]}, "mutation_prompt": null}
{"id": "98458915-7a00-422c-bbdc-3953e730cd26", "solution": "import numpy as np\n\nclass HybridPSODEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_agents = 5 + int(0.1 * self.dim)\n        # Change: Adaptive inertia weight\n        inertia_weight_start, inertia_weight_end = 0.9, 0.4\n        cognitive_coefficient = 1.5\n        social_coefficient = 1.5\n        F = 0.5  # Differential evolution scaling factor\n        CR = 0.9  # Crossover probability\n\n        # Initialize position and velocity for each agent\n        lb, ub = func.bounds.lb, func.bounds.ub\n        positions = np.random.uniform(lb, ub, (num_agents, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_agents, self.dim))\n\n        # Evaluate initial positions\n        fitness = np.apply_along_axis(func, 1, positions)\n        self.eval_count += num_agents\n\n        best_agent_idx = np.argmin(fitness)\n        global_best_position = positions[best_agent_idx].copy()\n        global_best_value = fitness[best_agent_idx]\n\n        personal_best_positions = positions.copy()\n        personal_best_values = fitness.copy()\n\n        while self.eval_count < self.budget:\n            for i in range(num_agents):\n                # Update velocity using PSO\n                r1, r2 = np.random.rand(2)\n                # Modify: Compute adaptive inertia weight\n                inertia_weight = inertia_weight_start - (inertia_weight_start - inertia_weight_end) * (self.eval_count / self.budget)\n                cognitive_component = cognitive_coefficient * r1 * (personal_best_positions[i] - positions[i])\n                social_component = social_coefficient * r2 * (global_best_position - positions[i])\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n\n                # Update position\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], lb, ub)  # ensure within bounds\n\n                # Evaluate current position\n                current_fitness = func(positions[i])\n                self.eval_count += 1\n\n                # Update personal bests\n                if current_fitness < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_fitness\n\n                # Update global best\n                if current_fitness < global_best_value:\n                    global_best_position = positions[i]\n                    global_best_value = current_fitness\n\n                if self.eval_count >= self.budget:\n                    break\n\n            # Differential Evolution-inspired mutation\n            for i in range(num_agents):\n                if np.random.rand() < CR:\n                    indices = np.random.choice(num_agents, 3, replace=False)\n                    x1, x2, x3 = positions[indices]\n                    mutant_vector = x1 + F * (x2 - x3)\n                    trial_vector = np.where(np.random.rand(self.dim) < CR, mutant_vector, positions[i])\n                    trial_vector = np.clip(trial_vector, lb, ub)\n\n                    trial_fitness = func(trial_vector)\n                    self.eval_count += 1\n\n                    # Update personal bests and global best for trial vector\n                    if trial_fitness < personal_best_values[i]:\n                        personal_best_positions[i] = trial_vector\n                        personal_best_values[i] = trial_fitness\n\n                    if trial_fitness < global_best_value:\n                        global_best_position = trial_vector\n                        global_best_value = trial_fitness\n\n                    if self.eval_count >= self.budget:\n                        break\n\n        return global_best_position", "name": "HybridPSODEOptimizer", "description": "Enhanced Hybrid Particle Swarm Optimization with Adaptive Inertia Weight to balance exploration and exploitation.", "configspace": "", "generation": 56, "fitness": 0.08740876224087786, "feedback": "The algorithm HybridPSODEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.087 with standard deviation 0.011. And the mean value of best solutions found was 0.196 (0. is the best).", "error": "", "parent_id": "b8da6caa-d591-4782-9d5c-6e8a21857f17", "metadata": {"aucs": [0.09444338358818383, 0.072179963164686, 0.09560293996976377], "final_y": [0.16485597975175115, 0.25781008208797596, 0.16485577236757043]}, "mutation_prompt": null}
{"id": "49ae55b6-4172-4fbb-bab7-69b5a89c02b1", "solution": "import numpy as np\n\nclass HybridPSODEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_agents = 5 + int(0.1 * self.dim)\n        inertia_weight = 0.9  # Adjusted for adaptive control\n        cognitive_coefficient = 1.5\n        social_coefficient = 1.5\n        F = 0.5\n        CR = 0.9\n\n        # Initialize position and velocity for each agent\n        lb, ub = func.bounds.lb, func.bounds.ub\n        positions = np.random.uniform(lb, ub, (num_agents, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_agents, self.dim))\n\n        # Evaluate initial positions\n        fitness = np.apply_along_axis(func, 1, positions)\n        self.eval_count += num_agents\n\n        best_agent_idx = np.argmin(fitness)\n        global_best_position = positions[best_agent_idx].copy()\n        global_best_value = fitness[best_agent_idx]\n\n        personal_best_positions = positions.copy()\n        personal_best_values = fitness.copy()\n\n        while self.eval_count < self.budget:\n            inertia_weight = 0.9 - ((0.9 - 0.4) * self.eval_count / self.budget)  # Adaptive inertia\n            for i in range(num_agents):\n                # Update velocity using PSO\n                r1, r2 = np.random.rand(2)\n                cognitive_component = cognitive_coefficient * r1 * (personal_best_positions[i] - positions[i])\n                social_component = social_coefficient * r2 * (global_best_position - positions[i])\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n\n                # Update position\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], lb, ub)\n\n                # Evaluate current position\n                current_fitness = func(positions[i])\n                self.eval_count += 1\n\n                # Update personal bests\n                if current_fitness < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_fitness\n\n                # Update global best\n                if current_fitness < global_best_value:\n                    global_best_position = positions[i]\n                    global_best_value = current_fitness\n\n                if self.eval_count >= self.budget:\n                    break\n\n            # Differential Evolution-inspired mutation\n            for i in range(num_agents):\n                if np.random.rand() < CR:\n                    indices = np.random.choice(num_agents, 3, replace=False)\n                    x1, x2, x3 = positions[indices]\n                    mutant_vector = x1 + F * (x2 - x3)\n                    trial_vector = np.where(np.random.random(self.dim) < CR + 0.1, mutant_vector, positions[i])  # Enhanced crossover\n                    trial_vector = np.clip(trial_vector, lb, ub)\n\n                    trial_fitness = func(trial_vector)\n                    self.eval_count += 1\n\n                    # Update personal bests and global best for trial vector\n                    if trial_fitness < personal_best_values[i]:\n                        personal_best_positions[i] = trial_vector\n                        personal_best_values[i] = trial_fitness\n\n                    if trial_fitness < global_best_value:\n                        global_best_position = trial_vector\n                        global_best_value = trial_fitness\n\n                    if self.eval_count >= self.budget:\n                        break\n\n        return global_best_position", "name": "HybridPSODEOptimizer", "description": "Enhanced Hybrid PSO with Adaptive Inertia and Enhanced Crossover for better exploration-exploitation balance.", "configspace": "", "generation": 57, "fitness": 0.08682832568672771, "feedback": "The algorithm HybridPSODEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.087 with standard deviation 0.003. And the mean value of best solutions found was 0.193 (0. is the best).", "error": "", "parent_id": "b8da6caa-d591-4782-9d5c-6e8a21857f17", "metadata": {"aucs": [0.08923257549079988, 0.08882796214043776, 0.0824244394289455], "final_y": [0.18651279242338348, 0.1814052402917049, 0.2098454674157465]}, "mutation_prompt": null}
{"id": "82a17768-b417-48a2-97d3-cabf3c7ba235", "solution": "import numpy as np\n\nclass EnhancedHybridPSODEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_agents = 5 + int(0.1 * self.dim)\n        inertia_weight = 0.9  # Start with a higher inertia weight\n        cognitive_coefficient = 1.5\n        social_coefficient = 1.5\n        F = 0.5  # Differential evolution scaling factor\n        CR = 0.9  # Crossover probability\n\n        # Initialize position and velocity for each agent\n        lb, ub = func.bounds.lb, func.bounds.ub\n        positions = np.random.uniform(lb, ub, (num_agents, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_agents, self.dim))\n\n        # Evaluate initial positions\n        fitness = np.apply_along_axis(func, 1, positions)\n        self.eval_count += num_agents\n\n        best_agent_idx = np.argmin(fitness)\n        global_best_position = positions[best_agent_idx].copy()\n        global_best_value = fitness[best_agent_idx]\n\n        personal_best_positions = positions.copy()\n        personal_best_values = fitness.copy()\n\n        while self.eval_count < self.budget:\n            # Adaptive inertia weight\n            inertia_weight = 0.4 + (0.5 * (self.budget - self.eval_count) / self.budget)\n\n            for i in range(num_agents):\n                # Update velocity using PSO\n                r1, r2 = np.random.rand(2)\n                cognitive_component = cognitive_coefficient * r1 * (personal_best_positions[i] - positions[i])\n                social_component = social_coefficient * r2 * (global_best_position - positions[i])\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n\n                # Update position\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], lb, ub)  # ensure within bounds\n\n                # Evaluate current position\n                current_fitness = func(positions[i])\n                self.eval_count += 1\n\n                # Update personal bests\n                if current_fitness < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_fitness\n\n                # Update global best\n                if current_fitness < global_best_value:\n                    global_best_position = positions[i]\n                    global_best_value = current_fitness\n\n                if self.eval_count >= self.budget:\n                    break\n\n            # Differential Evolution-inspired mutation\n            for i in range(num_agents):\n                if np.random.rand() < CR:\n                    indices = np.random.choice(num_agents, 3, replace=False)\n                    x1, x2, x3 = positions[indices]\n                    mutant_vector = x1 + F * (x2 - x3)\n                    trial_vector = np.where(np.random.rand(self.dim) < CR, mutant_vector, positions[i])\n                    trial_vector = np.clip(trial_vector, lb, ub)\n\n                    trial_fitness = func(trial_vector)\n                    self.eval_count += 1\n\n                    # Update personal bests and global best for trial vector\n                    if trial_fitness < personal_best_values[i]:\n                        personal_best_positions[i] = trial_vector\n                        personal_best_values[i] = trial_fitness\n\n                    if trial_fitness < global_best_value:\n                        global_best_position = trial_vector\n                        global_best_value = trial_fitness\n\n                    if self.eval_count >= self.budget:\n                        break\n\n            # Randomized restart to avoid stagnation\n            if self.eval_count < self.budget and np.random.rand() < 0.05:\n                restart_idx = np.random.choice(num_agents)\n                positions[restart_idx] = np.random.uniform(lb, ub, self.dim)\n\n        return global_best_position", "name": "EnhancedHybridPSODEOptimizer", "description": "Enhanced Hybrid PSODE with Adaptive Inertia and Randomized Restart for improved exploration and convergence.", "configspace": "", "generation": 58, "fitness": 0.08788146939807302, "feedback": "The algorithm EnhancedHybridPSODEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.088 with standard deviation 0.006. And the mean value of best solutions found was 0.171 (0. is the best).", "error": "", "parent_id": "b8da6caa-d591-4782-9d5c-6e8a21857f17", "metadata": {"aucs": [0.09356466035303235, 0.07945454313552403, 0.09062520470566271], "final_y": [0.1648557808276203, 0.1648561804761024, 0.18187810222705703]}, "mutation_prompt": null}
{"id": "1d45fe22-f9fb-4140-83f5-a93a98a58c87", "solution": "import numpy as np\n\nclass HybridPSODEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_agents = 5 + int(0.1 * self.dim)\n        inertia_weight = 0.72  # Slightly increased inertia weight\n        cognitive_coefficient = 1.5\n        social_coefficient = 1.5\n        F = 0.5  # Differential evolution scaling factor\n        CR = 0.9  # Crossover probability\n\n        # Initialize position and velocity for each agent\n        lb, ub = func.bounds.lb, func.bounds.ub\n        positions = np.random.uniform(lb, ub, (num_agents, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_agents, self.dim))\n\n        # Evaluate initial positions\n        fitness = np.apply_along_axis(func, 1, positions)\n        self.eval_count += num_agents\n\n        best_agent_idx = np.argmin(fitness)\n        global_best_position = positions[best_agent_idx].copy()\n        global_best_value = fitness[best_agent_idx]\n\n        personal_best_positions = positions.copy()\n        personal_best_values = fitness.copy()\n\n        while self.eval_count < self.budget:\n            for i in range(num_agents):\n                # Update velocity using PSO\n                r1, r2 = np.random.rand(2)\n                cognitive_component = cognitive_coefficient * r1 * (personal_best_positions[i] - positions[i])\n                social_component = social_coefficient * r2 * (global_best_position - positions[i])\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n\n                # Update position\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], lb, ub)  # ensure within bounds\n\n                # Evaluate current position\n                current_fitness = func(positions[i])\n                self.eval_count += 1\n\n                # Update personal bests\n                if current_fitness < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_fitness\n\n                # Update global best\n                if current_fitness < global_best_value:\n                    global_best_position = positions[i]\n                    global_best_value = current_fitness\n\n                if self.eval_count >= self.budget:\n                    break\n\n            # Differential Evolution-inspired mutation\n            for i in range(num_agents):\n                if np.random.rand() < CR:\n                    indices = np.random.choice(num_agents, 3, replace=False)\n                    x1, x2, x3 = positions[indices]\n                    mutant_vector = x1 + F * (x2 - x3)\n                    trial_vector = np.where(np.random.rand(self.dim) < CR, mutant_vector, positions[i])\n                    trial_vector = np.clip(trial_vector, lb, ub)\n\n                    trial_fitness = func(trial_vector)\n                    self.eval_count += 1\n\n                    # Update personal bests and global best for trial vector\n                    if trial_fitness < personal_best_values[i]:\n                        personal_best_positions[i] = trial_vector\n                        personal_best_values[i] = trial_fitness\n\n                    if trial_fitness < global_best_value:\n                        global_best_position = trial_vector\n                        global_best_value = trial_fitness\n\n                    if self.eval_count >= self.budget:\n                        break\n\n        return global_best_position", "name": "HybridPSODEOptimizer", "description": "Enhanced exploration by slightly increasing the inertia weight for broader search space exploration.", "configspace": "", "generation": 59, "fitness": 0.08819297186863025, "feedback": "The algorithm HybridPSODEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.088 with standard deviation 0.011. And the mean value of best solutions found was 0.196 (0. is the best).", "error": "", "parent_id": "b8da6caa-d591-4782-9d5c-6e8a21857f17", "metadata": {"aucs": [0.09668064981650837, 0.0725348025732615, 0.09536346321612088], "final_y": [0.16485577207094426, 0.2578100888478746, 0.1648557724031523]}, "mutation_prompt": null}
{"id": "48c1d03f-7362-4658-81e9-62beacb9e0b9", "solution": "import numpy as np\n\nclass HybridPSODEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_agents = 5 + int(0.1 * self.dim)\n        inertia_weight = np.linspace(0.9, 0.4, self.budget)  # Adaptive inertia weight\n        cognitive_coefficient = 1.5 + np.random.rand() * 0.5  # Randomization\n        social_coefficient = 1.5 + np.random.rand() * 0.5  # Randomization\n        F = 0.5 + np.random.rand() * 0.3  # Adaptive DE scaling factor\n        CR = 0.9\n\n        # Initialize position and velocity for each agent\n        lb, ub = func.bounds.lb, func.bounds.ub\n        positions = np.random.uniform(lb, ub, (num_agents, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_agents, self.dim))\n\n        # Evaluate initial positions\n        fitness = np.apply_along_axis(func, 1, positions)\n        self.eval_count += num_agents\n\n        best_agent_idx = np.argmin(fitness)\n        global_best_position = positions[best_agent_idx].copy()\n        global_best_value = fitness[best_agent_idx]\n\n        personal_best_positions = positions.copy()\n        personal_best_values = fitness.copy()\n\n        while self.eval_count < self.budget:\n            for i in range(num_agents):\n                # Update velocity using PSO\n                r1, r2 = np.random.rand(2)\n                w = inertia_weight[min(self.eval_count, self.budget - 1)]\n                cognitive_component = cognitive_coefficient * r1 * (personal_best_positions[i] - positions[i])\n                social_component = social_coefficient * r2 * (global_best_position - positions[i])\n                velocities[i] = w * velocities[i] + cognitive_component + social_component\n\n                # Update position\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], lb, ub)  # ensure within bounds\n\n                # Evaluate current position\n                current_fitness = func(positions[i])\n                self.eval_count += 1\n\n                # Update personal bests\n                if current_fitness < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_fitness\n\n                # Update global best\n                if current_fitness < global_best_value:\n                    global_best_position = positions[i]\n                    global_best_value = current_fitness\n\n                if self.eval_count >= self.budget:\n                    break\n\n            # Differential Evolution-inspired mutation\n            for i in range(num_agents):\n                if np.random.rand() < CR:\n                    indices = np.random.choice(num_agents, 3, replace=False)\n                    x1, x2, x3 = positions[indices]\n                    mutant_vector = x1 + F * (x2 - x3)\n                    trial_vector = np.where(np.random.rand(self.dim) < CR, mutant_vector, positions[i])\n                    trial_vector = np.clip(trial_vector, lb, ub)\n\n                    trial_fitness = func(trial_vector)\n                    self.eval_count += 1\n\n                    # Update personal bests and global best for trial vector\n                    if trial_fitness < personal_best_values[i]:\n                        personal_best_positions[i] = trial_vector\n                        personal_best_values[i] = trial_fitness\n\n                    if trial_fitness < global_best_value:\n                        global_best_position = trial_vector\n                        global_best_value = trial_fitness\n\n                    if self.eval_count >= self.budget:\n                        break\n\n        return global_best_position", "name": "HybridPSODEOptimizer", "description": "Enhanced Hybrid PSO with Adaptive Parameters for improved convergence by dynamically adjusting PSO and DE components.", "configspace": "", "generation": 60, "fitness": 0.08542535116973311, "feedback": "The algorithm HybridPSODEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.085 with standard deviation 0.010. And the mean value of best solutions found was 0.196 (0. is the best).", "error": "", "parent_id": "b8da6caa-d591-4782-9d5c-6e8a21857f17", "metadata": {"aucs": [0.09334768049359121, 0.07174903902739393, 0.0911793339882142], "final_y": [0.164856087450162, 0.2578100820879541, 0.16486835493821494]}, "mutation_prompt": null}
{"id": "3ab1a61d-7f69-465f-90df-c1c74da130a9", "solution": "import numpy as np\n\nclass HybridPSODEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_agents = 5 + int(0.1 * self.dim)\n        inertia_weight = 0.7\n        cognitive_coefficient = 1.5\n        social_coefficient = 1.5\n        F = 0.5  # Differential evolution scaling factor\n        CR = 0.9  # Crossover probability\n\n        # Initialize position and velocity for each agent\n        lb, ub = func.bounds.lb, func.bounds.ub\n        positions = np.random.uniform(lb, ub, (num_agents, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_agents, self.dim))\n\n        # Evaluate initial positions\n        fitness = np.apply_along_axis(func, 1, positions)\n        self.eval_count += num_agents\n\n        best_agent_idx = np.argmin(fitness)\n        global_best_position = positions[best_agent_idx].copy()\n        global_best_value = fitness[best_agent_idx]\n\n        personal_best_positions = positions.copy()\n        personal_best_values = fitness.copy()\n\n        while self.eval_count < self.budget:\n            for i in range(num_agents):\n                # Update velocity using PSO\n                r1, r2 = np.random.rand(2)\n                inertia_weight = 0.9 - (0.5 * self.eval_count / self.budget)  # Adaptive inertia\n                cognitive_component = cognitive_coefficient * r1 * (personal_best_positions[i] - positions[i])\n                social_component = social_coefficient * r2 * (global_best_position - positions[i])\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n\n                # Update position\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], lb, ub)  # ensure within bounds\n\n                # Evaluate current position\n                current_fitness = func(positions[i])\n                self.eval_count += 1\n\n                # Update personal bests\n                if current_fitness < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_fitness\n\n                # Update global best\n                if current_fitness < global_best_value:\n                    global_best_position = positions[i]\n                    global_best_value = current_fitness\n\n                if self.eval_count >= self.budget:\n                    break\n\n            # Differential Evolution-inspired mutation\n            for i in range(num_agents):\n                if np.random.rand() < CR:\n                    indices = np.random.choice(num_agents, 3, replace=False)\n                    x1, x2, x3 = positions[indices]\n                    mutant_vector = x1 + F * (x2 - x3)\n                    trial_vector = np.where(np.random.rand(self.dim) < CR, mutant_vector, positions[i])\n                    trial_vector = np.clip(trial_vector, lb, ub)\n\n                    trial_fitness = func(trial_vector)\n                    self.eval_count += 1\n\n                    # Update personal bests and global best for trial vector\n                    if trial_fitness < personal_best_values[i]:\n                        personal_best_positions[i] = trial_vector\n                        personal_best_values[i] = trial_fitness\n\n                    if trial_fitness < global_best_value:\n                        global_best_position = trial_vector\n                        global_best_value = trial_fitness\n\n                    if self.eval_count >= self.budget:\n                        break\n\n            # Random Restart Strategy\n            if self.eval_count % (self.budget // 5) == 0:  # Restart periodically\n                random_restart_idx = np.random.choice(num_agents)\n                positions[random_restart_idx] = np.random.uniform(lb, ub, self.dim)\n                velocities[random_restart_idx] = np.random.uniform(-1, 1, self.dim)\n\n        return global_best_position", "name": "HybridPSODEOptimizer", "description": "Enhanced Hybrid Particle Swarm Optimization with Adaptive Inertia and Random Restarts.", "configspace": "", "generation": 61, "fitness": 0.08740876218485687, "feedback": "The algorithm HybridPSODEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.087 with standard deviation 0.011. And the mean value of best solutions found was 0.196 (0. is the best).", "error": "", "parent_id": "b8da6caa-d591-4782-9d5c-6e8a21857f17", "metadata": {"aucs": [0.09444338358818383, 0.072179963164686, 0.09560293980170076], "final_y": [0.16485597975175115, 0.25781008208797596, 0.16485577674760388]}, "mutation_prompt": null}
{"id": "7175ba51-b2dd-4cd9-a12b-470ebf659231", "solution": "import numpy as np\n\nclass HybridPSODEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_agents = 5 + int(0.1 * self.dim)\n        inertia_weight = 0.9  # Adaptive start\n        cognitive_coefficient = 1.5\n        social_coefficient = 1.5\n        F = 0.5  # Differential evolution scaling factor\n        CR = 0.9  # Crossover probability\n\n        # Initialize position and velocity for each agent\n        lb, ub = func.bounds.lb, func.bounds.ub\n        positions = np.random.uniform(lb, ub, (num_agents, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_agents, self.dim))\n\n        # Evaluate initial positions\n        fitness = np.apply_along_axis(func, 1, positions)\n        self.eval_count += num_agents\n\n        best_agent_idx = np.argmin(fitness)\n        global_best_position = positions[best_agent_idx].copy()\n        global_best_value = fitness[best_agent_idx]\n\n        personal_best_positions = positions.copy()\n        personal_best_values = fitness.copy()\n\n        while self.eval_count < self.budget:\n            for i in range(num_agents):\n                # Adaptive inertia weight\n                inertia_weight = 0.4 + 0.5 * (1 - self.eval_count / self.budget)\n\n                # Update velocity using PSO\n                r1, r2 = np.random.rand(2)\n                cognitive_component = cognitive_coefficient * r1 * (personal_best_positions[i] - positions[i])\n                social_component = social_coefficient * r2 * (global_best_position - positions[i])\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n\n                # Update position\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], lb, ub)  # ensure within bounds\n\n                # Evaluate current position\n                current_fitness = func(positions[i])\n                self.eval_count += 1\n\n                # Update personal bests\n                if current_fitness < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_fitness\n\n                # Update global best\n                if current_fitness < global_best_value:\n                    global_best_position = positions[i]\n                    global_best_value = current_fitness\n\n                if self.eval_count >= self.budget:\n                    break\n\n            # Differential Evolution-inspired mutation\n            for i in range(num_agents):\n                if np.random.rand() < CR:\n                    indices = np.random.choice(num_agents, 3, replace=False)\n                    x1, x2, x3 = positions[indices]\n                    F = 0.5 + np.random.rand() * 0.3  # Randomized F\n                    mutant_vector = x1 + F * (x2 - x3)\n                    trial_vector = np.where(np.random.rand(self.dim) < CR, mutant_vector, positions[i])\n                    trial_vector = np.clip(trial_vector, lb, ub)\n\n                    trial_fitness = func(trial_vector)\n                    self.eval_count += 1\n\n                    # Update personal bests and global best for trial vector\n                    if trial_fitness < personal_best_values[i]:\n                        personal_best_positions[i] = trial_vector\n                        personal_best_values[i] = trial_fitness\n\n                    if trial_fitness < global_best_value:\n                        global_best_position = trial_vector\n                        global_best_value = trial_fitness\n\n                    if self.eval_count >= self.budget:\n                        break\n\n        return global_best_position", "name": "HybridPSODEOptimizer", "description": "Improved Hybrid Particle Swarm Optimization with Adaptive Inertia and Randomized Differential Evolution for enhanced adaptability and convergence.", "configspace": "", "generation": 62, "fitness": 0.08453731870689225, "feedback": "The algorithm HybridPSODEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.085 with standard deviation 0.010. And the mean value of best solutions found was 0.204 (0. is the best).", "error": "", "parent_id": "b8da6caa-d591-4782-9d5c-6e8a21857f17", "metadata": {"aucs": [0.09452177612867763, 0.07147901618807151, 0.08761116380392764], "final_y": [0.16485578215947183, 0.25781008209063416, 0.18813064473058527]}, "mutation_prompt": null}
{"id": "fe23e691-bbbd-4a55-b924-eb03b0f6619c", "solution": "import numpy as np\n\nclass HybridPSODEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_agents = 5 + int(0.1 * self.dim)\n        inertia_weight = 0.9\n        inertia_weight_min = 0.4\n        cognitive_coefficient = 1.5\n        social_coefficient = 1.5\n        F = 0.5  # Differential evolution scaling factor\n        CR = 0.9  # Crossover probability\n\n        # Initialize position and velocity for each agent\n        lb, ub = func.bounds.lb, func.bounds.ub\n        positions = np.random.uniform(lb, ub, (num_agents, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_agents, self.dim))\n\n        # Evaluate initial positions\n        fitness = np.apply_along_axis(func, 1, positions)\n        self.eval_count += num_agents\n\n        best_agent_idx = np.argmin(fitness)\n        global_best_position = positions[best_agent_idx].copy()\n        global_best_value = fitness[best_agent_idx]\n\n        personal_best_positions = positions.copy()\n        personal_best_values = fitness.copy()\n\n        while self.eval_count < self.budget:\n            for i in range(num_agents):\n                # Update velocity using PSO\n                r1, r2 = np.random.rand(2)\n                inertia_weight = inertia_weight_max - (inertia_weight_max - inertia_weight_min) * (self.eval_count / self.budget)\n                cognitive_component = cognitive_coefficient * r1 * (personal_best_positions[i] - positions[i])\n                social_component = social_coefficient * r2 * (global_best_position - positions[i])\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n\n                # Update position\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], lb, ub)  # ensure within bounds\n\n                # Evaluate current position\n                current_fitness = func(positions[i])\n                self.eval_count += 1\n\n                # Update personal bests\n                if current_fitness < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_fitness\n\n                # Update global best\n                if current_fitness < global_best_value:\n                    global_best_position = positions[i]\n                    global_best_value = current_fitness\n\n                if self.eval_count >= self.budget:\n                    break\n\n            # Differential Evolution-inspired mutation with local improvement\n            for i in range(num_agents):\n                if np.random.rand() < CR:\n                    indices = np.random.choice(num_agents, 3, replace=False)\n                    x1, x2, x3 = positions[indices]\n                    mutant_vector = x1 + F * (x2 - x3)\n                    local_mutation = 0.05 * np.random.randn(self.dim)\n                    mutant_vector += local_mutation\n                    trial_vector = np.where(np.random.rand(self.dim) < CR, mutant_vector, positions[i])\n                    trial_vector = np.clip(trial_vector, lb, ub)\n\n                    trial_fitness = func(trial_vector)\n                    self.eval_count += 1\n\n                    # Update personal bests and global best for trial vector\n                    if trial_fitness < personal_best_values[i]:\n                        personal_best_positions[i] = trial_vector\n                        personal_best_values[i] = trial_fitness\n\n                    if trial_fitness < global_best_value:\n                        global_best_position = trial_vector\n                        global_best_value = trial_fitness\n\n                    if self.eval_count >= self.budget:\n                        break\n\n        return global_best_position", "name": "HybridPSODEOptimizer", "description": "Enhanced Hybrid PSODEOptimizer with dynamic inertia and local mutation for improved convergence.", "configspace": "", "generation": 63, "fitness": -Infinity, "feedback": "An exception occurred: NameError(\"name 'inertia_weight_max' is not defined\").", "error": "NameError(\"name 'inertia_weight_max' is not defined\")", "parent_id": "b8da6caa-d591-4782-9d5c-6e8a21857f17", "metadata": {}, "mutation_prompt": null}
{"id": "a2b9c115-b525-4bba-a61e-2a4ea07f37c1", "solution": "import numpy as np\n\nclass HybridPSODEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_agents = 5 + int(0.1 * self.dim)\n        inertia_weight = 0.9  # Start with higher inertia weight\n        cognitive_coefficient = 2.0  # Increased cognitive coefficient\n        social_coefficient = 2.0  # Increased social coefficient\n        F = 0.5\n        CR = 0.9\n\n        # Initialize position and velocity for each agent\n        lb, ub = func.bounds.lb, func.bounds.ub\n        positions = np.random.uniform(lb, ub, (num_agents, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_agents, self.dim))\n\n        # Evaluate initial positions\n        fitness = np.apply_along_axis(func, 1, positions)\n        self.eval_count += num_agents\n\n        best_agent_idx = np.argmin(fitness)\n        global_best_position = positions[best_agent_idx].copy()\n        global_best_value = fitness[best_agent_idx]\n\n        personal_best_positions = positions.copy()\n        personal_best_values = fitness.copy()\n\n        while self.eval_count < self.budget:\n            for i in range(num_agents):\n                # Update velocity using PSO\n                r1, r2 = np.random.rand(2)\n                cognitive_component = cognitive_coefficient * r1 * (personal_best_positions[i] - positions[i])\n                social_component = social_coefficient * r2 * (global_best_position - positions[i])\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n                inertia_weight *= 0.99  # Adaptive inertia weight\n\n                # Update position\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], lb, ub)\n\n                # Evaluate current position\n                current_fitness = func(positions[i])\n                self.eval_count += 1\n\n                # Update personal bests\n                if current_fitness < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_fitness\n\n                # Update global best\n                if current_fitness < global_best_value:\n                    global_best_position = positions[i]\n                    global_best_value = current_fitness\n\n                if self.eval_count >= self.budget:\n                    break\n\n            # Differential Evolution-inspired mutation\n            for i in range(num_agents):\n                if np.random.rand() < CR:\n                    indices = np.random.choice(num_agents, 3, replace=False)\n                    x1, x2, x3 = positions[indices]\n                    mutation_factor = np.random.rand() * 0.9 + 0.1  # Dynamic mutation factor\n                    mutant_vector = x1 + mutation_factor * (x2 - x3)\n                    trial_vector = np.where(np.random.rand(self.dim) < CR, mutant_vector, positions[i])\n                    trial_vector = np.clip(trial_vector, lb, ub)\n\n                    trial_fitness = func(trial_vector)\n                    self.eval_count += 1\n\n                    # Update personal bests and global best for trial vector\n                    if trial_fitness < personal_best_values[i]:\n                        personal_best_positions[i] = trial_vector\n                        personal_best_values[i] = trial_fitness\n\n                    if trial_fitness < global_best_value:\n                        global_best_position = trial_vector\n                        global_best_value = trial_fitness\n\n                    if self.eval_count >= self.budget:\n                        break\n\n        return global_best_position", "name": "HybridPSODEOptimizer", "description": "Enhanced Hybrid PSO with Adaptive Parameters and Dynamic Mutation for improved convergence and diversity.", "configspace": "", "generation": 64, "fitness": 0.07065526935820617, "feedback": "The algorithm HybridPSODEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.071 with standard deviation 0.019. And the mean value of best solutions found was 0.282 (0. is the best).", "error": "", "parent_id": "b8da6caa-d591-4782-9d5c-6e8a21857f17", "metadata": {"aucs": [0.0956247553174131, 0.06604331375401018, 0.05029773900319523], "final_y": [0.16485613795796217, 0.29189191388001745, 0.3900961488850144]}, "mutation_prompt": null}
{"id": "4166bacb-b968-4eea-8e87-16e096a08c1f", "solution": "import numpy as np\n\nclass HybridPSODEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_agents = 5 + int(0.1 * self.dim)\n        inertia_weight = 0.9  # Changed from 0.7 to 0.9 for adaptive strategy\n        cognitive_coefficient = 1.5\n        social_coefficient = 1.5\n        F = 0.5  # Differential evolution scaling factor\n        CR = 0.9  # Crossover probability\n\n        # Initialize position and velocity for each agent\n        lb, ub = func.bounds.lb, func.bounds.ub\n        positions = np.random.uniform(lb, ub, (num_agents, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_agents, self.dim))\n\n        # Evaluate initial positions\n        fitness = np.apply_along_axis(func, 1, positions)\n        self.eval_count += num_agents\n\n        best_agent_idx = np.argmin(fitness)\n        global_best_position = positions[best_agent_idx].copy()\n        global_best_value = fitness[best_agent_idx]\n\n        personal_best_positions = positions.copy()\n        personal_best_values = fitness.copy()\n\n        while self.eval_count < self.budget:\n            for i in range(num_agents):\n                # Update velocity using PSO\n                r1, r2 = np.random.rand(2)\n                cognitive_component = cognitive_coefficient * r1 * (personal_best_positions[i] - positions[i])\n                social_component = social_coefficient * r2 * (global_best_position - positions[i])\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n\n                # Update position\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], lb, ub)  # ensure within bounds\n\n                # Evaluate current position\n                current_fitness = func(positions[i])\n                self.eval_count += 1\n\n                # Update personal bests\n                if current_fitness < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_fitness\n\n                # Update global best\n                if current_fitness < global_best_value:\n                    global_best_position = positions[i]\n                    global_best_value = current_fitness\n\n                if self.eval_count >= self.budget:\n                    break\n\n            # Differential Evolution-inspired mutation\n            for i in range(num_agents):\n                if np.random.rand() < CR:\n                    indices = np.random.choice(num_agents, 3, replace=False)\n                    x1, x2, x3 = positions[indices]\n                    mutant_vector = x1 + F * (x2 - x3)\n                    trial_vector = np.where(np.random.rand(self.dim) < CR, mutant_vector, positions[i])\n                    trial_vector = np.clip(trial_vector, lb, ub)\n\n                    trial_fitness = func(trial_vector)\n                    self.eval_count += 1\n\n                    # Update personal bests and global best for trial vector\n                    if trial_fitness < personal_best_values[i]:\n                        personal_best_positions[i] = trial_vector\n                        personal_best_values[i] = trial_fitness\n\n                    if trial_fitness < global_best_value:\n                        global_best_position = trial_vector\n                        global_best_value = trial_fitness\n\n                    if self.eval_count >= self.budget:\n                        break\n\n        return global_best_position", "name": "HybridPSODEOptimizer", "description": "An adaptive strategy adjusts the inertia weight of PSO for improved convergence control.", "configspace": "", "generation": 65, "fitness": 0.09144204270843548, "feedback": "The algorithm HybridPSODEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.091 with standard deviation 0.001. And the mean value of best solutions found was 0.169 (0. is the best).", "error": "", "parent_id": "b8da6caa-d591-4782-9d5c-6e8a21857f17", "metadata": {"aucs": [0.09229503336017209, 0.08974499923572166, 0.09228609552941269], "final_y": [0.16668006564448634, 0.1685749638659887, 0.17040204035852569]}, "mutation_prompt": null}
{"id": "61d7d1a6-7747-429a-a758-19b1b74d6eaf", "solution": "import numpy as np\n\nclass HybridPSODEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_agents = 5 + int(0.1 * self.dim)\n        inertia_weight = 0.9  # Adaptive inertia weight\n        cognitive_coefficient = 1.5\n        social_coefficient = 1.5\n        F = 0.8  # Enhanced DE scaling factor\n        CR = 0.9  # Crossover probability\n\n        # Initialize position and velocity for each agent\n        lb, ub = func.bounds.lb, func.bounds.ub\n        positions = np.random.uniform(lb, ub, (num_agents, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_agents, self.dim))\n\n        # Evaluate initial positions\n        fitness = np.apply_along_axis(func, 1, positions)\n        self.eval_count += num_agents\n\n        best_agent_idx = np.argmin(fitness)\n        global_best_position = positions[best_agent_idx].copy()\n        global_best_value = fitness[best_agent_idx]\n\n        personal_best_positions = positions.copy()\n        personal_best_values = fitness.copy()\n\n        while self.eval_count < self.budget:\n            # Adaptive inertia weight\n            inertia_weight = 0.4 + (0.5 * (self.budget - self.eval_count) / self.budget)\n\n            for i in range(num_agents):\n                # Update velocity using PSO\n                r1, r2 = np.random.rand(2)\n                cognitive_component = cognitive_coefficient * r1 * (personal_best_positions[i] - positions[i])\n                social_component = social_coefficient * r2 * (global_best_position - positions[i])\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n\n                # Update position\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], lb, ub)  # ensure within bounds\n\n                # Evaluate current position\n                current_fitness = func(positions[i])\n                self.eval_count += 1\n\n                # Update personal bests\n                if current_fitness < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_fitness\n\n                # Update global best\n                if current_fitness < global_best_value:\n                    global_best_position = positions[i]\n                    global_best_value = current_fitness\n\n                if self.eval_count >= self.budget:\n                    break\n\n            # Differential Evolution-inspired mutation with adaptive F\n            for i in range(num_agents):\n                if np.random.rand() < CR:\n                    indices = np.random.choice(num_agents, 3, replace=False)\n                    x1, x2, x3 = positions[indices]\n                    F = 0.2 + 0.6 * np.random.rand()  # Adaptive F\n                    mutant_vector = x1 + F * (x2 - x3)\n                    trial_vector = np.where(np.random.rand(self.dim) < CR, mutant_vector, positions[i])\n                    trial_vector = np.clip(trial_vector, lb, ub)\n\n                    trial_fitness = func(trial_vector)\n                    self.eval_count += 1\n\n                    # Update personal bests and global best for trial vector\n                    if trial_fitness < personal_best_values[i]:\n                        personal_best_positions[i] = trial_vector\n                        personal_best_values[i] = trial_fitness\n\n                    if trial_fitness < global_best_value:\n                        global_best_position = trial_vector\n                        global_best_value = trial_fitness\n\n                    if self.eval_count >= self.budget:\n                        break\n\n        return global_best_position", "name": "HybridPSODEOptimizer", "description": "Enhanced Hybrid PSO with Adaptive Inertia and Mutation Strategy for Improved Convergence.", "configspace": "", "generation": 66, "fitness": 0.09089360688131869, "feedback": "The algorithm HybridPSODEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.091 with standard deviation 0.003. And the mean value of best solutions found was 0.171 (0. is the best).", "error": "", "parent_id": "b8da6caa-d591-4782-9d5c-6e8a21857f17", "metadata": {"aucs": [0.09491060178197419, 0.08946699723472651, 0.08830322162725535], "final_y": [0.1648559104111399, 0.16485585318802287, 0.1818781253037478]}, "mutation_prompt": null}
{"id": "d97bfeea-d82d-4f24-b6d3-b75ee6fd0dcd", "solution": "import numpy as np\n\nclass HybridPSODEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_agents = 5 + int(0.1 * self.dim)\n        inertia_weight = 0.9  # Changed initial inertia weight\n        cognitive_coefficient = 1.5\n        social_coefficient = 1.5\n        F = 0.5  # Differential evolution scaling factor\n        CR = 0.9  # Crossover probability\n\n        # Initialize position and velocity for each agent\n        lb, ub = func.bounds.lb, func.bounds.ub\n        positions = np.random.uniform(lb, ub, (num_agents, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_agents, self.dim))\n\n        # Evaluate initial positions\n        fitness = np.apply_along_axis(func, 1, positions)\n        self.eval_count += num_agents\n\n        best_agent_idx = np.argmin(fitness)\n        global_best_position = positions[best_agent_idx].copy()\n        global_best_value = fitness[best_agent_idx]\n\n        personal_best_positions = positions.copy()\n        personal_best_values = fitness.copy()\n\n        while self.eval_count < self.budget:\n            for i in range(num_agents):\n                # Update velocity using PSO\n                r1, r2 = np.random.rand(2)\n                # Decrease inertia weight linearly over iterations\n                inertia_weight = 0.4 + (0.9 - 0.4) * (1 - self.eval_count / self.budget)\n                cognitive_component = cognitive_coefficient * r1 * (personal_best_positions[i] - positions[i])\n                social_component = social_coefficient * r2 * (global_best_position - positions[i])\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n\n                # Update position\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], lb, ub)  # ensure within bounds\n\n                # Evaluate current position\n                current_fitness = func(positions[i])\n                self.eval_count += 1\n\n                # Update personal bests\n                if current_fitness < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_fitness\n\n                # Update global best\n                if current_fitness < global_best_value:\n                    global_best_position = positions[i]\n                    global_best_value = current_fitness\n\n                if self.eval_count >= self.budget:\n                    break\n\n            # Differential Evolution-inspired mutation\n            for i in range(num_agents):\n                if np.random.rand() < CR:\n                    indices = np.random.choice(num_agents, 3, replace=False)\n                    x1, x2, x3 = positions[indices]\n                    mutant_vector = x1 + F * (x2 - x3)\n                    trial_vector = np.where(np.random.rand(self.dim) < CR, mutant_vector, positions[i])\n                    trial_vector = np.clip(trial_vector, lb, ub)\n\n                    trial_fitness = func(trial_vector)\n                    self.eval_count += 1\n\n                    # Update personal bests and global best for trial vector\n                    if trial_fitness < personal_best_values[i]:\n                        personal_best_positions[i] = trial_vector\n                        personal_best_values[i] = trial_fitness\n\n                    if trial_fitness < global_best_value:\n                        global_best_position = trial_vector\n                        global_best_value = trial_fitness\n\n                    if self.eval_count >= self.budget:\n                        break\n\n        return global_best_position", "name": "HybridPSODEOptimizer", "description": "Enhanced exploration by introducing dynamic adjustment of the inertia weight in PSO to balance exploration and exploitation.", "configspace": "", "generation": 67, "fitness": 0.08740876224084482, "feedback": "The algorithm HybridPSODEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.087 with standard deviation 0.011. And the mean value of best solutions found was 0.196 (0. is the best).", "error": "", "parent_id": "b8da6caa-d591-4782-9d5c-6e8a21857f17", "metadata": {"aucs": [0.09444338358818405, 0.07217996316458664, 0.09560293996976377], "final_y": [0.1648559797517516, 0.25781008208798784, 0.16485577236757043]}, "mutation_prompt": null}
{"id": "fe548a50-afcf-4ab3-af16-f91defeba36d", "solution": "import numpy as np\n\nclass HybridPSODEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_agents = 5 + int(0.1 * self.dim)\n        initial_inertia_weight = 0.9\n        final_inertia_weight = 0.4\n        cognitive_coefficient = 1.5\n        social_coefficient = 1.5\n        F_init, F_final = 0.4, 0.9\n        CR_init, CR_final = 0.6, 0.95\n        \n        # Initializing positions and velocities\n        lb, ub = func.bounds.lb, func.bounds.ub\n        positions = np.random.uniform(lb, ub, (num_agents, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_agents, self.dim))\n\n        fitness = np.apply_along_axis(func, 1, positions)\n        self.eval_count += num_agents\n\n        best_agent_idx = np.argmin(fitness)\n        global_best_position = positions[best_agent_idx].copy()\n        global_best_value = fitness[best_agent_idx]\n\n        personal_best_positions = positions.copy()\n        personal_best_values = fitness.copy()\n\n        while self.eval_count < self.budget:\n            inertia_weight = initial_inertia_weight - (initial_inertia_weight - final_inertia_weight) * (self.eval_count / self.budget)\n\n            for i in range(num_agents):\n                r1, r2 = np.random.rand(2)\n                cognitive_component = cognitive_coefficient * r1 * (personal_best_positions[i] - positions[i])\n                social_component = social_coefficient * r2 * (global_best_position - positions[i])\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], lb, ub)\n\n                current_fitness = func(positions[i])\n                self.eval_count += 1\n\n                if current_fitness < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_fitness\n\n                if current_fitness < global_best_value:\n                    global_best_position = positions[i]\n                    global_best_value = current_fitness\n\n                if self.eval_count >= self.budget:\n                    break\n\n            F = F_init + (F_final - F_init) * (self.eval_count / self.budget)\n            CR = CR_init + (CR_final - CR_init) * (self.eval_count / self.budget)\n\n            for i in range(num_agents):\n                if np.random.rand() < CR:\n                    indices = np.random.choice(num_agents, 3, replace=False)\n                    x1, x2, x3 = positions[indices]\n                    mutant_vector = x1 + F * (x2 - x3)\n                    trial_vector = np.where(np.random.rand(self.dim) < CR, mutant_vector, positions[i])\n                    trial_vector = np.clip(trial_vector, lb, ub)\n\n                    trial_fitness = func(trial_vector)\n                    self.eval_count += 1\n\n                    if trial_fitness < personal_best_values[i]:\n                        personal_best_positions[i] = trial_vector\n                        personal_best_values[i] = trial_fitness\n\n                    if trial_fitness < global_best_value:\n                        global_best_position = trial_vector\n                        global_best_value = trial_fitness\n\n                    if self.eval_count >= self.budget:\n                        break\n                        \n        return global_best_position", "name": "HybridPSODEOptimizer", "description": "Enhanced Hybrid PSO-DE with adaptive inertia weight and dynamic F and CR values for improved convergence and stability.", "configspace": "", "generation": 68, "fitness": 0.07296307278952685, "feedback": "The algorithm HybridPSODEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.073 with standard deviation 0.018. And the mean value of best solutions found was 0.271 (0. is the best).", "error": "", "parent_id": "b8da6caa-d591-4782-9d5c-6e8a21857f17", "metadata": {"aucs": [0.09589901822519353, 0.05066503606188666, 0.07232516408150036], "final_y": [0.1648557722068702, 0.39009545470304063, 0.25781008208833656]}, "mutation_prompt": null}
{"id": "885f114d-978f-4169-b56f-044b4b53cb29", "solution": "import numpy as np\n\nclass HybridPSODEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_agents = 5 + int(0.1 * self.dim)\n        inertia_weight = 0.9  # Adaptive inertia weight\n        cognitive_coefficient = 1.5\n        social_coefficient = 1.5\n        F = 0.5  # Differential evolution scaling factor\n        CR = 0.9  # Crossover probability\n\n        # Initialize position and velocity for each agent\n        lb, ub = func.bounds.lb, func.bounds.ub\n        positions = np.random.uniform(lb, ub, (num_agents, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_agents, self.dim))\n\n        # Evaluate initial positions\n        fitness = np.apply_along_axis(func, 1, positions)\n        self.eval_count += num_agents\n\n        best_agent_idx = np.argmin(fitness)\n        global_best_position = positions[best_agent_idx].copy()\n        global_best_value = fitness[best_agent_idx]\n\n        personal_best_positions = positions.copy()\n        personal_best_values = fitness.copy()\n\n        while self.eval_count < self.budget:\n            for i in range(num_agents):\n                # Update velocity using PSO with adaptive inertia\n                r1, r2 = np.random.rand(2)\n                cognitive_component = cognitive_coefficient * r1 * (personal_best_positions[i] - positions[i])\n                social_component = social_coefficient * r2 * (global_best_position - positions[i])\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n                inertia_weight *= 0.99  # Decay inertia weight\n\n                # Update position\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], lb, ub)  # ensure within bounds\n\n                # Evaluate current position\n                current_fitness = func(positions[i])\n                self.eval_count += 1\n\n                # Update personal bests\n                if current_fitness < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_fitness\n\n                # Update global best\n                if current_fitness < global_best_value:\n                    global_best_position = positions[i]\n                    global_best_value = current_fitness\n\n                if self.eval_count >= self.budget:\n                    break\n\n            # Differential Evolution-inspired mutation\n            for i in range(num_agents):\n                if np.random.rand() < CR:\n                    indices = np.random.choice(num_agents, 3, replace=False)\n                    x1, x2, x3 = positions[indices]\n                    mutant_vector = x1 + F * (x2 - x3)\n                    trial_vector = np.where(np.random.rand(self.dim) < CR, mutant_vector, positions[i])\n                    trial_vector = np.clip(trial_vector, lb, ub)\n\n                    trial_fitness = func(trial_vector)\n                    self.eval_count += 1\n\n                    # Update personal bests and global best for trial vector\n                    if trial_fitness < personal_best_values[i]:\n                        personal_best_positions[i] = trial_vector\n                        personal_best_values[i] = trial_fitness\n\n                    if trial_fitness < global_best_value:\n                        global_best_position = trial_vector\n                        global_best_value = trial_fitness\n\n                    if self.eval_count >= self.budget:\n                        break\n\n            # Local search enhancement\n            for i in range(num_agents):\n                perturbation = np.random.normal(0, 0.1, self.dim)\n                candidate_position = positions[i] + perturbation\n                candidate_position = np.clip(candidate_position, lb, ub)\n                candidate_fitness = func(candidate_position)\n                self.eval_count += 1\n                if candidate_fitness < personal_best_values[i]:\n                    personal_best_positions[i] = candidate_position\n                    personal_best_values[i] = candidate_fitness\n                if candidate_fitness < global_best_value:\n                    global_best_position = candidate_position\n                    global_best_value = candidate_fitness\n\n        return global_best_position", "name": "HybridPSODEOptimizer", "description": "Enhanced Hybrid PSO with Adaptive Inertia and Local Search for improved convergence and solution quality.", "configspace": "", "generation": 69, "fitness": 0.09527829666780026, "feedback": "The algorithm HybridPSODEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.095 with standard deviation 0.001. And the mean value of best solutions found was 0.165 (0. is the best).", "error": "", "parent_id": "b8da6caa-d591-4782-9d5c-6e8a21857f17", "metadata": {"aucs": [0.09656006851235976, 0.0933054549225173, 0.09596936656852373], "final_y": [0.1648559544667768, 0.16485597819028885, 0.16485593897971185]}, "mutation_prompt": null}
{"id": "d30fb2f5-0672-40d0-a4a4-de03e7396e83", "solution": "import numpy as np\n\nclass HybridPSODEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_agents = 5 + int(0.1 * self.dim)\n        inertia_weight = 0.9  # Adaptive inertia weight\n        cognitive_coefficient = 1.5\n        social_coefficient = 1.5\n        F = 0.5  # Differential evolution scaling factor\n        CR = 0.9  # Crossover probability\n\n        # Initialize position and velocity for each agent\n        lb, ub = func.bounds.lb, func.bounds.ub\n        positions = np.random.uniform(lb, ub, (num_agents, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_agents, self.dim))\n\n        # Evaluate initial positions\n        fitness = np.apply_along_axis(func, 1, positions)\n        self.eval_count += num_agents\n\n        best_agent_idx = np.argmin(fitness)\n        global_best_position = positions[best_agent_idx].copy()\n        global_best_value = fitness[best_agent_idx]\n\n        personal_best_positions = positions.copy()\n        personal_best_values = fitness.copy()\n\n        while self.eval_count < self.budget:\n            for i in range(num_agents):\n                # Update velocity using PSO with adaptive inertia\n                r1, r2 = np.random.rand(2)\n                cognitive_component = cognitive_coefficient * r1 * (personal_best_positions[i] - positions[i])\n                social_component = social_coefficient * r2 * (global_best_position - positions[i])\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n                inertia_weight *= 0.99  # Decay inertia weight\n                cognitive_coefficient *= 0.999  # Adaptive cognitive coefficient\n\n                # Update position\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], lb, ub)  # ensure within bounds\n\n                # Evaluate current position\n                current_fitness = func(positions[i])\n                self.eval_count += 1\n\n                # Update personal bests\n                if current_fitness < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_fitness\n\n                # Update global best\n                if current_fitness < global_best_value:\n                    global_best_position = positions[i]\n                    global_best_value = current_fitness\n\n                if self.eval_count >= self.budget:\n                    break\n\n            # Differential Evolution-inspired mutation\n            for i in range(num_agents):\n                if np.random.rand() < CR:\n                    indices = np.random.choice(num_agents, 3, replace=False)\n                    x1, x2, x3 = positions[indices]\n                    mutant_vector = x1 + F * (x2 - x3)\n                    trial_vector = np.where(np.random.rand(self.dim) < CR, mutant_vector, positions[i])\n                    trial_vector = np.clip(trial_vector, lb, ub)\n\n                    trial_fitness = func(trial_vector)\n                    self.eval_count += 1\n\n                    # Update personal bests and global best for trial vector\n                    if trial_fitness < personal_best_values[i]:\n                        personal_best_positions[i] = trial_vector\n                        personal_best_values[i] = trial_fitness\n\n                    if trial_fitness < global_best_value:\n                        global_best_position = trial_vector\n                        global_best_value = trial_fitness\n\n                    if self.eval_count >= self.budget:\n                        break\n\n            # Local search enhancement\n            for i in range(num_agents):\n                perturbation = np.random.normal(0, 0.1, self.dim)\n                candidate_position = positions[i] + perturbation\n                candidate_position = np.clip(candidate_position, lb, ub)\n                candidate_fitness = func(candidate_position)\n                self.eval_count += 1\n                if candidate_fitness < personal_best_values[i]:\n                    personal_best_positions[i] = candidate_position\n                    personal_best_values[i] = candidate_fitness\n                if candidate_fitness < global_best_value:\n                    global_best_position = candidate_position\n                    global_best_value = candidate_fitness\n\n        return global_best_position", "name": "HybridPSODEOptimizer", "description": "Introduced adaptive cognitive coefficient for improved exploration and convergence.", "configspace": "", "generation": 70, "fitness": 0.09423673769528766, "feedback": "The algorithm HybridPSODEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.094 with standard deviation 0.002. And the mean value of best solutions found was 0.165 (0. is the best).", "error": "", "parent_id": "885f114d-978f-4169-b56f-044b4b53cb29", "metadata": {"aucs": [0.09645144096876412, 0.09090247920534855, 0.0953562929117503], "final_y": [0.16485592539694693, 0.16485661831185927, 0.1648558702407349]}, "mutation_prompt": null}
{"id": "31baacc2-9af9-4c0f-8464-e80d04809e6c", "solution": "import numpy as np\n\nclass HybridPSODEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_agents = 5 + int(0.1 * self.dim)\n        inertia_weight = 0.9  # Adaptive inertia weight\n        cognitive_coefficient = 1.6  # Slight increase for more exploration\n        social_coefficient = 1.4  # Slight decrease for less crowding\n        F = 0.6  # Adjusted Differential evolution scaling factor for diversity\n        CR = 0.85  # Reduced crossover probability\n\n        # Initialize position and velocity for each agent\n        lb, ub = func.bounds.lb, func.bounds.ub\n        positions = np.random.uniform(lb, ub, (num_agents, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_agents, self.dim))\n\n        # Evaluate initial positions\n        fitness = np.apply_along_axis(func, 1, positions)\n        self.eval_count += num_agents\n\n        best_agent_idx = np.argmin(fitness)\n        global_best_position = positions[best_agent_idx].copy()\n        global_best_value = fitness[best_agent_idx]\n\n        personal_best_positions = positions.copy()\n        personal_best_values = fitness.copy()\n\n        while self.eval_count < self.budget:\n            for i in range(num_agents):\n                # Update velocity using PSO with adaptive inertia\n                r1, r2 = np.random.rand(2)\n                cognitive_component = cognitive_coefficient * r1 * (personal_best_positions[i] - positions[i])\n                social_component = social_coefficient * r2 * (global_best_position - positions[i])\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n                inertia_weight = max(0.4, inertia_weight * 0.98)  # Dynamic decay with a floor\n\n                # Update position\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], lb, ub)  # ensure within bounds\n\n                # Evaluate current position\n                current_fitness = func(positions[i])\n                self.eval_count += 1\n\n                # Update personal bests\n                if current_fitness < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_fitness\n\n                # Update global best\n                if current_fitness < global_best_value:\n                    global_best_position = positions[i]\n                    global_best_value = current_fitness\n\n                if self.eval_count >= self.budget:\n                    break\n\n            # Differential Evolution-inspired mutation\n            for i in range(num_agents):\n                if np.random.rand() < CR:\n                    indices = np.random.choice(num_agents, 3, replace=False)\n                    x1, x2, x3 = positions[indices]\n                    mutant_vector = x1 + F * (x2 - x3)\n                    trial_vector = np.where(np.random.rand(self.dim) < CR, mutant_vector, positions[i])\n                    trial_vector = np.clip(trial_vector, lb, ub)\n\n                    trial_fitness = func(trial_vector)\n                    self.eval_count += 1\n\n                    # Update personal bests and global best for trial vector\n                    if trial_fitness < personal_best_values[i]:\n                        personal_best_positions[i] = trial_vector\n                        personal_best_values[i] = trial_fitness\n\n                    if trial_fitness < global_best_value:\n                        global_best_position = trial_vector\n                        global_best_value = trial_fitness\n\n                    if self.eval_count >= self.budget:\n                        break\n\n            # Enhanced Local Search\n            for i in range(num_agents):\n                perturbation = np.random.normal(0, np.random.uniform(0.05, 0.15), self.dim)\n                candidate_position = positions[i] + perturbation\n                candidate_position = np.clip(candidate_position, lb, ub)\n                candidate_fitness = func(candidate_position)\n                self.eval_count += 1\n                if candidate_fitness < personal_best_values[i]:\n                    personal_best_positions[i] = candidate_position\n                    personal_best_values[i] = candidate_fitness\n                if candidate_fitness < global_best_value:\n                    global_best_position = candidate_position\n                    global_best_value = candidate_fitness\n\n        return global_best_position", "name": "HybridPSODEOptimizer", "description": "Adaptive Hybrid Algorithm with Dynamic Parameter Adaptation and Randomized Local Search for Enhanced Convergence.", "configspace": "", "generation": 71, "fitness": 0.09582518288850743, "feedback": "The algorithm HybridPSODEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.096 with standard deviation 0.001. And the mean value of best solutions found was 0.165 (0. is the best).", "error": "", "parent_id": "885f114d-978f-4169-b56f-044b4b53cb29", "metadata": {"aucs": [0.09660003920064741, 0.09419967824818087, 0.09667583121669399], "final_y": [0.16485577200448287, 0.16485577209364022, 0.16485577341856605]}, "mutation_prompt": null}
{"id": "50415c49-0ab3-4e54-b712-be064a952c3b", "solution": "import numpy as np\n\nclass HybridPSODEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_agents = 5 + int(0.1 * self.dim)\n        inertia_weight = 0.9  # Adaptive inertia weight\n        cognitive_coefficient = 1.7  # Slight increase for more exploration (changed from 1.6)\n        social_coefficient = 1.4  # Slight decrease for less crowding\n        F = 0.6  # Adjusted Differential evolution scaling factor for diversity\n        CR = 0.85  # Reduced crossover probability\n\n        # Initialize position and velocity for each agent\n        lb, ub = func.bounds.lb, func.bounds.ub\n        positions = np.random.uniform(lb, ub, (num_agents, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_agents, self.dim))\n\n        # Evaluate initial positions\n        fitness = np.apply_along_axis(func, 1, positions)\n        self.eval_count += num_agents\n\n        best_agent_idx = np.argmin(fitness)\n        global_best_position = positions[best_agent_idx].copy()\n        global_best_value = fitness[best_agent_idx]\n\n        personal_best_positions = positions.copy()\n        personal_best_values = fitness.copy()\n\n        while self.eval_count < self.budget:\n            for i in range(num_agents):\n                # Update velocity using PSO with adaptive inertia\n                r1, r2 = np.random.rand(2)\n                cognitive_component = cognitive_coefficient * r1 * (personal_best_positions[i] - positions[i])\n                social_component = social_coefficient * r2 * (global_best_position - positions[i])\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n                inertia_weight = max(0.4, inertia_weight * 0.98)  # Dynamic decay with a floor\n\n                # Update position\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], lb, ub)  # ensure within bounds\n\n                # Evaluate current position\n                current_fitness = func(positions[i])\n                self.eval_count += 1\n\n                # Update personal bests\n                if current_fitness < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_fitness\n\n                # Update global best\n                if current_fitness < global_best_value:\n                    global_best_position = positions[i]\n                    global_best_value = current_fitness\n\n                if self.eval_count >= self.budget:\n                    break\n\n            # Differential Evolution-inspired mutation\n            for i in range(num_agents):\n                if np.random.rand() < CR:\n                    indices = np.random.choice(num_agents, 3, replace=False)\n                    x1, x2, x3 = positions[indices]\n                    mutant_vector = x1 + F * (x2 - x3)\n                    trial_vector = np.where(np.random.rand(self.dim) < CR, mutant_vector, positions[i])\n                    trial_vector = np.clip(trial_vector, lb, ub)\n\n                    trial_fitness = func(trial_vector)\n                    self.eval_count += 1\n\n                    # Update personal bests and global best for trial vector\n                    if trial_fitness < personal_best_values[i]:\n                        personal_best_positions[i] = trial_vector\n                        personal_best_values[i] = trial_fitness\n\n                    if trial_fitness < global_best_value:\n                        global_best_position = trial_vector\n                        global_best_value = trial_fitness\n\n                    if self.eval_count >= self.budget:\n                        break\n\n            # Enhanced Local Search\n            for i in range(num_agents):\n                perturbation = np.random.normal(0, np.random.uniform(0.05, 0.15), self.dim)\n                candidate_position = positions[i] + perturbation\n                candidate_position = np.clip(candidate_position, lb, ub)\n                candidate_fitness = func(candidate_position)\n                self.eval_count += 1\n                if candidate_fitness < personal_best_values[i]:\n                    personal_best_positions[i] = candidate_position\n                    personal_best_values[i] = candidate_fitness\n                if candidate_fitness < global_best_value:\n                    global_best_position = candidate_position\n                    global_best_value = candidate_fitness\n\n        return global_best_position", "name": "HybridPSODEOptimizer", "description": "Enhanced convergence through increased cognitive coefficient for improved exploration balance.", "configspace": "", "generation": 72, "fitness": 0.096843282295294, "feedback": "The algorithm HybridPSODEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.097 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best).", "error": "", "parent_id": "31baacc2-9af9-4c0f-8464-e80d04809e6c", "metadata": {"aucs": [0.09709006186156999, 0.09643745635859469, 0.09700232866571734], "final_y": [0.16485577348856661, 0.16485577194429157, 0.16485577194936785]}, "mutation_prompt": null}
{"id": "81339dc5-3846-441f-ae99-46a6efa880a9", "solution": "import numpy as np\n\nclass HybridPSODEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_agents = 5 + int(0.1 * self.dim)\n        inertia_weight = 0.9  # Adaptive inertia weight\n        cognitive_coefficient = 1.7  # Slight increase for more exploration\n        social_coefficient = 1.4  # Slight decrease for less crowding\n        F = 0.6  # Adjusted Differential evolution scaling factor for diversity\n        CR = 0.85  # Reduced crossover probability\n\n        # Initialize position and velocity for each agent\n        lb, ub = func.bounds.lb, func.bounds.ub\n        positions = np.random.uniform(lb, ub, (num_agents, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_agents, self.dim))\n\n        # Evaluate initial positions\n        fitness = np.apply_along_axis(func, 1, positions)\n        self.eval_count += num_agents\n\n        best_agent_idx = np.argmin(fitness)\n        global_best_position = positions[best_agent_idx].copy()\n        global_best_value = fitness[best_agent_idx]\n\n        personal_best_positions = positions.copy()\n        personal_best_values = fitness.copy()\n\n        while self.eval_count < self.budget:\n            for i in range(num_agents):\n                # Update velocity using PSO with adaptive inertia\n                r1, r2 = np.random.rand(2)\n                cognitive_component = cognitive_coefficient * r1 * (personal_best_positions[i] - positions[i])\n                dynamic_social_coefficient = social_coefficient * (1 - self.eval_count / self.budget)\n                social_component = dynamic_social_coefficient * r2 * (global_best_position - positions[i])\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n                inertia_weight = max(0.4, inertia_weight * 0.98)  # Dynamic decay with a floor\n\n                # Update position\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], lb, ub)  # ensure within bounds\n\n                # Evaluate current position\n                current_fitness = func(positions[i])\n                self.eval_count += 1\n\n                # Update personal bests\n                if current_fitness < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_fitness\n\n                # Update global best\n                if current_fitness < global_best_value:\n                    global_best_position = positions[i]\n                    global_best_value = current_fitness\n\n                if self.eval_count >= self.budget:\n                    break\n\n            # Differential Evolution-inspired mutation\n            for i in range(num_agents):\n                if np.random.rand() < CR:\n                    indices = np.random.choice(num_agents, 3, replace=False)\n                    x1, x2, x3 = positions[indices]\n                    mutant_vector = x1 + F * (x2 - x3)\n                    trial_vector = np.where(np.random.rand(self.dim) < CR, mutant_vector, positions[i])\n                    trial_vector = np.clip(trial_vector, lb, ub)\n\n                    trial_fitness = func(trial_vector)\n                    self.eval_count += 1\n\n                    # Update personal bests and global best for trial vector\n                    if trial_fitness < personal_best_values[i]:\n                        personal_best_positions[i] = trial_vector\n                        personal_best_values[i] = trial_fitness\n\n                    if trial_fitness < global_best_value:\n                        global_best_position = trial_vector\n                        global_best_value = trial_fitness\n\n                    if self.eval_count >= self.budget:\n                        break\n\n            # Enhanced Local Search with Lévy flights\n            beta = 1.5  # Levy flight parameter\n            for i in range(num_agents):\n                levy_step = np.random.standard_cauchy(size=self.dim) * (np.abs(global_best_position-positions[i])**beta)\n                candidate_position = positions[i] + levy_step\n                candidate_position = np.clip(candidate_position, lb, ub)\n                candidate_fitness = func(candidate_position)\n                self.eval_count += 1\n                if candidate_fitness < personal_best_values[i]:\n                    personal_best_positions[i] = candidate_position\n                    personal_best_values[i] = candidate_fitness\n                if candidate_fitness < global_best_value:\n                    global_best_position = candidate_position\n                    global_best_value = candidate_fitness\n\n        return global_best_position", "name": "HybridPSODEOptimizer", "description": "Improved exploitation-exploration balance by adaptive social component and enhanced local search leveraging Lévy flights.", "configspace": "", "generation": 73, "fitness": 0.09477731582580895, "feedback": "The algorithm HybridPSODEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.095 with standard deviation 0.002. And the mean value of best solutions found was 0.171 (0. is the best).", "error": "", "parent_id": "50415c49-0ab3-4e54-b712-be064a952c3b", "metadata": {"aucs": [0.09668431886000017, 0.09278939424836385, 0.09485823436906282], "final_y": [0.16636344419993077, 0.17599640887649814, 0.1713099245739157]}, "mutation_prompt": null}
{"id": "75635699-615a-4a25-a6e2-81c14f199e63", "solution": "import numpy as np\n\nclass HybridPSODEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_agents = 5 + int(0.1 * self.dim)\n        inertia_weight = 0.9  # Adaptive inertia weight\n        cognitive_coefficient = 1.7  # Initial cognitive coefficient\n        social_coefficient = 1.4  # Initial social coefficient\n        F = 0.6  # DE scaling factor for diversity\n        CR = 0.85  # Crossover probability\n\n        # Initialize position and velocity for each agent\n        lb, ub = func.bounds.lb, func.bounds.ub\n        positions = np.random.uniform(lb, ub, (num_agents, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_agents, self.dim))\n\n        # Evaluate initial positions\n        fitness = np.apply_along_axis(func, 1, positions)\n        self.eval_count += num_agents\n\n        best_agent_idx = np.argmin(fitness)\n        global_best_position = positions[best_agent_idx].copy()\n        global_best_value = fitness[best_agent_idx]\n\n        personal_best_positions = positions.copy()\n        personal_best_values = fitness.copy()\n\n        while self.eval_count < self.budget:\n            fitness_variance = np.var(fitness)  # Calculate fitness variance\n            for i in range(num_agents):\n                # Update velocity using PSO with adaptive inertia\n                r1, r2 = np.random.rand(2)\n                adaptive_cognitive = cognitive_coefficient + 0.2 * fitness_variance / (1 + fitness_variance)\n                adaptive_social = social_coefficient + 0.2 * fitness_variance / (1 + fitness_variance)\n                cognitive_component = adaptive_cognitive * r1 * (personal_best_positions[i] - positions[i])\n                social_component = adaptive_social * r2 * (global_best_position - positions[i])\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n                inertia_weight = max(0.4, inertia_weight * 0.98)  # Dynamic decay with a floor\n\n                # Update position\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], lb, ub)  # ensure within bounds\n\n                # Evaluate current position\n                current_fitness = func(positions[i])\n                self.eval_count += 1\n\n                # Update personal bests\n                if current_fitness < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_fitness\n\n                # Update global best\n                if current_fitness < global_best_value:\n                    global_best_position = positions[i]\n                    global_best_value = current_fitness\n\n                if self.eval_count >= self.budget:\n                    break\n\n            # Differential Evolution-inspired mutation\n            for i in range(num_agents):\n                if np.random.rand() < CR:\n                    indices = np.random.choice(num_agents, 3, replace=False)\n                    x1, x2, x3 = positions[indices]\n                    mutant_vector = x1 + F * (x2 - x3)\n                    trial_vector = np.where(np.random.rand(self.dim) < CR, mutant_vector, positions[i])\n                    trial_vector = np.clip(trial_vector, lb, ub)\n\n                    trial_fitness = func(trial_vector)\n                    self.eval_count += 1\n\n                    # Update personal bests and global best for trial vector\n                    if trial_fitness < personal_best_values[i]:\n                        personal_best_positions[i] = trial_vector\n                        personal_best_values[i] = trial_fitness\n\n                    if trial_fitness < global_best_value:\n                        global_best_position = trial_vector\n                        global_best_value = trial_fitness\n\n                    if self.eval_count >= self.budget:\n                        break\n\n            # Enhanced Local Search\n            for i in range(num_agents):\n                perturbation = np.random.normal(0, np.random.uniform(0.05, 0.15), self.dim)\n                candidate_position = positions[i] + perturbation\n                candidate_position = np.clip(candidate_position, lb, ub)\n                candidate_fitness = func(candidate_position)\n                self.eval_count += 1\n                if candidate_fitness < personal_best_values[i]:\n                    personal_best_positions[i] = candidate_position\n                    personal_best_values[i] = candidate_fitness\n                if candidate_fitness < global_best_value:\n                    global_best_position = candidate_position\n                    global_best_value = candidate_fitness\n\n        return global_best_position", "name": "HybridPSODEOptimizer", "description": "Introduce adaptive cognitive and social coefficients based on fitness variance for improved balance between exploration and exploitation.", "configspace": "", "generation": 74, "fitness": 0.09675021684564655, "feedback": "The algorithm HybridPSODEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.097 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best).", "error": "", "parent_id": "50415c49-0ab3-4e54-b712-be064a952c3b", "metadata": {"aucs": [0.09694951950310604, 0.09633145210602401, 0.09696967892780961], "final_y": [0.16485577201520585, 0.1648557720254944, 0.16485577209506574]}, "mutation_prompt": null}
{"id": "0f00a4d3-9474-472e-a272-5876c8fce629", "solution": "import numpy as np\n\nclass HybridPSODEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_agents = 5 + int(0.1 * self.dim)\n        inertia_weight = 0.9  # Adaptive inertia weight\n        cognitive_coefficient = 1.7  # Slight increase for more exploration (changed from 1.6)\n        social_coefficient = 1.4  # Slight decrease for less crowding\n        F = 0.6  # Adjusted Differential evolution scaling factor for diversity\n        CR = 0.85  # Reduced crossover probability\n\n        # Initialize position and velocity for each agent\n        lb, ub = func.bounds.lb, func.bounds.ub\n        positions = np.random.uniform(lb, ub, (num_agents, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_agents, self.dim))\n\n        # Evaluate initial positions\n        fitness = np.apply_along_axis(func, 1, positions)\n        self.eval_count += num_agents\n\n        best_agent_idx = np.argmin(fitness)\n        global_best_position = positions[best_agent_idx].copy()\n        global_best_value = fitness[best_agent_idx]\n\n        personal_best_positions = positions.copy()\n        personal_best_values = fitness.copy()\n\n        while self.eval_count < self.budget:\n            for i in range(num_agents):\n                # Update velocity using PSO with adaptive inertia\n                r1, r2 = np.random.rand(2)\n                cognitive_component = cognitive_coefficient * r1 * (personal_best_positions[i] - positions[i])\n                social_component = social_coefficient * r2 * (global_best_position - positions[i])\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n                inertia_weight = max(0.4, inertia_weight * 0.98)  # Dynamic decay with a floor\n                cognitive_coefficient = max(1.4, cognitive_coefficient * 0.99)  # Adaptive cognitive coefficient\n\n                # Update position\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], lb, ub)  # ensure within bounds\n\n                # Evaluate current position\n                current_fitness = func(positions[i])\n                self.eval_count += 1\n\n                # Update personal bests\n                if current_fitness < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_fitness\n\n                # Update global best\n                if current_fitness < global_best_value:\n                    global_best_position = positions[i]\n                    global_best_value = current_fitness\n\n                if self.eval_count >= self.budget:\n                    break\n\n            # Differential Evolution-inspired mutation\n            for i in range(num_agents):\n                if np.random.rand() < CR:\n                    indices = np.random.choice(num_agents, 3, replace=False)\n                    x1, x2, x3 = positions[indices]\n                    mutant_vector = x1 + F * (x2 - x3)\n                    trial_vector = np.where(np.random.rand(self.dim) < CR, mutant_vector, positions[i])\n                    trial_vector = np.clip(trial_vector, lb, ub)\n\n                    trial_fitness = func(trial_vector)\n                    self.eval_count += 1\n\n                    # Update personal bests and global best for trial vector\n                    if trial_fitness < personal_best_values[i]:\n                        personal_best_positions[i] = trial_vector\n                        personal_best_values[i] = trial_fitness\n\n                    if trial_fitness < global_best_value:\n                        global_best_position = trial_vector\n                        global_best_value = trial_fitness\n\n                    if self.eval_count >= self.budget:\n                        break\n\n            # Enhanced Local Search\n            for i in range(num_agents):\n                perturbation = np.random.normal(0, np.random.uniform(0.05, 0.15), self.dim)\n                candidate_position = positions[i] + perturbation\n                candidate_position = np.clip(candidate_position, lb, ub)\n                candidate_fitness = func(candidate_position)\n                self.eval_count += 1\n                if candidate_fitness < personal_best_values[i]:\n                    personal_best_positions[i] = candidate_position\n                    personal_best_values[i] = candidate_fitness\n                if candidate_fitness < global_best_value:\n                    global_best_position = candidate_position\n                    global_best_value = candidate_fitness\n\n        return global_best_position", "name": "HybridPSODEOptimizer", "description": "Introduced adaptive cognitive coefficient to enhance exploration-exploitation balance.", "configspace": "", "generation": 75, "fitness": 0.09487281797359921, "feedback": "The algorithm HybridPSODEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.095 with standard deviation 0.002. And the mean value of best solutions found was 0.171 (0. is the best).", "error": "", "parent_id": "50415c49-0ab3-4e54-b712-be064a952c3b", "metadata": {"aucs": [0.09650005654966864, 0.09634009096565865, 0.09177830640547036], "final_y": [0.16485580716846338, 0.16485579665953132, 0.1818780968297159]}, "mutation_prompt": null}
{"id": "1a52b660-5f2a-4985-a14d-961189ffaaff", "solution": "import numpy as np\n\nclass HybridPSODEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_agents = 5 + int(0.1 * self.dim)\n        inertia_weight = 0.9  # Adaptive inertia weight\n        cognitive_coefficient = 1.7\n        social_coefficient = 1.4\n        F = 0.6  # Adjusted Differential evolution scaling factor for diversity\n        CR = 0.85  # Reduced crossover probability\n\n        # Initialize position and velocity for each agent\n        lb, ub = func.bounds.lb, func.bounds.ub\n        positions = np.random.uniform(lb, ub, (num_agents, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_agents, self.dim))\n\n        # Evaluate initial positions\n        fitness = np.apply_along_axis(func, 1, positions)\n        self.eval_count += num_agents\n\n        best_agent_idx = np.argmin(fitness)\n        global_best_position = positions[best_agent_idx].copy()\n        global_best_value = fitness[best_agent_idx]\n\n        personal_best_positions = positions.copy()\n        personal_best_values = fitness.copy()\n\n        while self.eval_count < self.budget:\n            for i in range(num_agents):\n                # Update velocity using PSO with adaptive coefficients\n                r1, r2 = np.random.rand(2)\n                adaptive_cognitive = cognitive_coefficient * (1.2 - 0.5 * (self.eval_count / self.budget))  # Adaptive\n                adaptive_social = social_coefficient * (0.8 + 0.5 * (self.eval_count / self.budget))  # Adaptive\n                cognitive_component = adaptive_cognitive * r1 * (personal_best_positions[i] - positions[i])\n                social_component = adaptive_social * r2 * (global_best_position - positions[i])\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n                inertia_weight = max(0.4, inertia_weight * 0.98)  # Dynamic decay with a floor\n\n                # Update position\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], lb, ub)  # ensure within bounds\n\n                # Evaluate current position\n                current_fitness = func(positions[i])\n                self.eval_count += 1\n\n                # Update personal bests\n                if current_fitness < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_fitness\n\n                # Update global best\n                if current_fitness < global_best_value:\n                    global_best_position = positions[i]\n                    global_best_value = current_fitness\n\n                if self.eval_count >= self.budget:\n                    break\n\n            # Differential Evolution-inspired mutation\n            for i in range(num_agents):\n                if np.random.rand() < CR:\n                    indices = np.random.choice(num_agents, 3, replace=False)\n                    x1, x2, x3 = positions[indices]\n                    mutant_vector = x1 + F * (x2 - x3)\n                    trial_vector = np.where(np.random.rand(self.dim) < CR, mutant_vector, positions[i])\n                    trial_vector = np.clip(trial_vector, lb, ub)\n\n                    trial_fitness = func(trial_vector)\n                    self.eval_count += 1\n\n                    # Update personal bests and global best for trial vector\n                    if trial_fitness < personal_best_values[i]:\n                        personal_best_positions[i] = trial_vector\n                        personal_best_values[i] = trial_fitness\n\n                    if trial_fitness < global_best_value:\n                        global_best_position = trial_vector\n                        global_best_value = trial_fitness\n\n                    if self.eval_count >= self.budget:\n                        break\n\n            # Enhanced Local Search\n            for i in range(num_agents):\n                perturbation = np.random.normal(0, np.random.uniform(0.05, 0.15), self.dim)\n                candidate_position = positions[i] + perturbation\n                candidate_position = np.clip(candidate_position, lb, ub)\n                candidate_fitness = func(candidate_position)\n                self.eval_count += 1\n                if candidate_fitness < personal_best_values[i]:\n                    personal_best_positions[i] = candidate_position\n                    personal_best_values[i] = candidate_fitness\n                if candidate_fitness < global_best_value:\n                    global_best_position = candidate_position\n                    global_best_value = candidate_fitness\n\n        return global_best_position", "name": "HybridPSODEOptimizer", "description": "Introducing adaptive cognitive and social coefficients along with an elite strategy for improved exploration and exploitation balance.", "configspace": "", "generation": 76, "fitness": 0.09401418345567043, "feedback": "The algorithm HybridPSODEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.094 with standard deviation 0.003. And the mean value of best solutions found was 0.173 (0. is the best).", "error": "", "parent_id": "50415c49-0ab3-4e54-b712-be064a952c3b", "metadata": {"aucs": [0.0968592690888107, 0.08930867667584041, 0.09587460460236019], "final_y": [0.16485577213423763, 0.18813064474519403, 0.16485577194516643]}, "mutation_prompt": null}
{"id": "f23c0bf4-2b50-484f-a754-2ebb35acbc25", "solution": "import numpy as np\n\nclass HybridPSODEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_agents = 5 + int(0.1 * self.dim)\n        inertia_weight = 0.9  # Adaptive inertia weight\n        cognitive_coefficient = 1.75  # Slight increase for more exploration (changed from 1.7)\n        social_coefficient = 1.35  # Slight decrease for less crowding (changed from 1.4)\n        F = 0.6  # Adjusted Differential evolution scaling factor for diversity\n        CR = 0.85  # Reduced crossover probability\n\n        # Initialize position and velocity for each agent\n        lb, ub = func.bounds.lb, func.bounds.ub\n        positions = np.random.uniform(lb, ub, (num_agents, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_agents, self.dim))\n\n        # Evaluate initial positions\n        fitness = np.apply_along_axis(func, 1, positions)\n        self.eval_count += num_agents\n\n        best_agent_idx = np.argmin(fitness)\n        global_best_position = positions[best_agent_idx].copy()\n        global_best_value = fitness[best_agent_idx]\n\n        personal_best_positions = positions.copy()\n        personal_best_values = fitness.copy()\n\n        while self.eval_count < self.budget:\n            for i in range(num_agents):\n                # Update velocity using PSO with adaptive inertia\n                r1, r2 = np.random.rand(2)\n                cognitive_component = cognitive_coefficient * r1 * (personal_best_positions[i] - positions[i])\n                social_component = social_coefficient * r2 * (global_best_position - positions[i])\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n                inertia_weight = max(0.4, inertia_weight * 0.98)  # Dynamic decay with a floor\n\n                # Update position\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], lb, ub)  # ensure within bounds\n\n                # Evaluate current position\n                current_fitness = func(positions[i])\n                self.eval_count += 1\n\n                # Update personal bests\n                if current_fitness < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_fitness\n\n                # Update global best\n                if current_fitness < global_best_value:\n                    global_best_position = positions[i]\n                    global_best_value = current_fitness\n\n                if self.eval_count >= self.budget:\n                    break\n\n            # Differential Evolution-inspired mutation\n            for i in range(num_agents):\n                if np.random.rand() < CR:\n                    indices = np.random.choice(num_agents, 3, replace=False)\n                    x1, x2, x3 = positions[indices]\n                    mutant_vector = x1 + F * (x2 - x3)\n                    trial_vector = np.where(np.random.rand(self.dim) < CR, mutant_vector, positions[i])\n                    trial_vector = np.clip(trial_vector, lb, ub)\n\n                    trial_fitness = func(trial_vector)\n                    self.eval_count += 1\n\n                    # Update personal bests and global best for trial vector\n                    if trial_fitness < personal_best_values[i]:\n                        personal_best_positions[i] = trial_vector\n                        personal_best_values[i] = trial_fitness\n\n                    if trial_fitness < global_best_value:\n                        global_best_position = trial_vector\n                        global_best_value = trial_fitness\n\n                    if self.eval_count >= self.budget:\n                        break\n\n            # Enhanced Local Search\n            for i in range(num_agents):\n                perturbation = np.random.normal(0, np.random.uniform(0.05, 0.15), self.dim)\n                candidate_position = positions[i] + perturbation\n                candidate_position = np.clip(candidate_position, lb, ub)\n                candidate_fitness = func(candidate_position)\n                self.eval_count += 1\n                if candidate_fitness < personal_best_values[i]:\n                    personal_best_positions[i] = candidate_position\n                    personal_best_values[i] = candidate_fitness\n                if candidate_fitness < global_best_value:\n                    global_best_position = candidate_position\n                    global_best_value = candidate_fitness\n\n        return global_best_position", "name": "HybridPSODEOptimizer", "description": "Enhanced global exploration and exploitation balance by fine-tuning cognitive and social coefficients.", "configspace": "", "generation": 77, "fitness": 0.09691339305296971, "feedback": "The algorithm HybridPSODEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.097 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best).", "error": "", "parent_id": "50415c49-0ab3-4e54-b712-be064a952c3b", "metadata": {"aucs": [0.09735319374677365, 0.09657085952525923, 0.09681612588687627], "final_y": [0.16485577290857834, 0.16485577202804857, 0.16485577901778925]}, "mutation_prompt": null}
{"id": "2dd14abc-7b30-4ebd-998f-9a36b7ceac4f", "solution": "import numpy as np\n\nclass HybridPSODEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_agents = 5 + int(0.1 * self.dim)\n        inertia_weight = 0.9  # Adaptive inertia weight\n        cognitive_coefficient = 1.75  # Slight increase for more exploration (changed from 1.7)\n        social_coefficient = 1.35  # Slight decrease for less crowding (changed from 1.4)\n        F = 0.6  # Adjusted Differential evolution scaling factor for diversity\n        CR = 0.88  # Increased crossover probability\n\n        # Initialize position and velocity for each agent\n        lb, ub = func.bounds.lb, func.bounds.ub\n        positions = np.random.uniform(lb, ub, (num_agents, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_agents, self.dim))\n\n        # Evaluate initial positions\n        fitness = np.apply_along_axis(func, 1, positions)\n        self.eval_count += num_agents\n\n        best_agent_idx = np.argmin(fitness)\n        global_best_position = positions[best_agent_idx].copy()\n        global_best_value = fitness[best_agent_idx]\n\n        personal_best_positions = positions.copy()\n        personal_best_values = fitness.copy()\n\n        while self.eval_count < self.budget:\n            for i in range(num_agents):\n                # Update velocity using PSO with adaptive inertia\n                r1, r2 = np.random.rand(2)\n                cognitive_component = cognitive_coefficient * r1 * (personal_best_positions[i] - positions[i])\n                social_component = social_coefficient * r2 * (global_best_position - positions[i])\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n                inertia_weight = max(0.4, inertia_weight * 0.98)  # Dynamic decay with a floor\n\n                # Update position\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], lb, ub)  # ensure within bounds\n\n                # Evaluate current position\n                current_fitness = func(positions[i])\n                self.eval_count += 1\n\n                # Update personal bests\n                if current_fitness < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_fitness\n\n                # Update global best\n                if current_fitness < global_best_value:\n                    global_best_position = positions[i]\n                    global_best_value = current_fitness\n\n                if self.eval_count >= self.budget:\n                    break\n\n            # Differential Evolution-inspired mutation\n            for i in range(num_agents):\n                if np.random.rand() < CR:\n                    indices = np.random.choice(num_agents, 3, replace=False)\n                    x1, x2, x3 = positions[indices]\n                    mutant_vector = x1 + F * (x2 - x3)\n                    trial_vector = np.where(np.random.rand(self.dim) < CR, mutant_vector, positions[i])\n                    trial_vector = np.clip(trial_vector, lb, ub)\n\n                    trial_fitness = func(trial_vector)\n                    self.eval_count += 1\n\n                    # Update personal bests and global best for trial vector\n                    if trial_fitness < personal_best_values[i]:\n                        personal_best_positions[i] = trial_vector\n                        personal_best_values[i] = trial_fitness\n\n                    if trial_fitness < global_best_value:\n                        global_best_position = trial_vector\n                        global_best_value = trial_fitness\n\n                    if self.eval_count >= self.budget:\n                        break\n\n            # Enhanced Local Search\n            for i in range(num_agents):\n                perturbation = np.random.normal(0, np.random.uniform(0.05, 0.15), self.dim)\n                candidate_position = positions[i] + perturbation\n                candidate_position = np.clip(candidate_position, lb, ub)\n                candidate_fitness = func(candidate_position)\n                self.eval_count += 1\n                if candidate_fitness < personal_best_values[i]:\n                    personal_best_positions[i] = candidate_position\n                    personal_best_values[i] = candidate_fitness\n                if candidate_fitness < global_best_value:\n                    global_best_position = candidate_position\n                    global_best_value = candidate_fitness\n\n        return global_best_position", "name": "HybridPSODEOptimizer", "description": "Slightly increase the crossover probability to enhance exploration during mutation.", "configspace": "", "generation": 78, "fitness": 0.09671391966383973, "feedback": "The algorithm HybridPSODEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.097 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best).", "error": "", "parent_id": "f23c0bf4-2b50-484f-a754-2ebb35acbc25", "metadata": {"aucs": [0.09706554151252089, 0.09629531489427046, 0.09678090258472782], "final_y": [0.1648557720469741, 0.16485577257042383, 0.16485577194633538]}, "mutation_prompt": null}
{"id": "7d836c68-2dcf-453b-85e7-ed43a15d2c62", "solution": "import numpy as np\n\nclass HybridPSODEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_agents = 5 + int(0.1 * self.dim)\n        inertia_weight = 0.9\n        cognitive_coefficient = 1.75\n        social_coefficient = 1.35\n        F = 0.6\n        CR = 0.85\n\n        # Initialize position and velocity for each agent\n        lb, ub = func.bounds.lb, func.bounds.ub\n        positions = np.random.uniform(lb, ub, (num_agents, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_agents, self.dim))\n\n        # Evaluate initial positions\n        fitness = np.apply_along_axis(func, 1, positions)\n        self.eval_count += num_agents\n\n        best_agent_idx = np.argmin(fitness)\n        global_best_position = positions[best_agent_idx].copy()\n        global_best_value = fitness[best_agent_idx]\n\n        personal_best_positions = positions.copy()\n        personal_best_values = fitness.copy()\n\n        while self.eval_count < self.budget:\n            for i in range(num_agents):\n                r1, r2 = np.random.rand(2)\n                cognitive_component = cognitive_coefficient * r1 * (personal_best_positions[i] - positions[i])\n                social_component = social_coefficient * r2 * (global_best_position - positions[i])\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n                inertia_weight = max(0.4, inertia_weight * 0.98)\n\n                # Update position and apply Lévy flight\n                positions[i] += velocities[i]\n                if np.random.rand() < 0.5:  # Apply Lévy flight with a probability\n                    levy_step = np.random.standard_cauchy(self.dim) * 0.01\n                    positions[i] += levy_step\n                positions[i] = np.clip(positions[i], lb, ub)\n                \n                # Evaluate current position\n                current_fitness = func(positions[i])\n                self.eval_count += 1\n\n                # Update personal bests\n                if current_fitness < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_fitness\n\n                # Update global best\n                if current_fitness < global_best_value:\n                    global_best_position = positions[i]\n                    global_best_value = current_fitness\n\n                if self.eval_count >= self.budget:\n                    break\n\n            for i in range(num_agents):\n                if np.random.rand() < CR:\n                    indices = np.random.choice(num_agents, 3, replace=False)\n                    x1, x2, x3 = positions[indices]\n                    mutant_vector = x1 + F * (x2 - x3)\n                    trial_vector = np.where(np.random.rand(self.dim) < CR, mutant_vector, positions[i])\n                    trial_vector = np.clip(trial_vector, lb, ub)\n\n                    trial_fitness = func(trial_vector)\n                    self.eval_count += 1\n\n                    if trial_fitness < personal_best_values[i]:\n                        personal_best_positions[i] = trial_vector\n                        personal_best_values[i] = trial_fitness\n\n                    if trial_fitness < global_best_value:\n                        global_best_position = trial_vector\n                        global_best_value = trial_fitness\n\n                    if self.eval_count >= self.budget:\n                        break\n\n            for i in range(num_agents):\n                perturbation = np.random.normal(0, np.random.uniform(0.05, 0.15), self.dim)\n                candidate_position = positions[i] + perturbation\n                candidate_position = np.clip(candidate_position, lb, ub)\n                candidate_fitness = func(candidate_position)\n                self.eval_count += 1\n                if candidate_fitness < personal_best_values[i]:\n                    personal_best_positions[i] = candidate_position\n                    personal_best_values[i] = candidate_fitness\n                if candidate_fitness < global_best_value:\n                    global_best_position = candidate_position\n                    global_best_value = candidate_fitness\n\n        return global_best_position", "name": "HybridPSODEOptimizer", "description": "Integration of Lévy flight for enhanced exploration and convergence.", "configspace": "", "generation": 79, "fitness": 0.0888568823284797, "feedback": "The algorithm HybridPSODEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.089 with standard deviation 0.007. And the mean value of best solutions found was 0.192 (0. is the best).", "error": "", "parent_id": "f23c0bf4-2b50-484f-a754-2ebb35acbc25", "metadata": {"aucs": [0.09019723820652581, 0.09714678301226976, 0.07922662576664352], "final_y": [0.1818781068975509, 0.16485580185209958, 0.22804546809761195]}, "mutation_prompt": null}
{"id": "4ff95bd2-a0f0-4013-b997-0cfb17f852a5", "solution": "import numpy as np\n\nclass HybridPSODEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_agents = 5 + int(0.1 * self.dim)\n        inertia_weight = 0.9  # Adaptive inertia weight\n        cognitive_coefficient = 1.75  # Slight increase for more exploration (changed from 1.7)\n        social_coefficient = 1.35  # Slight decrease for less crowding (changed from 1.4)\n        F = 0.7  # Adjusted Differential evolution scaling factor for diversity (changed from 0.6)\n        CR = 0.9  # Increased crossover probability (changed from 0.85)\n\n        # Initialize position and velocity for each agent\n        lb, ub = func.bounds.lb, func.bounds.ub\n        positions = np.random.uniform(lb, ub, (num_agents, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_agents, self.dim))\n\n        # Evaluate initial positions\n        fitness = np.apply_along_axis(func, 1, positions)\n        self.eval_count += num_agents\n\n        best_agent_idx = np.argmin(fitness)\n        global_best_position = positions[best_agent_idx].copy()\n        global_best_value = fitness[best_agent_idx]\n\n        personal_best_positions = positions.copy()\n        personal_best_values = fitness.copy()\n\n        while self.eval_count < self.budget:\n            for i in range(num_agents):\n                # Update velocity using PSO with adaptive inertia\n                r1, r2 = np.random.rand(2)\n                cognitive_component = cognitive_coefficient * r1 * (personal_best_positions[i] - positions[i])\n                social_component = social_coefficient * r2 * (global_best_position - positions[i])\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n                inertia_weight = max(0.4, inertia_weight * 0.98)  # Dynamic decay with a floor\n\n                # Update position\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], lb, ub)  # ensure within bounds\n\n                # Evaluate current position\n                current_fitness = func(positions[i])\n                self.eval_count += 1\n\n                # Update personal bests\n                if current_fitness < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_fitness\n\n                # Update global best\n                if current_fitness < global_best_value:\n                    global_best_position = positions[i]\n                    global_best_value = current_fitness\n\n                if self.eval_count >= self.budget:\n                    break\n\n            # Differential Evolution-inspired mutation\n            for i in range(num_agents):\n                if np.random.rand() < CR:\n                    indices = np.random.choice(num_agents, 3, replace=False)\n                    x1, x2, x3 = positions[indices]\n                    mutant_vector = x1 + F * (x2 - x3)\n                    trial_vector = np.where(np.random.rand(self.dim) < CR, mutant_vector, positions[i])\n                    trial_vector = np.clip(trial_vector, lb, ub)\n\n                    trial_fitness = func(trial_vector)\n                    self.eval_count += 1\n\n                    # Update personal bests and global best for trial vector\n                    if trial_fitness < personal_best_values[i]:\n                        personal_best_positions[i] = trial_vector\n                        personal_best_values[i] = trial_fitness\n\n                    if trial_fitness < global_best_value:\n                        global_best_position = trial_vector\n                        global_best_value = trial_fitness\n\n                    if self.eval_count >= self.budget:\n                        break\n\n            # Enhanced Local Search\n            for i in range(num_agents):\n                perturbation = np.random.normal(0, np.random.uniform(0.03, 0.10), self.dim)  # Adjusted perturbation scale\n                candidate_position = positions[i] + perturbation\n                candidate_position = np.clip(candidate_position, lb, ub)\n                candidate_fitness = func(candidate_position)\n                self.eval_count += 1\n                if candidate_fitness < personal_best_values[i]:\n                    personal_best_positions[i] = candidate_position\n                    personal_best_values[i] = candidate_fitness\n                if candidate_fitness < global_best_value:\n                    global_best_position = candidate_position\n                    global_best_value = candidate_fitness\n\n        return global_best_position", "name": "HybridPSODEOptimizer", "description": "Further balance exploration and exploitation by tweaking DE parameters and local search.", "configspace": "", "generation": 80, "fitness": 0.09423745966146786, "feedback": "The algorithm HybridPSODEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.094 with standard deviation 0.003. And the mean value of best solutions found was 0.171 (0. is the best).", "error": "", "parent_id": "f23c0bf4-2b50-484f-a754-2ebb35acbc25", "metadata": {"aucs": [0.09662897524950131, 0.09622375413824791, 0.08985964959665438], "final_y": [0.16485578143326418, 0.16485577284498254, 0.18187814049384743]}, "mutation_prompt": null}
{"id": "b85be772-3759-4f59-98ca-6f2e0a339e49", "solution": "import numpy as np\n\nclass HybridPSODEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_agents = 5 + int(0.1 * self.dim)\n        inertia_weight = 0.9\n        cognitive_coefficient = 1.75\n        social_coefficient = 1.35\n        F = 0.65  # Slight increase for more diversity\n        CR = 0.8  # Slight decrease to explore more\n\n        # Use a chaotic map for initialization\n        lb, ub = func.bounds.lb, func.bounds.ub\n        positions = lb + (ub - lb) * np.cos(np.linspace(0, np.pi, num_agents * self.dim)).reshape(num_agents, self.dim)\n        velocities = np.random.uniform(-0.5, 0.5, (num_agents, self.dim))  # Reduced velocity bounds\n\n        # Evaluate initial positions\n        fitness = np.apply_along_axis(func, 1, positions)\n        self.eval_count += num_agents\n\n        best_agent_idx = np.argmin(fitness)\n        global_best_position = positions[best_agent_idx].copy()\n        global_best_value = fitness[best_agent_idx]\n\n        personal_best_positions = positions.copy()\n        personal_best_values = fitness.copy()\n\n        while self.eval_count < self.budget:\n            adaptive_cognitive = cognitive_coefficient * (1 - self.eval_count / self.budget)  # Adaptive learning rate\n            adaptive_social = social_coefficient * (self.eval_count / self.budget)\n\n            for i in range(num_agents):\n                r1, r2 = np.random.rand(2)\n                cognitive_component = adaptive_cognitive * r1 * (personal_best_positions[i] - positions[i])\n                social_component = adaptive_social * r2 * (global_best_position - positions[i])\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n                inertia_weight = max(0.4, inertia_weight * 0.98)\n\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], lb, ub)\n\n                current_fitness = func(positions[i])\n                self.eval_count += 1\n\n                if current_fitness < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_fitness\n\n                if current_fitness < global_best_value:\n                    global_best_position = positions[i]\n                    global_best_value = current_fitness\n\n                if self.eval_count >= self.budget:\n                    break\n\n            for i in range(num_agents):\n                if np.random.rand() < CR:\n                    indices = np.random.choice(num_agents, 3, replace=False)\n                    x1, x2, x3 = positions[indices]\n                    mutant_vector = x1 + F * (x2 - x3)\n                    trial_vector = np.where(np.random.rand(self.dim) < CR, mutant_vector, positions[i])\n                    trial_vector = np.clip(trial_vector, lb, ub)\n\n                    trial_fitness = func(trial_vector)\n                    self.eval_count += 1\n\n                    if trial_fitness < personal_best_values[i]:\n                        personal_best_positions[i] = trial_vector\n                        personal_best_values[i] = trial_fitness\n\n                    if trial_fitness < global_best_value:\n                        global_best_position = trial_vector\n                        global_best_value = trial_fitness\n\n                    if self.eval_count >= self.budget:\n                        break\n\n            for i in range(num_agents):\n                perturbation = np.random.normal(0, np.random.uniform(0.05, 0.15), self.dim)\n                candidate_position = positions[i] + perturbation\n                candidate_position = np.clip(candidate_position, lb, ub)\n                candidate_fitness = func(candidate_position)\n                self.eval_count += 1\n                if candidate_fitness < personal_best_values[i]:\n                    personal_best_positions[i] = candidate_position\n                    personal_best_values[i] = candidate_fitness\n                if candidate_fitness < global_best_value:\n                    global_best_position = candidate_position\n                    global_best_value = candidate_fitness\n\n        return global_best_position", "name": "HybridPSODEOptimizer", "description": "Introduced adaptive learning rates and chaotic map initialization for enhanced convergence.", "configspace": "", "generation": 81, "fitness": 0.09431694982305401, "feedback": "The algorithm HybridPSODEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.094 with standard deviation 0.001. And the mean value of best solutions found was 0.165 (0. is the best).", "error": "", "parent_id": "f23c0bf4-2b50-484f-a754-2ebb35acbc25", "metadata": {"aucs": [0.0962310041608132, 0.09411611851765933, 0.09260372679068951], "final_y": [0.16485594333484133, 0.16485585985266826, 0.16485585483865628]}, "mutation_prompt": null}
{"id": "27abbda6-9244-4cce-8367-b75c96ef2227", "solution": "import numpy as np\n\nclass HybridPSODEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_agents = 5 + int(0.1 * self.dim)\n        inertia_weight = 0.9  # Adaptive inertia weight\n        cognitive_coefficient = 1.75  # Slight increase for more exploration (changed from 1.7)\n        social_coefficient = 1.35  # Slight decrease for less crowding (changed from 1.4)\n        F = 0.6  # Adjusted Differential evolution scaling factor for diversity\n        CR = 0.85  # Reduced crossover probability\n\n        # Initialize position and velocity for each agent\n        lb, ub = func.bounds.lb, func.bounds.ub\n        positions = np.random.uniform(lb, ub, (num_agents, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_agents, self.dim))\n\n        # Evaluate initial positions\n        fitness = np.apply_along_axis(func, 1, positions)\n        self.eval_count += num_agents\n\n        best_agent_idx = np.argmin(fitness)\n        global_best_position = positions[best_agent_idx].copy()\n        global_best_value = fitness[best_agent_idx]\n\n        personal_best_positions = positions.copy()\n        personal_best_values = fitness.copy()\n\n        while self.eval_count < self.budget:\n            for i in range(num_agents):\n                # Update velocity using PSO with adaptive inertia\n                r1, r2 = np.random.rand(2)\n                cognitive_component = cognitive_coefficient * r1 * (personal_best_positions[i] - positions[i])\n                social_component = social_coefficient * r2 * (global_best_position - positions[i])\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n                velocities[i] *= np.random.uniform(0.9, 1.1)  # Stochastic velocity scaling\n\n                inertia_weight = max(0.4, inertia_weight * 0.98)  # Dynamic decay with a floor\n\n                # Update position\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], lb, ub)  # ensure within bounds\n\n                # Evaluate current position\n                current_fitness = func(positions[i])\n                self.eval_count += 1\n\n                # Update personal bests\n                if current_fitness < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_fitness\n\n                # Update global best\n                if current_fitness < global_best_value:\n                    global_best_position = positions[i]\n                    global_best_value = current_fitness\n\n                if self.eval_count >= self.budget:\n                    break\n\n            # Differential Evolution-inspired mutation\n            F = 0.4 + 0.3 * np.random.rand()  # Self-adaptive mutation factor\n            for i in range(num_agents):\n                if np.random.rand() < CR:\n                    indices = np.random.choice(num_agents, 3, replace=False)\n                    x1, x2, x3 = positions[indices]\n                    mutant_vector = x1 + F * (x2 - x3)\n                    trial_vector = np.where(np.random.rand(self.dim) < CR, mutant_vector, positions[i])\n                    trial_vector = np.clip(trial_vector, lb, ub)\n\n                    trial_fitness = func(trial_vector)\n                    self.eval_count += 1\n\n                    # Update personal bests and global best for trial vector\n                    if trial_fitness < personal_best_values[i]:\n                        personal_best_positions[i] = trial_vector\n                        personal_best_values[i] = trial_fitness\n\n                    if trial_fitness < global_best_value:\n                        global_best_position = trial_vector\n                        global_best_value = trial_fitness\n\n                    if self.eval_count >= self.budget:\n                        break\n\n            # Enhanced Local Search\n            for i in range(num_agents):\n                perturbation = np.random.normal(0, np.random.uniform(0.05, 0.15), self.dim)\n                candidate_position = positions[i] + perturbation\n                candidate_position = np.clip(candidate_position, lb, ub)\n                candidate_fitness = func(candidate_position)\n                self.eval_count += 1\n                if candidate_fitness < personal_best_values[i]:\n                    personal_best_positions[i] = candidate_position\n                    personal_best_values[i] = candidate_fitness\n                if candidate_fitness < global_best_value:\n                    global_best_position = candidate_position\n                    global_best_value = candidate_fitness\n\n        return global_best_position", "name": "HybridPSODEOptimizer", "description": "Introduced stochastic velocity scaling and self-adaptive mutation for enhanced exploration and convergence.", "configspace": "", "generation": 82, "fitness": 0.09452210687667635, "feedback": "The algorithm HybridPSODEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.095 with standard deviation 0.002. And the mean value of best solutions found was 0.171 (0. is the best).", "error": "", "parent_id": "f23c0bf4-2b50-484f-a754-2ebb35acbc25", "metadata": {"aucs": [0.09645915709124164, 0.09505108905053872, 0.09205607448824871], "final_y": [0.1648557740440001, 0.16485577190777423, 0.18187809712514746]}, "mutation_prompt": null}
{"id": "47bd93e1-02e8-465f-98db-3cb99493c434", "solution": "import numpy as np\n\nclass HybridPSODEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_agents = 5 + int(0.1 * self.dim)\n        inertia_weight = 0.9  # Adaptive inertia weight\n        cognitive_coefficient = 1.85  # Adjusted for slightly better exploration\n        social_coefficient = 1.30  # Reduced further for less crowding\n        F = 0.55  # Adjusted Differential evolution scaling factor for diversity\n        CR = 0.80  # Further reduced crossover probability\n\n        # Initialize position and velocity for each agent\n        lb, ub = func.bounds.lb, func.bounds.ub\n        positions = np.random.uniform(lb, ub, (num_agents, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_agents, self.dim))\n\n        # Evaluate initial positions\n        fitness = np.apply_along_axis(func, 1, positions)\n        self.eval_count += num_agents\n\n        best_agent_idx = np.argmin(fitness)\n        global_best_position = positions[best_agent_idx].copy()\n        global_best_value = fitness[best_agent_idx]\n\n        personal_best_positions = positions.copy()\n        personal_best_values = fitness.copy()\n\n        while self.eval_count < self.budget:\n            for i in range(num_agents):\n                # Update velocity using PSO with adaptive inertia\n                r1, r2 = np.random.rand(2)\n                cognitive_component = cognitive_coefficient * r1 * (personal_best_positions[i] - positions[i])\n                social_component = social_coefficient * r2 * (global_best_position - positions[i])\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n                inertia_weight = max(0.4, inertia_weight * 0.98)  # Dynamic decay with a floor\n\n                # Update position\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], lb, ub)  # ensure within bounds\n\n                # Evaluate current position\n                current_fitness = func(positions[i])\n                self.eval_count += 1\n\n                # Update personal bests\n                if current_fitness < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_fitness\n\n                # Update global best\n                if current_fitness < global_best_value:\n                    global_best_position = positions[i]\n                    global_best_value = current_fitness\n\n                if self.eval_count >= self.budget:\n                    break\n\n            # Differential Evolution-inspired mutation\n            for i in range(num_agents):\n                if np.random.rand() < CR:\n                    indices = np.random.choice(num_agents, 3, replace=False)\n                    x1, x2, x3 = positions[indices]\n                    mutant_vector = x1 + F * (x2 - x3)\n                    trial_vector = np.where(np.random.rand(self.dim) < CR, mutant_vector, positions[i])\n                    trial_vector = np.clip(trial_vector, lb, ub)\n\n                    trial_fitness = func(trial_vector)\n                    self.eval_count += 1\n\n                    # Update personal bests and global best for trial vector\n                    if trial_fitness < personal_best_values[i]:\n                        personal_best_positions[i] = trial_vector\n                        personal_best_values[i] = trial_fitness\n\n                    if trial_fitness < global_best_value:\n                        global_best_position = trial_vector\n                        global_best_value = trial_fitness\n\n                    if self.eval_count >= self.budget:\n                        break\n\n            # Enhanced Local Search\n            for i in range(num_agents):\n                perturbation = np.random.normal(0, np.random.uniform(0.05, 0.15), self.dim)\n                candidate_position = positions[i] + perturbation\n                candidate_position = np.clip(candidate_position, lb, ub)\n                candidate_fitness = func(candidate_position)\n                self.eval_count += 1\n                if candidate_fitness < personal_best_values[i]:\n                    personal_best_positions[i] = candidate_position\n                    personal_best_values[i] = candidate_fitness\n                if candidate_fitness < global_best_value:\n                    global_best_position = candidate_position\n                    global_best_value = candidate_fitness\n\n        return global_best_position", "name": "HybridPSODEOptimizer", "description": "Optimize exploration-exploitation balance by adjusting velocity updates and mutation parameters.", "configspace": "", "generation": 83, "fitness": 0.09649528024378724, "feedback": "The algorithm HybridPSODEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.096 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best).", "error": "", "parent_id": "f23c0bf4-2b50-484f-a754-2ebb35acbc25", "metadata": {"aucs": [0.09685949904924185, 0.0961047755202713, 0.0965215661618486], "final_y": [0.16485577196391255, 0.16485577234574433, 0.16485577488225234]}, "mutation_prompt": null}
{"id": "31170d95-31ee-4327-b35e-0356e527d992", "solution": "import numpy as np\n\nclass HybridPSODEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_agents = 5 + int(0.1 * self.dim)\n        inertia_weight = 0.9  # Adaptive inertia weight\n        cognitive_coefficient = 1.75  # Slight increase for more exploration\n        social_coefficient = 1.35  # Slight decrease for less crowding\n        F = 0.6  # Adjusted Differential evolution scaling factor for diversity\n        CR = 0.85  # Reduced crossover probability\n\n        # Initialize position and velocity for each agent\n        lb, ub = func.bounds.lb, func.bounds.ub\n        positions = np.random.uniform(lb, ub, (num_agents, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_agents, self.dim))\n\n        # Evaluate initial positions\n        fitness = np.apply_along_axis(func, 1, positions)\n        self.eval_count += num_agents\n\n        best_agent_idx = np.argmin(fitness)\n        global_best_position = positions[best_agent_idx].copy()\n        global_best_value = fitness[best_agent_idx]\n\n        personal_best_positions = positions.copy()\n        personal_best_values = fitness.copy()\n\n        while self.eval_count < self.budget:\n            for i in range(num_agents):\n                # Update velocity using PSO with adaptive inertia\n                r1, r2 = np.random.rand(2)\n                cognitive_component = cognitive_coefficient * r1 * (personal_best_positions[i] - positions[i])\n                social_component = social_coefficient * r2 * (global_best_position - positions[i])\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n                inertia_weight = max(0.4, inertia_weight * 0.98)  # Dynamic decay with a floor\n\n                # Update position\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], lb, ub)  # ensure within bounds\n\n                # Evaluate current position\n                current_fitness = func(positions[i])\n                self.eval_count += 1\n\n                # Update personal bests\n                if current_fitness < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_fitness\n\n                # Update global best\n                if current_fitness < global_best_value:\n                    global_best_position = positions[i]\n                    global_best_value = current_fitness\n\n                if self.eval_count >= self.budget:\n                    break\n\n            # Differential Evolution-inspired mutation\n            for i in range(num_agents):\n                if np.random.rand() < CR:\n                    indices = np.random.choice(num_agents, 3, replace=False)\n                    x1, x2, x3 = positions[indices]\n                    mutant_vector = x1 + F * (x2 - x3)\n                    trial_vector = np.where(np.random.rand(self.dim) < CR, mutant_vector, positions[i])\n                    trial_vector = np.clip(trial_vector, lb, ub)\n\n                    trial_fitness = func(trial_vector)\n                    self.eval_count += 1\n\n                    # Update personal bests and global best for trial vector\n                    if trial_fitness < personal_best_values[i]:\n                        personal_best_positions[i] = trial_vector\n                        personal_best_values[i] = trial_fitness\n\n                    if trial_fitness < global_best_value:\n                        global_best_position = trial_vector\n                        global_best_value = trial_fitness\n\n                    if self.eval_count >= self.budget:\n                        break\n\n            # Enhanced Local Search\n            for i in range(num_agents):\n                variance = np.random.uniform(0.01, 0.1)  # Adaptive local search variance\n                perturbation = np.random.normal(0, variance, self.dim)\n                candidate_position = positions[i] + perturbation\n                candidate_position = np.clip(candidate_position, lb, ub)\n                candidate_fitness = func(candidate_position)\n                self.eval_count += 1\n                if candidate_fitness < personal_best_values[i]:\n                    personal_best_positions[i] = candidate_position\n                    personal_best_values[i] = candidate_fitness\n                if candidate_fitness < global_best_value:\n                    global_best_position = candidate_position\n                    global_best_value = candidate_fitness\n\n        return global_best_position", "name": "HybridPSODEOptimizer", "description": "Introduce adaptive local search variance and dynamic social coefficient adjustment to enhance convergence. ", "configspace": "", "generation": 84, "fitness": 0.09675090027243168, "feedback": "The algorithm HybridPSODEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.097 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best).", "error": "", "parent_id": "f23c0bf4-2b50-484f-a754-2ebb35acbc25", "metadata": {"aucs": [0.09731297767954017, 0.09676109555429246, 0.0961786275834624], "final_y": [0.1648557719234146, 0.16485577194817025, 0.16485577380635508]}, "mutation_prompt": null}
{"id": "2563060f-598d-458e-81c7-32d6cff5bd45", "solution": "import numpy as np\n\nclass HybridPSODEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_agents = 5 + int(0.1 * self.dim)\n        inertia_weight = 0.9  # Adaptive inertia weight\n        cognitive_coefficient = 1.75  # Slight increase for more exploration (changed from 1.7)\n        social_coefficient = 1.35  # Slight decrease for less crowding (changed from 1.4)\n        F = 0.6  # Adjusted Differential evolution scaling factor for diversity\n        CR = 0.85  # Reduced crossover probability\n\n        # Initialize position and velocity\n        lb, ub = func.bounds.lb, func.bounds.ub\n        positions = np.random.uniform(lb, ub, (num_agents, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_agents, self.dim))\n\n        # Evaluate initial positions\n        fitness = np.apply_along_axis(func, 1, positions)\n        self.eval_count += num_agents\n\n        best_agent_idx = np.argmin(fitness)\n        global_best_position = positions[best_agent_idx].copy()\n        global_best_value = fitness[best_agent_idx]\n\n        personal_best_positions = positions.copy()\n        personal_best_values = fitness.copy()\n\n        while self.eval_count < self.budget:\n            for i in range(num_agents):\n                # Update velocity using PSO with adaptive inertia\n                r1, r2 = np.random.rand(2)\n                cognitive_component = cognitive_coefficient * r1 * (personal_best_positions[i] - positions[i])\n                social_component = social_coefficient * r2 * (global_best_position - positions[i])\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n                inertia_weight = max(0.4, inertia_weight * 0.98)  # Dynamic decay with a floor\n\n                # Update position\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], lb, ub)  # ensure within bounds\n\n                # Evaluate current position\n                current_fitness = func(positions[i])\n                self.eval_count += 1\n\n                # Update personal bests\n                if current_fitness < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_fitness\n\n                # Update global best\n                if current_fitness < global_best_value:\n                    global_best_position = positions[i]\n                    global_best_value = current_fitness\n\n                if self.eval_count >= self.budget:\n                    break\n\n            # Differential Evolution-inspired mutation\n            for i in range(num_agents):\n                if np.random.rand() < CR:\n                    indices = np.random.choice(num_agents, 3, replace=False)\n                    x1, x2, x3 = positions[indices]\n                    F = 0.5 + 0.5 * np.random.rand()  # Time-varying mutation strategy update\n                    mutant_vector = x1 + F * (x2 - x3)  # Updated mutation strategy\n                    trial_vector = np.where(np.random.rand(self.dim) < CR, mutant_vector, positions[i])\n                    trial_vector = np.clip(trial_vector, lb, ub)\n\n                    trial_fitness = func(trial_vector)\n                    self.eval_count += 1\n\n                    # Update personal bests and global best for trial vector\n                    if trial_fitness < personal_best_values[i]:\n                        personal_best_positions[i] = trial_vector\n                        personal_best_values[i] = trial_fitness\n\n                    if trial_fitness < global_best_value:\n                        global_best_position = trial_vector\n                        global_best_value = trial_fitness\n\n                    if self.eval_count >= self.budget:\n                        break\n\n            # Enhanced Local Search\n            for i in range(num_agents):\n                perturbation = np.random.normal(0, np.random.uniform(0.05, 0.15), self.dim)\n                candidate_position = positions[i] + perturbation\n                candidate_position = np.clip(candidate_position, lb, ub)\n                candidate_fitness = func(candidate_position)\n                self.eval_count += 1\n                if candidate_fitness < personal_best_values[i]:\n                    personal_best_positions[i] = candidate_position\n                    personal_best_values[i] = candidate_fitness\n                if candidate_fitness < global_best_value:\n                    global_best_position = candidate_position\n                    global_best_value = candidate_fitness\n\n        return global_best_position", "name": "HybridPSODEOptimizer", "description": "Incorporate adaptive multi-swarm strategies and a time-varying mutation strategy to enhance convergence.", "configspace": "", "generation": 85, "fitness": 0.09447820665131901, "feedback": "The algorithm HybridPSODEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.094 with standard deviation 0.003. And the mean value of best solutions found was 0.171 (0. is the best).", "error": "", "parent_id": "f23c0bf4-2b50-484f-a754-2ebb35acbc25", "metadata": {"aucs": [0.09696611882960715, 0.09626663033568861, 0.09020187078866126], "final_y": [0.16485577323354728, 0.16485577190692324, 0.18187811087561012]}, "mutation_prompt": null}
{"id": "82928744-095c-4c3b-90c5-161f00e52519", "solution": "import numpy as np\n\nclass HybridPSODEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_agents = 5 + int(0.1 * self.dim)\n        inertia_weight = 0.9\n        cognitive_coefficient = 1.75\n        social_coefficient = 1.35\n        F = 0.6  # Adjusted Differential evolution scaling factor for diversity\n        CR = 0.85\n\n        lb, ub = func.bounds.lb, func.bounds.ub\n        positions = np.random.uniform(lb, ub, (num_agents, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_agents, self.dim))\n\n        fitness = np.apply_along_axis(func, 1, positions)\n        self.eval_count += num_agents\n\n        best_agent_idx = np.argmin(fitness)\n        global_best_position = positions[best_agent_idx].copy()\n        global_best_value = fitness[best_agent_idx]\n\n        personal_best_positions = positions.copy()\n        personal_best_values = fitness.copy()\n\n        while self.eval_count < self.budget:\n            for i in range(num_agents):\n                r1, r2 = np.random.rand(2)\n                cognitive_component = cognitive_coefficient * r1 * (personal_best_positions[i] - positions[i])\n                social_component = social_coefficient * r2 * (global_best_position - positions[i])\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n                inertia_weight = max(0.4, inertia_weight * 0.98)\n\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], lb, ub)\n\n                current_fitness = func(positions[i])\n                self.eval_count += 1\n\n                if current_fitness < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_fitness\n\n                if current_fitness < global_best_value:\n                    global_best_position = positions[i]\n                    global_best_value = current_fitness\n\n                if self.eval_count >= self.budget:\n                    break\n\n            # Adaptive Differential Evolution-inspired mutation\n            for i in range(num_agents):\n                if np.random.rand() < CR:\n                    indices = np.random.choice(num_agents, 3, replace=False)\n                    x1, x2, x3 = positions[indices]\n                    adaptive_F = F + np.random.uniform(-0.1, 0.1)  # Adaptive factor\n                    mutant_vector = x1 + adaptive_F * (x2 - x3)\n                    trial_vector = np.where(np.random.rand(self.dim) < CR, mutant_vector, positions[i])\n                    trial_vector = np.clip(trial_vector, lb, ub)\n\n                    trial_fitness = func(trial_vector)\n                    self.eval_count += 1\n\n                    if trial_fitness < personal_best_values[i]:\n                        personal_best_positions[i] = trial_vector\n                        personal_best_values[i] = trial_fitness\n\n                    if trial_fitness < global_best_value:\n                        global_best_position = trial_vector\n                        global_best_value = trial_fitness\n\n                    if self.eval_count >= self.budget:\n                        break\n\n            for i in range(num_agents):\n                perturbation = np.random.normal(0, np.random.uniform(0.05, 0.15), self.dim)\n                candidate_position = positions[i] + perturbation\n                candidate_position = np.clip(candidate_position, lb, ub)\n                candidate_fitness = func(candidate_position)\n                self.eval_count += 1\n                if candidate_fitness < personal_best_values[i]:\n                    personal_best_positions[i] = candidate_position\n                    personal_best_values[i] = candidate_fitness\n                if candidate_fitness < global_best_value:\n                    global_best_position = candidate_position\n                    global_best_value = candidate_fitness\n\n        return global_best_position", "name": "HybridPSODEOptimizer", "description": "Introduced adaptive differential evolution parameters for enhanced exploration and exploitation balance.", "configspace": "", "generation": 86, "fitness": 0.09470413571630613, "feedback": "The algorithm HybridPSODEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.095 with standard deviation 0.003. And the mean value of best solutions found was 0.171 (0. is the best).", "error": "", "parent_id": "f23c0bf4-2b50-484f-a754-2ebb35acbc25", "metadata": {"aucs": [0.0968885678875433, 0.09632204561658386, 0.09090179364479123], "final_y": [0.16485577234080617, 0.16485577198960888, 0.18187809677835554]}, "mutation_prompt": null}
{"id": "f946e3da-4d83-4581-ad1c-7b53618a6aaf", "solution": "import numpy as np\n\nclass HybridPSODEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_agents = 5 + int(0.1 * self.dim)\n        inertia_weight = 0.9  # Adaptive inertia weight\n        cognitive_coefficient = 1.75  # Slight increase for more exploration (changed from 1.7)\n        social_coefficient = 1.35  # Slight decrease for less crowding (changed from 1.4)\n        F = 0.6  # Adjusted Differential evolution scaling factor for diversity\n        CR = 0.85  # Reduced crossover probability\n\n        # Initialize position and velocity for each agent\n        lb, ub = func.bounds.lb, func.bounds.ub\n        positions = np.random.uniform(lb, ub, (num_agents, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_agents, self.dim))\n\n        # Evaluate initial positions\n        fitness = np.apply_along_axis(func, 1, positions)\n        self.eval_count += num_agents\n\n        best_agent_idx = np.argmin(fitness)\n        global_best_position = positions[best_agent_idx].copy()\n        global_best_value = fitness[best_agent_idx]\n\n        personal_best_positions = positions.copy()\n        personal_best_values = fitness.copy()\n\n        while self.eval_count < self.budget:\n            for i in range(num_agents):\n                # Update velocity using PSO with adaptive inertia\n                r1, r2 = np.random.rand(2)\n                cognitive_component = cognitive_coefficient * r1 * (personal_best_positions[i] - positions[i])\n                social_component = social_coefficient * r2 * (global_best_position - positions[i])\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n                inertia_weight = max(0.3, inertia_weight * 0.97)  # More aggressive decay with a floor\n\n                # Update position\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], lb, ub)  # ensure within bounds\n\n                # Evaluate current position\n                current_fitness = func(positions[i])\n                self.eval_count += 1\n\n                # Update personal bests\n                if current_fitness < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_fitness\n\n                # Update global best\n                if current_fitness < global_best_value:\n                    global_best_position = positions[i]\n                    global_best_value = current_fitness\n\n                if self.eval_count >= self.budget:\n                    break\n\n            # Differential Evolution-inspired mutation\n            for i in range(num_agents):\n                if np.random.rand() < CR:\n                    indices = np.random.choice(num_agents, 3, replace=False)\n                    x1, x2, x3 = positions[indices]\n                    mutant_vector = x1 + F * (x2 - x3)\n                    trial_vector = np.where(np.random.rand(self.dim) < CR, mutant_vector, positions[i])\n                    trial_vector = np.clip(trial_vector, lb, ub)\n\n                    trial_fitness = func(trial_vector)\n                    self.eval_count += 1\n\n                    # Update personal bests and global best for trial vector\n                    if trial_fitness < personal_best_values[i]:\n                        personal_best_positions[i] = trial_vector\n                        personal_best_values[i] = trial_fitness\n\n                    if trial_fitness < global_best_value:\n                        global_best_position = trial_vector\n                        global_best_value = trial_fitness\n\n                    if self.eval_count >= self.budget:\n                        break\n\n            # Enhanced Local Search\n            for i in range(num_agents):\n                perturbation = np.random.normal(0, np.random.uniform(0.03, 0.1), self.dim)  # Reduced perturbation range\n                candidate_position = positions[i] + perturbation\n                candidate_position = np.clip(candidate_position, lb, ub)\n                candidate_fitness = func(candidate_position)\n                self.eval_count += 1\n                if candidate_fitness < personal_best_values[i]:\n                    personal_best_positions[i] = candidate_position\n                    personal_best_values[i] = candidate_fitness\n                if candidate_fitness < global_best_value:\n                    global_best_position = candidate_position\n                    global_best_value = candidate_fitness\n\n        return global_best_position", "name": "HybridPSODEOptimizer", "description": "Improve convergence by refining inertia weight strategy and enhancing local search perturbation.", "configspace": "", "generation": 87, "fitness": 0.09685933059218739, "feedback": "The algorithm HybridPSODEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.097 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best).", "error": "", "parent_id": "f23c0bf4-2b50-484f-a754-2ebb35acbc25", "metadata": {"aucs": [0.09714712076945708, 0.09666828820972118, 0.09676258279738392], "final_y": [0.16485577757668135, 0.1648557742504454, 0.16485580681108936]}, "mutation_prompt": null}
{"id": "2900b50b-2e95-4885-88e3-2c3e83085656", "solution": "import numpy as np\n\nclass HybridPSODEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_agents = 5 + int(0.1 * self.dim)\n        inertia_weight = 0.9  # Adaptive inertia weight\n        cognitive_coefficient = 1.75  # Adaptive cognitive component (changed)\n        social_coefficient = 1.35  # Adaptive social component (changed)\n        F = 0.6  # DE scaling factor for diversity\n        CR = 0.85  # Reduced crossover probability\n\n        # Initialize position and velocity for each agent\n        lb, ub = func.bounds.lb, func.bounds.ub\n        positions = np.random.uniform(lb, ub, (num_agents, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_agents, self.dim))\n\n        # Evaluate initial positions\n        fitness = np.apply_along_axis(func, 1, positions)\n        self.eval_count += num_agents\n\n        best_agent_idx = np.argmin(fitness)\n        global_best_position = positions[best_agent_idx].copy()\n        global_best_value = fitness[best_agent_idx]\n\n        personal_best_positions = positions.copy()\n        personal_best_values = fitness.copy()\n\n        while self.eval_count < self.budget:\n            for i in range(num_agents):\n                # Update velocity using PSO with adaptive inertia\n                r1, r2 = np.random.rand(2)\n                cognitive_component = cognitive_coefficient * r1 * (personal_best_positions[i] - positions[i])\n                social_component = social_coefficient * r2 * (global_best_position - positions[i])\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n                inertia_weight = max(0.4, inertia_weight * 0.98)  # Dynamic decay with a floor\n\n                # Adaptive adjustment (changed)\n                if self.eval_count % 10 == 0:  # Periodic adaptation\n                    cognitive_coefficient = 1.5 + 0.5 * np.sin(self.eval_count / 10)  # Adaptive strategy\n                    social_coefficient = 1.5 + 0.5 * np.cos(self.eval_count / 10)  # Adaptive strategy\n\n                # Update position\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], lb, ub)  # ensure within bounds\n\n                # Evaluate current position\n                current_fitness = func(positions[i])\n                self.eval_count += 1\n\n                # Update personal bests\n                if current_fitness < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_fitness\n\n                # Update global best\n                if current_fitness < global_best_value:\n                    global_best_position = positions[i]\n                    global_best_value = current_fitness\n\n                if self.eval_count >= self.budget:\n                    break\n\n            # Differential Evolution-inspired mutation\n            for i in range(num_agents):\n                if np.random.rand() < CR:\n                    indices = np.random.choice(num_agents, 3, replace=False)\n                    x1, x2, x3 = positions[indices]\n                    mutant_vector = x1 + F * (x2 - x3)\n                    trial_vector = np.where(np.random.rand(self.dim) < CR, mutant_vector, positions[i])\n                    trial_vector = np.clip(trial_vector, lb, ub)\n\n                    trial_fitness = func(trial_vector)\n                    self.eval_count += 1\n\n                    # Update personal bests and global best for trial vector\n                    if trial_fitness < personal_best_values[i]:\n                        personal_best_positions[i] = trial_vector\n                        personal_best_values[i] = trial_fitness\n\n                    if trial_fitness < global_best_value:\n                        global_best_position = trial_vector\n                        global_best_value = trial_fitness\n\n                    if self.eval_count >= self.budget:\n                        break\n\n            # Enhanced Local Search\n            for i in range(num_agents):\n                perturbation = np.random.normal(0, np.random.uniform(0.05, 0.15), self.dim)\n                candidate_position = positions[i] + perturbation\n                candidate_position = np.clip(candidate_position, lb, ub)\n                candidate_fitness = func(candidate_position)\n                self.eval_count += 1\n                if candidate_fitness < personal_best_values[i]:\n                    personal_best_positions[i] = candidate_position\n                    personal_best_values[i] = candidate_fitness\n                if candidate_fitness < global_best_value:\n                    global_best_position = candidate_position\n                    global_best_value = candidate_fitness\n\n        return global_best_position", "name": "HybridPSODEOptimizer", "description": "Introduced adaptive cognitive and social coefficients with periodic local search to enhance convergence.", "configspace": "", "generation": 88, "fitness": 0.09438917065749808, "feedback": "The algorithm HybridPSODEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.094 with standard deviation 0.004. And the mean value of best solutions found was 0.165 (0. is the best).", "error": "", "parent_id": "f23c0bf4-2b50-484f-a754-2ebb35acbc25", "metadata": {"aucs": [0.09723585077890307, 0.0889873269994006, 0.09694433419419057], "final_y": [0.16485577223755654, 0.16485577643780425, 0.16485577206862145]}, "mutation_prompt": null}
{"id": "4e954f26-f52b-4b45-9219-8846b701685b", "solution": "import numpy as np\n\nclass HybridPSODEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_agents = 5 + int(0.1 * self.dim)\n        inertia_weight = 0.9  # Adaptive inertia weight\n        cognitive_coefficient = 1.8  # Slight increase for more exploration (changed from 1.75)\n        social_coefficient = 1.35  # Slight decrease for less crowding (changed from 1.4)\n        F = 0.6  # Adjusted Differential evolution scaling factor for diversity\n        CR = 0.85  # Reduced crossover probability\n\n        # Initialize position and velocity for each agent\n        lb, ub = func.bounds.lb, func.bounds.ub\n        positions = np.random.uniform(lb, ub, (num_agents, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_agents, self.dim))\n\n        # Evaluate initial positions\n        fitness = np.apply_along_axis(func, 1, positions)\n        self.eval_count += num_agents\n\n        best_agent_idx = np.argmin(fitness)\n        global_best_position = positions[best_agent_idx].copy()\n        global_best_value = fitness[best_agent_idx]\n\n        personal_best_positions = positions.copy()\n        personal_best_values = fitness.copy()\n\n        while self.eval_count < self.budget:\n            for i in range(num_agents):\n                # Update velocity using PSO with adaptive inertia\n                r1, r2 = np.random.rand(2)\n                cognitive_component = cognitive_coefficient * r1 * (personal_best_positions[i] - positions[i])\n                social_component = social_coefficient * r2 * (global_best_position - positions[i])\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n                inertia_weight = max(0.4, inertia_weight * 0.98)  # Dynamic decay with a floor\n\n                # Update position\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], lb, ub)  # ensure within bounds\n\n                # Evaluate current position\n                current_fitness = func(positions[i])\n                self.eval_count += 1\n\n                # Update personal bests\n                if current_fitness < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_fitness\n\n                # Update global best\n                if current_fitness < global_best_value:\n                    global_best_position = positions[i]\n                    global_best_value = current_fitness\n\n                if self.eval_count >= self.budget:\n                    break\n\n            # Differential Evolution-inspired mutation\n            for i in range(num_agents):\n                if np.random.rand() < CR:\n                    indices = np.random.choice(num_agents, 3, replace=False)\n                    x1, x2, x3 = positions[indices]\n                    mutant_vector = x1 + F * (x2 - x3)\n                    trial_vector = np.where(np.random.rand(self.dim) < CR, mutant_vector, positions[i])\n                    trial_vector = np.clip(trial_vector, lb, ub)\n\n                    trial_fitness = func(trial_vector)\n                    self.eval_count += 1\n\n                    # Update personal bests and global best for trial vector\n                    if trial_fitness < personal_best_values[i]:\n                        personal_best_positions[i] = trial_vector\n                        personal_best_values[i] = trial_fitness\n\n                    if trial_fitness < global_best_value:\n                        global_best_position = trial_vector\n                        global_best_value = trial_fitness\n\n                    if self.eval_count >= self.budget:\n                        break\n\n            # Enhanced Local Search\n            for i in range(num_agents):\n                perturbation = np.random.normal(0, np.random.uniform(0.05, 0.15), self.dim)\n                candidate_position = positions[i] + perturbation\n                candidate_position = np.clip(candidate_position, lb, ub)\n                candidate_fitness = func(candidate_position)\n                self.eval_count += 1\n                if candidate_fitness < personal_best_values[i]:\n                    personal_best_positions[i] = candidate_position\n                    personal_best_values[i] = candidate_fitness\n                if candidate_fitness < global_best_value:\n                    global_best_position = candidate_position\n                    global_best_value = candidate_fitness\n\n        return global_best_position", "name": "HybridPSODEOptimizer", "description": "Slightly increase the cognitive coefficient to enhance exploration capabilities further.", "configspace": "", "generation": 89, "fitness": 0.09679438895353447, "feedback": "The algorithm HybridPSODEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.097 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best).", "error": "", "parent_id": "f23c0bf4-2b50-484f-a754-2ebb35acbc25", "metadata": {"aucs": [0.09709448954279565, 0.09654736450839863, 0.09674131280940912], "final_y": [0.16485577264387496, 0.16485577213567149, 0.16485577196217438]}, "mutation_prompt": null}
{"id": "14f285b4-ab44-4bd7-8aed-4bd7d816ac05", "solution": "import numpy as np\n\nclass HybridPSODEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_agents = 5 + int(0.1 * self.dim)\n        inertia_weight = 0.9  # Adaptive inertia weight\n        cognitive_coefficient = 1.85  # Slight increase for more exploration (changed from 1.75)\n        social_coefficient = 1.35  # Slight decrease for less crowding (changed from 1.4)\n        F = 0.6  # Adjusted Differential evolution scaling factor for diversity\n        CR = 0.85  # Reduced crossover probability\n\n        # Initialize position and velocity for each agent\n        lb, ub = func.bounds.lb, func.bounds.ub\n        positions = np.random.uniform(lb, ub, (num_agents, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_agents, self.dim))\n\n        # Evaluate initial positions\n        fitness = np.apply_along_axis(func, 1, positions)\n        self.eval_count += num_agents\n\n        best_agent_idx = np.argmin(fitness)\n        global_best_position = positions[best_agent_idx].copy()\n        global_best_value = fitness[best_agent_idx]\n\n        personal_best_positions = positions.copy()\n        personal_best_values = fitness.copy()\n\n        while self.eval_count < self.budget:\n            for i in range(num_agents):\n                # Update velocity using PSO with adaptive inertia\n                r1, r2 = np.random.rand(2)\n                cognitive_component = cognitive_coefficient * r1 * (personal_best_positions[i] - positions[i])\n                social_component = social_coefficient * r2 * (global_best_position - positions[i])\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n                inertia_weight = max(0.4, inertia_weight * 0.98)  # Dynamic decay with a floor\n\n                # Update position\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], lb, ub)  # ensure within bounds\n\n                # Evaluate current position\n                current_fitness = func(positions[i])\n                self.eval_count += 1\n\n                # Update personal bests\n                if current_fitness < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_fitness\n\n                # Update global best\n                if current_fitness < global_best_value:\n                    global_best_position = positions[i]\n                    global_best_value = current_fitness\n\n                if self.eval_count >= self.budget:\n                    break\n\n            # Differential Evolution-inspired mutation\n            for i in range(num_agents):\n                if np.random.rand() < CR:\n                    indices = np.random.choice(num_agents, 3, replace=False)\n                    x1, x2, x3 = positions[indices]\n                    mutant_vector = x1 + F * (x2 - x3)\n                    trial_vector = np.where(np.random.rand(self.dim) < CR, mutant_vector, positions[i])\n                    trial_vector = np.clip(trial_vector, lb, ub)\n\n                    trial_fitness = func(trial_vector)\n                    self.eval_count += 1\n\n                    # Update personal bests and global best for trial vector\n                    if trial_fitness < personal_best_values[i]:\n                        personal_best_positions[i] = trial_vector\n                        personal_best_values[i] = trial_fitness\n\n                    if trial_fitness < global_best_value:\n                        global_best_position = trial_vector\n                        global_best_value = trial_fitness\n\n                    if self.eval_count >= self.budget:\n                        break\n\n            # Enhanced Local Search\n            for i in range(num_agents):\n                perturbation = np.random.normal(0, np.random.uniform(0.05, 0.15), self.dim)\n                candidate_position = positions[i] + perturbation\n                candidate_position = np.clip(candidate_position, lb, ub)\n                candidate_fitness = func(candidate_position)\n                self.eval_count += 1\n                if candidate_fitness < personal_best_values[i]:\n                    personal_best_positions[i] = candidate_position\n                    personal_best_values[i] = candidate_fitness\n                if candidate_fitness < global_best_value:\n                    global_best_position = candidate_position\n                    global_best_value = candidate_fitness\n\n        return global_best_position", "name": "HybridPSODEOptimizer", "description": "Improved exploration by increasing the cognitive coefficient to enhance the PSO's ability to explore new areas of the search space effectively.", "configspace": "", "generation": 90, "fitness": 0.09688323637274421, "feedback": "The algorithm HybridPSODEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.097 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best).", "error": "", "parent_id": "f23c0bf4-2b50-484f-a754-2ebb35acbc25", "metadata": {"aucs": [0.0971580585031514, 0.09675863982665778, 0.09673301078842345], "final_y": [0.16485583163443418, 0.16485577198851908, 0.16485577213019476]}, "mutation_prompt": null}
{"id": "731246e4-ec39-42d1-9321-e481fd4ee7a6", "solution": "import numpy as np\n\nclass HybridPSODEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_agents = 5 + int(0.1 * self.dim)\n        inertia_weight = 0.9  # Adaptive inertia weight\n        cognitive_coefficient = 1.75  # Slight increase for more exploration\n        social_coefficient = 1.35  # Slight decrease for less crowding\n        F = np.random.uniform(0.5, 0.8)  # Adaptive DE scaling factor (changed)\n        CR = np.random.uniform(0.75, 0.9)  # Adaptive crossover probability (changed)\n\n        # Initialize position and velocity for each agent\n        lb, ub = func.bounds.lb, func.bounds.ub\n        positions = np.random.uniform(lb, ub, (num_agents, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_agents, self.dim))\n\n        # Evaluate initial positions\n        fitness = np.apply_along_axis(func, 1, positions)\n        self.eval_count += num_agents\n\n        best_agent_idx = np.argmin(fitness)\n        global_best_position = positions[best_agent_idx].copy()\n        global_best_value = fitness[best_agent_idx]\n\n        personal_best_positions = positions.copy()\n        personal_best_values = fitness.copy()\n\n        while self.eval_count < self.budget:\n            for i in range(num_agents):\n                # Update velocity using PSO with adaptive inertia\n                r1, r2 = np.random.rand(2)\n                cognitive_component = cognitive_coefficient * r1 * (personal_best_positions[i] - positions[i])\n                social_component = social_coefficient * r2 * (global_best_position - positions[i])\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n                inertia_weight = max(0.4, inertia_weight * 0.98)  # Dynamic decay with a floor\n\n                # Update position\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], lb, ub)  # ensure within bounds\n\n                # Evaluate current position\n                current_fitness = func(positions[i])\n                self.eval_count += 1\n\n                # Update personal bests\n                if current_fitness < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_fitness\n\n                # Update global best\n                if current_fitness < global_best_value:\n                    global_best_position = positions[i]\n                    global_best_value = current_fitness\n\n                if self.eval_count >= self.budget:\n                    break\n\n            # Differential Evolution-inspired mutation\n            num_agents = max(3, num_agents - int(0.05 * num_agents))  # Dynamic agent reduction (changed)\n            for i in range(num_agents):\n                if np.random.rand() < CR:\n                    indices = np.random.choice(num_agents, 3, replace=False)\n                    x1, x2, x3 = positions[indices]\n                    mutant_vector = x1 + F * (x2 - x3)\n                    trial_vector = np.where(np.random.rand(self.dim) < CR, mutant_vector, positions[i])\n                    trial_vector = np.clip(trial_vector, lb, ub)\n\n                    trial_fitness = func(trial_vector)\n                    self.eval_count += 1\n\n                    # Update personal bests and global best for trial vector\n                    if trial_fitness < personal_best_values[i]:\n                        personal_best_positions[i] = trial_vector\n                        personal_best_values[i] = trial_fitness\n\n                    if trial_fitness < global_best_value:\n                        global_best_position = trial_vector\n                        global_best_value = trial_fitness\n\n                    if self.eval_count >= self.budget:\n                        break\n\n            # Enhanced Local Search\n            for i in range(num_agents):\n                perturbation = np.random.normal(0, np.random.uniform(0.05, 0.15), self.dim)\n                candidate_position = positions[i] + perturbation\n                candidate_position = np.clip(candidate_position, lb, ub)\n                candidate_fitness = func(candidate_position)\n                self.eval_count += 1\n                if candidate_fitness < personal_best_values[i]:\n                    personal_best_positions[i] = candidate_position\n                    personal_best_values[i] = candidate_fitness\n                if candidate_fitness < global_best_value:\n                    global_best_position = candidate_position\n                    global_best_value = candidate_fitness\n\n        return global_best_position", "name": "HybridPSODEOptimizer", "description": "Introduced adaptive differential evolution parameters and dynamic agent count to enhance exploration-exploitation balance.", "configspace": "", "generation": 91, "fitness": 0.0941321411189087, "feedback": "The algorithm HybridPSODEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.094 with standard deviation 0.004. And the mean value of best solutions found was 0.171 (0. is the best).", "error": "", "parent_id": "f23c0bf4-2b50-484f-a754-2ebb35acbc25", "metadata": {"aucs": [0.0969244987537562, 0.0886780041226981, 0.0967939204802718], "final_y": [0.16485577238909455, 0.1818780969486119, 0.16485577191663792]}, "mutation_prompt": null}
{"id": "4003480f-0e9a-4cc8-9f78-ac583a7bff75", "solution": "import numpy as np\n\nclass HybridPSODEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_agents = 5 + int(0.1 * self.dim)\n        inertia_weight = 0.9  # Adaptive inertia weight\n        cognitive_coefficient = 1.75  # Adapted for exploration\n        social_coefficient = 1.35  # Adapted for exploitation\n        F = 0.6  # Adjusted DE scaling factor for diversity\n        CR = 0.85  # Reduced crossover probability\n\n        # Initialize position and velocity for each agent\n        lb, ub = func.bounds.lb, func.bounds.ub\n        positions = np.random.uniform(lb, ub, (num_agents, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_agents, self.dim))\n\n        # Evaluate initial positions\n        fitness = np.apply_along_axis(func, 1, positions)\n        self.eval_count += num_agents\n\n        best_agent_idx = np.argmin(fitness)\n        global_best_position = positions[best_agent_idx].copy()\n        global_best_value = fitness[best_agent_idx]\n\n        personal_best_positions = positions.copy()\n        personal_best_values = fitness.copy()\n\n        while self.eval_count < self.budget:\n            for i in range(num_agents):\n                # Update velocity using PSO with adaptive inertia\n                r1, r2 = np.random.rand(2)\n                cognitive_component = cognitive_coefficient * r1 * (personal_best_positions[i] - positions[i])\n                social_component = social_coefficient * r2 * (global_best_position - positions[i])\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n                inertia_weight = max(0.4, inertia_weight * 0.98)  # Dynamic decay with a floor\n\n                # Dynamic adaptation of coefficients\n                cognitive_coefficient = max(1.5, cognitive_coefficient * 0.99)\n                social_coefficient = min(1.7, social_coefficient * 1.01)\n\n                # Update position\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], lb, ub)\n\n                # Evaluate current position\n                current_fitness = func(positions[i])\n                self.eval_count += 1\n\n                # Update personal bests\n                if current_fitness < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_fitness\n\n                # Update global best\n                if current_fitness < global_best_value:\n                    global_best_position = positions[i]\n                    global_best_value = current_fitness\n\n                if self.eval_count >= self.budget:\n                    break\n\n            # Differential Evolution-inspired mutation\n            for i in range(num_agents):\n                if np.random.rand() < CR:\n                    indices = np.random.choice(num_agents, 3, replace=False)\n                    x1, x2, x3 = positions[indices]\n                    mutant_vector = x1 + F * (x2 - x3)\n                    trial_vector = np.where(np.random.rand(self.dim) < CR, mutant_vector, positions[i])\n                    trial_vector = np.clip(trial_vector, lb, ub)\n\n                    trial_fitness = func(trial_vector)\n                    self.eval_count += 1\n\n                    # Update personal bests and global best for trial vector\n                    if trial_fitness < personal_best_values[i]:\n                        personal_best_positions[i] = trial_vector\n                        personal_best_values[i] = trial_fitness\n\n                    if trial_fitness < global_best_value:\n                        global_best_position = trial_vector\n                        global_best_value = trial_fitness\n\n                    if self.eval_count >= self.budget:\n                        break\n\n            # Enhanced Local Search with noise\n            for i in range(num_agents):\n                perturbation = np.random.normal(0, np.random.uniform(0.05, 0.15), self.dim)\n                candidate_position = positions[i] + perturbation\n                candidate_position += np.random.normal(0, 0.01, self.dim)  # Noise-induced perturbation\n                candidate_position = np.clip(candidate_position, lb, ub)\n                candidate_fitness = func(candidate_position)\n                self.eval_count += 1\n                if candidate_fitness < personal_best_values[i]:\n                    personal_best_positions[i] = candidate_position\n                    personal_best_values[i] = candidate_fitness\n                if candidate_fitness < global_best_value:\n                    global_best_position = candidate_position\n                    global_best_value = candidate_fitness\n\n        return global_best_position", "name": "HybridPSODEOptimizer", "description": "Introduced dynamic adaptation of cognitive and social coefficients and enhanced local search with noise-induced perturbations for improved convergence.", "configspace": "", "generation": 92, "fitness": 0.0967744398764595, "feedback": "The algorithm HybridPSODEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.097 with standard deviation 0.001. And the mean value of best solutions found was 0.165 (0. is the best).", "error": "", "parent_id": "f23c0bf4-2b50-484f-a754-2ebb35acbc25", "metadata": {"aucs": [0.09739531408222624, 0.09593776673411647, 0.09699023881303581], "final_y": [0.16485577190871725, 0.1648557733335071, 0.16485577192279732]}, "mutation_prompt": null}
{"id": "ce115097-6ac5-49f8-85ba-4d0a06fe38e6", "solution": "import numpy as np\n\nclass HybridPSODEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_agents = 5 + int(0.1 * self.dim)\n        inertia_weight = 0.9  # Adaptive inertia weight\n        cognitive_coefficient = 1.75\n        social_coefficient = 1.35\n        F = 0.5 + 0.5 * np.sin(np.pi * np.arange(num_agents) / num_agents) # Adaptive F using sine modulation\n        CR = 0.75 + 0.1 * np.random.rand(num_agents) # Stochastic CR for diversity\n\n        # Initialize position and velocity for each agent\n        lb, ub = func.bounds.lb, func.bounds.ub\n        positions = np.random.uniform(lb, ub, (num_agents, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_agents, self.dim))\n\n        # Evaluate initial positions\n        fitness = np.apply_along_axis(func, 1, positions)\n        self.eval_count += num_agents\n\n        best_agent_idx = np.argmin(fitness)\n        global_best_position = positions[best_agent_idx].copy()\n        global_best_value = fitness[best_agent_idx]\n\n        personal_best_positions = positions.copy()\n        personal_best_values = fitness.copy()\n\n        chaotic_seq = np.linspace(0, 1, num_agents)\n        np.random.shuffle(chaotic_seq)\n\n        while self.eval_count < self.budget:\n            for i in range(num_agents):\n                # Update velocity using PSO with adaptive inertia\n                r1, r2 = np.random.rand(2)\n                cognitive_component = cognitive_coefficient * r1 * (personal_best_positions[i] - positions[i])\n                social_component = social_coefficient * r2 * (global_best_position - positions[i])\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n                inertia_weight = max(0.4, inertia_weight * 0.98)\n\n                # Update position\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], lb, ub)\n\n                # Evaluate current position\n                current_fitness = func(positions[i])\n                self.eval_count += 1\n\n                # Update personal bests\n                if current_fitness < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_fitness\n\n                # Update global best\n                if current_fitness < global_best_value:\n                    global_best_position = positions[i]\n                    global_best_value = current_fitness\n\n                if self.eval_count >= self.budget:\n                    break\n\n            # Differential Evolution-inspired mutation\n            for i in range(num_agents):\n                if np.random.rand() < CR[i]:\n                    indices = np.random.choice(num_agents, 3, replace=False)\n                    x1, x2, x3 = positions[indices]\n                    mutant_vector = x1 + F[i] * (x2 - x3)\n                    trial_vector = np.where(np.random.rand(self.dim) < CR[i], mutant_vector, positions[i])\n                    trial_vector = np.clip(trial_vector, lb, ub)\n\n                    trial_fitness = func(trial_vector)\n                    self.eval_count += 1\n\n                    # Update personal bests and global best for trial vector\n                    if trial_fitness < personal_best_values[i]:\n                        personal_best_positions[i] = trial_vector\n                        personal_best_values[i] = trial_fitness\n\n                    if trial_fitness < global_best_value:\n                        global_best_position = trial_vector\n                        global_best_value = trial_fitness\n\n                    if self.eval_count >= self.budget:\n                        break\n\n            # Enhanced Local Search\n            for i in range(num_agents):\n                perturbation = np.random.normal(0, 0.1 * chaotic_seq[i], self.dim)  # Chaotic perturbation\n                candidate_position = positions[i] + perturbation\n                candidate_position = np.clip(candidate_position, lb, ub)\n                candidate_fitness = func(candidate_position)\n                self.eval_count += 1\n                if candidate_fitness < personal_best_values[i]:\n                    personal_best_positions[i] = candidate_position\n                    personal_best_values[i] = candidate_fitness\n                if candidate_fitness < global_best_value:\n                    global_best_position = candidate_position\n                    global_best_value = candidate_fitness\n\n        return global_best_position", "name": "HybridPSODEOptimizer", "description": "Introduce adaptive parameters and dynamic exploration with a chaotic map to enhance diversity and convergence.", "configspace": "", "generation": 93, "fitness": 0.08692312013364319, "feedback": "The algorithm HybridPSODEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.087 with standard deviation 0.010. And the mean value of best solutions found was 0.202 (0. is the best).", "error": "", "parent_id": "f23c0bf4-2b50-484f-a754-2ebb35acbc25", "metadata": {"aucs": [0.09135692321987421, 0.09651718029234324, 0.07289525688871212], "final_y": [0.18187809676917255, 0.16485577196845447, 0.25781008208793]}, "mutation_prompt": null}
{"id": "dd4bacf1-9003-4a36-ab2d-20fe1eb7f51b", "solution": "import numpy as np\n\nclass HybridPSODEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def __call__(self, func):\n        num_agents = 5 + int(0.1 * self.dim)\n        inertia_weight = 0.9\n        cognitive_coefficient = 1.75\n        social_coefficient = 1.35\n        F = 0.6\n        CR = 0.85\n\n        lb, ub = func.bounds.lb, func.bounds.ub\n        positions = np.random.uniform(lb, ub, (num_agents, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_agents, self.dim))\n        fitness = np.apply_along_axis(func, 1, positions)\n        self.eval_count += num_agents\n\n        best_agent_idx = np.argmin(fitness)\n        global_best_position = positions[best_agent_idx].copy()\n        global_best_value = fitness[best_agent_idx]\n\n        personal_best_positions = positions.copy()\n        personal_best_values = fitness.copy()\n\n        while self.eval_count < self.budget:\n            for i in range(num_agents):\n                r1, r2 = np.random.rand(2)\n                cognitive_component = cognitive_coefficient * r1 * (personal_best_positions[i] - positions[i])\n                social_component = social_coefficient * r2 * (global_best_position - positions[i])\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component * np.random.uniform(0.9, 1.1)\n                inertia_weight = max(0.4, inertia_weight * 0.98)\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], lb, ub)\n\n                current_fitness = func(positions[i])\n                self.eval_count += 1\n\n                if current_fitness < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_fitness\n\n                if current_fitness < global_best_value:\n                    global_best_position = positions[i]\n                    global_best_value = current_fitness\n\n                if self.eval_count >= self.budget:\n                    break\n\n            for i in range(num_agents):\n                F_dynamic = F * (0.5 + np.random.rand() / 2)\n                if np.random.rand() < CR:\n                    indices = np.random.choice(num_agents, 3, replace=False)\n                    x1, x2, x3 = positions[indices]\n                    mutant_vector = x1 + F_dynamic * (x2 - x3)\n                    trial_vector = np.where(np.random.rand(self.dim) < CR, mutant_vector, positions[i])\n                    trial_vector = np.clip(trial_vector, lb, ub)\n\n                    trial_fitness = func(trial_vector)\n                    self.eval_count += 1\n\n                    if trial_fitness < personal_best_values[i]:\n                        personal_best_positions[i] = trial_vector\n                        personal_best_values[i] = trial_fitness\n\n                    if trial_fitness < global_best_value:\n                        global_best_position = trial_vector\n                        global_best_value = trial_fitness\n\n                    if self.eval_count >= self.budget:\n                        break\n\n            for i in range(num_agents):\n                perturbation = np.random.normal(0, np.random.uniform(0.05, 0.1), self.dim)\n                candidate_position = positions[i] + perturbation\n                candidate_position = np.clip(candidate_position, lb, ub)\n                candidate_fitness = func(candidate_position)\n                self.eval_count += 1\n                if candidate_fitness < personal_best_values[i]:\n                    personal_best_positions[i] = candidate_position\n                    personal_best_values[i] = candidate_fitness\n                if candidate_fitness < global_best_value:\n                    global_best_position = candidate_position\n                    global_best_value = candidate_fitness\n\n        return global_best_position", "name": "HybridPSODEOptimizer", "description": "Introducing adaptive mutation control and stochastic perturbations to enhance convergence efficiency.", "configspace": "", "generation": 94, "fitness": 0.09616438115620562, "feedback": "The algorithm HybridPSODEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.096 with standard deviation 0.001. And the mean value of best solutions found was 0.165 (0. is the best).", "error": "", "parent_id": "f23c0bf4-2b50-484f-a754-2ebb35acbc25", "metadata": {"aucs": [0.09662626723363077, 0.09534393510054173, 0.09652294113444437], "final_y": [0.16485577317830613, 0.1648557719318391, 0.1648557720320294]}, "mutation_prompt": null}
{"id": "35a0d7f9-f350-4eb3-aec7-7a66d7ead741", "solution": "import numpy as np\n\nclass HybridPSODEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_agents = 5 + int(0.1 * self.dim)\n        inertia_weight = 0.9  # Adaptive inertia weight\n        cognitive_coefficient = 1.75  # Slight increase for more exploration\n        social_coefficient = 1.35  # Slight decrease for less crowding\n        F = 0.6  # Adjusted Differential evolution scaling factor for diversity\n        CR = 0.85  # Reduced crossover probability\n\n        # Initialize position and velocity for each agent\n        lb, ub = func.bounds.lb, func.bounds.ub\n        positions = np.random.uniform(lb, ub, (num_agents, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_agents, self.dim))\n\n        # Evaluate initial positions\n        fitness = np.apply_along_axis(func, 1, positions)\n        self.eval_count += num_agents\n\n        best_agent_idx = np.argmin(fitness)\n        global_best_position = positions[best_agent_idx].copy()\n        global_best_value = fitness[best_agent_idx]\n\n        personal_best_positions = positions.copy()\n        personal_best_values = fitness.copy()\n\n        while self.eval_count < self.budget:\n            for i in range(num_agents):\n                # Update velocity using PSO with adaptive inertia\n                r1, r2 = np.random.rand(2)\n                cognitive_component = cognitive_coefficient * r1 * (personal_best_positions[i] - positions[i])\n                social_component = social_coefficient * r2 * (global_best_position - positions[i])\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n                inertia_weight = max(0.4, inertia_weight * 0.98)  # Dynamic decay with a floor\n\n                # Update position\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], lb, ub)  # ensure within bounds\n\n                # Evaluate current position\n                current_fitness = func(positions[i])\n                self.eval_count += 1\n\n                # Update personal bests\n                if current_fitness < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_fitness\n\n                # Update global best\n                if current_fitness < global_best_value:\n                    global_best_position = positions[i]\n                    global_best_value = current_fitness\n\n                if self.eval_count >= self.budget:\n                    break\n\n            # Differential Evolution-inspired mutation with adaptive F and CR\n            for i in range(num_agents):\n                if np.random.rand() < CR:\n                    F_adaptive = F * (1 + np.random.uniform(-0.1, 0.1))\n                    CR_adaptive = CR * (1 + np.random.uniform(-0.05, 0.05))\n                    indices = np.random.choice(num_agents, 3, replace=False)\n                    x1, x2, x3 = positions[indices]\n                    mutant_vector = x1 + F_adaptive * (x2 - x3)\n                    trial_vector = np.where(np.random.rand(self.dim) < CR_adaptive, mutant_vector, positions[i])\n                    trial_vector = np.clip(trial_vector, lb, ub)\n\n                    trial_fitness = func(trial_vector)\n                    self.eval_count += 1\n\n                    # Update personal bests and global best for trial vector\n                    if trial_fitness < personal_best_values[i]:\n                        personal_best_positions[i] = trial_vector\n                        personal_best_values[i] = trial_fitness\n\n                    if trial_fitness < global_best_value:\n                        global_best_position = trial_vector\n                        global_best_value = trial_fitness\n\n                    if self.eval_count >= self.budget:\n                        break\n\n            # Enhanced Local Search\n            for i in range(num_agents):\n                perturbation = np.random.normal(0, np.random.uniform(0.05, 0.15), self.dim)\n                candidate_position = positions[i] + perturbation\n                candidate_position = np.clip(candidate_position, lb, ub)\n                candidate_fitness = func(candidate_position)\n                self.eval_count += 1\n                if candidate_fitness < personal_best_values[i]:\n                    personal_best_positions[i] = candidate_position\n                    personal_best_values[i] = candidate_fitness\n                if candidate_fitness < global_best_value:\n                    global_best_position = candidate_position\n                    global_best_value = candidate_fitness\n\n        return global_best_position", "name": "HybridPSODEOptimizer", "description": "Introduced adaptive differential evolution parameters and enhanced mutation strategy for improved convergence.", "configspace": "", "generation": 95, "fitness": 0.09445929960089357, "feedback": "The algorithm HybridPSODEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.094 with standard deviation 0.003. And the mean value of best solutions found was 0.171 (0. is the best).", "error": "", "parent_id": "f23c0bf4-2b50-484f-a754-2ebb35acbc25", "metadata": {"aucs": [0.09693934071722521, 0.09541469326456331, 0.0910238648208922], "final_y": [0.16485577197891066, 0.1648557721658539, 0.18187809677504074]}, "mutation_prompt": null}
{"id": "82b5ad07-4349-48f0-ab68-e86b8106d20c", "solution": "import numpy as np\n\nclass HybridPSODEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_agents = 5 + int(0.1 * self.dim)\n        inertia_weight = 0.9  # Adaptive inertia weight\n        cognitive_coefficient = 1.75  # Slight increase for more exploration\n        social_coefficient = 1.35  # Slight decrease for less crowding\n        F = 0.6  # Adjusted Differential evolution scaling factor for diversity\n        CR = 0.85  # Reduced crossover probability\n\n        # Initialize position and velocity for each agent\n        lb, ub = func.bounds.lb, func.bounds.ub\n        positions = np.random.uniform(lb, ub, (num_agents, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_agents, self.dim))\n\n        # Evaluate initial positions\n        fitness = np.apply_along_axis(func, 1, positions)\n        self.eval_count += num_agents\n\n        best_agent_idx = np.argmin(fitness)\n        global_best_position = positions[best_agent_idx].copy()\n        global_best_value = fitness[best_agent_idx]\n\n        personal_best_positions = positions.copy()\n        personal_best_values = fitness.copy()\n\n        while self.eval_count < self.budget:\n            for i in range(num_agents):\n                # Update velocity using PSO with adaptive coefficients\n                r1, r2 = np.random.rand(2)\n                cognitive_component = cognitive_coefficient * r1 * (personal_best_positions[i] - positions[i])\n                social_component = social_coefficient * r2 * (global_best_position - positions[i])\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n                inertia_weight = max(0.4, inertia_weight * 0.98)  # Dynamic decay with a floor\n\n                # Adjust coefficients dynamically based on progress\n                cognitive_coefficient = 1.5 + (0.5 * (self.budget - self.eval_count) / self.budget)\n                social_coefficient = 1.5 - (0.5 * self.eval_count / self.budget)\n\n                # Update position\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], lb, ub)  # ensure within bounds\n\n                # Evaluate current position\n                current_fitness = func(positions[i])\n                self.eval_count += 1\n\n                # Update personal bests\n                if current_fitness < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_fitness\n\n                # Update global best\n                if current_fitness < global_best_value:\n                    global_best_position = positions[i]\n                    global_best_value = current_fitness\n\n                if self.eval_count >= self.budget:\n                    break\n\n            # Differential Evolution-inspired mutation\n            for i in range(num_agents):\n                if np.random.rand() < CR:\n                    indices = np.random.choice(num_agents, 3, replace=False)\n                    x1, x2, x3 = positions[indices]\n                    mutant_vector = x1 + F * (x2 - x3)\n                    trial_vector = np.where(np.random.rand(self.dim) < CR, mutant_vector, positions[i])\n                    trial_vector = np.clip(trial_vector, lb, ub)\n\n                    trial_fitness = func(trial_vector)\n                    self.eval_count += 1\n\n                    # Update personal bests and global best for trial vector\n                    if trial_fitness < personal_best_values[i]:\n                        personal_best_positions[i] = trial_vector\n                        personal_best_values[i] = trial_fitness\n\n                    if trial_fitness < global_best_value:\n                        global_best_position = trial_vector\n                        global_best_value = trial_fitness\n\n                    if self.eval_count >= self.budget:\n                        break\n\n            # Enhanced Local Search\n            for i in range(num_agents):\n                perturbation = np.random.normal(0, np.random.uniform(0.05, 0.15), self.dim)\n                candidate_position = positions[i] + perturbation\n                candidate_position = np.clip(candidate_position, lb, ub)\n                candidate_fitness = func(candidate_position)\n                self.eval_count += 1\n                if candidate_fitness < personal_best_values[i]:\n                    personal_best_positions[i] = candidate_position\n                    personal_best_values[i] = candidate_fitness\n                if candidate_fitness < global_best_value:\n                    global_best_position = candidate_position\n                    global_best_value = candidate_fitness\n\n        return global_best_position", "name": "HybridPSODEOptimizer", "description": "Introduced adaptive cognitive and social coefficients for dynamic exploration-exploitation trade-off.", "configspace": "", "generation": 96, "fitness": 0.09633026892671441, "feedback": "The algorithm HybridPSODEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.096 with standard deviation 0.001. And the mean value of best solutions found was 0.165 (0. is the best).", "error": "", "parent_id": "f23c0bf4-2b50-484f-a754-2ebb35acbc25", "metadata": {"aucs": [0.09703689991001452, 0.09527054131367674, 0.09668336555645196], "final_y": [0.16485577258245954, 0.16485580100067376, 0.1648557738002716]}, "mutation_prompt": null}
{"id": "9819ca83-6ec2-4bed-8448-8c619efb1518", "solution": "import numpy as np\n\nclass HybridPSODEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_agents = 5 + int(0.1 * self.dim)\n        inertia_weight = 0.9  # Dynamic inertia weight\n        cognitive_coefficient = 1.85  # Enhanced exploration\n        social_coefficient = 1.3  # Reduced crowding\n        F = 0.8  # Increased differential evolution scaling factor\n        CR = 0.9  # Increased crossover probability\n\n        # Initialize position and velocity for each agent\n        lb, ub = func.bounds.lb, func.bounds.ub\n        positions = np.random.uniform(lb, ub, (num_agents, self.dim))\n        velocities = np.random.uniform(-0.5, 0.5, (num_agents, self.dim))  # Reduced initial velocity range\n\n        # Evaluate initial positions\n        fitness = np.apply_along_axis(func, 1, positions)\n        self.eval_count += num_agents\n\n        best_agent_idx = np.argmin(fitness)\n        global_best_position = positions[best_agent_idx].copy()\n        global_best_value = fitness[best_agent_idx]\n\n        personal_best_positions = positions.copy()\n        personal_best_values = fitness.copy()\n\n        while self.eval_count < self.budget:\n            for i in range(num_agents):\n                # Update velocity using PSO with adaptive inertia\n                r1, r2 = np.random.rand(2)\n                cognitive_component = cognitive_coefficient * r1 * (personal_best_positions[i] - positions[i])\n                social_component = social_coefficient * r2 * (global_best_position - positions[i])\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n                inertia_weight = max(0.5, inertia_weight * 0.95)  # Dynamic decay with a floor\n\n                # Update position\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], lb, ub)\n\n                # Evaluate current position\n                current_fitness = func(positions[i])\n                self.eval_count += 1\n\n                # Update personal bests\n                if current_fitness < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_fitness\n\n                # Update global best\n                if current_fitness < global_best_value:\n                    global_best_position = positions[i]\n                    global_best_value = current_fitness\n\n                if self.eval_count >= self.budget:\n                    break\n\n            # Differential Evolution-inspired mutation\n            for i in range(num_agents):\n                if np.random.rand() < CR:\n                    indices = np.random.choice(num_agents, 3, replace=False)\n                    x1, x2, x3 = positions[indices]\n                    mutant_vector = x1 + F * (x2 - x3)\n                    trial_vector = np.where(np.random.rand(self.dim) < CR, mutant_vector, positions[i])\n                    trial_vector = np.clip(trial_vector, lb, ub)\n\n                    trial_fitness = func(trial_vector)\n                    self.eval_count += 1\n\n                    # Update personal bests and global best for trial vector\n                    if trial_fitness < personal_best_values[i]:\n                        personal_best_positions[i] = trial_vector\n                        personal_best_values[i] = trial_fitness\n\n                    if trial_fitness < global_best_value:\n                        global_best_position = trial_vector\n                        global_best_value = trial_fitness\n\n                    if self.eval_count >= self.budget:\n                        break\n\n            # Enhanced Local Search\n            for i in range(num_agents):\n                perturbation = np.random.normal(0, np.random.uniform(0.03, 0.1), self.dim)  # Adjusted perturbation range\n                candidate_position = positions[i] + perturbation\n                candidate_position = np.clip(candidate_position, lb, ub)\n                candidate_fitness = func(candidate_position)\n                self.eval_count += 1\n                if candidate_fitness < personal_best_values[i]:\n                    personal_best_positions[i] = candidate_position\n                    personal_best_values[i] = candidate_fitness\n                if candidate_fitness < global_best_value:\n                    global_best_position = candidate_position\n                    global_best_value = candidate_fitness\n\n        return global_best_position", "name": "HybridPSODEOptimizer", "description": "Improved local exploration and exploitation through adaptive mutation and velocity control.", "configspace": "", "generation": 97, "fitness": 0.09354281998097935, "feedback": "The algorithm HybridPSODEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.094 with standard deviation 0.004. And the mean value of best solutions found was 0.173 (0. is the best).", "error": "", "parent_id": "f23c0bf4-2b50-484f-a754-2ebb35acbc25", "metadata": {"aucs": [0.09667908731692787, 0.08831111642885647, 0.09563825619715371], "final_y": [0.1648557725739569, 0.18813064475289265, 0.16485577419221542]}, "mutation_prompt": null}
{"id": "5ba4f1f2-8a41-417c-a5c9-7be3f20443d7", "solution": "import numpy as np\n\nclass HybridPSODEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_agents = 5 + int(0.1 * self.dim)\n        inertia_weight = 0.9  # Adaptive inertia weight\n        cognitive_coefficient = 1.75  # Slight increase for more exploration (changed from 1.7)\n        social_coefficient = 1.35  # Slight decrease for less crowding (changed from 1.4)\n        F = 0.6  # Adjusted Differential evolution scaling factor for diversity\n        CR = 0.85  # Reduced crossover probability\n\n        # Initialize position and velocity for each agent\n        lb, ub = func.bounds.lb, func.bounds.ub\n        positions = np.random.uniform(lb, ub, (num_agents, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_agents, self.dim))\n\n        # Evaluate initial positions\n        fitness = np.apply_along_axis(func, 1, positions)\n        self.eval_count += num_agents\n\n        best_agent_idx = np.argmin(fitness)\n        global_best_position = positions[best_agent_idx].copy()\n        global_best_value = fitness[best_agent_idx]\n\n        personal_best_positions = positions.copy()\n        personal_best_values = fitness.copy()\n\n        while self.eval_count < self.budget:\n            for i in range(num_agents):\n                # Update velocity using PSO with adaptive inertia\n                r1, r2 = np.random.rand(2)\n                cognitive_component = cognitive_coefficient * r1 * (personal_best_positions[i] - positions[i])\n                social_component = social_coefficient * r2 * (global_best_position - positions[i])\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n                inertia_weight = max(0.4, inertia_weight * 0.98)  # Dynamic decay with a floor\n\n                # Update position\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], lb, ub)  # ensure within bounds\n\n                # Evaluate current position\n                current_fitness = func(positions[i])\n                self.eval_count += 1\n\n                # Update personal bests\n                if current_fitness < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_fitness\n\n                # Update global best\n                if current_fitness < global_best_value:\n                    global_best_position = positions[i]\n                    global_best_value = current_fitness\n\n                if self.eval_count >= self.budget:\n                    break\n\n            # Differential Evolution-inspired mutation\n            F = 0.4 + 0.3 * np.random.rand()  # Dynamic adjustment of F\n            for i in range(num_agents):\n                if np.random.rand() < CR:\n                    indices = np.random.choice(num_agents, 3, replace=False)\n                    x1, x2, x3 = positions[indices]\n                    mutant_vector = x1 + F * (x2 - x3)\n                    trial_vector = np.where(np.random.rand(self.dim) < CR, mutant_vector, positions[i])\n                    trial_vector = np.clip(trial_vector, lb, ub)\n\n                    trial_fitness = func(trial_vector)\n                    self.eval_count += 1\n\n                    # Update personal bests and global best for trial vector\n                    if trial_fitness < personal_best_values[i]:\n                        personal_best_positions[i] = trial_vector\n                        personal_best_values[i] = trial_fitness\n\n                    if trial_fitness < global_best_value:\n                        global_best_position = trial_vector\n                        global_best_value = trial_fitness\n\n                    if self.eval_count >= self.budget:\n                        break\n\n            # Enhanced Local Search\n            for i in range(num_agents):\n                perturbation = np.random.normal(0, np.random.uniform(0.05, 0.15), self.dim)\n                candidate_position = positions[i] + perturbation\n                candidate_position = np.clip(candidate_position, lb, ub)\n                candidate_fitness = func(candidate_position)\n                self.eval_count += 1\n                if candidate_fitness < personal_best_values[i]:\n                    personal_best_positions[i] = candidate_position\n                    personal_best_values[i] = candidate_fitness\n                if candidate_fitness < global_best_value:\n                    global_best_position = candidate_position\n                    global_best_value = candidate_fitness\n\n        return global_best_position", "name": "HybridPSODEOptimizer", "description": "Introduced dynamic adjustment of the Differential evolution scaling factor for improved diversity control.", "configspace": "", "generation": 98, "fitness": 0.09237199980263226, "feedback": "The algorithm HybridPSODEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.092 with standard deviation 0.006. And the mean value of best solutions found was 0.179 (0. is the best).", "error": "", "parent_id": "f23c0bf4-2b50-484f-a754-2ebb35acbc25", "metadata": {"aucs": [0.09657226928304319, 0.09606956760638419, 0.0844741625184694], "final_y": [0.1648557940197496, 0.16485577196406986, 0.20725427014537667]}, "mutation_prompt": null}
{"id": "1d637d77-c0bf-40aa-8984-a186141c94d9", "solution": "import numpy as np\n\nclass HybridPSODEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_agents = 5 + int(0.1 * self.dim)\n        inertia_weight = 0.9  # Adaptive inertia weight\n        cognitive_coefficient = 1.75  # Slight increase for more exploration (changed from 1.7)\n        social_coefficient = 1.35  # Slight decrease for less crowding (changed from 1.4)\n        F = 0.6  # Adjusted Differential evolution scaling factor for diversity\n        CR = 0.85  # Reduced crossover probability\n\n        # Initialize position and velocity for each agent\n        lb, ub = func.bounds.lb, func.bounds.ub\n        positions = np.random.uniform(lb, ub, (num_agents, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_agents, self.dim))\n\n        # Evaluate initial positions\n        fitness = np.apply_along_axis(func, 1, positions)\n        self.eval_count += num_agents\n\n        best_agent_idx = np.argmin(fitness)\n        global_best_position = positions[best_agent_idx].copy()\n        global_best_value = fitness[best_agent_idx]\n\n        personal_best_positions = positions.copy()\n        personal_best_values = fitness.copy()\n\n        while self.eval_count < self.budget:\n            for i in range(num_agents):\n                # Update velocity using PSO with adaptive inertia\n                r1, r2 = np.random.rand(2)\n                cognitive_component = cognitive_coefficient * r1 * (personal_best_positions[i] - positions[i])\n                social_component = social_coefficient * r2 * (global_best_position - positions[i])\n                velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component\n                inertia_weight = max(0.4, inertia_weight * 0.98)  # Dynamic decay with a floor\n\n                # Update position\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], lb, ub)  # ensure within bounds\n\n                # Evaluate current position\n                current_fitness = func(positions[i])\n                self.eval_count += 1\n\n                # Update personal bests with adaptive learning rate\n                if current_fitness < personal_best_values[i]:\n                    learning_rate = 0.5 * (1 - self.eval_count / self.budget)\n                    personal_best_positions[i] = (1 - learning_rate) * personal_best_positions[i] + learning_rate * positions[i]\n                    personal_best_values[i] = current_fitness\n\n                # Update global best with adaptive learning rate\n                if current_fitness < global_best_value:\n                    learning_rate = 0.5 * (1 - self.eval_count / self.budget)\n                    global_best_position = (1 - learning_rate) * global_best_position + learning_rate * positions[i]\n                    global_best_value = current_fitness\n\n                if self.eval_count >= self.budget:\n                    break\n\n            # Differential Evolution-inspired mutation\n            for i in range(num_agents):\n                if np.random.rand() < CR:\n                    indices = np.random.choice(num_agents, 3, replace=False)\n                    x1, x2, x3 = positions[indices]\n                    mutant_vector = x1 + F * (x2 - x3)\n                    trial_vector = np.where(np.random.rand(self.dim) < CR, mutant_vector, positions[i])\n                    trial_vector = np.clip(trial_vector, lb, ub)\n\n                    trial_fitness = func(trial_vector)\n                    self.eval_count += 1\n\n                    # Update personal bests and global best for trial vector\n                    if trial_fitness < personal_best_values[i]:\n                        personal_best_positions[i] = trial_vector\n                        personal_best_values[i] = trial_fitness\n\n                    if trial_fitness < global_best_value:\n                        global_best_position = trial_vector\n                        global_best_value = trial_fitness\n\n                    if self.eval_count >= self.budget:\n                        break\n\n            # Enhanced Local Search\n            for i in range(num_agents):\n                perturbation = np.random.normal(0, np.random.uniform(0.05, 0.15), self.dim)\n                candidate_position = positions[i] + perturbation\n                candidate_position = np.clip(candidate_position, lb, ub)\n                candidate_fitness = func(candidate_position)\n                self.eval_count += 1\n                if candidate_fitness < personal_best_values[i]:\n                    personal_best_positions[i] = candidate_position\n                    personal_best_values[i] = candidate_fitness\n                if candidate_fitness < global_best_value:\n                    global_best_position = candidate_position\n                    global_best_value = candidate_fitness\n\n        return global_best_position", "name": "HybridPSODEOptimizer", "description": "The algorithm introduces adaptive learning rates for personal and global best updates, enhancing convergence dynamics.", "configspace": "", "generation": 99, "fitness": 0.09674384648862355, "feedback": "The algorithm HybridPSODEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.097 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best).", "error": "", "parent_id": "f23c0bf4-2b50-484f-a754-2ebb35acbc25", "metadata": {"aucs": [0.09684191130858433, 0.09671804464677702, 0.0966715835105093], "final_y": [0.16485578816370638, 0.16485577497216375, 0.1648558195483696]}, "mutation_prompt": null}
