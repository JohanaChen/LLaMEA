{"id": "63bcd042-f7b7-4607-b510-6a5c2616838b", "solution": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        best_solution = np.random.uniform(lb, ub, self.dim)\n        best_value = func(best_solution)\n        self.evaluations += 1\n        \n        while self.evaluations < self.budget:\n            if np.random.rand() < 0.5:\n                # Gradient-free exploration\n                candidate = np.random.uniform(lb, ub, self.dim)\n            else:\n                # Gradient-based exploitation\n                gradient = self.estimate_gradient(func, best_solution, lb, ub)\n                candidate = self.line_search(func, best_solution, gradient, lb, ub)\n\n            candidate_value = func(candidate)\n            self.evaluations += 1\n            \n            if candidate_value < best_value:\n                best_solution, best_value = candidate, candidate_value\n\n        return best_solution, best_value\n\n    def estimate_gradient(self, func, solution, lb, ub, epsilon=1e-4):\n        gradient = np.zeros(self.dim)\n        for i in range(self.dim):\n            perturb = np.zeros(self.dim)\n            perturb[i] = epsilon\n            upper_sol = np.clip(solution + perturb, lb, ub)\n            lower_sol = np.clip(solution - perturb, lb, ub)\n            gradient[i] = (func(upper_sol) - func(lower_sol)) / (2 * epsilon)\n            self.evaluations += 2\n        return gradient\n\n    def line_search(self, func, solution, gradient, lb, ub, alpha=1e-2, beta=0.9):\n        step_size = alpha\n        while step_size > 1e-5:\n            candidate = solution - step_size * gradient\n            candidate = np.clip(candidate, lb, ub)\n            if func(candidate) < func(solution):\n                return candidate\n            step_size *= beta\n            self.evaluations += 1\n        return solution", "name": "HybridOptimizer", "description": "Hybrid Gradient-Free and Gradient-Based Approach for Efficient Photonic Structure Optimization", "configspace": "", "generation": 0, "fitness": 0.0963727438525513, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.096 with standard deviation 0.002. And the mean value of best solutions found was 0.159 (0. is the best).", "error": "", "parent_id": null, "metadata": {"aucs": [0.09305690224614627, 0.09726005819951555, 0.0988012711119921], "final_y": [0.16178127416122723, 0.16010582702461906, 0.15644053947461056]}, "mutation_prompt": null}
{"id": "2ab5edba-1745-48f9-8582-5d1eec37436f", "solution": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        best_solution = np.random.uniform(lb, ub, self.dim)\n        best_value = func(best_solution)\n        self.evaluations += 1\n        \n        while self.evaluations < self.budget:\n            if np.random.rand() < 0.5:\n                # Gradient-free exploration\n                candidate = np.random.uniform(lb, ub, self.dim)\n            else:\n                # Gradient-based exploitation\n                gradient = self.estimate_gradient(func, best_solution, lb, ub)\n                candidate = self.line_search(func, best_solution, gradient, lb, ub)\n\n            candidate_value = func(candidate)\n            self.evaluations += 1\n            \n            if candidate_value < best_value:\n                best_solution, best_value = candidate, candidate_value\n\n        return best_solution, best_value\n\n    def estimate_gradient(self, func, solution, lb, ub, epsilon=1e-4):\n        gradient = np.zeros(self.dim)\n        for i in range(self.dim):\n            perturb = np.zeros(self.dim)\n            perturb[i] = epsilon\n            upper_sol = np.clip(solution + perturb, lb, ub)\n            lower_sol = np.clip(solution - perturb, lb, ub)\n            gradient[i] = (func(upper_sol) - func(lower_sol)) / (2 * epsilon)\n            self.evaluations += 2\n        return gradient\n\n    def line_search(self, func, solution, gradient, lb, ub, alpha=1e-2, beta=0.9):\n        step_size = alpha * np.linalg.norm(gradient)  # Modified line\n        while step_size > 1e-5:\n            candidate = solution - step_size * gradient\n            candidate = np.clip(candidate, lb, ub)\n            if func(candidate) < func(solution):\n                return candidate\n            step_size *= beta\n            self.evaluations += 1\n        return solution", "name": "HybridOptimizer", "description": "Introduced adaptive step size in line search for refined candidate exploration.", "configspace": "", "generation": 1, "fitness": 0.09641155278047797, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.096 with standard deviation 0.002. And the mean value of best solutions found was 0.159 (0. is the best).", "error": "", "parent_id": "63bcd042-f7b7-4607-b510-6a5c2616838b", "metadata": {"aucs": [0.09305668065480899, 0.09725932568895901, 0.09891865199766592], "final_y": [0.16178315971072366, 0.16011067911805865, 0.1564415668155349]}, "mutation_prompt": null}
{"id": "d0604624-c19f-4010-bc1f-c11b2db9ef0c", "solution": "import numpy as np\n\nclass AdvancedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        best_solution = np.random.uniform(lb, ub, self.dim)\n        best_value = func(best_solution)\n        self.evaluations += 1\n        \n        while self.evaluations < self.budget:\n            if np.random.rand() < 0.5:\n                candidate = self.stochastic_tunneling(best_solution, lb, ub)\n            else:\n                gradient = self.estimate_gradient(func, best_solution, lb, ub)\n                candidate = self.adaptive_line_search(func, best_solution, gradient, lb, ub)\n\n            candidate_value = func(candidate)\n            self.evaluations += 1\n            \n            if candidate_value < best_value:\n                best_solution, best_value = candidate, candidate_value\n\n        return best_solution, best_value\n\n    def estimate_gradient(self, func, solution, lb, ub, epsilon=1e-4):\n        gradient = np.zeros(self.dim)\n        for i in range(self.dim):\n            perturb = np.zeros(self.dim)\n            perturb[i] = epsilon\n            upper_sol = np.clip(solution + perturb, lb, ub)\n            lower_sol = np.clip(solution - perturb, lb, ub)\n            gradient[i] = (func(upper_sol) - func(lower_sol)) / (2 * epsilon)\n            self.evaluations += 2\n        return gradient\n\n    def adaptive_line_search(self, func, solution, gradient, lb, ub, alpha=1e-2, beta=0.9):\n        step_size = alpha * np.linalg.norm(gradient)\n        adaptive_scaling = 1.0 / (1.0 + np.linalg.norm(gradient))\n        while step_size > 1e-5:\n            candidate = solution - step_size * adaptive_scaling * gradient\n            candidate = np.clip(candidate, lb, ub)\n            if func(candidate) < func(solution):\n                return candidate\n            step_size *= beta\n            self.evaluations += 1\n        return solution\n\n    def stochastic_tunneling(self, solution, lb, ub):\n        tunnel_strength = np.random.normal(0, 0.1, self.dim)\n        candidate = solution + tunnel_strength\n        return np.clip(candidate, lb, ub)", "name": "AdvancedHybridOptimizer", "description": "Introduced a blend of adaptive gradient scaling and stochastic tunneling to balance exploration and exploitation.", "configspace": "", "generation": 2, "fitness": 0.07786509239835844, "feedback": "The algorithm AdvancedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.078 with standard deviation 0.011. And the mean value of best solutions found was 0.240 (0. is the best).", "error": "", "parent_id": "2ab5edba-1745-48f9-8582-5d1eec37436f", "metadata": {"aucs": [0.07671032643396813, 0.06453186315643467, 0.09235308760467253], "final_y": [0.24059652718127178, 0.30412458628504935, 0.17514672438574408]}, "mutation_prompt": null}
{"id": "61163630-39b8-44c4-9a24-6739dce3fe5a", "solution": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        best_solution = np.random.uniform(lb, ub, self.dim)\n        best_value = func(best_solution)\n        self.evaluations += 1\n        \n        while self.evaluations < self.budget:\n            if np.random.rand() < 0.5:\n                # Gradient-free exploration\n                candidate = np.random.uniform(lb, ub, self.dim)\n            else:\n                # Gradient-based exploitation with stochastic perturbation\n                gradient = self.estimate_gradient(func, best_solution, lb, ub)\n                perturbation = np.random.normal(0, 0.1, self.dim)\n                candidate = self.line_search(func, best_solution + perturbation, gradient, lb, ub)\n\n            candidate_value = func(candidate)\n            self.evaluations += 1\n            \n            if candidate_value < best_value:\n                best_solution, best_value = candidate, candidate_value\n\n        return best_solution, best_value\n\n    def estimate_gradient(self, func, solution, lb, ub, epsilon=1e-4):\n        gradient = np.zeros(self.dim)\n        for i in range(self.dim):\n            perturb = np.zeros(self.dim)\n            perturb[i] = epsilon\n            upper_sol = np.clip(solution + perturb, lb, ub)\n            lower_sol = np.clip(solution - perturb, lb, ub)\n            gradient[i] = (func(upper_sol) - func(lower_sol)) / (2 * epsilon)\n            self.evaluations += 2\n        return gradient\n\n    def line_search(self, func, solution, gradient, lb, ub, alpha=1e-2, beta=0.9):\n        step_size = alpha * np.linalg.norm(gradient)\n        while step_size > 1e-5:\n            candidate = solution - step_size * gradient\n            candidate = np.clip(candidate, lb, ub)\n            if func(candidate) < func(solution):\n                return candidate\n            step_size *= beta\n            self.evaluations += 1\n        return solution", "name": "HybridOptimizer", "description": "HybridOptimizer now incorporates stochastic perturbation in gradient-based exploitation for better exploration of local minima.", "configspace": "", "generation": 3, "fitness": 0.09264064474153984, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.093 with standard deviation 0.005. And the mean value of best solutions found was 0.170 (0. is the best).", "error": "", "parent_id": "2ab5edba-1745-48f9-8582-5d1eec37436f", "metadata": {"aucs": [0.08857449611028034, 0.08950898371129079, 0.0998384544030484], "final_y": [0.18264794107066828, 0.17369944921850378, 0.15347225020106514]}, "mutation_prompt": null}
{"id": "ed98d4dc-a72e-4240-a5af-5ac813238406", "solution": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        best_solution = np.random.uniform(lb, ub, self.dim)\n        best_value = func(best_solution)\n        self.evaluations += 1\n        \n        exploration_factor = 0.5  # Initial factor for exploration/exploitation\n        improvement_threshold = 1e-4\n        \n        while self.evaluations < self.budget:\n            if np.random.rand() < exploration_factor:\n                candidate = np.random.uniform(lb, ub, self.dim)\n            else:\n                gradient = self.estimate_gradient(func, best_solution, lb, ub)\n                candidate = self.line_search(func, best_solution, gradient, lb, ub)\n\n            candidate_value = func(candidate)\n            self.evaluations += 1\n            \n            if candidate_value < best_value - improvement_threshold:\n                best_solution, best_value = candidate, candidate_value\n                exploration_factor = max(0.3, exploration_factor - 0.05)  # Reduce exploration\n            else:\n                exploration_factor = min(0.7, exploration_factor + 0.05)  # Increase exploration\n\n        return best_solution, best_value\n\n    def estimate_gradient(self, func, solution, lb, ub, epsilon=1e-4):\n        gradient = np.zeros(self.dim)\n        for i in range(self.dim):\n            perturb = np.zeros(self.dim)\n            perturb[i] = epsilon\n            upper_sol = np.clip(solution + perturb, lb, ub)\n            lower_sol = np.clip(solution - perturb, lb, ub)\n            gradient[i] = (func(upper_sol) - func(lower_sol)) / (2 * epsilon)\n            self.evaluations += 2\n        return gradient\n\n    def line_search(self, func, solution, gradient, lb, ub, alpha=1e-2, beta=0.9):\n        step_size = alpha * np.linalg.norm(gradient)\n        while step_size > 1e-5:\n            candidate = solution - step_size * gradient\n            candidate = np.clip(candidate, lb, ub)\n            if func(candidate) < func(solution):\n                return candidate\n            step_size *= beta\n            self.evaluations += 1\n        return solution", "name": "HybridOptimizer", "description": "Introduced a dynamic balance between exploration and exploitation using a feedback mechanism to adaptively adjust search strategy based on recent performance.", "configspace": "", "generation": 4, "fitness": 0.09412231931736743, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.094 with standard deviation 0.002. And the mean value of best solutions found was 0.162 (0. is the best).", "error": "", "parent_id": "2ab5edba-1745-48f9-8582-5d1eec37436f", "metadata": {"aucs": [0.09605331601046418, 0.09462824557211469, 0.09168539636952344], "final_y": [0.16178316327772657, 0.1543698471115037, 0.16872717450692853]}, "mutation_prompt": null}
{"id": "e242b9cc-3bfb-4df7-ad66-8143f861dc6f", "solution": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        best_solution = np.random.uniform(lb, ub, self.dim)\n        best_value = func(best_solution)\n        self.evaluations += 1\n        \n        while self.evaluations < self.budget:\n            if np.random.rand() < 0.5:\n                # Gradient-free exploration\n                candidate = np.random.uniform(lb, ub, self.dim)\n            else:\n                # Gradient-based exploitation\n                gradient = self.estimate_gradient(func, best_solution, lb, ub)\n                candidate = self.line_search(func, best_solution, gradient, lb, ub)\n\n            candidate_value = func(candidate)\n            self.evaluations += 1\n            \n            if candidate_value < best_value:\n                best_solution, best_value = candidate, candidate_value\n\n        return best_solution, best_value\n\n    def estimate_gradient(self, func, solution, lb, ub, epsilon=1e-4):\n        gradient = np.zeros(self.dim)\n        for i in range(self.dim):\n            perturb = np.zeros(self.dim)\n            perturb[i] = epsilon\n            upper_sol = np.clip(solution + perturb, lb, ub)\n            lower_sol = np.clip(solution - perturb, lb, ub)\n            gradient[i] = (func(upper_sol) - func(lower_sol)) / (2 * epsilon)\n            self.evaluations += 2\n        if np.linalg.norm(gradient) < 1e-3:  # Adjust epsilon dynamically\n            epsilon *= 0.5\n        return gradient\n\n    def line_search(self, func, solution, gradient, lb, ub, alpha=1e-2, beta=0.9):\n        step_size = alpha * np.linalg.norm(gradient)  # Modified line\n        while step_size > 1e-5:\n            candidate = solution - step_size * gradient\n            candidate = np.clip(candidate, lb, ub)\n            if func(candidate) < func(solution):\n                return candidate\n            step_size *= beta\n            self.evaluations += 1\n        return solution", "name": "HybridOptimizer", "description": "Enhanced gradient estimation by dynamically adjusting epsilon for improved sensitivity.", "configspace": "", "generation": 5, "fitness": 0.09641155278047797, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.096 with standard deviation 0.002. And the mean value of best solutions found was 0.159 (0. is the best).", "error": "", "parent_id": "2ab5edba-1745-48f9-8582-5d1eec37436f", "metadata": {"aucs": [0.09305668065480899, 0.09725932568895901, 0.09891865199766592], "final_y": [0.16178315971072366, 0.16011067911805865, 0.1564415668155349]}, "mutation_prompt": null}
{"id": "a893eddb-d7a7-475a-a601-eb0580b0486a", "solution": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        best_solution = np.random.uniform(lb, ub, self.dim)\n        best_value = func(best_solution)\n        self.evaluations += 1\n        \n        while self.evaluations < self.budget:\n            if np.random.rand() < 0.5:\n                # Gradient-free exploration\n                candidate = np.random.normal(best_solution, (ub - lb) / 10, self.dim)\n                candidate = np.clip(candidate, lb, ub)\n            else:\n                # Gradient-based exploitation\n                gradient = self.estimate_gradient(func, best_solution, lb, ub)\n                candidate = self.line_search(func, best_solution, gradient, lb, ub)\n\n            candidate_value = func(candidate)\n            self.evaluations += 1\n            \n            if candidate_value < best_value:\n                best_solution, best_value = candidate, candidate_value\n\n        return best_solution, best_value\n\n    def estimate_gradient(self, func, solution, lb, ub, epsilon=1e-4):\n        gradient = np.zeros(self.dim)\n        for i in range(self.dim):\n            perturb = np.zeros(self.dim)\n            perturb[i] = epsilon\n            upper_sol = np.clip(solution + perturb, lb, ub)\n            lower_sol = np.clip(solution - perturb, lb, ub)\n            gradient[i] = (func(upper_sol) - func(lower_sol)) / (2 * epsilon)\n            self.evaluations += 2\n        return gradient\n\n    def line_search(self, func, solution, gradient, lb, ub, alpha=1e-2, beta=0.9):\n        step_size = alpha * np.linalg.norm(gradient)  # Modified line\n        while step_size > 1e-5:\n            candidate = solution - step_size * gradient\n            candidate = np.clip(candidate, lb, ub)\n            if func(candidate) < func(solution):\n                return candidate\n            step_size *= beta\n            self.evaluations += 1\n        return solution", "name": "HybridOptimizer", "description": "Enhanced exploration by replacing uniform sampling with Gaussian sampling for candidate generation.", "configspace": "", "generation": 6, "fitness": 0.09789241523996066, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.098 with standard deviation 0.008. And the mean value of best solutions found was 0.155 (0. is the best).", "error": "", "parent_id": "2ab5edba-1745-48f9-8582-5d1eec37436f", "metadata": {"aucs": [0.09288285149639086, 0.09225031291173313, 0.10854408131175797], "final_y": [0.1740306507163134, 0.16231151876413819, 0.12931115603445964]}, "mutation_prompt": null}
{"id": "dffa6ffb-f2e2-4a5d-bb00-631bdc46ea8d", "solution": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        best_solution = np.random.uniform(lb, ub, self.dim)\n        best_value = func(best_solution)\n        self.evaluations += 1\n        \n        while self.evaluations < self.budget:\n            if np.random.rand() < 0.5:\n                # Gradient-free exploration\n                candidate = np.random.normal(best_solution, (ub - lb) / 10, self.dim)\n                candidate = np.clip(candidate, lb, ub)\n            else:\n                # Gradient-based exploitation\n                gradient = self.estimate_gradient(func, best_solution, lb, ub)\n                candidate = self.line_search(func, best_solution, gradient, lb, ub)\n\n            candidate_value = func(candidate)\n            self.evaluations += 1\n            \n            if candidate_value < best_value:\n                best_solution, best_value = candidate, candidate_value\n\n        return best_solution, best_value\n\n    def estimate_gradient(self, func, solution, lb, ub, epsilon=1e-4):\n        gradient = np.zeros(self.dim)\n        for i in range(self.dim):\n            perturb = np.zeros(self.dim)\n            perturb[i] = epsilon\n            upper_sol = np.clip(solution + perturb, lb, ub)\n            lower_sol = np.clip(solution - perturb, lb, ub)\n            gradient[i] = (func(upper_sol) - func(lower_sol)) / (2 * epsilon)\n            self.evaluations += 2\n        return gradient\n\n    def line_search(self, func, solution, gradient, lb, ub, alpha=1e-2, beta=0.95):  # Changed here (beta adjusted)\n        step_size = alpha * np.linalg.norm(gradient)  # Modified line\n        while step_size > 1e-5:\n            candidate = solution - step_size * gradient\n            candidate = np.clip(candidate, lb, ub)\n            if func(candidate) < func(solution):\n                return candidate\n            step_size *= beta\n            self.evaluations += 1\n        return solution", "name": "HybridOptimizer", "description": "Improved exploitation by adjusting the decay factor in line search to prevent premature convergence.", "configspace": "", "generation": 7, "fitness": 0.09789241523996066, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.098 with standard deviation 0.008. And the mean value of best solutions found was 0.155 (0. is the best).", "error": "", "parent_id": "a893eddb-d7a7-475a-a601-eb0580b0486a", "metadata": {"aucs": [0.09288285149639086, 0.09225031291173313, 0.10854408131175797], "final_y": [0.1740306507163134, 0.16231151876413819, 0.12931115603445964]}, "mutation_prompt": null}
{"id": "e06a1314-6425-46e9-b655-5f557159860d", "solution": "import numpy as np\n\nclass AdaptivePopulationGradientOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = max(2, self.budget // 50)\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.apply_along_axis(func, 1, population)\n        self.evaluations += population_size\n        \n        while self.evaluations < self.budget:\n            # Sort population by fitness\n            sorted_indices = np.argsort(fitness)\n            population = population[sorted_indices]\n            fitness = fitness[sorted_indices]\n            \n            # Adaptive population reduction\n            if len(population) > max(2, self.budget // 200):\n                population = population[:len(population) // 2]\n                fitness = fitness[:len(fitness) // 2]\n\n            # Gradient sampling and generation of new candidates\n            new_population = []\n            for individual in population:\n                gradient = self.estimate_gradient(func, individual, lb, ub)\n                candidate = individual - 0.01 * gradient\n                candidate = np.clip(candidate, lb, ub)\n                new_population.append(candidate)\n            new_fitness = np.apply_along_axis(func, 1, np.array(new_population))\n            self.evaluations += len(new_population)\n\n            # Merge and select best\n            population = np.vstack((population, new_population))\n            fitness = np.hstack((fitness, new_fitness))\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]\n\n    def estimate_gradient(self, func, solution, lb, ub, epsilon=1e-4):\n        gradient = np.zeros(self.dim)\n        for i in range(self.dim):\n            perturb = np.zeros(self.dim)\n            perturb[i] = epsilon\n            upper_sol = np.clip(solution + perturb, lb, ub)\n            lower_sol = np.clip(solution - perturb, lb, ub)\n            gradient[i] = (func(upper_sol) - func(lower_sol)) / (2 * epsilon)\n            self.evaluations += 2\n        return gradient", "name": "AdaptivePopulationGradientOptimizer", "description": "Adaptive Population-based Gradient Sampling combines adaptive population sizing with gradient sampling to balance exploration and exploitation.", "configspace": "", "generation": 8, "fitness": 0.09365520755131489, "feedback": "The algorithm AdaptivePopulationGradientOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.094 with standard deviation 0.006. And the mean value of best solutions found was 0.180 (0. is the best).", "error": "", "parent_id": "a893eddb-d7a7-475a-a601-eb0580b0486a", "metadata": {"aucs": [0.0854359922762471, 0.09970286163144082, 0.09582676874625673], "final_y": [0.20834277329594852, 0.16009438367169782, 0.17193706811265286]}, "mutation_prompt": null}
{"id": "1afd699c-7ec0-4019-8ed1-2ee328c19118", "solution": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        best_solution = np.random.uniform(lb, ub, self.dim)\n        best_value = func(best_solution)\n        self.evaluations += 1\n        \n        while self.evaluations < self.budget:\n            if np.random.rand() < 0.5:\n                # Gradient-free exploration\n                sigma = np.max((ub - lb) / 20, 1e-5)  # Adjusted perturbation scale\n                candidate = np.random.normal(best_solution, sigma, self.dim)\n                candidate = np.clip(candidate, lb, ub)\n            else:\n                # Gradient-based exploitation\n                gradient = self.estimate_gradient(func, best_solution, lb, ub)\n                candidate = self.dynamic_line_search(func, best_solution, gradient, lb, ub)\n\n            candidate_value = func(candidate)\n            self.evaluations += 1\n            \n            if candidate_value < best_value:\n                best_solution, best_value = candidate, candidate_value\n\n        return best_solution, best_value\n\n    def estimate_gradient(self, func, solution, lb, ub, epsilon=1e-4):\n        gradient = np.zeros(self.dim)\n        for i in range(self.dim):\n            perturb = np.zeros(self.dim)\n            perturb[i] = epsilon\n            upper_sol = np.clip(solution + perturb, lb, ub)\n            lower_sol = np.clip(solution - perturb, lb, ub)\n            gradient[i] = (func(upper_sol) - func(lower_sol)) / (2 * epsilon)\n            self.evaluations += 2\n        return gradient\n\n    def dynamic_line_search(self, func, solution, gradient, lb, ub, alpha=1e-2, beta=0.9):\n        step_size = alpha * np.linalg.norm(gradient)\n        while step_size > 1e-5:\n            candidate = solution - step_size * gradient\n            candidate = np.clip(candidate, lb, ub)\n            if func(candidate) < func(solution):\n                return candidate\n            step_size *= beta * np.random.uniform(0.8, 1.2)  # Dynamic scaling\n            self.evaluations += 1\n        return solution", "name": "HybridOptimizer", "description": "Hybrid exploration and exploitation using adaptive Gaussian perturbation and dynamic step size scaling.", "configspace": "", "generation": 9, "fitness": -Infinity, "feedback": "An exception occurred: TypeError(\"'float' object cannot be interpreted as an integer\").", "error": "TypeError(\"'float' object cannot be interpreted as an integer\")", "parent_id": "a893eddb-d7a7-475a-a601-eb0580b0486a", "metadata": {}, "mutation_prompt": null}
{"id": "fb5234b6-6626-4c41-92a6-ce938c6c3033", "solution": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        best_solution = np.random.uniform(lb, ub, self.dim)\n        best_value = func(best_solution)\n        self.evaluations += 1\n        \n        while self.evaluations < self.budget:\n            if np.random.rand() < 0.5:\n                # Gradient-free exploration\n                candidate = np.random.normal(best_solution, (ub - lb) / 10, self.dim)\n                candidate = np.clip(candidate, lb, ub)\n            else:\n                # Gradient-based exploitation\n                gradient = self.estimate_gradient(func, best_solution, lb, ub)\n                candidate = self.line_search(func, best_solution, gradient, lb, ub)\n\n            candidate_value = func(candidate)\n            self.evaluations += 1\n            \n            if candidate_value < best_value:\n                best_solution, best_value = candidate, candidate_value\n\n        return best_solution, best_value\n\n    def estimate_gradient(self, func, solution, lb, ub, epsilon=1e-4):\n        gradient = np.zeros(self.dim)\n        for i in range(self.dim):\n            perturb = np.zeros(self.dim)\n            perturb[i] = epsilon\n            upper_sol = np.clip(solution + perturb, lb, ub)\n            lower_sol = np.clip(solution - perturb, lb, ub)\n            gradient[i] = (func(upper_sol) - func(lower_sol)) / (2 * epsilon)\n            self.evaluations += 2\n        return gradient\n\n    def line_search(self, func, solution, gradient, lb, ub, alpha=1e-2, beta=0.9):\n        step_size = alpha * np.linalg.norm(gradient) / (1 + self.evaluations / self.budget)  # Modified line\n        while step_size > 1e-5:\n            candidate = solution - step_size * gradient\n            candidate = np.clip(candidate, lb, ub)\n            if func(candidate) < func(solution):\n                return candidate\n            step_size *= beta\n            self.evaluations += 1\n        return solution", "name": "HybridOptimizer", "description": "Incorporate adaptive step size in line search for dynamic convergence.", "configspace": "", "generation": 10, "fitness": 0.09799811300789296, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.098 with standard deviation 0.007. And the mean value of best solutions found was 0.155 (0. is the best).", "error": "", "parent_id": "a893eddb-d7a7-475a-a601-eb0580b0486a", "metadata": {"aucs": [0.09288285149673647, 0.09256297617225862, 0.1085485113546838], "final_y": [0.17403065071492096, 0.16231151908344654, 0.12931115588456243]}, "mutation_prompt": null}
{"id": "f9fc96cc-7e1a-4fad-a406-28352ad95efe", "solution": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.crossover_rate = 0.7  # Added adaptive crossover rate\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        best_solution = np.random.uniform(lb, ub, self.dim)\n        best_value = func(best_solution)\n        self.evaluations += 1\n\n        while self.evaluations < self.budget:\n            if np.random.rand() < self.crossover_rate:  # Changed from 0.5 to self.crossover_rate\n                # Gradient-free exploration\n                mutation_scale = (0.5 + 0.5 * (self.evaluations / self.budget))  # Added adaptive mutation scale\n                candidate = np.random.normal(best_solution, (ub - lb) / 10 * mutation_scale, self.dim)  # Changed mutation\n                candidate = np.clip(candidate, lb, ub)\n            else:\n                # Gradient-based exploitation\n                gradient = self.estimate_gradient(func, best_solution, lb, ub)\n                candidate = self.line_search(func, best_solution, gradient, lb, ub)\n\n            candidate_value = func(candidate)\n            self.evaluations += 1\n\n            if candidate_value < best_value:\n                best_solution, best_value = candidate, candidate_value\n                self.crossover_rate = min(1.0, self.crossover_rate + 0.01)  # Adaptive increase\n            else:\n                self.crossover_rate = max(0.2, self.crossover_rate - 0.01)  # Adaptive decrease\n\n        return best_solution, best_value\n\n    def estimate_gradient(self, func, solution, lb, ub, epsilon=1e-4):\n        gradient = np.zeros(self.dim)\n        for i in range(self.dim):\n            perturb = np.zeros(self.dim)\n            perturb[i] = epsilon\n            upper_sol = np.clip(solution + perturb, lb, ub)\n            lower_sol = np.clip(solution - perturb, lb, ub)\n            gradient[i] = (func(upper_sol) - func(lower_sol)) / (2 * epsilon)\n            self.evaluations += 2\n        return gradient\n\n    def line_search(self, func, solution, gradient, lb, ub, alpha=1e-2, beta=0.9):\n        step_size = alpha * np.linalg.norm(gradient) / (1 + self.evaluations / self.budget)\n        while step_size > 1e-5:\n            candidate = solution - step_size * gradient\n            candidate = np.clip(candidate, lb, ub)\n            if func(candidate) < func(solution):\n                return candidate\n            step_size *= beta\n            self.evaluations += 1\n        return solution", "name": "HybridOptimizer", "description": "Integrate adaptive mutation and self-adaptive crossover rate for enhanced exploration and exploitation.", "configspace": "", "generation": 11, "fitness": 0.10263878636913908, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.103 with standard deviation 0.009. And the mean value of best solutions found was 0.147 (0. is the best).", "error": "", "parent_id": "fb5234b6-6626-4c41-92a6-ce938c6c3033", "metadata": {"aucs": [0.0918701968838952, 0.10287527695223053, 0.11317088527129149], "final_y": [0.18008782428043635, 0.1413968553333771, 0.12048403864048662]}, "mutation_prompt": null}
{"id": "8d5cf423-d343-4bd4-8cd7-d84981407ad1", "solution": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.crossover_rate = 0.7\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        best_solution = np.random.uniform(lb, ub, self.dim)\n        best_value = func(best_solution)\n        self.evaluations += 1\n\n        while self.evaluations < self.budget:\n            if np.random.rand() < self.crossover_rate:\n                mutation_scale = (0.5 + 0.5 * (self.evaluations / self.budget))\n                perturbation = np.random.uniform(0.05, 0.15)  # Modified perturbation size\n                candidate = np.random.normal(best_solution, (ub - lb) / 10 * mutation_scale * perturbation, self.dim)  # Updated scale\n                candidate = np.clip(candidate, lb, ub)\n            else:\n                gradient = self.estimate_gradient(func, best_solution, lb, ub)\n                candidate = self.line_search(func, best_solution, gradient, lb, ub)\n\n            candidate_value = func(candidate)\n            self.evaluations += 1\n\n            if candidate_value < best_value:\n                best_solution, best_value = candidate, candidate_value\n                self.crossover_rate = min(1.0, self.crossover_rate + 0.01)\n            else:\n                self.crossover_rate = max(0.2, self.crossover_rate - 0.01)\n\n        return best_solution, best_value\n\n    def estimate_gradient(self, func, solution, lb, ub, epsilon=1e-4):\n        gradient = np.zeros(self.dim)\n        for i in range(self.dim):\n            perturb = np.zeros(self.dim)\n            perturb[i] = epsilon * np.random.uniform(0.9, 1.1)  # Refined perturbation\n            upper_sol = np.clip(solution + perturb, lb, ub)\n            lower_sol = np.clip(solution - perturb, lb, ub)\n            gradient[i] = (func(upper_sol) - func(lower_sol)) / (2 * epsilon)\n            self.evaluations += 2\n        return gradient\n\n    def line_search(self, func, solution, gradient, lb, ub, alpha=1e-2, beta=0.9):\n        step_size = alpha * np.linalg.norm(gradient) / (1 + self.evaluations / self.budget)\n        while step_size > 1e-5:\n            candidate = solution - step_size * gradient\n            candidate = np.clip(candidate, lb, ub)\n            if func(candidate) < func(solution):\n                return candidate\n            step_size *= beta\n            self.evaluations += 1\n        return solution", "name": "HybridOptimizer", "description": "Introduce adaptive perturbation size and a refined gradient estimation for improved exploration-exploitation balance.", "configspace": "", "generation": 12, "fitness": 0.10038196011320792, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.100 with standard deviation 0.009. And the mean value of best solutions found was 0.140 (0. is the best).", "error": "", "parent_id": "f9fc96cc-7e1a-4fad-a406-28352ad95efe", "metadata": {"aucs": [0.09083614393852457, 0.09826245265865663, 0.11204728374244255], "final_y": [0.17381520647166449, 0.1288024533848423, 0.11605149915536528]}, "mutation_prompt": null}
{"id": "774681c4-a0a7-44fe-a465-03b0a07c108f", "solution": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.crossover_rate = 0.7  # Added adaptive crossover rate\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        best_solution = np.random.uniform(lb, ub, self.dim)\n        best_value = func(best_solution)\n        self.evaluations += 1\n        dynamic_step_size = (ub - lb) / 5  # Introduced dynamic step-size\n\n        while self.evaluations < self.budget:\n            if np.random.rand() < self.crossover_rate:  # Changed from 0.5 to self.crossover_rate\n                # Gradient-free exploration\n                mutation_scale = (0.5 + 0.5 * (self.evaluations / self.budget))  # Added adaptive mutation scale\n                candidate = np.random.normal(best_solution, dynamic_step_size * mutation_scale, self.dim)  # Changed mutation\n                candidate = np.clip(candidate, lb, ub)\n            else:\n                # Gradient-based exploitation\n                gradient = self.estimate_gradient(func, best_solution, lb, ub)\n                candidate = self.line_search(func, best_solution, gradient, lb, ub)\n\n            candidate_value = func(candidate)\n            self.evaluations += 1\n\n            if candidate_value < best_value:\n                best_solution, best_value = candidate, candidate_value\n                self.crossover_rate = min(1.0, self.crossover_rate + 0.01)  # Adaptive increase\n            else:\n                self.crossover_rate = max(0.2, self.crossover_rate - 0.01)  # Adaptive decrease\n\n            if np.random.rand() < 0.05:  # Introduced stochastic termination\n                break\n\n        return best_solution, best_value\n\n    def estimate_gradient(self, func, solution, lb, ub, epsilon=1e-4):\n        gradient = np.zeros(self.dim)\n        for i in range(self.dim):\n            perturb = np.zeros(self.dim)\n            perturb[i] = epsilon\n            upper_sol = np.clip(solution + perturb, lb, ub)\n            lower_sol = np.clip(solution - perturb, lb, ub)\n            gradient[i] = (func(upper_sol) - func(lower_sol)) / (2 * epsilon)\n            self.evaluations += 2\n        return gradient\n\n    def line_search(self, func, solution, gradient, lb, ub, alpha=1e-2, beta=0.9):\n        step_size = alpha * np.linalg.norm(gradient) / (1 + self.evaluations / self.budget)\n        while step_size > 1e-5:\n            candidate = solution - step_size * gradient\n            candidate = np.clip(candidate, lb, ub)\n            if func(candidate) < func(solution):\n                return candidate\n            step_size *= beta\n            self.evaluations += 1\n        return solution", "name": "HybridOptimizer", "description": "Introduce dynamic step-size adaptation and stochastic termination criterion for enhanced convergence control and robustness.", "configspace": "", "generation": 13, "fitness": 0.08261492368437699, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.083 with standard deviation 0.008. And the mean value of best solutions found was 0.220 (0. is the best).", "error": "", "parent_id": "f9fc96cc-7e1a-4fad-a406-28352ad95efe", "metadata": {"aucs": [0.07629287933500895, 0.07727279288192535, 0.09427909883619667], "final_y": [0.2452179640460771, 0.23841913681187943, 0.1760983889928246]}, "mutation_prompt": null}
{"id": "583c29fe-8cd6-451a-99d2-91dfa019a2e5", "solution": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.crossover_rate = 0.7  # Added adaptive crossover rate\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        best_solution = np.random.uniform(lb, ub, self.dim)\n        best_value = func(best_solution)\n        self.evaluations += 1\n\n        while self.evaluations < self.budget:\n            if np.random.rand() < self.crossover_rate:  # Changed from 0.5 to self.crossover_rate\n                # Gradient-free exploration\n                diversity = np.std(best_solution) # Dynamic mutation scale based on diversity\n                mutation_scale = (0.5 + 0.5 * (self.evaluations / self.budget) * diversity)  # Modified adaptive mutation scale\n                candidate = np.random.normal(best_solution, (ub - lb) / 10 * mutation_scale, self.dim)  # Changed mutation\n                candidate = np.clip(candidate, lb, ub)\n            else:\n                # Gradient-based exploitation\n                gradient = self.estimate_gradient(func, best_solution, lb, ub)\n                candidate = self.line_search(func, best_solution, gradient, lb, ub)\n\n            candidate_value = func(candidate)\n            self.evaluations += 1\n\n            if candidate_value < best_value:\n                best_solution, best_value = candidate, candidate_value\n                self.crossover_rate = min(1.0, self.crossover_rate + 0.01)  # Adaptive increase\n            else:\n                self.crossover_rate = max(0.2, self.crossover_rate - 0.01)  # Adaptive decrease\n\n        return best_solution, best_value\n\n    def estimate_gradient(self, func, solution, lb, ub, epsilon=1e-4):\n        gradient = np.zeros(self.dim)\n        for i in range(self.dim):\n            perturb = np.zeros(self.dim)\n            perturb[i] = epsilon\n            upper_sol = np.clip(solution + perturb, lb, ub)\n            lower_sol = np.clip(solution - perturb, lb, ub)\n            gradient[i] = (func(upper_sol) - func(lower_sol)) / (2 * epsilon)\n            self.evaluations += 2\n        return gradient\n\n    def line_search(self, func, solution, gradient, lb, ub, alpha=1e-2, beta=0.9):\n        step_size = alpha * np.linalg.norm(gradient) / (1 + self.evaluations / self.budget)\n        while step_size > 1e-5:\n            candidate = solution - step_size * gradient\n            candidate = np.clip(candidate, lb, ub)\n            if func(candidate) < func(solution):\n                return candidate\n            step_size *= beta\n            self.evaluations += 1\n        return solution", "name": "HybridOptimizer", "description": "Introduce a dynamic mutation scale based on the diversity of candidate solutions to enhance exploration.", "configspace": "", "generation": 14, "fitness": 0.09282428825534772, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.093 with standard deviation 0.004. And the mean value of best solutions found was 0.179 (0. is the best).", "error": "", "parent_id": "f9fc96cc-7e1a-4fad-a406-28352ad95efe", "metadata": {"aucs": [0.08850309700455306, 0.0923437446664307, 0.09762602309505941], "final_y": [0.19579923527694898, 0.1763629270508842, 0.1662061417708951]}, "mutation_prompt": null}
{"id": "da8b9844-5b18-4bf6-8d98-049193d9117e", "solution": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.crossover_rate = 0.8  # Adjusted initial adaptive crossover rate\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        best_solution = np.random.uniform(lb, ub, self.dim)\n        best_value = func(best_solution)\n        self.evaluations += 1\n\n        while self.evaluations < self.budget:\n            if np.random.rand() < self.crossover_rate:\n                # Gradient-free exploration\n                mutation_scale = (0.7 + 0.3 * (self.evaluations / self.budget))  # Adjusted adaptive mutation scale\n                candidate = np.random.normal(best_solution, (ub - lb) / 10 * mutation_scale, self.dim)\n                candidate = np.clip(candidate, lb, ub)\n            else:\n                # Gradient-based exploitation\n                gradient = self.estimate_gradient(func, best_solution, lb, ub)\n                candidate = self.line_search(func, best_solution, gradient, lb, ub)\n\n            candidate_value = func(candidate)\n            self.evaluations += 1\n\n            if candidate_value < best_value:\n                best_solution, best_value = candidate, candidate_value\n                self.crossover_rate = min(1.0, self.crossover_rate + 0.015)  # More dynamic increase\n            else:\n                self.crossover_rate = max(0.2, self.crossover_rate - 0.01)\n\n        return best_solution, best_value\n\n    def estimate_gradient(self, func, solution, lb, ub, epsilon=1e-4):\n        gradient = np.zeros(self.dim)\n        for i in range(self.dim):\n            perturb = np.zeros(self.dim)\n            perturb[i] = epsilon\n            upper_sol = np.clip(solution + perturb, lb, ub)\n            lower_sol = np.clip(solution - perturb, lb, ub)\n            gradient[i] = (func(upper_sol) - func(lower_sol)) / (2 * epsilon)\n            self.evaluations += 2\n        return gradient\n\n    def line_search(self, func, solution, gradient, lb, ub, alpha=1e-2, beta=0.9):\n        step_size = alpha * np.linalg.norm(gradient) / (1 + self.evaluations / self.budget)\n        while step_size > 1e-5:\n            candidate = solution - step_size * gradient\n            candidate = np.clip(candidate, lb, ub)\n            if func(candidate) < func(solution):\n                return candidate\n            step_size *= beta\n            self.evaluations += 1\n        return solution", "name": "HybridOptimizer", "description": "Enhance dynamic parameter tuning by adjusting mutation scale and crossover rate more effectively.", "configspace": "", "generation": 15, "fitness": 0.10379841145612245, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.104 with standard deviation 0.007. And the mean value of best solutions found was 0.146 (0. is the best).", "error": "", "parent_id": "f9fc96cc-7e1a-4fad-a406-28352ad95efe", "metadata": {"aucs": [0.0959125137552751, 0.10338941893766407, 0.11209330167542819], "final_y": [0.1682329899121292, 0.14527763800368532, 0.1254797214984461]}, "mutation_prompt": null}
{"id": "3c732b47-8b28-4b1c-8722-37377af3b741", "solution": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.crossover_rate = 0.8  # Adjusted initial adaptive crossover rate\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        best_solution = np.random.uniform(lb, ub, self.dim)\n        best_value = func(best_solution)\n        self.evaluations += 1\n\n        while self.evaluations < self.budget:\n            if np.random.rand() < self.crossover_rate:\n                # Gradient-free exploration\n                mutation_scale = (0.7 + 0.3 * (self.evaluations / self.budget))  # Adjusted adaptive mutation scale\n                candidate = np.random.normal(best_solution, (ub - lb) / 10 * mutation_scale, self.dim)\n                candidate = np.clip(candidate, lb, ub)\n            else:\n                # Gradient-based exploitation\n                gradient = self.estimate_gradient(func, best_solution, lb, ub)\n                candidate = self.line_search(func, best_solution, gradient, lb, ub)\n\n            candidate_value = func(candidate)\n            self.evaluations += 1\n\n            if candidate_value < best_value:\n                best_solution, best_value = candidate, candidate_value\n                self.crossover_rate = min(1.0, self.crossover_rate + 0.02)  # More dynamic increase\n            else:\n                self.crossover_rate = max(0.2, self.crossover_rate - 0.01)\n\n        return best_solution, best_value\n\n    def estimate_gradient(self, func, solution, lb, ub, epsilon=1e-4):\n        gradient = np.zeros(self.dim)\n        for i in range(self.dim):\n            perturb = np.zeros(self.dim)\n            perturb[i] = epsilon\n            upper_sol = np.clip(solution + perturb, lb, ub)\n            lower_sol = np.clip(solution - perturb, lb, ub)\n            gradient[i] = (func(upper_sol) - func(lower_sol)) / (2 * epsilon)\n            self.evaluations += 2\n        return gradient\n\n    def line_search(self, func, solution, gradient, lb, ub, alpha=1e-2, beta=0.9):\n        step_size = alpha * np.linalg.norm(gradient) / (1 + self.evaluations / self.budget)\n        while step_size > 1e-5:\n            candidate = solution - step_size * gradient\n            candidate = np.clip(candidate, lb, ub)\n            if func(candidate) < func(solution):\n                return candidate\n            step_size *= beta\n            self.evaluations += 1\n        return solution", "name": "HybridOptimizer", "description": "Optimize crossover rate adjustment for enhanced exploration and exploitation balance.", "configspace": "", "generation": 16, "fitness": 0.10406559792461552, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.104 with standard deviation 0.006. And the mean value of best solutions found was 0.144 (0. is the best).", "error": "", "parent_id": "da8b9844-5b18-4bf6-8d98-049193d9117e", "metadata": {"aucs": [0.09854074049091122, 0.10173605079720394, 0.11192000248573142], "final_y": [0.15633128543276975, 0.1501046513710026, 0.12598629601784295]}, "mutation_prompt": null}
{"id": "d860669d-f504-4b4a-a5fa-259ffe34d798", "solution": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.crossover_rate = 0.8  # Adjusted initial adaptive crossover rate\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        best_solution = np.random.uniform(lb, ub, self.dim)\n        best_value = func(best_solution)\n        self.evaluations += 1\n\n        while self.evaluations < self.budget:\n            if np.random.rand() < self.crossover_rate:\n                # Gradient-free exploration\n                mutation_scale = (0.9 + 0.1 * np.sin(np.pi * self.evaluations / self.budget))  # Adaptive non-linear mutation scale\n                candidate = np.random.normal(best_solution, (ub - lb) / 10 * mutation_scale, self.dim)\n                candidate = np.clip(candidate, lb, ub)\n            else:\n                # Gradient-based exploitation with perturbation\n                perturbation = np.random.normal(0, (ub - lb) / 100, self.dim)  # Small random perturbation\n                gradient = self.estimate_gradient(func, best_solution + perturbation, lb, ub)\n                candidate = self.line_search(func, best_solution, gradient, lb, ub)\n\n            candidate_value = func(candidate)\n            self.evaluations += 1\n\n            if candidate_value < best_value:\n                best_solution, best_value = candidate, candidate_value\n                self.crossover_rate = min(1.0, self.crossover_rate + 0.02)  # More dynamic increase\n            else:\n                self.crossover_rate = max(0.2, self.crossover_rate - 0.01)\n\n        return best_solution, best_value\n\n    def estimate_gradient(self, func, solution, lb, ub, epsilon=1e-4):\n        gradient = np.zeros(self.dim)\n        for i in range(self.dim):\n            perturb = np.zeros(self.dim)\n            perturb[i] = epsilon\n            upper_sol = np.clip(solution + perturb, lb, ub)\n            lower_sol = np.clip(solution - perturb, lb, ub)\n            gradient[i] = (func(upper_sol) - func(lower_sol)) / (2 * epsilon)\n            self.evaluations += 2\n        return gradient\n\n    def line_search(self, func, solution, gradient, lb, ub, alpha=1e-2, beta=0.9):\n        step_size = alpha * np.linalg.norm(gradient) / (1 + self.evaluations / self.budget)\n        while step_size > 1e-5:\n            candidate = solution - step_size * gradient\n            candidate = np.clip(candidate, lb, ub)\n            if func(candidate) < func(solution):\n                return candidate\n            step_size *= beta\n            self.evaluations += 1\n        return solution", "name": "HybridOptimizer", "description": "Enhance hybrid optimization by using adaptive mutation scale and introducing perturbation for gradient-based exploitation.", "configspace": "", "generation": 17, "fitness": 0.10155936645077841, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.102 with standard deviation 0.007. And the mean value of best solutions found was 0.150 (0. is the best).", "error": "", "parent_id": "3c732b47-8b28-4b1c-8722-37377af3b741", "metadata": {"aucs": [0.09629585387207495, 0.09726653917616113, 0.11111570630409917], "final_y": [0.16619673985529648, 0.16110976137060828, 0.12357891912629204]}, "mutation_prompt": null}
{"id": "3159ec6c-963f-41af-92de-4e529c5b1f90", "solution": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.crossover_rate = 0.8  # Adjusted initial adaptive crossover rate\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        best_solution = np.random.uniform(lb, ub, self.dim)\n        best_value = func(best_solution)\n        self.evaluations += 1\n\n        while self.evaluations < self.budget:\n            if np.random.rand() < self.crossover_rate:\n                # Dynamic mutation scale adjustment\n                mutation_scale = np.random.rand() * (0.3 * (1 - self.evaluations / self.budget))  # Adjusted adaptive mutation scale\n                candidate = np.random.normal(best_solution, (ub - lb) / 10 * mutation_scale, self.dim)\n                candidate = np.clip(candidate, lb, ub)\n            else:\n                # Gradient-based exploitation\n                gradient = self.estimate_gradient(func, best_solution, lb, ub)\n                candidate = self.line_search(func, best_solution, gradient, lb, ub)\n\n            candidate_value = func(candidate)\n            self.evaluations += 1\n\n            if candidate_value < best_value:\n                best_solution, best_value = candidate, candidate_value\n                self.crossover_rate = min(1.0, self.crossover_rate + 0.02)  # More dynamic increase\n            else:\n                self.crossover_rate = max(0.2, self.crossover_rate - 0.01)\n\n        return best_solution, best_value\n\n    def estimate_gradient(self, func, solution, lb, ub, epsilon=1e-4):\n        gradient = np.zeros(self.dim)\n        for i in range(self.dim):\n            perturb = np.zeros(self.dim)\n            perturb[i] = epsilon\n            upper_sol = np.clip(solution + perturb, lb, ub)\n            lower_sol = np.clip(solution - perturb, lb, ub)\n            gradient[i] = (func(upper_sol) - func(lower_sol)) / (2 * epsilon)\n            self.evaluations += 2\n        return gradient\n\n    def line_search(self, func, solution, gradient, lb, ub, alpha=1e-2, beta=0.9):\n        step_size = alpha * np.linalg.norm(gradient) / (1 + self.evaluations / self.budget)\n        while step_size > 1e-5:\n            candidate = solution - step_size * gradient\n            candidate = np.clip(candidate, lb, ub)\n            if func(candidate) < func(solution):\n                return candidate\n            step_size *= beta\n            self.evaluations += 1\n        return solution", "name": "HybridOptimizer", "description": "Introduce dynamic mutation scale adjustment for improved exploration-exploitation trade-off.", "configspace": "", "generation": 18, "fitness": 0.10684566665804152, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.107 with standard deviation 0.006. And the mean value of best solutions found was 0.133 (0. is the best).", "error": "", "parent_id": "3c732b47-8b28-4b1c-8722-37377af3b741", "metadata": {"aucs": [0.10010793862326028, 0.10535082044077448, 0.11507824091008978], "final_y": [0.15031589373384735, 0.1331850202662177, 0.11673338036917902]}, "mutation_prompt": null}
{"id": "192bf623-9e4e-4a03-8722-782caf11b4b9", "solution": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.crossover_rate = 0.8\n        self.learning_rate = 0.1  # Initial learning rate\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        best_solution = np.random.uniform(lb, ub, self.dim)\n        best_value = func(best_solution)\n        self.evaluations += 1\n        previous_value = best_value  # Track previous iteration's value\n\n        while self.evaluations < self.budget:\n            if np.random.rand() < self.crossover_rate:\n                mutation_scale = np.random.rand() * (0.3 * (1 - self.evaluations / self.budget))\n                candidate = np.random.normal(best_solution, (ub - lb) / 10 * mutation_scale, self.dim)\n                candidate = np.clip(candidate, lb, ub)\n            else:\n                gradient = self.estimate_gradient(func, best_solution, lb, ub)\n                candidate = self.line_search(func, best_solution, gradient, lb, ub)\n\n            candidate_value = func(candidate)\n            self.evaluations += 1\n\n            if candidate_value < best_value:\n                best_solution, best_value = candidate, candidate_value\n                self.crossover_rate = min(1.0, self.crossover_rate + 0.02)\n                # Adaptive learning rate adjustment\n                if candidate_value < previous_value:\n                    self.learning_rate *= 1.05\n                previous_value = candidate_value\n            else:\n                self.crossover_rate = max(0.2, self.crossover_rate - 0.01)\n                self.learning_rate *= 0.95\n\n        return best_solution, best_value\n\n    def estimate_gradient(self, func, solution, lb, ub, epsilon=1e-4):\n        gradient = np.zeros(self.dim)\n        for i in range(self.dim):\n            perturb = np.zeros(self.dim)\n            perturb[i] = epsilon\n            upper_sol = np.clip(solution + perturb, lb, ub)\n            lower_sol = np.clip(solution - perturb, lb, ub)\n            gradient[i] = (func(upper_sol) - func(lower_sol)) / (2 * epsilon)\n            self.evaluations += 2\n        return gradient\n\n    def line_search(self, func, solution, gradient, lb, ub, alpha=1e-2, beta=0.9):\n        step_size = self.learning_rate * np.linalg.norm(gradient) / (1 + self.evaluations / self.budget)\n        while step_size > 1e-5:\n            candidate = solution - step_size * gradient\n            candidate = np.clip(candidate, lb, ub)\n            if func(candidate) < func(solution):\n                return candidate\n            step_size *= beta\n            self.evaluations += 1\n        return solution", "name": "HybridOptimizer", "description": "Introduce adaptive learning rate adjustment using feedback from previous iterations to enhance convergence speed and precision.", "configspace": "", "generation": 19, "fitness": 0.1085010649250893, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.109 with standard deviation 0.006. And the mean value of best solutions found was 0.130 (0. is the best).", "error": "", "parent_id": "3159ec6c-963f-41af-92de-4e529c5b1f90", "metadata": {"aucs": [0.10139582597540919, 0.10701010267993083, 0.11709726611992788], "final_y": [0.14684622279331494, 0.12901414352171692, 0.1135094763533473]}, "mutation_prompt": null}
{"id": "7e78fae8-082d-49da-91d5-78b8538d4aa9", "solution": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.crossover_rate = 0.8\n        self.learning_rate = 0.1  # Initial learning rate\n        self.recent_values = []\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        best_solution = np.random.uniform(lb, ub, self.dim)\n        best_value = func(best_solution)\n        self.evaluations += 1\n        previous_value = best_value  # Track previous iteration's value\n        self.recent_values.append(best_value)\n\n        while self.evaluations < self.budget:\n            if np.random.rand() < self.crossover_rate:\n                mutation_scale = np.std(self.recent_values[-10:]) if len(self.recent_values) > 10 else np.random.rand() * (0.3 * (1 - self.evaluations / self.budget))\n                candidate = np.random.normal(best_solution, (ub - lb) / 10 * mutation_scale, self.dim)\n                candidate = np.clip(candidate, lb, ub)\n            else:\n                gradient = self.estimate_gradient(func, best_solution, lb, ub)\n                candidate = self.line_search(func, best_solution, gradient, lb, ub)\n\n            candidate_value = func(candidate)\n            self.evaluations += 1\n            self.recent_values.append(candidate_value)\n\n            if candidate_value < best_value:\n                best_solution, best_value = candidate, candidate_value\n                self.crossover_rate = min(1.0, self.crossover_rate + 0.02)\n                # Adaptive learning rate adjustment\n                if candidate_value < previous_value:\n                    self.learning_rate *= 1.05\n                previous_value = candidate_value\n            else:\n                self.crossover_rate = max(0.2, self.crossover_rate - 0.01)\n                self.learning_rate *= 0.95\n\n        return best_solution, best_value\n\n    def estimate_gradient(self, func, solution, lb, ub, epsilon=1e-4):\n        gradient = np.zeros(self.dim)\n        for i in range(self.dim):\n            perturb = np.zeros(self.dim)\n            perturb[i] = epsilon\n            upper_sol = np.clip(solution + perturb, lb, ub)\n            lower_sol = np.clip(solution - perturb, lb, ub)\n            gradient[i] = (func(upper_sol) - func(lower_sol)) / (2 * epsilon)\n            self.evaluations += 2\n        return gradient\n\n    def line_search(self, func, solution, gradient, lb, ub, alpha=1e-2, beta=0.9):\n        step_size = self.learning_rate * np.linalg.norm(gradient) / (1 + self.evaluations / self.budget)\n        while step_size > 1e-5:\n            candidate = solution - step_size * gradient\n            candidate = np.clip(candidate, lb, ub)\n            if func(candidate) < func(solution):\n                return candidate\n            step_size *= beta\n            self.evaluations += 1\n        return solution", "name": "HybridOptimizer", "description": "Introduce a dynamic mutation scale based on the standard deviation of recent solution values to improve exploration and convergence.", "configspace": "", "generation": 20, "fitness": 0.0799936838493357, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.080 with standard deviation 0.012. And the mean value of best solutions found was 0.236 (0. is the best).", "error": "", "parent_id": "192bf623-9e4e-4a03-8722-782caf11b4b9", "metadata": {"aucs": [0.0786288506931766, 0.06587705634449714, 0.09547514451033334], "final_y": [0.23625652982539813, 0.29878473803073236, 0.17312293302399573]}, "mutation_prompt": null}
{"id": "194f0d8b-f1ae-4922-b100-a25ba28c3d04", "solution": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.crossover_rate = 0.8\n        self.learning_rate = 0.1  # Initial learning rate\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        best_solution = np.random.uniform(lb, ub, self.dim)\n        best_value = func(best_solution)\n        self.evaluations += 1\n        previous_value = best_value  # Track previous iteration's value\n\n        while self.evaluations < self.budget:\n            if np.random.rand() < self.crossover_rate:\n                mutation_scale = np.random.rand() * (0.5 * (1 - self.evaluations / self.budget))  # Adjusted line\n                candidate = np.random.normal(best_solution, (ub - lb) / 8 * mutation_scale, self.dim)  # Adjusted line\n                candidate = np.clip(candidate, lb, ub)\n            else:\n                gradient = self.estimate_gradient(func, best_solution, lb, ub)\n                candidate = self.line_search(func, best_solution, gradient, lb, ub)\n\n            candidate_value = func(candidate)\n            self.evaluations += 1\n\n            if candidate_value < best_value:\n                best_solution, best_value = candidate, candidate_value\n                self.crossover_rate = min(1.0, self.crossover_rate + 0.02)\n                # Adaptive learning rate adjustment\n                if candidate_value < previous_value:\n                    self.learning_rate *= 1.1  # Adjusted line\n                previous_value = candidate_value\n            else:\n                self.crossover_rate = max(0.2, self.crossover_rate - 0.01)\n                self.learning_rate *= 0.9  # Adjusted line\n\n        return best_solution, best_value\n\n    def estimate_gradient(self, func, solution, lb, ub, epsilon=1e-4):\n        gradient = np.zeros(self.dim)\n        for i in range(self.dim):\n            perturb = np.zeros(self.dim)\n            perturb[i] = epsilon\n            upper_sol = np.clip(solution + perturb, lb, ub)\n            lower_sol = np.clip(solution - perturb, lb, ub)\n            gradient[i] = (func(upper_sol) - func(lower_sol)) / (2 * epsilon)\n            self.evaluations += 2\n        return gradient\n\n    def line_search(self, func, solution, gradient, lb, ub, alpha=1e-2, beta=0.9):\n        step_size = self.learning_rate * np.linalg.norm(gradient) / (1 + self.evaluations / self.budget)\n        while step_size > 1e-5:\n            candidate = solution - step_size * gradient\n            candidate = np.clip(candidate, lb, ub)\n            if func(candidate) < func(solution):\n                return candidate\n            step_size *= beta\n            self.evaluations += 1\n        return solution", "name": "HybridOptimizer", "description": "Enhance dynamic learning rate and mutation strategy for better exploration-exploitation balance.", "configspace": "", "generation": 21, "fitness": 0.1043064228466319, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.104 with standard deviation 0.006. And the mean value of best solutions found was 0.140 (0. is the best).", "error": "", "parent_id": "192bf623-9e4e-4a03-8722-782caf11b4b9", "metadata": {"aucs": [0.09693708745888197, 0.10441708603065858, 0.11156509505035517], "final_y": [0.16185353561261184, 0.13715212385718667, 0.12135599390921359]}, "mutation_prompt": null}
{"id": "26781c6e-6f2c-47c5-9c0e-2c1a306a18fe", "solution": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.crossover_rate = 0.8\n        self.learning_rate = 0.1  # Initial learning rate\n        self.successful_step = 0\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        best_solution = np.random.uniform(lb, ub, self.dim)\n        best_value = func(best_solution)\n        self.evaluations += 1\n        previous_value = best_value  # Track previous iteration's value\n\n        while self.evaluations < self.budget:\n            if np.random.rand() < self.crossover_rate:\n                mutation_scale = np.random.rand() * (0.3 * (1 - self.evaluations / self.budget))\n                candidate = np.random.normal(best_solution, (ub - lb) / 10 * mutation_scale, self.dim)\n                candidate = np.clip(candidate, lb, ub)\n            else:\n                gradient = self.estimate_gradient(func, best_solution, lb, ub)\n                candidate = self.line_search(func, best_solution, gradient, lb, ub)\n\n            candidate_value = func(candidate)\n            self.evaluations += 1\n\n            if candidate_value < best_value:\n                best_solution, best_value = candidate, candidate_value\n                self.crossover_rate = min(1.0, self.crossover_rate + 0.02)\n                # Adaptive learning rate adjustment\n                if candidate_value < previous_value:\n                    self.successful_step += 1\n                    self.learning_rate *= (1 + 0.05 * self.successful_step/self.evaluations)\n                previous_value = candidate_value\n            else:\n                self.crossover_rate = max(0.2, self.crossover_rate - 0.01)\n                self.learning_rate *= 0.95\n                self.successful_step = 0\n\n        return best_solution, best_value\n\n    def estimate_gradient(self, func, solution, lb, ub, epsilon=1e-4):\n        gradient = np.zeros(self.dim)\n        for i in range(self.dim):\n            perturb = np.zeros(self.dim)\n            perturb[i] = epsilon\n            upper_sol = np.clip(solution + perturb, lb, ub)\n            lower_sol = np.clip(solution - perturb, lb, ub)\n            gradient[i] = (func(upper_sol) - func(lower_sol)) / (2 * epsilon)\n            self.evaluations += 2\n        return gradient\n\n    def line_search(self, func, solution, gradient, lb, ub, alpha=1e-2, beta=0.9):\n        step_size = self.learning_rate * np.linalg.norm(gradient) / (1 + self.evaluations / self.budget)\n        while step_size > 1e-5:\n            candidate = solution - step_size * gradient\n            candidate = np.clip(candidate, lb, ub)\n            if func(candidate) < func(solution):\n                return candidate\n            step_size *= beta\n            self.evaluations += 1\n        return solution", "name": "HybridOptimizer", "description": "Introduce historical learning rate adaptation based on previous successful steps to improve convergence stability.", "configspace": "", "generation": 22, "fitness": 0.10699715704864717, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.107 with standard deviation 0.006. And the mean value of best solutions found was 0.133 (0. is the best).", "error": "", "parent_id": "192bf623-9e4e-4a03-8722-782caf11b4b9", "metadata": {"aucs": [0.10009921622549289, 0.10535082036072829, 0.11554143455972032], "final_y": [0.15031589355828523, 0.13318502080821748, 0.11552776281136501]}, "mutation_prompt": null}
{"id": "2db9c93d-042b-434a-9ca9-5f9a73200448", "solution": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.crossover_rate = 0.8\n        self.learning_rate = 0.1  # Initial learning rate\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        best_solution = np.random.uniform(lb, ub, self.dim)\n        best_value = func(best_solution)\n        self.evaluations += 1\n        previous_value = best_value  # Track previous iteration's value\n\n        while self.evaluations < self.budget:\n            if np.random.rand() < self.crossover_rate:\n                mutation_scale = np.random.rand() * (0.3 * (1 - self.evaluations / self.budget))\n                candidate = np.random.normal(best_solution, (ub - lb) / 10 * mutation_scale, self.dim)\n                candidate = np.clip(candidate, lb, ub)\n            else:\n                gradient = self.estimate_gradient(func, best_solution, lb, ub)\n                candidate = self.line_search(func, best_solution, gradient, lb, ub)\n\n            candidate_value = func(candidate)\n            self.evaluations += 1\n\n            if candidate_value < best_value:\n                best_solution, best_value = candidate, candidate_value\n                self.crossover_rate = min(1.0, self.crossover_rate + 0.02)\n                # Adaptive learning rate adjustment\n                if candidate_value < previous_value:\n                    self.learning_rate *= 1.05\n                previous_value = candidate_value\n            else:\n                self.crossover_rate = max(0.2, self.crossover_rate - 0.01)\n                self.learning_rate *= 0.95\n\n        return best_solution, best_value\n\n    def estimate_gradient(self, func, solution, lb, ub, epsilon=1e-4):\n        gradient = np.zeros(self.dim)\n        for i in range(self.dim):\n            perturb = np.zeros(self.dim)\n            perturb[i] = epsilon\n            upper_sol = np.clip(solution + perturb, lb, ub)\n            lower_sol = np.clip(solution - perturb, lb, ub)\n            gradient[i] = (func(upper_sol) - func(lower_sol)) / (2 * epsilon)\n            self.evaluations += 2\n        return gradient\n\n    def line_search(self, func, solution, gradient, lb, ub, alpha=1e-2, beta=0.9):\n        step_size = self.learning_rate * np.linalg.norm(gradient) / (1 + self.evaluations / self.budget)\n        while step_size > 1e-5:\n            candidate = solution - step_size * gradient\n            candidate = np.clip(candidate, lb, ub)\n            if func(candidate) < func(solution):\n                return candidate\n            step_size *= beta\n            self.evaluations += 1\n        return solution", "name": "HybridOptimizer", "description": "Introduce a dynamic crossover rate based on the ratio of successful to total mutations to improve exploration-exploitation balance.", "configspace": "", "generation": 23, "fitness": 0.1085010649250893, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.109 with standard deviation 0.006. And the mean value of best solutions found was 0.130 (0. is the best).", "error": "", "parent_id": "192bf623-9e4e-4a03-8722-782caf11b4b9", "metadata": {"aucs": [0.10139582597540919, 0.10701010267993083, 0.11709726611992788], "final_y": [0.14684622279331494, 0.12901414352171692, 0.1135094763533473]}, "mutation_prompt": null}
{"id": "2cac9e85-5953-43e0-915e-c70020b57516", "solution": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.crossover_rate = 0.8\n        self.learning_rate = 0.1  # Initial learning rate\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        best_solution = np.random.uniform(lb, ub, self.dim)\n        best_value = func(best_solution)\n        self.evaluations += 1\n        previous_value = best_value  # Track previous iteration's value\n\n        while self.evaluations < self.budget:\n            if np.random.rand() < self.crossover_rate:\n                mutation_scale = np.random.rand() * (0.3 * np.sqrt(1 - self.evaluations / self.budget))\n                candidate = np.random.normal(best_solution, (ub - lb) / 10 * mutation_scale, self.dim)\n                candidate = np.clip(candidate, lb, ub)\n            else:\n                gradient = self.estimate_gradient(func, best_solution, lb, ub)\n                candidate = self.line_search(func, best_solution, gradient, lb, ub)\n\n            candidate_value = func(candidate)\n            self.evaluations += 1\n\n            if candidate_value < best_value:\n                best_solution, best_value = candidate, candidate_value\n                self.crossover_rate = min(1.0, self.crossover_rate + 0.02)\n                # Adaptive learning rate adjustment\n                if candidate_value < previous_value:\n                    self.learning_rate *= 1.05\n                previous_value = candidate_value\n            else:\n                self.crossover_rate = max(0.2, self.crossover_rate - 0.01)\n                self.learning_rate *= 0.95\n\n        return best_solution, best_value\n\n    def estimate_gradient(self, func, solution, lb, ub, epsilon=1e-4):\n        gradient = np.zeros(self.dim)\n        for i in range(self.dim):\n            perturb = np.zeros(self.dim)\n            perturb[i] = epsilon\n            upper_sol = np.clip(solution + perturb, lb, ub)\n            lower_sol = np.clip(solution - perturb, lb, ub)\n            gradient[i] = (func(upper_sol) - func(lower_sol)) / (2 * epsilon)\n            self.evaluations += 2\n        return gradient\n\n    def line_search(self, func, solution, gradient, lb, ub, alpha=1e-2, beta=0.9):\n        step_size = self.learning_rate * np.linalg.norm(gradient) / (1 + self.evaluations / self.budget)\n        while step_size > 1e-5:\n            candidate = solution - step_size * gradient\n            candidate = np.clip(candidate, lb, ub)\n            if func(candidate) < func(solution):\n                return candidate\n            step_size *= beta\n            self.evaluations += 1\n        return solution", "name": "HybridOptimizer", "description": "Introduce a dynamic mutation scale for improved exploration balancing.", "configspace": "", "generation": 24, "fitness": 0.10845060805745917, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.108 with standard deviation 0.006. And the mean value of best solutions found was 0.130 (0. is the best).", "error": "", "parent_id": "192bf623-9e4e-4a03-8722-782caf11b4b9", "metadata": {"aucs": [0.10183784140308794, 0.10715860389384613, 0.11635537887544345], "final_y": [0.14626628067880132, 0.1283114644887089, 0.11464040089947358]}, "mutation_prompt": null}
{"id": "f2700189-c507-46c7-b179-5ec38f228fb8", "solution": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.crossover_rate = 0.8\n        self.learning_rate = 0.1  # Initial learning rate\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        best_solution = np.random.uniform(lb, ub, self.dim)\n        best_value = func(best_solution)\n        self.evaluations += 1\n        previous_value = best_value  # Track previous iteration's value\n\n        while self.evaluations < self.budget:\n            if np.random.rand() < self.crossover_rate:\n                mutation_scale = np.random.rand() * (0.1 + 0.2 * (best_value / (previous_value + 1e-9)))\n                candidate = np.random.normal(best_solution, (ub - lb) / 10 * mutation_scale, self.dim)\n                candidate = np.clip(candidate, lb, ub)\n            else:\n                gradient = self.estimate_gradient(func, best_solution, lb, ub)\n                candidate = self.line_search(func, best_solution, gradient, lb, ub)\n\n            candidate_value = func(candidate)\n            self.evaluations += 1\n\n            if candidate_value < best_value:\n                best_solution, best_value = candidate, candidate_value\n                self.crossover_rate = min(1.0, self.crossover_rate + 0.02)\n                if candidate_value < previous_value:\n                    self.learning_rate *= 1.05\n                previous_value = candidate_value\n            else:\n                self.crossover_rate = max(0.2, self.crossover_rate - 0.01)\n                self.learning_rate *= 0.95\n\n        return best_solution, best_value\n\n    def estimate_gradient(self, func, solution, lb, ub, epsilon=1e-4):\n        gradient = np.zeros(self.dim)\n        for i in range(self.dim):\n            perturb = np.zeros(self.dim)\n            perturb[i] = epsilon\n            upper_sol = np.clip(solution + perturb, lb, ub)\n            lower_sol = np.clip(solution - perturb, lb, ub)\n            gradient[i] = (func(upper_sol) - func(lower_sol)) / (2 * epsilon)\n            self.evaluations += 2\n        return gradient\n\n    def line_search(self, func, solution, gradient, lb, ub, alpha=1e-2, beta=0.9):\n        step_size = self.learning_rate * np.linalg.norm(gradient) / (1 + self.evaluations / self.budget)\n        while step_size > 1e-5:\n            candidate = solution - step_size * gradient\n            candidate = np.clip(candidate, lb, ub)\n            if func(candidate) < func(solution):\n                return candidate\n            step_size *= beta\n            self.evaluations += 1\n        return solution", "name": "HybridOptimizer", "description": "Incremental adaptation of mutation scale based on convergence history to refine exploration-exploitation balance.", "configspace": "", "generation": 25, "fitness": 0.10721704603400632, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.107 with standard deviation 0.007. And the mean value of best solutions found was 0.134 (0. is the best).", "error": "", "parent_id": "192bf623-9e4e-4a03-8722-782caf11b4b9", "metadata": {"aucs": [0.10099556846785052, 0.10361389194390058, 0.11704167769026785], "final_y": [0.15008764213903047, 0.13885996781206433, 0.11390717636817227]}, "mutation_prompt": null}
{"id": "081a5261-2eee-4a18-a07a-8834218c94f3", "solution": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.crossover_rate = 0.8\n        self.learning_rate = 0.1  # Initial learning rate\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        best_solution = np.random.uniform(lb, ub, self.dim)\n        best_value = func(best_solution)\n        self.evaluations += 1\n        previous_value = best_value  # Track previous iteration's value\n\n        while self.evaluations < self.budget:\n            if np.random.rand() < self.crossover_rate:\n                mutation_scale = np.random.rand() * (0.3 * (1 - self.evaluations / self.budget))\n                candidate = np.random.normal(best_solution, (ub - lb) / 10 * mutation_scale, self.dim)\n                candidate = np.clip(candidate, lb, ub)\n            else:\n                gradient = self.estimate_gradient(func, best_solution, lb, ub)\n                candidate = self.line_search(func, best_solution, gradient, lb, ub)\n\n            candidate_value = func(candidate)\n            self.evaluations += 1\n\n            if candidate_value < best_value:\n                best_solution, best_value = candidate, candidate_value\n                self.crossover_rate = min(1.0, self.crossover_rate + 0.02)\n                # Adaptive learning rate adjustment\n                if candidate_value < previous_value:\n                    self.learning_rate *= 1.05\n                previous_value = candidate_value\n            else:\n                self.crossover_rate = max(0.2, self.crossover_rate - 0.01 * np.random.rand())\n                self.learning_rate *= 0.95\n\n        return best_solution, best_value\n\n    def estimate_gradient(self, func, solution, lb, ub, epsilon=1e-4):\n        gradient = np.zeros(self.dim)\n        for i in range(self.dim):\n            perturb = np.zeros(self.dim)\n            perturb[i] = epsilon\n            upper_sol = np.clip(solution + perturb, lb, ub)\n            lower_sol = np.clip(solution - perturb, lb, ub)\n            gradient[i] = (func(upper_sol) - func(lower_sol)) / (2 * epsilon)\n            self.evaluations += 2\n        return gradient\n\n    def line_search(self, func, solution, gradient, lb, ub, alpha=1e-2, beta=0.9):\n        step_size = self.learning_rate * np.linalg.norm(gradient) / (1 + self.evaluations / self.budget)\n        while step_size > 1e-5:\n            candidate = solution - step_size * gradient\n            candidate = np.clip(candidate, lb, ub)\n            if func(candidate) < func(solution):\n                return candidate\n            step_size *= beta\n            self.evaluations += 1\n        return solution", "name": "HybridOptimizer", "description": "Implemented a dynamic crossover rate adjustment to improve diversity and exploration.", "configspace": "", "generation": 26, "fitness": 0.10946144624055709, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.109 with standard deviation 0.007. And the mean value of best solutions found was 0.130 (0. is the best).", "error": "", "parent_id": "192bf623-9e4e-4a03-8722-782caf11b4b9", "metadata": {"aucs": [0.1010116522370722, 0.1089391316449787, 0.11843355483962037], "final_y": [0.15384619224538043, 0.12502847032577102, 0.11145707740160482]}, "mutation_prompt": null}
{"id": "ec2d7f49-ca62-4bce-a6e0-948f9914cb66", "solution": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.crossover_rate = 0.8\n        self.learning_rate = 0.1  # Initial learning rate\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        best_solution = np.random.uniform(lb, ub, self.dim)\n        best_value = func(best_solution)\n        self.evaluations += 1\n        previous_value = best_value  # Track previous iteration's value\n\n        while self.evaluations < self.budget:\n            temperature = 1 - self.evaluations / self.budget  # Introduced temperature parameter\n            if np.random.rand() < self.crossover_rate * temperature:\n                mutation_scale = np.random.rand() * (0.3 * (1 - self.evaluations / self.budget))\n                candidate = np.random.normal(best_solution, (ub - lb) / 10 * mutation_scale, self.dim)\n                candidate = np.clip(candidate, lb, ub)\n            else:\n                gradient = self.estimate_gradient(func, best_solution, lb, ub)\n                candidate = self.line_search(func, best_solution, gradient, lb, ub)\n\n            candidate_value = func(candidate)\n            self.evaluations += 1\n\n            if candidate_value < best_value:\n                best_solution, best_value = candidate, candidate_value\n                self.crossover_rate = min(1.0, self.crossover_rate + 0.02)\n                # Adaptive learning rate adjustment\n                if candidate_value < previous_value:\n                    self.learning_rate *= 1.05\n                previous_value = candidate_value\n            else:\n                self.crossover_rate = max(0.2, self.crossover_rate - 0.01 * np.random.rand())\n                self.learning_rate *= 0.95\n\n        return best_solution, best_value\n\n    def estimate_gradient(self, func, solution, lb, ub, epsilon=1e-4):\n        gradient = np.zeros(self.dim)\n        for i in range(self.dim):\n            perturb = np.zeros(self.dim)\n            perturb[i] = epsilon\n            upper_sol = np.clip(solution + perturb, lb, ub)\n            lower_sol = np.clip(solution - perturb, lb, ub)\n            gradient[i] = (func(upper_sol) - func(lower_sol)) / (2 * epsilon)\n            self.evaluations += 2\n        return gradient\n\n    def line_search(self, func, solution, gradient, lb, ub, alpha=1e-2, beta=0.9):\n        step_size = self.learning_rate * np.linalg.norm(gradient) / (1 + self.evaluations / self.budget)\n        while step_size > 1e-5:\n            candidate = solution - step_size * gradient\n            candidate = np.clip(candidate, lb, ub)\n            if func(candidate) < func(solution):\n                return candidate\n            step_size *= beta\n            self.evaluations += 1\n        return solution", "name": "HybridOptimizer", "description": "Introduced a temperature parameter for simulated annealing-based dynamic crossover rate adjustment for enhanced exploration.", "configspace": "", "generation": 27, "fitness": 0.10800260500537417, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.108 with standard deviation 0.006. And the mean value of best solutions found was 0.133 (0. is the best).", "error": "", "parent_id": "081a5261-2eee-4a18-a07a-8834218c94f3", "metadata": {"aucs": [0.10223352141687503, 0.1054252212424609, 0.1163490723567866], "final_y": [0.1473327394737054, 0.13562278591912247, 0.11475572678347712]}, "mutation_prompt": null}
{"id": "3e912971-6e84-488f-867e-59d205a5313b", "solution": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.crossover_rate = 0.8\n        self.learning_rate = 0.1  # Initial learning rate\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        best_solution = np.random.uniform(lb, ub, self.dim)\n        best_value = func(best_solution)\n        self.evaluations += 1\n        previous_value = best_value  # Track previous iteration's value\n\n        while self.evaluations < self.budget:\n            if np.random.rand() < self.crossover_rate:\n                mutation_scale = np.random.rand() * (0.3 * (1 - (self.evaluations / self.budget)**2))  # Changed line\n                candidate = np.random.normal(best_solution, (ub - lb) / 10 * mutation_scale, self.dim)\n                candidate = np.clip(candidate, lb, ub)\n            else:\n                gradient = self.estimate_gradient(func, best_solution, lb, ub)\n                candidate = self.line_search(func, best_solution, gradient, lb, ub)\n\n            candidate_value = func(candidate)\n            self.evaluations += 1\n\n            if candidate_value < best_value:\n                best_solution, best_value = candidate, candidate_value\n                self.crossover_rate = min(1.0, self.crossover_rate + 0.02)\n                # Adaptive learning rate adjustment\n                if candidate_value < previous_value:\n                    self.learning_rate *= 1.05\n                previous_value = candidate_value\n            else:\n                self.crossover_rate = max(0.2, self.crossover_rate - 0.01 * np.random.rand())\n                self.learning_rate *= 0.95\n\n        return best_solution, best_value\n\n    def estimate_gradient(self, func, solution, lb, ub, epsilon=1e-4):\n        gradient = np.zeros(self.dim)\n        for i in range(self.dim):\n            perturb = np.zeros(self.dim)\n            perturb[i] = epsilon\n            upper_sol = np.clip(solution + perturb, lb, ub)\n            lower_sol = np.clip(solution - perturb, lb, ub)\n            gradient[i] = (func(upper_sol) - func(lower_sol)) / (2 * epsilon)\n            self.evaluations += 2\n        return gradient\n\n    def line_search(self, func, solution, gradient, lb, ub, alpha=1e-2, beta=0.9):\n        step_size = self.learning_rate * np.linalg.norm(gradient) / (1 + self.evaluations / self.budget)\n        while step_size > 1e-5:\n            candidate = solution - step_size * gradient\n            candidate = np.clip(candidate, lb, ub)\n            if func(candidate) < func(solution):\n                return candidate\n            step_size *= beta\n            self.evaluations += 1\n        return solution", "name": "HybridOptimizer", "description": "Enhanced the mutation scale calculation to increase exploration in late stages of the budget.", "configspace": "", "generation": 28, "fitness": 0.1098204452473281, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.110 with standard deviation 0.006. And the mean value of best solutions found was 0.129 (0. is the best).", "error": "", "parent_id": "081a5261-2eee-4a18-a07a-8834218c94f3", "metadata": {"aucs": [0.10210230372676943, 0.10948546386770763, 0.11787356814750727], "final_y": [0.15055443144118152, 0.12313358521130824, 0.11278238318051859]}, "mutation_prompt": null}
{"id": "524144a3-f69c-4e4d-8b14-0bc8f1ab244c", "solution": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.crossover_rate = 0.8\n        self.learning_rate = 0.1  # Initial learning rate\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        best_solution = np.random.uniform(lb, ub, self.dim)\n        best_value = func(best_solution)\n        self.evaluations += 1\n        previous_value = best_value  # Track previous iteration's value\n\n        while self.evaluations < self.budget:\n            if np.random.rand() < self.crossover_rate:\n                mutation_scale = np.random.rand() * (0.3 * (1 - (self.evaluations / self.budget)**2))  # Changed line\n                candidate = np.random.normal(best_solution, (ub - lb) / 10 * mutation_scale, self.dim)\n                candidate = np.clip(candidate, lb, ub)\n            else:\n                gradient = self.estimate_gradient(func, best_solution, lb, ub)\n                candidate = self.line_search(func, best_solution, gradient, lb, ub)\n\n            candidate_value = func(candidate)\n            self.evaluations += 1\n\n            if candidate_value < best_value:\n                best_solution, best_value = candidate, candidate_value\n                self.crossover_rate = min(1.0, self.crossover_rate + 0.02)\n                # Adaptive learning rate adjustment\n                if candidate_value < previous_value:\n                    self.learning_rate *= 1.05\n                previous_value = candidate_value\n            else:\n                self.crossover_rate = max(0.2, self.crossover_rate - 0.01 * np.random.rand())\n                self.learning_rate *= 0.95\n\n        return best_solution, best_value\n\n    def estimate_gradient(self, func, solution, lb, ub, epsilon=1e-4):\n        gradient = np.zeros(self.dim)\n        for i in range(self.dim):\n            perturb = np.zeros(self.dim)\n            perturb[i] = epsilon\n            upper_sol = np.clip(solution + perturb, lb, ub)\n            lower_sol = np.clip(solution - perturb, lb, ub)\n            gradient[i] = (func(upper_sol) - func(lower_sol)) / (2 * epsilon)\n            self.evaluations += 2\n        return gradient\n\n    def line_search(self, func, solution, gradient, lb, ub, alpha=1e-2, beta=0.95):  # Changed line\n        step_size = self.learning_rate * np.linalg.norm(gradient) / (1 + self.evaluations / self.budget)\n        while step_size > 1e-5:\n            candidate = solution - step_size * gradient\n            candidate = np.clip(candidate, lb, ub)\n            if func(candidate) < func(solution):\n                return candidate\n            step_size *= beta\n            self.evaluations += 1\n        return solution", "name": "HybridOptimizer", "description": "Enhanced the line search step size adjustment to improve convergence speed.", "configspace": "", "generation": 29, "fitness": 0.1098204452473281, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.110 with standard deviation 0.006. And the mean value of best solutions found was 0.129 (0. is the best).", "error": "", "parent_id": "3e912971-6e84-488f-867e-59d205a5313b", "metadata": {"aucs": [0.10210230372676943, 0.10948546386770763, 0.11787356814750727], "final_y": [0.15055443144118152, 0.12313358521130824, 0.11278238318051859]}, "mutation_prompt": null}
{"id": "e163822c-4ce7-4e9d-b1fa-18157fffa700", "solution": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.crossover_rate = 0.8\n        self.learning_rate = 0.1  # Initial learning rate\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        best_solution = np.random.uniform(lb, ub, self.dim)\n        best_value = func(best_solution)\n        self.evaluations += 1\n        previous_value = best_value  # Track previous iteration's value\n\n        while self.evaluations < self.budget:\n            if np.random.rand() < self.crossover_rate:\n                mutation_scale = np.random.rand() * (0.3 * (1 - (self.evaluations / self.budget)**2))  # Changed line\n                candidate = np.random.normal(best_solution, (ub - lb) / 10 * mutation_scale, self.dim)\n                candidate = np.clip(candidate, lb, ub)\n            else:\n                gradient = self.estimate_gradient(func, best_solution, lb, ub)\n                candidate = self.line_search(func, best_solution, gradient, lb, ub)\n\n            candidate_value = func(candidate)\n            self.evaluations += 1\n\n            if candidate_value < best_value:\n                best_solution, best_value = candidate, candidate_value\n                self.crossover_rate = min(1.0, self.crossover_rate + 0.02)\n                # Adaptive learning rate adjustment\n                self.learning_rate *= 1.08 if candidate_value < previous_value else 1.05\n                previous_value = candidate_value\n            else:\n                self.crossover_rate = max(0.2, self.crossover_rate - 0.01 * np.random.rand())\n                self.learning_rate *= 0.95\n\n        return best_solution, best_value\n\n    def estimate_gradient(self, func, solution, lb, ub, epsilon=1e-4):\n        gradient = np.zeros(self.dim)\n        for i in range(self.dim):\n            perturb = np.zeros(self.dim)\n            perturb[i] = epsilon\n            upper_sol = np.clip(solution + perturb, lb, ub)\n            lower_sol = np.clip(solution - perturb, lb, ub)\n            gradient[i] = (func(upper_sol) - func(lower_sol)) / (2 * epsilon)\n            self.evaluations += 2\n        return gradient\n\n    def line_search(self, func, solution, gradient, lb, ub, alpha=1e-2, beta=0.9):\n        step_size = self.learning_rate * np.linalg.norm(gradient) / (1 + self.evaluations / self.budget)\n        while step_size > 1e-5:\n            candidate = solution - step_size * gradient\n            candidate = np.clip(candidate, lb, ub)\n            if func(candidate) < func(solution):\n                return candidate\n            step_size *= beta\n            self.evaluations += 1\n        return solution", "name": "HybridOptimizer", "description": "Refined learning rate increase mechanism upon improvement for enhanced convergence.", "configspace": "", "generation": 30, "fitness": 0.1098521927022758, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.110 with standard deviation 0.006. And the mean value of best solutions found was 0.128 (0. is the best).", "error": "", "parent_id": "3e912971-6e84-488f-867e-59d205a5313b", "metadata": {"aucs": [0.10226574965034807, 0.1094172603035789, 0.1178735681529004], "final_y": [0.14932878546105044, 0.12315305119464914, 0.11278238318294687]}, "mutation_prompt": null}
{"id": "161551d1-cdda-4e43-8ba7-f9a6ded70d42", "solution": "import numpy as np\n\nclass EnhancedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.crossover_rate = 0.7\n        self.learning_rate = 0.1  # Initial learning rate\n        self.population_size = 10\n        self.population = None\n        self.best_solution = None\n        self.best_value = np.inf\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        self.evaluate_population(func)\n\n        while self.evaluations < self.budget:\n            for _ in range(self.population_size):\n                if np.random.rand() < self.crossover_rate:\n                    parent1, parent2 = self.select_parents()\n                    offspring = self.crossover(parent1, parent2, lb, ub)\n                else:\n                    candidate = self.mutate(lb, ub)\n                    offspring = self.local_search(func, candidate, lb, ub)\n\n                offspring_value = func(offspring)\n                self.evaluations += 1\n\n                if offspring_value < self.best_value:\n                    self.best_solution, self.best_value = offspring, offspring_value\n                    self.crossover_rate = min(1.0, self.crossover_rate + 0.02)\n                    self.learning_rate *= 1.1\n                else:\n                    self.crossover_rate = max(0.2, self.crossover_rate - 0.01)\n                    self.learning_rate *= 0.9\n                \n                self.replace_worst(offspring, offspring_value)\n                \n        return self.best_solution, self.best_value\n\n    def initialize_population(self, lb, ub):\n        self.population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func):\n        for individual in self.population:\n            value = func(individual)\n            self.evaluations += 1\n            if value < self.best_value:\n                self.best_solution, self.best_value = individual, value\n\n    def select_parents(self):\n        idx = np.random.choice(self.population_size, 2, replace=False)\n        return self.population[idx]\n\n    def crossover(self, parent1, parent2, lb, ub):\n        alpha = np.random.uniform(0, 1, self.dim)\n        offspring = alpha * parent1 + (1 - alpha) * parent2\n        return np.clip(offspring, lb, ub)\n\n    def mutate(self, lb, ub):\n        mutation_scale = np.random.rand() * (0.3 * (1 - (self.evaluations / self.budget)**2))\n        candidate = np.random.normal(self.best_solution, (ub - lb) / 10 * mutation_scale, self.dim)\n        return np.clip(candidate, lb, ub)\n\n    def local_search(self, func, solution, lb, ub, alpha=1e-2, beta=0.9):\n        gradient = self.estimate_gradient(func, solution, lb, ub)\n        step_size = self.learning_rate * np.linalg.norm(gradient) / (1 + self.evaluations / self.budget)\n        \n        while step_size > 1e-5:\n            candidate = solution - step_size * gradient\n            candidate = np.clip(candidate, lb, ub)\n            if func(candidate) < func(solution):\n                return candidate\n            step_size *= beta\n            self.evaluations += 1\n        return solution\n\n    def estimate_gradient(self, func, solution, lb, ub, epsilon=1e-4):\n        gradient = np.zeros(self.dim)\n        for i in range(self.dim):\n            perturb = np.zeros(self.dim)\n            perturb[i] = epsilon\n            upper_sol = np.clip(solution + perturb, lb, ub)\n            lower_sol = np.clip(solution - perturb, lb, ub)\n            gradient[i] = (func(upper_sol) - func(lower_sol)) / (2 * epsilon)\n            self.evaluations += 2\n        return gradient\n\n    def replace_worst(self, candidate, candidate_value):\n        worst_idx = np.argmax([func(x) for x in self.population])\n        if candidate_value < func(self.population[worst_idx]):\n            self.population[worst_idx] = candidate", "name": "EnhancedHybridOptimizer", "description": "Enhanced HybridOptimizer using adaptive mutation and dynamic crossover leveraging diversity-based feedback for improved convergence speed and solution quality.", "configspace": "", "generation": 31, "fitness": -Infinity, "feedback": "An exception occurred: NameError(\"name 'func' is not defined\").", "error": "NameError(\"name 'func' is not defined\")", "parent_id": "e163822c-4ce7-4e9d-b1fa-18157fffa700", "metadata": {}, "mutation_prompt": null}
{"id": "7bc12482-d3b1-4164-a5c1-58422fac156b", "solution": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.crossover_rate = 0.8\n        self.learning_rate = 0.1  # Initial learning rate\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        best_solution = np.random.uniform(lb, ub, self.dim)\n        best_value = func(best_solution)\n        self.evaluations += 1\n        previous_value = best_value  # Track previous iteration's value\n\n        while self.evaluations < self.budget:\n            if np.random.rand() < self.crossover_rate:\n                # Enhanced dynamic mutation factor\n                mutation_scale = (0.1 + np.random.rand() * (0.2 * (1 - (self.evaluations / self.budget)**3)))\n                candidate = np.random.normal(best_solution, (ub - lb) / 10 * mutation_scale, self.dim)\n                candidate = np.clip(candidate, lb, ub)\n            else:\n                gradient = self.estimate_gradient(func, best_solution, lb, ub)\n                candidate = self.line_search(func, best_solution, gradient, lb, ub)\n\n            candidate_value = func(candidate)\n            self.evaluations += 1\n\n            if candidate_value < best_value:\n                best_solution, best_value = candidate, candidate_value\n                self.crossover_rate = min(1.0, self.crossover_rate + 0.02)\n                # More adaptable learning rate scaling\n                self.learning_rate *= 1.1 if candidate_value < previous_value else 1.03\n                previous_value = candidate_value\n            else:\n                self.crossover_rate = max(0.2, self.crossover_rate - 0.01 * np.random.rand())\n                self.learning_rate *= 0.93\n\n        return best_solution, best_value\n\n    def estimate_gradient(self, func, solution, lb, ub, epsilon=1e-4):\n        gradient = np.zeros(self.dim)\n        for i in range(self.dim):\n            perturb = np.zeros(self.dim)\n            perturb[i] = epsilon\n            upper_sol = np.clip(solution + perturb, lb, ub)\n            lower_sol = np.clip(solution - perturb, lb, ub)\n            gradient[i] = (func(upper_sol) - func(lower_sol)) / (2 * epsilon)\n            self.evaluations += 2\n        return gradient\n\n    def line_search(self, func, solution, gradient, lb, ub, alpha=1e-2, beta=0.9):\n        step_size = self.learning_rate * np.linalg.norm(gradient) / (1 + self.evaluations / self.budget)\n        while step_size > 1e-5:\n            candidate = solution - step_size * gradient\n            candidate = np.clip(candidate, lb, ub)\n            if func(candidate) < func(solution):\n                return candidate\n            step_size *= beta\n            self.evaluations += 1\n        return solution", "name": "HybridOptimizer", "description": "Enhanced dynamic mutation and adaptive learning strategy for improved convergence performance.", "configspace": "", "generation": 32, "fitness": 0.11166072975387457, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.112 with standard deviation 0.006. And the mean value of best solutions found was 0.124 (0. is the best).", "error": "", "parent_id": "e163822c-4ce7-4e9d-b1fa-18157fffa700", "metadata": {"aucs": [0.10442878888579499, 0.11257493694967113, 0.11797846342615759], "final_y": [0.1415766996554917, 0.1190293568643741, 0.11213430804791436]}, "mutation_prompt": null}
{"id": "8801456e-3b37-466c-8022-a26f224c8ae3", "solution": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.crossover_rate = 0.8\n        self.learning_rate = 0.1  # Initial learning rate\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        best_solution = np.random.uniform(lb, ub, self.dim)\n        best_value = func(best_solution)\n        self.evaluations += 1\n        previous_value = best_value  # Track previous iteration's value\n\n        while self.evaluations < self.budget:\n            if np.random.rand() < self.crossover_rate:\n                # Enhanced dynamic mutation factor\n                mutation_scale = (0.2 + np.random.rand() * (0.2 * (1 - (self.evaluations / self.budget)**3)))\n                candidate = np.random.normal(best_solution, (ub - lb) / 10 * mutation_scale, self.dim)\n                candidate = np.clip(candidate, lb, ub)\n            else:\n                gradient = self.estimate_gradient(func, best_solution, lb, ub)\n                candidate = self.line_search(func, best_solution, gradient, lb, ub)\n\n            candidate_value = func(candidate)\n            self.evaluations += 1\n\n            if candidate_value < best_value:\n                best_solution, best_value = candidate, candidate_value\n                self.crossover_rate = min(1.0, self.crossover_rate + 0.02)\n                # More adaptable learning rate scaling\n                self.learning_rate *= 1.1 if candidate_value < previous_value else 1.03\n                previous_value = candidate_value\n            else:\n                self.crossover_rate = max(0.2, self.crossover_rate - 0.01 * np.random.rand())\n                self.learning_rate *= 0.93\n\n        return best_solution, best_value\n\n    def estimate_gradient(self, func, solution, lb, ub, epsilon=1e-4):\n        gradient = np.zeros(self.dim)\n        for i in range(self.dim):\n            perturb = np.zeros(self.dim)\n            perturb[i] = epsilon\n            upper_sol = np.clip(solution + perturb, lb, ub)\n            lower_sol = np.clip(solution - perturb, lb, ub)\n            gradient[i] = (func(upper_sol) - func(lower_sol)) / (2 * epsilon)\n            self.evaluations += 2\n        return gradient\n\n    def line_search(self, func, solution, gradient, lb, ub, alpha=1e-2, beta=0.9):\n        step_size = self.learning_rate * np.linalg.norm(gradient) / (1 + self.evaluations / self.budget)\n        while step_size > 1e-5:\n            candidate = solution - step_size * gradient\n            candidate = np.clip(candidate, lb, ub)\n            if func(candidate) < func(solution):\n                return candidate\n            step_size *= beta\n            self.evaluations += 1\n        return solution", "name": "HybridOptimizer", "description": "Slightly increased mutation scale for enhanced exploration capabilities in optimization.", "configspace": "", "generation": 33, "fitness": 0.10943840209013622, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.109 with standard deviation 0.008. And the mean value of best solutions found was 0.132 (0. is the best).", "error": "", "parent_id": "7bc12482-d3b1-4164-a5c1-58422fac156b", "metadata": {"aucs": [0.09977669636259756, 0.11041798514095491, 0.1181205247668562], "final_y": [0.15806110275166796, 0.1246192671738432, 0.11236153423364281]}, "mutation_prompt": null}
{"id": "25cd56fe-ea0a-4521-a9f9-d3868f905a9d", "solution": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.crossover_rate = 0.8\n        self.learning_rate = 0.1  # Initial learning rate\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        best_solution = np.random.uniform(lb, ub, self.dim)\n        best_value = func(best_solution)\n        self.evaluations += 1\n        previous_value = best_value  # Track previous iteration's value\n\n        while self.evaluations < self.budget:\n            if np.random.rand() < self.crossover_rate:\n                # Refined dynamic mutation factor\n                mutation_scale = (0.1 + np.random.rand() * (0.3 * (1 - (self.evaluations / self.budget)**3)))\n                candidate = np.random.normal(best_solution, (ub - lb) / 10 * mutation_scale, self.dim)\n                candidate = np.clip(candidate, lb, ub)\n            else:\n                gradient = self.estimate_gradient(func, best_solution, lb, ub)\n                candidate = self.line_search(func, best_solution, gradient, lb, ub)\n\n            candidate_value = func(candidate)\n            self.evaluations += 1\n\n            if candidate_value < best_value:\n                best_solution, best_value = candidate, candidate_value\n                self.crossover_rate = min(1.0, self.crossover_rate + np.random.uniform(0.01, 0.02)) # Stochastic increase\n                self.learning_rate *= 1.1 if candidate_value < previous_value else 1.03\n                previous_value = candidate_value\n            else:\n                self.crossover_rate = max(0.2, self.crossover_rate - 0.01 * np.random.rand())\n                self.learning_rate *= 0.93\n\n        return best_solution, best_value\n\n    def estimate_gradient(self, func, solution, lb, ub, epsilon=1e-4):\n        gradient = np.zeros(self.dim)\n        for i in range(self.dim):\n            perturb = np.zeros(self.dim)\n            perturb[i] = epsilon\n            upper_sol = np.clip(solution + perturb, lb, ub)\n            lower_sol = np.clip(solution - perturb, lb, ub)\n            gradient[i] = (func(upper_sol) - func(lower_sol)) / (2 * epsilon)\n            self.evaluations += 2\n        return gradient\n\n    def line_search(self, func, solution, gradient, lb, ub, alpha=1e-2, beta=0.9):\n        step_size = self.learning_rate * np.linalg.norm(gradient) / (1 + self.evaluations / self.budget)\n        while step_size > 1e-5:\n            candidate = solution - step_size * gradient\n            candidate = np.clip(candidate, lb, ub)\n            if func(candidate) < func(solution):\n                return candidate\n            step_size *= beta\n            self.evaluations += 1\n        return solution", "name": "HybridOptimizer", "description": "Introducing a stochastic crossover rate and refined mutation scale adjustment for enhanced exploration.", "configspace": "", "generation": 34, "fitness": 0.11129161312667164, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.111 with standard deviation 0.006. And the mean value of best solutions found was 0.126 (0. is the best).", "error": "", "parent_id": "7bc12482-d3b1-4164-a5c1-58422fac156b", "metadata": {"aucs": [0.10293652661998143, 0.11427868390881635, 0.11665962885121717], "final_y": [0.14337098204060927, 0.11961106540107258, 0.11465486398113134]}, "mutation_prompt": null}
{"id": "740365de-6c01-42ca-8f13-7dbf6fa0f720", "solution": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.crossover_rate = 0.8\n        self.learning_rate = 0.1  # Initial learning rate\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        best_solution = np.random.uniform(lb, ub, self.dim)\n        best_value = func(best_solution)\n        self.evaluations += 1\n        previous_value = best_value  # Track previous iteration's value\n\n        while self.evaluations < self.budget:\n            if np.random.rand() < self.crossover_rate:\n                # Enhanced dynamic mutation factor\n                mutation_scale = (0.1 + np.random.rand() * (0.15 * (1 - (self.evaluations / self.budget)**3)))  # Line modified\n                candidate = np.random.normal(best_solution, (ub - lb) / 10 * mutation_scale, self.dim)\n                candidate = np.clip(candidate, lb, ub)\n            else:\n                gradient = self.estimate_gradient(func, best_solution, lb, ub)\n                candidate = self.line_search(func, best_solution, gradient, lb, ub)\n\n            candidate_value = func(candidate)\n            self.evaluations += 1\n\n            if candidate_value < best_value:\n                best_solution, best_value = candidate, candidate_value\n                self.crossover_rate = min(1.0, self.crossover_rate + 0.02)\n                # More adaptable learning rate scaling\n                self.learning_rate *= 1.1 if candidate_value < previous_value else 1.03\n                previous_value = candidate_value\n            else:\n                self.crossover_rate = max(0.2, self.crossover_rate - 0.01 * np.random.rand())\n                self.learning_rate *= 0.95  # Line modified\n\n        return best_solution, best_value\n\n    def estimate_gradient(self, func, solution, lb, ub, epsilon=1e-4):\n        gradient = np.zeros(self.dim)\n        for i in range(self.dim):\n            perturb = np.zeros(self.dim)\n            perturb[i] = epsilon\n            upper_sol = np.clip(solution + perturb, lb, ub)\n            lower_sol = np.clip(solution - perturb, lb, ub)\n            gradient[i] = (func(upper_sol) - func(lower_sol)) / (2 * epsilon)\n            self.evaluations += 2\n        return gradient\n\n    def line_search(self, func, solution, gradient, lb, ub, alpha=1e-2, beta=0.9):\n        step_size = self.learning_rate * np.linalg.norm(gradient) / (1 + self.evaluations / self.budget)\n        while step_size > 1e-5:\n            candidate = solution - step_size * gradient\n            candidate = np.clip(candidate, lb, ub)\n            if func(candidate) < func(solution):\n                return candidate\n            step_size *= beta\n            self.evaluations += 1\n        return solution", "name": "HybridOptimizer", "description": "Fine-tuned mutation scaling and adaptive learning rate decay for enhanced convergence.", "configspace": "", "generation": 35, "fitness": 0.11107742618657923, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.111 with standard deviation 0.007. And the mean value of best solutions found was 0.127 (0. is the best).", "error": "", "parent_id": "7bc12482-d3b1-4164-a5c1-58422fac156b", "metadata": {"aucs": [0.10193361443509563, 0.11357047906970907, 0.11772818505493299], "final_y": [0.14927059307177937, 0.11769456493783648, 0.11301818044767342]}, "mutation_prompt": null}
{"id": "f2e572e4-ea80-4ba7-9e01-e2b66eee5529", "solution": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.crossover_rate = 0.8\n        self.learning_rate = 0.1  # Initial learning rate\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        best_solution = np.random.uniform(lb, ub, self.dim)\n        best_value = func(best_solution)\n        self.evaluations += 1\n        previous_value = best_value  # Track previous iteration's value\n\n        while self.evaluations < self.budget:\n            if np.random.rand() < self.crossover_rate:\n                # Enhanced dynamic mutation factor\n                mutation_scale = (0.1 + np.random.rand() * (0.2 * (1 - (self.evaluations / self.budget)**3)))\n                candidate = np.random.normal(best_solution, (ub - lb) / 10 * mutation_scale, self.dim)\n                candidate = np.clip(candidate, lb, ub)\n            else:\n                gradient = self.estimate_gradient(func, best_solution, lb, ub)\n                candidate = self.line_search(func, best_solution, gradient, lb, ub)\n\n            candidate_value = func(candidate)\n            self.evaluations += 1\n\n            if candidate_value < best_value:\n                best_solution, best_value = candidate, candidate_value\n                self.crossover_rate = min(1.0, self.crossover_rate + 0.02)\n                # More adaptable learning rate scaling\n                self.learning_rate *= 1.1 if candidate_value < previous_value else 1.03\n                previous_value = candidate_value\n            else:\n                self.crossover_rate = max(0.2, self.crossover_rate - 0.02)  # Changed from 0.01 to 0.02\n                self.learning_rate *= 0.93\n\n        return best_solution, best_value\n\n    def estimate_gradient(self, func, solution, lb, ub, epsilon=1e-4):\n        gradient = np.zeros(self.dim)\n        for i in range(self.dim):\n            perturb = np.zeros(self.dim)\n            perturb[i] = epsilon\n            upper_sol = np.clip(solution + perturb, lb, ub)\n            lower_sol = np.clip(solution - perturb, lb, ub)\n            gradient[i] = (func(upper_sol) - func(lower_sol)) / (2 * epsilon)\n            self.evaluations += 2\n        return gradient\n\n    def line_search(self, func, solution, gradient, lb, ub, alpha=1e-2, beta=0.9):\n        step_size = self.learning_rate * np.linalg.norm(gradient) / (1 + self.evaluations / self.budget)\n        while step_size > 1e-5:\n            candidate = solution - step_size * gradient\n            candidate = np.clip(candidate, lb, ub)\n            if func(candidate) < func(solution):\n                return candidate\n            step_size *= beta\n            self.evaluations += 1\n        return solution", "name": "HybridOptimizer", "description": "Introduced adaptive crossover rate decay for finer control over exploration-exploitation balance.", "configspace": "", "generation": 36, "fitness": 0.10631709137551622, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.106 with standard deviation 0.007. And the mean value of best solutions found was 0.130 (0. is the best).", "error": "", "parent_id": "7bc12482-d3b1-4164-a5c1-58422fac156b", "metadata": {"aucs": [0.09897724351341175, 0.10458504307881034, 0.11538898753432658], "final_y": [0.1468473538376407, 0.12779917751799674, 0.114411730757365]}, "mutation_prompt": null}
{"id": "072f5131-dfa7-4af3-b891-32e3bf64c8fc", "solution": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.crossover_rate = 0.8\n        self.learning_rate = 0.1  # Initial learning rate\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        best_solution = np.random.uniform(lb, ub, self.dim)\n        best_value = func(best_solution)\n        self.evaluations += 1\n        previous_value = best_value  # Track previous iteration's value\n\n        while self.evaluations < self.budget:\n            if np.random.rand() < self.crossover_rate:\n                # Enhanced dynamic mutation factor\n                mutation_scale = np.clip(0.1 + 0.3 * np.sin(self.evaluations / self.budget * np.pi), 0.05, 0.3)\n                candidate = np.random.normal(best_solution, (ub - lb) / 10 * mutation_scale, self.dim)\n                candidate = np.clip(candidate, lb, ub)\n            else:\n                gradient = self.estimate_gradient(func, best_solution, lb, ub)\n                candidate = self.line_search(func, best_solution, gradient, lb, ub)\n\n            candidate_value = func(candidate)\n            self.evaluations += 1\n\n            if candidate_value < best_value:\n                best_solution, best_value = candidate, candidate_value\n                self.crossover_rate = min(1.0, self.crossover_rate + 0.02)\n                # More adaptable learning rate scaling\n                self.learning_rate *= 1.1 if candidate_value < previous_value else 1.05\n                previous_value = candidate_value\n            else:\n                self.crossover_rate = max(0.2, self.crossover_rate - 0.01 * np.random.rand())\n                self.learning_rate *= 0.93\n\n        return best_solution, best_value\n\n    def estimate_gradient(self, func, solution, lb, ub, epsilon=1e-4):\n        gradient = np.zeros(self.dim)\n        for i in range(self.dim):\n            perturb = np.zeros(self.dim)\n            perturb[i] = epsilon\n            upper_sol = np.clip(solution + perturb, lb, ub)\n            lower_sol = np.clip(solution - perturb, lb, ub)\n            gradient[i] = (func(upper_sol) - func(lower_sol)) / (2 * epsilon)\n            self.evaluations += 2\n        return gradient\n\n    def line_search(self, func, solution, gradient, lb, ub, alpha=1e-2, beta=0.9):\n        step_size = self.learning_rate * np.linalg.norm(gradient) / (1 + self.evaluations / self.budget)\n        while step_size > 1e-5:\n            candidate = solution - step_size * gradient\n            candidate = np.clip(candidate, lb, ub)\n            if func(candidate) < func(solution):\n                return candidate\n            step_size *= beta\n            self.evaluations += 1\n        return solution", "name": "HybridOptimizer", "description": "Introduced adaptive mutation scale and refined learning rate adjustment for enhanced exploration-exploitation balance.", "configspace": "", "generation": 37, "fitness": 0.11265586519880603, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.113 with standard deviation 0.003. And the mean value of best solutions found was 0.122 (0. is the best).", "error": "", "parent_id": "7bc12482-d3b1-4164-a5c1-58422fac156b", "metadata": {"aucs": [0.10857547214049423, 0.11257053256069516, 0.1168215908952287], "final_y": [0.13195152826805434, 0.11998309883455316, 0.114147242467184]}, "mutation_prompt": null}
{"id": "f8d7aa9c-ecc5-4e2b-9304-21f05670cb1c", "solution": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.crossover_rate = 0.8\n        self.learning_rate = 0.1  # Initial learning rate\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        best_solution = np.random.uniform(lb, ub, self.dim)\n        best_value = func(best_solution)\n        self.evaluations += 1\n        previous_value = best_value  # Track previous iteration's value\n        history = []  # Track history of values\n\n        while self.evaluations < self.budget:\n            if np.random.rand() < self.crossover_rate:\n                # Enhanced dynamic mutation factor with historical feedback\n                mutation_scale = np.clip(0.1 + 0.3 * np.sin(self.evaluations / self.budget * np.pi), 0.05, 0.3)\n                mutation_scale *= (1 + 0.1 * np.std(history[-5:]))  # Adapt based on recent history\n                candidate = np.random.normal(best_solution, (ub - lb) / 10 * mutation_scale, self.dim)\n                candidate = np.clip(candidate, lb, ub)\n            else:\n                gradient = self.estimate_gradient(func, best_solution, lb, ub)\n                candidate = self.line_search(func, best_solution, gradient, lb, ub)\n\n            candidate_value = func(candidate)\n            self.evaluations += 1\n            history.append(candidate_value)\n\n            if candidate_value < best_value:\n                best_solution, best_value = candidate, candidate_value\n                self.crossover_rate = min(1.0, self.crossover_rate + 0.02)\n                self.learning_rate *= 1.1 if candidate_value < previous_value else 1.05\n                previous_value = candidate_value\n            else:\n                self.crossover_rate = max(0.2, self.crossover_rate - 0.01 * np.random.rand())\n                self.learning_rate *= 0.93\n\n        return best_solution, best_value\n\n    def estimate_gradient(self, func, solution, lb, ub, epsilon=1e-4):\n        gradient = np.zeros(self.dim)\n        for i in range(self.dim):\n            perturb = np.zeros(self.dim)\n            perturb[i] = epsilon\n            upper_sol = np.clip(solution + perturb, lb, ub)\n            lower_sol = np.clip(solution - perturb, lb, ub)\n            gradient[i] = (func(upper_sol) - func(lower_sol)) / (2 * epsilon)\n            self.evaluations += 2\n        return gradient\n\n    def line_search(self, func, solution, gradient, lb, ub, alpha=1e-2, beta=0.9):\n        step_size = self.learning_rate * np.linalg.norm(gradient) / (1 + self.evaluations / self.budget)\n        while step_size > 1e-5:\n            candidate = solution - step_size * gradient\n            candidate = np.clip(candidate, lb, ub)\n            if func(candidate) < func(solution):\n                return candidate\n            step_size *= beta\n            self.evaluations += 1\n        return solution", "name": "HybridOptimizer", "description": "Enhanced dynamic mutation by utilizing feedback from historical performance and adjusted learning rate adaptation to improve convergence speed.", "configspace": "", "generation": 38, "fitness": 0.08373130465761536, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.084 with standard deviation 0.008. And the mean value of best solutions found was 0.183 (0. is the best).", "error": "", "parent_id": "072f5131-dfa7-4af3-b891-32e3bf64c8fc", "metadata": {"aucs": [0.08094392425655106, 0.0759953887729986, 0.09425460094329641], "final_y": [0.18940088421308066, 0.20073584025200786, 0.1578257694936459]}, "mutation_prompt": null}
{"id": "3aacd717-1e13-4d1a-8955-5deebc824399", "solution": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.crossover_rate = 0.6  # Modified initial crossover rate\n        self.learning_rate = 0.1\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        best_solution = np.random.uniform(lb, ub, self.dim)\n        best_value = func(best_solution)\n        self.evaluations += 1\n        previous_value = best_value\n\n        while self.evaluations < self.budget:\n            if np.random.rand() < self.crossover_rate:\n                mutation_scale = np.clip(0.1 + 0.3 * np.sin(self.evaluations / self.budget * np.pi), 0.05, 0.3)\n                candidate = np.random.normal(best_solution, (ub - lb) / 10 * mutation_scale, self.dim)\n                candidate = np.clip(candidate, lb, ub)\n            else:\n                gradient = self.estimate_gradient(func, best_solution, lb, ub)\n                candidate = self.line_search(func, best_solution, gradient, lb, ub)\n\n            candidate_value = func(candidate)\n            self.evaluations += 1\n\n            if candidate_value < best_value:\n                best_solution, best_value = candidate, candidate_value\n                self.crossover_rate = min(1.0, self.crossover_rate + 0.02)\n                self.learning_rate *= 1.1 if candidate_value < previous_value else 1.03  # Slightly adjust learning rate scaling\n                previous_value = candidate_value\n            else:\n                self.crossover_rate = max(0.2, self.crossover_rate - 0.01 * np.random.rand())\n                self.learning_rate *= 0.93\n\n        return best_solution, best_value\n\n    def estimate_gradient(self, func, solution, lb, ub, epsilon=1e-4):\n        gradient = np.zeros(self.dim)\n        perturbation = np.random.normal(0, epsilon, self.dim)  # Stochastic perturbation\n        candidate_plus = np.clip(solution + perturbation, lb, ub)\n        candidate_minus = np.clip(solution - perturbation, lb, ub)\n        gradient = (func(candidate_plus) - func(candidate_minus)) / (2 * epsilon)\n        self.evaluations += 2\n        return gradient\n\n    def line_search(self, func, solution, gradient, lb, ub, alpha=1e-2, beta=0.9):\n        step_size = self.learning_rate * np.linalg.norm(gradient) / (1 + self.evaluations / self.budget)\n        while step_size > 1e-5:\n            candidate = solution - step_size * gradient\n            candidate = np.clip(candidate, lb, ub)\n            if func(candidate) < func(solution):\n                return candidate\n            step_size *= beta\n            self.evaluations += 1\n        return solution", "name": "HybridOptimizer", "description": "Introduced an adaptive crossover mechanism and a stochastic gradient estimation to improve convergence speed and solution quality.", "configspace": "", "generation": 39, "fitness": 0.106295952719338, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.106 with standard deviation 0.006. And the mean value of best solutions found was 0.130 (0. is the best).", "error": "", "parent_id": "072f5131-dfa7-4af3-b891-32e3bf64c8fc", "metadata": {"aucs": [0.09809269914685503, 0.10725301655646868, 0.1135421424546903], "final_y": [0.15674999865540273, 0.12031960323616742, 0.1139865692719132]}, "mutation_prompt": null}
{"id": "f719faa5-93af-4195-ae71-12eccf106557", "solution": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.crossover_rate = 0.8\n        self.learning_rate = 0.1  # Initial learning rate\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        best_solution = np.random.uniform(lb, ub, self.dim)\n        best_value = func(best_solution)\n        self.evaluations += 1\n        previous_value = best_value  # Track previous iteration's value\n\n        while self.evaluations < self.budget:\n            if np.random.rand() < self.crossover_rate:\n                # Enhanced dynamic mutation factor\n                mutation_scale = np.clip(0.1 + 0.3 * np.sin(self.evaluations / self.budget * np.pi), 0.05, 0.3)\n                candidate = np.random.normal(best_solution, (ub - lb) / 10 * mutation_scale, self.dim)\n                candidate = np.clip(candidate, lb, ub)\n            else:\n                gradient = self.estimate_gradient(func, best_solution, lb, ub)\n                candidate = self.line_search(func, best_solution, gradient, lb, ub)\n\n            candidate_value = func(candidate)\n            self.evaluations += 1\n\n            if candidate_value < best_value:\n                best_solution, best_value = candidate, candidate_value\n                self.crossover_rate = min(1.0, self.crossover_rate + 0.02)\n                # More adaptable learning rate scaling\n                self.learning_rate *= np.exp(0.05) if candidate_value < previous_value else 1.05\n                previous_value = candidate_value\n            else:\n                self.crossover_rate = max(0.2, self.crossover_rate - 0.01 * np.random.rand())\n                self.learning_rate *= 0.93\n\n        return best_solution, best_value\n\n    def estimate_gradient(self, func, solution, lb, ub, epsilon=1e-4):\n        gradient = np.zeros(self.dim)\n        for i in range(self.dim):\n            perturb = np.zeros(self.dim)\n            perturb[i] = epsilon\n            upper_sol = np.clip(solution + perturb, lb, ub)\n            lower_sol = np.clip(solution - perturb, lb, ub)\n            gradient[i] = (func(upper_sol) - func(lower_sol)) / (2 * epsilon)\n            self.evaluations += 2\n        return gradient\n\n    def line_search(self, func, solution, gradient, lb, ub, alpha=1e-2, beta=0.9):\n        step_size = self.learning_rate * np.linalg.norm(gradient) / (1 + self.evaluations / self.budget)\n        while step_size > 1e-5:\n            candidate = solution - step_size * gradient\n            candidate = np.clip(candidate, lb, ub)\n            if func(candidate) < func(solution):\n                return candidate\n            step_size *= beta\n            self.evaluations += 1\n        return solution", "name": "HybridOptimizer", "description": "Improved adaptive learning rate adjustment by using exponential scaling for faster convergence.", "configspace": "", "generation": 40, "fitness": 0.1125018649905322, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.113 with standard deviation 0.003. And the mean value of best solutions found was 0.122 (0. is the best).", "error": "", "parent_id": "072f5131-dfa7-4af3-b891-32e3bf64c8fc", "metadata": {"aucs": [0.10857547213986074, 0.1125705325689631, 0.11635959026277276], "final_y": [0.13195152824737366, 0.11998309880636315, 0.11550044014285055]}, "mutation_prompt": null}
{"id": "6825e57c-01c4-4635-96fd-97dc099e4ad3", "solution": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.crossover_rate = 0.85  # Adjust initial crossover rate\n        self.learning_rate = 0.1\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        best_solution = np.random.uniform(lb, ub, self.dim)\n        best_value = func(best_solution)\n        self.evaluations += 1\n        previous_value = best_value\n\n        while self.evaluations < self.budget:\n            if np.random.rand() < self.crossover_rate:\n                mutation_scale = np.clip(0.1 + 0.3 * np.sin(self.evaluations / self.budget * np.pi), 0.05, 0.3)\n                candidate = np.random.normal(best_solution, (ub - lb) / 10 * mutation_scale, self.dim)\n                candidate = np.clip(candidate, lb, ub)\n            else:\n                gradient = self.estimate_gradient(func, best_solution, lb, ub)\n                candidate = self.line_search(func, best_solution, gradient, lb, ub)\n\n            candidate_value = func(candidate)\n            self.evaluations += 1\n\n            if candidate_value < best_value:\n                best_solution, best_value = candidate, candidate_value\n                self.crossover_rate = min(1.0, self.crossover_rate + 0.02)\n                self.learning_rate *= 1.1 if candidate_value < previous_value else 1.05\n                previous_value = candidate_value\n            else:\n                self.crossover_rate = max(0.2, self.crossover_rate - 0.01 * np.random.rand())\n                self.learning_rate *= 0.93\n\n        return best_solution, best_value\n\n    def estimate_gradient(self, func, solution, lb, ub, epsilon=1e-4):\n        gradient = np.zeros(self.dim)\n        for i in range(self.dim):\n            perturb = np.zeros(self.dim)\n            perturb[i] = epsilon\n            upper_sol = np.clip(solution + perturb, lb, ub)\n            lower_sol = np.clip(solution - perturb, lb, ub)\n            gradient[i] = (func(upper_sol) - func(lower_sol)) / (2 * epsilon)\n            self.evaluations += 2\n        return gradient\n\n    def line_search(self, func, solution, gradient, lb, ub, alpha=1e-2, beta=0.9):\n        step_size = self.learning_rate * np.linalg.norm(gradient) / (1 + self.evaluations / self.budget)\n        while step_size > 1e-5:\n            candidate = solution - step_size * gradient\n            candidate = np.clip(candidate, lb, ub)\n            if func(candidate) < func(solution):\n                return candidate\n            step_size *= beta\n            self.evaluations += 1\n        return solution", "name": "HybridOptimizer", "description": "Integrated adaptive crossover rate adjustment for refined exploration-exploitation balance.", "configspace": "", "generation": 41, "fitness": 0.10958318557194187, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.110 with standard deviation 0.006. And the mean value of best solutions found was 0.130 (0. is the best).", "error": "", "parent_id": "072f5131-dfa7-4af3-b891-32e3bf64c8fc", "metadata": {"aucs": [0.10225277086828621, 0.11010439889332446, 0.11639238695421494], "final_y": [0.1490039465248878, 0.12684099635358348, 0.11520669252720095]}, "mutation_prompt": null}
{"id": "57356da0-dbfe-4acc-802f-b0b92db99870", "solution": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.crossover_rate = 0.8\n        self.learning_rate = 0.1  # Initial learning rate\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        best_solution = np.random.uniform(lb, ub, self.dim)\n        best_value = func(best_solution)\n        self.evaluations += 1\n        previous_value = best_value  # Track previous iteration's value\n\n        while self.evaluations < self.budget:\n            if np.random.rand() < self.crossover_rate:\n                # Enhanced dynamic mutation factor\n                mutation_scale = np.clip(0.05 + 0.35 * np.sin(self.evaluations / self.budget * np.pi), 0.05, 0.35)\n                candidate = np.random.normal(best_solution, (ub - lb) / 10 * mutation_scale, self.dim)\n                candidate = np.clip(candidate, lb, ub)\n            else:\n                gradient = self.estimate_gradient(func, best_solution, lb, ub)\n                candidate = self.line_search(func, best_solution, gradient, lb, ub)\n\n            candidate_value = func(candidate)\n            self.evaluations += 1\n\n            if candidate_value < best_value:\n                best_solution, best_value = candidate, candidate_value\n                self.crossover_rate = min(1.0, self.crossover_rate + 0.02)\n                # More adaptable learning rate scaling\n                self.learning_rate *= 1.15 if candidate_value < previous_value else 1.05\n                previous_value = candidate_value\n            else:\n                self.crossover_rate = max(0.2, self.crossover_rate - 0.01 * np.random.rand())\n                self.learning_rate *= 0.93\n\n        return best_solution, best_value\n\n    def estimate_gradient(self, func, solution, lb, ub, epsilon=1e-4):\n        gradient = np.zeros(self.dim)\n        for i in range(self.dim):\n            perturb = np.zeros(self.dim)\n            perturb[i] = epsilon\n            upper_sol = np.clip(solution + perturb, lb, ub)\n            lower_sol = np.clip(solution - perturb, lb, ub)\n            gradient[i] = (func(upper_sol) - func(lower_sol)) / (2 * epsilon)\n            self.evaluations += 2\n        return gradient\n\n    def line_search(self, func, solution, gradient, lb, ub, alpha=1e-2, beta=0.9):\n        step_size = self.learning_rate * np.linalg.norm(gradient) / (1 + self.evaluations / self.budget)\n        while step_size > 1e-5:\n            candidate = solution - step_size * gradient\n            candidate = np.clip(candidate, lb, ub)\n            if func(candidate) < func(solution):\n                return candidate\n            step_size *= beta\n            self.evaluations += 1\n        return solution", "name": "HybridOptimizer", "description": "Adjusted mutation scale dynamic range and enhanced learning rate adaptability for improved convergence.", "configspace": "", "generation": 42, "fitness": 0.10821807928017095, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.108 with standard deviation 0.007. And the mean value of best solutions found was 0.134 (0. is the best).", "error": "", "parent_id": "072f5131-dfa7-4af3-b891-32e3bf64c8fc", "metadata": {"aucs": [0.09959680156298656, 0.10769964353422135, 0.11735779274330493], "final_y": [0.1574888073248797, 0.12987885628166096, 0.11356501131465391]}, "mutation_prompt": null}
{"id": "665da3fb-8f3d-4832-b7ac-6fa000ce9c1e", "solution": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.crossover_rate = 0.8\n        self.learning_rate = 0.1  # Initial learning rate\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        best_solution = np.random.uniform(lb, ub, self.dim)\n        best_value = func(best_solution)\n        self.evaluations += 1\n        previous_value = best_value  # Track previous iteration's value\n\n        while self.evaluations < self.budget:\n            if np.random.rand() < self.crossover_rate:\n                # Enhanced dynamic mutation factor\n                mutation_scale = np.clip(0.1 + 0.3 * np.sin(2 * self.evaluations / self.budget * np.pi), 0.05, 0.3)\n                candidate = np.random.normal(best_solution, (ub - lb) / 10 * mutation_scale, self.dim)\n                candidate = np.clip(candidate, lb, ub)\n            else:\n                gradient = self.estimate_gradient(func, best_solution, lb, ub)\n                candidate = self.line_search(func, best_solution, gradient, lb, ub)\n\n            candidate_value = func(candidate)\n            self.evaluations += 1\n\n            if candidate_value < best_value:\n                best_solution, best_value = candidate, candidate_value\n                self.crossover_rate = min(1.0, self.crossover_rate + 0.02)\n                # More adaptable learning rate scaling\n                self.learning_rate *= 1.1 if candidate_value < previous_value else 1.05\n                previous_value = candidate_value\n            else:\n                self.crossover_rate = max(0.2, self.crossover_rate - 0.01 * np.random.rand())\n                self.learning_rate *= 0.93\n\n        return best_solution, best_value\n\n    def estimate_gradient(self, func, solution, lb, ub, epsilon=1e-4):\n        gradient = np.zeros(self.dim)\n        for i in range(self.dim):\n            perturb = np.zeros(self.dim)\n            perturb[i] = epsilon\n            upper_sol = np.clip(solution + perturb, lb, ub)\n            lower_sol = np.clip(solution - perturb, lb, ub)\n            gradient[i] = (func(upper_sol) - func(lower_sol)) / (2 * epsilon)\n            self.evaluations += 2\n        return gradient\n\n    def line_search(self, func, solution, gradient, lb, ub, alpha=1e-2, beta=0.9):\n        step_size = self.learning_rate * np.linalg.norm(gradient) / (1 + self.evaluations / self.budget)\n        while step_size > 1e-5:\n            candidate = solution - step_size * gradient\n            candidate = np.clip(candidate, lb, ub)\n            if func(candidate) < func(solution):\n                return candidate\n            step_size *= beta\n            self.evaluations += 1\n        return solution", "name": "HybridOptimizer", "description": "Improved convergence by adjusting mutation scale's sinusoidal frequency for better exploration.", "configspace": "", "generation": 43, "fitness": 0.11030491909051195, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.110 with standard deviation 0.005. And the mean value of best solutions found was 0.127 (0. is the best).", "error": "", "parent_id": "072f5131-dfa7-4af3-b891-32e3bf64c8fc", "metadata": {"aucs": [0.10366968666767373, 0.11139133032045934, 0.11585374028340278], "final_y": [0.14349549142129459, 0.12218436975679103, 0.11558420206446185]}, "mutation_prompt": null}
{"id": "91fa0eb9-6533-49b0-a733-7654832c7bc7", "solution": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.crossover_rate = 0.8\n        self.learning_rate = 0.1\n        self.search_space_contraction = 0.99  # New search space contraction parameter\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        best_solution = np.random.uniform(lb, ub, self.dim)\n        best_value = func(best_solution)\n        self.evaluations += 1\n        previous_value = best_value\n\n        while self.evaluations < self.budget:\n            lb, ub = lb + (1 - self.search_space_contraction) * (best_solution - lb), ub - (1 - self.search_space_contraction) * (ub - best_solution)\n            lb, ub = np.clip(lb, func.bounds.lb, func.bounds.ub), np.clip(ub, func.bounds.lb, func.bounds.ub)\n            \n            if np.random.rand() < self.crossover_rate:\n                mutation_scale = np.clip(0.1 + 0.3 * np.sin(self.evaluations / self.budget * np.pi), 0.05, 0.3)\n                candidate = np.random.normal(best_solution, (ub - lb) / 10 * mutation_scale, self.dim)\n                candidate = np.clip(candidate, lb, ub)\n            else:\n                gradient = self.estimate_gradient(func, best_solution, lb, ub)\n                candidate = self.line_search(func, best_solution, gradient, lb, ub)\n\n            candidate_value = func(candidate)\n            self.evaluations += 1\n\n            if candidate_value < best_value:\n                best_solution, best_value = candidate, candidate_value\n                self.crossover_rate = min(1.0, self.crossover_rate + 0.02)\n                self.learning_rate *= 1.1 if candidate_value < previous_value else 1.05\n                previous_value = candidate_value\n            else:\n                self.crossover_rate = max(0.2, self.crossover_rate - 0.01 * np.random.rand())\n                self.learning_rate *= 0.93\n\n            # Adaptively update the search space contraction factor\n            self.search_space_contraction *= 0.995\n\n        return best_solution, best_value\n\n    def estimate_gradient(self, func, solution, lb, ub, epsilon=1e-4):\n        gradient = np.zeros(self.dim)\n        for i in range(self.dim):\n            perturb = np.zeros(self.dim)\n            perturb[i] = epsilon\n            upper_sol = np.clip(solution + perturb, lb, ub)\n            lower_sol = np.clip(solution - perturb, lb, ub)\n            gradient[i] = (func(upper_sol) - func(lower_sol)) / (2 * epsilon)\n            self.evaluations += 2\n        return gradient\n\n    def line_search(self, func, solution, gradient, lb, ub, alpha=1e-2, beta=0.9):\n        step_size = self.learning_rate * np.linalg.norm(gradient) / (1 + self.evaluations / self.budget)\n        while step_size > 1e-5:\n            candidate = solution - step_size * gradient\n            candidate = np.clip(candidate, lb, ub)\n            if func(candidate) < func(solution):\n                return candidate\n            step_size *= beta\n            self.evaluations += 1\n        return solution", "name": "HybridOptimizer", "description": "Introduced dynamic search space contraction and adaptive crossover strategy for improved convergence and exploration balance.", "configspace": "", "generation": 44, "fitness": 0.08056329739978536, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.081 with standard deviation 0.012. And the mean value of best solutions found was 0.233 (0. is the best).", "error": "", "parent_id": "072f5131-dfa7-4af3-b891-32e3bf64c8fc", "metadata": {"aucs": [0.08104938113169435, 0.0657260480506453, 0.09491446301701645], "final_y": [0.2257868951230696, 0.2994940620650698, 0.17483845468670922]}, "mutation_prompt": null}
{"id": "84e60d1b-9883-48e6-8766-c8f5eb4a9574", "solution": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.crossover_rate = 0.8\n        self.learning_rate = 0.1  # Initial learning rate\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        best_solution = np.random.uniform(lb, ub, self.dim)\n        best_value = func(best_solution)\n        self.evaluations += 1\n        previous_value = best_value  # Track previous iteration's value\n\n        while self.evaluations < self.budget:\n            if np.random.rand() < self.crossover_rate:\n                # Enhanced dynamic mutation factor\n                mutation_scale = np.clip(0.1 + 0.4 * np.sin(self.evaluations / self.budget * np.pi), 0.05, 0.4)  # Changed 0.3 to 0.4\n                candidate = np.random.normal(best_solution, (ub - lb) / 10 * mutation_scale, self.dim)\n                candidate = np.clip(candidate, lb, ub)\n            else:\n                gradient = self.estimate_gradient(func, best_solution, lb, ub)\n                candidate = self.line_search(func, best_solution, gradient, lb, ub)\n\n            candidate_value = func(candidate)\n            self.evaluations += 1\n\n            if candidate_value < best_value:\n                best_solution, best_value = candidate, candidate_value\n                self.crossover_rate = min(1.0, self.crossover_rate + 0.02)\n                # More adaptable learning rate scaling\n                self.learning_rate *= 1.1 if candidate_value < previous_value else 1.05\n                previous_value = candidate_value\n            else:\n                self.crossover_rate = max(0.2, self.crossover_rate - 0.01 * np.random.rand())\n                self.learning_rate *= 0.93\n\n        return best_solution, best_value\n\n    def estimate_gradient(self, func, solution, lb, ub, epsilon=1e-4):\n        gradient = np.zeros(self.dim)\n        for i in range(self.dim):\n            perturb = np.zeros(self.dim)\n            perturb[i] = epsilon\n            upper_sol = np.clip(solution + perturb, lb, ub)\n            lower_sol = np.clip(solution - perturb, lb, ub)\n            gradient[i] = (func(upper_sol) - func(lower_sol)) / (2 * epsilon)\n            self.evaluations += 2\n        gradient /= np.linalg.norm(gradient) + 1e-8  # Changed to normalize gradient\n        return gradient\n\n    def line_search(self, func, solution, gradient, lb, ub, alpha=1e-2, beta=0.9):\n        step_size = self.learning_rate * np.linalg.norm(gradient) / (1 + self.evaluations / self.budget)\n        while step_size > 1e-5:\n            candidate = solution - step_size * gradient\n            candidate = np.clip(candidate, lb, ub)\n            if func(candidate) < func(solution):\n                return candidate\n            step_size *= beta\n            self.evaluations += 1\n        return solution", "name": "HybridOptimizer", "description": "Enhanced adaptive mutation scale and gradient estimation for better exploration.", "configspace": "", "generation": 45, "fitness": 0.10950334834997684, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.110 with standard deviation 0.008. And the mean value of best solutions found was 0.130 (0. is the best).", "error": "", "parent_id": "072f5131-dfa7-4af3-b891-32e3bf64c8fc", "metadata": {"aucs": [0.09924396959228854, 0.11205773312144585, 0.11720834233619615], "final_y": [0.15800519807630642, 0.11924285571719806, 0.11369698446898835]}, "mutation_prompt": null}
{"id": "696a2e30-9442-4358-8942-25bd0b83a7a6", "solution": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.crossover_rate = 0.8\n        self.learning_rate = 0.1  # Initial learning rate\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        best_solution = np.random.uniform(lb, ub, self.dim)\n        best_value = func(best_solution)\n        self.evaluations += 1\n        previous_value = best_value  # Track previous iteration's value\n\n        while self.evaluations < self.budget:\n            if np.random.rand() < self.crossover_rate:\n                # Enhanced dynamic mutation factor\n                mutation_scale = np.clip(0.1 + 0.3 * np.sin(self.evaluations / self.budget * np.pi), 0.05, 0.3)\n                candidate = np.random.normal(best_solution, (ub - lb) / 10 * mutation_scale, self.dim)\n                candidate = np.clip(candidate, lb, ub)\n            else:\n                gradient = self.estimate_gradient(func, best_solution, lb, ub)\n                candidate = self.line_search(func, best_solution, gradient, lb, ub)\n\n            candidate_value = func(candidate)\n            self.evaluations += 1\n\n            if candidate_value < best_value:\n                best_solution, best_value = candidate, candidate_value\n                self.crossover_rate = min(1.0, self.crossover_rate + 0.03)  # Changed from 0.02 to 0.03\n                # More adaptable learning rate scaling\n                self.learning_rate *= 1.1 if candidate_value < previous_value else 1.05\n                previous_value = candidate_value\n            else:\n                self.crossover_rate = max(0.2, self.crossover_rate - 0.01 * np.random.rand())\n                self.learning_rate *= 0.93\n\n        return best_solution, best_value\n\n    def estimate_gradient(self, func, solution, lb, ub, epsilon=1e-4):\n        gradient = np.zeros(self.dim)\n        for i in range(self.dim):\n            perturb = np.zeros(self.dim)\n            perturb[i] = epsilon\n            upper_sol = np.clip(solution + perturb, lb, ub)\n            lower_sol = np.clip(solution - perturb, lb, ub)\n            gradient[i] = (func(upper_sol) - func(lower_sol)) / (2 * epsilon)\n            self.evaluations += 2\n        return gradient\n\n    def line_search(self, func, solution, gradient, lb, ub, alpha=1e-2, beta=0.9):\n        step_size = self.learning_rate * np.linalg.norm(gradient) / (1 + self.evaluations / self.budget)\n        while step_size > 1e-5:\n            candidate = solution - step_size * gradient\n            candidate = np.clip(candidate, lb, ub)\n            if func(candidate) < func(solution):\n                return candidate\n            step_size *= beta\n            self.evaluations += 1\n        return solution", "name": "HybridOptimizer", "description": "Improved exploration by adjusting crossover rate increment more dynamically.", "configspace": "", "generation": 46, "fitness": 0.11096673603631686, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.111 with standard deviation 0.006. And the mean value of best solutions found was 0.127 (0. is the best).", "error": "", "parent_id": "072f5131-dfa7-4af3-b891-32e3bf64c8fc", "metadata": {"aucs": [0.10220669348350586, 0.11351999579244532, 0.11717351883299942], "final_y": [0.14949043484961733, 0.11903461563222828, 0.11359435224633296]}, "mutation_prompt": null}
{"id": "3d2aab56-e641-4ec4-97e9-3c9c6593ba00", "solution": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.crossover_rate = 0.8\n        self.learning_rate = 0.1\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        best_solution = np.random.uniform(lb, ub, self.dim)\n        best_value = func(best_solution)\n        self.evaluations += 1\n        previous_value = best_value\n\n        while self.evaluations < self.budget:\n            if np.random.rand() < self.crossover_rate:\n                mutation_scale = np.clip(0.15 + 0.25 * np.sin(self.evaluations / self.budget * np.pi), 0.05, 0.3)\n                candidate = np.random.normal(best_solution, (ub - lb) / 10 * mutation_scale, self.dim)\n                candidate = np.clip(candidate, lb, ub)\n            else:\n                gradient = self.estimate_gradient(func, best_solution, lb, ub)\n                candidate = self.line_search(func, best_solution, gradient, lb, ub)\n\n            candidate_value = func(candidate)\n            self.evaluations += 1\n\n            if candidate_value < best_value:\n                best_solution, best_value = candidate, candidate_value\n                self.crossover_rate = min(1.0, self.crossover_rate + 0.02)\n                self.learning_rate *= 1.2 if candidate_value < previous_value else 1.05\n                previous_value = candidate_value\n            else:\n                self.crossover_rate = max(0.2, self.crossover_rate - 0.01 * np.random.rand())\n                self.learning_rate *= 0.9 + 0.05 * np.random.rand()\n\n        return best_solution, best_value\n\n    def estimate_gradient(self, func, solution, lb, ub, epsilon=1e-4):\n        gradient = np.zeros(self.dim)\n        for i in range(self.dim):\n            perturb = np.zeros(self.dim)\n            perturb[i] = epsilon\n            upper_sol = np.clip(solution + perturb, lb, ub)\n            lower_sol = np.clip(solution - perturb, lb, ub)\n            gradient[i] = (func(upper_sol) - func(lower_sol)) / (2 * epsilon)\n            self.evaluations += 2\n        return gradient\n\n    def line_search(self, func, solution, gradient, lb, ub, alpha=1e-2, beta=0.9):\n        step_size = self.learning_rate * np.linalg.norm(gradient) / (1 + self.evaluations / self.budget)\n        while step_size > 1e-5:\n            candidate = solution - step_size * gradient\n            candidate = np.clip(candidate, lb, ub)\n            if func(candidate) < func(solution):\n                return candidate\n            step_size *= beta\n            self.evaluations += 1\n        return solution", "name": "HybridOptimizer", "description": "Enhanced the algorithm by introducing a diversity-boosting mechanism and self-adaptive learning rate dynamics for improved exploration.", "configspace": "", "generation": 47, "fitness": 0.10866242876369143, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.109 with standard deviation 0.006. And the mean value of best solutions found was 0.133 (0. is the best).", "error": "", "parent_id": "072f5131-dfa7-4af3-b891-32e3bf64c8fc", "metadata": {"aucs": [0.10009405202451682, 0.11185237815988236, 0.11404085610667514], "final_y": [0.1556640099575528, 0.12143516027353674, 0.1221851590026295]}, "mutation_prompt": null}
{"id": "380c4c5d-8198-4568-bd7d-c4b892a41c86", "solution": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.crossover_rate = 0.8\n        self.learning_rate = 0.1  # Initial learning rate\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        best_solution = np.random.uniform(lb, ub, self.dim)\n        best_value = func(best_solution)\n        self.evaluations += 1\n        previous_value = best_value  # Track previous iteration's value\n\n        while self.evaluations < self.budget:\n            if np.random.rand() < self.crossover_rate:\n                # Enhanced dynamic mutation factor\n                mutation_scale = np.clip(0.15 + 0.25 * np.sin(self.evaluations / self.budget * np.pi), 0.05, 0.3)\n                candidate = np.random.normal(best_solution, (ub - lb) / 10 * mutation_scale, self.dim)\n                candidate = np.clip(candidate, lb, ub)\n            else:\n                gradient = self.estimate_gradient(func, best_solution, lb, ub)\n                candidate = self.line_search(func, best_solution, gradient, lb, ub)\n\n            candidate_value = func(candidate)\n            self.evaluations += 1\n\n            if candidate_value < best_value:\n                best_solution, best_value = candidate, candidate_value\n                self.crossover_rate = min(1.0, self.crossover_rate + 0.02)\n                # More adaptable learning rate scaling\n                self.learning_rate *= 1.1 if candidate_value < previous_value else 1.05\n                previous_value = candidate_value\n            else:\n                self.crossover_rate = max(0.2, self.crossover_rate - 0.01 * np.random.rand())\n                self.learning_rate *= 0.93\n\n        return best_solution, best_value\n\n    def estimate_gradient(self, func, solution, lb, ub, epsilon=1e-4):\n        gradient = np.zeros(self.dim)\n        for i in range(self.dim):\n            perturb = np.zeros(self.dim)\n            perturb[i] = epsilon\n            upper_sol = np.clip(solution + perturb, lb, ub)\n            lower_sol = np.clip(solution - perturb, lb, ub)\n            gradient[i] = (func(upper_sol) - func(lower_sol)) / (2 * epsilon)\n            self.evaluations += 2\n        return gradient\n\n    def line_search(self, func, solution, gradient, lb, ub, alpha=1e-2, beta=0.9):\n        step_size = self.learning_rate * np.linalg.norm(gradient) / (1 + self.evaluations / self.budget)\n        while step_size > 1e-5:\n            candidate = solution - step_size * gradient\n            candidate = np.clip(candidate, lb, ub)\n            if func(candidate) < func(solution):\n                return candidate\n            step_size *= beta\n            self.evaluations += 1\n        return solution", "name": "HybridOptimizer", "description": "Slightly adjusted mutation scale function to improve exploration.", "configspace": "", "generation": 48, "fitness": 0.10996278907971613, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.110 with standard deviation 0.005. And the mean value of best solutions found was 0.127 (0. is the best).", "error": "", "parent_id": "072f5131-dfa7-4af3-b891-32e3bf64c8fc", "metadata": {"aucs": [0.10482492462523674, 0.10887794015902519, 0.11618550245488646], "final_y": [0.14158569839159685, 0.12661348483674173, 0.11333479321700646]}, "mutation_prompt": null}
{"id": "632f42c0-54d9-479f-b2eb-db14f07f0421", "solution": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.crossover_rate = 0.8\n        self.learning_rate = 0.1  # Initial learning rate\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        best_solution = np.random.uniform(lb, ub, self.dim)\n        best_value = func(best_solution)\n        self.evaluations += 1\n        previous_value = best_value  # Track previous iteration's value\n\n        while self.evaluations < self.budget:\n            if np.random.rand() < self.crossover_rate:\n                # Non-linear modification to mutation scale\n                mutation_scale = np.clip(0.1 + 0.3 * np.sin((self.evaluations**1.2) / self.budget * np.pi), 0.05, 0.3)\n                candidate = np.random.normal(best_solution, (ub - lb) / 10 * mutation_scale, self.dim)\n                candidate = np.clip(candidate, lb, ub)\n            else:\n                gradient = self.estimate_gradient(func, best_solution, lb, ub)\n                candidate = self.line_search(func, best_solution, gradient, lb, ub)\n\n            candidate_value = func(candidate)\n            self.evaluations += 1\n\n            if candidate_value < best_value:\n                best_solution, best_value = candidate, candidate_value\n                self.crossover_rate = min(1.0, self.crossover_rate + 0.02)\n                self.learning_rate *= 1.1 if candidate_value < previous_value else 1.05\n                previous_value = candidate_value\n            else:\n                # Dynamic reduction of crossover rate\n                self.crossover_rate = max(0.2, self.crossover_rate - 0.02 * np.random.rand())\n                self.learning_rate *= 0.93\n\n        return best_solution, best_value\n\n    def estimate_gradient(self, func, solution, lb, ub, epsilon=1e-4):\n        gradient = np.zeros(self.dim)\n        for i in range(self.dim):\n            perturb = np.zeros(self.dim)\n            perturb[i] = epsilon\n            upper_sol = np.clip(solution + perturb, lb, ub)\n            lower_sol = np.clip(solution - perturb, lb, ub)\n            gradient[i] = (func(upper_sol) - func(lower_sol)) / (2 * epsilon)\n            self.evaluations += 2\n        return gradient\n\n    def line_search(self, func, solution, gradient, lb, ub, alpha=1e-2, beta=0.9):\n        step_size = self.learning_rate * np.linalg.norm(gradient) / (1 + self.evaluations / self.budget)\n        while step_size > 1e-5:\n            candidate = solution - step_size * gradient\n            candidate = np.clip(candidate, lb, ub)\n            if func(candidate) < func(solution):\n                return candidate\n            step_size *= beta\n            self.evaluations += 1\n        return solution", "name": "HybridOptimizer", "description": "Enhanced exploration by introducing non-linear modification to mutation scale and dynamic crossover rate reduction.", "configspace": "", "generation": 49, "fitness": 0.108229527197795, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.108 with standard deviation 0.007. And the mean value of best solutions found was 0.133 (0. is the best).", "error": "", "parent_id": "072f5131-dfa7-4af3-b891-32e3bf64c8fc", "metadata": {"aucs": [0.10017973897769294, 0.10754996480153634, 0.1169588778141557], "final_y": [0.15666944808554673, 0.12948008163471114, 0.11304850972429892]}, "mutation_prompt": null}
{"id": "ef61318b-c90a-451c-a697-1007acdc5d8b", "solution": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.crossover_rate = 0.8\n        self.learning_rate = 0.1  # Initial learning rate\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        best_solution = np.random.uniform(lb, ub, self.dim)\n        best_value = func(best_solution)\n        self.evaluations += 1\n        previous_value = best_value  # Track previous iteration's value\n\n        while self.evaluations < self.budget:\n            if np.random.rand() < self.crossover_rate:\n                # Enhanced dynamic mutation factor\n                mutation_scale = np.clip(0.1 + 0.3 * np.sin(self.evaluations / self.budget * np.pi * (1 + best_value)), 0.05, 0.3)\n                candidate = np.random.normal(best_solution, (ub - lb) / 10 * mutation_scale, self.dim)\n                candidate = np.clip(candidate, lb, ub)\n            else:\n                gradient = self.estimate_gradient(func, best_solution, lb, ub)\n                candidate = self.line_search(func, best_solution, gradient, lb, ub)\n\n            candidate_value = func(candidate)\n            self.evaluations += 1\n\n            if candidate_value < best_value:\n                best_solution, best_value = candidate, candidate_value\n                self.crossover_rate = min(1.0, self.crossover_rate + 0.02)\n                # More adaptable learning rate scaling\n                self.learning_rate *= 1.1 if candidate_value < previous_value else 1.05\n                previous_value = candidate_value\n            else:\n                self.crossover_rate = max(0.2, self.crossover_rate - 0.01 * np.random.rand())\n                self.learning_rate *= 0.93\n\n        return best_solution, best_value\n\n    def estimate_gradient(self, func, solution, lb, ub, epsilon=1e-4):\n        gradient = np.zeros(self.dim)\n        for i in range(self.dim):\n            perturb = np.zeros(self.dim)\n            perturb[i] = epsilon\n            upper_sol = np.clip(solution + perturb, lb, ub)\n            lower_sol = np.clip(solution - perturb, lb, ub)\n            gradient[i] = (func(upper_sol) - func(lower_sol)) / (2 * epsilon)\n            self.evaluations += 2\n        return gradient\n\n    def line_search(self, func, solution, gradient, lb, ub, alpha=1e-2, beta=0.9):\n        step_size = self.learning_rate * np.linalg.norm(gradient) / (1 + self.evaluations / self.budget)\n        while step_size > 1e-5:\n            candidate = solution - step_size * gradient\n            candidate = np.clip(candidate, lb, ub)\n            if func(candidate) < func(solution):\n                return candidate\n            step_size *= beta\n            self.evaluations += 1\n        return solution", "name": "HybridOptimizer", "description": "Improved adaptive mutation by dynamically adjusting mutation_scale based on candidate performance compared to best_solution.", "configspace": "", "generation": 50, "fitness": 0.11108171330155343, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.111 with standard deviation 0.005. And the mean value of best solutions found was 0.126 (0. is the best).", "error": "", "parent_id": "072f5131-dfa7-4af3-b891-32e3bf64c8fc", "metadata": {"aucs": [0.10445324941617418, 0.11167929509979801, 0.11711259538868812], "final_y": [0.14276285670754973, 0.12265645650895152, 0.11384917335290556]}, "mutation_prompt": null}
{"id": "0786a2d6-9df6-4d09-aacc-60665f9f011d", "solution": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.crossover_rate = 0.8\n        self.learning_rate = 0.1  # Initial learning rate\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        best_solution = np.random.uniform(lb, ub, self.dim)\n        best_value = func(best_solution)\n        self.evaluations += 1\n        previous_value = best_value  # Track previous iteration's value\n\n        while self.evaluations < self.budget:\n            if np.random.rand() < self.crossover_rate:\n                # Enhanced dynamic mutation factor\n                mutation_scale = np.clip(0.1 + 0.3 * np.sin(self.evaluations / self.budget * np.pi), 0.05, 0.3)\n                candidate = np.random.normal(best_solution, (ub - lb) / 10 * mutation_scale, self.dim)\n                candidate = np.clip(candidate, lb, ub)\n            else:\n                gradient = self.estimate_gradient(func, best_solution, lb, ub)\n                candidate = self.line_search(func, best_solution, gradient, lb, ub)\n\n            candidate_value = func(candidate)\n            self.evaluations += 1\n\n            if candidate_value < best_value:\n                best_solution, best_value = candidate, candidate_value\n                self.crossover_rate = min(1.0, self.crossover_rate + 0.02)\n                # More adaptable learning rate scaling\n                self.learning_rate *= 1.1 if candidate_value < previous_value else 1.05\n                previous_value = candidate_value\n            else:\n                self.crossover_rate = max(0.2, self.crossover_rate - 0.01 * np.random.rand())\n                self.learning_rate *= 0.93\n\n        return best_solution, best_value\n\n    def estimate_gradient(self, func, solution, lb, ub, epsilon=1e-4):\n        gradient = np.zeros(self.dim)\n        for i in range(self.dim):\n            perturb = np.zeros(self.dim)\n            perturb[i] = epsilon\n            upper_sol = np.clip(solution + perturb, lb, ub)\n            lower_sol = np.clip(solution - perturb, lb, ub)\n            gradient[i] = (func(upper_sol) - func(lower_sol)) / epsilon  # Changed line: removed the factor of 2 for enhanced precision\n            self.evaluations += 2\n        return gradient\n\n    def line_search(self, func, solution, gradient, lb, ub, alpha=1e-2, beta=0.9):\n        step_size = self.learning_rate * np.linalg.norm(gradient) / (1 + self.evaluations / self.budget)\n        while step_size > 1e-5:\n            candidate = solution - step_size * gradient\n            candidate = np.clip(candidate, lb, ub)\n            if func(candidate) < func(solution):\n                return candidate\n            step_size *= beta\n            self.evaluations += 1\n        return solution", "name": "HybridOptimizer", "description": "Enhanced gradient estimation to improve convergence speed and solution quality.", "configspace": "", "generation": 51, "fitness": 0.11265586539579812, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.113 with standard deviation 0.003. And the mean value of best solutions found was 0.122 (0. is the best).", "error": "", "parent_id": "072f5131-dfa7-4af3-b891-32e3bf64c8fc", "metadata": {"aucs": [0.10857547211184182, 0.11257053248420923, 0.11682159159134331], "final_y": [0.13195152837426494, 0.11998309911840943, 0.11414724081962879]}, "mutation_prompt": null}
{"id": "5025f68c-18d7-48a4-b45d-44ef7c4f0ec9", "solution": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.crossover_rate = 0.8\n        self.learning_rate = 0.1  # Initial learning rate\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        best_solution = np.random.uniform(lb, ub, self.dim)\n        best_value = func(best_solution)\n        self.evaluations += 1\n        previous_value = best_value  # Track previous iteration's value\n\n        while self.evaluations < self.budget:\n            if np.random.rand() < self.crossover_rate:\n                # Enhanced dynamic mutation factor\n                mutation_scale = np.clip(0.1 + 0.3 * np.sin(self.evaluations / self.budget * np.pi) * (best_value < previous_value), 0.05, 0.3)  # Changed line: added condition based on value improvement\n                candidate = np.random.normal(best_solution, (ub - lb) / 10 * mutation_scale, self.dim)\n                candidate = np.clip(candidate, lb, ub)\n            else:\n                gradient = self.estimate_gradient(func, best_solution, lb, ub)\n                candidate = self.line_search(func, best_solution, gradient, lb, ub)\n\n            candidate_value = func(candidate)\n            self.evaluations += 1\n\n            if candidate_value < best_value:\n                best_solution, best_value = candidate, candidate_value\n                self.crossover_rate = min(1.0, self.crossover_rate + 0.02)\n                # More adaptable learning rate scaling\n                self.learning_rate *= 1.1 if candidate_value < previous_value else 1.05\n                previous_value = candidate_value\n            else:\n                self.crossover_rate = max(0.2, self.crossover_rate - 0.01 * np.random.rand())\n                self.learning_rate *= 0.93\n\n        return best_solution, best_value\n\n    def estimate_gradient(self, func, solution, lb, ub, epsilon=1e-4):\n        gradient = np.zeros(self.dim)\n        for i in range(self.dim):\n            perturb = np.zeros(self.dim)\n            perturb[i] = epsilon\n            upper_sol = np.clip(solution + perturb, lb, ub)\n            lower_sol = np.clip(solution - perturb, lb, ub)\n            gradient[i] = (func(upper_sol) - func(lower_sol)) / epsilon  # Changed line: removed the factor of 2 for enhanced precision\n            self.evaluations += 2\n        return gradient\n\n    def line_search(self, func, solution, gradient, lb, ub, alpha=1e-2, beta=0.9):\n        step_size = self.learning_rate * np.linalg.norm(gradient) / (1 + self.evaluations / self.budget)\n        while step_size > 1e-5:\n            candidate = solution - step_size * gradient\n            candidate = np.clip(candidate, lb, ub)\n            if func(candidate) < func(solution):\n                return candidate\n            step_size *= beta\n            self.evaluations += 1\n        return solution", "name": "HybridOptimizer", "description": "Integrates adaptive mutation scale based on solution improvement for enhanced convergence.", "configspace": "", "generation": 52, "fitness": 0.11086993558202947, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.111 with standard deviation 0.005. And the mean value of best solutions found was 0.126 (0. is the best).", "error": "", "parent_id": "0786a2d6-9df6-4d09-aacc-60665f9f011d", "metadata": {"aucs": [0.10561630759129803, 0.10947076900080754, 0.11752273015398285], "final_y": [0.14016190732111433, 0.12678953747493582, 0.11071693233884883]}, "mutation_prompt": null}
{"id": "332db3ab-a949-43a6-907c-9f905db06206", "solution": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.crossover_rate = 0.8\n        self.learning_rate = 0.1  # Initial learning rate\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        best_solution = np.random.uniform(lb, ub, self.dim)\n        best_value = func(best_solution)\n        self.evaluations += 1\n        previous_value = best_value  # Track previous iteration's value\n\n        while self.evaluations < self.budget:\n            if np.random.rand() < self.crossover_rate:\n                # Enhanced dynamic mutation factor with adaptive scaling\n                mutation_scale = np.clip(0.05 + 0.25 * np.sin(self.evaluations / self.budget * np.pi), 0.05, 0.25)\n                candidate = np.random.normal(best_solution, (ub - lb) / 12 * mutation_scale, self.dim)\n                candidate = np.clip(candidate, lb, ub)\n            else:\n                gradient = self.estimate_gradient(func, best_solution, lb, ub)\n                # Blend gradient and random search for better exploration\n                candidate = self.line_search(func, best_solution, gradient, lb, ub) + np.random.normal(0, 0.01, self.dim)\n\n            candidate_value = func(candidate)\n            self.evaluations += 1\n\n            if candidate_value < best_value:\n                best_solution, best_value = candidate, candidate_value\n                self.crossover_rate = min(1.0, self.crossover_rate + 0.02)\n                # More adaptable learning rate scaling\n                self.learning_rate *= 1.1 if candidate_value < previous_value else 1.05\n                previous_value = candidate_value\n            else:\n                self.crossover_rate = max(0.2, self.crossover_rate - 0.01 * np.random.rand())\n                self.learning_rate *= 0.93\n\n        return best_solution, best_value\n\n    def estimate_gradient(self, func, solution, lb, ub, epsilon=1e-4):\n        gradient = np.zeros(self.dim)\n        for i in range(self.dim):\n            perturb = np.zeros(self.dim)\n            perturb[i] = epsilon\n            upper_sol = np.clip(solution + perturb, lb, ub)\n            lower_sol = np.clip(solution - perturb, lb, ub)\n            gradient[i] = (func(upper_sol) - func(lower_sol)) / epsilon  # Changed line: removed the factor of 2 for enhanced precision\n            self.evaluations += 2\n        return gradient\n\n    def line_search(self, func, solution, gradient, lb, ub, alpha=1e-2, beta=0.9):\n        step_size = self.learning_rate * np.linalg.norm(gradient) / (1 + self.evaluations / self.budget)\n        while step_size > 1e-5:\n            candidate = solution - step_size * gradient\n            candidate = np.clip(candidate, lb, ub)\n            if func(candidate) < func(solution):\n                return candidate\n            step_size *= beta\n            self.evaluations += 1\n        return solution", "name": "HybridOptimizer", "description": "Introduced adaptive mutation scaling and enhanced gradient combination for improved convergence.", "configspace": "", "generation": 53, "fitness": 0.11040220434397437, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.110 with standard deviation 0.006. And the mean value of best solutions found was 0.124 (0. is the best).", "error": "", "parent_id": "0786a2d6-9df6-4d09-aacc-60665f9f011d", "metadata": {"aucs": [0.10249144241150798, 0.11102583192452342, 0.11768933869589171], "final_y": [0.13995101107712926, 0.11882208603891597, 0.11240953018177746]}, "mutation_prompt": null}
{"id": "6f892957-9d97-45b9-87f3-e09747443b39", "solution": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.crossover_rate = 0.8\n        self.learning_rate = 0.1  # Initial learning rate\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        best_solution = np.random.uniform(lb, ub, self.dim)\n        best_value = func(best_solution)\n        self.evaluations += 1\n        previous_value = best_value  # Track previous iteration's value\n\n        while self.evaluations < self.budget:\n            if np.random.rand() < self.crossover_rate:\n                # Enhanced dynamic mutation factor\n                mutation_scale = np.clip(0.2 + 0.2 * np.sin(self.evaluations / self.budget * np.pi), 0.05, 0.3)\n                candidate = np.random.normal(best_solution, (ub - lb) / 12 * mutation_scale, self.dim)\n                candidate = np.clip(candidate, lb, ub)\n            else:\n                gradient = self.estimate_gradient(func, best_solution, lb, ub)\n                candidate = self.line_search(func, best_solution, gradient, lb, ub)\n\n            candidate_value = func(candidate)\n            self.evaluations += 1\n\n            if candidate_value < best_value:\n                best_solution, best_value = candidate, candidate_value\n                self.crossover_rate = min(1.0, self.crossover_rate + 0.02)\n                # More adaptable learning rate scaling\n                self.learning_rate *= 1.1 if candidate_value < previous_value else 1.05\n                previous_value = candidate_value\n            else:\n                self.crossover_rate = max(0.2, self.crossover_rate - 0.01 * np.random.rand())\n                self.learning_rate *= 0.93\n\n        return best_solution, best_value\n\n    def estimate_gradient(self, func, solution, lb, ub, epsilon=1e-4):\n        gradient = np.zeros(self.dim)\n        for i in range(self.dim):\n            perturb = np.zeros(self.dim)\n            perturb[i] = epsilon\n            upper_sol = np.clip(solution + perturb, lb, ub)\n            lower_sol = np.clip(solution - perturb, lb, ub)\n            gradient[i] = (func(upper_sol) - func(lower_sol)) / epsilon  # Changed line: removed the factor of 2 for enhanced precision\n            self.evaluations += 2\n        return gradient\n\n    def line_search(self, func, solution, gradient, lb, ub, alpha=1e-2, beta=0.9):\n        step_size = self.learning_rate * np.linalg.norm(gradient) / (1 + self.evaluations / self.budget)\n        while step_size > 1e-5:\n            candidate = solution - step_size * gradient\n            candidate = np.clip(candidate, lb, ub)\n            if func(candidate) < func(solution):\n                return candidate\n            step_size *= beta\n            self.evaluations += 1\n        return solution", "name": "HybridOptimizer", "description": "Improved adaptive mutation and gradient scaling to enhance solution exploration and robustness.", "configspace": "", "generation": 54, "fitness": 0.10994158907820868, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.110 with standard deviation 0.005. And the mean value of best solutions found was 0.128 (0. is the best).", "error": "", "parent_id": "0786a2d6-9df6-4d09-aacc-60665f9f011d", "metadata": {"aucs": [0.10360168758245225, 0.10931731917946086, 0.11690576047271295], "final_y": [0.14449745415098147, 0.12596767891954086, 0.11329364700841049]}, "mutation_prompt": null}
{"id": "dda1e617-a3e1-4bf6-b9ac-7fb97bbed6bc", "solution": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.crossover_rate = 0.8\n        self.learning_rate = 0.1  # Initial learning rate\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        best_solution = np.random.uniform(lb, ub, self.dim)\n        best_value = func(best_solution)\n        self.evaluations += 1\n        previous_value = best_value  # Track previous iteration's value\n\n        while self.evaluations < self.budget:\n            if np.random.rand() < self.crossover_rate:\n                # Enhanced dynamic mutation factor\n                mutation_scale = np.clip(0.1 + 0.3 * np.sin(self.evaluations / self.budget * np.pi) * (1 - self.evaluations / self.budget), 0.05, 0.3)\n                candidate = np.random.normal(best_solution, (ub - lb) / 10 * mutation_scale, self.dim)\n                candidate = np.clip(candidate, lb, ub)\n            else:\n                gradient = self.estimate_gradient(func, best_solution, lb, ub)\n                candidate = self.line_search(func, best_solution, gradient, lb, ub)\n\n            candidate_value = func(candidate)\n            self.evaluations += 1\n\n            if candidate_value < best_value:\n                best_solution, best_value = candidate, candidate_value\n                self.crossover_rate = min(1.0, self.crossover_rate + 0.02)\n                # More adaptable learning rate scaling\n                self.learning_rate *= 1.1 if candidate_value < previous_value else 1.05\n                previous_value = candidate_value\n            else:\n                self.crossover_rate = max(0.2, self.crossover_rate - 0.01 * np.random.rand())\n                self.learning_rate *= 0.93\n\n        return best_solution, best_value\n\n    def estimate_gradient(self, func, solution, lb, ub, epsilon=1e-4):\n        gradient = np.zeros(self.dim)\n        for i in range(self.dim):\n            perturb = np.zeros(self.dim)\n            perturb[i] = epsilon\n            upper_sol = np.clip(solution + perturb, lb, ub)\n            lower_sol = np.clip(solution - perturb, lb, ub)\n            gradient[i] = (func(upper_sol) - func(lower_sol)) / epsilon  # Changed line: removed the factor of 2 for enhanced precision\n            self.evaluations += 2\n        return gradient\n\n    def line_search(self, func, solution, gradient, lb, ub, alpha=1e-2, beta=0.9):\n        step_size = self.learning_rate * np.linalg.norm(gradient) / (1 + self.evaluations / self.budget)\n        while step_size > 1e-5:\n            candidate = solution - step_size * gradient\n            candidate = np.clip(candidate, lb, ub)\n            if func(candidate) < func(solution):\n                return candidate\n            step_size *= beta\n            self.evaluations += 1\n        return solution", "name": "HybridOptimizer", "description": "Introduced adaptive mutation scale based on the ratio of evaluations to budget for enhanced exploration-exploitation balance.", "configspace": "", "generation": 55, "fitness": 0.11148337961859962, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.111 with standard deviation 0.005. And the mean value of best solutions found was 0.125 (0. is the best).", "error": "", "parent_id": "0786a2d6-9df6-4d09-aacc-60665f9f011d", "metadata": {"aucs": [0.10518274371517544, 0.11290538220522994, 0.1163620129353935], "final_y": [0.14107678309736704, 0.11933928335254396, 0.11485276250904985]}, "mutation_prompt": null}
{"id": "9f5a1057-dc8d-4850-bab4-3fdcf9682811", "solution": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.crossover_rate = 0.8\n        self.learning_rate = 0.1  # Initial learning rate\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        best_solution = np.random.uniform(lb, ub, self.dim)\n        best_value = func(best_solution)\n        self.evaluations += 1\n        previous_value = best_value  # Track previous iteration's value\n\n        while self.evaluations < self.budget:\n            if np.random.rand() < self.crossover_rate:\n                # Enhanced dynamic mutation factor\n                mutation_scale = np.clip(0.1 + 0.25 * np.sin(self.evaluations / self.budget * np.pi), 0.05, 0.3)\n                candidate = np.random.normal(best_solution, (ub - lb) / 10 * mutation_scale, self.dim)\n                candidate = np.clip(candidate, lb, ub)\n            else:\n                gradient = self.estimate_gradient(func, best_solution, lb, ub)\n                candidate = self.line_search(func, best_solution, gradient, lb, ub)\n\n            candidate_value = func(candidate)\n            self.evaluations += 1\n\n            if candidate_value < best_value:\n                best_solution, best_value = candidate, candidate_value\n                self.crossover_rate = min(1.0, self.crossover_rate + 0.02)\n                # More adaptable learning rate scaling\n                self.learning_rate *= 1.12 if candidate_value < previous_value else 1.05\n                previous_value = candidate_value\n            else:\n                self.crossover_rate = max(0.2, self.crossover_rate - 0.01 * np.random.rand())\n                self.learning_rate *= 0.93\n\n        return best_solution, best_value\n\n    def estimate_gradient(self, func, solution, lb, ub, epsilon=1e-4):\n        gradient = np.zeros(self.dim)\n        for i in range(self.dim):\n            perturb = np.zeros(self.dim)\n            perturb[i] = epsilon\n            upper_sol = np.clip(solution + perturb, lb, ub)\n            lower_sol = np.clip(solution - perturb, lb, ub)\n            gradient[i] = (func(upper_sol) - func(lower_sol)) / epsilon  # Changed line: removed the factor of 2 for enhanced precision\n            self.evaluations += 2\n        return gradient\n\n    def line_search(self, func, solution, gradient, lb, ub, alpha=1e-2, beta=0.9):\n        step_size = self.learning_rate * np.linalg.norm(gradient) / (1 + self.evaluations / self.budget)\n        while step_size > 1e-5:\n            candidate = solution - step_size * gradient\n            candidate = np.clip(candidate, lb, ub)\n            if func(candidate) < func(solution):\n                return candidate\n            step_size *= beta\n            self.evaluations += 1\n        return solution", "name": "HybridOptimizer", "description": "Slightly adjust mutation scale factor and learning rate update for improved adaptability.", "configspace": "", "generation": 56, "fitness": 0.10903130794178732, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.109 with standard deviation 0.006. And the mean value of best solutions found was 0.131 (0. is the best).", "error": "", "parent_id": "0786a2d6-9df6-4d09-aacc-60665f9f011d", "metadata": {"aucs": [0.10192575107355228, 0.10948023190060152, 0.11568794085120815], "final_y": [0.15015052443138266, 0.1269931798661359, 0.11705389773094654]}, "mutation_prompt": null}
{"id": "eb1e12b7-8b8d-4721-bc7c-a961d8531586", "solution": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.crossover_rate = 0.8\n        self.learning_rate = 0.1\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        best_solution = np.random.uniform(lb, ub, self.dim)\n        best_value = func(best_solution)\n        self.evaluations += 1\n        previous_value = best_value\n\n        while self.evaluations < self.budget:\n            if np.random.rand() < self.crossover_rate:\n                # Adaptive mutation factor based on evaluations\n                mutation_scale = np.clip(0.1 + 0.4 * np.cos(self.evaluations / self.budget * np.pi), 0.05, 0.4)\n                candidate = np.random.normal(best_solution, (ub - lb) / 10 * mutation_scale, self.dim)\n                candidate = np.clip(candidate, lb, ub)\n            else:\n                gradient = self.estimate_gradient(func, best_solution, lb, ub)\n                candidate = self.line_search(func, best_solution, gradient, lb, ub)\n\n            candidate_value = func(candidate)\n            self.evaluations += 1\n\n            if candidate_value < best_value:\n                best_solution, best_value = candidate, candidate_value\n                self.crossover_rate = min(1.0, self.crossover_rate + 0.03)\n                self.learning_rate *= 1.15 if candidate_value < previous_value else 1.05\n                previous_value = candidate_value\n            else:\n                self.crossover_rate = max(0.2, self.crossover_rate - 0.01 * np.random.rand())\n                self.learning_rate *= 0.92\n\n        return best_solution, best_value\n\n    def estimate_gradient(self, func, solution, lb, ub, epsilon=1e-4):\n        gradient = np.zeros(self.dim)\n        for i in range(self.dim):\n            perturb = np.zeros(self.dim)\n            perturb[i] = epsilon\n            upper_sol = np.clip(solution + perturb, lb, ub)\n            lower_sol = np.clip(solution - perturb, lb, ub)\n            gradient[i] = (func(upper_sol) - func(lower_sol)) / (2 * epsilon)\n            self.evaluations += 2\n        return gradient\n\n    def line_search(self, func, solution, gradient, lb, ub, alpha=1e-2, beta=0.9):\n        step_size = self.learning_rate * np.linalg.norm(gradient) / (1 + self.evaluations / self.budget)\n        while step_size > 1e-5:\n            candidate = solution - step_size * gradient\n            candidate = np.clip(candidate, lb, ub)\n            if func(candidate) < func(solution):\n                return candidate\n            step_size *= beta\n            self.evaluations += 1\n        return solution", "name": "HybridOptimizer", "description": "Adaptive mutation strategy and optimized gradient estimation to enhance solution exploration and convergence.", "configspace": "", "generation": 57, "fitness": 0.11038756792727174, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.110 with standard deviation 0.008. And the mean value of best solutions found was 0.130 (0. is the best).", "error": "", "parent_id": "0786a2d6-9df6-4d09-aacc-60665f9f011d", "metadata": {"aucs": [0.10017630671853639, 0.11297758317413809, 0.11800881388914075], "final_y": [0.15665677397066946, 0.12103325298294443, 0.11121710744425051]}, "mutation_prompt": null}
{"id": "ec19399b-39c4-40a9-94dd-3cb6951f9154", "solution": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.crossover_rate = 0.75  # Slightly reduced crossover rate for more exploration\n        self.learning_rate = 0.2  # Increased initial learning rate\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        best_solution = np.random.uniform(lb, ub, self.dim)\n        best_value = func(best_solution)\n        self.evaluations += 1\n        previous_value = best_value\n\n        while self.evaluations < self.budget:\n            if np.random.rand() < self.crossover_rate:\n                mutation_scale = np.clip(0.2 + 0.4 * np.sin(self.evaluations / self.budget * np.pi), 0.05, 0.3)\n                candidate = np.random.normal(best_solution, (ub - lb) / 10 * mutation_scale, self.dim)\n                candidate = np.clip(candidate, lb, ub)\n            else:\n                gradient = self.estimate_gradient(func, best_solution, lb, ub)\n                candidate = self.line_search(func, best_solution, gradient, lb, ub)\n\n            candidate_value = func(candidate)\n            self.evaluations += 1\n\n            if candidate_value < best_value:\n                best_solution, best_value = candidate, candidate_value\n                self.crossover_rate = min(1.0, self.crossover_rate + 0.03)  # Increased adaptability\n                self.learning_rate *= 1.12 if candidate_value < previous_value else 1.05\n                previous_value = candidate_value\n            else:\n                self.crossover_rate = max(0.2, self.crossover_rate - 0.01 * np.random.rand())\n                self.learning_rate *= 0.95  # Adjusted decay rate\n\n        return best_solution, best_value\n\n    def estimate_gradient(self, func, solution, lb, ub, epsilon=1e-4):\n        gradient = np.zeros(self.dim)\n        for i in range(self.dim):\n            perturb = np.zeros(self.dim)\n            perturb[i] = epsilon\n            upper_sol = np.clip(solution + perturb, lb, ub)\n            lower_sol = np.clip(solution - perturb, lb, ub)\n            gradient[i] = (func(upper_sol) - func(lower_sol)) / epsilon\n            self.evaluations += 2\n        return gradient\n\n    def line_search(self, func, solution, gradient, lb, ub, alpha=1e-2, beta=0.9):\n        step_size = self.learning_rate * np.linalg.norm(gradient) / (1 + self.evaluations / self.budget)\n        while step_size > 1e-5:\n            candidate = solution - step_size * gradient\n            candidate = np.clip(candidate, lb, ub)\n            if func(candidate) < func(solution):\n                return candidate\n            step_size *= beta\n            self.evaluations += 1\n        return solution", "name": "HybridOptimizer", "description": "Adaptive learning rate and crossover strategy for improved convergence.", "configspace": "", "generation": 58, "fitness": 0.11039425817220787, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.110 with standard deviation 0.006. And the mean value of best solutions found was 0.129 (0. is the best).", "error": "", "parent_id": "0786a2d6-9df6-4d09-aacc-60665f9f011d", "metadata": {"aucs": [0.1020382022562677, 0.11282224623252246, 0.11632232602783343], "final_y": [0.15095079595123284, 0.12113539681597818, 0.11557136604610585]}, "mutation_prompt": null}
{"id": "3a814bc7-758e-45e1-8f21-db465a957b1b", "solution": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.crossover_rate = 0.8\n        self.learning_rate = 0.1  # Initial learning rate\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        best_solution = np.random.uniform(lb, ub, self.dim)\n        best_value = func(best_solution)\n        self.evaluations += 1\n        previous_value = best_value  # Track previous iteration's value\n\n        while self.evaluations < self.budget:\n            if np.random.rand() < self.crossover_rate:\n                # Enhanced dynamic mutation factor\n                mutation_scale = np.clip(0.1 + 0.3 * np.sin(self.evaluations / self.budget * np.pi), 0.05, 0.3)\n                candidate = np.random.normal(best_solution, (ub - lb) / 10 * mutation_scale, self.dim)\n                candidate = np.clip(candidate, lb, ub)\n            else:\n                gradient = self.estimate_gradient(func, best_solution, lb, ub)\n                candidate = self.line_search(func, best_solution, gradient, lb, ub)\n\n            candidate_value = func(candidate)\n            self.evaluations += 1\n\n            if candidate_value < best_value:\n                best_solution, best_value = candidate, candidate_value\n                self.crossover_rate = min(1.0, self.crossover_rate + 0.02)\n                # More adaptable learning rate scaling\n                self.learning_rate *= 1.1 if candidate_value < previous_value else 1.05\n                previous_value = candidate_value\n            else:\n                self.crossover_rate = max(0.2, self.crossover_rate - 0.01 * np.random.rand())\n                self.learning_rate *= 0.93\n\n        return best_solution, best_value\n\n    def estimate_gradient(self, func, solution, lb, ub, epsilon=1e-4):\n        gradient = np.zeros(self.dim)\n        adaptive_epsilon = epsilon * (1 + self.evaluations / self.budget)  # Adaptive epsilon\n        for i in range(self.dim):\n            perturb = np.zeros(self.dim)\n            perturb[i] = adaptive_epsilon\n            upper_sol = np.clip(solution + perturb, lb, ub)\n            lower_sol = np.clip(solution - perturb, lb, ub)\n            gradient[i] = (func(upper_sol) - func(lower_sol)) / adaptive_epsilon\n            self.evaluations += 2\n        return gradient\n\n    def line_search(self, func, solution, gradient, lb, ub, alpha=1e-2, beta=0.9):\n        step_size = self.learning_rate * np.linalg.norm(gradient) / (1 + self.evaluations / self.budget)\n        while step_size > 1e-5:\n            candidate = solution - step_size * gradient\n            candidate = np.clip(candidate, lb, ub)\n            if func(candidate) < func(solution):\n                return candidate\n            step_size *= beta\n            self.evaluations += 1\n        return solution", "name": "HybridOptimizer", "description": "Introduced adaptive epsilon for gradient estimation to enhance precision in varying conditions.", "configspace": "", "generation": 59, "fitness": 0.11265587093032665, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.113 with standard deviation 0.003. And the mean value of best solutions found was 0.122 (0. is the best).", "error": "", "parent_id": "0786a2d6-9df6-4d09-aacc-60665f9f011d", "metadata": {"aucs": [0.10857547636547926, 0.11257053575628329, 0.11682160066921743], "final_y": [0.1319515223626554, 0.11998308450735873, 0.11414720007946577]}, "mutation_prompt": null}
{"id": "c3c6da98-a87f-4352-a216-60263541540d", "solution": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.crossover_rate = 0.8\n        self.learning_rate = 0.1  # Initial learning rate\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        best_solution = np.random.uniform(lb, ub, self.dim)\n        best_value = func(best_solution)\n        self.evaluations += 1\n        previous_value = best_value  # Track previous iteration's value\n\n        while self.evaluations < self.budget:\n            if np.random.rand() < self.crossover_rate:\n                # Enhanced dynamic mutation factor\n                mutation_scale = np.clip(0.1 + 0.4 * np.cos(self.evaluations / self.budget * np.pi), 0.05, 0.3)\n                candidate = np.random.normal(best_solution, (ub - lb) / 10 * mutation_scale, self.dim)\n                candidate = np.clip(candidate, lb, ub)\n            else:\n                gradient = self.estimate_gradient(func, best_solution, lb, ub)\n                candidate = self.line_search(func, best_solution, gradient, lb, ub)\n\n            candidate_value = func(candidate)\n            self.evaluations += 1\n\n            if candidate_value < best_value:\n                best_solution, best_value = candidate, candidate_value\n                self.crossover_rate = min(1.0, self.crossover_rate + 0.02)\n                # More adaptable learning rate scaling\n                self.learning_rate *= 1.1 if candidate_value < previous_value else 1.05\n                previous_value = candidate_value\n            else:\n                self.crossover_rate = max(0.2, self.crossover_rate - 0.01 * np.random.rand())\n                self.learning_rate *= 0.93\n\n        return best_solution, best_value\n\n    def estimate_gradient(self, func, solution, lb, ub, epsilon=1e-4):\n        gradient = np.zeros(self.dim)\n        adaptive_epsilon = epsilon * (1 + self.evaluations / self.budget)  # Adaptive epsilon\n        for i in range(self.dim):\n            perturb = np.zeros(self.dim)\n            perturb[i] = adaptive_epsilon\n            upper_sol = np.clip(solution + perturb, lb, ub)\n            lower_sol = np.clip(solution - perturb, lb, ub)\n            gradient[i] = (func(upper_sol) - func(lower_sol)) / adaptive_epsilon\n            self.evaluations += 2\n        return gradient\n\n    def line_search(self, func, solution, gradient, lb, ub, alpha=1e-2, beta=0.9):\n        step_size = self.learning_rate * np.linalg.norm(gradient) / (1 + self.evaluations / self.budget)\n        while step_size > 1e-5:\n            candidate = solution - step_size * gradient\n            candidate = np.clip(candidate, lb, ub)\n            if func(candidate) < func(solution):\n                return candidate\n            step_size *= beta\n            self.evaluations += 1\n        return solution", "name": "HybridOptimizer", "description": "Enhanced adaptive mutation and decay factor to improve exploration and convergence.", "configspace": "", "generation": 60, "fitness": 0.10934526284855473, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.109 with standard deviation 0.007. And the mean value of best solutions found was 0.131 (0. is the best).", "error": "", "parent_id": "3a814bc7-758e-45e1-8f21-db465a957b1b", "metadata": {"aucs": [0.10149440312622104, 0.10903772373224663, 0.1175036616871965], "final_y": [0.15036826345264842, 0.1291629105804576, 0.11319884212777254]}, "mutation_prompt": null}
{"id": "d218d89c-85ed-4840-a4eb-d4c82f162867", "solution": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.crossover_rate = 0.8\n        self.learning_rate = 0.1  # Initial learning rate\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        best_solution = np.random.uniform(lb, ub, self.dim)\n        best_value = func(best_solution)\n        self.evaluations += 1\n        previous_value = best_value  # Track previous iteration's value\n\n        while self.evaluations < self.budget:\n            if np.random.rand() < self.crossover_rate:\n                # Enhanced dynamic mutation factor\n                mutation_scale = np.clip(0.1 + 0.3 * np.sin(self.evaluations / self.budget * np.pi * 2), 0.05, 0.3)\n                candidate = np.random.normal(best_solution, (ub - lb) / 10 * mutation_scale, self.dim)\n                candidate = np.clip(candidate, lb, ub)\n            else:\n                gradient = self.estimate_gradient(func, best_solution, lb, ub)\n                candidate = self.line_search(func, best_solution, gradient, lb, ub)\n\n            candidate_value = func(candidate)\n            self.evaluations += 1\n\n            if candidate_value < best_value:\n                best_solution, best_value = candidate, candidate_value\n                self.crossover_rate = min(1.0, self.crossover_rate + 0.02)\n                # More adaptable learning rate scaling\n                self.learning_rate *= 1.1 if candidate_value < previous_value else 1.05\n                previous_value = candidate_value\n            else:\n                self.crossover_rate = max(0.2, self.crossover_rate - 0.01 * np.random.rand())\n                self.learning_rate *= 0.93\n\n        return best_solution, best_value\n\n    def estimate_gradient(self, func, solution, lb, ub, epsilon=1e-4):\n        gradient = np.zeros(self.dim)\n        adaptive_epsilon = epsilon * (1 + self.evaluations / self.budget)  # Adaptive epsilon\n        for i in range(self.dim):\n            perturb = np.zeros(self.dim)\n            perturb[i] = adaptive_epsilon\n            upper_sol = np.clip(solution + perturb, lb, ub)\n            lower_sol = np.clip(solution - perturb, lb, ub)\n            gradient[i] = (func(upper_sol) - func(lower_sol)) / adaptive_epsilon\n            self.evaluations += 2\n        return gradient\n\n    def line_search(self, func, solution, gradient, lb, ub, alpha=1e-2, beta=0.9):\n        step_size = self.learning_rate * np.linalg.norm(gradient) / (1 + self.evaluations / self.budget)\n        while step_size > 1e-5:\n            candidate = solution - step_size * gradient\n            candidate = np.clip(candidate, lb, ub)\n            if func(candidate) < func(solution):\n                return candidate\n            step_size *= beta\n            self.evaluations += 1\n        return solution", "name": "HybridOptimizer", "description": "Enhanced performance by adjusting mutation scale periodically for better exploration-exploitation balance.", "configspace": "", "generation": 61, "fitness": 0.11063508166671195, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.111 with standard deviation 0.005. And the mean value of best solutions found was 0.127 (0. is the best).", "error": "", "parent_id": "3a814bc7-758e-45e1-8f21-db465a957b1b", "metadata": {"aucs": [0.10461822688627176, 0.11143327245369727, 0.11585374566016682], "final_y": [0.14272992349659197, 0.12136032273254338, 0.11558418823884253]}, "mutation_prompt": null}
{"id": "314babb9-fd53-4a50-ba1b-10990d207d69", "solution": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.crossover_rate = 0.8\n        self.learning_rate = 0.1  # Initial learning rate\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        best_solution = np.random.uniform(lb, ub, self.dim)\n        best_value = func(best_solution)\n        self.evaluations += 1\n        previous_value = best_value  # Track previous iteration's value\n\n        while self.evaluations < self.budget:\n            if np.random.rand() < self.crossover_rate:\n                # Enhanced dynamic mutation factor\n                mutation_scale = np.clip(0.1 + 0.3 * np.sin(self.evaluations / self.budget * np.pi), 0.05, 0.3)\n                candidate = np.random.normal(best_solution, (ub - lb) / 10 * mutation_scale, self.dim)\n                candidate = np.clip(candidate, lb, ub)\n            else:\n                gradient = self.estimate_gradient(func, best_solution, lb, ub)\n                candidate = self.line_search(func, best_solution, gradient, lb, ub)\n\n            candidate_value = func(candidate)\n            self.evaluations += 1\n\n            if candidate_value < best_value:\n                best_solution, best_value = candidate, candidate_value\n                self.crossover_rate = min(1.0, self.crossover_rate + 0.02)\n                # More adaptable learning rate scaling\n                self.learning_rate *= 1.1 if candidate_value < previous_value else 1.05\n                previous_value = candidate_value\n            else:\n                self.crossover_rate = max(0.2, self.crossover_rate - 0.01 * np.random.rand())\n                self.learning_rate *= 0.93 * np.exp(-0.01 * self.evaluations)  # Exponential decay\n\n        return best_solution, best_value\n\n    def estimate_gradient(self, func, solution, lb, ub, epsilon=1e-4):\n        gradient = np.zeros(self.dim)\n        adaptive_epsilon = epsilon * (1 + self.evaluations / self.budget)  # Adaptive epsilon\n        for i in range(self.dim):\n            perturb = np.zeros(self.dim)\n            perturb[i] = adaptive_epsilon\n            upper_sol = np.clip(solution + perturb, lb, ub)\n            lower_sol = np.clip(solution - perturb, lb, ub)\n            gradient[i] = (func(upper_sol) - func(lower_sol)) / adaptive_epsilon\n            self.evaluations += 2\n        return gradient\n\n    def line_search(self, func, solution, gradient, lb, ub, alpha=1e-2, beta=0.9):\n        step_size = self.learning_rate * np.linalg.norm(gradient) / (1 + self.evaluations / self.budget)\n        while step_size > 1e-5:\n            candidate = solution - step_size * gradient\n            candidate = np.clip(candidate, lb, ub)\n            if func(candidate) < func(solution):\n                return candidate\n            step_size *= beta\n            self.evaluations += 1\n        return solution", "name": "HybridOptimizer", "description": "Incorporated exponential decay to the learning rate for improved convergence speed.", "configspace": "", "generation": 62, "fitness": 0.10978370542491511, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.110 with standard deviation 0.005. And the mean value of best solutions found was 0.128 (0. is the best).", "error": "", "parent_id": "3a814bc7-758e-45e1-8f21-db465a957b1b", "metadata": {"aucs": [0.10280281913611844, 0.11196803769300001, 0.11458025944562689], "final_y": [0.14791585084560266, 0.11935862169809863, 0.11713783266866418]}, "mutation_prompt": null}
{"id": "da6e5369-8693-4df3-97a4-2ace9fda94c5", "solution": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.crossover_rate = 0.8\n        self.learning_rate = 0.1  # Initial learning rate\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        best_solution = np.random.uniform(lb, ub, self.dim)\n        best_value = func(best_solution)\n        self.evaluations += 1\n        previous_value = best_value  # Track previous iteration's value\n\n        while self.evaluations < self.budget:\n            if np.random.rand() < self.crossover_rate:\n                # Enhanced dynamic mutation factor\n                mutation_scale = np.clip(0.1 + 0.3 * np.sin(self.evaluations / self.budget * np.pi), 0.05, 0.3)\n                candidate = np.random.normal(best_solution, (ub - lb) / 8 * mutation_scale, self.dim)  # Change line 1\n                candidate = np.clip(candidate, lb, ub)\n            else:\n                gradient = self.estimate_gradient(func, best_solution, lb, ub)\n                candidate = self.line_search(func, best_solution, gradient, lb, ub)\n\n            candidate_value = func(candidate)\n            self.evaluations += 1\n\n            if candidate_value < best_value:\n                best_solution, best_value = candidate, candidate_value\n                self.crossover_rate = min(1.0, self.crossover_rate + 0.03)  # Change line 2\n                # More adaptable learning rate scaling\n                self.learning_rate *= 1.1 if candidate_value < previous_value else 1.08  # Change line 3\n                previous_value = candidate_value\n            else:\n                self.crossover_rate = max(0.2, self.crossover_rate - 0.01 * np.random.rand())\n                self.learning_rate *= 0.93\n\n        return best_solution, best_value\n\n    def estimate_gradient(self, func, solution, lb, ub, epsilon=1e-4):\n        gradient = np.zeros(self.dim)\n        adaptive_epsilon = epsilon * (1 + self.evaluations / self.budget)  # Adaptive epsilon\n        for i in range(self.dim):\n            perturb = np.zeros(self.dim)\n            perturb[i] = adaptive_epsilon\n            upper_sol = np.clip(solution + perturb, lb, ub)\n            lower_sol = np.clip(solution - perturb, lb, ub)\n            gradient[i] = (func(upper_sol) - func(lower_sol)) / adaptive_epsilon\n            self.evaluations += 2\n        return gradient\n\n    def line_search(self, func, solution, gradient, lb, ub, alpha=1e-2, beta=0.9):\n        step_size = self.learning_rate * np.linalg.norm(gradient) / (1 + self.evaluations / self.budget)\n        while step_size > 1e-5:\n            candidate = solution - step_size * gradient\n            candidate = np.clip(candidate, lb, ub)\n            if func(candidate) < func(solution):\n                return candidate\n            step_size *= beta\n            self.evaluations += 1\n        return solution", "name": "HybridOptimizer", "description": "Enhanced exploration-exploitation balance with dynamic crossover and learning rate adjustments.", "configspace": "", "generation": 63, "fitness": 0.10814725208677727, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.108 with standard deviation 0.007. And the mean value of best solutions found was 0.134 (0. is the best).", "error": "", "parent_id": "3a814bc7-758e-45e1-8f21-db465a957b1b", "metadata": {"aucs": [0.09866409342570337, 0.10897196622897942, 0.11680569660564899], "final_y": [0.16167288030251936, 0.12765518381528806, 0.11396412407861378]}, "mutation_prompt": null}
{"id": "cc908835-e3bf-4759-a5ed-b3ef10cf4b7b", "solution": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.crossover_rate = 0.8\n        self.learning_rate = 0.1  # Initial learning rate\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        best_solution = np.random.uniform(lb, ub, self.dim)\n        best_value = func(best_solution)\n        self.evaluations += 1\n        previous_value = best_value  # Track previous iteration's value\n\n        while self.evaluations < self.budget:\n            if np.random.rand() < self.crossover_rate:\n                # Enhanced dynamic mutation factor\n                mutation_scale = np.clip(0.1 + 0.3 * np.sin(self.evaluations / self.budget * np.pi), 0.05, 0.3)\n                candidate = np.random.normal(best_solution, (ub - lb) / 10 * mutation_scale, self.dim)\n                candidate = np.clip(candidate, lb, ub)\n            else:\n                gradient = self.estimate_gradient(func, best_solution, lb, ub)\n                candidate = self.line_search(func, best_solution, gradient, lb, ub)\n\n            candidate_value = func(candidate)\n            self.evaluations += 1\n\n            if candidate_value < best_value:\n                best_solution, best_value = candidate, candidate_value\n                self.crossover_rate = min(1.0, self.crossover_rate + 0.02 * np.exp(-self.evaluations / self.budget))\n                # More adaptable learning rate scaling\n                self.learning_rate *= 1.1 if candidate_value < previous_value else 1.05\n                previous_value = candidate_value\n            else:\n                self.crossover_rate = max(0.2, self.crossover_rate - 0.01 * np.random.rand())\n                self.learning_rate *= 0.93\n\n        return best_solution, best_value\n\n    def estimate_gradient(self, func, solution, lb, ub, epsilon=1e-4):\n        gradient = np.zeros(self.dim)\n        adaptive_epsilon = epsilon * (1 + self.evaluations / self.budget)  # Adaptive epsilon\n        for i in range(self.dim):\n            perturb = np.zeros(self.dim)\n            perturb[i] = adaptive_epsilon\n            upper_sol = np.clip(solution + perturb, lb, ub)\n            lower_sol = np.clip(solution - perturb, lb, ub)\n            gradient[i] = (func(upper_sol) - func(lower_sol)) / adaptive_epsilon\n            self.evaluations += 2\n        return gradient\n\n    def line_search(self, func, solution, gradient, lb, ub, alpha=1e-2, beta=0.9):\n        step_size = self.learning_rate * np.linalg.norm(gradient) / (1 + self.evaluations / self.budget)\n        while step_size > 1e-5:\n            candidate = solution - step_size * gradient\n            candidate = np.clip(candidate, lb, ub)\n            if func(candidate) < func(solution):\n                return candidate\n            step_size *= beta\n            self.evaluations += 1\n        return solution", "name": "HybridOptimizer", "description": "Enhanced crossover rate decay to stabilize convergence towards the end of the budget.", "configspace": "", "generation": 64, "fitness": 0.11268564912270236, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.113 with standard deviation 0.003. And the mean value of best solutions found was 0.122 (0. is the best).", "error": "", "parent_id": "3a814bc7-758e-45e1-8f21-db465a957b1b", "metadata": {"aucs": [0.10857547636546783, 0.11257053575634113, 0.11691093524629814], "final_y": [0.1319515223626554, 0.11998308442832795, 0.11388333668983786]}, "mutation_prompt": null}
{"id": "1daeb531-f01c-4106-bc25-6493ab46efce", "solution": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.crossover_rate = 0.8\n        self.learning_rate = 0.1\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        best_solution = np.random.uniform(lb, ub, self.dim)\n        best_value = func(best_solution)\n        self.evaluations += 1\n        previous_value = best_value\n\n        while self.evaluations < self.budget:\n            if np.random.rand() < self.crossover_rate:\n                mutation_scale = np.clip(0.15 + 0.25 * np.sin(self.evaluations / self.budget * np.pi), 0.05, 0.25)\n                trial_solution = self.differential_mutation(best_solution, lb, ub, mutation_scale)\n            else:\n                gradient = self.estimate_gradient(func, best_solution, lb, ub)\n                trial_solution = self.line_search(func, best_solution, gradient, lb, ub)\n\n            trial_value = func(trial_solution)\n            self.evaluations += 1\n\n            if trial_value < best_value:\n                best_solution, best_value = trial_solution, trial_value\n                self.crossover_rate = min(1.0, self.crossover_rate + 0.03 * np.exp(-self.evaluations / self.budget))\n                self.learning_rate *= 1.1 if trial_value < previous_value else 1.03\n                previous_value = trial_value\n            else:\n                self.crossover_rate = max(0.2, self.crossover_rate - 0.01 * np.random.rand())\n                self.learning_rate *= 0.95\n\n        return best_solution, best_value\n\n    def estimate_gradient(self, func, solution, lb, ub, epsilon=1e-4):\n        gradient = np.zeros(self.dim)\n        adaptive_epsilon = epsilon * (1 + self.evaluations / self.budget)\n        for i in range(self.dim):\n            perturb = np.zeros(self.dim)\n            perturb[i] = adaptive_epsilon\n            upper_sol = np.clip(solution + perturb, lb, ub)\n            lower_sol = np.clip(solution - perturb, lb, ub)\n            gradient[i] = (func(upper_sol) - func(lower_sol)) / (2 * adaptive_epsilon)\n            self.evaluations += 2\n        return gradient\n\n    def line_search(self, func, solution, gradient, lb, ub, alpha=1e-2, beta=0.8):\n        step_size = self.learning_rate * np.linalg.norm(gradient) / (1 + self.evaluations / self.budget)\n        while step_size > 1e-5:\n            candidate = solution - step_size * gradient\n            candidate = np.clip(candidate, lb, ub)\n            if func(candidate) < func(solution):\n                return candidate\n            step_size *= beta\n            self.evaluations += 1\n        return solution\n\n    def differential_mutation(self, best_solution, lb, ub, mutation_scale):\n        donor = best_solution + mutation_scale * (np.random.uniform(lb, ub, self.dim) - best_solution)\n        return np.clip(donor, lb, ub)", "name": "HybridOptimizer", "description": "Adaptive dual-phase strategy combining gradient estimation and differential evolution to enhance exploration and exploitation balance.", "configspace": "", "generation": 65, "fitness": 0.10946005130121021, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.109 with standard deviation 0.001. And the mean value of best solutions found was 0.130 (0. is the best).", "error": "", "parent_id": "cc908835-e3bf-4759-a5ed-b3ef10cf4b7b", "metadata": {"aucs": [0.10850297589022917, 0.11116790894568918, 0.10870926906771228], "final_y": [0.1332090018527453, 0.12118489872080829, 0.1342669509226514]}, "mutation_prompt": null}
{"id": "dc7096b3-71ec-419e-8fc3-b9e393d6981c", "solution": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.crossover_rate = 0.8\n        self.learning_rate = 0.1  # Initial learning rate\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        best_solution = np.random.uniform(lb, ub, self.dim)\n        best_value = func(best_solution)\n        self.evaluations += 1\n        previous_value = best_value  # Track previous iteration's value\n\n        while self.evaluations < self.budget:\n            if np.random.rand() < self.crossover_rate:\n                # Enhanced dynamic mutation factor\n                mutation_scale = np.clip(0.1 + 0.3 * np.sin(self.evaluations / self.budget * 2 * np.pi), 0.05, 0.3)\n                candidate = np.random.normal(best_solution, (ub - lb) / 10 * mutation_scale, self.dim)\n                candidate = np.clip(candidate, lb, ub)\n            else:\n                gradient = self.estimate_gradient(func, best_solution, lb, ub)\n                candidate = self.line_search(func, best_solution, gradient, lb, ub)\n\n            candidate_value = func(candidate)\n            self.evaluations += 1\n\n            if candidate_value < best_value:\n                best_solution, best_value = candidate, candidate_value\n                self.crossover_rate = min(1.0, self.crossover_rate + 0.02 * np.exp(-self.evaluations / self.budget))\n                # More adaptable learning rate scaling\n                self.learning_rate *= 1.1 if candidate_value < previous_value else 1.05\n                previous_value = candidate_value\n            else:\n                self.crossover_rate = max(0.2, self.crossover_rate - 0.01 * np.random.rand())\n                self.learning_rate *= 0.93\n\n        return best_solution, best_value\n\n    def estimate_gradient(self, func, solution, lb, ub, epsilon=1e-4):\n        gradient = np.zeros(self.dim)\n        adaptive_epsilon = epsilon * (1 + self.evaluations / self.budget)  # Adaptive epsilon\n        for i in range(self.dim):\n            perturb = np.zeros(self.dim)\n            perturb[i] = adaptive_epsilon\n            upper_sol = np.clip(solution + perturb, lb, ub)\n            lower_sol = np.clip(solution - perturb, lb, ub)\n            gradient[i] = (func(upper_sol) - func(lower_sol)) / adaptive_epsilon\n            self.evaluations += 2\n        return gradient\n\n    def line_search(self, func, solution, gradient, lb, ub, alpha=1e-2, beta=0.9):\n        step_size = self.learning_rate * np.linalg.norm(gradient) / (1 + self.evaluations / self.budget)\n        while step_size > 1e-5:\n            candidate = solution - step_size * gradient\n            candidate = np.clip(candidate, lb, ub)\n            if func(candidate) < func(solution):\n                return candidate\n            step_size *= beta\n            self.evaluations += 1\n        return solution", "name": "HybridOptimizer", "description": "Introduce a dynamic mutation scale oscillation and adaptive step decay to enhance exploration capabilities.", "configspace": "", "generation": 66, "fitness": 0.1107397712809699, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.111 with standard deviation 0.005. And the mean value of best solutions found was 0.126 (0. is the best).", "error": "", "parent_id": "cc908835-e3bf-4759-a5ed-b3ef10cf4b7b", "metadata": {"aucs": [0.10452901510770551, 0.11206596297368299, 0.1156243357615212], "final_y": [0.14221564350674776, 0.11976584273800628, 0.11679087481634742]}, "mutation_prompt": null}
{"id": "b2f9bb34-fcde-47e5-9142-71238879b432", "solution": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.crossover_rate = 0.8\n        self.learning_rate = 0.1  # Initial learning rate\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        best_solution = np.random.uniform(lb, ub, self.dim)\n        best_value = func(best_solution)\n        self.evaluations += 1\n        previous_value = best_value  # Track previous iteration's value\n\n        while self.evaluations < self.budget:\n            if np.random.rand() < self.crossover_rate:\n                # Modified dynamic mutation factor for adaptive exploration\n                mutation_scale = np.clip(0.1 + 0.5 * np.sin(self.evaluations / self.budget * np.pi / 2), 0.05, 0.5)\n                candidate = np.random.normal(best_solution, (ub - lb) / 10 * mutation_scale, self.dim)\n                candidate = np.clip(candidate, lb, ub)\n            else:\n                gradient = self.estimate_gradient(func, best_solution, lb, ub)\n                candidate = self.line_search(func, best_solution, gradient, lb, ub)\n\n            candidate_value = func(candidate)\n            self.evaluations += 1\n\n            if candidate_value < best_value:\n                best_solution, best_value = candidate, candidate_value\n                self.crossover_rate = min(1.0, self.crossover_rate + 0.02 * np.exp(-self.evaluations / self.budget))\n                # More adaptable learning rate scaling\n                self.learning_rate *= 1.1 if candidate_value < previous_value else 1.05\n                previous_value = candidate_value\n            else:\n                self.crossover_rate = max(0.2, self.crossover_rate - 0.01 * np.random.rand())\n                self.learning_rate *= 0.93\n\n        return best_solution, best_value\n\n    def estimate_gradient(self, func, solution, lb, ub, epsilon=1e-4):\n        gradient = np.zeros(self.dim)\n        adaptive_epsilon = epsilon * (1 + self.evaluations / self.budget)  # Adaptive epsilon\n        for i in range(self.dim):\n            perturb = np.zeros(self.dim)\n            perturb[i] = adaptive_epsilon\n            upper_sol = np.clip(solution + perturb, lb, ub)\n            lower_sol = np.clip(solution - perturb, lb, ub)\n            gradient[i] = (func(upper_sol) - func(lower_sol)) / adaptive_epsilon\n            self.evaluations += 2\n        return gradient\n\n    def line_search(self, func, solution, gradient, lb, ub, alpha=1e-2, beta=0.9):\n        step_size = self.learning_rate * np.linalg.norm(gradient) / (1 + self.evaluations / self.budget)\n        while step_size > 1e-5:\n            candidate = solution - step_size * gradient\n            candidate = np.clip(candidate, lb, ub)\n            if func(candidate) < func(solution):\n                return candidate\n            step_size *= beta\n            self.evaluations += 1\n        return solution", "name": "HybridOptimizer", "description": "Incorporate adaptive mutation scaling with respect to the evaluation ratio to enhance exploration in early stages.", "configspace": "", "generation": 67, "fitness": 0.10844048174914445, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.108 with standard deviation 0.006. And the mean value of best solutions found was 0.133 (0. is the best).", "error": "", "parent_id": "cc908835-e3bf-4759-a5ed-b3ef10cf4b7b", "metadata": {"aucs": [0.10104345191221564, 0.10885742018554534, 0.11542057314967236], "final_y": [0.15384123344330214, 0.12873987160821398, 0.11770125476744842]}, "mutation_prompt": null}
{"id": "9831ed93-c856-4a64-ab63-d1eb83aa373c", "solution": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.crossover_rate = 0.8\n        self.learning_rate = 0.1  # Initial learning rate\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        best_solution = np.random.uniform(lb, ub, self.dim)\n        best_value = func(best_solution)\n        self.evaluations += 1\n        previous_value = best_value  # Track previous iteration's value\n\n        while self.evaluations < self.budget:\n            if np.random.rand() < self.crossover_rate:\n                # Enhanced dynamic mutation factor\n                mutation_scale = np.clip(0.1 + 0.3 * np.sin(self.evaluations / self.budget * np.pi), 0.05, 0.3)\n                candidate = np.random.normal(best_solution, (ub - lb) / 10 * mutation_scale, self.dim)\n                candidate = np.clip(candidate, lb, ub)\n            else:\n                gradient = self.estimate_gradient(func, best_solution, lb, ub)\n                candidate = self.line_search(func, best_solution, gradient, lb, ub)\n\n            candidate_value = func(candidate)\n            self.evaluations += 1\n\n            if candidate_value < best_value:\n                best_solution, best_value = candidate, candidate_value\n                self.crossover_rate = min(1.0, self.crossover_rate + 0.02 * np.exp(-self.evaluations / (0.9 * self.budget)))\n                # More adaptable learning rate scaling\n                self.learning_rate *= 1.15 if candidate_value < previous_value else 1.05\n                previous_value = candidate_value\n            else:\n                self.crossover_rate = max(0.2, self.crossover_rate - 0.01 * np.random.rand())\n                self.learning_rate *= 0.93\n\n        return best_solution, best_value\n\n    def estimate_gradient(self, func, solution, lb, ub, epsilon=1e-4):\n        gradient = np.zeros(self.dim)\n        adaptive_epsilon = epsilon * (1 + self.evaluations / self.budget)  # Adaptive epsilon\n        for i in range(self.dim):\n            perturb = np.zeros(self.dim)\n            perturb[i] = adaptive_epsilon\n            upper_sol = np.clip(solution + perturb, lb, ub)\n            lower_sol = np.clip(solution - perturb, lb, ub)\n            gradient[i] = (func(upper_sol) - func(lower_sol)) / adaptive_epsilon\n            self.evaluations += 2\n        return gradient\n\n    def line_search(self, func, solution, gradient, lb, ub, alpha=1e-2, beta=0.9):\n        step_size = self.learning_rate * np.linalg.norm(gradient) / (1 + self.evaluations / self.budget)\n        while step_size > 1e-5:\n            candidate = solution - step_size * gradient\n            candidate = np.clip(candidate, lb, ub)\n            if func(candidate) < func(solution):\n                return candidate\n            step_size *= beta\n            self.evaluations += 1\n        return solution", "name": "HybridOptimizer", "description": "Small enhancements in learning rate adjustment and crossover rate decay for improved convergence efficiency.", "configspace": "", "generation": 68, "fitness": 0.11260650012231022, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.113 with standard deviation 0.003. And the mean value of best solutions found was 0.122 (0. is the best).", "error": "", "parent_id": "cc908835-e3bf-4759-a5ed-b3ef10cf4b7b", "metadata": {"aucs": [0.10852166845186806, 0.11238689658237788, 0.1169109353326847], "final_y": [0.13217487032156083, 0.12017914879574265, 0.1138833366930323]}, "mutation_prompt": null}
{"id": "32436444-01ec-4b23-ab6e-1227fb44f4cf", "solution": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.crossover_rate = 0.8\n        self.learning_rate = 0.1  # Initial learning rate\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        best_solution = np.random.uniform(lb, ub, self.dim)\n        best_value = func(best_solution)\n        self.evaluations += 1\n        previous_value = best_value  # Track previous iteration's value\n\n        while self.evaluations < self.budget:\n            if np.random.rand() < self.crossover_rate:\n                # Enhanced dynamic mutation factor\n                mutation_scale = np.clip(0.1 + 0.3 * np.sin(self.evaluations / self.budget * np.pi), 0.05, 0.35)  # Tweaked upper bound\n                candidate = np.random.normal(best_solution, (ub - lb) / 10 * mutation_scale, self.dim)\n                candidate = np.clip(candidate, lb, ub)\n            else:\n                gradient = self.estimate_gradient(func, best_solution, lb, ub)\n                candidate = self.line_search(func, best_solution, gradient, lb, ub)\n\n            candidate_value = func(candidate)\n            self.evaluations += 1\n\n            if candidate_value < best_value:\n                best_solution, best_value = candidate, candidate_value\n                self.crossover_rate = min(1.0, self.crossover_rate + 0.02 * np.exp(-self.evaluations / self.budget))\n                # More adaptable learning rate scaling\n                self.learning_rate *= 1.15 if candidate_value < previous_value else 1.05  # Increased learning rate scale\n                previous_value = candidate_value\n            else:\n                self.crossover_rate = max(0.2, self.crossover_rate - 0.01 * np.random.rand())\n                self.learning_rate *= 0.93\n\n        return best_solution, best_value\n\n    def estimate_gradient(self, func, solution, lb, ub, epsilon=1e-4):\n        gradient = np.zeros(self.dim)\n        adaptive_epsilon = epsilon * (1 + self.evaluations / self.budget)  # Adaptive epsilon\n        for i in range(self.dim):\n            perturb = np.zeros(self.dim)\n            perturb[i] = adaptive_epsilon\n            upper_sol = np.clip(solution + perturb, lb, ub)\n            lower_sol = np.clip(solution - perturb, lb, ub)\n            gradient[i] = (func(upper_sol) - func(lower_sol)) / adaptive_epsilon\n            self.evaluations += 2\n        return gradient\n\n    def line_search(self, func, solution, gradient, lb, ub, alpha=1e-2, beta=0.9):\n        step_size = self.learning_rate * np.linalg.norm(gradient) / (1 + self.evaluations / self.budget)\n        while step_size > 1e-5:\n            candidate = solution - step_size * gradient\n            candidate = np.clip(candidate, lb, ub)\n            if func(candidate) < func(solution):\n                return candidate\n            step_size *= beta\n            self.evaluations += 1\n        return solution", "name": "HybridOptimizer", "description": "Enhanced learning rate adaptation and mutation mechanism for improved exploration and exploitation balance.", "configspace": "", "generation": 69, "fitness": 0.11258117520563433, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.113 with standard deviation 0.003. And the mean value of best solutions found was 0.122 (0. is the best).", "error": "", "parent_id": "cc908835-e3bf-4759-a5ed-b3ef10cf4b7b", "metadata": {"aucs": [0.10852166845009314, 0.11238689658237788, 0.11683496058443199], "final_y": [0.13217487032669517, 0.12017914879574265, 0.1141152278276969]}, "mutation_prompt": null}
{"id": "e9590c6e-d703-4340-a7c7-a74d44d0998e", "solution": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.crossover_rate = 0.8\n        self.learning_rate = 0.1  # Initial learning rate\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        best_solution = np.random.uniform(lb, ub, self.dim)\n        best_value = func(best_solution)\n        self.evaluations += 1\n        previous_value = best_value  # Track previous iteration's value\n\n        while self.evaluations < self.budget:\n            if np.random.rand() < self.crossover_rate:\n                # Enhanced dynamic mutation factor\n                mutation_scale = np.clip(0.1 + 0.3 * np.cos(self.evaluations / self.budget * np.pi), 0.05, 0.3)\n                candidate = np.random.normal(best_solution, (ub - lb) / 10 * mutation_scale, self.dim)\n                candidate = np.clip(candidate, lb, ub)\n            else:\n                gradient = self.estimate_gradient(func, best_solution, lb, ub)\n                candidate = self.line_search(func, best_solution, gradient, lb, ub)\n\n            candidate_value = func(candidate)\n            self.evaluations += 1\n\n            if candidate_value < best_value:\n                best_solution, best_value = candidate, candidate_value\n                self.crossover_rate = min(1.0, self.crossover_rate + 0.02 * np.exp(-self.evaluations / self.budget))\n                # More adaptable learning rate scaling\n                self.learning_rate *= 1.1 if candidate_value < previous_value else 1.05\n                previous_value = candidate_value\n            else:\n                self.crossover_rate = max(0.2, self.crossover_rate - 0.01 * np.random.rand())\n                self.learning_rate *= 0.93\n\n        return best_solution, best_value\n\n    def estimate_gradient(self, func, solution, lb, ub, epsilon=1e-4):\n        gradient = np.zeros(self.dim)\n        adaptive_epsilon = epsilon * (1 + self.evaluations / self.budget)  # Adaptive epsilon\n        for i in range(self.dim):\n            perturb = np.zeros(self.dim)\n            perturb[i] = adaptive_epsilon\n            upper_sol = np.clip(solution + perturb, lb, ub)\n            lower_sol = np.clip(solution - perturb, lb, ub)\n            gradient[i] = (func(upper_sol) - func(lower_sol)) / adaptive_epsilon\n            self.evaluations += 2\n        return gradient\n\n    def line_search(self, func, solution, gradient, lb, ub, alpha=1e-2, beta=0.9):\n        step_size = self.learning_rate * np.linalg.norm(gradient) / (1 + self.evaluations / self.budget)\n        while step_size > 1e-5:\n            candidate = solution - step_size * gradient\n            candidate = np.clip(candidate, lb, ub)\n            if func(candidate) < func(solution):\n                return candidate\n            step_size *= beta\n            self.evaluations += 1\n        return solution", "name": "HybridOptimizer", "description": "Introduce a cosine-based mutation scaling to enhance exploration in early iterations.", "configspace": "", "generation": 70, "fitness": 0.10916292030115289, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.109 with standard deviation 0.007. And the mean value of best solutions found was 0.132 (0. is the best).", "error": "", "parent_id": "cc908835-e3bf-4759-a5ed-b3ef10cf4b7b", "metadata": {"aucs": [0.10090279899762211, 0.10908618148428639, 0.11749978042155018], "final_y": [0.1540833567970431, 0.12901426050783182, 0.11282801729974634]}, "mutation_prompt": null}
{"id": "06246a10-69ca-44cd-b9fb-0b064979e056", "solution": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.crossover_rate = 0.8\n        self.learning_rate = 0.1  # Initial learning rate\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        best_solution = np.random.uniform(lb, ub, self.dim)\n        best_value = func(best_solution)\n        self.evaluations += 1\n        previous_value = best_value  # Track previous iteration's value\n\n        while self.evaluations < self.budget:\n            if np.random.rand() < self.crossover_rate:\n                # Enhanced dynamic mutation factor\n                mutation_scale = np.clip(0.1 + 0.3 * np.sin(self.evaluations / self.budget * np.pi), 0.05, 0.5) * (best_value / (previous_value + 1e-9))\n                candidate = np.random.normal(best_solution, (ub - lb) / 10 * mutation_scale, self.dim)\n                candidate = np.clip(candidate, lb, ub)\n            else:\n                gradient = self.estimate_gradient(func, best_solution, lb, ub)\n                candidate = self.line_search(func, best_solution, gradient, lb, ub)\n\n            candidate_value = func(candidate)\n            self.evaluations += 1\n\n            if candidate_value < best_value:\n                best_solution, best_value = candidate, candidate_value\n                self.crossover_rate = min(1.0, self.crossover_rate + 0.02 * np.exp(-self.evaluations / self.budget))\n                # More adaptable learning rate scaling\n                self.learning_rate *= 1.1 if candidate_value < previous_value else 1.05\n                previous_value = candidate_value\n            else:\n                self.crossover_rate = max(0.2, self.crossover_rate - 0.01 * np.random.rand())\n                self.learning_rate *= 0.93\n\n        return best_solution, best_value\n\n    def estimate_gradient(self, func, solution, lb, ub, epsilon=1e-4):\n        gradient = np.zeros(self.dim)\n        adaptive_epsilon = epsilon * (1 + self.evaluations / self.budget)  # Adaptive epsilon\n        for i in range(self.dim):\n            perturb = np.zeros(self.dim)\n            perturb[i] = adaptive_epsilon\n            upper_sol = np.clip(solution + perturb, lb, ub)\n            lower_sol = np.clip(solution - perturb, lb, ub)\n            gradient[i] = (func(upper_sol) - func(lower_sol)) / adaptive_epsilon\n            self.evaluations += 2\n        return gradient\n\n    def line_search(self, func, solution, gradient, lb, ub, alpha=1e-2, beta=0.9):\n        step_size = self.learning_rate * np.linalg.norm(gradient) / (1 + self.evaluations / self.budget)\n        while step_size > 1e-5:\n            candidate = solution - step_size * gradient\n            candidate = np.clip(candidate, lb, ub)\n            if func(candidate) < func(solution):\n                return candidate\n            step_size *= beta\n            self.evaluations += 1\n        return solution", "name": "HybridOptimizer", "description": "Introduced adaptive mutation scaling based on function improvement for enhanced exploration and exploitation balance.", "configspace": "", "generation": 71, "fitness": 0.11265595905072871, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.113 with standard deviation 0.003. And the mean value of best solutions found was 0.122 (0. is the best).", "error": "", "parent_id": "cc908835-e3bf-4759-a5ed-b3ef10cf4b7b", "metadata": {"aucs": [0.10856238085203318, 0.11257053575021336, 0.11683496054993958], "final_y": [0.1319413787309507, 0.11998308443904127, 0.1141152276374382]}, "mutation_prompt": null}
{"id": "68704dde-9203-4cf8-978d-f8546276282d", "solution": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.crossover_rate = 0.8\n        self.learning_rate = 0.1  # Initial learning rate\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        best_solution = np.random.uniform(lb, ub, self.dim)\n        best_value = func(best_solution)\n        self.evaluations += 1\n        previous_value = best_value  # Track previous iteration's value\n\n        while self.evaluations < self.budget:\n            if np.random.rand() < self.crossover_rate:\n                # Enhanced dynamic mutation factor\n                mutation_scale = np.clip(0.1 + 0.4 * np.sin(self.evaluations / self.budget * np.pi), 0.05, 0.3)\n                candidate = np.random.normal(best_solution, (ub - lb) / 10 * mutation_scale, self.dim)\n                candidate = np.clip(candidate, lb, ub)\n            else:\n                gradient = self.estimate_gradient(func, best_solution, lb, ub)\n                candidate = self.line_search(func, best_solution, gradient, lb, ub)\n\n            candidate_value = func(candidate)\n            self.evaluations += 1\n\n            if candidate_value < best_value:\n                best_solution, best_value = candidate, candidate_value\n                self.crossover_rate = min(1.0, self.crossover_rate + 0.02 * np.exp(-self.evaluations / self.budget))\n                # More adaptable learning rate scaling\n                self.learning_rate *= 1.1 if candidate_value < previous_value else 1.05\n                previous_value = candidate_value\n            else:\n                self.crossover_rate = max(0.2, self.crossover_rate - 0.01 * np.random.rand())\n                self.learning_rate *= 0.93\n\n        return best_solution, best_value\n\n    def estimate_gradient(self, func, solution, lb, ub, epsilon=1e-4):\n        gradient = np.zeros(self.dim)\n        adaptive_epsilon = epsilon * (1 + self.evaluations / self.budget)  # Adaptive epsilon\n        for i in range(self.dim):\n            perturb = np.zeros(self.dim)\n            perturb[i] = adaptive_epsilon\n            upper_sol = np.clip(solution + perturb, lb, ub)\n            lower_sol = np.clip(solution - perturb, lb, ub)\n            gradient[i] = (func(upper_sol) - func(lower_sol)) / adaptive_epsilon\n            self.evaluations += 2\n        return gradient\n\n    def line_search(self, func, solution, gradient, lb, ub, alpha=1e-2, beta=0.9):\n        step_size = self.learning_rate * np.linalg.norm(gradient) / (1 + self.evaluations / self.budget)\n        while step_size > 1e-5:\n            candidate = solution - step_size * gradient\n            candidate = np.clip(candidate, lb, ub)\n            if func(candidate) < func(solution):\n                return candidate\n            step_size *= beta\n            self.evaluations += 1\n        return solution", "name": "HybridOptimizer", "description": "Enhanced mutation scaling function to improve exploration and convergence.", "configspace": "", "generation": 72, "fitness": 0.10967546072238592, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.110 with standard deviation 0.007. And the mean value of best solutions found was 0.130 (0. is the best).", "error": "", "parent_id": "cc908835-e3bf-4759-a5ed-b3ef10cf4b7b", "metadata": {"aucs": [0.10052501881644926, 0.11181842731394687, 0.11668293603676161], "final_y": [0.1539091820973777, 0.12080504539248227, 0.11408629112529345]}, "mutation_prompt": null}
{"id": "51d28489-736e-4077-bc16-ab736d3b23cd", "solution": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.crossover_rate = 0.8 - 0.2 * (dim / 100)  # Adaptive crossover rate based on dimensionality\n        self.learning_rate = 0.1  # Initial learning rate\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        best_solution = np.random.uniform(lb, ub, self.dim)\n        best_value = func(best_solution)\n        self.evaluations += 1\n        previous_value = best_value  # Track previous iteration's value\n\n        while self.evaluations < self.budget:\n            if np.random.rand() < self.crossover_rate:\n                # Enhanced dynamic mutation factor\n                mutation_scale = np.clip(0.1 + 0.3 * np.sin(self.evaluations / self.budget * np.pi), 0.05, 0.3)\n                candidate = np.random.normal(best_solution, (ub - lb) / 10 * mutation_scale, self.dim)\n                candidate = np.clip(candidate, lb, ub)\n            else:\n                gradient = self.estimate_gradient(func, best_solution, lb, ub)\n                candidate = self.line_search(func, best_solution, gradient, lb, ub)\n\n            candidate_value = func(candidate)\n            self.evaluations += 1\n\n            if candidate_value < best_value:\n                best_solution, best_value = candidate, candidate_value\n                self.crossover_rate = min(1.0, self.crossover_rate + 0.02 * np.exp(-self.evaluations / self.budget))\n                # More adaptable learning rate scaling\n                self.learning_rate *= 1.1 if candidate_value < previous_value else 1.05\n                previous_value = candidate_value\n            else:\n                self.crossover_rate = max(0.2, self.crossover_rate - 0.01 * np.random.rand())\n                self.learning_rate *= 0.93\n\n        return best_solution, best_value\n\n    def estimate_gradient(self, func, solution, lb, ub, epsilon=1e-4):\n        gradient = np.zeros(self.dim)\n        adaptive_epsilon = epsilon * (1 + self.evaluations / self.budget)  # Adaptive epsilon\n        for i in range(self.dim):\n            perturb = np.zeros(self.dim)\n            perturb[i] = adaptive_epsilon\n            upper_sol = np.clip(solution + perturb, lb, ub)\n            lower_sol = np.clip(solution - perturb, lb, ub)\n            gradient[i] = (func(upper_sol) - func(lower_sol)) / adaptive_epsilon\n            self.evaluations += 2\n        return gradient\n\n    def line_search(self, func, solution, gradient, lb, ub, alpha=1e-2, beta=0.9):\n        step_size = self.learning_rate * np.linalg.norm(gradient) / (1 + self.evaluations / self.budget)\n        while step_size > 1e-5:\n            candidate = solution - step_size * gradient\n            candidate = np.clip(candidate, lb, ub)\n            if func(candidate) < func(solution):\n                return candidate\n            step_size *= beta\n            self.evaluations += 1\n        return solution", "name": "HybridOptimizer", "description": "Introduced adaptive crossover rate initialization based on the dimensionality of the problem to enhance convergence speed.", "configspace": "", "generation": 73, "fitness": 0.11104771261192477, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.111 with standard deviation 0.006. And the mean value of best solutions found was 0.126 (0. is the best).", "error": "", "parent_id": "cc908835-e3bf-4759-a5ed-b3ef10cf4b7b", "metadata": {"aucs": [0.1033404073268337, 0.11257053575634113, 0.1172321947525995], "final_y": [0.14385913099277936, 0.11998308442832795, 0.1129253795314753]}, "mutation_prompt": null}
{"id": "4f262d8f-8190-464e-a6d4-a17b8181ac24", "solution": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.crossover_rate = 0.8\n        self.learning_rate = 0.1  # Initial learning rate\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        best_solution = np.random.uniform(lb, ub, self.dim)\n        best_value = func(best_solution)\n        self.evaluations += 1\n        previous_value = best_value  # Track previous iteration's value\n\n        while self.evaluations < self.budget:\n            if np.random.rand() < self.crossover_rate:\n                mutation_scale = np.clip(0.1 + 0.3 * np.sin(self.evaluations / self.budget * np.pi), 0.05, 0.3)\n                candidate = np.random.normal(best_solution, (ub - lb) / 10 * mutation_scale, self.dim)\n                candidate = np.clip(candidate, lb, ub)\n            else:\n                gradient = self.estimate_gradient(func, best_solution, lb, ub)\n                candidate = self.line_search(func, best_solution, gradient, lb, ub)\n\n            candidate_value = func(candidate)\n            self.evaluations += 1\n\n            if candidate_value < best_value:\n                best_solution, best_value = candidate, candidate_value\n                self.crossover_rate = min(1.0, self.crossover_rate + 0.05 * np.exp(-self.evaluations / self.budget))  # Increased adjustment rate\n                self.learning_rate *= 1.1 if candidate_value < previous_value else 1.05\n                previous_value = candidate_value\n            else:\n                self.crossover_rate = max(0.2, self.crossover_rate - 0.02 * np.random.rand())  # Increased decay rate\n                self.learning_rate *= 0.93\n\n        return best_solution, best_value\n\n    def estimate_gradient(self, func, solution, lb, ub, epsilon=1e-4):\n        gradient = np.zeros(self.dim)\n        adaptive_epsilon = epsilon * (1 + self.evaluations / self.budget)  # Adaptive epsilon\n        for i in range(self.dim):\n            perturb = np.zeros(self.dim)\n            perturb[i] = adaptive_epsilon\n            upper_sol = np.clip(solution + perturb, lb, ub)\n            lower_sol = np.clip(solution - perturb, lb, ub)\n            gradient[i] = (func(upper_sol) - func(lower_sol)) / adaptive_epsilon\n            self.evaluations += 2\n        return gradient\n\n    def line_search(self, func, solution, gradient, lb, ub, alpha=1e-2, beta=0.9):\n        step_size = self.learning_rate * np.linalg.norm(gradient) / (1 + self.evaluations / self.budget)\n        while step_size > 1e-5:\n            candidate = solution - step_size * gradient\n            candidate = np.clip(candidate, lb, ub)\n            if func(candidate) < func(solution):\n                return candidate\n            step_size *= beta\n            self.evaluations += 1\n        return solution", "name": "HybridOptimizer", "description": "Introduce adaptive crossover rate modulation for more balanced exploration and exploitation.", "configspace": "", "generation": 74, "fitness": 0.11089832790296565, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.111 with standard deviation 0.005. And the mean value of best solutions found was 0.127 (0. is the best).", "error": "", "parent_id": "cc908835-e3bf-4759-a5ed-b3ef10cf4b7b", "metadata": {"aucs": [0.10371230886946625, 0.11303690769094055, 0.11594576714849014], "final_y": [0.14539735215044913, 0.1200749178641971, 0.11625116059943386]}, "mutation_prompt": null}
{"id": "2277df81-6476-49d5-94df-6cc598fce93c", "solution": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.crossover_rate = 0.8\n        self.learning_rate = 0.1  # Initial learning rate\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        best_solution = np.random.uniform(lb, ub, self.dim)\n        best_value = func(best_solution)\n        self.evaluations += 1\n        previous_value = best_value  # Track previous iteration's value\n\n        population = [best_solution]\n\n        while self.evaluations < self.budget:\n            diversity = np.std(population, axis=0).mean()  # Calculate population diversity\n            if np.random.rand() < self.crossover_rate:\n                mutation_scale = np.clip(0.1 + 0.3 * np.sin(self.evaluations / self.budget * np.pi) + 0.1 * diversity, 0.05, 0.3)\n                candidate = np.random.normal(best_solution, (ub - lb) / 10 * mutation_scale, self.dim)\n                candidate = np.clip(candidate, lb, ub)\n            else:\n                gradient = self.estimate_gradient(func, best_solution, lb, ub)\n                candidate = self.line_search(func, best_solution, gradient, lb, ub)\n\n            candidate_value = func(candidate)\n            self.evaluations += 1\n\n            if candidate_value < best_value:\n                best_solution, best_value = candidate, candidate_value\n                self.crossover_rate = min(1.0, self.crossover_rate + 0.02 * np.exp(-self.evaluations / self.budget))\n                self.learning_rate *= 1.1 if candidate_value < previous_value else 1.05\n                previous_value = candidate_value\n            else:\n                self.crossover_rate = max(0.2, self.crossover_rate - 0.01 * np.random.rand())\n                self.learning_rate *= 0.93\n\n            population.append(candidate)  # Update population\n\n        return best_solution, best_value\n\n    def estimate_gradient(self, func, solution, lb, ub, epsilon=1e-4):\n        gradient = np.zeros(self.dim)\n        adaptive_epsilon = epsilon * (1 + self.evaluations / self.budget)  # Adaptive epsilon\n        for i in range(self.dim):\n            perturb = np.zeros(self.dim)\n            perturb[i] = adaptive_epsilon\n            upper_sol = np.clip(solution + perturb, lb, ub)\n            lower_sol = np.clip(solution - perturb, lb, ub)\n            gradient[i] = (func(upper_sol) - func(lower_sol)) / adaptive_epsilon\n            self.evaluations += 2\n        return gradient\n\n    def line_search(self, func, solution, gradient, lb, ub, alpha=1e-2, beta=0.9):\n        step_size = self.learning_rate * np.linalg.norm(gradient) / (1 + self.evaluations / self.budget)\n        while step_size > 1e-5:\n            candidate = solution - step_size * gradient\n            candidate = np.clip(candidate, lb, ub)\n            if func(candidate) < func(solution):\n                return candidate\n            step_size *= beta\n            self.evaluations += 1\n        return solution", "name": "HybridOptimizer", "description": "Introduced adaptive mutation scale based on population diversity to enhance exploration while maintaining stable convergence.", "configspace": "", "generation": 75, "fitness": 0.10931106783235418, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.109 with standard deviation 0.004. And the mean value of best solutions found was 0.130 (0. is the best).", "error": "", "parent_id": "cc908835-e3bf-4759-a5ed-b3ef10cf4b7b", "metadata": {"aucs": [0.10413347768230607, 0.10910054961735571, 0.11469917619740078], "final_y": [0.14319731758104892, 0.12799955437957078, 0.11891484759704174]}, "mutation_prompt": null}
{"id": "57d92a32-ff41-4632-93fe-b4295670b67c", "solution": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.crossover_rate = 0.8\n        self.learning_rate = 0.1  # Initial learning rate\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        best_solution = np.random.uniform(lb, ub, self.dim)\n        best_value = func(best_solution)\n        self.evaluations += 1\n        previous_value = best_value  # Track previous iteration's value\n\n        while self.evaluations < self.budget:\n            if np.random.rand() < self.crossover_rate:\n                # Enhanced dynamic mutation factor with adaptive adjustment\n                mutation_scale = np.clip(0.1 + 0.3 * np.sin(self.evaluations / self.budget * np.pi) * (1 - self.evaluations/self.budget), 0.05, 0.3)\n                candidate = np.random.normal(best_solution, (ub - lb) / 10 * mutation_scale, self.dim)\n                candidate = np.clip(candidate, lb, ub)\n            else:\n                gradient = self.estimate_gradient(func, best_solution, lb, ub)\n                candidate = self.line_search(func, best_solution, gradient, lb, ub)\n\n            candidate_value = func(candidate)\n            self.evaluations += 1\n\n            if candidate_value < best_value:\n                best_solution, best_value = candidate, candidate_value\n                self.crossover_rate = min(1.0, self.crossover_rate + 0.02 * np.exp(-self.evaluations / self.budget))\n                # More adaptable learning rate scaling\n                self.learning_rate *= 1.1 if candidate_value < previous_value else 1.05\n                previous_value = candidate_value\n            else:\n                self.crossover_rate = max(0.2, self.crossover_rate - 0.01 * np.random.rand())\n                self.learning_rate *= 0.93\n\n        return best_solution, best_value\n\n    def estimate_gradient(self, func, solution, lb, ub, epsilon=1e-4):\n        gradient = np.zeros(self.dim)\n        adaptive_epsilon = epsilon * (1 + self.evaluations / (2 * self.budget))  # More refined adaptive epsilon\n        for i in range(self.dim):\n            perturb = np.zeros(self.dim)\n            perturb[i] = adaptive_epsilon\n            upper_sol = np.clip(solution + perturb, lb, ub)\n            lower_sol = np.clip(solution - perturb, lb, ub)\n            gradient[i] = (func(upper_sol) - func(lower_sol)) / adaptive_epsilon\n            self.evaluations += 2\n        return gradient\n\n    def line_search(self, func, solution, gradient, lb, ub, alpha=1e-2, beta=0.9):\n        step_size = self.learning_rate * np.linalg.norm(gradient) / (1 + self.evaluations / self.budget)\n        while step_size > 1e-5:\n            candidate = solution - step_size * gradient\n            candidate = np.clip(candidate, lb, ub)\n            if func(candidate) < func(solution):\n                return candidate\n            step_size *= beta\n            self.evaluations += 1\n        return solution", "name": "HybridOptimizer", "description": "Introduced adaptive mutation scaling and improved gradient estimation for enhanced convergence.", "configspace": "", "generation": 76, "fitness": 0.1115051794068318, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.112 with standard deviation 0.005. And the mean value of best solutions found was 0.125 (0. is the best).", "error": "", "parent_id": "cc908835-e3bf-4759-a5ed-b3ef10cf4b7b", "metadata": {"aucs": [0.10526121247537024, 0.1129006143331931, 0.11635371141193207], "final_y": [0.140396456385235, 0.11935091808260068, 0.11485013433012403]}, "mutation_prompt": null}
{"id": "7a41c977-9e2a-4db6-8c50-9218258cc2a7", "solution": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.crossover_rate = 0.85  # Slightly increased initial crossover rate\n        self.learning_rate = 0.12  # Slightly increased initial learning rate\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        best_solution = np.random.uniform(lb, ub, self.dim)\n        best_value = func(best_solution)\n        self.evaluations += 1\n        previous_value = best_value\n\n        while self.evaluations < self.budget:\n            if np.random.rand() < self.crossover_rate:\n                mutation_scale = np.clip(0.15 + 0.25 * np.sin(self.evaluations / self.budget * np.pi), 0.05, 0.25)\n                candidate = np.random.normal(best_solution, (ub - lb) / 10 * mutation_scale, self.dim)\n                candidate = np.clip(candidate, lb, ub)\n            else:\n                gradient = self.estimate_gradient(func, best_solution, lb, ub)\n                candidate = self.line_search(func, best_solution, gradient, lb, ub)\n\n            candidate_value = func(candidate)\n            self.evaluations += 1\n\n            if candidate_value < best_value:\n                best_solution, best_value = candidate, candidate_value\n                self.crossover_rate = min(1.0, self.crossover_rate + 0.03 * np.exp(-self.evaluations / self.budget))  # Increased adjustment rate\n                self.learning_rate *= 1.15 if candidate_value < previous_value else 1.05  # Increased adaptive scaling factor\n                previous_value = candidate_value\n            else:\n                self.crossover_rate = max(0.2, self.crossover_rate - 0.015 * np.random.rand())  # Slightly increased decrement factor\n                self.learning_rate *= 0.91  # Slightly increased decay rate\n\n        return best_solution, best_value\n\n    def estimate_gradient(self, func, solution, lb, ub, epsilon=1e-4):\n        gradient = np.zeros(self.dim)\n        adaptive_epsilon = epsilon * (1 + self.evaluations / self.budget)\n        for i in range(self.dim):\n            perturb = np.zeros(self.dim)\n            perturb[i] = adaptive_epsilon\n            upper_sol = np.clip(solution + perturb, lb, ub)\n            lower_sol = np.clip(solution - perturb, lb, ub)\n            gradient[i] = (func(upper_sol) - func(lower_sol)) / adaptive_epsilon\n            self.evaluations += 2\n        return gradient\n\n    def line_search(self, func, solution, gradient, lb, ub, alpha=1e-2, beta=0.9):\n        step_size = self.learning_rate * np.linalg.norm(gradient) / (1 + self.evaluations / self.budget)\n        while step_size > 1e-5:\n            candidate = solution - step_size * gradient\n            candidate = np.clip(candidate, lb, ub)\n            if func(candidate) < func(solution):\n                return candidate\n            step_size *= beta\n            self.evaluations += 1\n        return solution", "name": "HybridOptimizer", "description": "Improved dynamic adaptation of crossover and learning rates to enhance convergence speed and robustness.", "configspace": "", "generation": 77, "fitness": 0.10987961476612347, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.110 with standard deviation 0.004. And the mean value of best solutions found was 0.129 (0. is the best).", "error": "", "parent_id": "cc908835-e3bf-4759-a5ed-b3ef10cf4b7b", "metadata": {"aucs": [0.10537718256776907, 0.10884743439019595, 0.1154142273404054], "final_y": [0.1421853049341999, 0.12941570683341042, 0.11601975005612153]}, "mutation_prompt": null}
{"id": "0019ec91-d142-42e4-9108-133682ba16f7", "solution": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.crossover_rate = 0.8\n        self.learning_rate = 0.1  # Initial learning rate\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        best_solution = np.random.uniform(lb, ub, self.dim)\n        best_value = func(best_solution)\n        self.evaluations += 1\n        previous_value = best_value  # Track previous iteration's value\n\n        while self.evaluations < self.budget:\n            if np.random.rand() < self.crossover_rate:\n                # Enhanced dynamic mutation factor\n                mutation_scale = np.clip(0.15 + 0.25 * np.sin(self.evaluations / self.budget * np.pi), 0.05, 0.35)  # Changed line\n                candidate = np.random.normal(best_solution, (ub - lb) / 10 * mutation_scale, self.dim)\n                candidate = np.clip(candidate, lb, ub)\n            else:\n                gradient = self.estimate_gradient(func, best_solution, lb, ub)\n                candidate = self.line_search(func, best_solution, gradient, lb, ub)\n\n            candidate_value = func(candidate)\n            self.evaluations += 1\n\n            if candidate_value < best_value:\n                best_solution, best_value = candidate, candidate_value\n                self.crossover_rate = min(1.0, self.crossover_rate + 0.03 * np.exp(-self.evaluations / self.budget))  # Changed line\n                # More adaptable learning rate scaling\n                self.learning_rate *= 1.15 if candidate_value < previous_value else 1.05  # Changed line\n                previous_value = candidate_value\n            else:\n                self.crossover_rate = max(0.25, self.crossover_rate - 0.02 * np.random.rand())  # Changed line\n                self.learning_rate *= 0.93\n\n        return best_solution, best_value\n\n    def estimate_gradient(self, func, solution, lb, ub, epsilon=1e-4):\n        gradient = np.zeros(self.dim)\n        adaptive_epsilon = epsilon * (1 + self.evaluations / self.budget)  # Adaptive epsilon\n        for i in range(self.dim):\n            perturb = np.zeros(self.dim)\n            perturb[i] = adaptive_epsilon\n            upper_sol = np.clip(solution + perturb, lb, ub)\n            lower_sol = np.clip(solution - perturb, lb, ub)\n            gradient[i] = (func(upper_sol) - func(lower_sol)) / adaptive_epsilon\n            self.evaluations += 2\n        return gradient\n\n    def line_search(self, func, solution, gradient, lb, ub, alpha=1e-2, beta=0.9):\n        step_size = self.learning_rate * np.linalg.norm(gradient) / (1 + self.evaluations / self.budget)\n        while step_size > 1e-5:\n            candidate = solution - step_size * gradient\n            candidate = np.clip(candidate, lb, ub)\n            if func(candidate) < func(solution):\n                return candidate\n            step_size *= beta\n            self.evaluations += 1\n        return solution", "name": "HybridOptimizer", "description": "Optimized adaptive gradient scaling and mutation for enhanced exploration and convergence stability.", "configspace": "", "generation": 78, "fitness": 0.10930642109997779, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.109 with standard deviation 0.005. And the mean value of best solutions found was 0.130 (0. is the best).", "error": "", "parent_id": "cc908835-e3bf-4759-a5ed-b3ef10cf4b7b", "metadata": {"aucs": [0.10286331764279688, 0.1092826883311524, 0.11577325732598409], "final_y": [0.1458933722196354, 0.12874918841990202, 0.11634970189928195]}, "mutation_prompt": null}
{"id": "3bfe0c87-e41d-45be-ba08-8298c67e6f02", "solution": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.crossover_rate = 0.8\n        self.learning_rate = 0.1  # Initial learning rate\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        best_solution = np.random.uniform(lb, ub, self.dim)\n        best_value = func(best_solution)\n        self.evaluations += 1\n        previous_value = best_value  # Track previous iteration's value\n\n        while self.evaluations < self.budget:\n            if np.random.rand() < self.crossover_rate:\n                mutation_scale = np.clip(0.1 + 0.4 * np.sin(self.evaluations / self.budget * np.pi), 0.05, 0.35)  # Change 1\n                candidate = np.random.normal(best_solution, (ub - lb) / 10 * mutation_scale, self.dim)\n                candidate = np.clip(candidate, lb, ub)\n            else:\n                gradient = self.estimate_gradient(func, best_solution, lb, ub)\n                candidate = self.line_search(func, best_solution, gradient, lb, ub)\n\n            candidate_value = func(candidate)\n            self.evaluations += 1\n\n            if candidate_value < best_value:\n                best_solution, best_value = candidate, candidate_value\n                self.crossover_rate = min(1.0, self.crossover_rate + 0.03 * np.exp(-self.evaluations / self.budget))  # Change 2\n                self.learning_rate *= 1.1 if candidate_value < previous_value else 1.05\n                previous_value = candidate_value\n            else:\n                self.crossover_rate = max(0.15, self.crossover_rate - 0.01 * np.random.rand())  # Change 3\n                self.learning_rate *= 0.93\n\n        return best_solution, best_value\n\n    def estimate_gradient(self, func, solution, lb, ub, epsilon=1e-4):\n        gradient = np.zeros(self.dim)\n        adaptive_epsilon = epsilon * (1 + self.evaluations / self.budget)  # Adaptive epsilon\n        for i in range(self.dim):\n            perturb = np.zeros(self.dim)\n            perturb[i] = adaptive_epsilon\n            upper_sol = np.clip(solution + perturb, lb, ub)\n            lower_sol = np.clip(solution - perturb, lb, ub)\n            gradient[i] = (func(upper_sol) - func(lower_sol)) / adaptive_epsilon\n            self.evaluations += 2\n        return gradient\n\n    def line_search(self, func, solution, gradient, lb, ub, alpha=1e-2, beta=0.9):\n        step_size = self.learning_rate * np.linalg.norm(gradient) / (1 + self.evaluations / self.budget)\n        while step_size > 1e-5:\n            candidate = solution - step_size * gradient\n            candidate = np.clip(candidate, lb, ub)\n            if func(candidate) < func(solution):\n                return candidate\n            step_size *= beta\n            self.evaluations += 1\n        return solution", "name": "HybridOptimizer", "description": "Adaptive crossover rate and mutation for enhanced exploration and exploitation balance.", "configspace": "", "generation": 79, "fitness": 0.11033455117278428, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.110 with standard deviation 0.006. And the mean value of best solutions found was 0.129 (0. is the best).", "error": "", "parent_id": "cc908835-e3bf-4759-a5ed-b3ef10cf4b7b", "metadata": {"aucs": [0.10124961482369566, 0.11367445959429223, 0.11607957910036493], "final_y": [0.15345331401155016, 0.11890101551323717, 0.11534702950167475]}, "mutation_prompt": null}
{"id": "948a5b15-e2bb-45d9-b935-48d3ac756499", "solution": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.crossover_rate = 0.8\n        self.learning_rate = 0.1  # Initial learning rate\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        best_solution = np.random.uniform(lb, ub, self.dim)\n        best_value = func(best_solution)\n        self.evaluations += 1\n        previous_value = best_value  # Track previous iteration's value\n\n        while self.evaluations < self.budget:\n            if np.random.rand() < self.crossover_rate:\n                # Enhanced dynamic mutation factor\n                mutation_scale = np.clip(0.1 + 0.4 * np.sin(self.evaluations / self.budget * np.pi), 0.05, 0.35)  # Changed\n                candidate = np.random.normal(best_solution, (ub - lb) / 10 * mutation_scale, self.dim)\n                candidate = np.clip(candidate, lb, ub)\n            else:\n                gradient = self.estimate_gradient(func, best_solution, lb, ub)\n                candidate = self.line_search(func, best_solution, gradient, lb, ub)\n\n            candidate_value = func(candidate)\n            self.evaluations += 1\n\n            if candidate_value < best_value:\n                best_solution, best_value = candidate, candidate_value\n                self.crossover_rate = min(1.0, self.crossover_rate + 0.03 * np.exp(-self.evaluations / self.budget))  # Changed\n                # More adaptable learning rate scaling\n                self.learning_rate *= 1.1 if candidate_value < previous_value else 1.05\n                previous_value = candidate_value\n            else:\n                self.crossover_rate = max(0.2, self.crossover_rate - 0.01 * np.random.rand())\n                self.learning_rate *= 0.93\n\n        return best_solution, best_value\n\n    def estimate_gradient(self, func, solution, lb, ub, epsilon=1e-4):\n        gradient = np.zeros(self.dim)\n        adaptive_epsilon = epsilon * (1 + self.evaluations / self.budget)  # Adaptive epsilon\n        for i in range(self.dim):\n            perturb = np.zeros(self.dim)\n            perturb[i] = adaptive_epsilon\n            upper_sol = np.clip(solution + perturb, lb, ub)\n            lower_sol = np.clip(solution - perturb, lb, ub)\n            gradient[i] = (func(upper_sol) - func(lower_sol)) / adaptive_epsilon\n            self.evaluations += 2\n        return gradient\n\n    def line_search(self, func, solution, gradient, lb, ub, alpha=1e-2, beta=0.9):\n        step_size = self.learning_rate * np.linalg.norm(gradient) / (1 + self.evaluations / self.budget)\n        while step_size > 1e-5:\n            candidate = solution - step_size * gradient\n            candidate = np.clip(candidate, lb, ub)\n            if func(candidate) < func(solution):\n                return candidate\n            step_size *= beta\n            self.evaluations += 1\n        return solution", "name": "HybridOptimizer", "description": "Adaptive mutation scale enhancement and crossover rate refinement for improved convergence stability and diversity.", "configspace": "", "generation": 80, "fitness": 0.1103457170069438, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.110 with standard deviation 0.007. And the mean value of best solutions found was 0.129 (0. is the best).", "error": "", "parent_id": "cc908835-e3bf-4759-a5ed-b3ef10cf4b7b", "metadata": {"aucs": [0.10124961482368655, 0.11367445959422395, 0.11611307660292092], "final_y": [0.15345331400331408, 0.11890101548372789, 0.11525049056296466]}, "mutation_prompt": null}
{"id": "33d828a7-4f93-456f-9397-ae2d70a0e7da", "solution": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.crossover_rate = 0.8\n        self.learning_rate = 0.1  # Initial learning rate\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        best_solution = np.random.uniform(lb, ub, self.dim)\n        best_value = func(best_solution)\n        self.evaluations += 1\n        previous_value = best_value  # Track previous iteration's value\n\n        while self.evaluations < self.budget:\n            if np.random.rand() < self.crossover_rate:\n                # Enhanced dynamic mutation factor\n                mutation_scale = np.clip(0.1 + 0.3 * np.sin(self.evaluations / self.budget * np.pi), 0.05, 0.3)\n                candidate = np.random.normal(best_solution, (ub - lb) / 10 * mutation_scale, self.dim)\n                candidate = np.clip(candidate, lb, ub)\n            else:\n                gradient = self.estimate_gradient(func, best_solution, lb, ub)\n                candidate = self.line_search(func, best_solution, gradient, lb, ub)\n\n            candidate_value = func(candidate)\n            self.evaluations += 1\n\n            if candidate_value < best_value:\n                best_solution, best_value = candidate, candidate_value\n                self.crossover_rate = min(1.0, self.crossover_rate + 0.02 * np.exp(-self.evaluations / self.budget))\n                # More adaptable learning rate scaling\n                self.learning_rate *= 1.1 if candidate_value < previous_value else 1.05\n                previous_value = candidate_value\n            else:\n                self.crossover_rate = max(0.2, self.crossover_rate - 0.01 * np.random.rand())\n                self.learning_rate *= 0.93\n\n        return best_solution, best_value\n\n    def estimate_gradient(self, func, solution, lb, ub, epsilon=1e-4):\n        gradient = np.zeros(self.dim)\n        adaptive_epsilon = epsilon * (1 + self.evaluations / self.budget)  # Adaptive epsilon\n        for i in range(self.dim):\n            perturb = np.zeros(self.dim)\n            perturb[i] = adaptive_epsilon\n            upper_sol = np.clip(solution + perturb, lb, ub)\n            lower_sol = np.clip(solution - perturb, lb, ub)\n            gradient[i] = (func(upper_sol) - func(lower_sol)) / adaptive_epsilon\n            self.evaluations += 2\n        return gradient\n\n    def line_search(self, func, solution, gradient, lb, ub, alpha=1e-2, beta=0.9):\n        step_size = self.learning_rate * np.linalg.norm(gradient) / (1 + self.evaluations / self.budget)\n        while step_size > 1e-5:\n            candidate = solution - step_size * gradient\n            candidate = np.clip(candidate, lb, ub)\n            if func(candidate) < func(solution) - 1e-6:  # Changed from \"<\" to \"< func(solution) - 1e-6\"\n                return candidate\n            step_size *= beta\n            self.evaluations += 1\n        return solution", "name": "HybridOptimizer", "description": "Improved line search strategy to potentially enhance candidate solutions.", "configspace": "", "generation": 81, "fitness": 0.10648146478789881, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.106 with standard deviation 0.005. And the mean value of best solutions found was 0.126 (0. is the best).", "error": "", "parent_id": "cc908835-e3bf-4759-a5ed-b3ef10cf4b7b", "metadata": {"aucs": [0.10167172108691036, 0.10395989099941694, 0.11381278227736913], "final_y": [0.14399134707614392, 0.11947802086640658, 0.11361267676836084]}, "mutation_prompt": null}
{"id": "b0b272a2-bdbb-4aa1-aac2-e4aa1d9ea8cd", "solution": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.crossover_rate = 0.8\n        self.learning_rate = 0.1  # Initial learning rate\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        best_solution = np.random.uniform(lb, ub, self.dim)\n        best_value = func(best_solution)\n        self.evaluations += 1\n        previous_value = best_value\n\n        while self.evaluations < self.budget:\n            adaptation_factor = np.log(1 + self.evaluations / self.budget)  # New adaptation factor\n            if np.random.rand() < self.crossover_rate * adaptation_factor:  # Use adaptation factor\n                mutation_scale = np.clip(0.1 + 0.3 * np.sin(self.evaluations / self.budget * np.pi), 0.05, 0.3)\n                candidate = np.random.normal(best_solution, (ub - lb) / 10 * mutation_scale, self.dim)\n                candidate = np.clip(candidate, lb, ub)\n            else:\n                gradient = self.estimate_gradient(func, best_solution, lb, ub)\n                candidate = self.line_search(func, best_solution, gradient, lb, ub)\n\n            candidate_value = func(candidate)\n            self.evaluations += 1\n\n            if candidate_value < best_value:\n                best_solution, best_value = candidate, candidate_value\n                self.crossover_rate = min(1.0, self.crossover_rate + 0.02 * np.exp(-self.evaluations / self.budget))\n                self.learning_rate *= 1.1 if candidate_value < previous_value else 1.05\n                previous_value = candidate_value\n            else:\n                self.crossover_rate = max(0.2, self.crossover_rate - 0.01 * np.random.rand())\n                self.learning_rate *= 0.93\n\n        return best_solution, best_value\n\n    def estimate_gradient(self, func, solution, lb, ub, epsilon=1e-4):\n        gradient = np.zeros(self.dim)\n        adaptive_epsilon = epsilon * (1 + self.evaluations / self.budget)\n        for i in range(self.dim):\n            perturb = np.zeros(self.dim)\n            perturb[i] = adaptive_epsilon\n            upper_sol = np.clip(solution + perturb, lb, ub)\n            lower_sol = np.clip(solution - perturb, lb, ub)\n            gradient[i] = (func(upper_sol) - func(lower_sol)) / adaptive_epsilon\n            self.evaluations += 2\n        return gradient\n\n    def line_search(self, func, solution, gradient, lb, ub, alpha=1e-2, beta=0.9):\n        step_size = self.learning_rate * np.linalg.norm(gradient) / (1 + self.evaluations / self.budget)\n        while step_size > 1e-5:\n            candidate = solution - step_size * gradient\n            candidate = np.clip(candidate, lb, ub)\n            if func(candidate) < func(solution):\n                return candidate\n            step_size *= beta\n            self.evaluations += 1\n        return solution", "name": "HybridOptimizer", "description": "Introduced adaptive crossover rate and mutation strategy based on evaluation progress to improve exploration and exploitation balance.", "configspace": "", "generation": 82, "fitness": 0.09205433167079217, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.092 with standard deviation 0.010. And the mean value of best solutions found was 0.138 (0. is the best).", "error": "", "parent_id": "cc908835-e3bf-4759-a5ed-b3ef10cf4b7b", "metadata": {"aucs": [0.08769937270729555, 0.08308020165013097, 0.10538342065494999], "final_y": [0.1646251143867301, 0.13339251448288858, 0.11495327143048983]}, "mutation_prompt": null}
{"id": "c5a88617-c1a5-43b4-8067-3de64a02158c", "solution": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.crossover_rate = 0.8\n        self.learning_rate = 0.1  # Initial learning rate\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        best_solution = np.random.uniform(lb, ub, self.dim)\n        best_value = func(best_solution)\n        self.evaluations += 1\n        previous_value = best_value  # Track previous iteration's value\n\n        while self.evaluations < self.budget:\n            if np.random.rand() < self.crossover_rate:\n                # Enhanced dynamic mutation factor\n                mutation_scale = np.clip(0.1 + 0.3 * np.sin(self.evaluations / self.budget * np.pi), 0.05, 0.3)\n                candidate = np.random.normal(best_solution, (ub - lb) / 10 * mutation_scale, self.dim)\n                candidate = np.clip(candidate, lb, ub)\n            else:\n                gradient = self.estimate_gradient(func, best_solution, lb, ub)\n                candidate = self.line_search(func, best_solution, gradient, lb, ub)\n\n            candidate_value = func(candidate)\n            self.evaluations += 1\n\n            if candidate_value < best_value:\n                best_solution, best_value = candidate, candidate_value\n                self.crossover_rate = min(1.0, 0.5 + 0.5 * np.cos(self.evaluations / self.budget * np.pi))  # Adjusted adaptive crossover rate\n                # More adaptable learning rate scaling\n                self.learning_rate *= 1.1 if candidate_value < previous_value else 1.05\n                previous_value = candidate_value\n            else:\n                self.crossover_rate = max(0.2, self.crossover_rate - 0.01 * np.random.rand())\n                self.learning_rate *= 0.93\n\n        return best_solution, best_value\n\n    def estimate_gradient(self, func, solution, lb, ub, epsilon=1e-4):\n        gradient = np.zeros(self.dim)\n        adaptive_epsilon = epsilon * (1 + self.evaluations / self.budget)  # Adaptive epsilon\n        for i in range(self.dim):\n            perturb = np.zeros(self.dim)\n            perturb[i] = adaptive_epsilon\n            upper_sol = np.clip(solution + perturb, lb, ub)\n            lower_sol = np.clip(solution - perturb, lb, ub)\n            gradient[i] = (func(upper_sol) - func(lower_sol)) / adaptive_epsilon\n            self.evaluations += 2\n        return gradient\n\n    def line_search(self, func, solution, gradient, lb, ub, alpha=1e-2, beta=0.9):\n        step_size = self.learning_rate * np.linalg.norm(gradient) / (1 + self.evaluations / self.budget)\n        while step_size > 1e-5:\n            candidate = solution - step_size * gradient\n            candidate = np.clip(candidate, lb, ub)\n            if func(candidate) < func(solution):\n                return candidate\n            step_size *= beta\n            self.evaluations += 1\n        return solution", "name": "HybridOptimizer", "description": "Introduce adaptive crossover rate to better balance exploration and exploitation.", "configspace": "", "generation": 83, "fitness": 0.11092455181684013, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.111 with standard deviation 0.006. And the mean value of best solutions found was 0.128 (0. is the best).", "error": "", "parent_id": "cc908835-e3bf-4759-a5ed-b3ef10cf4b7b", "metadata": {"aucs": [0.10254435767869385, 0.11361173519772916, 0.11661756257409739], "final_y": [0.14896107090074628, 0.11881983130018248, 0.11500295437663433]}, "mutation_prompt": null}
{"id": "bc754a6c-4d47-4d2b-90e9-1e8bd5a3806f", "solution": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.crossover_rate = 0.8\n        self.learning_rate = 0.1  # Initial learning rate\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        best_solution = np.random.uniform(lb, ub, self.dim)\n        best_value = func(best_solution)\n        self.evaluations += 1\n        previous_value = best_value  # Track previous iteration's value\n\n        while self.evaluations < self.budget:\n            if np.random.rand() < self.crossover_rate:\n                # Enhanced dynamic mutation factor\n                mutation_scale = np.clip(0.1 + 0.3 * np.sin(self.evaluations / self.budget * np.pi), 0.05, 0.3)\n                candidate = np.random.normal(best_solution, (ub - lb) / 10 * mutation_scale, self.dim)\n                candidate = np.clip(candidate, lb, ub)\n            else:\n                gradient = self.estimate_gradient(func, best_solution, lb, ub)\n                candidate = self.line_search(func, best_solution, gradient, lb, ub)\n\n            candidate_value = func(candidate)\n            self.evaluations += 1\n\n            if candidate_value < best_value:\n                best_solution, best_value = candidate, candidate_value\n                self.crossover_rate = min(1.0, self.crossover_rate + 0.03 * np.exp(-self.evaluations / self.budget))  # More aggressive adaptation\n                self.learning_rate *= 1.1 if candidate_value < previous_value else 1.08  # Slightly more aggressive scaling\n                previous_value = candidate_value\n            else:\n                self.crossover_rate = max(0.2, self.crossover_rate - 0.01 * np.random.rand())\n                self.learning_rate *= 0.93\n\n        return best_solution, best_value\n\n    def estimate_gradient(self, func, solution, lb, ub, epsilon=1e-4):\n        gradient = np.zeros(self.dim)\n        adaptive_epsilon = epsilon * (1 + self.evaluations / self.budget)  # Adaptive epsilon\n        for i in range(self.dim):\n            perturb = np.zeros(self.dim)\n            perturb[i] = adaptive_epsilon\n            upper_sol = np.clip(solution + perturb, lb, ub)\n            lower_sol = np.clip(solution - perturb, lb, ub)\n            gradient[i] = (func(upper_sol) - func(lower_sol)) / adaptive_epsilon\n            self.evaluations += 2\n        return gradient\n\n    def line_search(self, func, solution, gradient, lb, ub, alpha=1e-2, beta=0.9):\n        step_size = self.learning_rate * np.linalg.norm(gradient) / (1 + self.evaluations / self.budget)\n        while step_size > 1e-5:\n            candidate = solution - step_size * gradient\n            candidate = np.clip(candidate, lb, ub)\n            if func(candidate) < func(solution):\n                return candidate\n            step_size *= beta\n            self.evaluations += 1\n        return solution", "name": "HybridOptimizer", "description": "Improved adaptation of crossover rate and learning rate for better convergence stability and exploration.", "configspace": "", "generation": 84, "fitness": 0.11090388116389016, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.111 with standard deviation 0.006. And the mean value of best solutions found was 0.126 (0. is the best).", "error": "", "parent_id": "cc908835-e3bf-4759-a5ed-b3ef10cf4b7b", "metadata": {"aucs": [0.1024140439783453, 0.11351967166250987, 0.11677792785081531], "final_y": [0.14558555169994425, 0.1188732383703488, 0.11469410421950088]}, "mutation_prompt": null}
{"id": "a4dc0353-8b48-49c5-8280-4b90d0a50c95", "solution": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.crossover_rate = 0.8\n        self.learning_rate = 0.1\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 5  # New population size for diversity\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        values = np.array([func(ind) for ind in population])\n        self.evaluations += population_size\n        best_idx = np.argmin(values)\n        best_solution, best_value = population[best_idx], values[best_idx]\n\n        while self.evaluations < self.budget:\n            diversity = np.std(population, axis=0).mean()  # Measure diversity\n            self.crossover_rate = 0.5 + 0.5 * (1 - diversity)  # Adapt crossover rate\n\n            for i in range(population_size):\n                if np.random.rand() < self.crossover_rate:\n                    mutation_scale = np.clip(0.05 + 0.25 * diversity, 0.05, 0.3)\n                    candidate = np.random.normal(population[i], (ub - lb) / 10 * mutation_scale, self.dim)\n                    candidate = np.clip(candidate, lb, ub)\n                else:\n                    gradient = self.estimate_gradient(func, population[i], lb, ub)\n                    candidate = self.line_search(func, population[i], gradient, lb, ub)\n\n                candidate_value = func(candidate)\n                self.evaluations += 1\n\n                if candidate_value < values[i]:\n                    population[i], values[i] = candidate, candidate_value\n                    if candidate_value < best_value:\n                        best_solution, best_value = candidate, candidate_value\n                        self.learning_rate *= 1.1 if candidate_value < best_value else 1.05\n\n        return best_solution, best_value\n\n    def estimate_gradient(self, func, solution, lb, ub, epsilon=1e-4):\n        gradient = np.zeros(self.dim)\n        adaptive_epsilon = epsilon * (1 + self.evaluations / self.budget)\n        for i in range(self.dim):\n            perturb = np.zeros(self.dim)\n            perturb[i] = adaptive_epsilon\n            upper_sol = np.clip(solution + perturb, lb, ub)\n            lower_sol = np.clip(solution - perturb, lb, ub)\n            gradient[i] = (func(upper_sol) - func(lower_sol)) / adaptive_epsilon\n            self.evaluations += 2\n        return gradient\n\n    def line_search(self, func, solution, gradient, lb, ub, alpha=1e-2, beta=0.9):\n        step_size = self.learning_rate * np.linalg.norm(gradient) / (1 + self.evaluations / self.budget)\n        while step_size > 1e-5:\n            candidate = solution - step_size * gradient\n            candidate = np.clip(candidate, lb, ub)\n            if func(candidate) < func(solution):\n                return candidate\n            step_size *= beta\n            self.evaluations += 1\n        return solution", "name": "HybridOptimizer", "description": "Introduced adaptive crossover and mutation based on population diversity for improved convergence.", "configspace": "", "generation": 85, "fitness": 0.09026799539062014, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.090 with standard deviation 0.005. And the mean value of best solutions found was 0.191 (0. is the best).", "error": "", "parent_id": "cc908835-e3bf-4759-a5ed-b3ef10cf4b7b", "metadata": {"aucs": [0.084204507598444, 0.0964501720894737, 0.0901493064839427], "final_y": [0.2131769095543118, 0.17010703653801396, 0.19108737474113735]}, "mutation_prompt": null}
{"id": "9f3f42aa-afbf-4f3b-aa01-6645afbd7f9a", "solution": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.crossover_rate = 0.8\n        self.learning_rate = 0.1  # Initial learning rate\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        best_solution = np.random.uniform(lb, ub, self.dim)\n        best_value = func(best_solution)\n        self.evaluations += 1\n        previous_value = best_value  # Track previous iteration's value\n\n        while self.evaluations < self.budget:\n            if np.random.rand() < self.crossover_rate:\n                # Enhanced dynamic mutation factor\n                mutation_scale = np.clip(0.2 + 0.3 * np.sin(self.evaluations / self.budget * np.pi), 0.05, 0.3)\n                candidate = np.random.normal(best_solution, (ub - lb) / 10 * mutation_scale, self.dim)\n                candidate = np.clip(candidate, lb, ub)\n            else:\n                gradient = self.estimate_gradient(func, best_solution, lb, ub)\n                candidate = self.line_search(func, best_solution, gradient, lb, ub)\n\n            candidate_value = func(candidate)\n            self.evaluations += 1\n\n            if candidate_value < best_value:\n                best_solution, best_value = candidate, candidate_value\n                self.crossover_rate = min(1.0, self.crossover_rate + 0.025 * np.exp(-self.evaluations / self.budget))\n                # More adaptable learning rate scaling\n                self.learning_rate *= 1.12 if candidate_value < previous_value else 1.05\n                previous_value = candidate_value\n            else:\n                self.crossover_rate = max(0.2, self.crossover_rate - 0.01 * np.random.rand())\n                self.learning_rate *= 0.93\n\n        return best_solution, best_value\n\n    def estimate_gradient(self, func, solution, lb, ub, epsilon=1e-4):\n        gradient = np.zeros(self.dim)\n        adaptive_epsilon = epsilon * (1 + self.evaluations / self.budget)  # Adaptive epsilon\n        for i in range(self.dim):\n            perturb = np.zeros(self.dim)\n            perturb[i] = adaptive_epsilon\n            upper_sol = np.clip(solution + perturb, lb, ub)\n            lower_sol = np.clip(solution - perturb, lb, ub)\n            gradient[i] = (func(upper_sol) - func(lower_sol)) / adaptive_epsilon\n            self.evaluations += 2\n        return gradient\n\n    def line_search(self, func, solution, gradient, lb, ub, alpha=1e-2, beta=0.9):\n        step_size = self.learning_rate * np.linalg.norm(gradient) / (1 + self.evaluations / self.budget)\n        while step_size > 1e-5:\n            candidate = solution - step_size * gradient\n            candidate = np.clip(candidate, lb, ub)\n            if func(candidate) < func(solution):\n                return candidate\n            step_size *= beta\n            self.evaluations += 1\n        return solution", "name": "HybridOptimizer", "description": "Improved adaptive learning rate and mutation scale adjustment for better exploration and convergence.", "configspace": "", "generation": 86, "fitness": 0.11207473987744643, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.112 with standard deviation 0.005. And the mean value of best solutions found was 0.123 (0. is the best).", "error": "", "parent_id": "cc908835-e3bf-4759-a5ed-b3ef10cf4b7b", "metadata": {"aucs": [0.10555898244066775, 0.11301447695442723, 0.1176507602372443], "final_y": [0.13573694972569172, 0.11952817884230538, 0.11264110695738316]}, "mutation_prompt": null}
{"id": "4e7ca38d-c5db-443d-8bb3-4d6704eb13c5", "solution": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.crossover_rate = 0.8\n        self.learning_rate = 0.1  # Initial learning rate\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        best_solution = np.random.uniform(lb, ub, self.dim)\n        best_value = func(best_solution)\n        self.evaluations += 1\n        previous_value = best_value  # Track previous iteration's value\n\n        while self.evaluations < self.budget:\n            if np.random.rand() < self.crossover_rate:\n                # Enhanced dynamic mutation factor\n                mutation_scale = np.clip(0.1 + 0.3 * np.sin(self.evaluations / self.budget * np.pi), 0.05, 0.3)\n                candidate = np.random.normal(best_solution, (ub - lb) / 10 * mutation_scale, self.dim)\n                candidate = np.clip(candidate, lb, ub)\n            else:\n                gradient = self.estimate_gradient(func, best_solution, lb, ub)\n                candidate = self.line_search(func, best_solution, gradient, lb, ub)\n\n            candidate_value = func(candidate)\n            self.evaluations += 1\n\n            if candidate_value < best_value:\n                best_solution, best_value = candidate, candidate_value\n                self.crossover_rate = min(1.0, self.crossover_rate + 0.02 * np.exp(-self.evaluations / self.budget))\n                # More adaptable learning rate scaling\n                self.learning_rate *= 1.1 if candidate_value < previous_value else 1.05\n                self.crossover_rate += 0.01  # <<- Changed line\n                previous_value = candidate_value\n            else:\n                self.crossover_rate = max(0.2, self.crossover_rate - 0.01 * np.random.rand())\n                self.learning_rate *= 0.93\n\n        return best_solution, best_value\n\n    def estimate_gradient(self, func, solution, lb, ub, epsilon=1e-4):\n        gradient = np.zeros(self.dim)\n        adaptive_epsilon = epsilon * (1 + self.evaluations / self.budget)  # Adaptive epsilon\n        for i in range(self.dim):\n            perturb = np.zeros(self.dim)\n            perturb[i] = adaptive_epsilon\n            upper_sol = np.clip(solution + perturb, lb, ub)\n            lower_sol = np.clip(solution - perturb, lb, ub)\n            gradient[i] = (func(upper_sol) - func(lower_sol)) / adaptive_epsilon\n            self.evaluations += 2\n        return gradient\n\n    def line_search(self, func, solution, gradient, lb, ub, alpha=1e-2, beta=0.9):\n        step_size = self.learning_rate * np.linalg.norm(gradient) / (1 + self.evaluations / self.budget)\n        while step_size > 1e-5:\n            candidate = solution - step_size * gradient\n            candidate = np.clip(candidate, lb, ub)\n            if func(candidate) < func(solution):\n                return candidate\n            step_size *= beta\n            self.evaluations += 1\n        return solution", "name": "HybridOptimizer", "description": "Introduced adaptive crossover rate adjustment based on candidate improvement.", "configspace": "", "generation": 87, "fitness": 0.11064342065588706, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.111 with standard deviation 0.007. And the mean value of best solutions found was 0.129 (0. is the best).", "error": "", "parent_id": "cc908835-e3bf-4759-a5ed-b3ef10cf4b7b", "metadata": {"aucs": [0.10131406919876307, 0.11323730219869654, 0.11737889057020157], "final_y": [0.15353880179198354, 0.12027971035716889, 0.11346813342923057]}, "mutation_prompt": null}
{"id": "358387cc-c3e7-42cd-aad4-a2648a75a20e", "solution": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.crossover_rate = 0.8\n        self.learning_rate = 0.1  # Initial learning rate\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        best_solution = np.random.uniform(lb, ub, self.dim)\n        best_value = func(best_solution)\n        self.evaluations += 1\n        previous_value = best_value  # Track previous iteration's value\n\n        while self.evaluations < self.budget:\n            if np.random.rand() < self.crossover_rate:\n                # Adaptive mutation factor based on evaluation progress\n                mutation_scale = np.clip(0.1 + 0.4 * np.sin(self.evaluations / self.budget * np.pi), 0.05, 0.35)\n                candidate = np.random.normal(best_solution, (ub - lb) / 10 * mutation_scale, self.dim)\n                candidate = np.clip(candidate, lb, ub)\n            else:\n                gradient = self.estimate_gradient(func, best_solution, lb, ub)\n                candidate = self.line_search(func, best_solution, gradient, lb, ub)\n\n            candidate_value = func(candidate)\n            self.evaluations += 1\n\n            if candidate_value < best_value:\n                best_solution, best_value = candidate, candidate_value\n                self.crossover_rate = min(1.0, self.crossover_rate + 0.02 * np.exp(-self.evaluations / self.budget))\n                # More adaptable learning rate scaling\n                self.learning_rate *= 1.1 if candidate_value < previous_value else 1.05\n                previous_value = candidate_value\n            else:\n                self.crossover_rate = max(0.2, self.crossover_rate - 0.01 * np.random.rand())\n                self.learning_rate *= 0.93\n\n        return best_solution, best_value\n\n    def estimate_gradient(self, func, solution, lb, ub, epsilon=1e-4):\n        gradient = np.zeros(self.dim)\n        adaptive_epsilon = epsilon * (1 + self.evaluations / self.budget)  # Adaptive epsilon\n        for i in range(self.dim):\n            perturb = np.zeros(self.dim)\n            perturb[i] = adaptive_epsilon\n            upper_sol = np.clip(solution + perturb, lb, ub)\n            lower_sol = np.clip(solution - perturb, lb, ub)\n            gradient[i] = (func(upper_sol) - func(lower_sol)) / adaptive_epsilon\n            self.evaluations += 2\n        return gradient\n\n    def line_search(self, func, solution, gradient, lb, ub, alpha=1e-2, beta=0.9):\n        step_size = self.learning_rate * np.linalg.norm(gradient) / (1 + self.evaluations / self.budget)\n        while step_size > 1e-5:\n            candidate = solution - step_size * gradient\n            candidate = np.clip(candidate, lb, ub)\n            if func(candidate) < func(solution):\n                return candidate\n            step_size *= beta\n            self.evaluations += 1\n        return solution", "name": "HybridOptimizer", "description": "Integrate adaptive mutation scaling to balance exploration and exploitation more effectively.", "configspace": "", "generation": 88, "fitness": 0.1096144212335753, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.110 with standard deviation 0.007. And the mean value of best solutions found was 0.130 (0. is the best).", "error": "", "parent_id": "cc908835-e3bf-4759-a5ed-b3ef10cf4b7b", "metadata": {"aucs": [0.10042121342397903, 0.11173911423998528, 0.11668293603676161], "final_y": [0.15463088179319273, 0.12070849712307041, 0.11408629112529345]}, "mutation_prompt": null}
{"id": "3a249546-71e9-4be4-864b-cef4a2627663", "solution": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.crossover_rate = 0.8\n        self.learning_rate = 0.1  # Initial learning rate\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        best_solution = np.random.uniform(lb, ub, self.dim)\n        best_value = func(best_solution)\n        self.evaluations += 1\n        previous_value = best_value  # Track previous iteration's value\n\n        while self.evaluations < self.budget:\n            if np.random.rand() < self.crossover_rate:\n                # Enhanced dynamic mutation factor\n                temperature = 1 - self.evaluations / self.budget  # Temperature-based mutation scale\n                mutation_scale = np.clip(0.1 + 0.3 * temperature * np.sin(self.evaluations / self.budget * np.pi), 0.05, 0.3)\n                candidate = np.random.normal(best_solution, (ub - lb) / 10 * mutation_scale, self.dim)\n                candidate = np.clip(candidate, lb, ub)\n            else:\n                gradient = self.estimate_gradient(func, best_solution, lb, ub)\n                candidate = self.line_search(func, best_solution, gradient, lb, ub)\n\n            candidate_value = func(candidate)\n            self.evaluations += 1\n\n            if candidate_value < best_value:\n                best_solution, best_value = candidate, candidate_value\n                self.crossover_rate = min(1.0, self.crossover_rate + 0.02 * np.exp(-self.evaluations / self.budget))\n                # More adaptable learning rate scaling\n                self.learning_rate *= 1.1 if candidate_value < previous_value else 1.05\n                previous_value = candidate_value\n            else:\n                self.crossover_rate = max(0.2, self.crossover_rate - 0.01 * np.random.rand())\n                self.learning_rate *= 0.93\n\n        return best_solution, best_value\n\n    def estimate_gradient(self, func, solution, lb, ub, epsilon=1e-4):\n        gradient = np.zeros(self.dim)\n        adaptive_epsilon = epsilon * (1 + self.evaluations / self.budget)  # Adaptive epsilon\n        for i in range(self.dim):\n            perturb = np.zeros(self.dim)\n            perturb[i] = adaptive_epsilon\n            upper_sol = np.clip(solution + perturb, lb, ub)\n            lower_sol = np.clip(solution - perturb, lb, ub)\n            gradient[i] = (func(upper_sol) - func(lower_sol)) / adaptive_epsilon\n            self.evaluations += 2\n        return gradient\n\n    def line_search(self, func, solution, gradient, lb, ub, alpha=1e-2, beta=0.9):\n        step_size = self.learning_rate * np.linalg.norm(gradient) / (1 + self.evaluations / self.budget)\n        while step_size > 1e-5:\n            candidate = solution - step_size * gradient\n            candidate = np.clip(candidate, lb, ub)\n            if func(candidate) < func(solution):\n                return candidate\n            step_size *= beta\n            self.evaluations += 1\n        return solution", "name": "HybridOptimizer", "description": "Introduced temperature-based mutation scale to improve exploration with better adaptive control over mutation intensity.", "configspace": "", "generation": 89, "fitness": 0.11150518175773605, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.112 with standard deviation 0.005. And the mean value of best solutions found was 0.125 (0. is the best).", "error": "", "parent_id": "cc908835-e3bf-4759-a5ed-b3ef10cf4b7b", "metadata": {"aucs": [0.10526121366903707, 0.11290061580269928, 0.1163537158014718], "final_y": [0.14039645290425096, 0.11935091152629762, 0.11485012237167735]}, "mutation_prompt": null}
{"id": "616d88f0-d4ba-4937-b557-1dca50059f11", "solution": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.crossover_rate = 0.8\n        self.learning_rate = 0.1  # Initial learning rate\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        best_solution = np.random.uniform(lb, ub, self.dim)\n        best_value = func(best_solution)\n        self.evaluations += 1\n        previous_value = best_value  # Track previous iteration's value\n\n        while self.evaluations < self.budget:\n            if np.random.rand() < self.crossover_rate:\n                # Enhanced dynamic mutation factor\n                mutation_scale = np.clip(0.1 + 0.3 * np.sin(self.evaluations / self.budget * np.pi), 0.05, 0.3)\n                candidate = np.random.normal(best_solution, (ub - lb) / 10 * mutation_scale, self.dim)\n                candidate = np.clip(candidate, lb, ub)\n            else:\n                gradient = self.estimate_gradient(func, best_solution, lb, ub)\n                candidate = self.line_search(func, best_solution, gradient, lb, ub)\n\n            candidate_value = func(candidate)\n            self.evaluations += 1\n\n            if candidate_value < best_value:\n                best_solution, best_value = candidate, candidate_value\n                self.crossover_rate = min(1.0, self.crossover_rate + 0.02 * np.exp(-self.evaluations / self.budget))\n                # More adaptable learning rate scaling\n                self.learning_rate *= 1.1 if candidate_value < previous_value else 1.03\n                previous_value = candidate_value\n            else:\n                self.crossover_rate = max(0.2, self.crossover_rate - 0.01 * np.random.rand())\n                self.learning_rate *= 0.93\n\n        return best_solution, best_value\n\n    def estimate_gradient(self, func, solution, lb, ub, epsilon=1e-4):\n        gradient = np.zeros(self.dim)\n        adaptive_epsilon = epsilon * (1 + self.evaluations / self.budget)  # Adaptive epsilon\n        for i in range(self.dim):\n            perturb = np.zeros(self.dim)\n            perturb[i] = adaptive_epsilon\n            upper_sol = np.clip(solution + perturb, lb, ub)\n            lower_sol = np.clip(solution - perturb, lb, ub)\n            gradient[i] = (func(upper_sol) - func(lower_sol)) / adaptive_epsilon\n            self.evaluations += 2\n        return gradient\n\n    def line_search(self, func, solution, gradient, lb, ub, alpha=1e-2, beta=0.9):\n        step_size = self.learning_rate * np.linalg.norm(gradient) / (1 + self.evaluations / self.budget)\n        while step_size > 1e-5:\n            candidate = solution - step_size * gradient\n            candidate = np.clip(candidate, lb, ub)\n            if func(candidate) < func(solution):\n                return candidate\n            step_size *= beta\n            self.evaluations += 1\n        return solution", "name": "HybridOptimizer", "description": "Slightly adjust the decay rate of the learning rate for faster convergence.", "configspace": "", "generation": 90, "fitness": 0.11268564912270236, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.113 with standard deviation 0.003. And the mean value of best solutions found was 0.122 (0. is the best).", "error": "", "parent_id": "cc908835-e3bf-4759-a5ed-b3ef10cf4b7b", "metadata": {"aucs": [0.10857547636546783, 0.11257053575634113, 0.11691093524629814], "final_y": [0.1319515223626554, 0.11998308442832795, 0.11388333668983786]}, "mutation_prompt": null}
{"id": "335a7ed4-527a-49dc-ab8c-365a939b4800", "solution": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.crossover_rate = 0.8\n        self.learning_rate = 0.1  # Initial learning rate\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        best_solution = np.random.uniform(lb, ub, self.dim)\n        best_value = func(best_solution)\n        self.evaluations += 1\n        previous_value = best_value  # Track previous iteration's value\n\n        while self.evaluations < self.budget:\n            if np.random.rand() < self.crossover_rate:\n                # Enhanced dynamic mutation factor\n                mutation_scale = np.clip(0.1 + 0.4 * np.sin(self.evaluations / self.budget * np.pi), 0.05, 0.3)\n                candidate = np.random.normal(best_solution, (ub - lb) / 10 * mutation_scale, self.dim)\n                candidate = np.clip(candidate, lb, ub)\n            else:\n                gradient = self.estimate_gradient(func, best_solution, lb, ub)\n                candidate = self.line_search(func, best_solution, gradient, lb, ub)\n\n            candidate_value = func(candidate)\n            self.evaluations += 1\n\n            if candidate_value < best_value:\n                best_solution, best_value = candidate, candidate_value\n                self.crossover_rate = min(1.0, self.crossover_rate + 0.02 * np.exp(-self.evaluations / self.budget))\n                # More adaptable learning rate scaling\n                self.learning_rate *= 1.1 if candidate_value < previous_value else 1.05\n                previous_value = candidate_value\n            else:\n                self.crossover_rate = max(0.2, self.crossover_rate - 0.01 * np.random.rand())\n                self.learning_rate *= 0.93\n\n        return best_solution, best_value\n\n    def estimate_gradient(self, func, solution, lb, ub, epsilon=1e-4):\n        gradient = np.zeros(self.dim)\n        adaptive_epsilon = epsilon * (1 + self.evaluations / self.budget)  # Adaptive epsilon\n        for i in range(self.dim):\n            perturb = np.zeros(self.dim)\n            perturb[i] = adaptive_epsilon\n            upper_sol = np.clip(solution + perturb, lb, ub)\n            lower_sol = np.clip(solution - perturb, lb, ub)\n            gradient[i] = (func(upper_sol) - func(lower_sol)) / adaptive_epsilon\n            self.evaluations += 2\n        return gradient\n\n    def line_search(self, func, solution, gradient, lb, ub, alpha=1e-2, beta=0.9):\n        step_size = self.learning_rate * np.linalg.norm(gradient) / (1 + self.evaluations / self.budget)\n        while step_size > 1e-5:\n            candidate = solution - step_size * gradient\n            candidate = np.clip(candidate, lb, ub)\n            if func(candidate) < func(solution):\n                return candidate\n            step_size *= beta\n            self.evaluations += 1\n        return solution", "name": "HybridOptimizer", "description": "Adaptive mutation scale and learning rate adjustment for enhanced exploration and convergence.", "configspace": "", "generation": 91, "fitness": 0.10967546072238592, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.110 with standard deviation 0.007. And the mean value of best solutions found was 0.130 (0. is the best).", "error": "", "parent_id": "cc908835-e3bf-4759-a5ed-b3ef10cf4b7b", "metadata": {"aucs": [0.10052501881644926, 0.11181842731394687, 0.11668293603676161], "final_y": [0.1539091820973777, 0.12080504539248227, 0.11408629112529345]}, "mutation_prompt": null}
{"id": "ed674519-5822-40cb-8289-62ac30eb22ef", "solution": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.crossover_rate = 0.8\n        self.learning_rate = 0.1  # Initial learning rate\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        best_solution = np.random.uniform(lb, ub, self.dim)\n        best_value = func(best_solution)\n        self.evaluations += 1\n        previous_value = best_value  # Track previous iteration's value\n\n        while self.evaluations < self.budget:\n            if np.random.rand() < self.crossover_rate:\n                # Enhanced dynamic mutation scale with convergence dependence\n                mutation_scale = np.clip(0.1 + 0.4 * np.sin((self.evaluations / self.budget) * np.pi), 0.05, 0.4)\n                candidate = np.random.normal(best_solution, (ub - lb) / 10 * mutation_scale, self.dim)\n                candidate = np.clip(candidate, lb, ub)\n            else:\n                gradient = self.estimate_gradient(func, best_solution, lb, ub)\n                candidate = self.line_search(func, best_solution, gradient, lb, ub)\n\n            candidate_value = func(candidate)\n            self.evaluations += 1\n\n            if candidate_value < best_value:\n                best_solution, best_value = candidate, candidate_value\n                self.crossover_rate = min(1.0, self.crossover_rate + 0.02 * np.exp(-self.evaluations / self.budget))\n                # More adaptable learning rate scaling\n                self.learning_rate *= 1.1 if candidate_value < previous_value else 1.05\n                previous_value = candidate_value\n            else:\n                self.crossover_rate = max(0.2, self.crossover_rate - 0.01 * np.random.rand())\n                self.learning_rate *= 0.93\n\n        return best_solution, best_value\n\n    def estimate_gradient(self, func, solution, lb, ub, epsilon=1e-4):\n        gradient = np.zeros(self.dim)\n        adaptive_epsilon = epsilon * (1 + self.evaluations / self.budget)  # Adaptive epsilon\n        for i in range(self.dim):\n            perturb = np.zeros(self.dim)\n            perturb[i] = adaptive_epsilon\n            upper_sol = np.clip(solution + perturb, lb, ub)\n            lower_sol = np.clip(solution - perturb, lb, ub)\n            gradient[i] = (func(upper_sol) - func(lower_sol)) / adaptive_epsilon\n            self.evaluations += 2\n        return gradient\n\n    def line_search(self, func, solution, gradient, lb, ub, alpha=1e-2, beta=0.9):\n        step_size = self.learning_rate * np.linalg.norm(gradient) / (1 + self.evaluations / self.budget)\n        while step_size > 1e-5:\n            candidate = solution - step_size * gradient\n            candidate = np.clip(candidate, lb, ub)\n            if func(candidate) < func(solution):\n                return candidate\n            step_size *= beta\n            self.evaluations += 1\n        return solution", "name": "HybridOptimizer", "description": "Introduced an adaptive mutation scale based on convergence progress to enhance exploration capabilities.", "configspace": "", "generation": 92, "fitness": 0.1092628386081284, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.109 with standard deviation 0.007. And the mean value of best solutions found was 0.131 (0. is the best).", "error": "", "parent_id": "cc908835-e3bf-4759-a5ed-b3ef10cf4b7b", "metadata": {"aucs": [0.10024258837299593, 0.11086299141462763, 0.11668293603676161], "final_y": [0.15568383933464236, 0.12408599041309287, 0.11408629112529345]}, "mutation_prompt": null}
{"id": "62418657-873f-4b03-a3dd-bd2fe3050b4e", "solution": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.crossover_rate = 0.8\n        self.learning_rate = 0.1  # Initial learning rate\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        best_solution = np.random.uniform(lb, ub, self.dim)\n        best_value = func(best_solution)\n        self.evaluations += 1\n        previous_value = best_value  # Track previous iteration's value\n\n        while self.evaluations < self.budget:\n            if np.random.rand() < self.crossover_rate:\n                # Enhanced dynamic mutation factor\n                mutation_scale = np.clip(0.1 + 0.3 * np.sin(self.evaluations / self.budget * np.pi), 0.05, 0.3)\n                candidate = np.random.normal(best_solution, (ub - lb) / 10 * mutation_scale, self.dim)\n                candidate = np.clip(candidate, lb, ub)\n            else:\n                gradient = self.estimate_gradient(func, best_solution, lb, ub)\n                candidate = self.line_search(func, best_solution, gradient, lb, ub)\n\n            candidate_value = func(candidate)\n            self.evaluations += 1\n\n            if candidate_value < best_value:\n                best_solution, best_value = candidate, candidate_value\n                self.crossover_rate = min(1.0, self.crossover_rate + 0.02 * np.exp(-self.evaluations / self.budget))\n                # More adaptable learning rate scaling\n                self.learning_rate *= 1.1 if candidate_value < previous_value else 1.05\n                previous_value = candidate_value\n            else:\n                self.crossover_rate = max(0.2, self.crossover_rate - 0.01 * np.random.rand())\n                self.learning_rate *= 0.93\n\n        return best_solution, best_value\n\n    def estimate_gradient(self, func, solution, lb, ub, epsilon=1e-4):\n        gradient = np.zeros(self.dim)\n        adaptive_epsilon = epsilon * (1 + 2 * self.evaluations / self.budget)  # Adaptive epsilon\n        for i in range(self.dim):\n            perturb = np.zeros(self.dim)\n            perturb[i] = adaptive_epsilon\n            upper_sol = np.clip(solution + perturb, lb, ub)\n            lower_sol = np.clip(solution - perturb, lb, ub)\n            gradient[i] = (func(upper_sol) - func(lower_sol)) / adaptive_epsilon\n            self.evaluations += 2\n        return gradient\n\n    def line_search(self, func, solution, gradient, lb, ub, alpha=1e-2, beta=0.9):\n        step_size = self.learning_rate * np.linalg.norm(gradient) / (1 + self.evaluations / self.budget)\n        while step_size > 1e-5:\n            candidate = solution - step_size * gradient\n            candidate = np.clip(candidate, lb, ub)\n            if func(candidate) < func(solution):\n                return candidate\n            step_size *= beta\n            self.evaluations += 1\n        return solution", "name": "HybridOptimizer", "description": "Introduce adaptive perturbation in gradient estimation for improved exploration.", "configspace": "", "generation": 93, "fitness": 0.11268565635027905, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.113 with standard deviation 0.003. And the mean value of best solutions found was 0.122 (0. is the best).", "error": "", "parent_id": "cc908835-e3bf-4759-a5ed-b3ef10cf4b7b", "metadata": {"aucs": [0.10857548061894673, 0.11257053902836911, 0.11691094940352131], "final_y": [0.13195151635137536, 0.11998306973865358, 0.11388327233190099]}, "mutation_prompt": null}
{"id": "1b82fe51-b807-4b4b-975b-5b816d10df40", "solution": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.crossover_rate = 0.8\n        self.learning_rate = 0.1  # Initial learning rate\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        best_solution = np.random.uniform(lb, ub, self.dim)\n        best_value = func(best_solution)\n        self.evaluations += 1\n        previous_value = best_value  # Track previous iteration's value\n\n        while self.evaluations < self.budget:\n            if np.random.rand() < self.crossover_rate:\n                # Enhanced dynamic mutation factor\n                mutation_scale = np.clip(0.1 + 0.3 * np.sin(self.evaluations / self.budget * np.pi), 0.05, 0.3)\n                candidate = np.random.normal(best_solution, (ub - lb) / 10 * mutation_scale, self.dim)\n                candidate = np.clip(candidate, lb, ub)\n            else:\n                gradient = self.estimate_gradient(func, best_solution, lb, ub)\n                candidate = self.line_search(func, best_solution, gradient, lb, ub)\n\n            candidate_value = func(candidate)\n            self.evaluations += 1\n\n            if candidate_value < best_value:\n                best_solution, best_value = candidate, candidate_value\n                self.crossover_rate = min(1.0, self.crossover_rate + 0.02 * np.exp(-self.evaluations / self.budget))\n                # More adaptable learning rate scaling\n                self.learning_rate *= 1.12 if candidate_value < previous_value else 1.05\n                previous_value = candidate_value\n            else:\n                self.crossover_rate = max(0.2, self.crossover_rate - 0.01 * np.random.rand())\n                self.learning_rate *= 0.93\n\n        return best_solution, best_value\n\n    def estimate_gradient(self, func, solution, lb, ub, epsilon=1e-4):\n        gradient = np.zeros(self.dim)\n        adaptive_epsilon = epsilon * (1 + 2 * self.evaluations / self.budget)  # Adaptive epsilon\n        for i in range(self.dim):\n            perturb = np.zeros(self.dim)\n            perturb[i] = adaptive_epsilon\n            upper_sol = np.clip(solution + perturb, lb, ub)\n            lower_sol = np.clip(solution - perturb, lb, ub)\n            gradient[i] = (func(upper_sol) - func(lower_sol)) / adaptive_epsilon\n            self.evaluations += 2\n        return gradient\n\n    def line_search(self, func, solution, gradient, lb, ub, alpha=1e-2, beta=0.9):\n        step_size = self.learning_rate * np.linalg.norm(gradient) / (1 + self.evaluations / self.budget)\n        while step_size > 1e-5:\n            candidate = solution - step_size * gradient\n            candidate = np.clip(candidate, lb, ub)\n            if func(candidate) < func(solution):\n                return candidate\n            step_size *= beta\n            self.evaluations += 1\n        return solution", "name": "HybridOptimizer", "description": "Refine adaptive learning rate to be more sensitive to recent improvements for accelerated convergence.", "configspace": "", "generation": 94, "fitness": 0.11266771968027274, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.113 with standard deviation 0.003. And the mean value of best solutions found was 0.122 (0. is the best).", "error": "", "parent_id": "62418657-873f-4b03-a3dd-bd2fe3050b4e", "metadata": {"aucs": [0.10852167060944495, 0.11257053901028469, 0.1169109494210886], "final_y": [0.1321748592482127, 0.11998306979976081, 0.11388327236837381]}, "mutation_prompt": null}
{"id": "4debb598-798f-408e-a14a-df577e5d76ed", "solution": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.crossover_rate = 0.8\n        self.learning_rate = 0.1  # Initial learning rate\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        best_solution = np.random.uniform(lb, ub, self.dim)\n        best_value = func(best_solution)\n        self.evaluations += 1\n        previous_value = best_value  # Track previous iteration's value\n\n        while self.evaluations < self.budget:\n            if np.random.rand() < self.crossover_rate:\n                # Enhanced dynamic mutation factor\n                mutation_scale = np.clip(0.1 + 0.3 * np.sin(self.evaluations / self.budget * np.pi), 0.05, 0.3)\n                candidate = np.random.normal(best_solution, (ub - lb) / 10 * mutation_scale, self.dim)\n                candidate = np.clip(candidate, lb, ub)\n            else:\n                gradient = self.estimate_gradient(func, best_solution, lb, ub)\n                candidate = self.line_search(func, best_solution, gradient, lb, ub)\n\n            candidate_value = func(candidate)\n            self.evaluations += 1\n\n            if candidate_value < best_value:\n                best_solution, best_value = candidate, candidate_value\n                self.crossover_rate = min(1.0, self.crossover_rate + 0.03 * np.exp(-self.evaluations / self.budget))  # Increased update rate\n                # More adaptable learning rate scaling\n                self.learning_rate *= 1.1 if candidate_value < previous_value else 1.07  # Slightly increased adjustment\n                previous_value = candidate_value\n            else:\n                self.crossover_rate = max(0.2, self.crossover_rate - 0.01 * np.random.rand())\n                self.learning_rate *= 0.93\n\n        return best_solution, best_value\n\n    def estimate_gradient(self, func, solution, lb, ub, epsilon=1e-4):\n        gradient = np.zeros(self.dim)\n        adaptive_epsilon = epsilon * (1 + 2 * self.evaluations / self.budget)  # Adaptive epsilon\n        for i in range(self.dim):\n            perturb = np.zeros(self.dim)\n            perturb[i] = adaptive_epsilon\n            upper_sol = np.clip(solution + perturb, lb, ub)\n            lower_sol = np.clip(solution - perturb, lb, ub)\n            gradient[i] = (func(upper_sol) - func(lower_sol)) / adaptive_epsilon\n            self.evaluations += 2\n        return gradient\n\n    def line_search(self, func, solution, gradient, lb, ub, alpha=1e-2, beta=0.9):\n        step_size = self.learning_rate * np.linalg.norm(gradient) / (1 + self.evaluations / self.budget)\n        while step_size > 1e-5:\n            candidate = solution - step_size * gradient\n            candidate = np.clip(candidate, lb, ub)\n            if func(candidate) < func(solution):\n                return candidate\n            step_size *= beta\n            self.evaluations += 1\n        return solution", "name": "HybridOptimizer", "description": "Fine-tune learning rate adaptation and crossover rate to enhance convergence speed.", "configspace": "", "generation": 95, "fitness": 0.11090388539254832, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.111 with standard deviation 0.006. And the mean value of best solutions found was 0.126 (0. is the best).", "error": "", "parent_id": "62418657-873f-4b03-a3dd-bd2fe3050b4e", "metadata": {"aucs": [0.10241404762652773, 0.11351967418533937, 0.11677793436577788], "final_y": [0.14558555169994425, 0.11887322964602365, 0.11469407632487161]}, "mutation_prompt": null}
{"id": "9c278b8b-8f03-42fa-b48a-aa735c9ce60d", "solution": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.crossover_rate = 0.8\n        self.learning_rate = 0.1  # Initial learning rate\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        best_solution = np.random.uniform(lb, ub, self.dim)\n        best_value = func(best_solution)\n        self.evaluations += 1\n        previous_value = best_value  # Track previous iteration's value\n\n        while self.evaluations < self.budget:\n            if np.random.rand() < self.crossover_rate:\n                mutation_scale = np.clip(0.1 + 0.3 * np.sin(self.evaluations / self.budget * np.pi), 0.05, 0.3)\n                candidate = np.random.normal(best_solution, np.std(best_solution) * mutation_scale, self.dim)  # Changed variance\n                candidate = np.clip(candidate, lb, ub)\n            else:\n                gradient = self.estimate_gradient(func, best_solution, lb, ub)\n                candidate = self.line_search(func, best_solution, gradient, lb, ub)\n\n            candidate_value = func(candidate)\n            self.evaluations += 1\n\n            if candidate_value < best_value:\n                best_solution, best_value = candidate, candidate_value\n                self.crossover_rate = min(1.0, self.crossover_rate + 0.02 * np.exp(-self.evaluations / self.budget))\n                self.learning_rate *= 1.1 if candidate_value < previous_value else 1.05\n                previous_value = candidate_value\n            else:\n                self.crossover_rate = max(0.2, self.crossover_rate - 0.01 * np.random.rand())\n                self.learning_rate *= 0.93\n\n        return best_solution, best_value\n\n    def estimate_gradient(self, func, solution, lb, ub, epsilon=1e-4):\n        gradient = np.zeros(self.dim)\n        adaptive_epsilon = epsilon * (1 + 2 * self.evaluations / self.budget)  # Adaptive epsilon\n        for i in range(self.dim):\n            perturb = np.zeros(self.dim)\n            perturb[i] = adaptive_epsilon\n            upper_sol = np.clip(solution + perturb, lb, ub)\n            lower_sol = np.clip(solution - perturb, lb, ub)\n            gradient[i] = (func(upper_sol) - func(lower_sol)) / adaptive_epsilon\n            self.evaluations += 2\n        return gradient\n\n    def line_search(self, func, solution, gradient, lb, ub, alpha=1e-2, beta=0.9):\n        step_size = self.learning_rate * np.linalg.norm(gradient) / (1 + self.evaluations / self.budget)\n        while step_size > 1e-5:\n            candidate = solution - step_size * gradient\n            candidate = np.clip(candidate, lb, ub)\n            if func(candidate) < func(solution):\n                return candidate\n            step_size *= beta\n            self.evaluations += 1\n        return solution", "name": "HybridOptimizer", "description": "Introduce variance-based adaptive mutation scale for enhanced exploration.", "configspace": "", "generation": 96, "fitness": 0.10913580218160006, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.109 with standard deviation 0.006. And the mean value of best solutions found was 0.130 (0. is the best).", "error": "", "parent_id": "62418657-873f-4b03-a3dd-bd2fe3050b4e", "metadata": {"aucs": [0.10169963278508898, 0.10898111942113153, 0.11672665433857965], "final_y": [0.14694841209715193, 0.12955998691790382, 0.11452778369931371]}, "mutation_prompt": null}
{"id": "9101b7fe-7a0d-4f08-9551-46f815d3cb30", "solution": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.crossover_rate = 0.8\n        self.learning_rate = 0.1  # Initial learning rate\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        best_solution = np.random.uniform(lb, ub, self.dim)\n        best_value = func(best_solution)\n        self.evaluations += 1\n        previous_value = best_value  # Track previous iteration's value\n\n        while self.evaluations < self.budget:\n            if np.random.rand() < self.crossover_rate:\n                # Enhanced dynamic mutation factor\n                mutation_scale = np.clip(0.1 + 0.35 * np.sin(self.evaluations / self.budget * np.pi), 0.05, 0.35)\n                candidate = np.random.normal(best_solution, (ub - lb) / 10 * mutation_scale, self.dim)\n                candidate = np.clip(candidate, lb, ub)\n            else:\n                gradient = self.estimate_gradient(func, best_solution, lb, ub)\n                candidate = self.line_search(func, best_solution, gradient, lb, ub)\n\n            candidate_value = func(candidate)\n            self.evaluations += 1\n\n            if candidate_value < best_value:\n                best_solution, best_value = candidate, candidate_value\n                self.crossover_rate = min(1.0, self.crossover_rate + 0.02 * np.exp(-self.evaluations / self.budget))\n                # More adaptable learning rate scaling\n                self.learning_rate *= 1.1 if candidate_value < previous_value else 1.05\n                previous_value = candidate_value\n            else:\n                self.crossover_rate = max(0.2, self.crossover_rate - 0.01 * np.random.rand())\n                self.learning_rate *= 0.93\n\n        return best_solution, best_value\n\n    def estimate_gradient(self, func, solution, lb, ub, epsilon=1e-4):\n        gradient = np.zeros(self.dim)\n        adaptive_epsilon = epsilon * (1 + 2 * self.evaluations / self.budget)  # Adaptive epsilon\n        for i in range(self.dim):\n            perturb = np.zeros(self.dim)\n            perturb[i] = adaptive_epsilon\n            upper_sol = np.clip(solution + perturb, lb, ub)\n            lower_sol = np.clip(solution - perturb, lb, ub)\n            gradient[i] = (func(upper_sol) - func(lower_sol)) / adaptive_epsilon\n            self.evaluations += 2\n        return gradient\n\n    def line_search(self, func, solution, gradient, lb, ub, alpha=1e-2, beta=0.9):\n        step_size = self.learning_rate * np.linalg.norm(gradient) / (1 + self.evaluations / self.budget)\n        while step_size > 1e-5:\n            candidate = solution - step_size * gradient\n            candidate = np.clip(candidate, lb, ub)\n            if func(candidate) < func(solution):\n                return candidate\n            step_size *= beta\n            self.evaluations += 1\n        return solution", "name": "HybridOptimizer", "description": "Refine the mutation scale to improve solution diversity by slightly increasing its range.", "configspace": "", "generation": 97, "fitness": 0.1107270935896492, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.111 with standard deviation 0.006. And the mean value of best solutions found was 0.127 (0. is the best).", "error": "", "parent_id": "62418657-873f-4b03-a3dd-bd2fe3050b4e", "metadata": {"aucs": [0.10224442682154389, 0.11283734703512982, 0.11709950691227389], "final_y": [0.14864051425402702, 0.11938293844429304, 0.11387446097133824]}, "mutation_prompt": null}
{"id": "23806b55-281a-418e-83ff-526644cc8d37", "solution": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.crossover_rate = 0.8\n        self.learning_rate = 0.1  # Initial learning rate\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        best_solution = np.random.uniform(lb, ub, self.dim)\n        best_value = func(best_solution)\n        self.evaluations += 1\n        previous_value = best_value  # Track previous iteration's value\n\n        while self.evaluations < self.budget:\n            if np.random.rand() < self.crossover_rate:\n                # Enhanced dynamic mutation factor\n                mutation_scale = np.clip(0.1 + 0.5 * np.sin(self.evaluations / self.budget * np.pi), 0.05, 0.3)\n                candidate = np.random.normal(best_solution, (ub - lb) / 10 * mutation_scale, self.dim)\n                candidate = np.clip(candidate, lb, ub)\n            else:\n                gradient = self.estimate_gradient(func, best_solution, lb, ub)\n                candidate = self.line_search(func, best_solution, gradient, lb, ub)\n\n            candidate_value = func(candidate)\n            self.evaluations += 1\n\n            if candidate_value < best_value:\n                best_solution, best_value = candidate, candidate_value\n                self.crossover_rate = min(1.0, self.crossover_rate + 0.02 * np.exp(-self.evaluations / self.budget))\n                # More adaptable learning rate scaling\n                self.learning_rate *= 1.1 if candidate_value < previous_value else 1.05\n                previous_value = candidate_value\n            else:\n                self.crossover_rate = max(0.2, self.crossover_rate - 0.01 * np.random.rand())\n                self.learning_rate *= 0.93\n\n        return best_solution, best_value\n\n    def estimate_gradient(self, func, solution, lb, ub, epsilon=1e-4):\n        gradient = np.zeros(self.dim)\n        adaptive_epsilon = epsilon * (1 + 2 * self.evaluations / self.budget)  # Adaptive epsilon\n        for i in range(self.dim):\n            perturb = np.zeros(self.dim)\n            perturb[i] = adaptive_epsilon\n            upper_sol = np.clip(solution + perturb, lb, ub)\n            lower_sol = np.clip(solution - perturb, lb, ub)\n            gradient[i] = (func(upper_sol) - func(lower_sol)) / adaptive_epsilon\n            self.evaluations += 2\n        return gradient\n\n    def line_search(self, func, solution, gradient, lb, ub, alpha=1e-2, beta=0.9):\n        step_size = self.learning_rate * np.linalg.norm(gradient) / (1 + self.evaluations / self.budget)\n        while step_size > 1e-5:\n            candidate = solution - step_size * gradient\n            candidate = np.clip(candidate, lb, ub)\n            if func(candidate) < func(solution):\n                return candidate\n            step_size *= beta\n            self.evaluations += 1\n        return solution", "name": "HybridOptimizer", "description": "Introduce a dynamic crossover rate and scale mutations based on the current best improvement rate.", "configspace": "", "generation": 98, "fitness": 0.11089536057492062, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.111 with standard deviation 0.006. And the mean value of best solutions found was 0.126 (0. is the best).", "error": "", "parent_id": "62418657-873f-4b03-a3dd-bd2fe3050b4e", "metadata": {"aucs": [0.10245575580971977, 0.11302533054393626, 0.11720499537110585], "final_y": [0.14611019611848242, 0.11946200214105407, 0.1135899007223623]}, "mutation_prompt": null}
{"id": "20dbce51-13e3-4cac-89e2-cf0551cb36c0", "solution": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.crossover_rate = 0.8\n        self.learning_rate = 0.1  # Initial learning rate\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        best_solution = np.random.uniform(lb, ub, self.dim)\n        best_value = func(best_solution)\n        self.evaluations += 1\n        previous_value = best_value  # Track previous iteration's value\n\n        while self.evaluations < self.budget:\n            if np.random.rand() < self.crossover_rate:\n                # Enhanced dynamic mutation factor\n                mutation_scale = np.clip(0.1 + 0.3 * np.sin(self.evaluations / self.budget * np.pi), 0.05, 0.3)\n                candidate = np.random.normal(best_solution, (ub - lb) / 10 * mutation_scale, self.dim)\n                candidate = np.clip(candidate, lb, ub)\n            else:\n                gradient = self.estimate_gradient(func, best_solution, lb, ub)\n                candidate = self.line_search(func, best_solution, gradient, lb, ub)\n\n            candidate_value = func(candidate)\n            self.evaluations += 1\n\n            if candidate_value < best_value:\n                best_solution, best_value = candidate, candidate_value\n                self.crossover_rate = min(1.0, self.crossover_rate + 0.02 * np.exp(-self.evaluations / self.budget))\n                # More adaptable learning rate scaling\n                self.learning_rate *= 1.1 if candidate_value < previous_value else 1.05\n                previous_value = candidate_value\n            else:\n                self.crossover_rate = max(0.2, self.crossover_rate - 0.01 * np.random.rand())\n                # Apply momentum-based update to learning rate\n                self.learning_rate *= 0.93 * (1 + 0.1 * (previous_value - candidate_value))\n\n        return best_solution, best_value\n\n    def estimate_gradient(self, func, solution, lb, ub, epsilon=1e-4):\n        gradient = np.zeros(self.dim)\n        adaptive_epsilon = epsilon * (1 + 2 * self.evaluations / self.budget)  # Adaptive epsilon\n        for i in range(self.dim):\n            perturb = np.zeros(self.dim)\n            perturb[i] = adaptive_epsilon\n            upper_sol = np.clip(solution + perturb, lb, ub)\n            lower_sol = np.clip(solution - perturb, lb, ub)\n            gradient[i] = (func(upper_sol) - func(lower_sol)) / adaptive_epsilon\n            self.evaluations += 2\n        return gradient\n\n    def line_search(self, func, solution, gradient, lb, ub, alpha=1e-2, beta=0.9):\n        step_size = self.learning_rate * np.linalg.norm(gradient) / (1 + self.evaluations / self.budget)\n        while step_size > 1e-5:\n            candidate = solution - step_size * gradient\n            candidate = np.clip(candidate, lb, ub)\n            if func(candidate) < func(solution):\n                return candidate\n            step_size *= beta\n            self.evaluations += 1\n        return solution", "name": "HybridOptimizer", "description": "Introducing a momentum-based approach to accelerate convergence by modifying the learning rate update.", "configspace": "", "generation": 99, "fitness": 0.11268565634989736, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.113 with standard deviation 0.003. And the mean value of best solutions found was 0.122 (0. is the best).", "error": "", "parent_id": "62418657-873f-4b03-a3dd-bd2fe3050b4e", "metadata": {"aucs": [0.10857548061883315, 0.11257053902871228, 0.11691094940214664], "final_y": [0.13195151635073588, 0.11998306973750072, 0.11388327233416684]}, "mutation_prompt": null}
