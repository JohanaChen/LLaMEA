{"id": "c4a83ae5-1197-415e-ad7f-bffbc6700501", "solution": "import numpy as np\n\nclass AGAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.num_particles = 50\n        self.inertia_weight = 0.7\n        self.cognitive_coeff = 1.5\n        self.social_coeff = 1.5\n        self.learning_rate = 0.01\n        self.grad_epsilon = 1e-8\n\n    def _initialize_particles(self):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.num_particles, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        return positions, velocities\n\n    def _calculate_gradients(self, func, positions, current_best):\n        gradients = []\n        for i, position in enumerate(positions):\n            grad = np.zeros(self.dim)\n            for j in range(self.dim):\n                perturbed_position = np.copy(position)\n                perturbed_position[j] += self.grad_epsilon\n                grad[j] = (func(perturbed_position) - func(position)) / self.grad_epsilon\n            gradients.append(grad)\n        gradients = np.array(gradients)\n        return (current_best - positions) * gradients\n    \n    def __call__(self, func):\n        positions, velocities = self._initialize_particles()\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.num_particles, float('inf'))\n        global_best_position = None\n        global_best_score = float('inf')\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            scores = np.array([func(pos) for pos in positions])\n            evaluations += self.num_particles\n\n            better_personal_mask = scores < personal_best_scores\n            personal_best_scores[better_personal_mask] = scores[better_personal_mask]\n            personal_best_positions[better_personal_mask] = positions[better_personal_mask]\n\n            if np.min(scores) < global_best_score:\n                global_best_position = positions[np.argmin(scores)]\n                global_best_score = np.min(scores)\n\n            gradients = self._calculate_gradients(func, positions, global_best_position)\n            velocities = (\n                self.inertia_weight * velocities +\n                self.cognitive_coeff * np.random.rand(self.num_particles, self.dim) * (personal_best_positions - positions) +\n                self.social_coeff * np.random.rand(self.num_particles, self.dim) * (global_best_position - positions) +\n                self.learning_rate * gradients\n            )\n            positions = positions + velocities\n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n\n        return global_best_position, global_best_score", "name": "AGAPSO", "description": "Adaptive Gradient-Assisted Particle Swarm Optimization (AGAPSO) enhances PSO's exploration and exploitation by incorporating adaptive gradient information.", "configspace": "", "generation": 0, "fitness": 0.10545610060024108, "feedback": "", "error": "", "parent_id": null, "metadata": {"aucs": [0.2621275032929613, 0.2486428118180003, 0.22279431734355637, 0.2628248625921247, 0.21103583964598738, 0.2645209249704339, 0.24396634871781064, 0.2673399336454545, 0.23239900815477954, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.024654960613754495, 0.04582292095999385, 0.04324042400155026, 0.034124054294014994, 0.026326938438153258, 0.033497848467958447, 0.040391319808866455, 0.030627333182339456, 0.025444075659644683, 0.026905692331328113, 0.010045454382221708, 0.004312748883903095, 0.022101663061796506, 0.026261764924299547, 0.014071420255305167, 0.02545830355534051, 0.013646828400679989, 0.024027954337884738, 0.8493988167905975, 0.8026674828819428, 0.8487786855658134, 0.6036255207197556, 0.7499870606387908, 0.750783382915481, 0.8478842583451006, 0.8467341620879762, 0.7575677015671947, 0.06274794456508026, 9.999999999998899e-05, 9.999999999998899e-05, 0.0009547447421811572, 0.007074598071476479, 0.027980039633984766, 9.999999999998899e-05, 0.012337913438326131, 9.999999999998899e-05, 0.18685139417572516, 0.1531676825283501, 0.14651070481576445, 0.17330058000829174, 0.1665353116638969, 0.16053649957719718, 0.1403816423369848, 0.10702264361658331, 0.16077857179136046, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.24584527898956554, 0.24934541732126858, 0.2608438117405536, 0.2937421840142852, 0.22819211449009968, 0.21859536258599566, 0.28324259218711967, 0.24701910361290913, 0.25299865278881994, 0.04032463443595102, 0.021871858253829535, 0.024995622262201644, 0.054089018997047655, 0.01794112780069612, 0.04948831894191352, 0.042729947301825666, 0.026660187295032833, 0.0349375441410098, 0.1064872580231544, 0.09282456119304938, 0.11842189618558563, 0.11170072158873279, 0.10852978096894605, 0.10798722374439307, 0.1193948066181929, 0.08099223934302135, 0.07811454907254123, 0.19002422157035004, 0.18549591097643692, 0.2186577555362783, 0.1794902201551113, 0.19238376946456748, 0.18532554901306053, 0.1744341055122417, 0.20729844264983788, 0.20863071409572465, 0.06113132685050238, 0.06472990942024992, 0.1164638267442959, 0.09282051477456044, 0.07843658219130378, 0.07711515457553397, 0.09645170176740181, 0.10293769130776531, 0.08627284637198696, 0.11943369038869045, 0.1023246132639335, 0.13345160249702437, 0.11035694628655202, 0.09504668276734374, 0.10896910112734548, 0.12245978824380144, 0.07872976557222011, 0.13887274328069288, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.11011722277274583, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.19633600882060764, 0.1086220792788235, 0.1402398731623472, 0.18373971746419604, 0.16791693132810614, 0.14017770612443814, 0.12638466697236794, 0.12469223659434925, 0.31405907700751234, 0.2313805679281392, 0.17555058901480936, 0.07151828258089676, 0.15313360333125914, 0.18768698508803583, 0.12283485913670467, 0.09425608499729266, 0.09847035754502576, 0.0953547630593099, 0.1406048477092472, 0.15885532902766486, 0.15355818366434815, 0.16525456146202266, 0.14605650546619475, 0.1591959810677237, 0.15834953767405335, 0.15711120332584727, 0.16801930881589666, 0.018253593293135584, 0.017049344368661123, 0.03139349861453389, 0.02701529989432272, 0.034250945061873095, 0.020045519440459958, 0.016685075025402507, 0.03803355308265022, 0.023532241930514286]}, "mutation_prompt": null}
{"id": "ea72cf0f-11f7-44de-b81b-4c6709d3bd5e", "solution": "import numpy as np\n\nclass AGAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.num_particles = 50\n        self.inertia_weight = 0.7\n        self.cognitive_coeff = 1.5\n        self.social_coeff = 1.5\n        self.learning_rate = 0.01\n        self.grad_epsilon = 1e-8\n\n    def _initialize_particles(self):\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.num_particles, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.num_particles, self.dim))\n        return positions, velocities\n\n    def _calculate_gradients(self, func, positions, current_best):\n        gradients = []\n        for i, position in enumerate(positions):\n            grad = np.zeros(self.dim)\n            for j in range(self.dim):\n                perturbed_position = np.copy(position)\n                perturbed_position[j] += self.grad_epsilon\n                grad[j] = (func(perturbed_position) - func(position)) / self.grad_epsilon\n            gradients.append(grad)\n        gradients = np.array(gradients)\n        return (current_best - positions) * gradients\n    \n    def __call__(self, func):\n        positions, velocities = self._initialize_particles()\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.num_particles, float('inf'))\n        global_best_position = None\n        global_best_score = float('inf')\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            scores = np.array([func(pos) for pos in positions])\n            evaluations += self.num_particles\n\n            better_personal_mask = scores < personal_best_scores\n            personal_best_scores[better_personal_mask] = scores[better_personal_mask]\n            personal_best_positions[better_personal_mask] = positions[better_personal_mask]\n\n            if np.min(scores) < global_best_score:\n                global_best_position = positions[np.argmin(scores)]\n                global_best_score = np.min(scores)\n\n            gradients = self._calculate_gradients(func, positions, global_best_position)\n            velocities = (\n                self.inertia_weight * velocities +\n                self.cognitive_coeff * np.random.rand(self.num_particles, self.dim) * (personal_best_positions - positions) +\n                self.social_coeff * np.random.rand(self.num_particles, self.dim) * (global_best_position - positions) +\n                self.learning_rate * gradients\n            )\n            positions = positions + velocities\n            positions = np.clip(positions, self.lower_bound, self.upper_bound)\n\n        return global_best_position, global_best_score", "name": "AGAPSO", "description": "Adaptive Gradient-Assisted Particle Swarm Optimization (AGAPSO) enhances PSO's exploration and exploitation by incorporating adaptive gradient information.", "configspace": "", "generation": 1, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "c4a83ae5-1197-415e-ad7f-bffbc6700501", "metadata": {"aucs": [0.2621275032929613, 0.2486428118180003, 0.22279431734355637, 0.2628248625921247, 0.21103583964598738, 0.2645209249704339, 0.24396634871781064, 0.2673399336454545, 0.23239900815477954, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.024654960613754495, 0.04582292095999385, 0.04324042400155026, 0.034124054294014994, 0.026326938438153258, 0.033497848467958447, 0.040391319808866455, 0.030627333182339456, 0.025444075659644683, 0.026905692331328113, 0.010045454382221708, 0.004312748883903095, 0.022101663061796506, 0.026261764924299547, 0.014071420255305167, 0.02545830355534051, 0.013646828400679989, 0.024027954337884738, 0.8493988167905975, 0.8026674828819428, 0.8487786855658134, 0.6036255207197556, 0.7499870606387908, 0.750783382915481, 0.8478842583451006, 0.8467341620879762, 0.7575677015671947, 0.06274794456508026, 9.999999999998899e-05, 9.999999999998899e-05, 0.0009547447421811572, 0.007074598071476479, 0.027980039633984766, 9.999999999998899e-05, 0.012337913438326131, 9.999999999998899e-05, 0.18685139417572516, 0.1531676825283501, 0.14651070481576445, 0.17330058000829174, 0.1665353116638969, 0.16053649957719718, 0.1403816423369848, 0.10702264361658331, 0.16077857179136046, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.24584527898956554, 0.24934541732126858, 0.2608438117405536, 0.2937421840142852, 0.22819211449009968, 0.21859536258599566, 0.28324259218711967, 0.24701910361290913, 0.25299865278881994, 0.04032463443595102, 0.021871858253829535, 0.024995622262201644, 0.054089018997047655, 0.01794112780069612, 0.04948831894191352, 0.042729947301825666, 0.026660187295032833, 0.0349375441410098, 0.1064872580231544, 0.09282456119304938, 0.11842189618558563, 0.11170072158873279, 0.10852978096894605, 0.10798722374439307, 0.1193948066181929, 0.08099223934302135, 0.07811454907254123, 0.19002422157035004, 0.18549591097643692, 0.2186577555362783, 0.1794902201551113, 0.19238376946456748, 0.18532554901306053, 0.1744341055122417, 0.20729844264983788, 0.20863071409572465, 0.06113132685050238, 0.06472990942024992, 0.1164638267442959, 0.09282051477456044, 0.07843658219130378, 0.07711515457553397, 0.09645170176740181, 0.10293769130776531, 0.08627284637198696, 0.11943369038869045, 0.1023246132639335, 0.13345160249702437, 0.11035694628655202, 0.09504668276734374, 0.10896910112734548, 0.12245978824380144, 0.07872976557222011, 0.13887274328069288, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.11011722277274583, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.19633600882060764, 0.1086220792788235, 0.1402398731623472, 0.18373971746419604, 0.16791693132810614, 0.14017770612443814, 0.12638466697236794, 0.12469223659434925, 0.31405907700751234, 0.2313805679281392, 0.17555058901480936, 0.07151828258089676, 0.15313360333125914, 0.18768698508803583, 0.12283485913670467, 0.09425608499729266, 0.09847035754502576, 0.0953547630593099, 0.1406048477092472, 0.15885532902766486, 0.15355818366434815, 0.16525456146202266, 0.14605650546619475, 0.1591959810677237, 0.15834953767405335, 0.15711120332584727, 0.16801930881589666, 0.018253593293135584, 0.017049344368661123, 0.03139349861453389, 0.02701529989432272, 0.034250945061873095, 0.020045519440459958, 0.016685075025402507, 0.03803355308265022, 0.023532241930514286]}, "mutation_prompt": null}
