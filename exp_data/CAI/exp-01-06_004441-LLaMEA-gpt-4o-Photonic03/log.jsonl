{"id": "f0485faf-2990-4fa2-a723-ffa228107152", "solution": "import numpy as np\n\nclass NovelSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particle_count = min(30, budget // dim)\n        self.inertia_weight = 0.729  # inertia weight\n        self.cognitive_const = 1.49445  # cognitive constant\n        self.social_const = 1.49445  # social constant\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        # Initialize particles' positions and velocities\n        positions = np.random.uniform(lb, ub, (self.particle_count, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particle_count, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.particle_count, np.inf)\n\n        global_best_position = None\n        global_best_score = np.inf\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            # Evaluate current positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                current_score = func(positions[i])\n                evaluations += 1\n                \n                # Update personal best\n                if current_score < personal_best_scores[i]:\n                    personal_best_scores[i] = current_score\n                    personal_best_positions[i] = positions[i]\n                \n                # Update global best\n                if current_score < global_best_score:\n                    global_best_score = current_score\n                    global_best_position = positions[i]\n\n            # Update velocities and positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                r1, r2 = np.random.rand(2, self.dim)\n                velocities[i] = (self.inertia_weight * velocities[i] +\n                                 self.cognitive_const * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.social_const * r2 * (global_best_position - positions[i]))\n                positions[i] = np.clip(positions[i] + velocities[i], lb, ub)\n\n        return global_best_position", "name": "NovelSwarmOptimizer", "description": "A cooperative swarm-based algorithm that adaptively balances exploration and exploitation by sharing information among particles to optimize photonic structures.", "configspace": "", "generation": 0, "fitness": 0.13075954923552066, "feedback": "The algorithm NovelSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.13 with standard deviation 0.00.", "error": "", "parent_id": null, "metadata": {"aucs": [0.13132998110365957, 0.13199654103595848, 0.1289521255669439]}, "mutation_prompt": null}
{"id": "5cd2d8cf-b81f-4b27-ab54-43e3fbac35de", "solution": "import numpy as np\n\nclass NovelSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particle_count = min(30, budget // dim)\n        self.inertia_weight = 0.729  # inertia weight\n        self.cognitive_const = 1.49445  # cognitive constant\n        self.social_const = 1.49445  # social constant\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        # Initialize particles' positions and velocities\n        positions = np.random.uniform(lb, ub, (self.particle_count, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particle_count, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.particle_count, np.inf)\n\n        global_best_position = None\n        global_best_score = np.inf\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            # Evaluate current positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                current_score = func(positions[i])\n                evaluations += 1\n\n                # Update personal best\n                if current_score < personal_best_scores[i]:\n                    personal_best_scores[i] = current_score\n                    personal_best_positions[i] = positions[i]\n\n                # Update global best\n                if current_score < global_best_score:\n                    global_best_score = current_score\n                    global_best_position = positions[i]\n\n            # Update velocities and positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                r1, r2 = np.random.rand(2, self.dim)\n                inertia_weight = 0.9 - 0.5 * (evaluations / self.budget)  # Dynamic inertia adjustment\n                velocities[i] = (inertia_weight * velocities[i] +\n                                 self.cognitive_const * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.social_const * r2 * (global_best_position - positions[i]))\n                positions[i] = np.clip(positions[i] + velocities[i], lb, ub)\n\n        return global_best_position", "name": "NovelSwarmOptimizer", "description": "Incorporate a dynamic inertia weight adjustment strategy for enhanced exploration and exploitation balance.", "configspace": "", "generation": 1, "fitness": 0.13034906323112141, "feedback": "The algorithm NovelSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.13 with standard deviation 0.00.", "error": "", "parent_id": "f0485faf-2990-4fa2-a723-ffa228107152", "metadata": {"aucs": [0.1308508927092369, 0.131478944734428, 0.12871735224969938]}, "mutation_prompt": null}
{"id": "4d4b4437-6820-49c4-9827-b0409579ba82", "solution": "import numpy as np\n\nclass NovelSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particle_count = min(30, budget // dim)\n        self.inertia_weight = 0.729  # inertia weight\n        self.cognitive_const = 1.49445  # cognitive constant\n        self.social_const = 1.49445  # social constant\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        # Initialize particles' positions and velocities\n        positions = np.random.uniform(lb, ub, (self.particle_count, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particle_count, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.particle_count, np.inf)\n\n        global_best_position = None\n        global_best_score = np.inf\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            # Evaluate current positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                current_score = func(positions[i])\n                evaluations += 1\n                \n                # Update personal best\n                if current_score < personal_best_scores[i]:\n                    personal_best_scores[i] = current_score\n                    personal_best_positions[i] = positions[i]\n                \n                # Update global best\n                if current_score < global_best_score:\n                    global_best_score = current_score\n                    global_best_position = positions[i]\n\n            # Update velocities and positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                r1, r2 = np.random.rand(2, self.dim)\n                velocities[i] = (self.inertia_weight * velocities[i] +\n                                 self.cognitive_const * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.social_const * r2 * (global_best_position - positions[i]))\n                velocities[i] *= 0.9  # Adaptive velocity scaling\n                positions[i] = np.clip(positions[i] + velocities[i], lb, ub)\n\n        return global_best_position", "name": "NovelSwarmOptimizer", "description": "Enhanced particle dynamics by introducing adaptive velocity scaling for improved convergence in photonic structure optimization.", "configspace": "", "generation": 2, "fitness": 0.13167456369146954, "feedback": "The algorithm NovelSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.13 with standard deviation 0.00.", "error": "", "parent_id": "f0485faf-2990-4fa2-a723-ffa228107152", "metadata": {"aucs": [0.13262125287909854, 0.13318510763145486, 0.1292173305638552]}, "mutation_prompt": null}
{"id": "998ce641-9536-43af-93bb-7df75dfca574", "solution": "import numpy as np\n\nclass NovelSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particle_count = min(30, budget // dim)\n        self.inertia_weight = 0.729  # inertia weight\n        self.cognitive_const = 1.49445  # cognitive constant\n        self.social_const = 1.49445  # social constant\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        # Initialize particles' positions and velocities\n        positions = np.random.uniform(lb, ub, (self.particle_count, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particle_count, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.particle_count, np.inf)\n\n        global_best_position = None\n        global_best_score = np.inf\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            # Evaluate current positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                current_score = func(positions[i])\n                evaluations += 1\n                \n                # Update personal best\n                if current_score < personal_best_scores[i]:\n                    personal_best_scores[i] = current_score\n                    personal_best_positions[i] = positions[i]\n                \n                # Update global best\n                if current_score < global_best_score:\n                    global_best_score = current_score\n                    global_best_position = positions[i]\n\n            # Update velocities and positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                r1, r2 = np.random.rand(2, self.dim)\n                velocities[i] = (self.inertia_weight * velocities[i] +\n                                 self.cognitive_const * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.social_const * r2 * (global_best_position - positions[i]))\n                velocities[i] *= 0.9  # Adaptive velocity scaling\n                positions[i] = np.clip(positions[i] + velocities[i], lb, ub)\n\n            # Adaptive inertia weight decay\n            self.inertia_weight *= 0.99\n            \n        return global_best_position", "name": "NovelSwarmOptimizer", "description": "Introduced adaptive inertia weight decay to balance exploration and exploitation for improved convergence.", "configspace": "", "generation": 3, "fitness": 0.13224112534684362, "feedback": "The algorithm NovelSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.13 with standard deviation 0.00.", "error": "", "parent_id": "4d4b4437-6820-49c4-9827-b0409579ba82", "metadata": {"aucs": [0.13258690857466948, 0.13318981584478806, 0.13094665162107333]}, "mutation_prompt": null}
{"id": "4ceed576-865e-4c25-815c-a67c47b4a50f", "solution": "import numpy as np\n\nclass NovelSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particle_count = min(30, budget // dim)\n        self.inertia_weight = 0.729  # inertia weight\n        self.cognitive_const = 1.49445  # cognitive constant\n        self.social_const = 1.49445  # social constant\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        # Initialize particles' positions and velocities\n        positions = np.random.uniform(lb, ub, (self.particle_count, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particle_count, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.particle_count, np.inf)\n\n        global_best_position = None\n        global_best_score = np.inf\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            # Evaluate current positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                current_score = func(positions[i])\n                evaluations += 1\n                \n                # Update personal best\n                if current_score < personal_best_scores[i]:\n                    personal_best_scores[i] = current_score\n                    personal_best_positions[i] = positions[i]\n                \n                # Update global best\n                if current_score < global_best_score:\n                    global_best_score = current_score\n                    global_best_position = positions[i]\n\n            # Update velocities and positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                r1, r2 = np.random.rand(2, self.dim)\n                velocities[i] = (self.inertia_weight * velocities[i] +\n                                 self.cognitive_const * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.social_const * r2 * (global_best_position - positions[i]))\n                velocities[i] *= 0.9  # Adaptive velocity scaling\n                positions[i] = np.clip(positions[i] + velocities[i], lb, ub)\n\n            # Adaptive inertia weight decay\n            self.inertia_weight *= 0.99\n            \n            # Diversity-driven restart mechanism\n            if evaluations % (self.budget // 10) == 0:  # Check every 10% of budget\n                if np.std(personal_best_scores) < 1e-5:  # Low diversity trigger\n                    positions = np.random.uniform(lb, ub, (self.particle_count, self.dim))\n            \n        return global_best_position", "name": "NovelSwarmOptimizer", "description": "Enhanced global best update by incorporating a diversity-driven restart mechanism to escape local optima.", "configspace": "", "generation": 4, "fitness": 0.13224112534684362, "feedback": "The algorithm NovelSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.13 with standard deviation 0.00.", "error": "", "parent_id": "998ce641-9536-43af-93bb-7df75dfca574", "metadata": {"aucs": [0.13258690857466948, 0.13318981584478806, 0.13094665162107333]}, "mutation_prompt": null}
{"id": "c1317cad-1029-4c4a-8640-86eb5dfb0f79", "solution": "import numpy as np\n\nclass NovelSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particle_count = min(30, budget // dim)\n        self.inertia_weight = 0.729  # inertia weight\n        self.cognitive_const = 1.49445  # cognitive constant\n        self.social_const = 1.49445  # social constant\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        # Initialize particles' positions and velocities\n        positions = np.random.uniform(lb, ub, (self.particle_count, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particle_count, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.particle_count, np.inf)\n\n        global_best_position = None\n        global_best_score = np.inf\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            # Evaluate current positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                current_score = func(positions[i])\n                evaluations += 1\n                \n                # Update personal best\n                if current_score < personal_best_scores[i]:\n                    personal_best_scores[i] = current_score\n                    personal_best_positions[i] = positions[i]\n                else:  # Random reinitialization\n                    if np.random.rand() < 0.01:\n                        positions[i] = np.random.uniform(lb, ub, self.dim)\n                \n                # Update global best\n                if current_score < global_best_score:\n                    global_best_score = current_score\n                    global_best_position = positions[i]\n\n            # Update velocities and positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                r1, r2 = np.random.rand(2, self.dim)\n                velocities[i] = (self.inertia_weight * velocities[i] +\n                                 self.cognitive_const * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.social_const * r2 * (global_best_position - positions[i]))\n                velocities[i] *= 0.9  # Adaptive velocity scaling\n                positions[i] = np.clip(positions[i] + velocities[i], lb, ub)\n\n            # Adaptive inertia weight decay\n            self.inertia_weight *= 0.99\n            \n        return global_best_position", "name": "NovelSwarmOptimizer", "description": "Added random reinitialization for particles stuck in local optima to enhance global search capability.", "configspace": "", "generation": 5, "fitness": 0.13200012640693023, "feedback": "The algorithm NovelSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.13 with standard deviation 0.00.", "error": "", "parent_id": "998ce641-9536-43af-93bb-7df75dfca574", "metadata": {"aucs": [0.13226384830223803, 0.13215712733682838, 0.13157940358172426]}, "mutation_prompt": null}
{"id": "e20d66b3-56e8-4109-ab4c-259018637231", "solution": "import numpy as np\n\nclass NovelSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particle_count = min(30, budget // dim)\n        self.inertia_weight = 0.729  # inertia weight\n        self.cognitive_const = 1.49445  # cognitive constant\n        self.social_const = 1.49445  # social constant\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        # Initialize particles' positions and velocities\n        positions = np.random.uniform(lb, ub, (self.particle_count, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particle_count, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.particle_count, np.inf)\n\n        global_best_position = None\n        global_best_score = np.inf\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            # Evaluate current positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                current_score = func(positions[i])\n                evaluations += 1\n                \n                # Update personal best\n                if current_score < personal_best_scores[i]:\n                    personal_best_scores[i] = current_score\n                    personal_best_positions[i] = positions[i]\n                \n                # Update global best\n                if current_score < global_best_score:\n                    global_best_score = current_score\n                    global_best_position = positions[i]\n\n            # Update velocities and positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                r1, r2 = np.random.rand(2, self.dim)\n                velocities[i] = (self.inertia_weight * velocities[i] +\n                                 self.cognitive_const * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.social_const * r2 * (global_best_position - positions[i]))\n                velocities[i] *= 0.9  # Adaptive velocity scaling\n                positions[i] = np.clip(positions[i] + velocities[i], lb, ub)\n                positions[i] = np.clip(positions[i], lb, ub)  # Ensure positions stay within bounds\n\n            # Adaptive inertia weight decay\n            self.inertia_weight *= 0.99\n            \n        return global_best_position", "name": "NovelSwarmOptimizer", "description": "Introduced a position update bound check to ensure particles stay within valid search space.", "configspace": "", "generation": 6, "fitness": 0.13224112534684362, "feedback": "The algorithm NovelSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.13 with standard deviation 0.00.", "error": "", "parent_id": "998ce641-9536-43af-93bb-7df75dfca574", "metadata": {"aucs": [0.13258690857466948, 0.13318981584478806, 0.13094665162107333]}, "mutation_prompt": null}
{"id": "20731bae-c283-4f77-b072-2a9d98e4ab93", "solution": "import numpy as np\n\nclass NovelSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particle_count = min(30, budget // dim)\n        self.inertia_weight = 0.729  # inertia weight\n        self.cognitive_const = 1.49445  # cognitive constant\n        self.social_const = 1.49445  # social constant\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        # Initialize particles' positions and velocities\n        positions = np.random.uniform(lb, ub, (self.particle_count, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particle_count, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.particle_count, np.inf)\n\n        global_best_position = None\n        global_best_score = np.inf\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            # Evaluate current positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                current_score = func(positions[i])\n                evaluations += 1\n                \n                # Update personal best\n                if current_score < personal_best_scores[i]:\n                    personal_best_scores[i] = current_score\n                    personal_best_positions[i] = positions[i]\n                \n                # Update global best\n                if current_score < global_best_score:\n                    global_best_score = current_score\n                    global_best_position = positions[i]\n\n            # Update velocities and positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                r1, r2 = np.random.rand(2, self.dim)\n                velocities[i] = (self.inertia_weight * velocities[i] +\n                                 self.cognitive_const * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.social_const * r2 * (global_best_position - positions[i]))\n                velocities[i] *= 0.9  # Adaptive velocity scaling\n                positions[i] = np.clip(positions[i] + velocities[i], lb, ub)\n                if np.random.rand() < 0.05:  # Random jump mechanism\n                    positions[i] = np.random.uniform(lb, ub, self.dim)\n\n            # Adaptive inertia weight decay\n            self.inertia_weight *= 0.99\n            \n        return global_best_position", "name": "NovelSwarmOptimizer", "description": "Enhanced exploration by introducing a random jump mechanism into particle positions to avoid local minima.", "configspace": "", "generation": 7, "fitness": 0.13110268865174665, "feedback": "The algorithm NovelSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.13 with standard deviation 0.00.", "error": "", "parent_id": "998ce641-9536-43af-93bb-7df75dfca574", "metadata": {"aucs": [0.13344526606196228, 0.1331200502230263, 0.12674274967025134]}, "mutation_prompt": null}
{"id": "0a119f6a-d701-4ca9-a360-5fd1262d66f7", "solution": "import numpy as np\n\nclass NovelSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particle_count = min(30, budget // dim)\n        self.inertia_weight = 0.729  # inertia weight\n        self.cognitive_const = 1.49445  # cognitive constant\n        self.social_const = 1.49445  # social constant\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        # Initialize particles' positions using chaotic map for better exploration\n        positions = lb + (ub - lb) * np.sin(np.random.uniform(0, np.pi/2, (self.particle_count, self.dim)))\n        velocities = np.random.uniform(-1, 1, (self.particle_count, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.particle_count, np.inf)\n\n        global_best_position = None\n        global_best_score = np.inf\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            # Evaluate current positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                current_score = func(positions[i])\n                evaluations += 1\n                \n                # Update personal best\n                if current_score < personal_best_scores[i]:\n                    personal_best_scores[i] = current_score\n                    personal_best_positions[i] = positions[i]\n                \n                # Update global best\n                if current_score < global_best_score:\n                    global_best_score = current_score\n                    global_best_position = positions[i]\n\n            # Update velocities and positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                r1, r2 = np.random.rand(2, self.dim)\n                velocities[i] = (self.inertia_weight * velocities[i] +\n                                 self.cognitive_const * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.social_const * r2 * (global_best_position - positions[i]))\n                velocities[i] *= 0.9  # Adaptive velocity scaling\n                positions[i] = np.clip(positions[i] + velocities[i], lb, ub)\n\n            # Adaptive inertia weight decay\n            self.inertia_weight *= 0.99\n            self.cognitive_const *= 0.995  # Dynamic balance tweak\n            \n        return global_best_position", "name": "NovelSwarmOptimizer", "description": "Enhanced exploration by introducing chaotic mapping for initial position distribution and dynamic cognitive-social balance.", "configspace": "", "generation": 8, "fitness": 0.12720794305814687, "feedback": "The algorithm NovelSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.13 with standard deviation 0.00.", "error": "", "parent_id": "998ce641-9536-43af-93bb-7df75dfca574", "metadata": {"aucs": [0.12590139136542555, 0.12734980069885848, 0.12837263711015656]}, "mutation_prompt": null}
{"id": "4e62298e-771a-4869-aeae-d9ec8a52e85d", "solution": "import numpy as np\n\nclass NovelSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particle_count = min(30, budget // dim)\n        self.inertia_weight = 0.729  # inertia weight\n        self.cognitive_const = 1.49445  # cognitive constant\n        self.social_const = 1.49445  # social constant\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        # Initialize particles' positions and velocities\n        positions = np.random.uniform(lb, ub, (self.particle_count, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particle_count, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.particle_count, np.inf)\n\n        global_best_position = None\n        global_best_score = np.inf\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            # Evaluate current positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                current_score = func(positions[i])\n                evaluations += 1\n                \n                # Update personal best\n                if current_score < personal_best_scores[i]:\n                    personal_best_scores[i] = current_score\n                    personal_best_positions[i] = positions[i]\n                \n                # Update global best\n                if current_score < global_best_score:\n                    global_best_score = current_score\n                    global_best_position = positions[i]\n\n            # Update velocities and positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                r1, r2 = np.random.rand(2, self.dim)\n                velocities[i] = (self.inertia_weight * velocities[i] +\n                                 self.cognitive_const * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.social_const * r2 * (global_best_position - positions[i]))\n                velocities[i] *= 0.9  # Adaptive velocity scaling\n                positions[i] = np.clip(positions[i] + velocities[i], lb, ub)\n\n            # Nonlinear adaptive inertia weight decay\n            self.inertia_weight *= 0.99 * (1 + 0.01 * np.log1p(evaluations))\n            \n        return global_best_position", "name": "NovelSwarmOptimizer", "description": "Introduced nonlinear adaptive inertia weight decay for improved adaptive behavior and convergence.", "configspace": "", "generation": 9, "fitness": 0.13120615738934127, "feedback": "The algorithm NovelSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.13 with standard deviation 0.00.", "error": "", "parent_id": "998ce641-9536-43af-93bb-7df75dfca574", "metadata": {"aucs": [0.13256118791852578, 0.13369145745158106, 0.12736582679791697]}, "mutation_prompt": null}
{"id": "3b2caf67-5782-46de-a95b-a2d7f5142f73", "solution": "import numpy as np\n\nclass NovelSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particle_count = min(30, budget // dim)\n        self.inertia_weight = 0.729  # inertia weight\n        self.cognitive_const = 1.49445  # cognitive constant\n        self.social_const = 1.49445  # social constant\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        # Initialize particles' positions and velocities\n        positions = np.random.uniform(lb, ub, (self.particle_count, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particle_count, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.particle_count, np.inf)\n\n        global_best_position = None\n        global_best_score = np.inf\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            # Evaluate current positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                current_score = func(positions[i])\n                evaluations += 1\n                \n                # Update personal best\n                if current_score < personal_best_scores[i]:\n                    personal_best_scores[i] = current_score\n                    personal_best_positions[i] = positions[i]\n                \n                # Update global best\n                if current_score < global_best_score:\n                    global_best_score = current_score\n                    global_best_position = positions[i]\n\n            # Update velocities and positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                r1, r2 = np.random.rand(2, self.dim)\n                velocities[i] = (self.inertia_weight * velocities[i] +\n                                 self.cognitive_const * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.social_const * r2 * (global_best_position - positions[i]))\n                velocities[i] *= 0.9  # Adaptive velocity scaling\n                positions[i] = np.clip(positions[i] + velocities[i], lb, ub)\n\n            # Adaptive inertia weight decay\n            self.inertia_weight *= 0.99\n            self.social_const *= 0.995  # Adaptive social constant decay\n            \n        return global_best_position", "name": "NovelSwarmOptimizer", "description": "Enhanced dynamic parameter adjustment with adaptive social constant to improve global convergence.", "configspace": "", "generation": 10, "fitness": 0.13154326110381495, "feedback": "The algorithm NovelSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.13 with standard deviation 0.00.", "error": "", "parent_id": "998ce641-9536-43af-93bb-7df75dfca574", "metadata": {"aucs": [0.1325210465392085, 0.13314410505903485, 0.1289646317132015]}, "mutation_prompt": null}
{"id": "8581e173-93b3-4112-8341-3f204509725b", "solution": "import numpy as np\n\nclass NovelSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particle_count = min(30, budget // dim)\n        self.inertia_weight = 0.729  # inertia weight\n        self.cognitive_const = 1.49445  # cognitive constant\n        self.social_const = 1.49445  # social constant\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        # Initialize particles' positions and velocities\n        positions = np.random.uniform(lb, ub, (self.particle_count, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particle_count, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.particle_count, np.inf)\n\n        global_best_position = None\n        global_best_score = np.inf\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            # Evaluate current positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                current_score = func(positions[i])\n                evaluations += 1\n                \n                # Update personal best\n                if current_score < personal_best_scores[i]:\n                    personal_best_scores[i] = current_score\n                    personal_best_positions[i] = positions[i]\n                \n                # Update global best\n                if current_score < global_best_score:\n                    global_best_score = current_score\n                    global_best_position = positions[i]\n\n            # Update velocities and positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                r1, r2 = np.random.rand(2, self.dim)\n                velocities[i] = (self.inertia_weight * velocities[i] +\n                                 self.cognitive_const * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.social_const * r2 * (global_best_position - positions[i]))\n                velocities[i] *= 0.9  # Adaptive velocity scaling\n                positions[i] = np.clip(positions[i] + velocities[i], lb, ub)\n\n            # Adaptive inertia weight decay\n            self.inertia_weight *= 0.99\n            self.social_const = 1.49445 * (1 - evaluations / self.budget) + 0.5 * (evaluations / self.budget)\n            \n        return global_best_position", "name": "NovelSwarmOptimizer", "description": "Introduced dynamic social constant adjustment to enhance convergence speed and accuracy.", "configspace": "", "generation": 11, "fitness": 0.13175324922766674, "feedback": "The algorithm NovelSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.13 with standard deviation 0.00.", "error": "", "parent_id": "998ce641-9536-43af-93bb-7df75dfca574", "metadata": {"aucs": [0.13254078878525322, 0.1328897644716327, 0.1298291944261143]}, "mutation_prompt": null}
{"id": "1074466d-db0e-4cea-9793-23e55f6fec3b", "solution": "import numpy as np\n\nclass NovelSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particle_count = min(30, budget // dim)\n        self.inertia_weight = 0.729  # inertia weight\n        self.cognitive_const = 1.49445  # cognitive constant\n        self.social_const = 1.49445  # social constant\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        # Initialize particles' positions and velocities\n        positions = np.random.uniform(lb, ub, (self.particle_count, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particle_count, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.particle_count, np.inf)\n\n        global_best_position = None\n        global_best_score = np.inf\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            # Evaluate current positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                current_score = func(positions[i])\n                evaluations += 1\n                \n                # Update personal best\n                if current_score < personal_best_scores[i]:\n                    personal_best_scores[i] = current_score\n                    personal_best_positions[i] = positions[i]\n                \n                # Update global best\n                if current_score < global_best_score:\n                    global_best_score = current_score\n                    global_best_position = positions[i]\n\n            # Update velocities and positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                r1, r2 = np.random.rand(2, self.dim)\n                velocities[i] = (self.inertia_weight * velocities[i] +\n                                 self.cognitive_const * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.social_const * r2 * (global_best_position - positions[i]))\n                velocities[i] *= 0.9  # Adaptive velocity scaling\n                if evaluations < self.budget // 2:  # Random perturbation in early iterations\n                    velocities[i] += np.random.normal(0, 0.1, self.dim)\n                positions[i] = np.clip(positions[i] + velocities[i], lb, ub)\n\n            # Adaptive inertia weight decay\n            self.inertia_weight *= 0.99\n            \n        return global_best_position", "name": "NovelSwarmOptimizer", "description": "Incorporate random velocity direction perturbation to enhance exploration during early iterations.", "configspace": "", "generation": 12, "fitness": 0.1317525433726963, "feedback": "The algorithm NovelSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.13 with standard deviation 0.00.", "error": "", "parent_id": "998ce641-9536-43af-93bb-7df75dfca574", "metadata": {"aucs": [0.13326965933481538, 0.13265196516848032, 0.1293360056147932]}, "mutation_prompt": null}
{"id": "625f7a29-f8ae-410f-8a9a-fd7fa0ed07db", "solution": "import numpy as np\n\nclass NovelSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particle_count = min(30, budget // dim)\n        self.inertia_weight = 0.729  # inertia weight\n        self.cognitive_const = 1.49445  # cognitive constant\n        self.social_const = 1.49445  # social constant\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        # Initialize particles' positions and velocities\n        positions = np.random.uniform(lb, ub, (self.particle_count, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particle_count, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.particle_count, np.inf)\n\n        global_best_position = None\n        global_best_score = np.inf\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            # Evaluate current positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                current_score = func(positions[i])\n                evaluations += 1\n                \n                # Update personal best\n                if current_score < personal_best_scores[i]:\n                    personal_best_scores[i] = current_score\n                    personal_best_positions[i] = positions[i]\n                \n                # Update global best\n                if current_score < global_best_score:\n                    global_best_score = current_score\n                    global_best_position = positions[i]\n\n            # Update velocities and positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                r1, r2 = np.random.rand(2, self.dim)\n                velocities[i] = (self.inertia_weight * velocities[i] +\n                                 self.cognitive_const * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.social_const * r2 * (global_best_position - positions[i]))\n                velocities[i] *= 0.9  # Adaptive velocity scaling\n                positions[i] = np.clip(positions[i] + velocities[i], lb, ub)\n\n            # Adaptive inertia weight decay\n            self.inertia_weight *= 0.99\n\n            # Stochastic perturbation\n            if evaluations < self.budget:\n                perturbation = (np.random.rand(self.dim) - 0.5) * 0.1 * (ub - lb)\n                test_position = np.clip(global_best_position + perturbation, lb, ub)\n                test_score = func(test_position)\n                evaluations += 1\n                if test_score < global_best_score:\n                    global_best_score = test_score\n                    global_best_position = test_position\n            \n        return global_best_position", "name": "NovelSwarmOptimizer", "description": "Enhanced convergence by incorporating stochastic local search through random perturbation of the global best position.", "configspace": "", "generation": 13, "fitness": 0.13017184484726438, "feedback": "The algorithm NovelSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.13 with standard deviation 0.00.", "error": "", "parent_id": "998ce641-9536-43af-93bb-7df75dfca574", "metadata": {"aucs": [0.13029648389837467, 0.13201244731336925, 0.12820660333004918]}, "mutation_prompt": null}
{"id": "212471a3-92f7-4233-a07c-d341d6aaf5f0", "solution": "import numpy as np\n\nclass NovelSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particle_count = min(30, budget // dim)\n        self.inertia_weight = 0.729  # inertia weight\n        self.cognitive_const = 1.49445  # cognitive constant\n        self.social_const = 1.49445  # social constant\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        # Initialize particles' positions and velocities\n        positions = np.random.uniform(lb, ub, (self.particle_count, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particle_count, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.particle_count, np.inf)\n\n        global_best_position = None\n        global_best_score = np.inf\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            # Evaluate current positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                current_score = func(positions[i])\n                evaluations += 1\n                \n                # Update personal best\n                if current_score < personal_best_scores[i]:\n                    personal_best_scores[i] = current_score\n                    personal_best_positions[i] = positions[i]\n                \n                # Update global best\n                if current_score < global_best_score:\n                    global_best_score = current_score\n                    global_best_position = positions[i]\n\n            # Update velocities and positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                r1, r2 = np.random.rand(2, self.dim)\n                velocities[i] = (self.inertia_weight * velocities[i] +\n                                 self.cognitive_const * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.social_const * r2 * (global_best_position - positions[i]))\n                velocities[i] *= 0.9  # Adaptive velocity scaling\n                positions[i] = np.clip(positions[i] + velocities[i]*0.5, lb, ub)  # Using constriction factor\n\n            # Adaptive inertia weight decay\n            self.inertia_weight *= 0.99\n            \n        return global_best_position", "name": "NovelSwarmOptimizer", "description": "Enhanced position update by adding constriction factor for better convergence.", "configspace": "", "generation": 14, "fitness": 0.1331888930633216, "feedback": "The algorithm NovelSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.13 with standard deviation 0.00.", "error": "", "parent_id": "998ce641-9536-43af-93bb-7df75dfca574", "metadata": {"aucs": [0.13408542152214886, 0.13293798066679774, 0.1325432770010182]}, "mutation_prompt": null}
{"id": "e226bdde-4ddb-4390-a7b8-2e26386dd647", "solution": "import numpy as np\n\nclass NovelSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particle_count = min(30, budget // dim)\n        self.inertia_weight = 0.729  # inertia weight\n        self.cognitive_const = 1.49445  # cognitive constant\n        self.social_const = 1.49445  # social constant\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        # Initialize particles' positions and velocities\n        positions = np.random.uniform(lb, ub, (self.particle_count, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particle_count, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.particle_count, np.inf)\n\n        global_best_position = None\n        global_best_score = np.inf\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            # Evaluate current positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                current_score = func(positions[i])\n                evaluations += 1\n                \n                # Update personal best\n                if current_score < personal_best_scores[i]:\n                    personal_best_scores[i] = current_score\n                    personal_best_positions[i] = positions[i]\n                \n                # Update global best\n                if current_score < global_best_score:\n                    global_best_score = current_score\n                    global_best_position = positions[i]\n\n            # Update velocities and positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                r1, r2 = np.random.rand(2, self.dim)\n                velocities[i] = (self.inertia_weight * velocities[i] +\n                                 self.cognitive_const * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.social_const * r2 * (global_best_position - positions[i]))\n                velocities[i] *= 0.95  # Adaptive velocity scaling\n                positions[i] = np.clip(positions[i] + velocities[i]*0.6, lb, ub)  # Using constriction factor\n\n            # Adaptive inertia weight decay\n            self.inertia_weight *= 0.99\n            \n        return global_best_position", "name": "NovelSwarmOptimizer", "description": "Enhanced velocity update through adaptive dynamic scaling for improved convergence.", "configspace": "", "generation": 15, "fitness": 0.13120434246724502, "feedback": "The algorithm NovelSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.13 with standard deviation 0.00.", "error": "", "parent_id": "212471a3-92f7-4233-a07c-d341d6aaf5f0", "metadata": {"aucs": [0.13189285485803315, 0.13280706171727807, 0.12891311082642387]}, "mutation_prompt": null}
{"id": "2106dfc3-e09b-4a02-b89d-445ba131ded2", "solution": "import numpy as np\n\nclass NovelSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particle_count = min(30, budget // dim)\n        self.inertia_weight = 0.729  # inertia weight\n        self.cognitive_const = 1.49445  # cognitive constant\n        self.social_const = 1.49445  # social constant\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        # Initialize particles' positions and velocities\n        positions = np.random.uniform(lb, ub, (self.particle_count, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particle_count, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.particle_count, np.inf)\n\n        global_best_position = None\n        global_best_score = np.inf\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            # Evaluate current positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                current_score = func(positions[i])\n                evaluations += 1\n                \n                # Update personal best\n                if current_score < personal_best_scores[i]:\n                    personal_best_scores[i] = current_score\n                    personal_best_positions[i] = positions[i]\n                \n                # Update global best\n                if current_score < global_best_score:\n                    global_best_score = current_score\n                    global_best_position = positions[i]\n\n            # Update velocities and positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                r1, r2 = np.random.rand(2, self.dim)\n                velocities[i] = (self.inertia_weight * velocities[i] +\n                                 self.cognitive_const * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.social_const * r2 * (global_best_position - positions[i]))\n                velocities[i] *= 0.9  # Adaptive velocity scaling\n                positions[i] = np.clip(positions[i] + velocities[i]*0.5, lb, ub)  # Using constriction factor\n\n            # Adaptive inertia weight decay\n            self.inertia_weight *= 0.99\n\n            # Adaptive social constant\n            self.social_const *= 1.01 if evaluations < self.budget / 2 else 0.99\n            \n        return global_best_position", "name": "NovelSwarmOptimizer", "description": "Improved convergence using adaptive social constant to adjust exploration-exploitation balance.", "configspace": "", "generation": 16, "fitness": 0.13331662164043004, "feedback": "The algorithm NovelSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.13 with standard deviation 0.00.", "error": "", "parent_id": "212471a3-92f7-4233-a07c-d341d6aaf5f0", "metadata": {"aucs": [0.13415207913135396, 0.1330777619009893, 0.13272002388894688]}, "mutation_prompt": null}
{"id": "6269fb9f-856f-4ac1-818a-c68bb12d3e7a", "solution": "import numpy as np\n\nclass NovelSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particle_count = min(30, budget // dim)\n        self.inertia_weight = 0.729  # inertia weight\n        self.cognitive_const = 1.49445  # cognitive constant\n        self.social_const = 1.49445  # social constant\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        # Initialize particles' positions and velocities\n        positions = np.random.uniform(lb, ub, (self.particle_count, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particle_count, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.particle_count, np.inf)\n\n        global_best_position = None\n        global_best_score = np.inf\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            # Evaluate current positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                current_score = func(positions[i])\n                evaluations += 1\n                \n                # Update personal best\n                if current_score < personal_best_scores[i]:\n                    personal_best_scores[i] = current_score\n                    personal_best_positions[i] = positions[i]\n                \n                # Update global best\n                if current_score < global_best_score:\n                    global_best_score = current_score\n                    global_best_position = positions[i]\n\n            # Update velocities and positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                r1, r2 = np.random.rand(2, self.dim)\n                velocities[i] = (self.inertia_weight * velocities[i] +\n                                 self.cognitive_const * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.social_const * r2 * (global_best_position - positions[i]))\n                velocities[i] *= 0.9  # Adaptive velocity scaling\n                positions[i] = np.clip(positions[i] + velocities[i]*0.5, lb, ub)  # Using constriction factor\n\n            # Adaptive inertia weight decay\n            self.inertia_weight *= 0.99\n\n            # Adaptive social constant\n            self.social_const *= 1.01 if evaluations < self.budget / 2 else 0.99\n            # Adaptive cognitive constant\n            self.cognitive_const *= 0.99 if evaluations < self.budget / 2 else 1.01\n            \n        return global_best_position", "name": "NovelSwarmOptimizer", "description": "Enhanced swarm dynamics by introducing a dynamic cognitive constant adaptation for better exploration-exploitation trade-off.", "configspace": "", "generation": 17, "fitness": 0.13326975697873378, "feedback": "The algorithm NovelSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.13 with standard deviation 0.00.", "error": "", "parent_id": "2106dfc3-e09b-4a02-b89d-445ba131ded2", "metadata": {"aucs": [0.1341077131128795, 0.13304968401975048, 0.13265187380357135]}, "mutation_prompt": null}
{"id": "d140a7ba-2f32-487d-94ae-8b8b55a7281e", "solution": "import numpy as np\n\nclass NovelSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particle_count = min(30, budget // dim)\n        self.inertia_weight = 0.729  # inertia weight\n        self.cognitive_const = 1.49445  # cognitive constant\n        self.social_const = 1.49445  # social constant\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        # Initialize particles' positions and velocities\n        positions = np.random.uniform(lb, ub, (self.particle_count, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particle_count, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.particle_count, np.inf)\n\n        global_best_position = None\n        global_best_score = np.inf\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            # Evaluate current positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                current_score = func(positions[i])\n                evaluations += 1\n                \n                # Update personal best\n                if current_score < personal_best_scores[i]:\n                    personal_best_scores[i] = current_score\n                    personal_best_positions[i] = positions[i]\n                \n                # Update global best\n                if current_score < global_best_score:\n                    global_best_score = current_score\n                    global_best_position = positions[i]\n\n            # Update velocities and positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                r1, r2 = np.random.rand(2, self.dim)\n                velocities[i] = (self.inertia_weight * velocities[i] +\n                                 self.cognitive_const * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.social_const * r2 * (global_best_position - positions[i]))\n                velocities[i] *= 0.95  # Adaptive velocity scaling\n                positions[i] = np.clip(positions[i] + velocities[i], lb, ub)  # Using constriction factor\n\n            # Adaptive inertia weight decay\n            self.inertia_weight *= 0.98\n\n            # Adaptive social constant\n            self.social_const *= 1.01 if evaluations < self.budget / 2 else 0.99\n            \n        return global_best_position", "name": "NovelSwarmOptimizer", "description": "Enhanced inertia weight decay and velocity scaling for better convergence precision.", "configspace": "", "generation": 18, "fitness": 0.13204833330075452, "feedback": "The algorithm NovelSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.13 with standard deviation 0.00.", "error": "", "parent_id": "2106dfc3-e09b-4a02-b89d-445ba131ded2", "metadata": {"aucs": [0.13365183359048083, 0.13288477188685777, 0.129608394424925]}, "mutation_prompt": null}
{"id": "dd6cb364-685b-4f6c-b9e6-90c4ffa377e4", "solution": "import numpy as np\n\nclass NovelSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particle_count = min(30, budget // dim)\n        self.inertia_weight = 0.729  # inertia weight\n        self.cognitive_const = 1.49445  # cognitive constant\n        self.social_const = 1.49445  # social constant\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        # Initialize particles' positions and velocities\n        positions = np.random.uniform(lb, ub, (self.particle_count, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particle_count, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.particle_count, np.inf)\n\n        global_best_position = None\n        global_best_score = np.inf\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            # Evaluate current positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                current_score = func(positions[i])\n                evaluations += 1\n                \n                # Update personal best\n                if current_score < personal_best_scores[i]:\n                    personal_best_scores[i] = current_score\n                    personal_best_positions[i] = positions[i]\n                \n                # Update global best\n                if current_score < global_best_score:\n                    global_best_score = current_score\n                    global_best_position = positions[i]\n\n            # Update velocities and positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                r1, r2 = np.random.rand(2, self.dim)\n                velocities[i] = (self.inertia_weight * velocities[i] +\n                                 self.cognitive_const * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.social_const * r2 * (global_best_position - positions[i]))\n                velocities[i] *= 0.9  # Adaptive velocity scaling\n                positions[i] = np.clip(positions[i] + velocities[i]*0.5, lb, ub)  # Using constriction factor\n\n            # Adaptive inertia weight decay\n            self.inertia_weight *= 0.99\n\n            # Adaptive cognitive and social constants\n            self.cognitive_const *= 0.99 if evaluations < self.budget / 2 else 1.01\n            self.social_const *= 1.01 if evaluations < self.budget / 2 else 0.99\n            \n        return global_best_position", "name": "NovelSwarmOptimizer", "description": "Enhanced convergence by adapting cognitive and social constants inversely to encourage exploration early and exploitation later.", "configspace": "", "generation": 19, "fitness": 0.13326975697873378, "feedback": "The algorithm NovelSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.13 with standard deviation 0.00.", "error": "", "parent_id": "2106dfc3-e09b-4a02-b89d-445ba131ded2", "metadata": {"aucs": [0.1341077131128795, 0.13304968401975048, 0.13265187380357135]}, "mutation_prompt": null}
{"id": "7af01735-1a52-4e2a-ad6f-16965294e9b4", "solution": "import numpy as np\n\nclass NovelSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particle_count = min(30, budget // dim)\n        self.inertia_weight = 0.729  # inertia weight\n        self.cognitive_const = 1.49445  # cognitive constant\n        self.social_const = 1.49445  # social constant\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        # Initialize particles' positions and velocities\n        positions = np.random.uniform(lb, ub, (self.particle_count, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particle_count, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.particle_count, np.inf)\n\n        global_best_position = None\n        global_best_score = np.inf\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            # Evaluate current positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                current_score = func(positions[i])\n                evaluations += 1\n                \n                # Update personal best\n                if current_score < personal_best_scores[i]:\n                    personal_best_scores[i] = current_score\n                    personal_best_positions[i] = positions[i]\n                \n                # Update global best\n                if current_score < global_best_score:\n                    global_best_score = current_score\n                    global_best_position = positions[i]\n\n            # Update velocities and positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                r1, r2 = np.random.rand(2, self.dim)\n                velocities[i] = (self.inertia_weight * velocities[i] +\n                                 self.cognitive_const * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.social_const * r2 * (global_best_position - positions[i]))\n                velocities[i] *= 0.9  # Adaptive velocity scaling\n                positions[i] = np.clip(positions[i] + velocities[i]*0.5, lb, ub)  # Using constriction factor\n\n            # Adaptive inertia weight decay\n            self.inertia_weight *= 0.99\n\n            # Adaptive cognitive and social constants\n            self.cognitive_const *= 1.01 if evaluations < self.budget / 2 else 0.99\n            self.social_const *= 1.01 if evaluations < self.budget / 2 else 0.99\n            \n        return global_best_position", "name": "NovelSwarmOptimizer", "description": "Enhanced convergence by dynamically adjusting both cognitive and social constants for improved exploration-exploitation balance.", "configspace": "", "generation": 20, "fitness": 0.13310733818808831, "feedback": "The algorithm NovelSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.13 with standard deviation 0.00.", "error": "", "parent_id": "2106dfc3-e09b-4a02-b89d-445ba131ded2", "metadata": {"aucs": [0.13413638443024445, 0.13274521902173408, 0.13244041111228644]}, "mutation_prompt": null}
{"id": "266f962a-f6a1-438d-93a9-de840b858102", "solution": "import numpy as np\n\nclass NovelSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particle_count = min(30, budget // dim)\n        self.inertia_weight = 0.729  # inertia weight\n        self.cognitive_const = 1.49445  # cognitive constant\n        self.social_const = 1.49445  # social constant\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        # Initialize particles' positions and velocities\n        positions = np.random.uniform(lb, ub, (self.particle_count, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particle_count, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.particle_count, np.inf)\n\n        global_best_position = None\n        global_best_score = np.inf\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            # Evaluate current positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                current_score = func(positions[i])\n                evaluations += 1\n                \n                # Update personal best\n                if current_score < personal_best_scores[i]:\n                    personal_best_scores[i] = current_score\n                    personal_best_positions[i] = positions[i]\n                \n                # Update global best\n                if current_score < global_best_score:\n                    global_best_score = current_score\n                    global_best_position = positions[i]\n\n            # Update velocities and positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                r1, r2 = np.random.rand(2, self.dim)\n                velocities[i] = (self.inertia_weight * velocities[i] +\n                                 self.cognitive_const * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.social_const * r2 * (global_best_position - positions[i]))\n                velocities[i] *= 0.9  # Adaptive velocity scaling\n                positions[i] = np.clip(positions[i] + velocities[i]*0.5, lb, ub)  # Using constriction factor\n\n            # Adaptive inertia weight decay\n            self.inertia_weight *= 0.99\n\n            # Adaptive cognitive and social constants\n            progress_ratio = evaluations / self.budget\n            self.cognitive_const = 1.5 - 0.5 * progress_ratio\n            self.social_const = 1.5 + 0.5 * progress_ratio\n            \n        return global_best_position", "name": "NovelSwarmOptimizer", "description": "Enhanced exploration by introducing dynamic cognitive and social constants based on evaluation progress.", "configspace": "", "generation": 21, "fitness": 0.13344792634483946, "feedback": "The algorithm NovelSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.13 with standard deviation 0.00.", "error": "", "parent_id": "2106dfc3-e09b-4a02-b89d-445ba131ded2", "metadata": {"aucs": [0.13401419834064998, 0.13362455912203985, 0.1327050215718285]}, "mutation_prompt": null}
{"id": "d3ee6178-450d-48eb-8505-18d12b03b43f", "solution": "import numpy as np\n\nclass NovelSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particle_count = min(30, budget // dim)\n        self.inertia_weight = 0.729  # inertia weight\n        self.cognitive_const = 1.49445  # cognitive constant\n        self.social_const = 1.49445  # social constant\n        self.mutation_rate = 0.1  # mutation rate\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        # Initialize particles' positions and velocities\n        positions = np.random.uniform(lb, ub, (self.particle_count, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particle_count, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.particle_count, np.inf)\n\n        global_best_position = None\n        global_best_score = np.inf\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            # Evaluate current positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                current_score = func(positions[i])\n                evaluations += 1\n                \n                # Update personal best\n                if current_score < personal_best_scores[i]:\n                    personal_best_scores[i] = current_score\n                    personal_best_positions[i] = positions[i]\n                \n                # Update global best\n                if current_score < global_best_score:\n                    global_best_score = current_score\n                    global_best_position = positions[i]\n\n            # Update velocities and positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                r1, r2 = np.random.rand(2, self.dim)\n                velocities[i] = (self.inertia_weight * velocities[i] +\n                                 self.cognitive_const * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.social_const * r2 * (global_best_position - positions[i]))\n                velocities[i] *= 0.9  # Adaptive velocity scaling\n                positions[i] = np.clip(positions[i] + velocities[i]*0.5, lb, ub)  # Using constriction factor\n\n                # Add mutation to occasionally disrupt solutions\n                if np.random.rand() < self.mutation_rate:\n                    positions[i] += np.random.normal(0, 0.1, self.dim)\n\n            # Adaptive inertia weight decay\n            self.inertia_weight *= 0.99\n\n            # Adaptive cognitive and social constants\n            progress_ratio = evaluations / self.budget\n            self.cognitive_const = 1.5 - 0.5 * progress_ratio\n            self.social_const = 1.5 + 0.5 * progress_ratio\n            \n        return global_best_position", "name": "NovelSwarmOptimizer", "description": "Enhanced particle diversity by introducing a random mutation mechanism to occasionally perturb particle positions.", "configspace": "", "generation": 22, "fitness": 0.1329268986680124, "feedback": "The algorithm NovelSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.13 with standard deviation 0.00.", "error": "", "parent_id": "266f962a-f6a1-438d-93a9-de840b858102", "metadata": {"aucs": [0.13435335119059344, 0.13336931140011998, 0.13105803341332378]}, "mutation_prompt": null}
{"id": "3e210e30-789a-42ae-8b6e-5aacba9595cf", "solution": "import numpy as np\n\nclass NovelSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particle_count = min(30, budget // dim)\n        self.inertia_weight = 0.729  # inertia weight\n        self.cognitive_const = 1.49445  # cognitive constant\n        self.social_const = 1.49445  # social constant\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        # Initialize particles' positions and velocities\n        positions = np.random.uniform(lb, ub, (self.particle_count, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particle_count, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.particle_count, np.inf)\n\n        global_best_position = None\n        global_best_score = np.inf\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            # Evaluate current positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                current_score = func(positions[i])\n                evaluations += 1\n                \n                # Update personal best\n                if current_score < personal_best_scores[i]:\n                    personal_best_scores[i] = current_score\n                    personal_best_positions[i] = positions[i]\n                \n                # Update global best\n                if current_score < global_best_score:\n                    global_best_score = current_score\n                    global_best_position = positions[i]\n\n            # Update velocities and positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                r1, r2 = np.random.rand(2, self.dim)\n                velocities[i] = (self.inertia_weight * velocities[i] +\n                                 self.cognitive_const * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.social_const * r2 * (global_best_position - positions[i]))\n                velocities[i] *= 0.9  # Adaptive velocity scaling\n                positions[i] = np.clip(positions[i] + velocities[i]*0.5, lb, ub)  # Using constriction factor\n\n            # Adaptive inertia weight decay\n            self.inertia_weight *= 0.99\n\n            # Adaptive cognitive and social constants\n            progress_ratio = evaluations / self.budget\n            self.cognitive_const = 1.5 - 0.5 * progress_ratio\n            self.social_const = 1.5 + 0.5 * progress_ratio\n\n            positions += np.random.normal(0, 0.01, positions.shape)  # Random perturbation\n\n        return global_best_position", "name": "NovelSwarmOptimizer", "description": "Introduce random perturbation to global best position to enhance exploration.", "configspace": "", "generation": 23, "fitness": 0.13270984311460154, "feedback": "The algorithm NovelSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.13 with standard deviation 0.00.", "error": "", "parent_id": "266f962a-f6a1-438d-93a9-de840b858102", "metadata": {"aucs": [0.13293490738841018, 0.1330371050463638, 0.13215751690903066]}, "mutation_prompt": null}
{"id": "1a854dda-368e-42e3-9427-45515947a20d", "solution": "import numpy as np\n\nclass NovelSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particle_count = min(30, budget // dim)\n        self.inertia_weight = 0.729  # inertia weight\n        self.cognitive_const = 1.49445  # cognitive constant\n        self.social_const = 1.49445  # social constant\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        # Initialize particles' positions and velocities\n        positions = np.random.uniform(lb, ub, (self.particle_count, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particle_count, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.particle_count, np.inf)\n\n        global_best_position = None\n        global_best_score = np.inf\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            # Evaluate current positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                current_score = func(positions[i])\n                evaluations += 1\n                \n                # Update personal best\n                if current_score < personal_best_scores[i]:\n                    personal_best_scores[i] = current_score\n                    personal_best_positions[i] = positions[i]\n                \n                # Update global best\n                if current_score < global_best_score:\n                    global_best_score = current_score\n                    global_best_position = positions[i]\n\n            # Update velocities and positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                r1, r2 = np.random.rand(2, self.dim)\n                velocities[i] = (self.inertia_weight * velocities[i] +\n                                 self.cognitive_const * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.social_const * r2 * (global_best_position - positions[i]))\n                velocities[i] *= 0.9  # Adaptive velocity scaling\n                positions[i] = np.clip(positions[i] + velocities[i]*0.5, lb, ub)  # Using constriction factor\n\n            # Adaptive inertia weight decay\n            self.inertia_weight *= 0.99\n\n            # Adaptive cognitive and social constants\n            progress_ratio = evaluations / self.budget\n            self.cognitive_const = 1.5 - 0.5 * progress_ratio\n            self.social_const = 1.5 + 0.25 * progress_ratio  # Adjusted decay rate\n            \n        return global_best_position", "name": "NovelSwarmOptimizer", "description": "Improved convergence by adjusting the social constant decay rate in correlation with budget usage.", "configspace": "", "generation": 24, "fitness": 0.13294125771027762, "feedback": "The algorithm NovelSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.13 with standard deviation 0.00.", "error": "", "parent_id": "266f962a-f6a1-438d-93a9-de840b858102", "metadata": {"aucs": [0.13401760157706843, 0.1328018459507848, 0.1320043256029796]}, "mutation_prompt": null}
{"id": "40dee9bd-c5b3-43a0-b4e0-12a1a8ef56e9", "solution": "import numpy as np\n\nclass NovelSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particle_count = min(30, budget // dim)\n        self.inertia_weight = 0.729\n        self.cognitive_const = 1.49445\n        self.social_const = 1.49445\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        positions = np.random.uniform(lb, ub, (self.particle_count, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particle_count, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.particle_count, np.inf)\n\n        global_best_position = None\n        global_best_score = np.inf\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                current_score = func(positions[i])\n                evaluations += 1\n                \n                if current_score < personal_best_scores[i]:\n                    personal_best_scores[i] = current_score\n                    personal_best_positions[i] = positions[i]\n                \n                if current_score < global_best_score:\n                    global_best_score = current_score\n                    global_best_position = positions[i]\n\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                r1, r2 = np.random.rand(2, self.dim)\n                velocities[i] = (self.inertia_weight * velocities[i] +\n                                 self.cognitive_const * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.social_const * r2 * (global_best_position - positions[i]))\n                velocities[i] *= 0.95  # Changed from 0.9 to improve convergence\n                positions[i] = np.clip(positions[i] + velocities[i]*0.55, lb, ub)  # Changed from 0.5 to enhance exploration\n\n            self.inertia_weight *= 0.99\n\n            progress_ratio = evaluations / self.budget\n            self.cognitive_const = 1.5 - 0.5 * progress_ratio\n            self.social_const = 1.5 + 0.5 * progress_ratio\n            \n        return global_best_position", "name": "NovelSwarmOptimizer", "description": "Adaptive scaling for velocity and position updates to enhance convergence efficiency.", "configspace": "", "generation": 25, "fitness": 0.13205088559027167, "feedback": "The algorithm NovelSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.13 with standard deviation 0.00.", "error": "", "parent_id": "266f962a-f6a1-438d-93a9-de840b858102", "metadata": {"aucs": [0.1320980206296618, 0.13307923639822117, 0.13097539974293204]}, "mutation_prompt": null}
{"id": "9569f154-d7d1-4df4-9204-b9c1f2437a1c", "solution": "import numpy as np\n\nclass NovelSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particle_count = min(30, budget // dim)\n        self.inertia_weight = 0.729  # inertia weight\n        self.cognitive_const = 1.49445  # cognitive constant\n        self.social_const = 1.49445  # social constant\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        # Initialize particles' positions and velocities\n        positions = np.random.uniform(lb, ub, (self.particle_count, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particle_count, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.particle_count, np.inf)\n\n        global_best_position = None\n        global_best_score = np.inf\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            # Evaluate current positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                current_score = func(positions[i])\n                evaluations += 1\n                \n                # Update personal best\n                if current_score < personal_best_scores[i]:\n                    personal_best_scores[i] = current_score\n                    personal_best_positions[i] = positions[i]\n                \n                # Update global best\n                if current_score < global_best_score:\n                    global_best_score = current_score\n                    global_best_position = positions[i]\n\n            # Update velocities and positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                r1, r2 = np.random.rand(2, self.dim)\n                velocities[i] = (self.inertia_weight * velocities[i] +\n                                 self.cognitive_const * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.social_const * r2 * (global_best_position - positions[i]))\n                velocities[i] *= 0.93  # Adaptive velocity scaling\n                positions[i] = np.clip(positions[i] + velocities[i]*0.5, lb, ub)  # Using constriction factor\n\n            # Adaptive inertia weight decay (non-linear)\n            self.inertia_weight *= (0.99)**(evaluations/self.budget)\n\n            # Adaptive cognitive and social constants\n            progress_ratio = evaluations / self.budget\n            self.cognitive_const = 1.5 - 0.5 * progress_ratio\n            self.social_const = 1.5 + 0.5 * progress_ratio\n            \n        return global_best_position", "name": "NovelSwarmOptimizer", "description": "Introduced a non-linear decay for the inertia weight and adjusted velocity scaling to enhance convergence stability.", "configspace": "", "generation": 26, "fitness": 0.13316323758580773, "feedback": "The algorithm NovelSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.13 with standard deviation 0.00.", "error": "", "parent_id": "266f962a-f6a1-438d-93a9-de840b858102", "metadata": {"aucs": [0.13400767164694927, 0.1339635150715388, 0.1315185260389351]}, "mutation_prompt": null}
{"id": "d35bc243-c567-4cc0-b0f0-4a2be6f218f8", "solution": "import numpy as np\n\nclass NovelSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particle_count = min(30, budget // dim)\n        self.inertia_weight = 0.729  # inertia weight\n        self.cognitive_const = 1.49445  # cognitive constant\n        self.social_const = 1.49445  # social constant\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        # Initialize particles' positions and velocities\n        positions = np.random.uniform(lb, ub, (self.particle_count, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particle_count, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.particle_count, np.inf)\n\n        global_best_position = None\n        global_best_score = np.inf\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            # Evaluate current positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                current_score = func(positions[i])\n                evaluations += 1\n                \n                # Update personal best\n                if current_score < personal_best_scores[i]:\n                    personal_best_scores[i] = current_score\n                    personal_best_positions[i] = positions[i]\n                \n                # Update global best\n                if current_score < global_best_score:\n                    global_best_score = current_score\n                    global_best_position = positions[i]\n\n            # Update velocities and positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                r1, r2 = np.random.rand(2, self.dim)\n                velocities[i] = (self.inertia_weight * velocities[i] +\n                                 self.cognitive_const * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.social_const * r2 * (global_best_position - positions[i]))\n                velocities[i] *= 0.95  # Adaptive velocity scaling adjustment\n                positions[i] = np.clip(positions[i] + velocities[i]*0.6, lb, ub)  # Adjust constriction factor\n\n            # Adaptive inertia weight decay\n            self.inertia_weight *= 0.98\n\n            # Adaptive cognitive and social constants\n            progress_ratio = evaluations / self.budget\n            self.cognitive_const = 1.5 - 0.5 * progress_ratio\n            self.social_const = 1.5 + 0.5 * progress_ratio\n            \n        return global_best_position", "name": "NovelSwarmOptimizer", "description": "Refine dynamic parameter adaptation by enhancing inertia weight and constriction factor adjustments during optimization.", "configspace": "", "generation": 27, "fitness": 0.1316440836638597, "feedback": "The algorithm NovelSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.13 with standard deviation 0.00.", "error": "", "parent_id": "266f962a-f6a1-438d-93a9-de840b858102", "metadata": {"aucs": [0.13244832430706666, 0.13303182560105464, 0.12945210108345784]}, "mutation_prompt": null}
{"id": "632ba4ab-6f35-4008-acf5-fe3e6c07633c", "solution": "import numpy as np\n\nclass NovelSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particle_count = min(30, budget // dim)\n        self.inertia_weight = 0.729  # inertia weight\n        self.cognitive_const = 1.49445  # cognitive constant\n        self.social_const = 1.49445  # social constant\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        # Initialize particles' positions and velocities\n        positions = np.random.uniform(lb, ub, (self.particle_count, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particle_count, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.particle_count, np.inf)\n\n        global_best_position = None\n        global_best_score = np.inf\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            # Evaluate current positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                current_score = func(positions[i])\n                evaluations += 1\n                \n                # Update personal best\n                if current_score < personal_best_scores[i]:\n                    personal_best_scores[i] = current_score\n                    personal_best_positions[i] = positions[i]\n                \n                # Update global best\n                if current_score < global_best_score:\n                    global_best_score = current_score\n                    global_best_position = positions[i]\n\n            # Update velocities and positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                r1, r2 = np.random.rand(2, self.dim)\n                velocities[i] = (self.inertia_weight * velocities[i] +\n                                 self.cognitive_const * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.social_const * r2 * (global_best_position - positions[i]))\n                velocities[i] *= 0.95 + 0.05 * np.sin(0.1 * evaluations)  # Adaptive velocity scaling\n                positions[i] = np.clip(positions[i] + velocities[i]*0.5, lb, ub)  # Using constriction factor\n\n            # Adaptive inertia weight decay\n            self.inertia_weight *= 0.98  # Non-linear decay\n\n            # Adaptive cognitive and social constants\n            progress_ratio = evaluations / self.budget\n            self.cognitive_const = 1.5 - 0.5 * progress_ratio\n            self.social_const = 1.5 + 0.5 * progress_ratio\n            \n        return global_best_position", "name": "NovelSwarmOptimizer", "description": "Introduce a non-linear inertia weight decay and adaptive velocity scaling for improved convergence.", "configspace": "", "generation": 28, "fitness": 0.1330913063495711, "feedback": "The algorithm NovelSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.13 with standard deviation 0.00.", "error": "", "parent_id": "266f962a-f6a1-438d-93a9-de840b858102", "metadata": {"aucs": [0.13348227551720182, 0.13372070738755082, 0.1320709361439606]}, "mutation_prompt": null}
{"id": "c89379ed-070c-41da-a6d7-832a53990e80", "solution": "import numpy as np\n\nclass NovelSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particle_count = min(30, budget // dim)\n        self.inertia_weight = 0.729  # inertia weight\n        self.cognitive_const = 1.49445  # cognitive constant\n        self.social_const = 1.49445  # social constant\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        # Initialize particles' positions and velocities\n        positions = np.random.uniform(lb, ub, (self.particle_count, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particle_count, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.particle_count, np.inf)\n\n        global_best_position = None\n        global_best_score = np.inf\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            # Evaluate current positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                current_score = func(positions[i])\n                evaluations += 1\n                \n                # Update personal best\n                if current_score < personal_best_scores[i]:\n                    personal_best_scores[i] = current_score\n                    personal_best_positions[i] = positions[i]\n                \n                # Update global best\n                if current_score < global_best_score:\n                    global_best_score = current_score\n                    global_best_position = positions[i]\n\n            # Update velocities and positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                r1, r2 = np.random.rand(2, self.dim)\n                velocities[i] = (self.inertia_weight * velocities[i] +\n                                 self.cognitive_const * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.social_const * r2 * (global_best_position - positions[i]))\n                velocities[i] += np.random.normal(0, 0.1, self.dim)  # Stochastic component added\n                velocities[i] *= 0.9  # Adaptive velocity scaling\n                positions[i] = np.clip(positions[i] + velocities[i]*0.5, lb, ub)  # Using constriction factor\n\n            # Adaptive inertia weight decay\n            self.inertia_weight *= 0.99\n\n            # Adaptive cognitive and social constants\n            progress_ratio = evaluations / self.budget\n            self.cognitive_const = 1.5 - 0.5 * progress_ratio\n            self.social_const = 1.5 + 0.5 * progress_ratio\n            \n        return global_best_position", "name": "NovelSwarmOptimizer", "description": "Introduce a dynamic stochastic component in velocity updates to enhance exploration toward global convergence.", "configspace": "", "generation": 29, "fitness": 0.13295801847572505, "feedback": "The algorithm NovelSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.13 with standard deviation 0.00.", "error": "", "parent_id": "266f962a-f6a1-438d-93a9-de840b858102", "metadata": {"aucs": [0.13416866745323164, 0.13305774257915104, 0.13164764539479246]}, "mutation_prompt": null}
{"id": "7f64e988-3a9f-404a-8c5b-d55547cbd7b1", "solution": "import numpy as np\n\nclass NovelSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particle_count = min(30, budget // dim)\n        self.inertia_weight = 0.729  # inertia weight\n        self.cognitive_const = 1.49445  # cognitive constant\n        self.social_const = 1.49445  # social constant\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        # Initialize particles' positions and velocities\n        positions = np.random.uniform(lb, ub, (self.particle_count, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particle_count, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.particle_count, np.inf)\n\n        global_best_position = None\n        global_best_score = np.inf\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            # Evaluate current positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                current_score = func(positions[i])\n                evaluations += 1\n                \n                # Update personal best\n                if current_score < personal_best_scores[i]:\n                    personal_best_scores[i] = current_score\n                    personal_best_positions[i] = positions[i]\n                \n                # Update global best\n                if current_score < global_best_score:\n                    global_best_score = current_score\n                    global_best_position = positions[i]\n\n            # Update velocities and positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                r1, r2 = np.random.rand(2, self.dim)\n                velocities[i] = (self.inertia_weight * velocities[i] +\n                                 self.cognitive_const * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.social_const * r2 * (global_best_position - positions[i]))\n                velocities[i] *= 0.95  # Enhanced velocity scaling\n                positions[i] = np.clip(positions[i] + velocities[i]*0.5, lb, ub)  # Using constriction factor\n\n            # Non-linear inertia weight decay\n            self.inertia_weight *= 0.98 ** (evaluations / self.budget)\n\n            # Adaptive cognitive and social constants\n            progress_ratio = evaluations / self.budget\n            self.cognitive_const = 1.5 - 0.5 * progress_ratio\n            self.social_const = 1.5 + 0.5 * progress_ratio\n            \n        return global_best_position", "name": "NovelSwarmOptimizer", "description": "Introduce non-linear inertia weight decay and enhanced velocity scaling for improved convergence.", "configspace": "", "generation": 30, "fitness": 0.13286066609759528, "feedback": "The algorithm NovelSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.13 with standard deviation 0.00.", "error": "", "parent_id": "266f962a-f6a1-438d-93a9-de840b858102", "metadata": {"aucs": [0.13372359217245555, 0.13327241905378628, 0.131585987066544]}, "mutation_prompt": null}
{"id": "9d9daae8-4ee7-41f3-b628-54c9236b7909", "solution": "import numpy as np\n\nclass NovelSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particle_count = min(30, budget // dim)\n        self.inertia_weight = 0.729  # inertia weight\n        self.cognitive_const = 1.49445  # cognitive constant\n        self.social_const = 1.49445  # social constant\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        # Initialize particles' positions and velocities\n        positions = np.random.uniform(lb, ub, (self.particle_count, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particle_count, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.particle_count, np.inf)\n\n        global_best_position = None\n        global_best_score = np.inf\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            # Evaluate current positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                current_score = func(positions[i])\n                evaluations += 1\n                \n                # Update personal best\n                if current_score < personal_best_scores[i]:\n                    personal_best_scores[i] = current_score\n                    personal_best_positions[i] = positions[i]\n                \n                # Update global best\n                if current_score < global_best_score:\n                    global_best_score = current_score\n                    global_best_position = positions[i]\n\n            # Update velocities and positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                r1, r2 = np.random.rand(2, self.dim)\n                velocities[i] = (self.inertia_weight * velocities[i] +\n                                 self.cognitive_const * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.social_const * r2 * (global_best_position - positions[i]))\n                velocities[i] *= 0.95 - 0.05 * (evaluations / self.budget)  # Fine-tuned adaptive velocity scaling\n                positions[i] = np.clip(positions[i] + velocities[i]*0.5, lb, ub)  # Using constriction factor\n\n            # Adaptive inertia weight decay\n            self.inertia_weight *= 0.985  # Fine-tuned decay\n\n            # Adaptive cognitive and social constants\n            progress_ratio = evaluations / self.budget\n            self.cognitive_const = 1.5 - 0.5 * progress_ratio\n            self.social_const = 1.5 + 0.5 * progress_ratio\n            \n        return global_best_position", "name": "NovelSwarmOptimizer", "description": "Enhance convergence by fine-tuning velocity scaling and decay factors based on budget usage.", "configspace": "", "generation": 31, "fitness": 0.13296228166181467, "feedback": "The algorithm NovelSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.13 with standard deviation 0.00.", "error": "", "parent_id": "266f962a-f6a1-438d-93a9-de840b858102", "metadata": {"aucs": [0.13338292820112974, 0.13380077352173414, 0.13170314326258015]}, "mutation_prompt": null}
{"id": "121e3de3-2c5f-43a2-84e9-99ed8fdeb37c", "solution": "import numpy as np\n\nclass NovelSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particle_count = min(30, budget // dim)\n        self.inertia_weight = 0.729  # inertia weight\n        self.cognitive_const = 1.49445  # cognitive constant\n        self.social_const = 1.49445  # social constant\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        # Initialize particles' positions and velocities\n        positions = np.random.uniform(lb, ub, (self.particle_count, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particle_count, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.particle_count, np.inf)\n\n        global_best_position = None\n        global_best_score = np.inf\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            # Evaluate current positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                current_score = func(positions[i])\n                evaluations += 1\n                \n                # Update personal best\n                if current_score < personal_best_scores[i]:\n                    personal_best_scores[i] = current_score\n                    personal_best_positions[i] = positions[i]\n                \n                # Update global best\n                if current_score < global_best_score:\n                    global_best_score = current_score\n                    global_best_position = positions[i]\n\n            # Update velocities and positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                r1, r2 = np.random.rand(2, self.dim)\n                velocities[i] = (self.inertia_weight * velocities[i] +\n                                 self.cognitive_const * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.social_const * r2 * (global_best_position - positions[i]))\n                velocities[i] *= np.tanh(0.9)  # Nonlinear damping to velocities\n                positions[i] = np.clip(positions[i] + velocities[i]*0.5, lb, ub)  # Using constriction factor\n                # Introduce small perturbation for diversity\n                positions[i] += np.random.normal(0, 0.01, self.dim) * progress_ratio\n\n            # Adaptive inertia weight decay\n            self.inertia_weight *= 0.99\n\n            # Adaptive cognitive and social constants\n            progress_ratio = evaluations / self.budget\n            self.cognitive_const = 1.5 - 0.5 * progress_ratio\n            self.social_const = 1.5 + 0.5 * progress_ratio\n            \n        return global_best_position", "name": "NovelSwarmOptimizer", "description": "Introduced nonlinear damping to velocities and enhanced global search via perturbation.", "configspace": "", "generation": 32, "fitness": -Infinity, "feedback": "An exception occurred: UnboundLocalError(\"local variable 'progress_ratio' referenced before assignment\").", "error": "UnboundLocalError(\"local variable 'progress_ratio' referenced before assignment\")", "parent_id": "266f962a-f6a1-438d-93a9-de840b858102", "metadata": {}, "mutation_prompt": null}
{"id": "a06fc690-69b5-4b75-bce9-c05106afb33e", "solution": "import numpy as np\n\nclass NovelSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particle_count = min(30, budget // dim)\n        self.inertia_weight = 0.729  # inertia weight\n        self.cognitive_const = 1.49445  # cognitive constant\n        self.social_const = 1.49445  # social constant\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        positions = np.random.uniform(lb, ub, (self.particle_count, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particle_count, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.particle_count, np.inf)\n\n        global_best_position = None\n        global_best_score = np.inf\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                current_score = func(positions[i])\n                evaluations += 1\n                \n                if current_score < personal_best_scores[i]:\n                    personal_best_scores[i] = current_score\n                    personal_best_positions[i] = positions[i]\n                \n                if current_score < global_best_score:\n                    global_best_score = current_score\n                    global_best_position = positions[i]\n\n            global_neighbors = np.argsort(personal_best_scores)[:self.particle_count // 2] # Adaptive neighborhood\n            local_best_position = np.mean(personal_best_positions[global_neighbors], axis=0) # Memory component\n\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                r1, r2, r3 = np.random.rand(3, self.dim) # Adding r3 for memory component\n                velocities[i] = (self.inertia_weight * velocities[i] +\n                                 self.cognitive_const * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.social_const * r2 * (global_best_position - positions[i]) +\n                                 0.5 * r3 * (local_best_position - positions[i])) # Memory influence\n                velocities[i] *= 0.9\n                positions[i] = np.clip(positions[i] + velocities[i]*0.5, lb, ub)\n\n            self.inertia_weight *= 0.99\n            progress_ratio = evaluations / self.budget\n            self.cognitive_const = 1.5 - 0.5 * progress_ratio\n            self.social_const = 1.5 + 0.5 * progress_ratio\n            \n        return global_best_position", "name": "NovelSwarmOptimizer", "description": "Improved convergence by incorporating a memory-based velocity component and adaptive neighborhood selection.", "configspace": "", "generation": 33, "fitness": 0.13273280635963045, "feedback": "The algorithm NovelSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.13 with standard deviation 0.00.", "error": "", "parent_id": "266f962a-f6a1-438d-93a9-de840b858102", "metadata": {"aucs": [0.13316585703062556, 0.1330012680527798, 0.13203129399548597]}, "mutation_prompt": null}
{"id": "2bf0eec9-3d06-4486-be43-043631da2895", "solution": "import numpy as np\n\nclass NovelSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particle_count = min(30, budget // dim)\n        self.inertia_weight = 0.729  # inertia weight\n        self.cognitive_const = 1.49445  # cognitive constant\n        self.social_const = 1.49445  # social constant\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        # Initialize particles' positions and velocities\n        positions = np.random.uniform(lb, ub, (self.particle_count, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particle_count, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.particle_count, np.inf)\n\n        global_best_position = None\n        global_best_score = np.inf\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            # Evaluate current positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                current_score = func(positions[i])\n                evaluations += 1\n                \n                # Update personal best\n                if current_score < personal_best_scores[i]:\n                    personal_best_scores[i] = current_score\n                    personal_best_positions[i] = positions[i]\n                \n                # Update global best\n                if current_score < global_best_score:\n                    global_best_score = current_score\n                    global_best_position = positions[i]\n\n            # Update velocities and positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                r1, r2 = np.random.rand(2, self.dim)\n                velocities[i] = (self.inertia_weight * velocities[i] +\n                                 self.cognitive_const * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.social_const * r2 * (global_best_position - positions[i]))\n                velocities[i] *= 0.9 + 0.1 * (global_best_score < np.inf)  # Adaptive velocity scaling\n                positions[i] = np.clip(positions[i] + velocities[i]*0.5, lb, ub)  # Using constriction factor\n\n            # Adaptive inertia weight decay\n            self.inertia_weight *= 0.99\n\n            # Adaptive cognitive and social constants\n            progress_ratio = evaluations / self.budget\n            self.cognitive_const = 1.5 - 0.5 * progress_ratio\n            self.social_const = 1.5 + 0.5 * progress_ratio\n            \n        return global_best_position", "name": "NovelSwarmOptimizer", "description": "Dynamically adjust velocity scaling for better convergence by switching from fixed to adaptive scaling based on performance. ", "configspace": "", "generation": 34, "fitness": 0.13202582256025094, "feedback": "The algorithm NovelSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.13 with standard deviation 0.00.", "error": "", "parent_id": "266f962a-f6a1-438d-93a9-de840b858102", "metadata": {"aucs": [0.13318472520325353, 0.13263767762446865, 0.13025506485303062]}, "mutation_prompt": null}
{"id": "2fa6817f-c721-4240-99f1-fdf30e5a4a6f", "solution": "import numpy as np\n\nclass NovelSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particle_count = min(30, budget // dim)\n        self.inertia_weight = 0.729  # inertia weight\n        self.cognitive_const = 1.49445  # cognitive constant\n        self.social_const = 1.49445  # social constant\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        # Initialize particles' positions and velocities\n        positions = np.random.uniform(lb, ub, (self.particle_count, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particle_count, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.particle_count, np.inf)\n\n        global_best_position = None\n        global_best_score = np.inf\n\n        evaluations = 0\n        global_best_history = []\n\n        while evaluations < self.budget:\n            # Evaluate current positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                current_score = func(positions[i])\n                evaluations += 1\n                \n                # Update personal best\n                if current_score < personal_best_scores[i]:\n                    personal_best_scores[i] = current_score\n                    personal_best_positions[i] = positions[i]\n                \n                # Update global best\n                if current_score < global_best_score:\n                    global_best_score = current_score\n                    global_best_position = positions[i]\n                    global_best_history.append(global_best_position)\n\n            # Update velocities and positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                r1, r2 = np.random.rand(2, self.dim)\n                memory_influence = np.mean(global_best_history[-min(len(global_best_history), 5):], axis=0)\n                velocities[i] = (self.inertia_weight * velocities[i] +\n                                 self.cognitive_const * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.social_const * r2 * (memory_influence - positions[i]))\n                velocities[i] *= 0.9  # Adaptive velocity scaling\n                positions[i] = np.clip(positions[i] + velocities[i]*0.5, lb, ub)  # Using constriction factor\n\n            # Adaptive inertia weight decay\n            self.inertia_weight *= 0.99\n\n            # Adaptive cognitive and social constants\n            progress_ratio = evaluations / self.budget\n            self.cognitive_const = 1.5 - 0.5 * progress_ratio\n            self.social_const = 1.5 + 0.5 * progress_ratio\n            \n        return global_best_position", "name": "NovelSwarmOptimizer", "description": "Enhanced swarm communication by introducing a memory-based adjustment to the global best influence on particle velocities.", "configspace": "", "generation": 35, "fitness": 0.13364883671088731, "feedback": "The algorithm NovelSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.13 with standard deviation 0.00.", "error": "", "parent_id": "266f962a-f6a1-438d-93a9-de840b858102", "metadata": {"aucs": [0.13310227853440493, 0.13497095448903695, 0.13287327710922003]}, "mutation_prompt": null}
{"id": "167ffa2b-61cc-4383-b330-1c46309df468", "solution": "import numpy as np\n\nclass NovelSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particle_count = min(30, budget // dim)\n        self.inertia_weight = 0.729  # inertia weight\n        self.cognitive_const = 1.49445  # cognitive constant\n        self.social_const = 1.49445  # social constant\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        # Initialize particles' positions and velocities\n        positions = np.random.uniform(lb, ub, (self.particle_count, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particle_count, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.particle_count, np.inf)\n\n        global_best_position = None\n        global_best_score = np.inf\n\n        evaluations = 0\n        global_best_history = []\n\n        while evaluations < self.budget:\n            # Evaluate current positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                current_score = func(positions[i])\n                evaluations += 1\n                \n                # Update personal best\n                if current_score < personal_best_scores[i]:\n                    personal_best_scores[i] = current_score\n                    personal_best_positions[i] = positions[i]\n                \n                # Update global best\n                if current_score < global_best_score:\n                    global_best_score = current_score\n                    global_best_position = positions[i]\n                    global_best_history.append(global_best_position)\n\n            # Update velocities and positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                r1, r2 = np.random.rand(2, self.dim)\n                memory_influence = np.mean(global_best_history[-min(len(global_best_history), 5):], axis=0)\n                velocities[i] = (self.inertia_weight * velocities[i] +\n                                 self.cognitive_const * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.social_const * r2 * (memory_influence - positions[i]))\n                velocities[i] *= 0.8  # Enhanced adaptive velocity scaling\n                positions[i] = np.clip(positions[i] + velocities[i]*0.7, lb, ub)  # Adjusted constriction factor\n\n            # Adaptive inertia weight decay\n            self.inertia_weight *= 0.99\n\n            # Adaptive cognitive and social constants\n            progress_ratio = evaluations / self.budget\n            self.cognitive_const = 1.5 - 0.5 * progress_ratio\n            self.social_const = 1.5 + 0.5 * progress_ratio\n            \n        return global_best_position", "name": "NovelSwarmOptimizer", "description": "Introduce neighborhood exploration by enhancing velocity scaling and adaptive boundary check for improved local search.", "configspace": "", "generation": 36, "fitness": 0.13276563266989996, "feedback": "The algorithm NovelSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.13 with standard deviation 0.00.", "error": "", "parent_id": "2fa6817f-c721-4240-99f1-fdf30e5a4a6f", "metadata": {"aucs": [0.13026606328990487, 0.13430823737419584, 0.13372259734559921]}, "mutation_prompt": null}
{"id": "4a2255d2-4e67-4e9d-a934-83c420e99d6c", "solution": "import numpy as np\n\nclass NovelSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particle_count = min(30, budget // dim)\n        self.inertia_weight = 0.729  # inertia weight\n        self.cognitive_const = 1.49445  # cognitive constant\n        self.social_const = 1.49445  # social constant\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        # Initialize particles' positions and velocities\n        positions = np.random.uniform(lb, ub, (self.particle_count, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particle_count, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.particle_count, np.inf)\n\n        global_best_position = None\n        global_best_score = np.inf\n\n        evaluations = 0\n        global_best_history = []\n\n        while evaluations < self.budget:\n            # Evaluate current positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                current_score = func(positions[i])\n                evaluations += 1\n                \n                # Update personal best\n                if current_score < personal_best_scores[i]:\n                    personal_best_scores[i] = current_score\n                    personal_best_positions[i] = positions[i]\n                \n                # Update global best\n                if current_score < global_best_score:\n                    global_best_score = current_score\n                    global_best_position = positions[i]\n                    global_best_history.append(global_best_position)\n\n            # Update velocities and positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                r1, r2, r3 = np.random.rand(3, self.dim)  # Added r3 for dynamic inertia\n                memory_influence = np.mean(global_best_history[-min(len(global_best_history), 5):], axis=0)\n                velocities[i] = (self.inertia_weight * velocities[i] +\n                                 self.cognitive_const * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.social_const * r2 * (memory_influence - positions[i]) +\n                                 r3 * (positions[i] - np.mean(personal_best_positions, axis=0)))  # New term for adaptive influence\n                velocities[i] *= 0.9  # Adaptive velocity scaling\n                positions[i] = np.clip(positions[i] + velocities[i]*0.5, lb, ub)  # Using constriction factor\n\n            # Adaptive inertia weight decay\n            self.inertia_weight *= 0.99\n\n            # Adaptive cognitive and social constants\n            progress_ratio = evaluations / self.budget\n            self.cognitive_const = 1.5 - 0.5 * progress_ratio\n            self.social_const = 1.5 + 0.5 * progress_ratio\n            \n        return global_best_position", "name": "NovelSwarmOptimizer", "description": "Improved velocity adjustment by introducing a dynamic inertia weight and an adaptive memory-based personal influence.", "configspace": "", "generation": 37, "fitness": 0.1294578679799656, "feedback": "The algorithm NovelSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.13 with standard deviation 0.00.", "error": "", "parent_id": "2fa6817f-c721-4240-99f1-fdf30e5a4a6f", "metadata": {"aucs": [0.1303668106023984, 0.12894584462747183, 0.1290609487100266]}, "mutation_prompt": null}
{"id": "9bc998d8-8ed8-4710-8189-da1e14aa5f1b", "solution": "import numpy as np\n\nclass NovelSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particle_count = min(30, budget // dim)\n        self.inertia_weight = 0.729  # inertia weight\n        self.cognitive_const = 1.49445  # cognitive constant\n        self.social_const = 1.49445  # social constant\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        # Initialize particles' positions and velocities\n        positions = np.random.uniform(lb, ub, (self.particle_count, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particle_count, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.particle_count, np.inf)\n\n        global_best_position = None\n        global_best_score = np.inf\n\n        evaluations = 0\n        global_best_history = []\n\n        while evaluations < self.budget:\n            # Evaluate current positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                current_score = func(positions[i])\n                evaluations += 1\n                \n                # Update personal best\n                if current_score < personal_best_scores[i]:\n                    personal_best_scores[i] = current_score\n                    personal_best_positions[i] = positions[i]\n                \n                # Update global best\n                if current_score < global_best_score:\n                    global_best_score = current_score\n                    global_best_position = positions[i]\n                    global_best_history.append(global_best_position)\n\n            # Update velocities and positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                r1, r2 = np.random.rand(2, self.dim)\n                memory_influence = np.mean(global_best_history[-min(len(global_best_history), 5):], axis=0) * 0.95  # Added decay factor\n                velocities[i] = (self.inertia_weight * velocities[i] +\n                                 self.cognitive_const * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.social_const * r2 * (memory_influence - positions[i]))\n                velocities[i] *= 0.9  # Adaptive velocity scaling\n                positions[i] = np.clip(positions[i] + velocities[i]*0.5, lb, ub)  # Using constriction factor\n\n            # Adaptive inertia weight decay\n            self.inertia_weight *= 0.99\n\n            # Adaptive cognitive and social constants\n            progress_ratio = evaluations / self.budget\n            self.cognitive_const = 1.5 - 0.5 * progress_ratio\n            self.social_const = 1.5 + 0.5 * progress_ratio\n            \n        return global_best_position", "name": "NovelSwarmOptimizer", "description": "Introduced a decay factor to further adjust the memory influence for enhanced convergence control.", "configspace": "", "generation": 38, "fitness": 0.13251008447047483, "feedback": "The algorithm NovelSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.13 with standard deviation 0.00.", "error": "", "parent_id": "2fa6817f-c721-4240-99f1-fdf30e5a4a6f", "metadata": {"aucs": [0.13288095605843708, 0.1331300363760458, 0.13151926097694155]}, "mutation_prompt": null}
{"id": "56d695d8-79b2-44f0-bdad-7eed0f5c8d44", "solution": "import numpy as np\n\nclass NovelSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particle_count = min(30, budget // dim)\n        self.inertia_weight = 0.729  # inertia weight\n        self.cognitive_const = 1.49445  # cognitive constant\n        self.social_const = 1.49445  # social constant\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        # Initialize particles' positions and velocities\n        positions = np.random.uniform(lb, ub, (self.particle_count, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particle_count, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.particle_count, np.inf)\n\n        global_best_position = None\n        global_best_score = np.inf\n\n        evaluations = 0\n        global_best_history = []\n\n        while evaluations < self.budget:\n            # Evaluate current positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                current_score = func(positions[i])\n                evaluations += 1\n                \n                # Update personal best\n                if current_score < personal_best_scores[i]:\n                    personal_best_scores[i] = current_score\n                    personal_best_positions[i] = positions[i]\n                \n                # Update global best\n                if current_score < global_best_score:\n                    global_best_score = current_score\n                    global_best_position = positions[i]\n                    global_best_history.append(global_best_position)\n\n            # Update velocities and positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                r1, r2 = np.random.rand(2, self.dim)\n                memory_influence = np.mean(global_best_history[-min(len(global_best_history), 5):], axis=0)\n                velocities[i] = (self.inertia_weight * velocities[i] +\n                                 self.cognitive_const * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.social_const * r2 * (memory_influence - positions[i]))\n                velocities[i] *= 0.95  # Adaptive velocity scaling with a refined factor\n                positions[i] = np.clip(positions[i] + velocities[i], lb, ub)  # Refined position update\n\n            # Adaptive inertia weight decay\n            self.inertia_weight *= 0.98  # Adjusted decay for quicker adaptation\n\n            # Adaptive cognitive and social constants\n            progress_ratio = evaluations / self.budget\n            self.cognitive_const = 1.5 - 0.5 * progress_ratio\n            self.social_const = 1.5 + 0.5 * progress_ratio\n            \n        return global_best_position", "name": "NovelSwarmOptimizer", "description": "Introduce adaptive memory decay and refined position update strategy for improved convergence.", "configspace": "", "generation": 39, "fitness": 0.13314374428784714, "feedback": "The algorithm NovelSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.13 with standard deviation 0.00.", "error": "", "parent_id": "2fa6817f-c721-4240-99f1-fdf30e5a4a6f", "metadata": {"aucs": [0.133262527219567, 0.13534325121568203, 0.1308254544282924]}, "mutation_prompt": null}
{"id": "1484c3f9-e6d9-4faf-84f5-501a3f974676", "solution": "import numpy as np\n\nclass NovelSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particle_count = min(30, budget // dim)\n        self.inertia_weight = 0.729  # inertia weight\n        self.cognitive_const = 1.49445  # cognitive constant\n        self.social_const = 1.49445  # social constant\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        # Initialize particles' positions and velocities\n        positions = np.random.uniform(lb, ub, (self.particle_count, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particle_count, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.particle_count, np.inf)\n\n        global_best_position = None\n        global_best_score = np.inf\n\n        evaluations = 0\n        global_best_history = []\n\n        while evaluations < self.budget:\n            # Evaluate current positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                current_score = func(positions[i])\n                evaluations += 1\n                \n                # Update personal best\n                if current_score < personal_best_scores[i]:\n                    personal_best_scores[i] = current_score\n                    personal_best_positions[i] = positions[i]\n                \n                # Update global best\n                if current_score < global_best_score:\n                    global_best_score = current_score\n                    global_best_position = positions[i]\n                    global_best_history.append(global_best_position)\n\n            # Update velocities and positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                r1, r2 = np.random.rand(2, self.dim)\n                memory_influence = np.mean(global_best_history[-min(len(global_best_history), 5):], axis=0)\n                feedback_factor = np.tanh((global_best_score - current_score) / (global_best_score + 1e-10))\n                velocities[i] = (self.inertia_weight * velocities[i] +\n                                 self.cognitive_const * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.social_const * r2 * (memory_influence - positions[i]) + feedback_factor)\n                velocities[i] *= 0.95  # More adaptive velocity scaling\n                positions[i] = np.clip(positions[i] + velocities[i]*0.5, lb, ub)  # Using constriction factor\n\n            # Adaptive inertia weight decay\n            self.inertia_weight *= 0.99\n\n            # Adaptive cognitive and social constants\n            progress_ratio = evaluations / self.budget\n            self.cognitive_const = 1.5 - 0.5 * progress_ratio\n            self.social_const = 1.5 + 0.5 * progress_ratio\n            \n        return global_best_position", "name": "NovelSwarmOptimizer", "description": "Enhanced memory influence by introducing a feedback mechanism and a more adaptive velocity scaling.", "configspace": "", "generation": 40, "fitness": 0.133662456104615, "feedback": "The algorithm NovelSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.13 with standard deviation 0.00.", "error": "", "parent_id": "2fa6817f-c721-4240-99f1-fdf30e5a4a6f", "metadata": {"aucs": [0.13259752642713996, 0.1348871881800897, 0.1335026537066153]}, "mutation_prompt": null}
{"id": "b6c4cb82-037e-402e-b3f2-5919bfd43251", "solution": "import numpy as np\n\nclass NovelSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particle_count = min(30, budget // dim)\n        self.inertia_weight = 0.729  # inertia weight\n        self.cognitive_const = 1.49445  # cognitive constant\n        self.social_const = 1.49445  # social constant\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        # Initialize particles' positions and velocities\n        positions = np.random.uniform(lb, ub, (self.particle_count, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particle_count, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.particle_count, np.inf)\n\n        global_best_position = None\n        global_best_score = np.inf\n\n        evaluations = 0\n        global_best_history = []\n\n        while evaluations < self.budget:\n            # Evaluate current positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                current_score = func(positions[i])\n                evaluations += 1\n                \n                # Update personal best\n                if current_score < personal_best_scores[i]:\n                    personal_best_scores[i] = current_score\n                    personal_best_positions[i] = positions[i]\n                \n                # Update global best\n                if current_score < global_best_score:\n                    global_best_score = current_score\n                    global_best_position = positions[i]\n                    global_best_history.append(global_best_position)\n\n            # Update velocities and positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                r1, r2 = np.random.rand(2, self.dim)\n                memory_window = max(5, len(global_best_history) // 4)\n                memory_influence = np.mean(global_best_history[-memory_window:], axis=0)\n                feedback_factor = np.tanh((global_best_score - current_score) / (global_best_score + 1e-10)) * (0.95 ** evaluations)\n                velocities[i] = (self.inertia_weight * velocities[i] +\n                                 self.cognitive_const * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.social_const * r2 * (memory_influence - positions[i]) + feedback_factor)\n                velocities[i] *= 0.95  # More adaptive velocity scaling\n                positions[i] = np.clip(positions[i] + velocities[i]*0.5, lb, ub)  # Using constriction factor\n\n            # Adaptive inertia weight decay\n            self.inertia_weight *= 0.99\n\n            # Adaptive cognitive and social constants\n            progress_ratio = evaluations / self.budget\n            self.cognitive_const = 1.5 - 0.5 * progress_ratio\n            self.social_const = 1.5 + 0.5 * progress_ratio\n            \n        return global_best_position", "name": "NovelSwarmOptimizer", "description": "Introduced a decaying feedback mechanism and a dynamic memory window to enhance adaptability and convergence speed.", "configspace": "", "generation": 41, "fitness": 0.13369114256805126, "feedback": "The algorithm NovelSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.13 with standard deviation 0.00.", "error": "", "parent_id": "1484c3f9-e6d9-4faf-84f5-501a3f974676", "metadata": {"aucs": [0.13317976539493148, 0.13469238736393818, 0.1332012749452841]}, "mutation_prompt": null}
{"id": "b71c4acd-e9e8-47d3-b0e2-cdcb718547e9", "solution": "import numpy as np\n\nclass NovelSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particle_count = min(30, budget // dim)\n        self.inertia_weight = 0.729  # inertia weight\n        self.cognitive_const = 1.49445  # cognitive constant\n        self.social_const = 1.49445  # social constant\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        # Initialize particles' positions and velocities\n        positions = np.random.uniform(lb, ub, (self.particle_count, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particle_count, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.particle_count, np.inf)\n\n        global_best_position = None\n        global_best_score = np.inf\n\n        evaluations = 0\n        global_best_history = []\n\n        while evaluations < self.budget:\n            # Evaluate current positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                current_score = func(positions[i])\n                evaluations += 1\n                \n                # Update personal best\n                if current_score < personal_best_scores[i]:\n                    personal_best_scores[i] = current_score\n                    personal_best_positions[i] = positions[i]\n                \n                # Update global best\n                if current_score < global_best_score:\n                    global_best_score = current_score\n                    global_best_position = positions[i]\n                    global_best_history.append(global_best_position)\n\n            # Update velocities and positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                r1, r2 = np.random.rand(2, self.dim)\n                memory_window = max(5, len(global_best_history) // 4)\n                memory_influence = np.mean(global_best_history[-memory_window:], axis=0)\n                feedback_factor = np.tanh((global_best_score - current_score) / (global_best_score + 1e-10)) * (0.95 ** evaluations)\n                velocities[i] = (self.inertia_weight * velocities[i] +\n                                 self.cognitive_const * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.social_const * r2 * (memory_influence - positions[i]) + feedback_factor)\n                velocities[i] *= np.exp(-0.01 * evaluations)  # More adaptive velocity scaling\n                positions[i] = np.clip(positions[i] + np.sin(velocities[i]*0.5), lb, ub)  # Using nonlinear update\n\n            # Adaptive inertia weight decay\n            self.inertia_weight *= 0.99\n\n            # Adaptive cognitive and social constants\n            progress_ratio = evaluations / self.budget\n            self.cognitive_const = 1.5 - 0.5 * progress_ratio\n            self.social_const = 1.5 + 0.5 * progress_ratio\n            \n        return global_best_position", "name": "NovelSwarmOptimizer", "description": "Enhanced convergence by introducing nonlinear position updates and adaptive inertia scaling.", "configspace": "", "generation": 42, "fitness": 0.1217972101700286, "feedback": "The algorithm NovelSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.12 with standard deviation 0.01.", "error": "", "parent_id": "b6c4cb82-037e-402e-b3f2-5919bfd43251", "metadata": {"aucs": [0.1256461201257144, 0.12757259873356552, 0.1121729116508059]}, "mutation_prompt": null}
{"id": "69a96122-8ede-4de9-bea5-ac0e8239d6de", "solution": "import numpy as np\n\nclass NovelSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particle_count = min(30, budget // dim)\n        self.inertia_weight = 0.729  # inertia weight\n        self.cognitive_const = 1.49445  # cognitive constant\n        self.social_const = 1.49445  # social constant\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        # Initialize particles' positions and velocities\n        positions = np.random.uniform(lb, ub, (self.particle_count, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particle_count, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.particle_count, np.inf)\n\n        global_best_position = None\n        global_best_score = np.inf\n\n        evaluations = 0\n        global_best_history = []\n\n        while evaluations < self.budget:\n            # Evaluate current positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                current_score = func(positions[i])\n                evaluations += 1\n                \n                # Update personal best\n                if current_score < personal_best_scores[i]:\n                    personal_best_scores[i] = current_score\n                    personal_best_positions[i] = positions[i]\n                \n                # Update global best\n                if current_score < global_best_score:\n                    global_best_score = current_score\n                    global_best_position = positions[i]\n                    global_best_history.append(global_best_position)\n\n            # Update velocities and positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                r1, r2 = np.random.rand(2, self.dim)\n                memory_window = max(5, len(global_best_history) // 3)  # Changed memory influence\n                memory_influence = np.mean(global_best_history[-memory_window:], axis=0)\n                feedback_factor = np.tanh((global_best_score - current_score) / (global_best_score + 1e-10)) * (0.95 ** evaluations)\n                velocities[i] = (self.inertia_weight * velocities[i] +\n                                 self.cognitive_const * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.social_const * r2 * (memory_influence - positions[i]) + feedback_factor)\n                velocities[i] *= 0.95  # More adaptive velocity scaling\n                positions[i] = np.clip(positions[i] + velocities[i]*0.5, lb, ub)  # Using constriction factor\n\n            # Adaptive inertia weight decay\n            self.inertia_weight *= 0.99 * (1 - 0.9 * (evaluations / self.budget))  # Nonlinear decay\n\n            # Adaptive cognitive and social constants\n            progress_ratio = evaluations / self.budget\n            self.cognitive_const = 1.5 - 0.5 * progress_ratio\n            self.social_const = 1.5 + 0.5 * progress_ratio\n            \n        return global_best_position", "name": "NovelSwarmOptimizer", "description": "Introduced nonlinear decay to inertia weight and increased memory influence for better exploration-exploitation balance.", "configspace": "", "generation": 43, "fitness": 0.13164445804645886, "feedback": "The algorithm NovelSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.13 with standard deviation 0.00.", "error": "", "parent_id": "b6c4cb82-037e-402e-b3f2-5919bfd43251", "metadata": {"aucs": [0.12766442881959505, 0.13526091954444197, 0.13200802577533954]}, "mutation_prompt": null}
{"id": "8b0b34cb-329d-4e77-b76d-fde21c9173cb", "solution": "import numpy as np\n\nclass NovelSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particle_count = min(30, budget // dim)\n        self.inertia_weight = 0.729  # inertia weight\n        self.cognitive_const = 1.49445  # cognitive constant\n        self.social_const = 1.49445  # social constant\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        # Initialize particles' positions and velocities\n        positions = np.random.uniform(lb, ub, (self.particle_count, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particle_count, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.particle_count, np.inf)\n\n        global_best_position = None\n        global_best_score = np.inf\n\n        evaluations = 0\n        global_best_history = []\n\n        while evaluations < self.budget:\n            # Evaluate current positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                current_score = func(positions[i])\n                evaluations += 1\n                \n                # Update personal best\n                if current_score < personal_best_scores[i]:\n                    personal_best_scores[i] = current_score\n                    personal_best_positions[i] = positions[i]\n                \n                # Update global best\n                if current_score < global_best_score:\n                    global_best_score = current_score\n                    global_best_position = positions[i]\n                    global_best_history.append(global_best_position)\n\n            # Update velocities and positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                r1, r2 = np.random.rand(2, self.dim)\n                memory_window = max(5, len(global_best_history) // 4)\n                memory_influence = np.mean(global_best_history[-memory_window:], axis=0)\n                feedback_factor = np.tanh((global_best_score - current_score) / (global_best_score + 1e-10)) * (0.95 ** evaluations)\n                velocities[i] = (self.inertia_weight * velocities[i] +\n                                 self.cognitive_const * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.social_const * r2 * (memory_influence - positions[i]) + feedback_factor)\n                velocities[i] *= 0.95  # More adaptive velocity scaling\n                positions[i] = np.clip(positions[i] + velocities[i]*0.5, lb, ub)  # Using constriction factor\n\n            # Adaptive inertia weight decay\n            self.inertia_weight *= 0.995\n\n            # Adaptive cognitive and social constants\n            progress_ratio = evaluations / self.budget\n            self.cognitive_const = 1.5 - 0.5 * progress_ratio\n            self.social_const = 1.5 + 0.5 * progress_ratio\n            \n        return global_best_position", "name": "NovelSwarmOptimizer", "description": "Enhanced inertia weight adaptation for improved convergence precision.", "configspace": "", "generation": 44, "fitness": 0.13368690333185598, "feedback": "The algorithm NovelSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.13 with standard deviation 0.00.", "error": "", "parent_id": "b6c4cb82-037e-402e-b3f2-5919bfd43251", "metadata": {"aucs": [0.13323646182497806, 0.13467246222390827, 0.13315178594668164]}, "mutation_prompt": null}
{"id": "48fdfe31-f79a-46b1-853a-2c8498f4fc4e", "solution": "import numpy as np\n\nclass NovelSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particle_count = min(30, budget // dim)\n        self.inertia_weight = 0.729  # inertia weight\n        self.cognitive_const = 1.49445  # cognitive constant\n        self.social_const = 1.49445  # social constant\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        # Initialize particles' positions and velocities\n        positions = np.random.uniform(lb, ub, (self.particle_count, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particle_count, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.particle_count, np.inf)\n\n        global_best_position = None\n        global_best_score = np.inf\n\n        evaluations = 0\n        global_best_history = []\n\n        while evaluations < self.budget:\n            # Evaluate current positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                current_score = func(positions[i])\n                evaluations += 1\n                \n                # Update personal best\n                if current_score < personal_best_scores[i]:\n                    personal_best_scores[i] = current_score\n                    personal_best_positions[i] = positions[i]\n                \n                # Update global best\n                if current_score < global_best_score:\n                    global_best_score = current_score\n                    global_best_position = positions[i]\n                    global_best_history.append(global_best_position)\n\n            # Update velocities and positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                r1, r2 = np.random.rand(2, self.dim)\n                memory_window = max(5, len(global_best_history) // 4)\n                memory_influence = np.mean(global_best_history[-memory_window:], axis=0)\n                feedback_factor = np.tanh((global_best_score - current_score) / (global_best_score + 1e-10)) * (0.95 ** evaluations)\n                velocities[i] = (self.inertia_weight * velocities[i] +\n                                 self.cognitive_const * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.social_const * r2 * (memory_influence - positions[i]) + feedback_factor)\n                velocities[i] *= (0.95 - 0.45 * (evaluations / self.budget))  # Adaptive velocity damping\n                positions[i] = np.clip(positions[i] + velocities[i]*0.5, lb, ub)  # Using constriction factor\n\n            # Adaptive inertia weight decay\n            self.inertia_weight *= 0.99\n\n            # Adaptive cognitive and social constants\n            progress_ratio = evaluations / self.budget\n            self.cognitive_const = 1.5 - 0.5 * progress_ratio\n            self.social_const = 1.5 + 0.5 * progress_ratio\n            \n        return global_best_position", "name": "NovelSwarmOptimizer", "description": "Introduced adaptive velocity damping and progressive exploration-convergence ratio to balance exploration and exploitation dynamically.", "configspace": "", "generation": 45, "fitness": 0.13316626793000705, "feedback": "The algorithm NovelSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.13 with standard deviation 0.00.", "error": "", "parent_id": "b6c4cb82-037e-402e-b3f2-5919bfd43251", "metadata": {"aucs": [0.1310758396926326, 0.13503827793447798, 0.13338468616291055]}, "mutation_prompt": null}
{"id": "d8bdc455-9d77-4f18-b100-fa8447877afa", "solution": "import numpy as np\n\nclass NovelSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particle_count = min(30, budget // dim)\n        self.inertia_weight = 0.729  # inertia weight\n        self.cognitive_const = 1.49445  # cognitive constant\n        self.social_const = 1.49445  # social constant\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        # Initialize particles' positions and velocities\n        positions = np.random.uniform(lb, ub, (self.particle_count, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particle_count, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.particle_count, np.inf)\n\n        global_best_position = None\n        global_best_score = np.inf\n\n        evaluations = 0\n        global_best_history = []\n\n        while evaluations < self.budget:\n            # Evaluate current positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                current_score = func(positions[i])\n                evaluations += 1\n                \n                # Update personal best\n                if current_score < personal_best_scores[i]:\n                    personal_best_scores[i] = current_score\n                    personal_best_positions[i] = positions[i]\n                \n                # Update global best\n                if current_score < global_best_score:\n                    global_best_score = current_score\n                    global_best_position = positions[i]\n                    global_best_history.append(global_best_position)\n\n            # Update velocities and positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                r1, r2 = np.random.rand(2, self.dim)\n                memory_window = max(5, len(global_best_history) // 4)\n                memory_influence = np.mean(global_best_history[-memory_window:], axis=0)\n                feedback_factor = np.tanh((global_best_score - current_score) / (global_best_score + 1e-10)) * (0.95 ** evaluations)\n                velocities[i] = (self.inertia_weight * velocities[i] +\n                                 self.cognitive_const * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.social_const * r2 * (memory_influence - positions[i]) + feedback_factor)\n                velocities[i] *= 0.95  # More adaptive velocity scaling\n                positions[i] = np.clip(positions[i] + velocities[i]*0.5, lb, ub)  # Using constriction factor\n\n                # Stochastic reinitialization for stagnating particles\n                if np.random.rand() < 0.01:  # 1% chance\n                    positions[i] = np.random.uniform(lb, ub, self.dim)\n\n            # Adaptive inertia weight decay\n            self.inertia_weight *= 0.99\n\n            # Adaptive cognitive and social constants\n            progress_ratio = evaluations / self.budget\n            self.cognitive_const = 1.5 - 0.5 * progress_ratio\n            self.social_const = 1.5 + 0.5 * progress_ratio\n            \n        return global_best_position", "name": "NovelSwarmOptimizer", "description": "Introduced a stochastic reinitialization mechanism for particles that stagnate to enhance exploration and prevent premature convergence.", "configspace": "", "generation": 46, "fitness": 0.1323417966373572, "feedback": "The algorithm NovelSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.13 with standard deviation 0.00.", "error": "", "parent_id": "b6c4cb82-037e-402e-b3f2-5919bfd43251", "metadata": {"aucs": [0.1286119187505964, 0.13505039370554717, 0.13336307745592801]}, "mutation_prompt": null}
{"id": "8a486d1c-75b3-4495-9823-196304afd3cd", "solution": "import numpy as np\n\nclass NovelSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particle_count = min(30, budget // dim)\n        self.inertia_weight = 0.729  # inertia weight\n        self.cognitive_const = 1.49445  # cognitive constant\n        self.social_const = 1.49445  # social constant\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        # Initialize particles' positions and velocities\n        positions = np.random.uniform(lb, ub, (self.particle_count, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particle_count, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.particle_count, np.inf)\n\n        global_best_position = None\n        global_best_score = np.inf\n\n        evaluations = 0\n        global_best_history = []\n\n        while evaluations < self.budget:\n            # Evaluate current positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                current_score = func(positions[i])\n                evaluations += 1\n                \n                # Update personal best\n                if current_score < personal_best_scores[i]:\n                    personal_best_scores[i] = current_score\n                    personal_best_positions[i] = positions[i]\n                \n                # Update global best\n                if current_score < global_best_score:\n                    global_best_score = current_score\n                    global_best_position = positions[i]\n                    global_best_history.append(global_best_position)\n\n            # Update velocities and positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                r1, r2 = np.random.rand(2, self.dim)\n                memory_window = max(5, len(global_best_history) // 4)\n                memory_influence = np.mean(global_best_history[-memory_window:], axis=0)\n                feedback_factor = np.tanh((global_best_score - current_score) / (global_best_score + 1e-10)) * (0.95 ** evaluations)\n                velocities[i] = (self.inertia_weight * velocities[i] +\n                                 self.cognitive_const * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.social_const * r2 * (memory_influence - positions[i]) + feedback_factor)\n                velocities[i] *= 0.93  # More adaptive velocity scaling\n                perturbation = np.random.normal(0, 0.1, self.dim) * (1 - evaluations / self.budget)\n                positions[i] = np.clip(positions[i] + velocities[i]*0.5 + perturbation, lb, ub)  # Using constriction factor\n\n            # Adaptive inertia weight decay\n            self.inertia_weight *= 0.98\n\n            # Adaptive cognitive and social constants\n            progress_ratio = evaluations / self.budget\n            self.cognitive_const = 1.5 - 0.5 * progress_ratio\n            self.social_const = 1.5 + 0.5 * progress_ratio\n            \n        return global_best_position", "name": "NovelSwarmOptimizer", "description": "Enhanced exploration with stochastic perturbation and adaptive memory influence to improve convergence.", "configspace": "", "generation": 47, "fitness": 0.13046356755149854, "feedback": "The algorithm NovelSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.13 with standard deviation 0.00.", "error": "", "parent_id": "b6c4cb82-037e-402e-b3f2-5919bfd43251", "metadata": {"aucs": [0.12901194197609978, 0.1342108565373561, 0.12816790414103973]}, "mutation_prompt": null}
{"id": "23ba6f85-faaa-4ab7-bfe1-31360704b83e", "solution": "import numpy as np\n\nclass NovelSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particle_count = min(30, budget // dim)\n        self.inertia_weight = 0.729  # inertia weight\n        self.cognitive_const = 1.49445  # cognitive constant\n        self.social_const = 1.49445  # social constant\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        # Initialize particles' positions and velocities\n        positions = np.random.uniform(lb, ub, (self.particle_count, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particle_count, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.particle_count, np.inf)\n\n        global_best_position = None\n        global_best_score = np.inf\n\n        evaluations = 0\n        global_best_history = []\n\n        while evaluations < self.budget:\n            # Evaluate current positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                current_score = func(positions[i])\n                evaluations += 1\n                \n                # Update personal best\n                if current_score < personal_best_scores[i]:\n                    personal_best_scores[i] = current_score\n                    personal_best_positions[i] = positions[i]\n                \n                # Update global best\n                if current_score < global_best_score:\n                    global_best_score = current_score\n                    global_best_position = positions[i]\n                    global_best_history.append(global_best_position)\n\n            # Update velocities and positions\n            diversity_factor = np.std(positions, axis=0) * 0.1  # Introduce diversity factor\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                r1, r2 = np.random.rand(2, self.dim)\n                memory_window = max(5, len(global_best_history) // 4)\n                memory_influence = np.mean(global_best_history[-memory_window:], axis=0)\n                feedback_factor = np.tanh((global_best_score - current_score) / (global_best_score + 1e-10)) * (0.95 ** evaluations)\n                velocities[i] = (self.inertia_weight * velocities[i] +\n                                 self.cognitive_const * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.social_const * r2 * (memory_influence - positions[i] + diversity_factor) + feedback_factor)\n                velocities[i] *= 0.95  # More adaptive velocity scaling\n                positions[i] = np.clip(positions[i] + velocities[i]*0.5, lb, ub)  # Using constriction factor\n\n            # Adaptive inertia weight decay\n            self.inertia_weight *= 0.99\n\n            # Adaptive cognitive and social constants\n            progress_ratio = evaluations / self.budget\n            self.cognitive_const = 1.5 - 0.5 * progress_ratio\n            self.social_const = 1.5 + 0.5 * progress_ratio\n            \n        return global_best_position", "name": "NovelSwarmOptimizer", "description": "Improved convergence by enhancing the social component with diversity-driven perturbation.", "configspace": "", "generation": 48, "fitness": 0.13234372193759114, "feedback": "The algorithm NovelSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.13 with standard deviation 0.00.", "error": "", "parent_id": "b6c4cb82-037e-402e-b3f2-5919bfd43251", "metadata": {"aucs": [0.13097685415886384, 0.13479521738472555, 0.13125909426918403]}, "mutation_prompt": null}
{"id": "6eafcb91-e906-4c29-9ec6-1f75b13865d6", "solution": "import numpy as np\n\nclass NovelSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particle_count = min(30, budget // dim)\n        self.inertia_weight = 0.729  # inertia weight\n        self.cognitive_const = 1.49445  # cognitive constant\n        self.social_const = 1.49445  # social constant\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        # Initialize particles' positions and velocities\n        positions = np.random.uniform(lb, ub, (self.particle_count, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particle_count, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.particle_count, np.inf)\n\n        global_best_position = None\n        global_best_score = np.inf\n\n        evaluations = 0\n        global_best_history = []\n\n        while evaluations < self.budget:\n            # Evaluate current positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                current_score = func(positions[i])\n                evaluations += 1\n                \n                # Update personal best\n                if current_score < personal_best_scores[i]:\n                    personal_best_scores[i] = current_score\n                    personal_best_positions[i] = positions[i]\n                \n                # Update global best\n                if current_score < global_best_score:\n                    global_best_score = current_score\n                    global_best_position = positions[i]\n                    global_best_history.append(global_best_position)\n\n            # Update velocities and positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                r1, r2 = np.random.rand(2, self.dim)\n                memory_window = max(5, len(global_best_history) // 4)\n                memory_influence = np.mean(global_best_history[-memory_window:], axis=0)\n                feedback_factor = np.tanh((global_best_score - current_score) / (global_best_score + 1e-10)) * (0.95 ** evaluations)\n                velocities[i] = (self.inertia_weight * velocities[i] +\n                                 self.cognitive_const * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.social_const * r2 * (memory_influence - positions[i]) + feedback_factor)\n                velocities[i] *= 0.95  # More adaptive velocity scaling\n                positions[i] = np.clip(positions[i] + velocities[i]*0.5 + np.random.normal(0, (ub-lb)*0.01, self.dim), lb, ub)  # Diversity perturbation\n\n            # Adaptive inertia weight decay\n            self.inertia_weight *= 0.99\n\n            # Adaptive cognitive and social constants\n            progress_ratio = evaluations / self.budget\n            self.cognitive_const = 1.5 - 0.5 * progress_ratio\n            self.social_const = 1.5 + 0.5 * progress_ratio\n            \n        return global_best_position", "name": "NovelSwarmOptimizer", "description": "Improved global best position exploitation by incorporating diversity preservation through adaptive random perturbation.", "configspace": "", "generation": 49, "fitness": 0.13113860715775316, "feedback": "The algorithm NovelSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.13 with standard deviation 0.00.", "error": "", "parent_id": "b6c4cb82-037e-402e-b3f2-5919bfd43251", "metadata": {"aucs": [0.12764668808003343, 0.13401762989254884, 0.13175150350067721]}, "mutation_prompt": null}
{"id": "a4dcb681-c001-4e9b-bf6e-388591c01901", "solution": "import numpy as np\n\nclass NovelSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particle_count = min(30, budget // dim)\n        self.inertia_weight = 0.729  # inertia weight\n        self.cognitive_const = 1.49445  # cognitive constant\n        self.social_const = 1.49445  # social constant\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        # Initialize particles' positions and velocities\n        positions = np.random.uniform(lb, ub, (self.particle_count, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particle_count, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.particle_count, np.inf)\n\n        global_best_position = None\n        global_best_score = np.inf\n\n        evaluations = 0\n        global_best_history = []\n\n        while evaluations < self.budget:\n            # Evaluate current positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                current_score = func(positions[i])\n                evaluations += 1\n                \n                # Update personal best\n                if current_score < personal_best_scores[i]:\n                    personal_best_scores[i] = current_score\n                    personal_best_positions[i] = positions[i]\n                \n                # Update global best\n                if current_score < global_best_score:\n                    global_best_score = current_score\n                    global_best_position = positions[i]\n                    global_best_history.append(global_best_position)\n\n            # Update velocities and positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                r1, r2 = np.random.rand(2, self.dim)\n                memory_window = max(5, len(global_best_history) // 4)\n                memory_influence = np.mean(global_best_history[-memory_window:], axis=0)\n                feedback_factor = np.tanh((global_best_score - current_score) / (global_best_score + 1e-10)) * (0.95 ** evaluations)\n                velocities[i] = (self.inertia_weight * velocities[i] +\n                                 self.cognitive_const * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.social_const * r2 * (memory_influence - positions[i]) + feedback_factor)\n                velocities[i] *= 0.95  # More adaptive velocity scaling\n                positions[i] = np.clip(positions[i] + velocities[i]*0.5, lb, ub)  # Using constriction factor\n\n            # Adaptive inertia weight decay\n            self.inertia_weight *= 0.99\n\n            # Adaptive cognitive and social constants based on diversity\n            diversity = np.mean(np.std(positions, axis=0))\n            self.cognitive_const = 1.5 - diversity * 0.5  # Change line 1\n            self.social_const = 1.5 + diversity * 0.5    # Change line 2\n            \n        return global_best_position", "name": "NovelSwarmOptimizer", "description": "Introduced a dynamic adaptation of the cognitive and social constants based on the diversity of particles to enhance exploration and exploitation balance.", "configspace": "", "generation": 50, "fitness": 0.12676019054703028, "feedback": "The algorithm NovelSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.13 with standard deviation 0.00.", "error": "", "parent_id": "b6c4cb82-037e-402e-b3f2-5919bfd43251", "metadata": {"aucs": [0.1251271809150829, 0.1274424380380501, 0.12771095268795785]}, "mutation_prompt": null}
{"id": "aa88f854-9c33-42c8-9cf4-1efaecbfc6d3", "solution": "import numpy as np\n\nclass NovelSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particle_count = min(30, budget // dim)\n        self.inertia_weight = 0.729  # inertia weight\n        self.cognitive_const = 1.49445  # cognitive constant\n        self.social_const = 1.49445  # social constant\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        # Initialize particles' positions and velocities\n        positions = np.random.uniform(lb, ub, (self.particle_count, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particle_count, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.particle_count, np.inf)\n\n        global_best_position = None\n        global_best_score = np.inf\n\n        evaluations = 0\n        global_best_history = []\n\n        while evaluations < self.budget:\n            # Evaluate current positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                current_score = func(positions[i])\n                evaluations += 1\n                \n                # Update personal best\n                if current_score < personal_best_scores[i]:\n                    personal_best_scores[i] = current_score\n                    personal_best_positions[i] = positions[i]\n                \n                # Update global best\n                if current_score < global_best_score:\n                    global_best_score = current_score\n                    global_best_position = positions[i]\n                    global_best_history.append(global_best_position)\n\n            # Update velocities and positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                r1, r2 = np.random.rand(2, self.dim)\n                memory_window = max(5, len(global_best_history) // 4)\n                memory_influence = np.mean(global_best_history[-memory_window:], axis=0)\n                feedback_factor = np.tanh((global_best_score - current_score) / (global_best_score + 1e-10)) * (0.95 ** evaluations)\n                velocities[i] = (self.inertia_weight * velocities[i] +\n                                 self.cognitive_const * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.social_const * r2 * (memory_influence - positions[i]) + feedback_factor)\n                velocities[i] *= np.random.uniform(0.9, 1.1)  # Introduced adaptive velocity scaling\n                positions[i] = np.clip(positions[i] + velocities[i]*0.5, lb, ub)  # Using constriction factor\n\n            # Adaptive inertia weight decay\n            self.inertia_weight *= 0.99\n\n            # Adaptive cognitive and social constants\n            progress_ratio = evaluations / self.budget\n            self.cognitive_const = 1.5 - 0.5 * progress_ratio\n            self.social_const = 1.5 + 0.5 * progress_ratio\n            \n        return global_best_position", "name": "NovelSwarmOptimizer", "description": "Introduced self-adaptive learning rates for particle velocities to enhance convergence.", "configspace": "", "generation": 51, "fitness": 0.13172217611847814, "feedback": "The algorithm NovelSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.13 with standard deviation 0.00.", "error": "", "parent_id": "b6c4cb82-037e-402e-b3f2-5919bfd43251", "metadata": {"aucs": [0.12892591136142229, 0.13519662464539384, 0.1310439923486183]}, "mutation_prompt": null}
{"id": "fe24e69d-5428-410d-9e7c-c379f36164cc", "solution": "import numpy as np\n\nclass NovelSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particle_count = min(30, budget // dim)\n        self.inertia_weight = 0.729  # inertia weight\n        self.cognitive_const = 1.49445  # cognitive constant\n        self.social_const = 1.49445  # social constant\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        # Initialize particles' positions and velocities\n        positions = np.random.uniform(lb, ub, (self.particle_count, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particle_count, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.particle_count, np.inf)\n\n        global_best_position = None\n        global_best_score = np.inf\n\n        evaluations = 0\n        global_best_history = []\n\n        while evaluations < self.budget:\n            # Evaluate current positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                current_score = func(positions[i])\n                evaluations += 1\n                \n                # Update personal best\n                if current_score < personal_best_scores[i]:\n                    personal_best_scores[i] = current_score\n                    personal_best_positions[i] = positions[i]\n                \n                # Update global best\n                if current_score < global_best_score:\n                    global_best_score = current_score\n                    global_best_position = positions[i]\n                    global_best_history.append(global_best_position)\n\n            # Update velocities and positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                r1, r2 = np.random.rand(2, self.dim)\n                memory_window = max(5, len(global_best_history) // 4)\n                memory_influence = np.mean(global_best_history[-memory_window:], axis=0)\n                feedback_factor = np.tanh((global_best_score - current_score) / (global_best_score + 1e-10)) * (0.95 ** evaluations)\n                velocities[i] = (self.inertia_weight * velocities[i] +\n                                 self.cognitive_const * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.social_const * r2 * (memory_influence - positions[i]) + feedback_factor)\n                velocities[i] = 0.9 * velocities[i]  # Added momentum term for velocity\n                positions[i] = np.clip(positions[i] + velocities[i]*0.5, lb, ub)  # Using constriction factor\n\n            # Adaptive inertia weight decay\n            self.inertia_weight *= 0.99\n\n            # Adaptive cognitive and social constants\n            progress_ratio = evaluations / self.budget\n            self.cognitive_const = 1.5 - 0.5 * progress_ratio\n            self.social_const = 1.5 + 0.5 * progress_ratio\n            \n        return global_best_position", "name": "NovelSwarmOptimizer", "description": "Enhanced the velocity update rule with a momentum term to improve convergence speed.", "configspace": "", "generation": 52, "fitness": 0.13365742523098315, "feedback": "The algorithm NovelSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.13 with standard deviation 0.00.", "error": "", "parent_id": "b6c4cb82-037e-402e-b3f2-5919bfd43251", "metadata": {"aucs": [0.13309674844946617, 0.13497513865967414, 0.13290038858380915]}, "mutation_prompt": null}
{"id": "093a78c3-345c-4c41-8880-dfdfe544a3f8", "solution": "import numpy as np\n\nclass NovelSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particle_count = min(30, budget // dim)\n        self.inertia_weight = 0.729  # inertia weight\n        self.cognitive_const = 1.49445  # cognitive constant\n        self.social_const = 1.49445  # social constant\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        # Initialize particles' positions and velocities\n        positions = np.random.uniform(lb, ub, (self.particle_count, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particle_count, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.particle_count, np.inf)\n\n        global_best_position = None\n        global_best_score = np.inf\n\n        evaluations = 0\n        global_best_history = []\n\n        while evaluations < self.budget:\n            # Evaluate current positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                current_score = func(positions[i])\n                evaluations += 1\n                \n                # Update personal best\n                if current_score < personal_best_scores[i]:\n                    personal_best_scores[i] = current_score\n                    personal_best_positions[i] = positions[i]\n                \n                # Update global best\n                if current_score < global_best_score:\n                    global_best_score = current_score\n                    global_best_position = positions[i]\n                    global_best_history.append(global_best_position)\n\n            # Update velocities and positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                r1, r2 = np.random.rand(2, self.dim)\n                memory_window = max(5, len(global_best_history) // 4)\n                memory_influence = np.mean(global_best_history[-memory_window:], axis=0)\n                feedback_factor = np.tanh((global_best_score - current_score) / (global_best_score + 1e-10)) * (0.95 ** evaluations)\n                velocities[i] = (self.inertia_weight * velocities[i] +\n                                 self.cognitive_const * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.social_const * r2 * (memory_influence - positions[i]) + feedback_factor)\n                velocities[i] *= 0.95  # More adaptive velocity scaling\n                positions[i] = np.clip(positions[i] + velocities[i]*0.5, lb, ub)  # Using constriction factor\n\n            # Adaptive inertia weight decay\n            self.inertia_weight *= 0.99\n\n            # Adaptive cognitive and social constants\n            progress_ratio = evaluations / self.budget\n            diversity = np.mean(np.std(positions, axis=0))\n            self.cognitive_const = 1.5 - 0.5 * progress_ratio\n            self.social_const = 1.5 + 0.5 * progress_ratio * (1 + diversity)\n            \n        return global_best_position", "name": "NovelSwarmOptimizer", "description": "Introduced a dynamic social constant adjustment strategy based on diversity to enhance exploration capabilities.", "configspace": "", "generation": 53, "fitness": 0.13213014285527225, "feedback": "The algorithm NovelSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.13 with standard deviation 0.00.", "error": "", "parent_id": "b6c4cb82-037e-402e-b3f2-5919bfd43251", "metadata": {"aucs": [0.13117316763885845, 0.13276918492104828, 0.13244807600591002]}, "mutation_prompt": null}
{"id": "796522a6-ef48-4a7e-a04e-3671a89619a4", "solution": "import numpy as np\n\nclass NovelSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particle_count = min(30, budget // dim)\n        self.inertia_weight = 0.729  # inertia weight\n        self.cognitive_const = 1.49445  # cognitive constant\n        self.social_const = 1.49445  # social constant\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        # Initialize particles' positions and velocities\n        positions = np.random.uniform(lb, ub, (self.particle_count, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particle_count, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.particle_count, np.inf)\n\n        global_best_position = None\n        global_best_score = np.inf\n\n        evaluations = 0\n        global_best_history = []\n\n        while evaluations < self.budget:\n            # Evaluate current positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                current_score = func(positions[i])\n                evaluations += 1\n                \n                # Update personal best\n                if current_score < personal_best_scores[i]:\n                    personal_best_scores[i] = current_score\n                    personal_best_positions[i] = positions[i]\n                \n                # Update global best\n                if current_score < global_best_score:\n                    global_best_score = current_score\n                    global_best_position = positions[i]\n                    global_best_history.append(global_best_position)\n\n            # Update velocities and positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                r1, r2 = np.random.rand(2, self.dim)\n                memory_window = max(5, len(global_best_history) // 4)\n                memory_influence = np.mean(global_best_history[-memory_window:], axis=0)\n                feedback_factor = np.tanh((global_best_score - current_score) / (global_best_score + 1e-10)) * (0.9 ** evaluations)\n                velocities[i] = (self.inertia_weight * velocities[i] +\n                                 self.cognitive_const * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.social_const * r2 * (memory_influence - positions[i]) + feedback_factor)\n                velocities[i] *= 0.93  # More adaptive velocity scaling\n                positions[i] = np.clip(positions[i] + velocities[i]*0.5, lb, ub)  # Using constriction factor\n\n            # Adaptive inertia weight decay\n            self.inertia_weight *= (0.98 - 0.01 * (evaluations / self.budget))\n\n            # Adaptive cognitive and social constants\n            progress_ratio = evaluations / self.budget\n            self.cognitive_const = 1.5 - 0.5 * progress_ratio\n            self.social_const = 1.5 + 0.5 * progress_ratio\n            \n        return global_best_position", "name": "NovelSwarmOptimizer", "description": "Improved convergence by introducing a dynamic adaptive mechanism for inertia weight and feedback factor, enhancing position updates.", "configspace": "", "generation": 54, "fitness": 0.13353172634371221, "feedback": "The algorithm NovelSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.13 with standard deviation 0.00.", "error": "", "parent_id": "b6c4cb82-037e-402e-b3f2-5919bfd43251", "metadata": {"aucs": [0.13211111708158285, 0.13497648820226837, 0.1335075737472854]}, "mutation_prompt": null}
{"id": "76f97ff9-e15b-421a-af45-4ac0075d47fd", "solution": "import numpy as np\n\nclass NovelSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particle_count = min(30, budget // dim)\n        self.inertia_weight = 0.729  # inertia weight\n        self.cognitive_const = 1.49445  # cognitive constant\n        self.social_const = 1.49445  # social constant\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        # Initialize particles' positions and velocities\n        positions = np.random.uniform(lb, ub, (self.particle_count, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particle_count, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.particle_count, np.inf)\n\n        global_best_position = None\n        global_best_score = np.inf\n\n        evaluations = 0\n        global_best_history = []\n\n        while evaluations < self.budget:\n            # Evaluate current positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                current_score = func(positions[i])\n                evaluations += 1\n                \n                # Update personal best\n                if current_score < personal_best_scores[i]:\n                    personal_best_scores[i] = current_score\n                    personal_best_positions[i] = positions[i]\n                \n                # Update global best\n                if current_score < global_best_score:\n                    global_best_score = current_score\n                    global_best_position = positions[i]\n                    global_best_history.append(global_best_position)\n\n            # Update velocities and positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                r1, r2 = np.random.rand(2, self.dim)\n                memory_window = max(5, len(global_best_history) // 4)\n                memory_influence = np.mean(global_best_history[-memory_window:], axis=0)\n                feedback_factor = np.tanh((global_best_score - current_score) / (global_best_score + 1e-10)) * (0.95 ** evaluations)\n                velocities[i] = (self.inertia_weight * velocities[i] +\n                                 self.cognitive_const * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.social_const * r2 * (memory_influence - positions[i]) + feedback_factor)\n                velocities[i] *= 0.9  # Modified adaptive velocity scaling\n                perturbation = np.random.normal(0, 0.1, self.dim)  # Added perturbation factor\n                positions[i] = np.clip(positions[i] + velocities[i]*0.5 + perturbation, lb, ub)  # Position update with perturbation\n\n            # Adaptive inertia weight decay\n            self.inertia_weight *= 0.99\n\n            # Adaptive cognitive and social constants\n            progress_ratio = evaluations / self.budget\n            self.cognitive_const = 1.5 - 0.5 * progress_ratio\n            self.social_const = 1.5 + 0.5 * progress_ratio\n            \n        return global_best_position", "name": "NovelSwarmOptimizer", "description": "Enhanced local exploration by adjusting velocity scaling and incorporating a perturbation factor in position updates for improved convergence.", "configspace": "", "generation": 55, "fitness": 0.13093308372478818, "feedback": "The algorithm NovelSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.13 with standard deviation 0.00.", "error": "", "parent_id": "b6c4cb82-037e-402e-b3f2-5919bfd43251", "metadata": {"aucs": [0.1289318397678979, 0.13460747638947756, 0.12925993501698907]}, "mutation_prompt": null}
{"id": "76215d4b-fcd4-450e-a92f-f1a2713c25e4", "solution": "import numpy as np\n\nclass NovelSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particle_count = min(30, budget // dim)\n        self.inertia_weight = 0.729  # inertia weight\n        self.cognitive_const = 1.49445  # cognitive constant\n        self.social_const = 1.49445  # social constant\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        # Initialize particles' positions and velocities\n        positions = np.random.uniform(lb, ub, (self.particle_count, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particle_count, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.particle_count, np.inf)\n\n        global_best_position = None\n        global_best_score = np.inf\n\n        evaluations = 0\n        global_best_history = []\n\n        while evaluations < self.budget:\n            # Evaluate current positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                current_score = func(positions[i])\n                evaluations += 1\n                \n                # Update personal best\n                if current_score < personal_best_scores[i]:\n                    personal_best_scores[i] = current_score\n                    personal_best_positions[i] = positions[i]\n                \n                # Update global best\n                if current_score < global_best_score:\n                    global_best_score = current_score\n                    global_best_position = positions[i]\n                    global_best_history.append(global_best_position)\n\n            # Update velocities and positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                r1, r2 = np.random.rand(2, self.dim)\n                memory_window = max(5, len(global_best_history) // 4)\n                memory_influence = np.mean(global_best_history[-memory_window:], axis=0)\n                feedback_factor = np.tanh((global_best_score - current_score) / (global_best_score + 1e-10)) * (0.95 ** evaluations)\n                velocities[i] = (self.inertia_weight * velocities[i] +\n                                 self.cognitive_const * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.social_const * r2 * (memory_influence - positions[i]) + feedback_factor)\n                velocities[i] *= 0.95  # More adaptive velocity scaling\n                positions[i] = np.clip(positions[i] + velocities[i]*0.5, lb, ub)  # Using constriction factor\n\n            # Oscillating inertia weight\n            self.inertia_weight = 0.5 + 0.5 * np.cos(2 * np.pi * evaluations / self.budget)\n\n            # Adaptive cognitive and social constants\n            progress_ratio = evaluations / self.budget\n            self.cognitive_const = 1.5 - 0.5 * progress_ratio\n            self.social_const = 1.5 + 0.5 * progress_ratio\n            \n        return global_best_position", "name": "NovelSwarmOptimizer", "description": "Enhanced convergence by introducing oscillating inertia weight and memory influence based on current evaluations.", "configspace": "", "generation": 56, "fitness": 0.13215015290057308, "feedback": "The algorithm NovelSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.13 with standard deviation 0.00.", "error": "", "parent_id": "b6c4cb82-037e-402e-b3f2-5919bfd43251", "metadata": {"aucs": [0.1279147337107469, 0.134359489408177, 0.13417623558279534]}, "mutation_prompt": null}
{"id": "67186e8c-159e-4729-92ff-178d5583d990", "solution": "import numpy as np\n\nclass NovelSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particle_count = min(30, budget // dim)\n        self.inertia_weight = 0.729  # inertia weight\n        self.cognitive_const = 1.49445  # cognitive constant\n        self.social_const = 1.49445  # social constant\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        # Initialize particles' positions and velocities\n        positions = np.random.uniform(lb, ub, (self.particle_count, self.dim))\n        velocities = np.random.uniform(-np.abs(ub-lb), np.abs(ub-lb), (self.particle_count, self.dim))  # diversified initialization\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.particle_count, np.inf)\n\n        global_best_position = None\n        global_best_score = np.inf\n\n        evaluations = 0\n        global_best_history = []\n\n        while evaluations < self.budget:\n            # Evaluate current positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                current_score = func(positions[i])\n                evaluations += 1\n                \n                # Update personal best\n                if current_score < personal_best_scores[i]:\n                    personal_best_scores[i] = current_score\n                    personal_best_positions[i] = positions[i]\n                \n                # Update global best\n                if current_score < global_best_score:\n                    global_best_score = current_score\n                    global_best_position = positions[i]\n                    global_best_history.append(global_best_position)\n\n            # Update velocities and positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                r1, r2 = np.random.rand(2, self.dim)\n                memory_window = max(5, len(global_best_history) // 4)\n                memory_influence = np.mean(global_best_history[-memory_window:], axis=0)\n                feedback_factor = np.tanh((global_best_score - current_score) / (global_best_score + 1e-10)) * (0.95 ** evaluations)\n                velocities[i] = (self.inertia_weight * velocities[i] +\n                                 self.cognitive_const * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.social_const * r2 * (memory_influence - positions[i]) + feedback_factor)\n                velocities[i] *= 0.95  # More adaptive velocity scaling\n                positions[i] = np.clip(positions[i] + velocities[i]*0.5, lb, ub)  # Using constriction factor\n\n            # Adaptive inertia weight decay\n            self.inertia_weight *= 0.99 + (0.01 * np.sin(0.1 * evaluations))  # non-linear decay\n\n            # Adaptive cognitive and social constants\n            progress_ratio = evaluations / self.budget\n            self.cognitive_const = 1.5 - 0.5 * progress_ratio\n            self.social_const = 1.5 + 0.5 * progress_ratio\n            \n        return global_best_position", "name": "NovelSwarmOptimizer", "description": "Enhanced dynamic adaptation by introducing a non-linear decay function for inertia and diversified the initialization of velocities for improved exploration.", "configspace": "", "generation": 57, "fitness": 0.13078337598417536, "feedback": "The algorithm NovelSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.13 with standard deviation 0.00.", "error": "", "parent_id": "b6c4cb82-037e-402e-b3f2-5919bfd43251", "metadata": {"aucs": [0.13241215186795408, 0.13027300244395712, 0.1296649736406149]}, "mutation_prompt": null}
{"id": "8e7bb2cd-8552-4612-8e51-3201f79384b1", "solution": "import numpy as np\n\nclass NovelSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particle_count = min(30, budget // dim)\n        self.inertia_weight = 0.729  # inertia weight\n        self.cognitive_const = 1.49445  # cognitive constant\n        self.social_const = 1.49445  # social constant\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        positions = np.random.uniform(lb, ub, (self.particle_count, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particle_count, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.particle_count, np.inf)\n\n        global_best_position = None\n        global_best_score = np.inf\n\n        evaluations = 0\n        global_best_history = []\n\n        while evaluations < self.budget:\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                current_score = func(positions[i])\n                evaluations += 1\n                \n                if current_score < personal_best_scores[i]:\n                    personal_best_scores[i] = current_score\n                    personal_best_positions[i] = positions[i]\n                \n                if current_score < global_best_score:\n                    global_best_score = current_score\n                    global_best_position = positions[i]\n                    global_best_history.append(global_best_position)\n\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                r1, r2, r3 = np.random.rand(3, self.dim)\n                memory_window = max(5, len(global_best_history) // 4)\n                memory_influence = np.mean(global_best_history[-memory_window:], axis=0)\n                neighbor_influence = np.mean(positions, axis=0)\n                feedback_factor = np.tanh((global_best_score - current_score) / (global_best_score + 1e-10)) * (0.95 ** evaluations)\n                velocities[i] = (self.inertia_weight * velocities[i] +\n                                 self.cognitive_const * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.social_const * r2 * (memory_influence - positions[i]) +\n                                 r3 * (neighbor_influence - positions[i]) + feedback_factor)  # Updated line\n                velocities[i] *= 0.95\n                positions[i] = np.clip(positions[i] + velocities[i]*0.5, lb, ub)\n\n            self.inertia_weight *= 0.99\n\n            progress_ratio = evaluations / self.budget\n            self.cognitive_const = 1.5 - 0.5 * progress_ratio\n            self.social_const = 1.5 + 0.5 * progress_ratio\n            \n        return global_best_position", "name": "NovelSwarmOptimizer", "description": "Enhanced swarm dynamics by adjusting velocity update based on dynamic neighborhood influence for improved convergence.", "configspace": "", "generation": 58, "fitness": 0.13296604442916493, "feedback": "The algorithm NovelSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.13 with standard deviation 0.00.", "error": "", "parent_id": "b6c4cb82-037e-402e-b3f2-5919bfd43251", "metadata": {"aucs": [0.13297977787294935, 0.13336898673448105, 0.1325493686800644]}, "mutation_prompt": null}
{"id": "199d3c44-df1b-4cd3-a392-4d0cb37463c2", "solution": "import numpy as np\n\nclass NovelSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particle_count = min(30, budget // dim)\n        self.inertia_weight = 0.729  # inertia weight\n        self.cognitive_const = 1.49445  # cognitive constant\n        self.social_const = 1.49445  # social constant\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        # Initialize particles' positions and velocities\n        positions = np.random.uniform(lb, ub, (self.particle_count, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particle_count, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.particle_count, np.inf)\n\n        global_best_position = None\n        global_best_score = np.inf\n\n        evaluations = 0\n        global_best_history = []\n\n        while evaluations < self.budget:\n            # Evaluate current positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                current_score = func(positions[i])\n                evaluations += 1\n                \n                # Update personal best\n                if current_score < personal_best_scores[i]:\n                    personal_best_scores[i] = current_score\n                    personal_best_positions[i] = positions[i]\n                \n                # Update global best\n                if current_score < global_best_score:\n                    global_best_score = current_score\n                    global_best_position = positions[i]\n                    global_best_history.append(global_best_position)\n\n            # Update velocities and positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                r1, r2 = np.random.rand(2, self.dim)\n                memory_window = max(5, len(global_best_history) // 4)\n                memory_influence = np.mean(global_best_history[-memory_window:], axis=0)\n                feedback_factor = np.tanh((global_best_score - current_score) / (global_best_score + 1e-10)) * (0.95 ** evaluations)\n                velocities[i] = (self.inertia_weight * velocities[i] +\n                                 self.cognitive_const * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.social_const * r2 * (memory_influence - positions[i]) + feedback_factor)\n                velocities[i] *= 0.95  # More adaptive velocity scaling\n                scale_factor = 0.5 + 0.0001 * evaluations  # dynamic scaling based on evaluations\n                positions[i] = np.clip(positions[i] + velocities[i] * scale_factor, lb, ub)  # Using dynamic constriction factor\n\n            # Adaptive inertia weight decay\n            self.inertia_weight *= 0.99\n\n            # Adaptive cognitive and social constants\n            progress_ratio = evaluations / self.budget\n            self.cognitive_const = 1.5 - 0.5 * progress_ratio\n            self.social_const = 1.5 + 0.5 * progress_ratio\n            \n        return global_best_position", "name": "NovelSwarmOptimizer", "description": "Introduced a dynamic scaling factor for velocity constriction to enhance convergence precision.", "configspace": "", "generation": 59, "fitness": 0.13375084791939398, "feedback": "The algorithm NovelSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.13 with standard deviation 0.00.", "error": "", "parent_id": "b6c4cb82-037e-402e-b3f2-5919bfd43251", "metadata": {"aucs": [0.13324633700110022, 0.1347665513959646, 0.13323965536111715]}, "mutation_prompt": null}
{"id": "7366a5f2-156d-4b77-a65e-194acb24b20c", "solution": "import numpy as np\n\nclass NovelSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particle_count = min(30, budget // dim)\n        self.inertia_weight = 0.729  # inertia weight\n        self.cognitive_const = 1.49445  # cognitive constant\n        self.social_const = 1.49445  # social constant\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        # Initialize particles' positions and velocities\n        positions = np.random.uniform(lb, ub, (self.particle_count, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particle_count, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.particle_count, np.inf)\n\n        global_best_position = None\n        global_best_score = np.inf\n\n        evaluations = 0\n        global_best_history = []\n\n        while evaluations < self.budget:\n            # Evaluate current positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                current_score = func(positions[i])\n                evaluations += 1\n                \n                # Update personal best\n                if current_score < personal_best_scores[i]:\n                    personal_best_scores[i] = current_score\n                    personal_best_positions[i] = positions[i]\n                \n                # Update global best\n                if current_score < global_best_score:\n                    global_best_score = current_score\n                    global_best_position = positions[i]\n                    global_best_history.append(global_best_position)\n\n            # Update velocities and positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                r1, r2 = np.random.rand(2, self.dim)\n                memory_window = max(5, len(global_best_history) // 4)\n                memory_influence = np.mean(global_best_history[-memory_window:], axis=0)\n                feedback_factor = np.tanh((global_best_score - current_score) / (global_best_score + 1e-10)) * (0.95 ** evaluations)\n                # Differential evolution-inspired mutation for velocity\n                rand_idx = np.random.choice(self.particle_count, 3, replace=False)\n                velocities[i] = (self.inertia_weight * velocities[i] +\n                                 self.cognitive_const * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.social_const * r2 * (memory_influence - positions[i]) + feedback_factor +\n                                 0.5 * (positions[rand_idx[0]] - positions[rand_idx[1]]))\n                velocities[i] *= 0.95  # More adaptive velocity scaling\n                scale_factor = 0.5 + 0.0001 * evaluations  # dynamic scaling based on evaluations\n                positions[i] = np.clip(positions[i] + velocities[i] * scale_factor, lb, ub)  # Using dynamic constriction factor\n\n            # Adaptive inertia weight decay\n            self.inertia_weight *= 0.99\n\n            # Adaptive cognitive and social constants\n            progress_ratio = evaluations / self.budget\n            self.cognitive_const = 1.5 - 0.5 * progress_ratio\n            self.social_const = 1.5 + 0.5 * progress_ratio\n            \n        return global_best_position", "name": "NovelSwarmOptimizer", "description": "Enhanced velocity updates by incorporating differential evolution-inspired mutation for improved exploration-exploitation balance.", "configspace": "", "generation": 60, "fitness": 0.13319445350565143, "feedback": "The algorithm NovelSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.13 with standard deviation 0.00.", "error": "", "parent_id": "199d3c44-df1b-4cd3-a392-4d0cb37463c2", "metadata": {"aucs": [0.1324468268787119, 0.13366690755552235, 0.13346962608272006]}, "mutation_prompt": null}
{"id": "e8db4c1e-0b7a-4701-9e3e-3aab4696a7e7", "solution": "import numpy as np\n\nclass NovelSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particle_count = min(30, budget // dim)\n        self.inertia_weight = 0.729  # inertia weight\n        self.cognitive_const = 1.49445  # cognitive constant\n        self.social_const = 1.49445  # social constant\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        # Initialize particles' positions and velocities\n        positions = np.random.uniform(lb, ub, (self.particle_count, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particle_count, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.particle_count, np.inf)\n\n        global_best_position = None\n        global_best_score = np.inf\n\n        evaluations = 0\n        global_best_history = []\n\n        while evaluations < self.budget:\n            # Evaluate current positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                current_score = func(positions[i])\n                evaluations += 1\n                \n                # Update personal best\n                if current_score < personal_best_scores[i]:\n                    personal_best_scores[i] = current_score\n                    personal_best_positions[i] = positions[i]\n                \n                # Update global best\n                if current_score < global_best_score:\n                    global_best_score = current_score\n                    global_best_position = positions[i]\n                    global_best_history.append(global_best_position)\n\n            # Update velocities and positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                r1, r2 = np.random.rand(2, self.dim)\n                memory_window = max(5, len(global_best_history) // 4)\n                memory_influence = np.mean(global_best_history[-memory_window:], axis=0)\n                feedback_factor = np.tanh((global_best_score - current_score) / (global_best_score + 1e-10)) * (0.95 ** evaluations)\n                velocities[i] = (self.inertia_weight * velocities[i] +\n                                 self.cognitive_const * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.social_const * r2 * (memory_influence - positions[i] + feedback_factor * np.random.randn(self.dim)))\n                velocities[i] *= 0.95  # More adaptive velocity scaling\n                scale_factor = 0.5 + 0.0001 * evaluations  # dynamic scaling based on evaluations\n                positions[i] = np.clip(positions[i] + velocities[i] * scale_factor, lb, ub)  # Using dynamic constriction factor\n\n            # Adaptive inertia weight decay\n            self.inertia_weight *= 0.99\n\n            # Adaptive cognitive and social constants\n            progress_ratio = evaluations / self.budget\n            self.cognitive_const = 1.5 - 0.5 * progress_ratio\n            self.social_const = 1.5 + 0.5 * progress_ratio\n            \n        return global_best_position", "name": "NovelSwarmOptimizer", "description": "Enhanced velocity update with a memory-based perturbation for improving convergence in complex landscapes.", "configspace": "", "generation": 61, "fitness": 0.1304468608189084, "feedback": "The algorithm NovelSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.13 with standard deviation 0.00.", "error": "", "parent_id": "199d3c44-df1b-4cd3-a392-4d0cb37463c2", "metadata": {"aucs": [0.12681680225891023, 0.13423319489824637, 0.1302905852995686]}, "mutation_prompt": null}
{"id": "2ef552c1-fcd4-4317-9aee-f4d21d65a5f0", "solution": "import numpy as np\n\nclass NovelSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particle_count = min(30, budget // dim)\n        self.inertia_weight = 0.729  # inertia weight\n        self.cognitive_const = 1.49445  # cognitive constant\n        self.social_const = 1.49445  # social constant\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        # Initialize particles' positions and velocities\n        positions = np.random.uniform(lb, ub, (self.particle_count, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particle_count, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.particle_count, np.inf)\n\n        global_best_position = None\n        global_best_score = np.inf\n\n        evaluations = 0\n        global_best_history = []\n\n        while evaluations < self.budget:\n            # Evaluate current positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                current_score = func(positions[i])\n                evaluations += 1\n                \n                # Update personal best\n                if current_score < personal_best_scores[i]:\n                    personal_best_scores[i] = current_score\n                    personal_best_positions[i] = positions[i]\n                \n                # Update global best\n                if current_score < global_best_score:\n                    global_best_score = current_score\n                    global_best_position = positions[i]\n                    global_best_history.append(global_best_position)\n\n            # Update velocities and positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                r1, r2 = np.random.rand(2, self.dim)\n                memory_window = max(5, len(global_best_history) // 4)\n                memory_influence = np.mean(global_best_history[-memory_window:], axis=0)\n                feedback_factor = np.tanh((global_best_score - current_score) / (global_best_score + 1e-10)) * (0.95 ** evaluations)\n                velocities[i] = (self.inertia_weight * velocities[i] +\n                                 self.cognitive_const * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.social_const * r2 * (memory_influence - positions[i]) + feedback_factor)\n                scale_factor = 0.6 + 0.00015 * evaluations  # Enhanced dynamic scaling\n                positions[i] = np.clip(positions[i] + velocities[i] * scale_factor, lb, ub)  # Using dynamic constriction factor\n\n            # Adaptive inertia weight decay\n            self.inertia_weight *= 0.98  # Slight adjustment for better exploration\n\n            # Adaptive cognitive and social constants\n            progress_ratio = evaluations / self.budget\n            self.cognitive_const = 1.5 - 0.5 * progress_ratio\n            self.social_const = 1.5 + 0.5 * progress_ratio\n            \n        return global_best_position", "name": "NovelSwarmOptimizer", "description": "Enhanced adaptive velocity scaling and dynamic parameter adjustment to improve exploration and exploitation balance.", "configspace": "", "generation": 62, "fitness": 0.13196110495649516, "feedback": "The algorithm NovelSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.13 with standard deviation 0.00.", "error": "", "parent_id": "199d3c44-df1b-4cd3-a392-4d0cb37463c2", "metadata": {"aucs": [0.1295996008817768, 0.1344435299667739, 0.13184018402093478]}, "mutation_prompt": null}
{"id": "9864bb26-d702-4614-baf9-0272bd86016f", "solution": "import numpy as np\n\nclass NovelSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particle_count = min(30, budget // dim)\n        self.inertia_weight = 0.729  # inertia weight\n        self.cognitive_const = 1.49445  # cognitive constant\n        self.social_const = 1.49445  # social constant\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        # Initialize particles' positions and velocities\n        positions = np.random.uniform(lb, ub, (self.particle_count, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particle_count, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.particle_count, np.inf)\n\n        global_best_position = None\n        global_best_score = np.inf\n\n        evaluations = 0\n        global_best_history = []\n\n        while evaluations < self.budget:\n            # Evaluate current positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                current_score = func(positions[i])\n                evaluations += 1\n                \n                # Update personal best\n                if current_score < personal_best_scores[i]:\n                    personal_best_scores[i] = current_score\n                    personal_best_positions[i] = positions[i]\n                \n                # Update global best\n                if current_score < global_best_score:\n                    global_best_score = current_score\n                    global_best_position = positions[i]\n                    global_best_history.append(global_best_position)\n\n            # Update velocities and positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                r1, r2 = np.random.rand(2, self.dim)\n                memory_window = max(5, len(global_best_history) // 3)  # Adjusted memory window\n                memory_influence = np.mean(global_best_history[-memory_window:], axis=0)\n                feedback_factor = np.tanh((global_best_score - current_score) / (global_best_score + 1e-10)) * (0.95 ** evaluations)\n                velocities[i] = (self.inertia_weight * velocities[i] +\n                                 self.cognitive_const * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.social_const * r2 * (memory_influence - positions[i]) + feedback_factor)\n                velocities[i] *= 0.94  # More adaptive velocity scaling\n                scale_factor = 0.5 + 0.0001 * evaluations  # dynamic scaling based on evaluations\n                positions[i] = np.clip(positions[i] + velocities[i] * scale_factor, lb, ub)  # Using dynamic constriction factor\n\n            # Adaptive inertia weight decay\n            self.inertia_weight *= 0.985  # Adjusted inertia weight decay\n\n            # Adaptive cognitive and social constants\n            progress_ratio = evaluations / self.budget\n            self.cognitive_const = 1.5 - 0.5 * progress_ratio\n            self.social_const = 1.5 + 0.5 * progress_ratio\n            \n        return global_best_position", "name": "NovelSwarmOptimizer", "description": "Enhanced convergence by slightly adjusting memory influence and inertia weight decay within the velocity update mechanism.", "configspace": "", "generation": 63, "fitness": 0.13362782601580844, "feedback": "The algorithm NovelSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.13 with standard deviation 0.00.", "error": "", "parent_id": "199d3c44-df1b-4cd3-a392-4d0cb37463c2", "metadata": {"aucs": [0.13294395684156501, 0.1347360871878438, 0.1332034340180165]}, "mutation_prompt": null}
{"id": "ddb7ff25-ae78-434a-bd18-1df65a9f7d04", "solution": "import numpy as np\n\nclass NovelSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particle_count = min(30, budget // dim)\n        self.inertia_weight = 0.729  # inertia weight\n        self.cognitive_const = 1.49445  # cognitive constant\n        self.social_const = 1.49445  # social constant\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        # Initialize particles' positions and velocities\n        positions = np.random.uniform(lb, ub, (self.particle_count, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particle_count, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.particle_count, np.inf)\n\n        global_best_position = None\n        global_best_score = np.inf\n\n        evaluations = 0\n        global_best_history = []\n\n        while evaluations < self.budget:\n            # Evaluate current positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                current_score = func(positions[i])\n                evaluations += 1\n                \n                # Update personal best\n                if current_score < personal_best_scores[i]:\n                    personal_best_scores[i] = current_score\n                    personal_best_positions[i] = positions[i]\n                \n                # Update global best\n                if current_score < global_best_score:\n                    global_best_score = current_score\n                    global_best_position = positions[i]\n                    global_best_history.append(global_best_position)\n\n            # Update velocities and positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                r1, r2 = np.random.rand(2, self.dim)\n                memory_window = max(5, len(global_best_history) // 4)\n                memory_influence = np.mean(global_best_history[-memory_window:], axis=0)\n                feedback_factor = np.tanh((global_best_score - current_score) / (global_best_score + 1e-10)) * (0.95 ** evaluations)\n                velocities[i] = (self.inertia_weight * velocities[i] +\n                                 self.cognitive_const * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.social_const * r2 * (memory_influence - positions[i]) + feedback_factor)\n                velocities[i] *= 0.95  # More adaptive velocity scaling\n                scale_factor = 0.5 + 0.0001 * evaluations + 0.001 * (global_best_score / (current_score + 1e-10))  # dynamic scaling based on evaluations and convergence speed\n                positions[i] = np.clip(positions[i] + velocities[i] * scale_factor, lb, ub)  # Using dynamic constriction factor\n\n            # Adaptive inertia weight decay\n            self.inertia_weight *= 0.99\n\n            # Adaptive cognitive and social constants\n            progress_ratio = evaluations / self.budget\n            self.cognitive_const = 1.5 - 0.5 * progress_ratio\n            self.social_const = 1.5 + 0.5 * progress_ratio\n            \n        return global_best_position", "name": "NovelSwarmOptimizer", "description": "Introduced adaptive learning rate for velocity scaling based on convergence speed to improve exploration.", "configspace": "", "generation": 64, "fitness": 0.1337458222559, "feedback": "The algorithm NovelSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.13 with standard deviation 0.00.", "error": "", "parent_id": "199d3c44-df1b-4cd3-a392-4d0cb37463c2", "metadata": {"aucs": [0.1332437320974571, 0.1347620316896877, 0.1332317029805552]}, "mutation_prompt": null}
{"id": "2f6891be-94e9-46ea-89e9-909eba8f6db6", "solution": "import numpy as np\n\nclass NovelSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particle_count = min(30, budget // dim)\n        self.inertia_weight = 0.729  # inertia weight\n        self.cognitive_const = 1.49445  # cognitive constant\n        self.social_const = 1.49445  # social constant\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        # Initialize particles' positions and velocities\n        positions = np.random.uniform(lb, ub, (self.particle_count, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particle_count, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.particle_count, np.inf)\n\n        global_best_position = None\n        global_best_score = np.inf\n\n        evaluations = 0\n        global_best_history = []\n\n        while evaluations < self.budget:\n            # Evaluate current positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                current_score = func(positions[i])\n                evaluations += 1\n                \n                # Update personal best\n                if current_score < personal_best_scores[i]:\n                    personal_best_scores[i] = current_score\n                    personal_best_positions[i] = positions[i]\n                \n                # Update global best\n                if current_score < global_best_score:\n                    global_best_score = current_score\n                    global_best_position = positions[i]\n                    global_best_history.append(global_best_position)\n\n            # Update velocities and positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                r1, r2 = np.random.rand(2, self.dim)\n                memory_window = max(5, len(global_best_history) // 4)\n                memory_weights = np.linspace(0.5, 1.0, memory_window)\n                memory_influence = np.average(global_best_history[-memory_window:], axis=0, weights=memory_weights)\n                feedback_factor = np.tanh((global_best_score - current_score) / (global_best_score + 1e-10)) * (0.95 ** evaluations)\n                velocities[i] = (self.inertia_weight * velocities[i] +\n                                 self.cognitive_const * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.social_const * r2 * (memory_influence - positions[i]) + feedback_factor)\n                velocities[i] *= 0.95  # More adaptive velocity scaling\n                scale_factor = 0.5 + 0.0001 * evaluations  # dynamic scaling based on evaluations\n                positions[i] = np.clip(positions[i] + velocities[i] * scale_factor, lb, ub)  # Using dynamic constriction factor\n\n            # Adaptive inertia weight decay\n            self.inertia_weight *= 0.99\n\n            # Adaptive cognitive and social constants\n            progress_ratio = evaluations / self.budget\n            self.cognitive_const = 1.5 - 0.5 * progress_ratio\n            self.social_const = 1.5 + 0.5 * progress_ratio\n            \n        return global_best_position", "name": "NovelSwarmOptimizer", "description": "Improved memory influence by using a weighted sum of recent best positions to enhance exploration.", "configspace": "", "generation": 65, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('Length of weights not compatible with specified axis.').", "error": "ValueError('Length of weights not compatible with specified axis.')", "parent_id": "199d3c44-df1b-4cd3-a392-4d0cb37463c2", "metadata": {}, "mutation_prompt": null}
{"id": "20cafa33-a939-496c-be85-f99448fb4de8", "solution": "import numpy as np\n\nclass NovelSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particle_count = min(30, budget // dim)\n        self.inertia_weight = 0.729  # inertia weight\n        self.cognitive_const = 1.49445  # cognitive constant\n        self.social_const = 1.49445  # social constant\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        # Initialize particles' positions and velocities\n        positions = np.random.uniform(lb, ub, (self.particle_count, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particle_count, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.particle_count, np.inf)\n\n        global_best_position = None\n        global_best_score = np.inf\n\n        evaluations = 0\n        global_best_history = []\n\n        while evaluations < self.budget:\n            # Evaluate current positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                current_score = func(positions[i])\n                evaluations += 1\n                \n                # Update personal best\n                if current_score < personal_best_scores[i]:\n                    personal_best_scores[i] = current_score\n                    personal_best_positions[i] = positions[i]\n                \n                # Update global best\n                if current_score < global_best_score:\n                    global_best_score = current_score\n                    global_best_position = positions[i]\n                    global_best_history.append(global_best_position)\n\n            # Update velocities and positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                r1, r2 = np.random.rand(2, self.dim)\n                memory_window = max(5, len(global_best_history) // 4)\n                memory_influence = np.mean(global_best_history[-memory_window:], axis=0)\n                feedback_factor = np.tanh((global_best_score - current_score) / (global_best_score + 1e-10)) * (0.95 ** evaluations)\n                velocities[i] = (self.inertia_weight * velocities[i] +\n                                 self.cognitive_const * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.social_const * r2 * (memory_influence - positions[i]) + feedback_factor)\n                velocities[i] *= 0.95  # More adaptive velocity scaling\n                scale_factor = 0.5 + 0.0001 * evaluations  # dynamic scaling based on evaluations\n                positions[i] = np.clip(positions[i] + velocities[i] * scale_factor * (1.1 - progress_ratio), lb, ub)  # Using dynamic constriction factor\n\n            # Adaptive inertia weight decay\n            self.inertia_weight *= 0.99\n\n            # Adaptive cognitive and social constants\n            progress_ratio = evaluations / self.budget\n            self.cognitive_const = 1.5 - 0.5 * progress_ratio\n            self.social_const = 1.5 + 0.5 * progress_ratio\n            \n        return global_best_position", "name": "NovelSwarmOptimizer", "description": "Added adaptive velocity scaling based on exploration-exploitation balance.", "configspace": "", "generation": 66, "fitness": -Infinity, "feedback": "An exception occurred: UnboundLocalError(\"local variable 'progress_ratio' referenced before assignment\").", "error": "UnboundLocalError(\"local variable 'progress_ratio' referenced before assignment\")", "parent_id": "199d3c44-df1b-4cd3-a392-4d0cb37463c2", "metadata": {}, "mutation_prompt": null}
{"id": "594b2d27-0b45-45ce-a089-f3f2d5707376", "solution": "import numpy as np\nfrom scipy.stats.qmc import Sobol\n\nclass NovelSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particle_count = min(30, budget // dim)\n        self.inertia_weight = 0.729  # inertia weight\n        self.cognitive_const = 1.49445  # cognitive constant\n        self.social_const = 1.49445  # social constant\n        self.sobol_sampler = Sobol(d=dim, scramble=True)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        # Initialize particles' positions using Sobol sequences\n        positions = lb + (ub - lb) * self.sobol_sampler.random_base2(m=int(np.log2(self.particle_count)))\n        velocities = np.random.uniform(-1, 1, (self.particle_count, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.particle_count, np.inf)\n\n        global_best_position = None\n        global_best_score = np.inf\n\n        evaluations = 0\n        global_best_history = []\n\n        while evaluations < self.budget:\n            # Evaluate current positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                current_score = func(positions[i])\n                evaluations += 1\n\n                # Update personal best\n                if current_score < personal_best_scores[i]:\n                    personal_best_scores[i] = current_score\n                    personal_best_positions[i] = positions[i]\n\n                # Update global best\n                if current_score < global_best_score:\n                    global_best_score = current_score\n                    global_best_position = positions[i]\n                    global_best_history.append(global_best_position)\n\n            # Update velocities and positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                r1, r2 = np.random.rand(2, self.dim)\n                memory_window = max(5, len(global_best_history) // 4)\n                memory_influence = np.mean(global_best_history[-memory_window:], axis=0)\n                feedback_factor = np.tanh((global_best_score - current_score) / (global_best_score + 1e-10)) * (0.95 ** evaluations)\n                velocities[i] = (self.inertia_weight * velocities[i] +\n                                 self.cognitive_const * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.social_const * r2 * (memory_influence - positions[i]) + feedback_factor)\n                velocities[i] *= 0.95  # More adaptive velocity scaling\n                scale_factor = 0.5 + 0.0001 * evaluations  # dynamic scaling based on evaluations\n                positions[i] = np.clip(positions[i] + velocities[i] * scale_factor, lb, ub)  # Using dynamic constriction factor\n\n            # Adaptive inertia weight decay\n            self.inertia_weight *= 0.99\n\n            # Adaptive cognitive and social constants\n            progress_ratio = evaluations / self.budget\n            self.cognitive_const = 1.5 - 0.5 * progress_ratio\n            self.social_const = 1.5 + 0.5 * progress_ratio\n\n        return global_best_position", "name": "NovelSwarmOptimizer", "description": "Introduced a parallel exploration strategy using Sobol sequences to enhance diversity and convergence speed.", "configspace": "", "generation": 67, "fitness": -Infinity, "feedback": "An exception occurred: IndexError('index 16 is out of bounds for axis 0 with size 16').", "error": "IndexError('index 16 is out of bounds for axis 0 with size 16')", "parent_id": "199d3c44-df1b-4cd3-a392-4d0cb37463c2", "metadata": {}, "mutation_prompt": null}
{"id": "cf2bdff8-c910-4705-b0cf-573e4ec10d9b", "solution": "import numpy as np\n\nclass NovelSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particle_count = min(30, budget // dim)\n        self.inertia_weight = 0.729  # inertia weight\n        self.cognitive_const = 1.49445  # cognitive constant\n        self.social_const = 1.49445  # social constant\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        # Initialize particles' positions and velocities\n        positions = np.random.uniform(lb, ub, (self.particle_count, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particle_count, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.particle_count, np.inf)\n\n        global_best_position = None\n        global_best_score = np.inf\n\n        evaluations = 0\n        global_best_history = []\n\n        while evaluations < self.budget:\n            # Evaluate current positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                current_score = func(positions[i])\n                evaluations += 1\n                \n                # Update personal best\n                if current_score < personal_best_scores[i]:\n                    personal_best_scores[i] = current_score\n                    personal_best_positions[i] = positions[i]\n                \n                # Update global best\n                if current_score < global_best_score:\n                    global_best_score = current_score\n                    global_best_position = positions[i]\n                    global_best_history.append(global_best_position)\n\n            # Update velocities and positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                r1, r2 = np.random.rand(2, self.dim)\n                memory_window = max(5, len(global_best_history) // 4)\n                memory_influence = np.mean(global_best_history[-memory_window:], axis=0)\n                feedback_factor = np.tanh((global_best_score - current_score) / (global_best_score + 1e-10)) * (0.95 ** evaluations)\n                velocities[i] = (self.inertia_weight * velocities[i] +\n                                 self.cognitive_const * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.social_const * r2 * (memory_influence - positions[i]) + feedback_factor)\n                velocities[i] *= 0.95  # More adaptive velocity scaling\n                scale_factor = 0.5 + 0.0001 * evaluations  # dynamic scaling based on evaluations\n                positions[i] = np.clip(positions[i] + velocities[i] * scale_factor, lb, ub)  # Using dynamic constriction factor\n\n            # Adaptive inertia weight decay\n            self.inertia_weight *= 0.99\n\n            # Adaptive cognitive and social constants\n            progress_ratio = evaluations / self.budget\n            self.cognitive_const = 1.7 - 0.7 * progress_ratio  # slightly more aggressive adjustment\n            self.social_const = 1.7 + 0.7 * progress_ratio\n            \n        return global_best_position", "name": "NovelSwarmOptimizer", "description": "Enhanced convergence by dynamically adjusting cognitive and social constants more aggressively.", "configspace": "", "generation": 68, "fitness": 0.13283653869315895, "feedback": "The algorithm NovelSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.13 with standard deviation 0.00.", "error": "", "parent_id": "199d3c44-df1b-4cd3-a392-4d0cb37463c2", "metadata": {"aucs": [0.13021541820300497, 0.1345806132517411, 0.13371358462473082]}, "mutation_prompt": null}
{"id": "0f2d9103-e299-4d3b-8827-f78182d9aa62", "solution": "import numpy as np\n\nclass NovelSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particle_count = min(30, budget // dim)\n        self.inertia_weight = 0.729  # inertia weight\n        self.cognitive_const = 1.49445  # cognitive constant\n        self.social_const = 1.49445  # social constant\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        # Initialize particles' positions and velocities\n        positions = np.random.uniform(lb, ub, (self.particle_count, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particle_count, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.particle_count, np.inf)\n\n        global_best_position = None\n        global_best_score = np.inf\n\n        evaluations = 0\n        global_best_history = []\n\n        while evaluations < self.budget:\n            # Evaluate current positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                current_score = func(positions[i])\n                evaluations += 1\n                \n                # Update personal best\n                if current_score < personal_best_scores[i]:\n                    personal_best_scores[i] = current_score\n                    personal_best_positions[i] = positions[i]\n                \n                # Update global best\n                if current_score < global_best_score:\n                    global_best_score = current_score\n                    global_best_position = positions[i]\n                    global_best_history.append(global_best_position)\n\n            # Update velocities and positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                r1, r2 = np.random.rand(2, self.dim)\n                r3 = np.random.normal(0, 0.1, self.dim)  # Added adaptive random noise\n                memory_window = max(5, len(global_best_history) // 4)\n                memory_influence = np.mean(global_best_history[-memory_window:], axis=0)\n                feedback_factor = np.tanh((global_best_score - current_score) / (global_best_score + 1e-10)) * (0.95 ** evaluations)\n                velocities[i] = (self.inertia_weight * velocities[i] +\n                                 self.cognitive_const * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.social_const * r2 * (memory_influence - positions[i]) + feedback_factor + r3)\n                velocities[i] *= 0.95  # More adaptive velocity scaling\n                scale_factor = 0.5 + 0.0001 * evaluations  # dynamic scaling based on evaluations\n                positions[i] = np.clip(positions[i] + velocities[i] * scale_factor, lb, ub)  # Using dynamic constriction factor\n\n            # Adaptive inertia weight decay\n            self.inertia_weight *= 0.99\n\n            # Adaptive cognitive and social constants\n            progress_ratio = evaluations / self.budget\n            self.cognitive_const = 1.5 - 0.5 * progress_ratio\n            self.social_const = 1.5 + 0.5 * progress_ratio\n            \n        return global_best_position", "name": "NovelSwarmOptimizer", "description": "Added adaptive random noise to the velocity updates to enhance exploration in the early stages.", "configspace": "", "generation": 69, "fitness": 0.1296978301878371, "feedback": "The algorithm NovelSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.13 with standard deviation 0.00.", "error": "", "parent_id": "199d3c44-df1b-4cd3-a392-4d0cb37463c2", "metadata": {"aucs": [0.12684056685295764, 0.13425158023741646, 0.12800134347313719]}, "mutation_prompt": null}
{"id": "49deead2-f43e-4259-a2b1-ffe50a862332", "solution": "import numpy as np\n\nclass NovelSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particle_count = min(30, budget // dim)\n        self.inertia_weight = 0.729  # inertia weight\n        self.cognitive_const = 1.49445  # cognitive constant\n        self.social_const = 1.49445  # social constant\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        # Initialize particles' positions and velocities\n        positions = np.random.uniform(lb, ub, (self.particle_count, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particle_count, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.particle_count, np.inf)\n\n        global_best_position = None\n        global_best_score = np.inf\n\n        evaluations = 0\n        global_best_history = []\n\n        while evaluations < self.budget:\n            # Evaluate current positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                current_score = func(positions[i])\n                evaluations += 1\n                \n                # Update personal best\n                if current_score < personal_best_scores[i]:\n                    personal_best_scores[i] = current_score\n                    personal_best_positions[i] = positions[i]\n                \n                # Update global best\n                if current_score < global_best_score:\n                    global_best_score = current_score\n                    global_best_position = positions[i]\n                    global_best_history.append(global_best_position)\n\n            # Update velocities and positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                r1, r2 = np.random.rand(2, self.dim)\n                memory_weight = np.linspace(1, 0.1, len(global_best_history))  # New line\n                memory_influence = np.average(global_best_history[-len(memory_weight):], axis=0, weights=memory_weight)  # Changed line\n                feedback_factor = np.tanh((global_best_score - current_score) / (global_best_score + 1e-10)) * (0.95 ** evaluations)\n                velocities[i] = (self.inertia_weight * velocities[i] +\n                                 self.cognitive_const * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.social_const * r2 * (memory_influence - positions[i]) + feedback_factor)\n                velocities[i] *= 0.95  # More adaptive velocity scaling\n                scale_factor = 0.5 + 0.0001 * evaluations  # dynamic scaling based on evaluations\n                positions[i] = np.clip(positions[i] + velocities[i] * scale_factor, lb, ub)  # Using dynamic constriction factor\n\n            # Adaptive inertia weight decay\n            self.inertia_weight *= 0.99 - 0.009 * np.sin(evaluations * np.pi / self.budget)  # Changed line\n\n            # Adaptive cognitive and social constants\n            progress_ratio = evaluations / self.budget\n            self.cognitive_const = 1.5 - 0.5 * progress_ratio\n            self.social_const = 1.5 + 0.5 * progress_ratio\n            \n        return global_best_position", "name": "NovelSwarmOptimizer", "description": "Enhanced memory influence by using a weighted sum of the global best history and added a nonlinear decay to the inertia weight.", "configspace": "", "generation": 70, "fitness": 0.13189672145088413, "feedback": "The algorithm NovelSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.13 with standard deviation 0.00.", "error": "", "parent_id": "199d3c44-df1b-4cd3-a392-4d0cb37463c2", "metadata": {"aucs": [0.12747554432747965, 0.1348968828620849, 0.1333177371630878]}, "mutation_prompt": null}
{"id": "ff9fe159-4c63-4743-a1a7-e569240d5711", "solution": "import numpy as np\n\nclass NovelSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particle_count = min(30, budget // dim)\n        self.inertia_weight = 0.729  # inertia weight\n        self.cognitive_const = 1.49445  # cognitive constant\n        self.social_const = 1.49445  # social constant\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        # Initialize particles' positions and velocities\n        positions = np.random.uniform(lb, ub, (self.particle_count, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particle_count, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.particle_count, np.inf)\n\n        global_best_position = None\n        global_best_score = np.inf\n\n        evaluations = 0\n        global_best_history = []\n\n        while evaluations < self.budget:\n            # Evaluate current positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                current_score = func(positions[i])\n                evaluations += 1\n                \n                # Update personal best\n                if current_score < personal_best_scores[i]:\n                    personal_best_scores[i] = current_score\n                    personal_best_positions[i] = positions[i]\n                \n                # Update global best\n                if current_score < global_best_score:\n                    global_best_score = current_score\n                    global_best_position = positions[i]\n                    global_best_history.append(global_best_position)\n\n            # Update velocities and positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                r1, r2 = np.random.rand(2, self.dim)\n                memory_window = max(5, len(global_best_history) // 4)\n                memory_influence = np.mean(global_best_history[-memory_window:], axis=0)\n                feedback_factor = np.tanh((global_best_score - current_score) / (global_best_score + 1e-10)) * (0.95 ** evaluations)\n                velocities[i] = (self.inertia_weight * velocities[i] +\n                                 self.cognitive_const * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.social_const * r2 * (memory_influence - positions[i]) + feedback_factor)\n                velocities[i] *= 0.95  # More adaptive velocity scaling\n                scale_factor = 0.5 + 0.0001 * evaluations  # dynamic scaling based on evaluations\n                positions[i] = np.clip(positions[i] + velocities[i] * scale_factor, lb, ub)  # Using dynamic constriction factor\n\n            # Adaptive inertia weight decay\n            self.inertia_weight *= 0.99\n            feedback_factor *= (1 - 0.1 * evaluations / self.budget)  # Learning rate decay for feedback\n\n            # Adaptive cognitive and social constants\n            progress_ratio = evaluations / self.budget\n            self.cognitive_const = 1.5 - 0.5 * progress_ratio\n            self.social_const = 1.5 + 0.5 * progress_ratio\n            \n        return global_best_position", "name": "NovelSwarmOptimizer", "description": "Enhanced convergence speed by introducing a learning rate decay for the feedback factor.", "configspace": "", "generation": 71, "fitness": 0.13375084791939398, "feedback": "The algorithm NovelSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.13 with standard deviation 0.00.", "error": "", "parent_id": "199d3c44-df1b-4cd3-a392-4d0cb37463c2", "metadata": {"aucs": [0.13324633700110022, 0.1347665513959646, 0.13323965536111715]}, "mutation_prompt": null}
{"id": "3310d6a2-b4d5-47cc-83e7-ed8c7921449a", "solution": "import numpy as np\n\nclass NovelSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particle_count = min(30, budget // dim)\n        self.inertia_weight = 0.729  # inertia weight\n        self.cognitive_const = 1.49445  # cognitive constant\n        self.social_const = 1.49445  # social constant\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        # Initialize particles' positions and velocities\n        positions = np.random.uniform(lb, ub, (self.particle_count, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particle_count, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.particle_count, np.inf)\n\n        global_best_position = None\n        global_best_score = np.inf\n\n        evaluations = 0\n        global_best_history = []\n\n        while evaluations < self.budget:\n            # Evaluate current positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                current_score = func(positions[i])\n                evaluations += 1\n                \n                # Update personal best\n                if current_score < personal_best_scores[i]:\n                    personal_best_scores[i] = current_score\n                    personal_best_positions[i] = positions[i]\n                \n                # Update global best\n                if current_score < global_best_score:\n                    global_best_score = current_score\n                    global_best_position = positions[i]\n                    global_best_history.append(global_best_position)\n\n            # Update velocities and positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                r1, r2 = np.random.rand(2, self.dim)\n                memory_window = max(5, len(global_best_history) // 4)\n                # Adjusted the method for calculating memory influence\n                memory_influence = np.average(global_best_history[-memory_window:], axis=0, weights=np.exp(np.arange(memory_window)))\n                feedback_factor = np.tanh((global_best_score - current_score) / (global_best_score + 1e-10)) * (0.95 ** evaluations)\n                velocities[i] = (self.inertia_weight * velocities[i] +\n                                 self.cognitive_const * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.social_const * r2 * (memory_influence - positions[i]) + feedback_factor)\n                velocities[i] *= 0.95  # More adaptive velocity scaling\n                scale_factor = 0.5 + 0.0001 * evaluations  # dynamic scaling based on evaluations\n                positions[i] = np.clip(positions[i] + velocities[i] * scale_factor, lb, ub)  # Using dynamic constriction factor\n\n            # Adaptive inertia weight decay\n            self.inertia_weight *= 0.99\n\n            # Adaptive cognitive and social constants\n            progress_ratio = evaluations / self.budget\n            self.cognitive_const = 1.5 - 0.5 * progress_ratio\n            self.social_const = 1.5 + 0.5 * progress_ratio\n            \n        return global_best_position", "name": "NovelSwarmOptimizer", "description": "Enhanced memory influence by incorporating exponentially weighted moving averages to improve convergence.", "configspace": "", "generation": 72, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('Length of weights not compatible with specified axis.').", "error": "ValueError('Length of weights not compatible with specified axis.')", "parent_id": "199d3c44-df1b-4cd3-a392-4d0cb37463c2", "metadata": {}, "mutation_prompt": null}
{"id": "6bd0c022-f499-4337-91b3-fbb89195faab", "solution": "import numpy as np\n\nclass NovelSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particle_count = min(30, budget // dim)\n        self.inertia_weight = 0.729  # inertia weight\n        self.cognitive_const = 1.49445  # cognitive constant\n        self.social_const = 1.49445  # social constant\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        # Initialize particles' positions and velocities\n        positions = np.random.uniform(lb, ub, (self.particle_count, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particle_count, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.particle_count, np.inf)\n\n        global_best_position = None\n        global_best_score = np.inf\n\n        evaluations = 0\n        global_best_history = []\n\n        while evaluations < self.budget:\n            # Evaluate current positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                current_score = func(positions[i])\n                evaluations += 1\n                \n                # Update personal best\n                if current_score < personal_best_scores[i]:\n                    personal_best_scores[i] = current_score\n                    personal_best_positions[i] = positions[i]\n                \n                # Update global best\n                if current_score < global_best_score:\n                    global_best_score = current_score\n                    global_best_position = positions[i]\n                    global_best_history.append(global_best_position)\n\n            # Update velocities and positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                r1, r2 = np.random.rand(2, self.dim)\n                memory_window = max(5, len(global_best_history) // 4)\n                memory_influence = np.mean(global_best_history[-memory_window:], axis=0)\n                feedback_factor = np.tanh((global_best_score - current_score) / (global_best_score + 1e-10)) * (0.99 ** evaluations)  # Enhanced feedback factor\n                velocities[i] = (self.inertia_weight * velocities[i] +\n                                 self.cognitive_const * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.social_const * r2 * (memory_influence - positions[i]) + feedback_factor)\n                velocities[i] *= 0.95  # More adaptive velocity scaling\n                scale_factor = 0.5 + 0.0001 * evaluations  # dynamic scaling based on evaluations\n                positions[i] = np.clip(positions[i] + velocities[i] * scale_factor, lb, ub)  # Using dynamic constriction factor\n\n            # Adaptive inertia weight decay\n            self.inertia_weight *= 0.99\n\n            # Adaptive cognitive and social constants\n            progress_ratio = evaluations / self.budget\n            self.cognitive_const = 1.5 - 0.5 * progress_ratio\n            self.social_const = 1.5 + 0.5 * progress_ratio\n            \n        return global_best_position", "name": "NovelSwarmOptimizer", "description": "Enhanced convergence by introducing a feedback factor finely tuned to evaluations.", "configspace": "", "generation": 73, "fitness": 0.13382396117250658, "feedback": "The algorithm NovelSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.13 with standard deviation 0.00.", "error": "", "parent_id": "199d3c44-df1b-4cd3-a392-4d0cb37463c2", "metadata": {"aucs": [0.13316572018831718, 0.13487143273762725, 0.1334347305915753]}, "mutation_prompt": null}
{"id": "f48cd224-f839-43ed-9975-7bb7ed65fa57", "solution": "import numpy as np\n\nclass NovelSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particle_count = min(30, budget // dim)\n        self.inertia_weight = 0.729  # inertia weight\n        self.cognitive_const = 1.49445  # cognitive constant\n        self.social_const = 1.49445  # social constant\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        # Initialize particles' positions and velocities\n        positions = np.random.uniform(lb, ub, (self.particle_count, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particle_count, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.particle_count, np.inf)\n\n        global_best_position = None\n        global_best_score = np.inf\n\n        evaluations = 0\n        global_best_history = []\n\n        while evaluations < self.budget:\n            # Evaluate current positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                current_score = func(positions[i])\n                evaluations += 1\n                \n                # Update personal best\n                if current_score < personal_best_scores[i]:\n                    personal_best_scores[i] = current_score\n                    personal_best_positions[i] = positions[i]\n                \n                # Update global best\n                if current_score < global_best_score:\n                    global_best_score = current_score\n                    global_best_position = positions[i]\n                    global_best_history.append(global_best_position)\n\n            # Update velocities and positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                r1, r2 = np.random.rand(2, self.dim)\n                memory_window = max(5, len(global_best_history) // 4)\n                memory_influence = np.mean(global_best_history[-memory_window:], axis=0)\n                diversity_factor = np.std(positions, axis=0).mean()  # Added diversity factor\n                feedback_factor = np.tanh((global_best_score - current_score) / (global_best_score + 1e-10)) * (0.99 ** evaluations) * diversity_factor  # Enhanced feedback factor\n                velocities[i] = (self.inertia_weight * velocities[i] +\n                                 self.cognitive_const * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.social_const * r2 * (memory_influence - positions[i]) + feedback_factor)\n                velocities[i] *= 0.90  # Adaptive velocity scaling\n                scale_factor = 0.5 + 0.0001 * evaluations  # dynamic scaling based on evaluations\n                positions[i] = np.clip(positions[i] + velocities[i] * scale_factor, lb, ub)  # Using dynamic constriction factor\n\n            # Adaptive inertia weight decay\n            self.inertia_weight *= 0.99\n\n            # Adaptive cognitive and social constants\n            progress_ratio = evaluations / self.budget\n            self.cognitive_const = 1.5 - 0.5 * progress_ratio\n            self.social_const = 1.5 + 0.5 * progress_ratio\n            \n        return global_best_position", "name": "NovelSwarmOptimizer", "description": "Improved adaptability by adjusting feedback factor and velocity scaling based on particle diversity.", "configspace": "", "generation": 74, "fitness": 0.13149141249801508, "feedback": "The algorithm NovelSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.13 with standard deviation 0.00.", "error": "", "parent_id": "6bd0c022-f499-4337-91b3-fbb89195faab", "metadata": {"aucs": [0.12819741742305024, 0.13320461368240843, 0.13307220638858652]}, "mutation_prompt": null}
{"id": "503e40ed-7c2e-4e87-a578-3e7520c1a0df", "solution": "import numpy as np\n\nclass NovelSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particle_count = min(30, budget // dim)\n        self.inertia_weight = 0.729  # inertia weight\n        self.cognitive_const = 1.49445  # cognitive constant\n        self.social_const = 1.49445  # social constant\n        self.momentum_factor = 0.9  # Added momentum factor\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        # Initialize particles' positions and velocities\n        positions = np.random.uniform(lb, ub, (self.particle_count, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particle_count, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.particle_count, np.inf)\n\n        global_best_position = None\n        global_best_score = np.inf\n\n        evaluations = 0\n        global_best_history = []\n\n        while evaluations < self.budget:\n            # Evaluate current positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                current_score = func(positions[i])\n                evaluations += 1\n                \n                # Update personal best\n                if current_score < personal_best_scores[i]:\n                    personal_best_scores[i] = current_score\n                    personal_best_positions[i] = positions[i]\n                \n                # Update global best\n                if current_score < global_best_score:\n                    global_best_score = current_score\n                    global_best_position = positions[i]\n                    global_best_history.append(global_best_position)\n\n            # Update velocities and positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                r1, r2 = np.random.rand(2, self.dim)\n                memory_window = max(5, len(global_best_history) // 4)\n                memory_influence = np.mean(global_best_history[-memory_window:], axis=0)\n                feedback_factor = np.tanh((global_best_score - current_score) / (global_best_score + 1e-10)) * (0.99 ** evaluations)  # Enhanced feedback factor\n                velocities[i] = (self.inertia_weight * velocities[i] +\n                                 self.cognitive_const * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.social_const * r2 * (memory_influence - positions[i]) + feedback_factor)\n                velocities[i] = self.momentum_factor * velocities[i] + (1 - self.momentum_factor) * np.random.uniform(-1, 1, self.dim)  # Apply momentum\n                velocities[i] *= 0.95  # More adaptive velocity scaling\n                scale_factor = 0.5 + 0.0001 * evaluations  # dynamic scaling based on evaluations\n                positions[i] = np.clip(positions[i] + velocities[i] * scale_factor, lb, ub)  # Using dynamic constriction factor\n\n            # Adaptive inertia weight decay\n            self.inertia_weight *= 0.99\n\n            # Adaptive cognitive and social constants\n            progress_ratio = evaluations / self.budget\n            self.cognitive_const = 1.5 - 0.5 * progress_ratio\n            self.social_const = 1.5 + 0.5 * progress_ratio\n            \n        return global_best_position", "name": "NovelSwarmOptimizer", "description": "Introduce momentum factor to stabilize velocity updates and improve convergence.", "configspace": "", "generation": 75, "fitness": 0.13313496137477507, "feedback": "The algorithm NovelSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.13 with standard deviation 0.00.", "error": "", "parent_id": "6bd0c022-f499-4337-91b3-fbb89195faab", "metadata": {"aucs": [0.13097520085887193, 0.1352867932390459, 0.13314289002640733]}, "mutation_prompt": null}
{"id": "d4572ae5-014b-424a-bf8d-483903cf2615", "solution": "import numpy as np\n\nclass NovelSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particle_count = min(30, budget // dim)\n        self.inertia_weight = 0.729  # inertia weight\n        self.cognitive_const = 1.49445  # cognitive constant\n        self.social_const = 1.49445  # social constant\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        # Initialize particles' positions and velocities\n        positions = np.random.uniform(lb, ub, (self.particle_count, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particle_count, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.particle_count, np.inf)\n\n        global_best_position = None\n        global_best_score = np.inf\n\n        evaluations = 0\n        global_best_history = []\n\n        while evaluations < self.budget:\n            # Evaluate current positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                current_score = func(positions[i])\n                evaluations += 1\n                \n                # Update personal best\n                if current_score < personal_best_scores[i]:\n                    personal_best_scores[i] = current_score\n                    personal_best_positions[i] = positions[i]\n                \n                # Update global best\n                if current_score < global_best_score:\n                    global_best_score = current_score\n                    global_best_position = positions[i]\n                    global_best_history.append(global_best_position)\n\n            # Update velocities and positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                r1, r2 = np.random.rand(2, self.dim)\n                memory_window = max(5, len(global_best_history) // 4)\n                memory_influence = np.mean(global_best_history[-memory_window:], axis=0)\n                feedback_factor = np.tanh((global_best_score - current_score) / (global_best_score + 1e-10)) * (0.99 ** evaluations)  # Enhanced feedback factor\n                velocities[i] = (self.inertia_weight * velocities[i] +\n                                 self.cognitive_const * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.social_const * r2 * (memory_influence - positions[i]) + feedback_factor)\n                velocities[i] *= 0.95  # More adaptive velocity scaling\n                scale_factor = min(0.6, 0.5 + 0.0001 * evaluations)  # dynamic scaling with a ceiling\n                positions[i] = np.clip(positions[i] + velocities[i] * scale_factor, lb, ub)  # Using dynamic constriction factor\n\n            # Adaptive inertia weight decay\n            self.inertia_weight *= 0.99\n\n            # Adaptive cognitive and social constants\n            progress_ratio = evaluations / self.budget\n            self.cognitive_const = 1.5 - 0.5 * progress_ratio\n            self.social_const = 1.5 + 0.5 * progress_ratio\n            \n        return global_best_position", "name": "NovelSwarmOptimizer", "description": "Introduced a ceiling to the dynamic constriction factor to stabilize the convergence rate and prevent overshooting.", "configspace": "", "generation": 76, "fitness": 0.13382396117250658, "feedback": "The algorithm NovelSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.13 with standard deviation 0.00.", "error": "", "parent_id": "6bd0c022-f499-4337-91b3-fbb89195faab", "metadata": {"aucs": [0.13316572018831718, 0.13487143273762725, 0.1334347305915753]}, "mutation_prompt": null}
{"id": "8f707b40-7869-49f8-ad0e-d4ba9638dabd", "solution": "import numpy as np\n\nclass NovelSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particle_count = min(30, budget // dim)\n        self.inertia_weight = 0.729  # inertia weight\n        self.cognitive_const = 1.49445  # cognitive constant\n        self.social_const = 1.49445  # social constant\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        # Initialize particles' positions and velocities\n        positions = np.random.uniform(lb, ub, (self.particle_count, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particle_count, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.particle_count, np.inf)\n\n        global_best_position = None\n        global_best_score = np.inf\n\n        evaluations = 0\n        global_best_history = []\n\n        while evaluations < self.budget:\n            # Evaluate current positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                current_score = func(positions[i])\n                evaluations += 1\n                \n                # Update personal best\n                if current_score < personal_best_scores[i]:\n                    personal_best_scores[i] = current_score\n                    personal_best_positions[i] = positions[i]\n                \n                # Update global best\n                if current_score < global_best_score:\n                    global_best_score = current_score\n                    global_best_position = positions[i]\n                    global_best_history.append(global_best_position)\n\n            # Update velocities and positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                r1, r2 = np.random.rand(2, self.dim)\n                memory_window = max(5, len(global_best_history) // 4)\n                memory_influence = np.mean(global_best_history[-memory_window:], axis=0)\n                feedback_factor = np.tanh((global_best_score - current_score) / (global_best_score + 1e-10)) * (0.99 ** evaluations)  # Enhanced feedback factor\n                velocities[i] = (self.inertia_weight * velocities[i] +\n                                 self.cognitive_const * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.social_const * r2 * (memory_influence - positions[i]) + feedback_factor)\n                velocities[i] *= 0.93  # More adaptive velocity scaling\n                scale_factor = 0.6 + 0.0001 * evaluations  # dynamic scaling based on evaluations\n                positions[i] = np.clip(positions[i] + velocities[i] * scale_factor, lb, ub)  # Using dynamic constriction factor\n\n            # Adaptive inertia weight decay\n            self.inertia_weight *= 0.98\n\n            # Adaptive cognitive and social constants\n            progress_ratio = evaluations / self.budget\n            self.cognitive_const = 1.5 - 0.5 * progress_ratio\n            self.social_const = 1.5 + 0.5 * progress_ratio\n            \n        return global_best_position", "name": "NovelSwarmOptimizer", "description": "Enhanced exploration by incorporating memory influence and dynamic constriction factor for improved convergence.", "configspace": "", "generation": 77, "fitness": 0.13296710812825485, "feedback": "The algorithm NovelSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.13 with standard deviation 0.00.", "error": "", "parent_id": "6bd0c022-f499-4337-91b3-fbb89195faab", "metadata": {"aucs": [0.13315272030539826, 0.13389309438058172, 0.13185550969878457]}, "mutation_prompt": null}
{"id": "f9c8bedc-9cff-4d66-8f60-62259b9f30f8", "solution": "import numpy as np\n\nclass NovelSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particle_count = min(30, budget // dim)\n        self.inertia_weight = 0.9  # Modified initial inertia weight\n        self.cognitive_const = 1.49445  # cognitive constant\n        self.social_const = 1.49445  # social constant\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        positions = np.random.uniform(lb, ub, (self.particle_count, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particle_count, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.particle_count, np.inf)\n\n        global_best_position = None\n        global_best_score = np.inf\n\n        evaluations = 0\n        global_best_history = []\n\n        while evaluations < self.budget:\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                current_score = func(positions[i])\n                evaluations += 1\n                \n                if current_score < personal_best_scores[i]:\n                    personal_best_scores[i] = current_score\n                    personal_best_positions[i] = positions[i]\n                \n                if current_score < global_best_score:\n                    global_best_score = current_score\n                    global_best_position = positions[i]\n                    global_best_history.append(global_best_position)\n\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                r1, r2 = np.random.rand(2, self.dim)\n                memory_window = max(5, len(global_best_history) // 4)\n                memory_influence = np.mean(global_best_history[-memory_window:], axis=0)\n                feedback_factor = np.tanh((global_best_score - current_score) / (global_best_score + 1e-10)) * (0.99 ** evaluations)\n                velocities[i] = (self.inertia_weight * velocities[i] +\n                                 self.cognitive_const * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.social_const * r2 * (memory_influence - positions[i]) + feedback_factor)\n                velocities[i] *= 0.95\n                scale_factor = 0.5 + 0.0001 * evaluations\n                positions[i] = np.clip(positions[i] + velocities[i] * scale_factor, lb, ub)\n\n            self.inertia_weight = 0.9 - 0.5 * (evaluations / self.budget)  # Dynamic inertia adjustment\n\n            progress_ratio = evaluations / self.budget\n            self.cognitive_const = 1.5 - 0.3 * progress_ratio  # Smaller range for cognitive adjustment\n            self.social_const = 1.5 + 0.3 * progress_ratio  # Smaller range for social adjustment\n            \n        return global_best_position", "name": "NovelSwarmOptimizer", "description": "Introduce a dynamic inertia weight and cognitive-social balance to improve convergence efficiency.", "configspace": "", "generation": 78, "fitness": 0.1329210670571175, "feedback": "The algorithm NovelSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.13 with standard deviation 0.00.", "error": "", "parent_id": "6bd0c022-f499-4337-91b3-fbb89195faab", "metadata": {"aucs": [0.1304082039215272, 0.1348843694954024, 0.13347062775442298]}, "mutation_prompt": null}
{"id": "580ac556-0b32-4fcd-ac71-d9ce6a53abaf", "solution": "import numpy as np\n\nclass NovelSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particle_count = min(30, budget // dim)\n        self.inertia_weight = 0.729  # inertia weight\n        self.cognitive_const = 1.49445  # cognitive constant\n        self.social_const = 1.49445  # social constant\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        # Initialize particles' positions and velocities\n        positions = np.random.uniform(lb, ub, (self.particle_count, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particle_count, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.particle_count, np.inf)\n\n        global_best_position = None\n        global_best_score = np.inf\n\n        evaluations = 0\n        global_best_history = []\n\n        while evaluations < self.budget:\n            # Evaluate current positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                current_score = func(positions[i])\n                evaluations += 1\n                \n                # Update personal best\n                if current_score < personal_best_scores[i]:\n                    personal_best_scores[i] = current_score\n                    personal_best_positions[i] = positions[i]\n                \n                # Update global best\n                if current_score < global_best_score:\n                    global_best_score = current_score\n                    global_best_position = positions[i]\n                    global_best_history.append(global_best_position)\n\n            # Update velocities and positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                r1, r2 = np.random.rand(2, self.dim)\n                memory_window = max(5, len(global_best_history) // 4)\n                memory_influence = np.mean(global_best_history[-memory_window:], axis=0)\n                feedback_factor = np.tanh((global_best_score - current_score) / (global_best_score + 1e-10)) * (0.95 ** evaluations)  # Enhanced feedback factor\n                velocities[i] = (self.inertia_weight * velocities[i] +\n                                 self.cognitive_const * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.social_const * r2 * (memory_influence - positions[i]) + feedback_factor)\n                velocities[i] *= 0.92  # More adaptive velocity scaling\n                scale_factor = 0.5 + 0.0002 * evaluations  # dynamic scaling based on evaluations\n                positions[i] = np.clip(positions[i] + velocities[i] * scale_factor, lb, ub)  # Using dynamic constriction factor\n\n            # Adaptive inertia weight decay\n            self.inertia_weight *= 0.99\n\n            # Adaptive cognitive and social constants\n            progress_ratio = evaluations / self.budget\n            self.cognitive_const = 1.5 - 0.5 * progress_ratio\n            self.social_const = 1.5 + 0.5 * progress_ratio\n            \n        return global_best_position", "name": "NovelSwarmOptimizer", "description": "Refined swarm intelligence using adaptive learning rates and improved fitness feedback mechanism.", "configspace": "", "generation": 79, "fitness": 0.13374819599922258, "feedback": "The algorithm NovelSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.13 with standard deviation 0.00.", "error": "", "parent_id": "6bd0c022-f499-4337-91b3-fbb89195faab", "metadata": {"aucs": [0.1330133678270492, 0.135127781390612, 0.13310343878000652]}, "mutation_prompt": null}
{"id": "d8409ef3-48b2-4221-829e-19bf0d4e9561", "solution": "import numpy as np\n\nclass NovelSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particle_count = min(30, budget // dim)\n        self.inertia_weight = 0.729  # inertia weight\n        self.cognitive_const = 1.49445  # cognitive constant\n        self.social_const = 1.49445  # social constant\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        # Initialize particles' positions and velocities\n        positions = np.random.uniform(lb, ub, (self.particle_count, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particle_count, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.particle_count, np.inf)\n\n        global_best_position = None\n        global_best_score = np.inf\n\n        evaluations = 0\n        global_best_history = []\n\n        while evaluations < self.budget:\n            # Evaluate current positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                current_score = func(positions[i])\n                evaluations += 1\n                \n                # Update personal best\n                if current_score < personal_best_scores[i]:\n                    personal_best_scores[i] = current_score\n                    personal_best_positions[i] = positions[i]\n                \n                # Update global best\n                if current_score < global_best_score:\n                    global_best_score = current_score\n                    global_best_position = positions[i]\n                    global_best_history.append(global_best_position)\n\n            # Update velocities and positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                r1, r2 = np.random.rand(2, self.dim)\n                memory_window = max(5, len(global_best_history) // 4)\n                memory_influence = np.mean(global_best_history[-memory_window:], axis=0)\n                feedback_factor = np.tanh((global_best_score - current_score) / (global_best_score + 1e-10)) * (0.98 ** evaluations)  # Enhanced feedback factor\n                velocities[i] = (self.inertia_weight * velocities[i] +\n                                 self.cognitive_const * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.social_const * r2 * (memory_influence - positions[i]) + feedback_factor)\n                velocities[i] *= 0.9  # More adaptive velocity scaling\n                scale_factor = 0.5 + 0.0001 * evaluations  # dynamic scaling based on evaluations\n                positions[i] = np.clip(positions[i] + velocities[i] * scale_factor, lb, ub)  # Using dynamic constriction factor\n\n            # Adaptive inertia weight decay\n            self.inertia_weight *= 0.99\n\n            # Adaptive cognitive and social constants\n            progress_ratio = evaluations / self.budget\n            self.cognitive_const = 1.5 - 0.5 * progress_ratio\n            self.social_const = 1.5 + 0.5 * progress_ratio\n            \n        return global_best_position", "name": "NovelSwarmOptimizer", "description": "Improved adaptive strategy by tweaking velocity scaling and feedback factor for enhanced convergence.", "configspace": "", "generation": 80, "fitness": 0.13368145832423906, "feedback": "The algorithm NovelSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.13 with standard deviation 0.00.", "error": "", "parent_id": "6bd0c022-f499-4337-91b3-fbb89195faab", "metadata": {"aucs": [0.13267990741092772, 0.1350607605317543, 0.13330370703003513]}, "mutation_prompt": null}
{"id": "cb06e2c9-0f5c-42b1-814f-019a6f1c61e1", "solution": "import numpy as np\n\nclass NovelSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particle_count = min(30, budget // dim)\n        self.inertia_weight = 0.729  # inertia weight\n        self.cognitive_const = 1.49445  # cognitive constant\n        self.social_const = 1.49445  # social constant\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        # Initialize particles' positions and velocities\n        positions = np.random.uniform(lb, ub, (self.particle_count, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particle_count, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.particle_count, np.inf)\n\n        global_best_position = None\n        global_best_score = np.inf\n\n        evaluations = 0\n        global_best_history = []\n\n        while evaluations < self.budget:\n            # Evaluate current positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                current_score = func(positions[i])\n                evaluations += 1\n                \n                # Update personal best\n                if current_score < personal_best_scores[i]:\n                    personal_best_scores[i] = current_score\n                    personal_best_positions[i] = positions[i]\n                \n                # Update global best\n                if current_score < global_best_score:\n                    global_best_score = current_score\n                    global_best_position = positions[i]\n                    global_best_history.append(global_best_position)\n\n            # Update velocities and positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                r1, r2 = np.random.rand(2, self.dim)\n                memory_window = max(5, len(global_best_history) // 4)\n                memory_influence = np.mean(global_best_history[-memory_window:], axis=0)\n                feedback_factor = np.tanh((global_best_score - current_score) / (global_best_score + 1e-10)) * (0.99 ** evaluations)  # Enhanced feedback factor\n                velocities[i] = (self.inertia_weight * velocities[i] +\n                                 self.cognitive_const * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.social_const * r2 * (memory_influence - positions[i]) + feedback_factor)\n                velocities[i] *= np.exp(-0.001 * evaluations)  # Non-linear deceleration factor\n                scale_factor = 0.5 + 0.0001 * evaluations  # dynamic scaling based on evaluations\n                positions[i] = np.clip(positions[i] + velocities[i] * scale_factor, lb, ub)  # Using dynamic constriction factor\n\n            # Adaptive inertia weight decay\n            self.inertia_weight *= 0.99\n\n            # Adaptive cognitive and social constants\n            progress_ratio = evaluations / self.budget\n            self.cognitive_const = 1.5 - 0.7 * progress_ratio  # Adjusted balance\n            self.social_const = 1.5 + 0.3 * progress_ratio  # Adjusted balance\n            \n        return global_best_position", "name": "NovelSwarmOptimizer", "description": "Improved convergence by introducing a non-linear deceleration factor and adaptive social-cognitive balance.", "configspace": "", "generation": 81, "fitness": 0.13303201501129538, "feedback": "The algorithm NovelSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.13 with standard deviation 0.00.", "error": "", "parent_id": "6bd0c022-f499-4337-91b3-fbb89195faab", "metadata": {"aucs": [0.13058142365201075, 0.13483752467207566, 0.13367709670979977]}, "mutation_prompt": null}
{"id": "4c86e396-7a25-4f05-a329-f647fa2ca996", "solution": "import numpy as np\n\nclass NovelSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particle_count = min(30, budget // dim)\n        self.inertia_weight = 0.729  # inertia weight\n        self.cognitive_const = 1.49445  # cognitive constant\n        self.social_const = 1.49445  # social constant\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        # Initialize particles' positions and velocities\n        positions = np.random.uniform(lb, ub, (self.particle_count, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particle_count, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.particle_count, np.inf)\n\n        global_best_position = None\n        global_best_score = np.inf\n\n        evaluations = 0\n        global_best_history = []\n\n        while evaluations < self.budget:\n            # Evaluate current positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                current_score = func(positions[i])\n                evaluations += 1\n                \n                # Update personal best\n                if current_score < personal_best_scores[i]:\n                    personal_best_scores[i] = current_score\n                    personal_best_positions[i] = positions[i]\n                \n                # Update global best\n                if current_score < global_best_score:\n                    global_best_score = current_score\n                    global_best_position = positions[i]\n                    global_best_history.append(global_best_position)\n\n            # Update velocities and positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                r1, r2 = np.random.rand(2, self.dim)\n                memory_window = max(5, len(global_best_history) // 4)\n                memory_influence = np.mean(global_best_history[-memory_window:], axis=0)\n                feedback_factor = np.tanh((global_best_score - current_score) / (global_best_score + 1e-10)) * (0.98 ** evaluations)  # Enhanced feedback factor\n                velocities[i] = (self.inertia_weight * velocities[i] +\n                                 self.cognitive_const * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.social_const * r2 * (memory_influence - positions[i]) + feedback_factor)\n                velocities[i] *= 0.92  # More adaptive velocity scaling\n                scale_factor = 0.5 + 0.0001 * evaluations  # dynamic scaling based on evaluations\n                positions[i] = np.clip(positions[i] + velocities[i] * scale_factor, lb, ub)  # Using dynamic constriction factor\n\n            # Adaptive inertia weight decay\n            self.inertia_weight *= 0.98\n\n            # Adaptive cognitive and social constants\n            progress_ratio = evaluations / self.budget\n            self.cognitive_const = 1.5 - 0.5 * progress_ratio\n            self.social_const = 1.5 + 0.5 * progress_ratio\n            \n        return global_best_position", "name": "NovelSwarmOptimizer", "description": "Improved global exploration by adapting the inertia weight scaling and feedback factor.", "configspace": "", "generation": 82, "fitness": 0.13362277381392682, "feedback": "The algorithm NovelSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.13 with standard deviation 0.00.", "error": "", "parent_id": "6bd0c022-f499-4337-91b3-fbb89195faab", "metadata": {"aucs": [0.13259073748640748, 0.1352207821156448, 0.13305680183972812]}, "mutation_prompt": null}
{"id": "eaad5036-8e2f-41fb-a088-48d72acc28e5", "solution": "import numpy as np\n\nclass NovelSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particle_count = min(30, budget // dim)\n        self.inertia_weight = 0.729  # inertia weight\n        self.cognitive_const = 1.49445  # cognitive constant\n        self.social_const = 1.49445  # social constant\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        # Initialize particles' positions and velocities\n        positions = np.random.uniform(lb, ub, (self.particle_count, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particle_count, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.particle_count, np.inf)\n\n        global_best_position = None\n        global_best_score = np.inf\n\n        evaluations = 0\n        global_best_history = []\n\n        while evaluations < self.budget:\n            # Evaluate current positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                current_score = func(positions[i])\n                evaluations += 1\n                \n                # Update personal best\n                if current_score < personal_best_scores[i]:\n                    personal_best_scores[i] = current_score\n                    personal_best_positions[i] = positions[i]\n                \n                # Update global best\n                if current_score < global_best_score:\n                    global_best_score = current_score\n                    global_best_position = positions[i]\n                    global_best_history.append(global_best_position)\n\n            # Update velocities and positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                r1, r2 = np.random.rand(2, self.dim)\n                memory_window = max(5, len(global_best_history) // 4)\n                memory_influence = np.mean(global_best_history[-memory_window:], axis=0)\n                feedback_factor = np.tanh((global_best_score - current_score) / (global_best_score + 1e-10)) * (0.99 ** evaluations)  # Enhanced feedback factor\n                velocities[i] = (self.inertia_weight * velocities[i] +\n                                 self.cognitive_const * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.social_const * r2 * (memory_influence - positions[i]) + feedback_factor)\n                velocities[i] *= 0.95  # More adaptive velocity scaling\n                scale_factor = 0.5 + 0.0001 * evaluations  # dynamic scaling based on evaluations\n                positions[i] = np.clip(positions[i] + velocities[i] * scale_factor, lb, ub)  # Using dynamic constriction factor\n                # Introduce adaptive random mutations\n                mutation_prob = 0.01 + 0.1 * (1 - progress_ratio)\n                if np.random.rand() < mutation_prob:\n                    mutation_strength = np.random.normal(0, 0.1 * (ub - lb), self.dim)\n                    positions[i] = np.clip(positions[i] + mutation_strength, lb, ub)\n\n            # Adaptive inertia weight decay\n            self.inertia_weight *= 0.99\n\n            # Adaptive cognitive and social constants\n            progress_ratio = evaluations / self.budget\n            self.cognitive_const = 1.5 - 0.5 * progress_ratio\n            self.social_const = 1.5 + 0.5 * progress_ratio\n            \n        return global_best_position", "name": "NovelSwarmOptimizer", "description": "Enhanced swarm diversity by introducing adaptive random mutations to improve exploration.", "configspace": "", "generation": 83, "fitness": -Infinity, "feedback": "An exception occurred: UnboundLocalError(\"local variable 'progress_ratio' referenced before assignment\").", "error": "UnboundLocalError(\"local variable 'progress_ratio' referenced before assignment\")", "parent_id": "6bd0c022-f499-4337-91b3-fbb89195faab", "metadata": {}, "mutation_prompt": null}
{"id": "2b7c1dfe-809a-462a-8073-842b53fe856c", "solution": "import numpy as np\n\nclass NovelSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particle_count = min(30, budget // dim)\n        self.inertia_weight = 0.9  # inertia weight\n        self.cognitive_const = 1.49445  # cognitive constant\n        self.social_const = 1.49445  # social constant\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        positions = np.random.uniform(lb, ub, (self.particle_count, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particle_count, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.particle_count, np.inf)\n\n        global_best_position = None\n        global_best_score = np.inf\n\n        evaluations = 0\n        global_best_history = []\n\n        while evaluations < self.budget:\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                current_score = func(positions[i])\n                evaluations += 1\n                \n                if current_score < personal_best_scores[i]:\n                    personal_best_scores[i] = current_score\n                    personal_best_positions[i] = positions[i]\n                \n                if current_score < global_best_score:\n                    global_best_score = current_score\n                    global_best_position = positions[i]\n                    global_best_history.append(global_best_position)\n\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                r1, r2 = np.random.rand(2, self.dim)\n                memory_window = max(5, len(global_best_history) // 4)\n                memory_influence = np.mean(global_best_history[-memory_window:], axis=0)\n                feedback_factor = np.tanh((global_best_score - current_score) / (global_best_score + 1e-10)) * (0.97 ** evaluations)  # Enhanced feedback factor\n                velocities[i] = (self.inertia_weight * velocities[i] +\n                                 self.cognitive_const * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.social_const * r2 * (memory_influence - positions[i]) + feedback_factor)\n                velocities[i] *= 0.92  # More adaptive velocity scaling\n                scale_factor = 0.5 + 0.0001 * evaluations  \n                positions[i] = np.clip(positions[i] + velocities[i] * scale_factor, lb, ub)\n\n            self.inertia_weight *= 0.995  # Slower inertia decay\n\n            progress_ratio = evaluations / self.budget\n            self.cognitive_const = 1.5 - 0.5 * progress_ratio\n            self.social_const = 1.5 + 0.5 * progress_ratio\n            \n        return global_best_position", "name": "NovelSwarmOptimizer", "description": "Improved adaptive feedback mechanism with dynamic inertia weight adjustment for enhanced global exploration.", "configspace": "", "generation": 84, "fitness": 0.13273824092000006, "feedback": "The algorithm NovelSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.13 with standard deviation 0.00.", "error": "", "parent_id": "6bd0c022-f499-4337-91b3-fbb89195faab", "metadata": {"aucs": [0.1308504801261876, 0.1345763831241631, 0.13278785950964944]}, "mutation_prompt": null}
{"id": "da92796c-6b42-4306-9cb4-aa9e904bdec6", "solution": "import numpy as np\n\nclass NovelSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particle_count = min(30, budget // dim)\n        self.inertia_weight = 0.729  # inertia weight\n        self.cognitive_const = 1.49445  # cognitive constant\n        self.social_const = 1.49445  # social constant\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        # Initialize particles' positions and velocities\n        positions = np.random.uniform(lb, ub, (self.particle_count, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particle_count, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.particle_count, np.inf)\n\n        global_best_position = None\n        global_best_score = np.inf\n\n        evaluations = 0\n        global_best_history = []\n\n        while evaluations < self.budget:\n            # Evaluate current positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                current_score = func(positions[i])\n                evaluations += 1\n                \n                # Update personal best\n                if current_score < personal_best_scores[i]:\n                    personal_best_scores[i] = current_score\n                    personal_best_positions[i] = positions[i]\n                \n                # Update global best\n                if current_score < global_best_score:\n                    global_best_score = current_score\n                    global_best_position = positions[i]\n                    global_best_history.append(global_best_position)\n\n            # Update velocities and positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                r1, r2 = np.random.rand(2, self.dim)\n                memory_window = max(5, len(global_best_history) // 4)\n                memory_influence = np.mean(global_best_history[-memory_window:], axis=0)\n                feedback_factor = np.tanh((global_best_score - current_score) / (global_best_score + 1e-10)) * (0.99 ** evaluations)  # Enhanced feedback factor\n                neighbor_influence = np.mean(positions[max(0, i-2):min(self.particle_count, i+3)], axis=0)  # Adaptive neighborhood learning\n                velocities[i] = (self.inertia_weight * velocities[i] +\n                                 self.cognitive_const * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.social_const * r2 * (neighbor_influence - positions[i]) + feedback_factor)\n                velocities[i] *= 0.97  # More adaptive velocity scaling\n                scale_factor = 0.5 + 0.0001 * evaluations  # dynamic scaling based on evaluations\n                positions[i] = np.clip(positions[i] + velocities[i] * scale_factor, lb, ub)  # Using dynamic constriction factor\n\n            # Adaptive inertia weight decay\n            self.inertia_weight *= 0.99\n\n            # Adaptive cognitive and social constants\n            progress_ratio = evaluations / self.budget\n            self.cognitive_const = 1.5 - 0.5 * progress_ratio\n            self.social_const = 1.5 + 0.5 * progress_ratio\n            \n        return global_best_position", "name": "NovelSwarmOptimizer", "description": "Improved swarm intelligence by introducing adaptive neighborhood-based learning and dynamic velocity scaling.", "configspace": "", "generation": 85, "fitness": 0.12776311769893337, "feedback": "The algorithm NovelSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.13 with standard deviation 0.00.", "error": "", "parent_id": "6bd0c022-f499-4337-91b3-fbb89195faab", "metadata": {"aucs": [0.12911150286825512, 0.1274424380380501, 0.1267354121904949]}, "mutation_prompt": null}
{"id": "c19aa9dc-7f7d-4d5d-aed9-8a42aff11c1d", "solution": "import numpy as np\n\nclass NovelSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particle_count = min(30, budget // dim)\n        self.inertia_weight = 0.729  # inertia weight\n        self.cognitive_const = 1.49445  # cognitive constant\n        self.social_const = 1.49445  # social constant\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        # Initialize particles' positions and velocities\n        positions = np.random.uniform(lb, ub, (self.particle_count, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particle_count, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.particle_count, np.inf)\n\n        global_best_position = None\n        global_best_score = np.inf\n\n        evaluations = 0\n        global_best_history = []\n\n        while evaluations < self.budget:\n            # Evaluate current positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                current_score = func(positions[i])\n                evaluations += 1\n                \n                # Update personal best\n                if current_score < personal_best_scores[i]:\n                    personal_best_scores[i] = current_score\n                    personal_best_positions[i] = positions[i]\n                \n                # Update global best\n                if current_score < global_best_score:\n                    global_best_score = current_score\n                    global_best_position = positions[i]\n                    global_best_history.append(global_best_position)\n\n            # Update velocities and positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                r1, r2 = np.random.rand(2, self.dim)\n                memory_window = max(5, len(global_best_history) // 4)\n                memory_influence = np.mean(global_best_history[-memory_window:], axis=0)\n                feedback_factor = np.tanh((global_best_score - current_score) / (global_best_score + 1e-10)) * (0.99 ** evaluations)  # Enhanced feedback factor\n                velocities[i] = (self.inertia_weight * velocities[i] +\n                                 self.cognitive_const * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.social_const * r2 * (memory_influence - positions[i]) + feedback_factor)\n                velocities[i] *= 0.95  # More adaptive velocity scaling\n                scale_factor = 0.5 + 0.0001 * evaluations  # dynamic scaling based on evaluations\n                damping_factor = 0.9 + 0.1 * (1 - (evaluations / self.budget))  # Dynamic damping factor\n                positions[i] = np.clip(positions[i] + velocities[i] * scale_factor * damping_factor, lb, ub)\n\n            # Adaptive inertia weight decay\n            self.inertia_weight *= 0.99\n\n            # Adaptive cognitive and social constants\n            progress_ratio = evaluations / self.budget\n            self.cognitive_const = 1.5 - 0.5 * progress_ratio\n            self.social_const = 1.5 + 0.5 * progress_ratio\n            \n        return global_best_position", "name": "NovelSwarmOptimizer", "description": "Introducing a dynamic damping factor for velocities to enhance convergence stability.", "configspace": "", "generation": 86, "fitness": 0.1337418638637744, "feedback": "The algorithm NovelSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.13 with standard deviation 0.00.", "error": "", "parent_id": "6bd0c022-f499-4337-91b3-fbb89195faab", "metadata": {"aucs": [0.13322458124060554, 0.1348967346442348, 0.13310427570648287]}, "mutation_prompt": null}
{"id": "5a133cf0-a8b3-4972-9205-9304d9ab5af8", "solution": "import numpy as np\n\nclass NovelSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particle_count = min(30, budget // dim)\n        self.inertia_weight = 0.729  # inertia weight\n        self.cognitive_const = 1.49445  # cognitive constant\n        self.social_const = 1.49445  # social constant\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        # Initialize particles' positions and velocities\n        positions = np.random.uniform(lb, ub, (self.particle_count, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particle_count, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.particle_count, np.inf)\n\n        global_best_position = None\n        global_best_score = np.inf\n\n        evaluations = 0\n        global_best_history = []\n\n        while evaluations < self.budget:\n            # Evaluate current positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                current_score = func(positions[i])\n                evaluations += 1\n                \n                # Update personal best\n                if current_score < personal_best_scores[i]:\n                    personal_best_scores[i] = current_score\n                    personal_best_positions[i] = positions[i]\n                \n                # Update global best\n                if current_score < global_best_score:\n                    global_best_score = current_score\n                    global_best_position = positions[i]\n                    global_best_history.append(global_best_position)\n\n            # Update velocities and positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                r1, r2 = np.random.rand(2, self.dim)\n                memory_window = max(5, len(global_best_history) // 4)\n                memory_influence = np.mean(global_best_history[-memory_window:], axis=0)\n                history_factor = np.linalg.norm(positions[i] - memory_influence) / (ub - lb).mean()  # New line: history-based adjustment\n                feedback_factor = np.tanh((global_best_score - current_score) / (global_best_score + 1e-10)) * (0.99 ** evaluations)\n                velocities[i] = (self.inertia_weight * velocities[i] +\n                                 self.cognitive_const * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.social_const * r2 * (memory_influence - positions[i]) + feedback_factor)\n                velocities[i] *= 0.95 * (1 + 0.05 * history_factor)  # Modified line: incorporate history factor\n                scale_factor = 0.5 + 0.0001 * evaluations\n                positions[i] = np.clip(positions[i] + velocities[i] * scale_factor, lb, ub)\n\n            # Adaptive inertia weight decay\n            self.inertia_weight *= 0.99\n\n            # Adaptive cognitive and social constants\n            progress_ratio = evaluations / self.budget\n            self.cognitive_const = 1.5 - 0.5 * progress_ratio\n            self.social_const = 1.5 + 0.5 * progress_ratio\n            \n        return global_best_position", "name": "NovelSwarmOptimizer", "description": "Introducing history-based velocity adjustment for enhanced exploration and exploitation balance.", "configspace": "", "generation": 87, "fitness": 0.13309192264951056, "feedback": "The algorithm NovelSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.13 with standard deviation 0.00.", "error": "", "parent_id": "6bd0c022-f499-4337-91b3-fbb89195faab", "metadata": {"aucs": [0.13151632513412814, 0.13471548709190873, 0.13304395572249483]}, "mutation_prompt": null}
{"id": "d83446f1-2238-4e53-9737-e61b95cdda36", "solution": "import numpy as np\n\nclass NovelSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particle_count = min(30, budget // dim)\n        self.inertia_weight = 0.729\n        self.cognitive_const = 1.49445\n        self.social_const = 1.49445\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        positions = np.random.uniform(lb, ub, (self.particle_count, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particle_count, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.particle_count, np.inf)\n\n        global_best_position = None\n        global_best_score = np.inf\n\n        evaluations = 0\n        global_best_history = []\n\n        while evaluations < self.budget:\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                current_score = func(positions[i])\n                evaluations += 1\n                \n                if current_score < personal_best_scores[i]:\n                    personal_best_scores[i] = current_score\n                    personal_best_positions[i] = positions[i]\n                \n                if current_score < global_best_score:\n                    global_best_score = current_score\n                    global_best_position = positions[i]\n                    global_best_history.append(global_best_position)\n\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                r1, r2 = np.random.rand(2, self.dim)\n                memory_window = max(5, len(global_best_history) // 4)\n                memory_influence = np.mean(global_best_history[-memory_window:], axis=0)\n                feedback_factor = np.tanh((global_best_score - current_score) / (global_best_score + 1e-10)) * (0.99 ** evaluations)\n                velocities[i] = (self.inertia_weight * velocities[i] +\n                                 self.cognitive_const * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.social_const * r2 * (memory_influence - positions[i]) + feedback_factor)\n                velocities[i] *= 0.92  # Changed adaptive velocity scaling\n                scale_factor = 0.6 + 0.0001 * evaluations  # Modified dynamic scaling\n                positions[i] = np.clip(positions[i] + velocities[i] * scale_factor, lb, ub)\n\n            self.inertia_weight *= 0.985  # Updated inertia weight decay\n\n            progress_ratio = evaluations / self.budget\n            self.cognitive_const = 1.5 - 0.5 * progress_ratio\n            self.social_const = 1.5 + 0.5 * progress_ratio\n            \n        return global_best_position", "name": "NovelSwarmOptimizer", "description": "Enhanced convergence by introducing a feedback factor finely tuned to evaluations.", "configspace": "", "generation": 88, "fitness": 0.13353410546054936, "feedback": "The algorithm NovelSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.13 with standard deviation 0.00.", "error": "", "parent_id": "6bd0c022-f499-4337-91b3-fbb89195faab", "metadata": {"aucs": [0.13306606233410145, 0.134613473706785, 0.13292278034076166]}, "mutation_prompt": null}
{"id": "429eca5b-7295-45f0-b594-07b1f2d81f9e", "solution": "import numpy as np\n\nclass NovelSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particle_count = min(30, budget // dim)\n        self.inertia_weight = 0.729  # inertia weight\n        self.cognitive_const = 1.49445  # cognitive constant\n        self.social_const = 1.49445  # social constant\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        # Initialize particles' positions and velocities\n        positions = np.random.uniform(lb, ub, (self.particle_count, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particle_count, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.particle_count, np.inf)\n\n        global_best_position = None\n        global_best_score = np.inf\n\n        evaluations = 0\n        global_best_history = []\n\n        while evaluations < self.budget:\n            # Evaluate current positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                current_score = func(positions[i])\n                evaluations += 1\n                \n                # Update personal best\n                if current_score < personal_best_scores[i]:\n                    personal_best_scores[i] = current_score\n                    personal_best_positions[i] = positions[i]\n                \n                # Update global best\n                if current_score < global_best_score:\n                    global_best_score = current_score\n                    global_best_position = positions[i]\n                    global_best_history.append(global_best_position)\n\n            # Update velocities and positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                r1, r2 = np.random.rand(2, self.dim)\n                memory_window = max(5, len(global_best_history) // 3)  # Changed memory influence\n                memory_influence = np.median(global_best_history[-memory_window:], axis=0)  # Changed to median\n                feedback_factor = np.tanh((global_best_score - current_score) / (global_best_score + 1e-10)) * (0.98 ** evaluations)  # Adjusted feedback factor\n                velocities[i] = (self.inertia_weight * velocities[i] +\n                                 self.cognitive_const * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.social_const * r2 * (memory_influence - positions[i]) + feedback_factor)\n                velocities[i] *= 0.95  # More adaptive velocity scaling\n                scale_factor = 0.5 + 0.0001 * evaluations  # dynamic scaling based on evaluations\n                positions[i] = np.clip(positions[i] + velocities[i] * scale_factor, lb, ub)  # Using dynamic constriction factor\n\n            # Adaptive inertia weight decay\n            self.inertia_weight *= 0.99\n\n            # Adaptive cognitive and social constants\n            progress_ratio = evaluations / self.budget\n            self.cognitive_const = 1.5 - 0.5 * progress_ratio\n            self.social_const = 1.5 + 0.5 * progress_ratio\n            \n        return global_best_position", "name": "NovelSwarmOptimizer", "description": "Optimize convergence by fine-tuning feedback factor and memory influence using progressive adjustments.", "configspace": "", "generation": 89, "fitness": 0.1324479730498962, "feedback": "The algorithm NovelSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.13 with standard deviation 0.00.", "error": "", "parent_id": "6bd0c022-f499-4337-91b3-fbb89195faab", "metadata": {"aucs": [0.13165864959469198, 0.13291936004425986, 0.13276590951073675]}, "mutation_prompt": null}
{"id": "fd152610-8d48-415a-895d-4d2b994bf599", "solution": "import numpy as np\n\nclass NovelSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particle_count = min(30, budget // dim)\n        self.inertia_weight = 0.729  # inertia weight\n        self.cognitive_const = 1.49445  # cognitive constant\n        self.social_const = 1.49445  # social constant\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        # Initialize particles' positions and velocities\n        positions = np.random.uniform(lb, ub, (self.particle_count, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particle_count, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.particle_count, np.inf)\n\n        global_best_position = None\n        global_best_score = np.inf\n\n        evaluations = 0\n        global_best_history = []\n\n        while evaluations < self.budget:\n            # Evaluate current positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                current_score = func(positions[i])\n                evaluations += 1\n                \n                # Update personal best\n                if current_score < personal_best_scores[i]:\n                    personal_best_scores[i] = current_score\n                    personal_best_positions[i] = positions[i]\n                \n                # Update global best\n                if current_score < global_best_score:\n                    global_best_score = current_score\n                    global_best_position = positions[i]\n                    global_best_history.append(global_best_position)\n\n            # Update velocities and positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                r1, r2 = np.random.rand(2, self.dim)\n                memory_window = max(5, len(global_best_history) // 4)\n                memory_influence = np.mean(global_best_history[-memory_window:], axis=0)\n                feedback_factor = np.tanh((global_best_score - current_score) / (global_best_score + 1e-10)) * (0.99 ** evaluations)  # Enhanced feedback factor\n                velocities[i] = (self.inertia_weight * velocities[i] +\n                                 self.cognitive_const * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.social_const * r2 * (memory_influence * (1 + 0.1 * np.std(velocities, axis=0)) - positions[i]) + feedback_factor)  # Dynamic memory influence\n                velocities[i] *= 0.95  # More adaptive velocity scaling\n                scale_factor = 0.5 + 0.0002 * evaluations  # dynamic scaling based on evaluations\n                positions[i] = np.clip(positions[i] + velocities[i] * scale_factor, lb, ub)  # Using dynamic constriction factor\n\n            # Adaptive inertia weight decay\n            self.inertia_weight *= 0.99\n\n            # Adaptive cognitive and social constants\n            progress_ratio = evaluations / self.budget\n            self.cognitive_const = 1.5 - 0.5 * progress_ratio\n            self.social_const = 1.5 + 0.5 * progress_ratio\n            \n        return global_best_position", "name": "NovelSwarmOptimizer", "description": "Introduce dynamic memory influence and adaptive scaling based on particle synchronization to enhance convergence precision.", "configspace": "", "generation": 90, "fitness": 0.11991344147465444, "feedback": "The algorithm NovelSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.12 with standard deviation 0.01.", "error": "", "parent_id": "6bd0c022-f499-4337-91b3-fbb89195faab", "metadata": {"aucs": [0.12355662757180885, 0.12598634942163534, 0.11019734743051912]}, "mutation_prompt": null}
{"id": "9d335abb-53f0-4d66-a36a-78cd2a2c643d", "solution": "import numpy as np\n\nclass NovelSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particle_count = min(50, budget // dim)  # Increased initial particle count\n        self.inertia_weight = 0.729\n        self.cognitive_const = 1.49445\n        self.social_const = 1.49445\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        positions = np.random.uniform(lb, ub, (self.particle_count, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particle_count, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.particle_count, np.inf)\n\n        global_best_position = None\n        global_best_score = np.inf\n\n        evaluations = 0\n        global_best_history = []\n\n        while evaluations < self.budget:\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                current_score = func(positions[i])\n                evaluations += 1\n                \n                if current_score < personal_best_scores[i]:\n                    personal_best_scores[i] = current_score\n                    personal_best_positions[i] = positions[i]\n                \n                if current_score < global_best_score:\n                    global_best_score = current_score\n                    global_best_position = positions[i]\n                    global_best_history.append(global_best_position)\n\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                r1, r2 = np.random.rand(2, self.dim)\n                memory_window = max(5, len(global_best_history) // 3)  # Adjusted memory influence range\n                memory_influence = np.mean(global_best_history[-memory_window:], axis=0)\n                feedback_factor = np.tanh((global_best_score - current_score) / (global_best_score + 1e-10)) * (0.99 ** evaluations)\n                velocities[i] = (self.inertia_weight * velocities[i] +\n                                 self.cognitive_const * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.social_const * r2 * (memory_influence - positions[i]) + feedback_factor)\n                velocities[i] *= 0.95\n                scale_factor = 0.5 + 0.0001 * evaluations\n                positions[i] = np.clip(positions[i] + velocities[i] * scale_factor, lb, ub)\n\n            self.inertia_weight *= 0.99\n\n            progress_ratio = evaluations / self.budget\n            self.cognitive_const = 1.5 - 0.5 * progress_ratio\n            self.social_const = 1.5 + 0.5 * progress_ratio\n            \n        return global_best_position", "name": "NovelSwarmOptimizer", "description": "Improved convergence by adapting particle count and enhancing memory influence.", "configspace": "", "generation": 91, "fitness": 0.1312568857451855, "feedback": "The algorithm NovelSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.13 with standard deviation 0.00.", "error": "", "parent_id": "6bd0c022-f499-4337-91b3-fbb89195faab", "metadata": {"aucs": [0.13317548722772, 0.1319372973638322, 0.1286578726440043]}, "mutation_prompt": null}
{"id": "630e9e23-2f25-4242-8197-89c93d363c82", "solution": "import numpy as np\n\nclass NovelSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particle_count = min(30, budget // dim)\n        self.inertia_weight = 0.729  # inertia weight\n        self.cognitive_const = 1.49445  # cognitive constant\n        self.social_const = 1.49445  # social constant\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        # Initialize particles' positions and velocities\n        positions = np.random.uniform(lb, ub, (self.particle_count, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particle_count, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.particle_count, np.inf)\n\n        global_best_position = None\n        global_best_score = np.inf\n\n        evaluations = 0\n        global_best_history = []\n\n        while evaluations < self.budget:\n            # Evaluate current positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                current_score = func(positions[i])\n                evaluations += 1\n                \n                # Update personal best\n                if current_score < personal_best_scores[i]:\n                    personal_best_scores[i] = current_score\n                    personal_best_positions[i] = positions[i]\n                \n                # Update global best\n                if current_score < global_best_score:\n                    global_best_score = current_score\n                    global_best_position = positions[i]\n                    global_best_history.append(global_best_position)\n\n            # Update velocities and positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                r1, r2 = np.random.rand(2, self.dim)\n                memory_window = max(5, len(global_best_history) // 4)\n                memory_influence = np.mean(global_best_history[-memory_window:], axis=0)\n                feedback_factor = np.tanh((global_best_score - current_score) / (global_best_score + 1e-10)) * (0.99 ** evaluations)  # Enhanced feedback factor\n                velocities[i] = (self.inertia_weight * velocities[i] +\n                                 self.cognitive_const * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.social_const * r2 * (memory_influence - positions[i]) + feedback_factor)\n                velocities[i] *= 0.98  # More adaptive velocity scaling\n                scale_factor = 0.55 + 0.00015 * evaluations  # dynamic scaling based on evaluations\n                positions[i] = np.clip(positions[i] + velocities[i] * scale_factor, lb, ub)  # Using dynamic constriction factor\n\n            # Adaptive inertia weight decay\n            self.inertia_weight *= 0.99\n\n            # Adaptive cognitive and social constants\n            progress_ratio = evaluations / self.budget\n            self.cognitive_const = 1.5 - 0.5 * progress_ratio\n            self.social_const = 1.5 + 0.5 * progress_ratio\n            \n        return global_best_position", "name": "NovelSwarmOptimizer", "description": "Improved particle swarm scalability by updating velocity scaling and dynamic constriction for better convergence.", "configspace": "", "generation": 92, "fitness": 0.13213956254752082, "feedback": "The algorithm NovelSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.13 with standard deviation 0.00.", "error": "", "parent_id": "6bd0c022-f499-4337-91b3-fbb89195faab", "metadata": {"aucs": [0.130015211817881, 0.13391436574426907, 0.13248911008041242]}, "mutation_prompt": null}
{"id": "23b4751c-da96-4980-9514-069a21d134a2", "solution": "import numpy as np\n\nclass NovelSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particle_count = min(30, budget // dim)\n        self.inertia_weight = 0.729  # inertia weight\n        self.cognitive_const = 1.49445  # cognitive constant\n        self.social_const = 1.49445  # social constant\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        # Initialize particles' positions and velocities\n        positions = np.random.uniform(lb, ub, (self.particle_count, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particle_count, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.particle_count, np.inf)\n\n        global_best_position = None\n        global_best_score = np.inf\n\n        evaluations = 0\n        global_best_history = []\n\n        while evaluations < self.budget:\n            # Evaluate current positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                current_score = func(positions[i])\n                evaluations += 1\n                \n                # Update personal best\n                if current_score < personal_best_scores[i]:\n                    personal_best_scores[i] = current_score\n                    personal_best_positions[i] = positions[i]\n                \n                # Update global best\n                if current_score < global_best_score:\n                    global_best_score = current_score\n                    global_best_position = positions[i]\n                    global_best_history.append(global_best_position)\n\n            # Update velocities and positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                r1, r2 = np.random.rand(2, self.dim)\n                memory_window = max(5, len(global_best_history) // 4)\n                memory_influence = np.mean(global_best_history[-memory_window:], axis=0)\n                progress_ratio = evaluations / self.budget\n                feedback_factor = np.tanh((global_best_score - current_score) / (global_best_score + 1e-10)) * (0.99 ** (evaluations * progress_ratio))  # Adaptive Enhanced feedback factor\n                velocities[i] = (self.inertia_weight * velocities[i] +\n                                 self.cognitive_const * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.social_const * r2 * (memory_influence - positions[i]) + feedback_factor)\n                velocities[i] *= 0.95  # More adaptive velocity scaling\n                scale_factor = 0.5 + 0.0001 * evaluations  # dynamic scaling based on evaluations\n                positions[i] = np.clip(positions[i] + velocities[i] * scale_factor, lb, ub)  # Using dynamic constriction factor\n\n            # Adaptive inertia weight decay\n            self.inertia_weight *= 0.99\n\n            # Adaptive cognitive and social constants\n            self.cognitive_const = 1.5 - 0.5 * progress_ratio\n            self.social_const = 1.5 + 0.5 * progress_ratio\n            \n        return global_best_position", "name": "NovelSwarmOptimizer", "description": "Introduced adaptive feedback factor scaling based on the evolutionary stage for improved convergence.", "configspace": "", "generation": 93, "fitness": 0.1335929139129166, "feedback": "The algorithm NovelSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.13 with standard deviation 0.00.", "error": "", "parent_id": "6bd0c022-f499-4337-91b3-fbb89195faab", "metadata": {"aucs": [0.13250472503849187, 0.13480380345844745, 0.13347021324181052]}, "mutation_prompt": null}
{"id": "1bee300e-afc8-4101-8344-8636027026d3", "solution": "import numpy as np\n\nclass NovelSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particle_count = min(30, budget // dim)\n        self.inertia_weight = 0.729  # inertia weight\n        self.cognitive_const = 1.49445  # cognitive constant\n        self.social_const = 1.49445  # social constant\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        # Initialize particles' positions and velocities\n        positions = np.random.uniform(lb, ub, (self.particle_count, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particle_count, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.particle_count, np.inf)\n\n        global_best_position = None\n        global_best_score = np.inf\n\n        evaluations = 0\n        global_best_history = []\n\n        while evaluations < self.budget:\n            # Evaluate current positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                current_score = func(positions[i])\n                evaluations += 1\n                \n                # Update personal best\n                if current_score < personal_best_scores[i]:\n                    personal_best_scores[i] = current_score\n                    personal_best_positions[i] = positions[i]\n                \n                # Update global best\n                if current_score < global_best_score:\n                    global_best_score = current_score\n                    global_best_position = positions[i]\n                    global_best_history.append(global_best_position)\n\n            # Update velocities and positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                r1, r2 = np.random.rand(2, self.dim)\n                memory_window = max(3, len(global_best_history) // 5)  # Adjusted memory window\n                memory_influence = np.mean(global_best_history[-memory_window:], axis=0)\n                feedback_factor = np.tanh((global_best_score - current_score) / (global_best_score + 1e-10)) * (0.97 ** evaluations)  # Adjusted feedback factor\n                velocities[i] = (self.inertia_weight * velocities[i] +\n                                 self.cognitive_const * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.social_const * r2 * (memory_influence - positions[i]) + feedback_factor)\n                velocities[i] *= 0.95  # More adaptive velocity scaling\n                scale_factor = 0.5 + 0.0001 * evaluations  # dynamic scaling based on evaluations\n                positions[i] = np.clip(positions[i] + velocities[i] * scale_factor, lb, ub)  # Using dynamic constriction factor\n\n            # Adaptive inertia weight decay\n            self.inertia_weight *= 0.99\n\n            # Adaptive cognitive and social constants\n            progress_ratio = evaluations / self.budget\n            self.cognitive_const = 1.5 - 0.5 * progress_ratio\n            self.social_const = 1.5 + 0.5 * progress_ratio\n            \n        return global_best_position", "name": "NovelSwarmOptimizer", "description": "Optimized feedback mechanism by adjusting the influence of feedback factor and memory window for better convergence efficiency.", "configspace": "", "generation": 94, "fitness": 0.13417369797777534, "feedback": "The algorithm NovelSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.13 with standard deviation 0.00.", "error": "", "parent_id": "6bd0c022-f499-4337-91b3-fbb89195faab", "metadata": {"aucs": [0.13489717596707318, 0.13472520131285648, 0.13289871665339636]}, "mutation_prompt": null}
{"id": "9327e98f-eff7-4782-9d29-5ef97775b5d8", "solution": "import numpy as np\n\nclass NovelSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particle_count = min(30, budget // dim)\n        self.inertia_weight = 0.729  # inertia weight\n        self.cognitive_const = 1.49445  # cognitive constant\n        self.social_const = 1.49445  # social constant\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        # Initialize particles' positions and velocities\n        positions = np.random.uniform(lb, ub, (self.particle_count, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particle_count, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.particle_count, np.inf)\n\n        global_best_position = None\n        global_best_score = np.inf\n\n        evaluations = 0\n        global_best_history = []\n\n        while evaluations < self.budget:\n            # Evaluate current positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                current_score = func(positions[i])\n                evaluations += 1\n                \n                # Update personal best\n                if current_score < personal_best_scores[i]:\n                    personal_best_scores[i] = current_score\n                    personal_best_positions[i] = positions[i]\n                \n                # Update global best\n                if current_score < global_best_score:\n                    global_best_score = current_score\n                    global_best_position = positions[i]\n                    global_best_history.append(global_best_position)\n\n            # Update velocities and positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                r1, r2 = np.random.rand(2, self.dim)\n                memory_window = max(3, len(global_best_history) // 5)  # Adjusted memory window\n                memory_influence = np.mean(global_best_history[-memory_window:], axis=0)\n                feedback_factor = np.tanh((global_best_score - current_score) / (global_best_score + 1e-10)) * (0.98 ** evaluations)  # Adjusted feedback factor\n                velocities[i] = (self.inertia_weight * velocities[i] +\n                                 self.cognitive_const * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.social_const * r2 * (memory_influence - positions[i]) + feedback_factor)\n                velocities[i] *= 0.95  # More adaptive velocity scaling\n                scale_factor = 0.5 + 0.0001 * evaluations  # dynamic scaling based on evaluations\n                positions[i] = np.clip(positions[i] + velocities[i] * scale_factor, lb, ub)  # Using dynamic constriction factor\n\n            # Adaptive inertia weight decay\n            self.inertia_weight *= 0.99\n\n            # Adaptive cognitive and social constants\n            progress_ratio = evaluations / self.budget\n            self.cognitive_const = 1.5 - 0.6 * progress_ratio\n            self.social_const = 1.5 + 0.6 * progress_ratio\n            \n        return global_best_position", "name": "NovelSwarmOptimizer", "description": "Enhanced swarm dynamics by introducing adaptive cognitive-social balance and refined feedback adaptation.", "configspace": "", "generation": 95, "fitness": 0.13420638110135766, "feedback": "The algorithm NovelSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.13 with standard deviation 0.00.", "error": "", "parent_id": "1bee300e-afc8-4101-8344-8636027026d3", "metadata": {"aucs": [0.1347827233122394, 0.13491964367738285, 0.13291677631445076]}, "mutation_prompt": null}
{"id": "80aac7fd-4632-44d4-bcf4-d562b4410e13", "solution": "import numpy as np\n\nclass NovelSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particle_count = min(30, budget // dim)\n        self.inertia_weight = 0.729  # inertia weight\n        self.cognitive_const = 1.49445  # cognitive constant\n        self.social_const = 1.49445  # social constant\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        # Initialize particles' positions and velocities\n        positions = np.random.uniform(lb, ub, (self.particle_count, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particle_count, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.particle_count, np.inf)\n\n        global_best_position = None\n        global_best_score = np.inf\n\n        evaluations = 0\n        global_best_history = []\n\n        while evaluations < self.budget:\n            # Evaluate current positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                current_score = func(positions[i])\n                evaluations += 1\n                \n                # Update personal best\n                if current_score < personal_best_scores[i]:\n                    personal_best_scores[i] = current_score\n                    personal_best_positions[i] = positions[i]\n                \n                # Update global best\n                if current_score < global_best_score:\n                    global_best_score = current_score\n                    global_best_position = positions[i]\n                    global_best_history.append(global_best_position)\n\n            # Update velocities and positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                r1, r2 = np.random.rand(2, self.dim)\n                memory_window = max(3, len(global_best_history) // 5)  # Adjusted memory window\n                memory_influence = np.mean(global_best_history[-memory_window:], axis=0)\n                feedback_factor = np.tanh((global_best_score - current_score) / (global_best_score + 1e-10)) * (0.98 ** evaluations)  # Adjusted feedback factor\n                velocities[i] = (self.inertia_weight * velocities[i] +\n                                 self.cognitive_const * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.social_const * r2 * (memory_influence - positions[i]) + feedback_factor)\n                velocities[i] *= 0.95  # More adaptive velocity scaling\n                diversity_factor = np.std(positions, axis=0).mean()  # Calculate diversity\n                scale_factor = 0.5 + 0.0001 * evaluations + 0.1 * diversity_factor  # dynamic scaling based on evaluations and diversity\n                positions[i] = np.clip(positions[i] + velocities[i] * scale_factor, lb, ub)  # Using dynamic constriction factor\n\n            # Adaptive inertia weight decay\n            self.inertia_weight *= 0.99\n\n            # Adaptive cognitive and social constants\n            progress_ratio = evaluations / self.budget\n            self.cognitive_const = 1.5 - 0.6 * progress_ratio\n            self.social_const = 1.5 + 0.6 * progress_ratio\n            \n        return global_best_position", "name": "NovelSwarmOptimizer", "description": "Introduced an adaptive exploration mechanism by adjusting velocity scaling dynamically based on diversity and improvements.", "configspace": "", "generation": 96, "fitness": 0.1233333573699595, "feedback": "The algorithm NovelSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.12 with standard deviation 0.00.", "error": "", "parent_id": "9327e98f-eff7-4782-9d29-5ef97775b5d8", "metadata": {"aucs": [0.12501671548974513, 0.1274424380380501, 0.11754091858208326]}, "mutation_prompt": null}
{"id": "fea1325d-4b91-4d65-ae3c-1c4a12ffb77e", "solution": "import numpy as np\n\nclass NovelSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particle_count = min(30, budget // dim)\n        self.inertia_weight = 0.729  # inertia weight\n        self.cognitive_const = 1.49445  # cognitive constant\n        self.social_const = 1.49445  # social constant\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        positions = np.random.uniform(lb, ub, (self.particle_count, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particle_count, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.particle_count, np.inf)\n\n        global_best_position = None\n        global_best_score = np.inf\n\n        evaluations = 0\n        global_best_history = []\n\n        while evaluations < self.budget:\n            current_particle_count = max(5, self.particle_count - evaluations // (self.budget // 3))  # Dynamic reduction\n            for i in range(current_particle_count):\n                if evaluations >= self.budget:\n                    break\n                current_score = func(positions[i])\n                evaluations += 1\n                \n                if current_score < personal_best_scores[i]:\n                    personal_best_scores[i] = current_score\n                    personal_best_positions[i] = positions[i]\n                \n                if current_score < global_best_score:\n                    global_best_score = current_score\n                    global_best_position = positions[i]\n                    global_best_history.append(global_best_position)\n\n            for i in range(current_particle_count):\n                if evaluations >= self.budget:\n                    break\n                r1, r2 = np.random.rand(2, self.dim)\n                memory_window = max(3, len(global_best_history) // 5)\n                memory_influence = np.mean(global_best_history[-memory_window:], axis=0)\n                feedback_factor = np.tanh((global_best_score - current_score) / (global_best_score + 1e-10)) * (0.98 ** evaluations)\n                velocities[i] = (self.inertia_weight * velocities[i] +\n                                 self.cognitive_const * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.social_const * r2 * (memory_influence - positions[i]) + feedback_factor)\n                velocities[i] *= 0.95\n                scale_factor = 0.5 + 0.0001 * evaluations\n                positions[i] = np.clip(positions[i] + velocities[i] * scale_factor, lb, ub)\n\n            self.inertia_weight *= 0.99\n\n            progress_ratio = evaluations / self.budget\n            self.cognitive_const = 1.5 - 0.6 * progress_ratio\n            self.social_const = 1.5 + 0.6 * progress_ratio\n            \n        return global_best_position", "name": "NovelSwarmOptimizer", "description": "Introduce dynamic swarm size reduction to enhance exploitation as the optimization progresses.", "configspace": "", "generation": 97, "fitness": 0.13409897703712417, "feedback": "The algorithm NovelSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.13 with standard deviation 0.00.", "error": "", "parent_id": "9327e98f-eff7-4782-9d29-5ef97775b5d8", "metadata": {"aucs": [0.13491100461665229, 0.1341952287716578, 0.13319069772306247]}, "mutation_prompt": null}
{"id": "649f0c4d-8885-48e9-a67e-6a45f1729533", "solution": "import numpy as np\n\nclass NovelSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particle_count = min(30, budget // dim)\n        self.inertia_weight = 0.729  # inertia weight\n        self.cognitive_const = 1.49445  # cognitive constant\n        self.social_const = 1.49445  # social constant\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        # Initialize particles' positions and velocities\n        positions = np.random.uniform(lb, ub, (self.particle_count, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particle_count, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.particle_count, np.inf)\n\n        global_best_position = None\n        global_best_score = np.inf\n\n        evaluations = 0\n        global_best_history = []\n\n        while evaluations < self.budget:\n            # Evaluate current positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                current_score = func(positions[i])\n                evaluations += 1\n                \n                # Update personal best\n                if current_score < personal_best_scores[i]:\n                    personal_best_scores[i] = current_score\n                    personal_best_positions[i] = positions[i]\n                \n                # Update global best\n                if current_score < global_best_score:\n                    global_best_score = current_score\n                    global_best_position = positions[i]\n                    global_best_history.append(global_best_position)\n\n            # Update velocities and positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                r1, r2 = np.random.rand(2, self.dim)\n                memory_window = max(3, len(global_best_history) // 5)  # Adjusted memory window\n                memory_influence = np.mean(global_best_history[-memory_window:], axis=0)\n                feedback_factor = np.tanh((global_best_score - current_score) / (global_best_score + 1e-10)) * (0.98 ** evaluations)  # Adjusted feedback factor\n                velocities[i] = (self.inertia_weight * velocities[i] +\n                                 self.cognitive_const * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.social_const * r2 * (memory_influence - positions[i])**1.01 + feedback_factor)  # Added mild non-linearity\n                velocities[i] *= 0.95  # More adaptive velocity scaling\n                scale_factor = 0.5 + 0.0001 * evaluations  # dynamic scaling based on evaluations\n                positions[i] = np.clip(positions[i] + velocities[i] * scale_factor, lb, ub)  # Using dynamic constriction factor\n\n            # Adaptive inertia weight decay\n            self.inertia_weight *= 0.99\n\n            # Adaptive cognitive and social constants\n            progress_ratio = evaluations / self.budget\n            self.cognitive_const = 1.5 - 0.6 * progress_ratio\n            self.social_const = 1.5 + 0.6 * progress_ratio\n            \n        return global_best_position", "name": "NovelSwarmOptimizer", "description": "Introduced mild non-linearity in velocity update to enhance convergence through improved exploration-exploitation balance.", "configspace": "", "generation": 98, "fitness": 0.11991344147465406, "feedback": "The algorithm NovelSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.12 with standard deviation 0.01.", "error": "", "parent_id": "9327e98f-eff7-4782-9d29-5ef97775b5d8", "metadata": {"aucs": [0.12355662757181085, 0.12598634942163223, 0.11019734743051912]}, "mutation_prompt": null}
{"id": "92820ccf-d2fa-4440-b168-9e0460b18f56", "solution": "import numpy as np\n\nclass NovelSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particle_count = min(30, budget // dim)\n        self.inertia_weight = 0.729  # inertia weight\n        self.cognitive_const = 1.49445  # cognitive constant\n        self.social_const = 1.49445  # social constant\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        # Initialize particles' positions and velocities\n        positions = np.random.uniform(lb, ub, (self.particle_count, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particle_count, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.full(self.particle_count, np.inf)\n\n        global_best_position = None\n        global_best_score = np.inf\n\n        evaluations = 0\n        global_best_history = []\n\n        while evaluations < self.budget:\n            # Evaluate current positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                current_score = func(positions[i])\n                evaluations += 1\n\n                # Tournament selection for personal best update\n                if np.random.rand() < 0.5 and current_score < personal_best_scores[i]:\n                    personal_best_scores[i] = current_score\n                    personal_best_positions[i] = positions[i]\n\n                # Tournament selection for global best update\n                if np.random.rand() < 0.5 and current_score < global_best_score:\n                    global_best_score = current_score\n                    global_best_position = positions[i]\n                    global_best_history.append(global_best_position)\n\n            # Update velocities and positions\n            for i in range(self.particle_count):\n                if evaluations >= self.budget:\n                    break\n                r1, r2 = np.random.rand(2, self.dim)\n                memory_window = max(3, len(global_best_history) // 5)  # Adjusted memory window\n                memory_influence = np.mean(global_best_history[-memory_window:], axis=0)\n                feedback_factor = np.tanh((global_best_score - current_score) / (global_best_score + 1e-10)) * (0.98 ** evaluations)  # Adjusted feedback factor\n                velocities[i] = (self.inertia_weight * velocities[i] +\n                                 self.cognitive_const * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.social_const * r2 * (memory_influence - positions[i]) + feedback_factor)\n                velocities[i] *= 0.95  # More adaptive velocity scaling\n                scale_factor = 0.5 + 0.0001 * evaluations  # dynamic scaling based on evaluations\n                positions[i] = np.clip(positions[i] + velocities[i] * scale_factor, lb, ub)  # Using dynamic constriction factor\n\n            # Adaptive inertia weight decay\n            self.inertia_weight *= 0.99\n\n            # Adaptive cognitive and social constants\n            progress_ratio = evaluations / self.budget\n            self.cognitive_const = 1.5 - 0.6 * progress_ratio\n            self.social_const = 1.5 + 0.6 * progress_ratio\n            \n        return global_best_position", "name": "NovelSwarmOptimizer", "description": "Introduce a tournament selection mechanism for better personal and global best updates.", "configspace": "", "generation": 99, "fitness": 0.12964132913146634, "feedback": "The algorithm NovelSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.13 with standard deviation 0.00.", "error": "", "parent_id": "9327e98f-eff7-4782-9d29-5ef97775b5d8", "metadata": {"aucs": [0.1263660529341737, 0.13019761059350898, 0.13236032386671637]}, "mutation_prompt": null}
