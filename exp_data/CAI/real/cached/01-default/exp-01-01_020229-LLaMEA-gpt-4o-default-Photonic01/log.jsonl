{"id": "38d046de-25f3-44f7-89f1-0e1977f97fb7", "solution": "import numpy as np\n\nclass PSO_DE_Optimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20  # Number of particles\n        self.c1 = 2.0  # Cognitive component\n        self.c2 = 2.0  # Social component\n        self.w = 0.7  # Inertia weight\n        self.mutation_factor = 0.8\n        self.crossover_rate = 0.9\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        np.random.seed(42)\n        \n        # Initialize the swarm\n        position = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocity = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_position = np.copy(position)\n        personal_best_value = np.array([func(p) for p in personal_best_position])\n        global_best_position = personal_best_position[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n        \n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                # PSO Update\n                r1, r2 = np.random.rand(2)\n                velocity[i] = (self.w * velocity[i] + \n                               self.c1 * r1 * (personal_best_position[i] - position[i]) + \n                               self.c2 * r2 * (global_best_position - position[i]))\n                position[i] += velocity[i]\n                position[i] = np.clip(position[i], lb, ub)\n\n                # Evaluate\n                current_value = func(position[i])\n                evaluations += 1\n                \n                # Update personal best\n                if current_value < personal_best_value[i]:\n                    personal_best_position[i] = position[i]\n                    personal_best_value[i] = current_value\n                \n                # Update global best\n                if current_value < global_best_value:\n                    global_best_position = position[i]\n                    global_best_value = current_value\n\n                if evaluations >= self.budget:\n                    break\n\n            # DE Variant incorporated into PSO\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n                indices = [index for index in range(self.population_size) if index != i]\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                \n                mutant_vector = personal_best_position[a] + self.mutation_factor * (personal_best_position[b] - personal_best_position[c])\n                mutant_vector = np.clip(mutant_vector, lb, ub)\n                \n                trial_vector = np.copy(position[i])\n                for j in range(self.dim):\n                    if np.random.rand() < self.crossover_rate:\n                        trial_vector[j] = mutant_vector[j]\n\n                trial_value = func(trial_vector)\n                evaluations += 1\n\n                # If trial vector is better, replace the target\n                if trial_value < personal_best_value[i]:\n                    personal_best_position[i] = trial_vector\n                    personal_best_value[i] = trial_value\n                \n                # Update global best after DE step\n                if trial_value < global_best_value:\n                    global_best_position = trial_vector\n                    global_best_value = trial_value\n\n        return global_best_position, global_best_value", "name": "PSO_DE_Optimizer", "description": "A hybrid metaheuristic combining Particle Swarm Optimization (PSO) with Differential Evolution (DE) for enhanced exploration and exploitation in high-dimensional optimization.", "configspace": "", "generation": 0, "fitness": 0.27746868320143103, "feedback": "The algorithm PSO_DE_Optimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28 with standard deviation 0.00.", "error": "", "parent_id": null, "metadata": {"aucs": [0.27746868320143103, 0.27746868320143103, 0.27746868320143103]}, "mutation_prompt": null}
{"id": "63a5519e-d4a1-4d32-a3e0-fbc4f2506511", "solution": "import numpy as np\n\nclass HS_AHM_Optimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.harmony_memory_size = 30\n        self.hmcr = 0.9  # Harmony Memory Consideration Rate\n        self.par_min = 0.1  # Minimum Pitch Adjustment Rate\n        self.par_max = 0.5  # Maximum Pitch Adjustment Rate\n        self.bw = 0.02  # Bandwidth for pitch adjustment\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        np.random.seed(42)\n        \n        # Initialize Harmony Memory (HM)\n        harmony_memory = np.random.uniform(lb, ub, (self.harmony_memory_size, self.dim))\n        harmony_values = np.array([func(h) for h in harmony_memory])\n        evaluations = self.harmony_memory_size\n        \n        while evaluations < self.budget:\n            new_harmony = np.zeros(self.dim)\n            for i in range(self.dim):\n                if np.random.rand() < self.hmcr:\n                    # Memory consideration\n                    idx = np.random.randint(0, self.harmony_memory_size)\n                    new_harmony[i] = harmony_memory[idx, i]\n                    if np.random.rand() < self.par_min + (self.par_max - self.par_min) * (evaluations / self.budget):\n                        # Pitch adjustment\n                        new_harmony[i] += self.bw * np.random.uniform(-1, 1)\n                else:\n                    # Random selection\n                    new_harmony[i] = np.random.uniform(lb[i], ub[i])\n            new_harmony = np.clip(new_harmony, lb, ub)\n            \n            new_value = func(new_harmony)\n            evaluations += 1\n            \n            # Update harmony memory if new harmony is better\n            if new_value < np.max(harmony_values):\n                worst_idx = np.argmax(harmony_values)\n                harmony_memory[worst_idx] = new_harmony\n                harmony_values[worst_idx] = new_value\n        \n        best_idx = np.argmin(harmony_values)\n        return harmony_memory[best_idx], harmony_values[best_idx]", "name": "HS_AHM_Optimizer", "description": "A novel Harmony Search with Adaptive Harmony Memory (HS_AHM) leveraging dynamic parameter tuning for robust exploration and exploitation in global optimization.", "configspace": "", "generation": 1, "fitness": 0.27669996312787537, "feedback": "The algorithm HS_AHM_Optimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28 with standard deviation 0.00.", "error": "", "parent_id": "38d046de-25f3-44f7-89f1-0e1977f97fb7", "metadata": {"aucs": [0.27669996312787537, 0.27669996312787537, 0.27669996312787537]}, "mutation_prompt": null}
{"id": "4b98587a-12f7-40d6-9806-f18902fa3697", "solution": "import numpy as np\n\nclass GA_SA_Optimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 30  # Initial population size\n        self.crossover_rate = 0.8\n        self.mutation_rate = 0.1\n        self.initial_temperature = 1000.0\n        self.cooling_rate = 0.99\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        np.random.seed(42)\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_index = np.argmin(fitness)\n        best_solution = population[best_index]\n        best_fitness = fitness[best_index]\n        \n        evaluations = self.population_size\n        temperature = self.initial_temperature\n        \n        while evaluations < self.budget:\n            # Genetic Algorithm Step\n            new_population = []\n            for _ in range(self.population_size // 2):\n                parents = np.random.choice(self.population_size, 2, replace=False, p=fitness/fitness.sum())\n                if np.random.rand() < self.crossover_rate:\n                    crossover_point = np.random.randint(1, self.dim)\n                    child1 = np.concatenate([population[parents[0], :crossover_point], \n                                             population[parents[1], crossover_point:]])\n                    child2 = np.concatenate([population[parents[1], :crossover_point], \n                                             population[parents[0], crossover_point:]])\n                else:\n                    child1, child2 = population[parents]\n                    \n                new_population.extend([child1, child2])\n            \n            # Mutation\n            for i in range(len(new_population)):\n                if np.random.rand() < self.mutation_rate:\n                    mutation_index = np.random.randint(self.dim)\n                    new_population[i][mutation_index] = np.random.uniform(lb[mutation_index], ub[mutation_index])\n            \n            # Evaluate new population\n            new_population = np.array(new_population)\n            new_fitness = np.array([func(ind) for ind in new_population])\n            evaluations += len(new_population)\n            \n            # Combine and select the best\n            combined_population = np.vstack((population, new_population))\n            combined_fitness = np.hstack((fitness, new_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n            \n            current_index = np.argmin(fitness)\n            current_solution = population[current_index]\n            current_fitness = fitness[current_index]\n            \n            # Simulated Annealing Step\n            if np.random.rand() < np.exp((best_fitness - current_fitness) / temperature):\n                best_solution = current_solution\n                best_fitness = current_fitness\n\n            # Cooling\n            temperature *= self.cooling_rate\n\n        return best_solution, best_fitness", "name": "GA_SA_Optimizer", "description": "A novel hybrid algorithm combining Genetic Algorithm (GA) with Simulated Annealing (SA) for adaptive cooling and diversification in high-dimensional optimization.", "configspace": "", "generation": 2, "fitness": 0.2731320967757537, "feedback": "The algorithm GA_SA_Optimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.27 with standard deviation 0.00.", "error": "", "parent_id": "38d046de-25f3-44f7-89f1-0e1977f97fb7", "metadata": {"aucs": [0.2731320967757537, 0.2731320967757537, 0.2731320967757537]}, "mutation_prompt": null}
{"id": "a8424a95-3e63-4e7f-96a9-103c8eb2717d", "solution": "import numpy as np\n\nclass Enhanced_PSO_DE_Optimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20  # Number of particles\n        self.c1 = 2.0  # Cognitive component\n        self.c2 = 2.0  # Social component\n        self.w_max = 0.9  # Maximum inertia weight\n        self.w_min = 0.4  # Minimum inertia weight\n        self.mutation_factor = 0.8\n        self.crossover_rate = 0.9\n        self.alpha = 1.5  # Lévy flight parameter\n\n    def levy_flight(self):\n        u = np.random.normal(0, 1, self.dim) * ((np.math.gamma(1 + self.alpha) * np.sin(np.pi * self.alpha / 2)) /\n                                                (np.math.gamma((1 + self.alpha) / 2) * self.alpha * \n                                                 2**((self.alpha - 1) / 2)))**(1 / self.alpha)\n        v = np.random.normal(0, 1, self.dim)\n        step = u / abs(v)**(1 / self.alpha)\n        return step\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        np.random.seed(42)\n        \n        # Initialize the swarm\n        position = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocity = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_position = np.copy(position)\n        personal_best_value = np.array([func(p) for p in personal_best_position])\n        global_best_position = personal_best_position[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n        \n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            w = self.w_max - (self.w_max - self.w_min) * (evaluations / self.budget)\n            for i in range(self.population_size):\n                # PSO Update with adaptive inertia and Lévy flight\n                r1, r2 = np.random.rand(2)\n                velocity[i] = (w * velocity[i] + \n                               self.c1 * r1 * (personal_best_position[i] - position[i]) + \n                               self.c2 * r2 * (global_best_position - position[i]))\n                \n                position[i] += velocity[i] + self.levy_flight()\n                position[i] = np.clip(position[i], lb, ub)\n\n                # Evaluate\n                current_value = func(position[i])\n                evaluations += 1\n                \n                # Update personal best\n                if current_value < personal_best_value[i]:\n                    personal_best_position[i] = position[i]\n                    personal_best_value[i] = current_value\n                \n                # Update global best\n                if current_value < global_best_value:\n                    global_best_position = position[i]\n                    global_best_value = current_value\n\n                if evaluations >= self.budget:\n                    break\n\n            # DE Variant incorporated into PSO\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n                indices = [index for index in range(self.population_size) if index != i]\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                \n                mutant_vector = personal_best_position[a] + self.mutation_factor * (personal_best_position[b] - personal_best_position[c])\n                mutant_vector = np.clip(mutant_vector, lb, ub)\n                \n                trial_vector = np.copy(position[i])\n                for j in range(self.dim):\n                    if np.random.rand() < self.crossover_rate:\n                        trial_vector[j] = mutant_vector[j]\n\n                trial_value = func(trial_vector)\n                evaluations += 1\n\n                # If trial vector is better, replace the target\n                if trial_value < personal_best_value[i]:\n                    personal_best_position[i] = trial_vector\n                    personal_best_value[i] = trial_value\n                \n                # Update global best after DE step\n                if trial_value < global_best_value:\n                    global_best_position = trial_vector\n                    global_best_value = trial_value\n\n        return global_best_position, global_best_value", "name": "Enhanced_PSO_DE_Optimizer", "description": "A novel hybrid metaheuristic using adaptive inertia weight and Lévy flight to improve exploration and exploitation in high-dimensional optimization.", "configspace": "", "generation": 3, "fitness": 0.2743470776644734, "feedback": "The algorithm Enhanced_PSO_DE_Optimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.27 with standard deviation 0.00.", "error": "", "parent_id": "38d046de-25f3-44f7-89f1-0e1977f97fb7", "metadata": {"aucs": [0.2743470776644734, 0.2743470776644734, 0.2743470776644734]}, "mutation_prompt": null}
{"id": "0ef9bc1c-fcff-4500-9d1d-59ee2761af76", "solution": "import numpy as np\n\nclass Adaptive_PSO_DE_Optimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20  # Number of particles\n        self.c1 = 2.0  # Cognitive component\n        self.c2 = 2.0  # Social component\n        self.w_min = 0.4\n        self.w_max = 0.9\n        self.mutation_factor_base = 0.5\n        self.crossover_rate = 0.9\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        np.random.seed(42)\n        \n        # Initialize the swarm\n        position = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocity = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_position = np.copy(position)\n        personal_best_value = np.array([func(p) for p in personal_best_position])\n        global_best_position = personal_best_position[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n        \n        evaluations = self.population_size\n        iteration = 0\n\n        while evaluations < self.budget:\n            w = self.w_max - (self.w_max - self.w_min) * (evaluations / self.budget)\n            adaptive_mutation_factor = self.mutation_factor_base + (0.5 * np.sin(2 * np.pi * iteration / 20))\n\n            for i in range(self.population_size):\n                # PSO Update\n                r1, r2 = np.random.rand(2)\n                velocity[i] = (w * velocity[i] + \n                               self.c1 * r1 * (personal_best_position[i] - position[i]) + \n                               self.c2 * r2 * (global_best_position - position[i]))\n                position[i] += velocity[i]\n                position[i] = np.clip(position[i], lb, ub)\n\n                # Evaluate\n                current_value = func(position[i])\n                evaluations += 1\n                \n                # Update personal best\n                if current_value < personal_best_value[i]:\n                    personal_best_position[i] = position[i]\n                    personal_best_value[i] = current_value\n                \n                # Update global best\n                if current_value < global_best_value:\n                    global_best_position = position[i]\n                    global_best_value = current_value\n\n                if evaluations >= self.budget:\n                    break\n\n            # Adaptive DE Variant incorporated into PSO\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n                indices = [index for index in range(self.population_size) if index != i]\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                \n                mutant_vector = personal_best_position[a] + adaptive_mutation_factor * (personal_best_position[b] - personal_best_position[c])\n                mutant_vector = np.clip(mutant_vector, lb, ub)\n                \n                trial_vector = np.copy(position[i])\n                for j in range(self.dim):\n                    if np.random.rand() < self.crossover_rate:\n                        trial_vector[j] = mutant_vector[j]\n\n                trial_value = func(trial_vector)\n                evaluations += 1\n\n                # If trial vector is better, replace the target\n                if trial_value < personal_best_value[i]:\n                    personal_best_position[i] = trial_vector\n                    personal_best_value[i] = trial_value\n                \n                # Update global best after DE step\n                if trial_value < global_best_value:\n                    global_best_position = trial_vector\n                    global_best_value = trial_value\n\n            iteration += 1\n\n        return global_best_position, global_best_value", "name": "Adaptive_PSO_DE_Optimizer", "description": "Hybrid PSO with Adaptive DE Mutation: Integrates Particle Swarm Optimization with an adaptive Differential Evolution mutation strategy to dynamically balance exploration and exploitation for global optimization of photonic structures.", "configspace": "", "generation": 4, "fitness": 0.276750729978961, "feedback": "The algorithm Adaptive_PSO_DE_Optimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28 with standard deviation 0.00.", "error": "", "parent_id": "38d046de-25f3-44f7-89f1-0e1977f97fb7", "metadata": {"aucs": [0.276750729978961, 0.276750729978961, 0.276750729978961]}, "mutation_prompt": null}
{"id": "f4d79331-93c8-4b83-897b-46340b8d5c83", "solution": "import numpy as np\n\nclass QSA_Optimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_temp = 1000\n        self.final_temp = 1\n        self.alpha = 0.95  # Cooling rate\n        self.quantum_amplitude = 0.1  # Quantum fluctuation control\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        np.random.seed(42)\n\n        # Initialize position randomly\n        current_position = np.random.uniform(lb, ub, self.dim)\n        current_value = func(current_position)\n        best_position = np.copy(current_position)\n        best_value = current_value\n\n        temperature = self.initial_temp\n        evaluations = 1\n\n        while evaluations < self.budget and temperature > self.final_temp:\n            # Quantum-inspired transition with superposition\n            quantum_step = np.random.uniform(-1, 1, self.dim) * self.quantum_amplitude * temperature\n            candidate_position = current_position + quantum_step\n            candidate_position = np.clip(candidate_position, lb, ub)\n\n            # Evaluate candidate solution\n            candidate_value = func(candidate_position)\n            evaluations += 1\n\n            # Acceptance criteria based on simulated annealing\n            delta_value = candidate_value - current_value\n            acceptance_probability = np.exp(-delta_value / temperature) if delta_value > 0 else 1\n\n            if np.random.rand() < acceptance_probability:\n                current_position = candidate_position\n                current_value = candidate_value\n\n                # Update best found solution\n                if current_value < best_value:\n                    best_position = current_position\n                    best_value = current_value\n\n            # Cooling schedule\n            temperature *= self.alpha\n\n        return best_position, best_value", "name": "QSA_Optimizer", "description": "Quantum-inspired Simulated Annealing (QSA) algorithm leveraging quantum superposition and tunneling concepts for enhanced exploration in global optimization.", "configspace": "", "generation": 5, "fitness": 0.22730097144013717, "feedback": "The algorithm QSA_Optimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.23 with standard deviation 0.00.", "error": "", "parent_id": "38d046de-25f3-44f7-89f1-0e1977f97fb7", "metadata": {"aucs": [0.22730097144013717, 0.22730097144013717, 0.22730097144013717]}, "mutation_prompt": null}
{"id": "5e4ff1a0-404b-480f-8a52-66008c5fd6c8", "solution": "import numpy as np\n\nclass CCPSO_Optimizer:\n    def __init__(self, budget, dim, subcomponents=5):\n        self.budget = budget\n        self.dim = dim\n        self.subcomponents = subcomponents\n        self.population_size = 20\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.w = 0.7\n        self.sub_dim = self.dim // self.subcomponents\n        self.eval_budget_per_component = budget // self.subcomponents\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        np.random.seed(42)\n        \n        # Initialize the swarm for each subcomponent\n        positions = [np.random.uniform(lb, ub, (self.population_size, self.sub_dim)) for _ in range(self.subcomponents)]\n        velocities = [np.random.uniform(-1, 1, (self.population_size, self.sub_dim)) for _ in range(self.subcomponents)]\n        personal_best_positions = [np.copy(pos) for pos in positions]\n        personal_best_values = [np.array([func(np.concatenate([p if j == i else positions[j][0] \n                                           for j in range(self.subcomponents)])) for p in pos]) \n                                for i, pos in enumerate(personal_best_positions)]\n        \n        global_best_position = np.concatenate([pb[np.argmin(pv)] for pb, pv in zip(personal_best_positions, personal_best_values)])\n        global_best_value = np.min([np.min(pv) for pv in personal_best_values])\n        \n        evaluations = self.population_size * self.subcomponents\n\n        for component in range(self.subcomponents):\n            while evaluations < self.eval_budget_per_component * (component + 1):\n                for i in range(self.population_size):\n                    # Update particle velocity and position for the current subcomponent\n                    r1, r2 = np.random.rand(2)\n                    velocities[component][i] = (self.w * velocities[component][i] +\n                                                self.c1 * r1 * (personal_best_positions[component][i] - positions[component][i]) +\n                                                self.c2 * r2 * (global_best_position[component*self.sub_dim:(component+1)*self.sub_dim] - positions[component][i]))\n                    positions[component][i] += velocities[component][i]\n                    positions[component][i] = np.clip(positions[component][i], lb, ub)\n\n                    # Evaluate\n                    candidate_solution = np.concatenate([positions[component][i] if j == component else positions[j][0] \n                                                         for j in range(self.subcomponents)])\n                    current_value = func(candidate_solution)\n                    evaluations += 1\n\n                    # Update personal best for this subcomponent\n                    if current_value < personal_best_values[component][i]:\n                        personal_best_positions[component][i] = positions[component][i]\n                        personal_best_values[component][i] = current_value\n\n                    # Update global best\n                    if current_value < global_best_value:\n                        global_best_position = candidate_solution\n                        global_best_value = current_value\n\n        return global_best_position, global_best_value", "name": "CCPSO_Optimizer", "description": "A Cooperative Coevolutionary Particle Swarm Optimization (CCPSO) that decomposes the problem into smaller subcomponents and optimizes them cooperatively to enhance performance on high-dimensional problems.", "configspace": "", "generation": 6, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('shape mismatch: objects cannot be broadcast to a single shape.  Mismatch is between arg 0 with shape (20, 2) and arg 1 with shape (10,).').", "error": "ValueError('shape mismatch: objects cannot be broadcast to a single shape.  Mismatch is between arg 0 with shape (20, 2) and arg 1 with shape (10,).')", "parent_id": "38d046de-25f3-44f7-89f1-0e1977f97fb7", "metadata": {}, "mutation_prompt": null}
{"id": "b26e239c-1a32-4547-a84d-c5d167d7a46a", "solution": "import numpy as np\n\nclass Refined_PSO_DE_Optimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.w_max = 0.9\n        self.w_min = 0.4\n        self.mutation_factor = 0.8\n        self.crossover_rate = 0.9\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        np.random.seed(42)\n        \n        position = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocity = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_position = np.copy(position)\n        personal_best_value = np.array([func(p) for p in personal_best_position])\n        global_best_position = personal_best_position[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n        \n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            w = self.w_max - ((self.w_max - self.w_min) * evaluations / self.budget)\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(2)\n                velocity[i] = (w * velocity[i] + \n                               self.c1 * r1 * (personal_best_position[i] - position[i]) + \n                               self.c2 * r2 * (global_best_position - position[i]))\n                position[i] += velocity[i]\n                position[i] = np.clip(position[i], lb, ub)\n\n                current_value = func(position[i])\n                evaluations += 1\n                \n                if current_value < personal_best_value[i]:\n                    personal_best_position[i] = position[i]\n                    personal_best_value[i] = current_value\n                \n                if current_value < global_best_value:\n                    global_best_position = position[i]\n                    global_best_value = current_value\n\n                if evaluations >= self.budget:\n                    break\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n                indices = [index for index in range(self.population_size) if index != i]\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                \n                mutant_vector = personal_best_position[a] + self.mutation_factor * (personal_best_position[b] - personal_best_position[c])\n                mutant_vector = np.clip(mutant_vector, lb, ub)\n                \n                trial_vector = np.copy(position[i])\n                crossover_points = np.random.rand(self.dim) < self.crossover_rate\n                trial_vector[crossover_points] = mutant_vector[crossover_points]\n\n                trial_value = func(trial_vector)\n                evaluations += 1\n\n                if trial_value < personal_best_value[i]:\n                    personal_best_position[i] = trial_vector\n                    personal_best_value[i] = trial_value\n                \n                if trial_value < global_best_value:\n                    global_best_position = trial_vector\n                    global_best_value = trial_value\n\n        return global_best_position, global_best_value", "name": "Refined_PSO_DE_Optimizer", "description": "A refined hybrid optimizer integrating an adaptive inertia weight in PSO with a more robust DE mutation strategy for improved convergence in diverse search spaces.", "configspace": "", "generation": 7, "fitness": 0.2769595152380362, "feedback": "The algorithm Refined_PSO_DE_Optimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28 with standard deviation 0.00.", "error": "", "parent_id": "38d046de-25f3-44f7-89f1-0e1977f97fb7", "metadata": {"aucs": [0.2769595152380362, 0.2769595152380362, 0.2769595152380362]}, "mutation_prompt": null}
{"id": "a27561b9-e012-4228-b59d-151948108782", "solution": "import numpy as np\n\nclass Enhanced_PSO_DE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.initial_w = 0.9\n        self.final_w = 0.4\n        self.mutation_factor = 0.8\n        self.crossover_rate = 0.9\n        self.success_threshold = 0.2\n\n    def adaptive_inertia_weight(self, evaluations):\n        return (self.initial_w - self.final_w) * (1 - evaluations / self.budget) + self.final_w\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        np.random.seed(42)\n        \n        position = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocity = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_position = np.copy(position)\n        personal_best_value = np.array([func(p) for p in personal_best_position])\n        global_best_position = personal_best_position[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n        \n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            success_count = 0\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n                \n                # Adaptive Inertia Weight\n                w = self.adaptive_inertia_weight(evaluations)\n                \n                # PSO Update\n                r1, r2 = np.random.rand(2)\n                velocity[i] = (w * velocity[i] + \n                               self.c1 * r1 * (personal_best_position[i] - position[i]) + \n                               self.c2 * r2 * (global_best_position - position[i]))\n                position[i] += velocity[i]\n                position[i] = np.clip(position[i], lb, ub)\n\n                current_value = func(position[i])\n                evaluations += 1\n\n                if current_value < personal_best_value[i]:\n                    personal_best_position[i] = position[i]\n                    personal_best_value[i] = current_value\n                    success_count += 1\n                \n                if current_value < global_best_value:\n                    global_best_position = position[i]\n                    global_best_value = current_value\n\n            # Success-based parameter adaptation\n            if success_count / self.population_size > self.success_threshold:\n                self.crossover_rate *= 1.05\n                self.mutation_factor *= 1.05\n            else:\n                self.crossover_rate *= 0.95\n                self.mutation_factor *= 0.95\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n                indices = [index for index in range(self.population_size) if index != i]\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                \n                mutant_vector = personal_best_position[a] + self.mutation_factor * (personal_best_position[b] - personal_best_position[c])\n                mutant_vector = np.clip(mutant_vector, lb, ub)\n                \n                trial_vector = np.copy(position[i])\n                for j in range(self.dim):\n                    if np.random.rand() < self.crossover_rate:\n                        trial_vector[j] = mutant_vector[j]\n\n                trial_value = func(trial_vector)\n                evaluations += 1\n\n                if trial_value < personal_best_value[i]:\n                    personal_best_position[i] = trial_vector\n                    personal_best_value[i] = trial_value\n                \n                if trial_value < global_best_value:\n                    global_best_position = trial_vector\n                    global_best_value = trial_value\n\n        return global_best_position, global_best_value", "name": "Enhanced_PSO_DE", "description": "An enhanced PSO-DE hybrid algorithm incorporating Adaptive Inertia Weight and Success-Based Parameter Adaptation for improved convergence and adaptability across different problem landscapes.", "configspace": "", "generation": 8, "fitness": 0.27668512116865274, "feedback": "The algorithm Enhanced_PSO_DE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28 with standard deviation 0.00.", "error": "", "parent_id": "38d046de-25f3-44f7-89f1-0e1977f97fb7", "metadata": {"aucs": [0.27668512116865274, 0.27668512116865274, 0.27668512116865274]}, "mutation_prompt": null}
{"id": "41cd3822-3456-4256-9e0a-85ed920336ba", "solution": "import numpy as np\n\nclass QuantumPSO_DE_Optimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.c1_base = 2.0\n        self.c2_base = 2.0\n        self.w = 0.7\n        self.mutation_factor = 0.8\n        self.crossover_rate = 0.9\n\n    def quantum_behavior(self, position, global_best_position):\n        delta = np.random.uniform(-1, 1, position.shape)\n        quantum_position = position + delta * np.exp(-np.linalg.norm(position - global_best_position))\n        return quantum_position\n\n    def update_parameters(self, evaluations):\n        progress = evaluations / self.budget\n        self.w = 0.9 - 0.5 * progress\n        self.c1 = self.c1_base - 0.5 * progress\n        self.c2 = self.c2_base + 0.5 * progress\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        np.random.seed(42)\n        \n        position = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocity = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_position = np.copy(position)\n        personal_best_value = np.array([func(p) for p in personal_best_position])\n        global_best_position = personal_best_position[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n        \n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            self.update_parameters(evaluations)\n\n            for i in range(self.population_size):\n                # Quantum-inspired PSO Update\n                r1, r2 = np.random.rand(2)\n                velocity[i] = (self.w * velocity[i] + \n                               self.c1 * r1 * (personal_best_position[i] - position[i]) + \n                               self.c2 * r2 * (global_best_position - position[i]))\n                position[i] += velocity[i]\n                position[i] = np.clip(position[i], lb, ub)\n                position[i] = self.quantum_behavior(position[i], global_best_position)\n\n                current_value = func(position[i])\n                evaluations += 1\n\n                if current_value < personal_best_value[i]:\n                    personal_best_position[i] = position[i]\n                    personal_best_value[i] = current_value\n\n                if current_value < global_best_value:\n                    global_best_position = position[i]\n                    global_best_value = current_value\n\n                if evaluations >= self.budget:\n                    break\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n                indices = [index for index in range(self.population_size) if index != i]\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                \n                mutant_vector = personal_best_position[a] + self.mutation_factor * (personal_best_position[b] - personal_best_position[c])\n                mutant_vector = np.clip(mutant_vector, lb, ub)\n                \n                trial_vector = np.copy(position[i])\n                for j in range(self.dim):\n                    if np.random.rand() < self.crossover_rate:\n                        trial_vector[j] = mutant_vector[j]\n\n                trial_value = func(trial_vector)\n                evaluations += 1\n\n                if trial_value < personal_best_value[i]:\n                    personal_best_position[i] = trial_vector\n                    personal_best_value[i] = trial_value\n\n                if trial_value < global_best_value:\n                    global_best_position = trial_vector\n                    global_best_value = trial_value\n\n        return global_best_position, global_best_value", "name": "QuantumPSO_DE_Optimizer", "description": "Quantum-inspired PSO-DE with adaptive parameters leverages quantum mechanics concepts and dynamic parameter adjustment for enhanced convergence and diversity.", "configspace": "", "generation": 9, "fitness": 0.2774096875297518, "feedback": "The algorithm QuantumPSO_DE_Optimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28 with standard deviation 0.00.", "error": "", "parent_id": "38d046de-25f3-44f7-89f1-0e1977f97fb7", "metadata": {"aucs": [0.2774096875297518, 0.2774096875297518, 0.2774096875297518]}, "mutation_prompt": null}
{"id": "124f5fd2-0fd1-42f1-b604-6cc6b92b51f2", "solution": "import numpy as np\n\nclass FA_SA_Optimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.alpha = 0.5  # Randomness reduction coefficient\n        self.beta0 = 1.0  # Attraction coefficient base value\n        self.gamma = 1.0  # Absorption coefficient\n        self.initial_temperature = 1000\n        self.cooling_rate = 0.95\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        np.random.seed(42)\n\n        # Initialize fireflies\n        position = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        intensity = np.array([func(p) for p in position])\n        evaluations = self.population_size\n\n        temperature = self.initial_temperature\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                for j in range(self.population_size):\n                    if intensity[i] > intensity[j]:\n                        r = np.linalg.norm(position[i] - position[j]) / self.dim\n                        beta = self.beta0 * np.exp(-self.gamma * r ** 2)\n                        position[i] += beta * (position[j] - position[i]) + self.alpha * (np.random.rand(self.dim) - 0.5)\n                        position[i] = np.clip(position[i], lb, ub)\n\n                        new_intensity = func(position[i])\n                        evaluations += 1\n\n                        if new_intensity < intensity[i]:\n                            intensity[i] = new_intensity\n\n                        if evaluations >= self.budget:\n                            break\n\n            # Simulated Annealing-like step\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                neighbor = position[i] + np.random.normal(0, self.alpha, self.dim)\n                neighbor = np.clip(neighbor, lb, ub)\n                neighbor_intensity = func(neighbor)\n                evaluations += 1\n\n                delta = neighbor_intensity - intensity[i]\n                if delta < 0 or np.exp(-delta / temperature) > np.random.rand():\n                    position[i] = neighbor\n                    intensity[i] = neighbor_intensity\n\n            temperature *= self.cooling_rate\n\n        best_index = np.argmin(intensity)\n        return position[best_index], intensity[best_index]", "name": "FA_SA_Optimizer", "description": "A novel metaheuristic combining Firefly Algorithm with Simulated Annealing (FA_SA) to balance exploration and exploitation through dynamic light intensity and temperature-based transitions.", "configspace": "", "generation": 10, "fitness": 0.2285396966039106, "feedback": "The algorithm FA_SA_Optimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.23 with standard deviation 0.00.", "error": "", "parent_id": "38d046de-25f3-44f7-89f1-0e1977f97fb7", "metadata": {"aucs": [0.2285396966039106, 0.2285396966039106, 0.2285396966039106]}, "mutation_prompt": null}
{"id": "c272598b-f8be-429e-86ca-8526dc8693c0", "solution": "import numpy as np\n\nclass Symbiotic_Optimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.mutualism_factor = 1.5\n        self.parasitism_factor = 0.5\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        np.random.seed(42)\n        \n        # Initialize the population\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n        \n        # Finding the best solution\n        best_index = np.argmin(fitness)\n        best_solution = population[best_index]\n        best_value = fitness[best_index]\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                # Mutualistic interaction\n                partner_index = np.random.randint(0, self.population_size)\n                if partner_index == i:\n                    continue\n                partner = population[partner_index]\n                \n                mutual_vector = (population[i] + partner) / 2\n                mutual_vector = mutual_vector + self.mutualism_factor * (best_solution - mutual_vector)\n                mutual_vector = np.clip(mutual_vector, lb, ub)\n                \n                mutual_value = func(mutual_vector)\n                evaluations += 1\n\n                if mutual_value < fitness[i]:\n                    population[i] = mutual_vector\n                    fitness[i] = mutual_value\n                \n                if mutual_value < best_value:\n                    best_solution = mutual_vector\n                    best_value = mutual_value\n                \n                if evaluations >= self.budget:\n                    break\n                \n                # Parasitic interaction\n                parasite_vector = np.random.uniform(lb, ub, self.dim)\n                parasite_vector = parasite_vector + self.parasitism_factor * (best_solution - parasite_vector)\n                parasite_vector = np.clip(parasite_vector, lb, ub)\n                \n                parasite_value = func(parasite_vector)\n                evaluations += 1\n\n                if parasite_value < fitness[partner_index]:\n                    population[partner_index] = parasite_vector\n                    fitness[partner_index] = parasite_value\n                \n                if parasite_value < best_value:\n                    best_solution = parasite_vector\n                    best_value = parasite_value\n                \n                if evaluations >= self.budget:\n                    break\n        \n        return best_solution, best_value", "name": "Symbiotic_Optimizer", "description": "A novel Symbiosis-inspired optimizer that combines mutualistic and parasitic interactions to balance exploration and exploitation for flexible adaptation across varying dimensions.", "configspace": "", "generation": 11, "fitness": 0.27288252496571896, "feedback": "The algorithm Symbiotic_Optimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.27 with standard deviation 0.00.", "error": "", "parent_id": "38d046de-25f3-44f7-89f1-0e1977f97fb7", "metadata": {"aucs": [0.27288252496571896, 0.27288252496571896, 0.27288252496571896]}, "mutation_prompt": null}
{"id": "af94a484-c7a1-495e-8858-6cdd411f94ab", "solution": "import numpy as np\n\nclass CoopBAT:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 30  # Number of bats\n        self.f_min = 0            # Minimum frequency\n        self.f_max = 2            # Maximum frequency\n        self.alpha = 0.9          # Loudness reduction factor\n        self.gamma = 0.9          # Pulse rate increase factor\n        self.initial_loudness = 1.0\n        self.initial_pulse_rate = 0.5\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        np.random.seed(42)\n\n        # Initialize the population\n        position = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocity = np.zeros((self.population_size, self.dim))\n        \n        loudness = np.full(self.population_size, self.initial_loudness)\n        pulse_rate = np.full(self.population_size, self.initial_pulse_rate)\n\n        fitness = np.array([func(p) for p in position])\n        best_index = np.argmin(fitness)\n        global_best_position = position[best_index]\n        global_best_value = fitness[best_index]\n        \n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                frequency = np.random.uniform(self.f_min, self.f_max)\n                velocity[i] += (position[i] - global_best_position) * frequency\n                candidate_position = position[i] + velocity[i]\n\n                # Simple bounds check\n                candidate_position = np.clip(candidate_position, lb, ub)\n\n                # If a random number is greater than the pulse rate, perform a local search\n                if np.random.rand() > pulse_rate[i]:\n                    candidate_position = global_best_position + 0.001 * np.random.randn(self.dim)\n                    candidate_position = np.clip(candidate_position, lb, ub)\n                \n                candidate_value = func(candidate_position)\n                evaluations += 1\n\n                # Check if the new solution is better and within the loudness criteria\n                if (candidate_value < fitness[i]) and (np.random.rand() < loudness[i]):\n                    position[i] = candidate_position\n                    fitness[i] = candidate_value\n                    loudness[i] *= self.alpha\n                    pulse_rate[i] = self.initial_pulse_rate * (1 - np.exp(-self.gamma * evaluations/self.budget))\n\n                    # Update global best found so far\n                    if candidate_value < global_best_value:\n                        global_best_position = candidate_position\n                        global_best_value = candidate_value\n\n                if evaluations >= self.budget:\n                    break\n\n        return global_best_position, global_best_value", "name": "CoopBAT", "description": "A cooperative bat algorithm (CoopBAT) that leverages the echolocation-based exploration of Bat Algorithm with cooperative communication among bats to enhance solution convergence in high-dimensional spaces.", "configspace": "", "generation": 12, "fitness": 0.24039653240594494, "feedback": "The algorithm CoopBAT got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.24 with standard deviation 0.00.", "error": "", "parent_id": "38d046de-25f3-44f7-89f1-0e1977f97fb7", "metadata": {"aucs": [0.24039653240594494, 0.24039653240594494, 0.24039653240594494]}, "mutation_prompt": null}
{"id": "73445946-75f6-4c5a-a981-c93638f87f20", "solution": "import numpy as np\n\nclass Enhanced_PSO_DE_Optimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.w = 0.7\n        self.crossover_rate = 0.9\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        np.random.seed(42)\n        \n        position = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocity = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_position = np.copy(position)\n        personal_best_value = np.array([func(p) for p in personal_best_position])\n        global_best_position = personal_best_position[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n        \n        evaluations = self.population_size\n        adaptive_factor = 0.8\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(2)\n                velocity[i] = (self.w * velocity[i] + \n                               self.c1 * r1 * (personal_best_position[i] - position[i]) + \n                               self.c2 * r2 * (global_best_position - position[i]))\n                position[i] += velocity[i]\n                position[i] = np.clip(position[i], lb, ub)\n\n                current_value = func(position[i])\n                evaluations += 1\n                \n                if current_value < personal_best_value[i]:\n                    personal_best_position[i] = position[i]\n                    personal_best_value[i] = current_value\n                \n                if current_value < global_best_value:\n                    global_best_position = position[i]\n                    global_best_value = current_value\n\n                if evaluations >= self.budget:\n                    break\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n                indices = [index for index in range(self.population_size) if index != i]\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                \n                mutation_factor = adaptive_factor * (1 - (evaluations / self.budget))\n                mutant_vector = personal_best_position[a] + mutation_factor * (personal_best_position[b] - personal_best_position[c])\n                mutant_vector = np.clip(mutant_vector, lb, ub)\n                \n                trial_vector = np.copy(position[i])\n                for j in range(self.dim):\n                    if np.random.rand() < self.crossover_rate:\n                        trial_vector[j] = mutant_vector[j]\n\n                trial_value = func(trial_vector)\n                evaluations += 1\n\n                if trial_value < personal_best_value[i]:\n                    personal_best_position[i] = trial_vector\n                    personal_best_value[i] = trial_value\n                \n                if trial_value < global_best_value:\n                    global_best_position = trial_vector\n                    global_best_value = trial_value\n\n        return global_best_position, global_best_value", "name": "Enhanced_PSO_DE_Optimizer", "description": "A multi-strategy optimizer combining Particle Swarm Optimization (PSO) with Differential Evolution (DE) and an adaptive mutation factor for improved convergence in complex high-dimensional spaces.", "configspace": "", "generation": 13, "fitness": 0.2774854060888192, "feedback": "The algorithm Enhanced_PSO_DE_Optimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28 with standard deviation 0.00.", "error": "", "parent_id": "38d046de-25f3-44f7-89f1-0e1977f97fb7", "metadata": {"aucs": [0.2774854060888192, 0.2774854060888192, 0.2774854060888192]}, "mutation_prompt": null}
{"id": "f7218408-9396-493a-a178-8ea436f8c809", "solution": "import numpy as np\n\nclass Quantum_PSO_ADE_Optimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.w = 0.5\n        self.q_factor = 0.9\n        self.crossover_rate = 0.85\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        np.random.seed(42)\n        \n        position = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocity = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_position = np.copy(position)\n        personal_best_value = np.array([func(p) for p in personal_best_position])\n        global_best_position = personal_best_position[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n        \n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(2)\n                velocity[i] = (self.w * velocity[i] + \n                               self.c1 * r1 * (personal_best_position[i] - position[i]) + \n                               self.c2 * r2 * (global_best_position - position[i]))\n                position[i] += velocity[i] + self.q_factor * np.random.normal(size=self.dim)\n                position[i] = np.clip(position[i], lb, ub)\n\n                current_value = func(position[i])\n                evaluations += 1\n                \n                if current_value < personal_best_value[i]:\n                    personal_best_position[i] = position[i]\n                    personal_best_value[i] = current_value\n                \n                if current_value < global_best_value:\n                    global_best_position = position[i]\n                    global_best_value = current_value\n\n                if evaluations >= self.budget:\n                    break\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n                indices = [index for index in range(self.population_size) if index != i]\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                \n                mutation_factor = 0.5 + 0.5 * (np.random.rand() - 0.5)\n                mutant_vector = global_best_position + mutation_factor * (personal_best_position[b] - personal_best_position[c])\n                mutant_vector = np.clip(mutant_vector, lb, ub)\n                \n                trial_vector = np.copy(position[i])\n                for j in range(self.dim):\n                    if np.random.rand() < self.crossover_rate:\n                        trial_vector[j] = mutant_vector[j]\n\n                trial_value = func(trial_vector)\n                evaluations += 1\n\n                if trial_value < personal_best_value[i]:\n                    personal_best_position[i] = trial_vector\n                    personal_best_value[i] = trial_value\n                \n                if trial_value < global_best_value:\n                    global_best_position = trial_vector\n                    global_best_value = trial_value\n\n        return global_best_position, global_best_value", "name": "Quantum_PSO_ADE_Optimizer", "description": "A hybrid optimization approach combining Quantum-inspired PSO with Adaptive Differential Evolution using dynamic neighborhood topologies for enhanced exploration and exploitation.", "configspace": "", "generation": 14, "fitness": 0.2779277634931928, "feedback": "The algorithm Quantum_PSO_ADE_Optimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28 with standard deviation 0.00.", "error": "", "parent_id": "73445946-75f6-4c5a-a981-c93638f87f20", "metadata": {"aucs": [0.2779277634931928, 0.2779277634931928, 0.2779277634931928]}, "mutation_prompt": null}
{"id": "44f4ca99-cfbe-4e5a-9f6b-1c5d1e1e8c63", "solution": "import numpy as np\n\nclass Levy_Firefly_CMA_Optimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.alpha = 0.2\n        self.gamma = 1.0\n        self.beta_min = 0.2\n        self.beta_max = 1.0\n        self.sigma = 0.5\n\n    def levy_flight(self, scale, size):\n        beta = 1.5\n        sigma = (np.gamma(1 + beta) * np.sin(np.pi * beta / 2) / \n                (np.gamma((1 + beta) / 2) * beta * 2 ** ((beta - 1) / 2))) ** (1 / beta)\n        u = np.random.normal(0, sigma, size=size)\n        v = np.random.normal(0, 1, size=size)\n        step = u / np.abs(v) ** (1 / beta)\n        return scale * step\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        np.random.seed(42)\n        \n        position = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(p) for p in position])\n        \n        global_best_position = position[np.argmin(fitness)]\n        global_best_value = np.min(fitness)\n        \n        evaluations = self.population_size\n        mean = np.mean(position, axis=0)\n        cov = np.cov(position, rowvar=False) + np.eye(self.dim) * 1e-5\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                for j in range(self.population_size):\n                    if fitness[i] > fitness[j]:\n                        r = np.linalg.norm(position[i] - position[j])\n                        beta = self.beta_min + (self.beta_max - self.beta_min) * np.exp(-self.gamma * r ** 2)\n                        e = self.alpha * (np.random.rand(self.dim) - 0.5)\n                        position[i] += beta * (position[j] - position[i]) + e\n                        position[i] = np.clip(position[i], lb, ub)\n                        position[i] += self.levy_flight(0.01, self.dim)\n                        position[i] = np.clip(position[i], lb, ub)\n\n                current_value = func(position[i])\n                evaluations += 1\n                \n                if current_value < fitness[i]:\n                    fitness[i] = current_value\n\n                if current_value < global_best_value:\n                    global_best_position = position[i]\n                    global_best_value = current_value\n\n                if evaluations >= self.budget:\n                    break\n            \n            offspring = np.random.multivariate_normal(mean, cov, self.population_size)\n            offspring = np.clip(offspring, lb, ub)\n            offspring_fitness = np.array([func(o) for o in offspring])\n            evaluations += self.population_size\n\n            combined_position = np.vstack((position, offspring))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            position = combined_position[best_indices]\n            fitness = combined_fitness[best_indices]\n            \n            mean = np.mean(position, axis=0)\n            cov = np.cov(position, rowvar=False) + np.eye(self.dim) * 1e-5\n            \n            if evaluations >= self.budget:\n                break\n\n        return global_best_position, global_best_value", "name": "Levy_Firefly_CMA_Optimizer", "description": "An adaptive hybrid algorithm that combines Lévy flight-enhanced Firefly Algorithm with Covariance Matrix Adaptation for global search and local refinement.", "configspace": "", "generation": 15, "fitness": -Infinity, "feedback": "An exception occurred: AttributeError(\"module 'numpy' has no attribute 'gamma'\").", "error": "AttributeError(\"module 'numpy' has no attribute 'gamma'\")", "parent_id": "f7218408-9396-493a-a178-8ea436f8c809", "metadata": {}, "mutation_prompt": null}
{"id": "6e7aa152-30f1-4049-8221-b84ce58b1e02", "solution": "import numpy as np\n\nclass Predator_Prey_Swarm_Optimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = 30\n        self.predator_count = 5\n        self.c1 = 1.5\n        self.c2 = 1.5\n        self.alpha = 0.5\n        self.beta = 0.3\n        self.predator_effect = 1.0\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        np.random.seed(42)\n\n        position = np.random.uniform(lb, ub, (self.swarm_size, self.dim))\n        velocity = np.random.uniform(-1, 1, (self.swarm_size, self.dim))\n        personal_best_position = np.copy(position)\n        personal_best_value = np.array([func(p) for p in personal_best_position])\n        global_best_position = personal_best_position[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        evaluations = self.swarm_size\n\n        while evaluations < self.budget:\n            for i in range(self.swarm_size):\n                if i < self.predator_count:\n                    # Predator dynamics\n                    velocity[i] += self.predator_effect * np.random.normal(size=self.dim)\n                else:\n                    # Prey dynamics\n                    r1, r2 = np.random.rand(2)\n                    velocity[i] = (self.alpha * velocity[i] +\n                                   self.c1 * r1 * (personal_best_position[i] - position[i]) +\n                                   self.c2 * r2 * (global_best_position - position[i]))\n\n                position[i] += velocity[i]\n                position[i] = np.clip(position[i], lb, ub)\n                \n                current_value = func(position[i])\n                evaluations += 1\n                \n                if current_value < personal_best_value[i]:\n                    personal_best_position[i] = position[i]\n                    personal_best_value[i] = current_value\n                \n                if current_value < global_best_value:\n                    global_best_position = position[i]\n                    global_best_value = current_value\n\n                if evaluations >= self.budget:\n                    break\n\n            # Update predator effect dynamically\n            self.predator_effect *= (1.0 - self.beta)\n\n        return global_best_position, global_best_value", "name": "Predator_Prey_Swarm_Optimizer", "description": "A Bio-inspired Swarm Optimization algorithm using Predator-Prey dynamics for adaptive search space exploration and exploitation balance.", "configspace": "", "generation": 16, "fitness": 0.27660803903787345, "feedback": "The algorithm Predator_Prey_Swarm_Optimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28 with standard deviation 0.00.", "error": "", "parent_id": "f7218408-9396-493a-a178-8ea436f8c809", "metadata": {"aucs": [0.27660803903787345, 0.27660803903787345, 0.27660803903787345]}, "mutation_prompt": null}
{"id": "8e38c26a-010d-402c-9c9a-02dc184a337a", "solution": "import numpy as np\n\nclass Enhanced_Quantum_PSO_ADE_Optimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.c1_initial = 2.5\n        self.c2_initial = 0.5\n        self.c1_final = 0.5\n        self.c2_final = 2.5\n        self.w_initial = 0.9\n        self.w_final = 0.4\n        self.q_factor = 0.9\n        self.crossover_rate = 0.85\n    \n    def adaptive_parameters(self, current_eval):\n        progress = current_eval / self.budget\n        self.c1 = self.c1_initial - (self.c1_initial - self.c1_final) * progress\n        self.c2 = self.c2_initial + (self.c2_final - self.c2_initial) * progress\n        self.w = self.w_initial - (self.w_initial - self.w_final) * progress\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        np.random.seed(42)\n        \n        position = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocity = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_position = np.copy(position)\n        personal_best_value = np.array([func(p) for p in personal_best_position])\n        global_best_position = personal_best_position[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n        \n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            self.adaptive_parameters(evaluations)\n            \n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(2)\n                velocity[i] = (self.w * velocity[i] + \n                               self.c1 * r1 * (personal_best_position[i] - position[i]) + \n                               self.c2 * r2 * (global_best_position - position[i]))\n                position[i] += velocity[i] + self.q_factor * np.random.normal(size=self.dim)\n                position[i] = np.clip(position[i], lb, ub)\n\n                current_value = func(position[i])\n                evaluations += 1\n                \n                if current_value < personal_best_value[i]:\n                    personal_best_position[i] = position[i]\n                    personal_best_value[i] = current_value\n                \n                if current_value < global_best_value:\n                    global_best_position = position[i]\n                    global_best_value = current_value\n\n                if evaluations >= self.budget:\n                    break\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n                indices = [index for index in range(self.population_size) if index != i]\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                \n                mutation_factor = 0.5 + 0.5 * (np.random.rand() - 0.5)\n                mutant_vector = global_best_position + mutation_factor * (personal_best_position[b] - personal_best_position[c])\n                mutant_vector = np.clip(mutant_vector, lb, ub)\n                \n                trial_vector = np.copy(position[i])\n                for j in range(self.dim):\n                    if np.random.rand() < self.crossover_rate:\n                        trial_vector[j] = mutant_vector[j]\n\n                trial_value = func(trial_vector)\n                evaluations += 1\n\n                if trial_value < personal_best_value[i]:\n                    personal_best_position[i] = trial_vector\n                    personal_best_value[i] = trial_value\n                \n                if trial_value < global_best_value:\n                    global_best_position = trial_vector\n                    global_best_value = trial_value\n\n        return global_best_position, global_best_value", "name": "Enhanced_Quantum_PSO_ADE_Optimizer", "description": "Enhanced Quantum PSO-ADE with a dual-phase adaptive strategy for dynamic exploration and exploitation balance.", "configspace": "", "generation": 17, "fitness": 0.2736559581685519, "feedback": "The algorithm Enhanced_Quantum_PSO_ADE_Optimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.27 with standard deviation 0.00.", "error": "", "parent_id": "f7218408-9396-493a-a178-8ea436f8c809", "metadata": {"aucs": [0.2736559581685519, 0.2736559581685519, 0.2736559581685519]}, "mutation_prompt": null}
{"id": "8c386c83-7a0c-4144-bd0f-39c042270cc4", "solution": "import numpy as np\n\nclass Swarm_Enhanced_Memetic_Algorithm:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.c1 = 1.5\n        self.c2 = 1.5\n        self.w = 0.7\n        self.local_search_iterations = 5\n        self.elitism_rate = 0.3\n\n    def local_search(self, individual, func, lb, ub):\n        best_local = individual\n        best_value = func(individual)\n        for _ in range(self.local_search_iterations):\n            candidate = best_local + 0.1 * np.random.normal(size=self.dim)\n            candidate = np.clip(candidate, lb, ub)\n            candidate_value = func(candidate)\n            if candidate_value < best_value:\n                best_local = candidate\n                best_value = candidate_value\n        return best_local, best_value\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        np.random.seed(42)\n\n        position = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocity = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_position = np.copy(position)\n        personal_best_value = np.array([func(p) for p in personal_best_position])\n        global_best_position = personal_best_position[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(2)\n                velocity[i] = (self.w * velocity[i] + \n                               self.c1 * r1 * (personal_best_position[i] - position[i]) + \n                               self.c2 * r2 * (global_best_position - position[i]))\n                position[i] += velocity[i]\n                position[i] = np.clip(position[i], lb, ub)\n\n                current_value = func(position[i])\n                evaluations += 1\n\n                if current_value < personal_best_value[i]:\n                    personal_best_position[i] = position[i]\n                    personal_best_value[i] = current_value\n\n                if current_value < global_best_value:\n                    global_best_position = position[i]\n                    global_best_value = current_value\n\n                if evaluations >= self.budget:\n                    break\n\n            # Apply local search to a fraction of best individuals\n            elite_count = int(self.elitism_rate * self.population_size)\n            elite_indices = np.argsort(personal_best_value)[:elite_count]\n            for i in elite_indices:\n                if evaluations >= self.budget:\n                    break\n                new_position, new_value = self.local_search(personal_best_position[i], func, lb, ub)\n                evaluations += 1\n                if new_value < personal_best_value[i]:\n                    personal_best_position[i] = new_position\n                    personal_best_value[i] = new_value\n\n                if new_value < global_best_value:\n                    global_best_position = new_position\n                    global_best_value = new_value\n\n        return global_best_position, global_best_value", "name": "Swarm_Enhanced_Memetic_Algorithm", "description": "A novel Swarm-Enhanced Memetic Algorithm (SEMA) combining Swarm Intelligence with Memetic Strategies via local search and elitist recombination for efficient global optimization.", "configspace": "", "generation": 18, "fitness": 0.2774755867262937, "feedback": "The algorithm Swarm_Enhanced_Memetic_Algorithm got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28 with standard deviation 0.00.", "error": "", "parent_id": "f7218408-9396-493a-a178-8ea436f8c809", "metadata": {"aucs": [0.2774755867262937, 0.2774755867262937, 0.2774755867262937]}, "mutation_prompt": null}
{"id": "b4f90c45-a14a-4de8-9cfa-1a8e652093b2", "solution": "import numpy as np\n\nclass Quantum_Pso_Gauss_Exploration_Optimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.w = 0.5\n        self.q_factor = 0.9\n        self.gaussian_scale = 0.1\n        self.reset_chance = 0.1\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        np.random.seed(42)\n\n        position = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocity = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_position = np.copy(position)\n        personal_best_value = np.array([func(p) for p in personal_best_position])\n        global_best_position = personal_best_position[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n        \n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(2)\n                velocity[i] = (self.w * velocity[i] + \n                               self.c1 * r1 * (personal_best_position[i] - position[i]) + \n                               self.c2 * r2 * (global_best_position - position[i]))\n                position[i] += velocity[i] + self.q_factor * np.random.normal(scale=self.gaussian_scale, size=self.dim)\n                position[i] = np.clip(position[i], lb, ub)\n\n                current_value = func(position[i])\n                evaluations += 1\n                \n                if current_value < personal_best_value[i]:\n                    personal_best_position[i] = position[i]\n                    personal_best_value[i] = current_value\n                \n                if current_value < global_best_value:\n                    global_best_position = position[i]\n                    global_best_value = current_value\n\n                if evaluations >= self.budget:\n                    break\n\n            # Global Position Reset Mechanism\n            if np.random.rand() < self.reset_chance:\n                random_index = np.random.randint(self.population_size)\n                position[random_index] = np.random.uniform(lb, ub, self.dim)\n                current_value = func(position[random_index])\n                evaluations += 1\n                \n                if current_value < personal_best_value[random_index]:\n                    personal_best_position[random_index] = position[random_index]\n                    personal_best_value[random_index] = current_value\n                \n                if current_value < global_best_value:\n                    global_best_position = position[random_index]\n                    global_best_value = current_value\n\n        return global_best_position, global_best_value", "name": "Quantum_Pso_Gauss_Exploration_Optimizer", "description": "Quantum-Pso-Gauss-Exploration-Optimizer: Integrates Quantum PSO with Gaussian exploration for enhanced local search and a global position reset mechanism to escape local minima.", "configspace": "", "generation": 19, "fitness": 0.2779432265221423, "feedback": "The algorithm Quantum_Pso_Gauss_Exploration_Optimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28 with standard deviation 0.00.", "error": "", "parent_id": "f7218408-9396-493a-a178-8ea436f8c809", "metadata": {"aucs": [0.2779432265221423, 0.2779432265221423, 0.2779432265221423]}, "mutation_prompt": null}
{"id": "72769495-7f16-48f9-a769-e005ba337f30", "solution": "import numpy as np\n\nclass Quantum_Pso_Gauss_Mutation_Optimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.w = 0.5\n        self.q_factor = 0.9\n        self.gaussian_scale = 0.1\n        self.reset_chance = 0.1\n        self.mutation_rate = 0.2\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        np.random.seed(42)\n\n        position = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocity = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_position = np.copy(position)\n        personal_best_value = np.array([func(p) for p in personal_best_position])\n        global_best_position = personal_best_position[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n        \n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(2)\n                velocity[i] = (self.w * velocity[i] + \n                               self.c1 * r1 * (personal_best_position[i] - position[i]) + \n                               self.c2 * r2 * (global_best_position - position[i]))\n                \n                position[i] += velocity[i] + self.q_factor * np.random.normal(scale=self.gaussian_scale, size=self.dim)\n                position[i] = np.clip(position[i], lb, ub)\n\n                if np.random.rand() < self.mutation_rate:\n                    mutation = np.random.normal(scale=self.gaussian_scale, size=self.dim)\n                    position[i] += mutation\n                    position[i] = np.clip(position[i], lb, ub)\n\n                current_value = func(position[i])\n                evaluations += 1\n                \n                if current_value < personal_best_value[i]:\n                    personal_best_position[i] = position[i]\n                    personal_best_value[i] = current_value\n                \n                if current_value < global_best_value:\n                    global_best_position = position[i]\n                    global_best_value = current_value\n\n                if evaluations >= self.budget:\n                    break\n\n            # Global Position Reset Mechanism\n            if np.random.rand() < self.reset_chance:\n                random_index = np.random.randint(self.population_size)\n                position[random_index] = np.random.uniform(lb, ub, self.dim)\n                current_value = func(position[random_index])\n                evaluations += 1\n                \n                if current_value < personal_best_value[random_index]:\n                    personal_best_position[random_index] = position[random_index]\n                    personal_best_value[random_index] = current_value\n                \n                if current_value < global_best_value:\n                    global_best_position = position[random_index]\n                    global_best_value = current_value\n\n        return global_best_position, global_best_value", "name": "Quantum_Pso_Gauss_Mutation_Optimizer", "description": "Quantum-Pso-Gauss-Mutation-Optimizer: Enhances exploration by incorporating adaptive Gaussian mutation and stochastic velocity update, boosting escape from local minima and convergence speed.", "configspace": "", "generation": 20, "fitness": 0.2775450183393735, "feedback": "The algorithm Quantum_Pso_Gauss_Mutation_Optimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28 with standard deviation 0.00.", "error": "", "parent_id": "b4f90c45-a14a-4de8-9cfa-1a8e652093b2", "metadata": {"aucs": [0.2775450183393735, 0.2775450183393735, 0.2775450183393735]}, "mutation_prompt": null}
{"id": "5c5cb982-9014-4cb7-b678-93a0f3a52963", "solution": "import numpy as np\n\nclass Adaptive_Swarm_Quantum_Collaborative_Optimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.inertia_weight = 0.9\n        self.collab_weight = 0.5\n        self.inertia_damping = 0.99\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        np.random.seed(42)\n\n        position = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocity = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_position = np.copy(position)\n        personal_best_value = np.array([func(p) for p in personal_best_position])\n        global_best_position = personal_best_position[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(2)\n                inertia = self.inertia_weight * velocity[i]\n                cognitive = self.c1 * r1 * (personal_best_position[i] - position[i])\n                social = self.c2 * r2 * (global_best_position - position[i])\n                \n                # Quantum-inspired collaboration\n                partner_index = np.random.randint(self.population_size)\n                collaborative = self.collab_weight * (position[partner_index] - position[i])\n\n                velocity[i] = inertia + cognitive + social + collaborative\n                position[i] += velocity[i]\n                position[i] = np.clip(position[i], lb, ub)\n\n                current_value = func(position[i])\n                evaluations += 1\n\n                if current_value < personal_best_value[i]:\n                    personal_best_position[i] = position[i]\n                    personal_best_value[i] = current_value\n                \n                if current_value < global_best_value:\n                    global_best_position = position[i]\n                    global_best_value = current_value\n\n                if evaluations >= self.budget:\n                    break\n\n            self.inertia_weight *= self.inertia_damping\n\n        return global_best_position, global_best_value", "name": "Adaptive_Swarm_Quantum_Collaborative_Optimizer", "description": "Adaptive-Swarm-Quantum-Collaborative-Optimizer: Combines adaptive inertia in PSO with quantum-inspired collaboration to dynamically balance exploration and exploitation for photonic structure optimization.", "configspace": "", "generation": 21, "fitness": 0.2694078714655076, "feedback": "The algorithm Adaptive_Swarm_Quantum_Collaborative_Optimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.27 with standard deviation 0.00.", "error": "", "parent_id": "b4f90c45-a14a-4de8-9cfa-1a8e652093b2", "metadata": {"aucs": [0.2694078714655076, 0.2694078714655076, 0.2694078714655076]}, "mutation_prompt": null}
{"id": "16652381-2d56-40dc-b538-0dc5fdeaa0d5", "solution": "import numpy as np\n\nclass Quantum_Pso_Adaptive_Gauss_Optimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.w_max = 0.9\n        self.w_min = 0.4\n        self.q_factor = 0.9\n        self.gaussian_scale = 0.1\n        self.reset_chance = 0.1\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        np.random.seed(42)\n\n        position = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocity = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_position = np.copy(position)\n        personal_best_value = np.array([func(p) for p in personal_best_position])\n        global_best_position = personal_best_position[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n        \n        evaluations = self.population_size\n        iteration = 0\n        max_iterations = self.budget // self.population_size\n\n        while evaluations < self.budget:\n            w = self.w_max - ((self.w_max - self.w_min) * (iteration / max_iterations))\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(2)\n                velocity[i] = (w * velocity[i] + \n                               self.c1 * r1 * (personal_best_position[i] - position[i]) + \n                               self.c2 * r2 * (global_best_position - position[i]))\n                position[i] += velocity[i] + self.q_factor * np.random.normal(scale=self.gaussian_scale * (1 - iteration / max_iterations), size=self.dim)\n                position[i] = np.clip(position[i], lb, ub)\n\n                current_value = func(position[i])\n                evaluations += 1\n                \n                if current_value < personal_best_value[i]:\n                    personal_best_position[i] = position[i]\n                    personal_best_value[i] = current_value\n                \n                if current_value < global_best_value:\n                    global_best_position = position[i]\n                    global_best_value = current_value\n\n                if evaluations >= self.budget:\n                    break\n\n            if np.random.rand() < self.reset_chance:\n                random_index = np.random.randint(self.population_size)\n                position[random_index] = np.random.uniform(lb, ub, self.dim)\n                current_value = func(position[random_index])\n                evaluations += 1\n                \n                if current_value < personal_best_value[random_index]:\n                    personal_best_position[random_index] = position[random_index]\n                    personal_best_value[random_index] = current_value\n                \n                if current_value < global_best_value:\n                    global_best_position = position[random_index]\n                    global_best_value = current_value\n\n            iteration += 1\n\n        return global_best_position, global_best_value", "name": "Quantum_Pso_Adaptive_Gauss_Optimizer", "description": "Quantum PSO with Adaptive Gaussian Mutation: Enhances exploration and exploitation balance by integrating adaptive Gaussian mutation based on current convergence rate and dynamically adjusting inertia weight.", "configspace": "", "generation": 22, "fitness": 0.27630408004601426, "feedback": "The algorithm Quantum_Pso_Adaptive_Gauss_Optimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28 with standard deviation 0.00.", "error": "", "parent_id": "b4f90c45-a14a-4de8-9cfa-1a8e652093b2", "metadata": {"aucs": [0.27630408004601426, 0.27630408004601426, 0.27630408004601426]}, "mutation_prompt": null}
{"id": "dc2af1a8-aadd-4210-84c2-ca8a3d6dfc52", "solution": "import numpy as np\n\nclass Adaptive_Quantum_Simulated_Annealing_Optimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.initial_temperature = 100.0\n        self.cooling_rate = 0.99\n        self.min_temperature = 1e-3\n        self.quantum_tunneling_factor = 0.1\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        np.random.seed(42)\n\n        position = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        best_position = np.copy(position)\n        best_value = np.array([func(p) for p in best_position])\n        global_best_position = best_position[np.argmin(best_value)]\n        global_best_value = np.min(best_value)\n\n        temperature = self.initial_temperature\n        evaluations = self.population_size\n\n        while evaluations < self.budget and temperature > self.min_temperature:\n            for i in range(self.population_size):\n                current = position[i]\n                new_position = current + np.random.normal(scale=temperature, size=self.dim)\n                new_position = np.clip(new_position, lb, ub)\n                \n                current_value = func(current)\n                new_value = func(new_position)\n                evaluations += 2\n\n                if new_value < current_value or np.exp((current_value - new_value) / temperature) > np.random.rand():\n                    position[i] = new_position\n                    if new_value < best_value[i]:\n                        best_position[i] = new_position\n                        best_value[i] = new_value\n                        if new_value < global_best_value:\n                            global_best_position = new_position\n                            global_best_value = new_value\n\n                if evaluations >= self.budget:\n                    break\n\n            # Quantum Tunneling Mechanism\n            if np.random.rand() < self.quantum_tunneling_factor:\n                random_index = np.random.randint(self.population_size)\n                tunneling_position = np.random.uniform(lb, ub, self.dim)\n                tunneling_value = func(tunneling_position)\n                evaluations += 1\n                \n                if tunneling_value < best_value[random_index]:\n                    position[random_index] = tunneling_position\n                    best_position[random_index] = tunneling_position\n                    best_value[random_index] = tunneling_value\n                    if tunneling_value < global_best_value:\n                        global_best_position = tunneling_position\n                        global_best_value = tunneling_value\n\n            temperature *= self.cooling_rate\n\n        return global_best_position, global_best_value", "name": "Adaptive_Quantum_Simulated_Annealing_Optimizer", "description": "Adaptive Quantum Simulated Annealing optimizes global exploration with quantum-inspired tunneling and adaptive cooling for enhanced performance.", "configspace": "", "generation": 23, "fitness": 0.2508860355020054, "feedback": "The algorithm Adaptive_Quantum_Simulated_Annealing_Optimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.25 with standard deviation 0.00.", "error": "", "parent_id": "b4f90c45-a14a-4de8-9cfa-1a8e652093b2", "metadata": {"aucs": [0.2508860355020054, 0.2508860355020054, 0.2508860355020054]}, "mutation_prompt": null}
{"id": "c71499fd-868b-49ff-a9e6-a0bf747e8697", "solution": "import numpy as np\n\nclass Enhanced_Quantum_Pso_Optimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.w_max = 0.9\n        self.w_min = 0.4\n        self.q_factor = 0.9\n        self.initial_gaussian_scale = 0.1\n        self.reset_chance = 0.1\n        self.share_frequency = 5\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        np.random.seed(42)\n\n        position = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocity = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_position = np.copy(position)\n        personal_best_value = np.array([func(p) for p in personal_best_position])\n        global_best_position = personal_best_position[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n        \n        evaluations = self.population_size\n        iteration = 0\n\n        while evaluations < self.budget:\n            w = self.w_max - ((self.w_max - self.w_min) * (evaluations / self.budget))\n            gaussian_scale = self.initial_gaussian_scale * (1 - evaluations / self.budget)\n            \n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(2)\n                velocity[i] = (w * velocity[i] + \n                               self.c1 * r1 * (personal_best_position[i] - position[i]) + \n                               self.c2 * r2 * (global_best_position - position[i]))\n                position[i] += velocity[i] + self.q_factor * np.random.normal(scale=gaussian_scale, size=self.dim)\n                position[i] = np.clip(position[i], lb, ub)\n\n                current_value = func(position[i])\n                evaluations += 1\n                \n                if current_value < personal_best_value[i]:\n                    personal_best_position[i] = position[i]\n                    personal_best_value[i] = current_value\n                \n                if current_value < global_best_value:\n                    global_best_position = position[i]\n                    global_best_value = current_value\n\n                if evaluations >= self.budget:\n                    break\n\n            # Global Position Reset Mechanism\n            if np.random.rand() < self.reset_chance:\n                random_index = np.random.randint(self.population_size)\n                position[random_index] = np.random.uniform(lb, ub, self.dim)\n                current_value = func(position[random_index])\n                evaluations += 1\n                \n                if current_value < personal_best_value[random_index]:\n                    personal_best_position[random_index] = position[random_index]\n                    personal_best_value[random_index] = current_value\n                \n                if current_value < global_best_value:\n                    global_best_position = position[random_index]\n                    global_best_value = current_value\n\n            # Share Global Best Position every few iterations\n            if iteration % self.share_frequency == 0:\n                for i in range(self.population_size):\n                    if np.random.rand() < 0.5:\n                        position[i] = global_best_position + np.random.normal(scale=gaussian_scale, size=self.dim)\n                        position[i] = np.clip(position[i], lb, ub)\n\n            iteration += 1\n\n        return global_best_position, global_best_value", "name": "Enhanced_Quantum_Pso_Optimizer", "description": "Enhanced Quantum PSO with Dynamic Inertia, Adaptive Gaussian Exploration, and Frequent Global Best Sharing for improved convergence and robustness.", "configspace": "", "generation": 24, "fitness": 0.27677468958703344, "feedback": "The algorithm Enhanced_Quantum_Pso_Optimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28 with standard deviation 0.00.", "error": "", "parent_id": "b4f90c45-a14a-4de8-9cfa-1a8e652093b2", "metadata": {"aucs": [0.27677468958703344, 0.27677468958703344, 0.27677468958703344]}, "mutation_prompt": null}
{"id": "23a31d7a-9c0f-430c-a37f-f7caeaedc69a", "solution": "import numpy as np\n\nclass Enhanced_Quantum_Pso_Gauss_Temporal_Momentum_Optimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.w = 0.5\n        self.q_factor = 0.9\n        self.gaussian_scale = 0.1\n        self.reset_chance = 0.1\n        self.momentum_factor = 1.0\n        self.adaptive_rate = 0.95\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        np.random.seed(42)\n\n        position = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocity = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_position = np.copy(position)\n        personal_best_value = np.array([func(p) for p in personal_best_position])\n        global_best_position = personal_best_position[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n        \n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(2)\n                self.w *= self.adaptive_rate  # Dynamically adjust inertia weight\n                velocity[i] = (self.w * velocity[i] +\n                               self.c1 * r1 * (personal_best_position[i] - position[i]) +\n                               self.c2 * r2 * (global_best_position - position[i]))\n                \n                adaptive_gaussian_scale = self.gaussian_scale * (1 - evaluations / self.budget)\n                position[i] += (velocity[i] + \n                                self.q_factor * np.random.normal(scale=adaptive_gaussian_scale, size=self.dim))\n                position[i] = np.clip(position[i], lb, ub)\n\n                current_value = func(position[i])\n                evaluations += 1\n                \n                if current_value < personal_best_value[i]:\n                    personal_best_position[i] = position[i]\n                    personal_best_value[i] = current_value\n                \n                if current_value < global_best_value:\n                    global_best_position = position[i]\n                    global_best_value = current_value\n\n                if evaluations >= self.budget:\n                    break\n\n            # Global Position Reset Mechanism\n            if np.random.rand() < self.reset_chance:\n                random_index = np.random.randint(self.population_size)\n                position[random_index] = np.random.uniform(lb, ub, self.dim)\n                current_value = func(position[random_index])\n                evaluations += 1\n                \n                if current_value < personal_best_value[random_index]:\n                    personal_best_position[random_index] = position[random_index]\n                    personal_best_value[random_index] = current_value\n                \n                if current_value < global_best_value:\n                    global_best_position = position[random_index]\n                    global_best_value = current_value\n\n        return global_best_position, global_best_value", "name": "Enhanced_Quantum_Pso_Gauss_Temporal_Momentum_Optimizer", "description": "Enhanced_Quantum_Pso_Gauss_Temporal_Momentum_Optimizer: Introduces adaptive temporal momentum and dynamic parameter adjustment to improve convergence speed and maintain diversity in global and local search spaces.", "configspace": "", "generation": 25, "fitness": 0.27805510120396415, "feedback": "The algorithm Enhanced_Quantum_Pso_Gauss_Temporal_Momentum_Optimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28 with standard deviation 0.00.", "error": "", "parent_id": "b4f90c45-a14a-4de8-9cfa-1a8e652093b2", "metadata": {"aucs": [0.27805510120396415, 0.27805510120396415, 0.27805510120396415]}, "mutation_prompt": null}
{"id": "b23ccec9-5059-48d4-91cd-d292af8c4d79", "solution": "import numpy as np\nimport scipy.stats\n\nclass Adaptive_Quantum_PSO_Levy_LocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.w = 0.729  # Inertia weight\n        self.q_factor = 0.9\n        self.alpha = 1.5  # Parameter for Lévy flight\n        self.local_search_chance = 0.3\n        self.ls_scale = 0.02  # Scale of local search\n\n    def levy_flight(self):\n        return scipy.stats.levy_stable.rvs(self.alpha, 0, size=self.dim)\n\n    def local_search(self, position, lb, ub):\n        perturbation = np.random.normal(0, self.ls_scale, size=position.shape)\n        return np.clip(position + perturbation, lb, ub)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        np.random.seed(42)\n\n        position = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocity = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_position = np.copy(position)\n        personal_best_value = np.array([func(p) for p in personal_best_position])\n        global_best_position = personal_best_position[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n        \n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(2)\n                velocity[i] = (self.w * velocity[i] +\n                               self.c1 * r1 * (personal_best_position[i] - position[i]) +\n                               self.c2 * r2 * (global_best_position - position[i]))\n\n                # Apply Levy flight for exploration\n                if np.random.rand() < 0.5:\n                    position[i] += self.levy_flight() * self.q_factor\n\n                position[i] += velocity[i]\n                position[i] = np.clip(position[i], lb, ub)\n\n                current_value = func(position[i])\n                evaluations += 1\n                \n                if current_value < personal_best_value[i]:\n                    personal_best_position[i] = position[i]\n                    personal_best_value[i] = current_value\n                \n                if current_value < global_best_value:\n                    global_best_position = position[i]\n                    global_best_value = current_value\n\n                if evaluations >= self.budget:\n                    break\n\n            # Local Search Mechanism\n            if np.random.rand() < self.local_search_chance:\n                for i in range(self.population_size):\n                    new_position = self.local_search(personal_best_position[i], lb, ub)\n                    new_value = func(new_position)\n                    evaluations += 1\n\n                    if new_value < personal_best_value[i]:\n                        personal_best_position[i] = new_position\n                        personal_best_value[i] = new_value\n\n                    if new_value < global_best_value:\n                        global_best_position = new_position\n                        global_best_value = new_value\n\n                    if evaluations >= self.budget:\n                        break\n\n        return global_best_position, global_best_value", "name": "Adaptive_Quantum_PSO_Levy_LocalSearch", "description": "Adaptive Quantum Particle Swarm with Lévy Flights and Local Search: Introduces Lévy flights for exploration and adaptive local search phases to enhance convergence and solution quality in complex optimization landscapes.", "configspace": "", "generation": 26, "fitness": 0.27769650193978523, "feedback": "The algorithm Adaptive_Quantum_PSO_Levy_LocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28 with standard deviation 0.00.", "error": "", "parent_id": "23a31d7a-9c0f-430c-a37f-f7caeaedc69a", "metadata": {"aucs": [0.27769650193978523, 0.27769650193978523, 0.27769650193978523]}, "mutation_prompt": null}
{"id": "496309f4-3eda-4484-a961-48007d69dd50", "solution": "import numpy as np\n\nclass Quantum_Inspired_Dynamic_Inertia_PSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.w_max = 0.9\n        self.w_min = 0.4\n        self.q_factor = 0.9\n        self.gaussian_scale = 0.1\n        self.reset_chance = 0.1\n        self.momentum_factor = 1.0\n        self.mutation_prob = 0.1\n        self.mutation_scale = 0.02\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        np.random.seed(42)\n\n        position = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocity = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_position = np.copy(position)\n        personal_best_value = np.array([func(p) for p in personal_best_position])\n        global_best_position = personal_best_position[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n        \n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(2)\n                w = self.w_max - (self.w_max - self.w_min) * (evaluations / self.budget)\n                velocity[i] = (w * velocity[i] +\n                               self.c1 * r1 * (personal_best_position[i] - position[i]) +\n                               self.c2 * r2 * (global_best_position - position[i]))\n                \n                adaptive_gaussian_scale = self.gaussian_scale * (1 - evaluations / self.budget)\n                position[i] += (velocity[i] + \n                                self.q_factor * np.random.normal(scale=adaptive_gaussian_scale, size=self.dim))\n                position[i] = np.clip(position[i], lb, ub)\n\n                if np.random.rand() < self.mutation_prob:\n                    mutation = np.random.normal(0, self.mutation_scale, self.dim)\n                    position[i] += mutation\n                    position[i] = np.clip(position[i], lb, ub)\n\n                current_value = func(position[i])\n                evaluations += 1\n                \n                if current_value < personal_best_value[i]:\n                    personal_best_position[i] = position[i]\n                    personal_best_value[i] = current_value\n                \n                if current_value < global_best_value:\n                    global_best_position = position[i]\n                    global_best_value = current_value\n\n                if evaluations >= self.budget:\n                    break\n\n            if np.random.rand() < self.reset_chance:\n                random_index = np.random.randint(self.population_size)\n                position[random_index] = np.random.uniform(lb, ub, self.dim)\n                current_value = func(position[random_index])\n                evaluations += 1\n                \n                if current_value < personal_best_value[random_index]:\n                    personal_best_position[random_index] = position[random_index]\n                    personal_best_value[random_index] = current_value\n                \n                if current_value < global_best_value:\n                    global_best_position = position[random_index]\n                    global_best_value = current_value\n\n        return global_best_position, global_best_value", "name": "Quantum_Inspired_Dynamic_Inertia_PSO", "description": "Quantum-Inspired Dynamic Inertia PSO integrates adaptive mutation and variable inertia for enhanced exploration-exploitation balance.", "configspace": "", "generation": 27, "fitness": 0.275610178920206, "feedback": "The algorithm Quantum_Inspired_Dynamic_Inertia_PSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28 with standard deviation 0.00.", "error": "", "parent_id": "23a31d7a-9c0f-430c-a37f-f7caeaedc69a", "metadata": {"aucs": [0.275610178920206, 0.275610178920206, 0.275610178920206]}, "mutation_prompt": null}
{"id": "e815ab0a-24e3-49b0-a087-4fe266881433", "solution": "import numpy as np\n\nclass Quantum_Genetic_Adaptive_Network_Optimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.crossover_rate = 0.8\n        self.mutation_rate = 0.1\n        self.learning_rate = 0.1\n        self.adaptive_decay = 0.99\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        np.random.seed(42)\n\n        position = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(p) for p in position])\n        best_idx = np.argmin(fitness)\n        global_best_position = np.copy(position[best_idx])\n        global_best_value = fitness[best_idx]\n        \n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            new_population = np.copy(position)\n\n            # Genetic crossover and mutation\n            for i in range(self.population_size):\n                if np.random.rand() < self.crossover_rate:\n                    parent2 = position[np.random.randint(self.population_size)]\n                    crossover_point = np.random.randint(self.dim)\n                    new_population[i, :crossover_point] = position[i, :crossover_point]\n                    new_population[i, crossover_point:] = parent2[crossover_point:]\n\n                if np.random.rand() < self.mutation_rate:\n                    mutation_indices = np.random.choice(self.dim, size=int(0.1 * self.dim), replace=False)\n                    new_population[i, mutation_indices] += np.random.normal(0, 0.1, len(mutation_indices))\n                    new_population[i] = np.clip(new_population[i], lb, ub)\n\n            # Quantum-inspired update\n            for i in range(self.population_size):\n                phi = np.random.uniform(0, 2 * np.pi, self.dim)\n                shift = self.learning_rate * (global_best_position - position[i]) * np.random.random(self.dim)\n                position[i] += shift * np.sin(phi)\n                position[i] = np.clip(position[i], lb, ub)\n\n                current_value = func(position[i])\n                evaluations += 1\n                \n                if current_value < fitness[i]:\n                    new_population[i] = position[i]\n                    fitness[i] = current_value\n                \n                if current_value < global_best_value:\n                    global_best_position = position[i]\n                    global_best_value = current_value\n\n                if evaluations >= self.budget:\n                    break\n\n            # Adaptive learning rate decay\n            self.learning_rate *= self.adaptive_decay\n            position = new_population\n\n        return global_best_position, global_best_value", "name": "Quantum_Genetic_Adaptive_Network_Optimizer", "description": "Quantum_Genetic_Adaptive_Network_Optimizer: Combines quantum-inspired position updates with genetic crossover and adaptive learning rate for enhanced exploration and exploitation in complex search spaces.", "configspace": "", "generation": 28, "fitness": 0.24101603032867547, "feedback": "The algorithm Quantum_Genetic_Adaptive_Network_Optimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.24 with standard deviation 0.00.", "error": "", "parent_id": "23a31d7a-9c0f-430c-a37f-f7caeaedc69a", "metadata": {"aucs": [0.24101603032867547, 0.24101603032867547, 0.24101603032867547]}, "mutation_prompt": null}
{"id": "28184bb8-289d-4d9e-b79d-e4f112978508", "solution": "import numpy as np\n\nclass Adaptive_Differential_Evolution_Quantum_Levy:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 30\n        self.F = 0.7\n        self.CR = 0.9\n        self.adaptive_scale = 0.5\n        self.q_factor = 0.1\n        self.levy_alpha = 1.5\n        self.levy_beta = 0.007 \n\n    def levy_flight(self, u):\n        num = np.random.normal(0, self.levy_beta, self.dim)\n        den = np.power(np.abs(np.random.normal(0, 1, self.dim)), 1 / self.levy_alpha)\n        step = num / den\n        return u + step\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        np.random.seed(42)\n\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n\n        best_idx = np.argmin(fitness)\n        best_pos = population[best_idx]\n        best_val = fitness[best_idx]\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n\n                mutant = a + self.F * (b - c)\n                mutant = np.clip(mutant, lb, ub)\n\n                trial = np.where(np.random.rand(self.dim) < self.CR, mutant, population[i])\n                trial = self.levy_flight(trial)\n                trial = np.clip(trial, lb, ub)\n\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n                    if trial_fitness < best_val:\n                        best_pos = trial\n                        best_val = trial_fitness\n\n                if evaluations >= self.budget:\n                    break\n\n            self.F *= self.adaptive_scale  # Dynamically adjust differential weight\n\n        return best_pos, best_val", "name": "Adaptive_Differential_Evolution_Quantum_Levy", "description": "Adaptive Differential Evolution with Quantum-Inspired Levy Flights to enhance exploration and exploitation balance.", "configspace": "", "generation": 29, "fitness": 0.26254089758054844, "feedback": "The algorithm Adaptive_Differential_Evolution_Quantum_Levy got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.26 with standard deviation 0.00.", "error": "", "parent_id": "23a31d7a-9c0f-430c-a37f-f7caeaedc69a", "metadata": {"aucs": [0.26254089758054844, 0.26254089758054844, 0.26254089758054844]}, "mutation_prompt": null}
{"id": "af1b48b0-6301-4723-acea-045b529c5336", "solution": "import numpy as np\n\nclass Integrated_Quantum_Pso_Adaptive_Inertia_Elite_OBL:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.w = 0.9\n        self.q_factor = 0.9\n        self.gaussian_scale = 0.1\n        self.reset_chance = 0.05\n        self.adaptive_rate = 0.99\n        self.elite_fraction = 0.1\n        self.evaluations = 0\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        np.random.seed(42)\n        \n        # Initialize positions and velocities\n        position = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocity = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_position = np.copy(position)\n        personal_best_value = np.array([func(p) for p in personal_best_position])\n        global_best_position = personal_best_position[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        self.evaluations = self.population_size\n\n        while self.evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(2)\n                self.w *= self.adaptive_rate  # Gradually decrease inertia weight\n                velocity[i] = (self.w * velocity[i] +\n                               self.c1 * r1 * (personal_best_position[i] - position[i]) +\n                               self.c2 * r2 * (global_best_position - position[i]))\n\n                adaptive_gaussian_scale = self.gaussian_scale * (1 - self.evaluations / self.budget)\n                position[i] += (velocity[i] + \n                                self.q_factor * np.random.normal(scale=adaptive_gaussian_scale, size=self.dim))\n                position[i] = np.clip(position[i], lb, ub)\n\n                current_value = func(position[i])\n                self.evaluations += 1\n\n                if current_value < personal_best_value[i]:\n                    personal_best_position[i] = position[i]\n                    personal_best_value[i] = current_value\n\n                if current_value < global_best_value:\n                    global_best_position = position[i]\n                    global_best_value = current_value\n\n                if self.evaluations >= self.budget:\n                    break\n\n            # Elite Opposition-Based Learning\n            elite_count = int(self.elite_fraction * self.population_size)\n            elite_indices = np.argsort(personal_best_value)[:elite_count]\n            for idx in elite_indices:\n                opposite_position = lb + ub - personal_best_position[idx]\n                opposite_position = np.clip(opposite_position, lb, ub)\n                opposite_value = func(opposite_position)\n                self.evaluations += 1\n\n                if opposite_value < personal_best_value[idx]:\n                    personal_best_position[idx] = opposite_position\n                    personal_best_value[idx] = opposite_value\n\n                if opposite_value < global_best_value:\n                    global_best_position = opposite_position\n                    global_best_value = opposite_value\n\n                if self.evaluations >= self.budget:\n                    break\n\n            # Global Position Reset Mechanism\n            if np.random.rand() < self.reset_chance:\n                random_index = np.random.randint(self.population_size)\n                position[random_index] = np.random.uniform(lb, ub, self.dim)\n                current_value = func(position[random_index])\n                self.evaluations += 1\n\n                if current_value < personal_best_value[random_index]:\n                    personal_best_position[random_index] = position[random_index]\n                    personal_best_value[random_index] = current_value\n\n                if current_value < global_best_value:\n                    global_best_position = position[random_index]\n                    global_best_value = current_value\n\n        return global_best_position, global_best_value", "name": "Integrated_Quantum_Pso_Adaptive_Inertia_Elite_OBL", "description": "Integrated Quantum PSO with Adaptive Inertia and Elite Opposition-Based Learning for efficient exploration and exploitation.", "configspace": "", "generation": 30, "fitness": 0.2779613646310156, "feedback": "The algorithm Integrated_Quantum_Pso_Adaptive_Inertia_Elite_OBL got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28 with standard deviation 0.00.", "error": "", "parent_id": "23a31d7a-9c0f-430c-a37f-f7caeaedc69a", "metadata": {"aucs": [0.2779613646310156, 0.2779613646310156, 0.2779613646310156]}, "mutation_prompt": null}
{"id": "1bd1c031-96b2-4f76-91ca-a712c55107b3", "solution": "import numpy as np\n\nclass Quantum_Inspired_PSO_Adaptive_Gaussian_Diversity:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.w_initial = 0.9\n        self.w_final = 0.4\n        self.q_factor = 0.8\n        self.gaussian_scale = 0.1\n        self.momentum_factor = 1.0\n        self.diversity_preservation = 0.15\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        np.random.seed(42)\n\n        position = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocity = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_position = np.copy(position)\n        personal_best_value = np.array([func(p) for p in personal_best_position])\n        global_best_position = personal_best_position[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n        \n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(2)\n                w = self.w_final + (self.w_initial - self.w_final) * (1 - evaluations / self.budget)\n                velocity[i] = (w * velocity[i] +\n                               self.c1 * r1 * (personal_best_position[i] - position[i]) +\n                               self.c2 * r2 * (global_best_position - position[i]))\n                \n                adaptive_gaussian_scale = self.gaussian_scale * (1 - evaluations / self.budget)\n                position[i] += (velocity[i] + \n                                self.q_factor * np.random.normal(scale=adaptive_gaussian_scale, size=self.dim))\n                position[i] = np.clip(position[i], lb, ub)\n\n                current_value = func(position[i])\n                evaluations += 1\n                \n                if current_value < personal_best_value[i]:\n                    personal_best_position[i] = position[i]\n                    personal_best_value[i] = current_value\n                \n                if current_value < global_best_value:\n                    global_best_position = position[i]\n                    global_best_value = current_value\n\n                if evaluations >= self.budget:\n                    break\n\n            # Diversity Preservation Mechanism\n            diversity_index = np.random.choice(self.population_size, int(self.population_size * self.diversity_preservation), replace=False)\n            for idx in diversity_index:\n                if np.random.rand() < 0.5:\n                    position[idx] = np.random.uniform(lb, ub, self.dim)\n                else:\n                    position[idx] += self.q_factor * np.random.normal(scale=self.gaussian_scale, size=self.dim)\n                position[idx] = np.clip(position[idx], lb, ub)\n                current_value = func(position[idx])\n                evaluations += 1\n                \n                if current_value < personal_best_value[idx]:\n                    personal_best_position[idx] = position[idx]\n                    personal_best_value[idx] = current_value\n                \n                if current_value < global_best_value:\n                    global_best_position = position[idx]\n                    global_best_value = current_value\n\n        return global_best_position, global_best_value", "name": "Quantum_Inspired_PSO_Adaptive_Gaussian_Diversity", "description": "Quantum-Inspired Particle Swarm Optimizer with Adaptive Gaussian Perturbation and Diversity Preservation Mechanism for Enhanced Exploration-Exploitation Balance.", "configspace": "", "generation": 31, "fitness": 0.2740385239265142, "feedback": "The algorithm Quantum_Inspired_PSO_Adaptive_Gaussian_Diversity got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.27 with standard deviation 0.00.", "error": "", "parent_id": "23a31d7a-9c0f-430c-a37f-f7caeaedc69a", "metadata": {"aucs": [0.2740385239265142, 0.2740385239265142, 0.2740385239265142]}, "mutation_prompt": null}
{"id": "3f65b79d-d55c-4185-a2c1-f706db2f8a77", "solution": "import numpy as np\n\nclass Quantum_Pso_Adaptive_Gauss_Stochastic_Momentum:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.c1 = 1.5\n        self.c2 = 2.5\n        self.w = 0.7\n        self.q_factor = 0.8\n        self.gaussian_scale = 0.1\n        self.reset_chance = 0.1\n        self.adaptive_rate = 0.95\n        self.stochastic_momentum_scale = 0.2\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        np.random.seed(42)\n\n        position = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocity = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_position = np.copy(position)\n        personal_best_value = np.array([func(p) for p in personal_best_position])\n        global_best_position = personal_best_position[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n        \n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(2)\n                stochastic_momentum = self.stochastic_momentum_scale * np.random.randn(self.dim)\n                self.w *= self.adaptive_rate  # Dynamically adjust inertia weight\n                velocity[i] = (self.w * velocity[i] +\n                               self.c1 * r1 * (personal_best_position[i] - position[i]) +\n                               self.c2 * r2 * (global_best_position - position[i]) +\n                               stochastic_momentum)\n                \n                adaptive_gaussian_scale = self.gaussian_scale * (1 - evaluations / self.budget)\n                position[i] += (velocity[i] + \n                                self.q_factor * np.random.normal(scale=adaptive_gaussian_scale, size=self.dim))\n                position[i] = np.clip(position[i], lb, ub)\n\n                current_value = func(position[i])\n                evaluations += 1\n                \n                if current_value < personal_best_value[i]:\n                    personal_best_position[i] = position[i]\n                    personal_best_value[i] = current_value\n                \n                if current_value < global_best_value:\n                    global_best_position = position[i]\n                    global_best_value = current_value\n\n                if evaluations >= self.budget:\n                    break\n\n            # Global Position Reset Mechanism\n            if np.random.rand() < self.reset_chance:\n                random_index = np.random.randint(self.population_size)\n                position[random_index] = np.random.uniform(lb, ub, self.dim)\n                current_value = func(position[random_index])\n                evaluations += 1\n                \n                if current_value < personal_best_value[random_index]:\n                    personal_best_position[random_index] = position[random_index]\n                    personal_best_value[random_index] = current_value\n                \n                if current_value < global_best_value:\n                    global_best_position = position[random_index]\n                    global_best_value = current_value\n\n        return global_best_position, global_best_value", "name": "Quantum_Pso_Adaptive_Gauss_Stochastic_Momentum", "description": "Quantum Particle Swarm Optimizer with Adaptive Gaussian and Stochastic Momentum integrates stochastic momentum and dynamically scaled Gaussian perturbations to enhance exploration and exploitation balance.", "configspace": "", "generation": 32, "fitness": 0.2780107631250135, "feedback": "The algorithm Quantum_Pso_Adaptive_Gauss_Stochastic_Momentum got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28 with standard deviation 0.00.", "error": "", "parent_id": "23a31d7a-9c0f-430c-a37f-f7caeaedc69a", "metadata": {"aucs": [0.2780107631250135, 0.2780107631250135, 0.2780107631250135]}, "mutation_prompt": null}
{"id": "d6576d57-ec29-4641-b20e-accc471d4a53", "solution": "import numpy as np\n\nclass Adaptive_Covariance_Matrix_Estmation_PSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.w = 0.7\n        self.q_factor = 0.9\n        self.covariance_adapt_rate = 0.1\n        self.reset_chance = 0.05\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        np.random.seed(42)\n\n        position = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocity = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_position = np.copy(position)\n        personal_best_value = np.array([func(p) for p in personal_best_position])\n        global_best_position = personal_best_position[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n        \n        evaluations = self.population_size\n\n        cov_matrix = np.eye(self.dim)\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(2)\n                velocity[i] = (self.w * velocity[i] +\n                               self.c1 * r1 * (personal_best_position[i] - position[i]) +\n                               self.c2 * r2 * (global_best_position - position[i]))\n                \n                if evaluations % 10 == 0:\n                    cov_matrix += self.covariance_adapt_rate * np.cov(position.T)\n                \n                quantum_step = np.random.multivariate_normal(np.zeros(self.dim), cov_matrix)\n                position[i] += (velocity[i] + self.q_factor * quantum_step)\n                position[i] = np.clip(position[i], lb, ub)\n\n                current_value = func(position[i])\n                evaluations += 1\n                \n                if current_value < personal_best_value[i]:\n                    personal_best_position[i] = position[i]\n                    personal_best_value[i] = current_value\n                \n                if current_value < global_best_value:\n                    global_best_position = position[i]\n                    global_best_value = current_value\n\n                if evaluations >= self.budget:\n                    break\n\n            if np.random.rand() < self.reset_chance:\n                random_index = np.random.randint(self.population_size)\n                position[random_index] = np.random.uniform(lb, ub, self.dim)\n                current_value = func(position[random_index])\n                evaluations += 1\n                \n                if current_value < personal_best_value[random_index]:\n                    personal_best_position[random_index] = position[random_index]\n                    personal_best_value[random_index] = current_value\n                \n                if current_value < global_best_value:\n                    global_best_position = position[random_index]\n                    global_best_value = current_value\n\n        return global_best_position, global_best_value", "name": "Adaptive_Covariance_Matrix_Estmation_PSO", "description": "Adaptive Covariance Matrix Estimation Particle Swarm Optimizer: Integrates adaptive covariance estimation and quantum-inspired updates to enhance exploration and exploitation in high-dimensional spaces.", "configspace": "", "generation": 33, "fitness": 0.25898906893364393, "feedback": "The algorithm Adaptive_Covariance_Matrix_Estmation_PSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.26 with standard deviation 0.00.", "error": "", "parent_id": "23a31d7a-9c0f-430c-a37f-f7caeaedc69a", "metadata": {"aucs": [0.25898906893364393, 0.25898906893364393, 0.25898906893364393]}, "mutation_prompt": null}
{"id": "dbb08fb3-0076-4df6-9b24-85079907db81", "solution": "import numpy as np\n\nclass Quantum_Inspired_Adaptive_Pso_Exponential_Momentum_Optimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.w_initial = 0.9\n        self.w_final = 0.4\n        self.q_factor = 0.9\n        self.gaussian_scale = 0.1\n        self.reset_chance = 0.05  # Lower reset chance to avoid too frequent random restarts\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        np.random.seed(42)\n\n        position = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocity = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_position = np.copy(position)\n        personal_best_value = np.array([func(p) for p in personal_best_position])\n        global_best_position = personal_best_position[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n        \n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(2)\n                \n                # Exponentially decrease inertia weight for better exploration-exploitation balance\n                self.w = self.w_final + (self.w_initial - self.w_final) * np.exp(-0.05 * evaluations / self.budget)\n                \n                velocity[i] = (self.w * velocity[i] +\n                               self.c1 * r1 * (personal_best_position[i] - position[i]) +\n                               self.c2 * r2 * (global_best_position - position[i]))\n                \n                adaptive_gaussian_scale = self.gaussian_scale * (1 - evaluations / self.budget)\n                position[i] += (velocity[i] + \n                                self.q_factor * np.random.normal(scale=adaptive_gaussian_scale, size=self.dim))\n                position[i] = np.clip(position[i], lb, ub)\n\n                current_value = func(position[i])\n                evaluations += 1\n                \n                if current_value < personal_best_value[i]:\n                    personal_best_position[i] = position[i]\n                    personal_best_value[i] = current_value\n                \n                if current_value < global_best_value:\n                    global_best_position = position[i]\n                    global_best_value = current_value\n\n                if evaluations >= self.budget:\n                    break\n\n            # Global Position Reset Mechanism\n            if np.random.rand() < self.reset_chance:\n                random_index = np.random.randint(self.population_size)\n                position[random_index] = np.random.uniform(lb, ub, self.dim)\n                current_value = func(position[random_index])\n                evaluations += 1\n                \n                if current_value < personal_best_value[random_index]:\n                    personal_best_position[random_index] = position[random_index]\n                    personal_best_value[random_index] = current_value\n                \n                if current_value < global_best_value:\n                    global_best_position = position[random_index]\n                    global_best_value = current_value\n\n        return global_best_position, global_best_value", "name": "Quantum_Inspired_Adaptive_Pso_Exponential_Momentum_Optimizer", "description": "Quantum-Inspired Adaptive Particle Swarm Optimizer with Exponential Decrease in Momentum and Position Reset to enhance exploration and exploitation balance.", "configspace": "", "generation": 34, "fitness": 0.269578006164647, "feedback": "The algorithm Quantum_Inspired_Adaptive_Pso_Exponential_Momentum_Optimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.27 with standard deviation 0.00.", "error": "", "parent_id": "23a31d7a-9c0f-430c-a37f-f7caeaedc69a", "metadata": {"aucs": [0.269578006164647, 0.269578006164647, 0.269578006164647]}, "mutation_prompt": null}
{"id": "8f158f80-5bba-4a46-ade1-e47aade42fbe", "solution": "import numpy as np\n\nclass Quantum_Harmony_Search_Optimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.harmony_memory_size = 20\n        self.harmony_consideration_rate = 0.9\n        self.pitch_adjustment_rate = 0.3\n        self.noise_scale = 0.1\n        self.adaptive_rate = 0.98\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        np.random.seed(42)\n\n        harmony_memory = np.random.uniform(lb, ub, (self.harmony_memory_size, self.dim))\n        harmony_values = np.array([func(harmony) for harmony in harmony_memory])\n        best_harmony = harmony_memory[np.argmin(harmony_values)]\n        best_value = np.min(harmony_values)\n        \n        evaluations = self.harmony_memory_size\n\n        while evaluations < self.budget:\n            new_harmony = np.zeros(self.dim)\n            for j in range(self.dim):\n                if np.random.rand() < self.harmony_consideration_rate:\n                    # Harmony memory consideration\n                    idx = np.random.randint(self.harmony_memory_size)\n                    new_harmony[j] = harmony_memory[idx, j]\n                    \n                    # Pitch adjustment\n                    if np.random.rand() < self.pitch_adjustment_rate:\n                        pitch_adjustment = np.random.uniform(-self.noise_scale, self.noise_scale)\n                        new_harmony[j] += pitch_adjustment\n                else:\n                    # Random selection\n                    new_harmony[j] = np.random.uniform(lb[j], ub[j])\n\n            new_harmony = np.clip(new_harmony, lb, ub)\n            current_value = func(new_harmony)\n            evaluations += 1\n\n            if current_value < best_value:\n                best_harmony = new_harmony\n                best_value = current_value\n\n            # Update harmony memory if the new harmony is better\n            worst_idx = np.argmax(harmony_values)\n            if current_value < harmony_values[worst_idx]:\n                harmony_memory[worst_idx] = new_harmony\n                harmony_values[worst_idx] = current_value\n\n            # Adaptive noise scale adjustment\n            self.noise_scale *= self.adaptive_rate\n\n        return best_harmony, best_value", "name": "Quantum_Harmony_Search_Optimizer", "description": "Quantum Harmony Search Optimizer with Adaptive Noise Reduction: Combines quantum-inspired harmony search with adaptive noise reduction to balance exploration and exploitation for efficient global optimization.", "configspace": "", "generation": 35, "fitness": 0.27676489418327965, "feedback": "The algorithm Quantum_Harmony_Search_Optimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28 with standard deviation 0.00.", "error": "", "parent_id": "23a31d7a-9c0f-430c-a37f-f7caeaedc69a", "metadata": {"aucs": [0.27676489418327965, 0.27676489418327965, 0.27676489418327965]}, "mutation_prompt": null}
{"id": "db15a71a-252f-4f5b-b0c1-a1804af95fa9", "solution": "import numpy as np\n\nclass Quantum_Levy_Mutated_PSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.c1 = 1.5\n        self.c2 = 1.5\n        self.w = 0.7\n        self.levy_alpha = 1.5\n        self.q_factor = 0.9\n        self.mutation_chance = 0.1\n\n    def levy_flight(self, size):\n        sigma_u = (np.math.gamma(1 + self.levy_alpha) * np.sin(np.pi * self.levy_alpha / 2) /\n                   (np.math.gamma((1 + self.levy_alpha) / 2) * self.levy_alpha * 2 ** ((self.levy_alpha - 1) / 2))) ** (1 / self.levy_alpha)\n        u = np.random.normal(0, sigma_u, size)\n        v = np.random.normal(0, 1, size)\n        step = u / np.abs(v) ** (1 / self.levy_alpha)\n        return step\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        np.random.seed(42)\n\n        position = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocity = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_position = np.copy(position)\n        personal_best_value = np.array([func(p) for p in personal_best_position])\n        global_best_position = personal_best_position[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n        \n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(2)\n                velocity[i] = (self.w * velocity[i] +\n                               self.c1 * r1 * (personal_best_position[i] - position[i]) +\n                               self.c2 * r2 * (global_best_position - position[i]))\n                               \n                position[i] += velocity[i] + self.q_factor * np.random.normal(size=self.dim)\n                position[i] = np.clip(position[i], lb, ub)\n\n                # Levy Mutation\n                if np.random.rand() < self.mutation_chance:\n                    position[i] += self.levy_flight(self.dim)\n                    position[i] = np.clip(position[i], lb, ub)\n\n                current_value = func(position[i])\n                evaluations += 1\n\n                if current_value < personal_best_value[i]:\n                    personal_best_position[i] = position[i]\n                    personal_best_value[i] = current_value\n\n                if current_value < global_best_value:\n                    global_best_position = position[i]\n                    global_best_value = current_value\n\n                if evaluations >= self.budget:\n                    break\n\n        return global_best_position, global_best_value", "name": "Quantum_Levy_Mutated_PSO", "description": "Quantum-Inspired Levy Mutated Particle Swarm Optimizer: Combines quantum particle swarm principles with Levy flight mutation to enhance exploration and prevent premature convergence.", "configspace": "", "generation": 36, "fitness": 0.27804774181306213, "feedback": "The algorithm Quantum_Levy_Mutated_PSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28 with standard deviation 0.00.", "error": "", "parent_id": "23a31d7a-9c0f-430c-a37f-f7caeaedc69a", "metadata": {"aucs": [0.27804774181306213, 0.27804774181306213, 0.27804774181306213]}, "mutation_prompt": null}
{"id": "6908e7bb-4d2d-4577-8907-e646f9be444d", "solution": "import numpy as np\n\nclass Quantum_Inspired_Pso_Dynamic_Neighborhood:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.c1 = 1.5\n        self.c2 = 1.5\n        self.w = 0.7\n        self.q_factor = 0.8\n        self.gaussian_scale = 0.1\n        self.reset_chance = 0.1\n        self.adaptive_rate = 0.9\n        self.neighbor_size = max(1, self.population_size // 5)\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        np.random.seed(42)\n\n        position = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocity = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_position = np.copy(position)\n        personal_best_value = np.array([func(p) for p in personal_best_position])\n        global_best_position = personal_best_position[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n        \n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(2)\n                self.w = 0.5 + 0.5 * np.cos(np.pi * evaluations / self.budget)  # Update inertia weight\n                velocity[i] = (self.w * velocity[i] +\n                               self.c1 * r1 * (personal_best_position[i] - position[i]) +\n                               self.c2 * r2 * (global_best_position - position[i]))\n                \n                adaptive_gaussian_scale = self.gaussian_scale * (1 - evaluations / self.budget)\n                position[i] += (velocity[i] + \n                                self.q_factor * np.random.normal(scale=adaptive_gaussian_scale, size=self.dim))\n                position[i] = np.clip(position[i], lb, ub)\n\n                current_value = func(position[i])\n                evaluations += 1\n                \n                if current_value < personal_best_value[i]:\n                    personal_best_position[i] = position[i]\n                    personal_best_value[i] = current_value\n                \n                if current_value < global_best_value:\n                    global_best_position = position[i]\n                    global_best_value = current_value\n\n                if evaluations >= self.budget:\n                    break\n\n            # Dynamic Neighborhood Adaptation\n            for _ in range(self.neighbor_size):\n                random_index = np.random.randint(self.population_size)\n                neighbor_indices = np.random.choice(self.population_size, self.neighbor_size, replace=False)\n                local_best_index = neighbor_indices[np.argmin(personal_best_value[neighbor_indices])]\n                if personal_best_value[local_best_index] < personal_best_value[random_index]:\n                    personal_best_position[random_index] = personal_best_position[local_best_index]\n                    personal_best_value[random_index] = personal_best_value[local_best_index]\n\n            # Global Position Reset Mechanism\n            if np.random.rand() < self.reset_chance:\n                random_index = np.random.randint(self.population_size)\n                position[random_index] = np.random.uniform(lb, ub, self.dim)\n                current_value = func(position[random_index])\n                evaluations += 1\n                \n                if current_value < personal_best_value[random_index]:\n                    personal_best_position[random_index] = position[random_index]\n                    personal_best_value[random_index] = current_value\n                \n                if current_value < global_best_value:\n                    global_best_position = position[random_index]\n                    global_best_value = current_value\n\n        return global_best_position, global_best_value", "name": "Quantum_Inspired_Pso_Dynamic_Neighborhood", "description": "Quantum-Inspired PSO with Dynamic Neighborhood Adaptation: Introduces quantum-inspired exploration and dynamic neighborhood adaptation to balance exploration and exploitation, focusing on enhanced convergence and solution diversity.", "configspace": "", "generation": 37, "fitness": 0.2759384873775159, "feedback": "The algorithm Quantum_Inspired_Pso_Dynamic_Neighborhood got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28 with standard deviation 0.00.", "error": "", "parent_id": "23a31d7a-9c0f-430c-a37f-f7caeaedc69a", "metadata": {"aucs": [0.2759384873775159, 0.2759384873775159, 0.2759384873775159]}, "mutation_prompt": null}
{"id": "99476fb1-6724-451c-bff1-dc6d52418569", "solution": "import numpy as np\n\nclass Quantum_Enhanced_Firefly_Adaptive_Attraction_Optimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.alpha = 0.5  # Randomness parameter\n        self.beta0 = 1.0  # Base attraction\n        self.gamma = 1.0  # Light absorption coefficient\n        self.quantum_scale = 0.1\n        self.anomaly_threshold = 0.1\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        np.random.seed(42)\n\n        position = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        light_intensity = np.array([func(p) for p in position])\n        best_position = position[np.argmin(light_intensity)]\n        best_value = np.min(light_intensity)\n\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                for j in range(self.population_size):\n                    if light_intensity[j] < light_intensity[i]:\n                        r = np.linalg.norm(position[i] - position[j])\n                        beta = self.beta0 * np.exp(-self.gamma * r ** 2)\n                        position[i] += beta * (position[j] - position[i])\n                        position[i] += self.alpha * (np.random.uniform(-0.5, 0.5, self.dim) +\n                                                     self.quantum_scale * np.random.normal(size=self.dim))\n                        position[i] = np.clip(position[i], lb, ub)\n                \n                current_value = func(position[i])\n                evaluations += 1\n                \n                if current_value < light_intensity[i]:\n                    light_intensity[i] = current_value\n\n                if current_value < best_value:\n                    best_position = position[i]\n                    best_value = current_value\n\n                if evaluations >= self.budget:\n                    break\n\n            # Anomaly Detection Mechanism: Random perturbation if stagnation is detected\n            if np.std(light_intensity) < self.anomaly_threshold:\n                perturb_index = np.random.randint(self.population_size)\n                position[perturb_index] = np.random.uniform(lb, ub, self.dim)\n                current_value = func(position[perturb_index])\n                evaluations += 1\n                \n                if current_value < light_intensity[perturb_index]:\n                    light_intensity[perturb_index] = current_value\n                \n                if current_value < best_value:\n                    best_position = position[perturb_index]\n                    best_value = current_value\n\n        return best_position, best_value", "name": "Quantum_Enhanced_Firefly_Adaptive_Attraction_Optimizer", "description": "Quantum-Enhanced Firefly Algorithm with Adaptive Attraction and Anomaly Detection for Enhanced Exploration and Convergence.", "configspace": "", "generation": 38, "fitness": 0.22707581739209426, "feedback": "The algorithm Quantum_Enhanced_Firefly_Adaptive_Attraction_Optimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.23 with standard deviation 0.00.", "error": "", "parent_id": "23a31d7a-9c0f-430c-a37f-f7caeaedc69a", "metadata": {"aucs": [0.22707581739209426, 0.22707581739209426, 0.22707581739209426]}, "mutation_prompt": null}
{"id": "dba22e86-beac-46ea-8f63-5cdfccc0af8c", "solution": "import numpy as np\n\nclass Quantum_Multi_Swarm_Adaptive_Gradient_Optimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.swarm_count = 5\n        self.alpha = 0.9\n        self.beta = 0.4\n        self.gamma = 0.1\n        self.delta = 0.1\n        self.epsilon = 1e-8\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        np.random.seed(42)\n\n        swarms = [np.random.uniform(lb, ub, (self.population_size, self.dim)) for _ in range(self.swarm_count)]\n        velocities = [np.random.uniform(-1, 1, (self.population_size, self.dim)) for _ in range(self.swarm_count)]\n        personal_best_positions = [np.copy(swarm) for swarm in swarms]\n        personal_best_values = [np.array([func(p) for p in personal_best]) for personal_best in personal_best_positions]\n        global_best_position = min((min(p_best, key=lambda p: func(p)) for p_best in personal_best_positions), key=lambda p: func(p))\n        global_best_value = func(global_best_position)\n        \n        evaluations = self.population_size * self.swarm_count\n\n        while evaluations < self.budget:\n            for swarm_idx in range(self.swarm_count):\n                for i in range(self.population_size):\n                    r1, r2 = np.random.rand(2)\n                    velocities[swarm_idx][i] = (self.alpha * velocities[swarm_idx][i] +\n                                                self.beta * r1 * (personal_best_positions[swarm_idx][i] - swarms[swarm_idx][i]) +\n                                                self.gamma * r2 * (global_best_position - swarms[swarm_idx][i]))\n                    \n                    gradient = (func(swarms[swarm_idx][i] + self.epsilon) - func(swarms[swarm_idx][i])) / self.epsilon\n                    adaptive_gradient = self.delta * gradient\n                    swarms[swarm_idx][i] += velocities[swarm_idx][i] + adaptive_gradient\n                    \n                    swarms[swarm_idx][i] = np.clip(swarms[swarm_idx][i], lb, ub)\n\n                    current_value = func(swarms[swarm_idx][i])\n                    evaluations += 1\n\n                    if current_value < personal_best_values[swarm_idx][i]:\n                        personal_best_positions[swarm_idx][i] = swarms[swarm_idx][i]\n                        personal_best_values[swarm_idx][i] = current_value\n\n                    if current_value < global_best_value:\n                        global_best_position = swarms[swarm_idx][i]\n                        global_best_value = current_value\n\n                    if evaluations >= self.budget:\n                        break\n\n        return global_best_position, global_best_value", "name": "Quantum_Multi_Swarm_Adaptive_Gradient_Optimizer", "description": "Quantum-inspired Multi-Swarm Adaptive Gradient Optimizer utilizes a multi-swarm strategy with adaptive gradient-based updates to enhance exploration and exploitation in high-dimensional search spaces.", "configspace": "", "generation": 39, "fitness": 0.27432023238786474, "feedback": "The algorithm Quantum_Multi_Swarm_Adaptive_Gradient_Optimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.27 with standard deviation 0.00.", "error": "", "parent_id": "23a31d7a-9c0f-430c-a37f-f7caeaedc69a", "metadata": {"aucs": [0.27432023238786474, 0.27432023238786474, 0.27432023238786474]}, "mutation_prompt": null}
{"id": "d034b0b7-f625-4608-9d26-7359a7b7d0f5", "solution": "import numpy as np\n\nclass Adaptive_Quantum_Inspired_Evolutionary_Algorithm:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.alpha = 0.5  # balance between exploration and exploitation\n        self.beta = 2.0   # scaling factor for quantum superposition\n        self.mutation_rate = 0.1\n        self.adaptive_rate = 0.95\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        np.random.seed(42)\n\n        # Initial population\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(indiv) for indiv in population])\n        evaluations = self.population_size\n\n        best_individual = population[np.argmin(fitness)]\n        best_fitness = np.min(fitness)\n\n        while evaluations < self.budget:\n            # Quantum-inspired crossover\n            new_population = []\n            for _ in range(self.population_size):\n                parents = np.random.choice(range(self.population_size), size=2, replace=False)\n                parent1, parent2 = population[parents[0]], population[parents[1]]\n                gamma = np.random.rand(self.dim)\n                offspring = (gamma * parent1 + (1 - gamma) * parent2) + \\\n                            self.beta * np.random.normal(0, 1, self.dim)\n                offspring = np.clip(offspring, lb, ub)\n                new_population.append(offspring)\n\n            # Mutation\n            for i in range(self.population_size):\n                if np.random.rand() < self.mutation_rate:\n                    mutation_vector = np.random.normal(0, 0.1, self.dim)\n                    new_population[i] += mutation_vector\n                    new_population[i] = np.clip(new_population[i], lb, ub)\n\n            # Combine and select\n            combined_population = np.vstack((population, np.array(new_population)))\n            combined_fitness = np.array([func(indiv) for indiv in combined_population])\n            evaluations += len(combined_population)\n            selected_indices = np.argsort(combined_fitness)[:self.population_size]\n            population = combined_population[selected_indices]\n            fitness = combined_fitness[selected_indices]\n\n            current_best = np.min(fitness)\n            if current_best < best_fitness:\n                best_fitness = current_best\n                best_individual = population[np.argmin(fitness)]\n\n            # Dynamic adjustment of parameters\n            self.beta *= self.adaptive_rate\n            self.mutation_rate *= self.adaptive_rate\n\n            if evaluations >= self.budget:\n                break\n\n        return best_individual, best_fitness", "name": "Adaptive_Quantum_Inspired_Evolutionary_Algorithm", "description": "Adaptive Quantum-Inspired Evolutionary Algorithm with Dynamic Multimodal Search integrates quantum-inspired superposition with adaptive crossover and mutation strategies for enhanced exploration and exploitation.", "configspace": "", "generation": 40, "fitness": 0.27747989487980074, "feedback": "The algorithm Adaptive_Quantum_Inspired_Evolutionary_Algorithm got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28 with standard deviation 0.00.", "error": "", "parent_id": "23a31d7a-9c0f-430c-a37f-f7caeaedc69a", "metadata": {"aucs": [0.27747989487980074, 0.27747989487980074, 0.27747989487980074]}, "mutation_prompt": null}
{"id": "a82905fe-c797-4906-88f1-da23159a2298", "solution": "import numpy as np\n\nclass Adaptive_Quantum_Pso_Cross_Learning:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.w = 0.7\n        self.q_factor = 0.9\n        self.learning_factor = 0.5\n        self.mutation_rate = 0.1\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        np.random.seed(24)\n\n        position = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocity = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_position = np.copy(position)\n        personal_best_value = np.array([func(p) for p in personal_best_position])\n        global_best_position = personal_best_position[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n        \n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(2)\n                r_learning = np.random.rand()\n\n                velocity[i] = (self.w * velocity[i] +\n                               self.c1 * r1 * (personal_best_position[i] - position[i]) +\n                               self.c2 * r2 * (global_best_position - position[i]))\n\n                if r_learning < self.learning_factor:\n                    partner_idx = np.random.randint(self.population_size)\n                    position[i] = (self.q_factor * np.random.normal(loc=partner_idx, scale=abs(ub-lb)/2, size=self.dim))\n\n                position[i] += velocity[i]\n                position[i] = np.clip(position[i], lb, ub)\n\n                if np.random.rand() < self.mutation_rate:\n                    position[i] += np.random.normal(scale=(ub-lb)/10, size=self.dim)\n                    position[i] = np.clip(position[i], lb, ub)\n\n                current_value = func(position[i])\n                evaluations += 1\n                \n                if current_value < personal_best_value[i]:\n                    personal_best_position[i] = position[i]\n                    personal_best_value[i] = current_value\n                \n                if current_value < global_best_value:\n                    global_best_position = position[i]\n                    global_best_value = current_value\n\n                if evaluations >= self.budget:\n                    break\n\n        return global_best_position, global_best_value", "name": "Adaptive_Quantum_Pso_Cross_Learning", "description": "Adaptive Quantum Particle Swarm with Cross-Population Learning: Leverages cross-population knowledge sharing and adaptive mutation strategies to enhance exploration and convergence.", "configspace": "", "generation": 41, "fitness": 0.2452021965984651, "feedback": "The algorithm Adaptive_Quantum_Pso_Cross_Learning got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.25 with standard deviation 0.00.", "error": "", "parent_id": "23a31d7a-9c0f-430c-a37f-f7caeaedc69a", "metadata": {"aucs": [0.2452021965984651, 0.2452021965984651, 0.2452021965984651]}, "mutation_prompt": null}
{"id": "9dc0818a-5971-4508-9c41-324da5c6897d", "solution": "import numpy as np\n\nclass Hybrid_Swarm_Genetic_Optimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 30\n        self.c1 = 1.5\n        self.c2 = 1.5\n        self.w = 0.7\n        self.crossover_rate = 0.5\n        self.q_factor = 0.8\n        self.mutation_prob = 0.1\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        np.random.seed(42)\n\n        position = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocity = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_position = np.copy(position)\n        personal_best_value = np.array([func(p) for p in personal_best_position])\n        global_best_position = personal_best_position[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n        \n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(2)\n                velocity[i] = (self.w * velocity[i] +\n                               self.c1 * r1 * (personal_best_position[i] - position[i]) +\n                               self.c2 * r2 * (global_best_position - position[i]))\n                \n                position[i] += self.q_factor * (velocity[i] + np.random.normal(size=self.dim))\n                position[i] = np.clip(position[i], lb, ub)\n\n                current_value = func(position[i])\n                evaluations += 1\n                \n                if current_value < personal_best_value[i]:\n                    personal_best_position[i] = position[i]\n                    personal_best_value[i] = current_value\n                \n                if current_value < global_best_value:\n                    global_best_position = position[i]\n                    global_best_value = current_value\n\n                if evaluations >= self.budget:\n                    break\n            \n            # Genetic Crossover and Mutation\n            if np.random.rand() < self.crossover_rate:\n                parent1, parent2 = np.random.choice(self.population_size, 2, replace=False)\n                crossover_point = np.random.randint(1, self.dim)\n                child = np.concatenate((personal_best_position[parent1][:crossover_point],\n                                        personal_best_position[parent2][crossover_point:]))\n                \n                if np.random.rand() < self.mutation_prob:\n                    mutation_index = np.random.randint(self.dim)\n                    child[mutation_index] = np.random.uniform(lb[mutation_index], ub[mutation_index])\n                \n                child_value = func(child)\n                evaluations += 1\n                if child_value < global_best_value:\n                    global_best_position = child\n                    global_best_value = child_value\n\n        return global_best_position, global_best_value", "name": "Hybrid_Swarm_Genetic_Optimizer", "description": "Hybrid Swarm Genetic Optimizer: Combines particle swarm dynamics with genetic crossover to maintain diversity and enhance exploration in high-dimensional spaces.", "configspace": "", "generation": 42, "fitness": 0.2775810899642319, "feedback": "The algorithm Hybrid_Swarm_Genetic_Optimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28 with standard deviation 0.00.", "error": "", "parent_id": "23a31d7a-9c0f-430c-a37f-f7caeaedc69a", "metadata": {"aucs": [0.2775810899642319, 0.2775810899642319, 0.2775810899642319]}, "mutation_prompt": null}
{"id": "9fb564a9-8d7a-4f0f-be13-589be6425e96", "solution": "import numpy as np\n\nclass Quantum_Inspired_DE_Adaptive_Niching:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.F = 0.5  # Scaling factor for mutation\n        self.CR = 0.9  # Crossover probability\n        self.q_factor = 0.9\n        self.adaptive_mutation_scale = 0.1\n        self.niching_radius = 0.1\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        np.random.seed(42)\n\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_individual = population[best_idx]\n        best_fitness = fitness[best_idx]\n\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                a, b, c = np.random.choice([x for x in range(self.population_size) if x != i], 3, replace=False)\n                mutant = population[a] + self.F * (population[b] - population[c])\n                mutant = np.clip(mutant, lb, ub)\n\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                current_fitness = func(trial)\n                evaluations += 1\n\n                if current_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = current_fitness\n\n                    if current_fitness < best_fitness:\n                        best_individual = trial\n                        best_fitness = current_fitness\n\n                # Quantum-Inspired Position Update\n                if np.random.rand() < self.q_factor:\n                    quantum_shift = np.random.normal(scale=self.adaptive_mutation_scale * (1 - evaluations / self.budget), size=self.dim)\n                    quantum_position = np.clip(population[i] + quantum_shift, lb, ub)\n                    quantum_fitness = func(quantum_position)\n                    evaluations += 1\n\n                    if quantum_fitness < fitness[i]:\n                        population[i] = quantum_position\n                        fitness[i] = quantum_fitness\n\n                        if quantum_fitness < best_fitness:\n                            best_individual = quantum_position\n                            best_fitness = quantum_fitness\n                \n                if evaluations >= self.budget:\n                    break\n\n            # Niching Strategy to Maintain Diversity\n            for i in range(self.population_size):\n                distances = np.linalg.norm(population - population[i], axis=1)\n                similar_indices = np.where(distances < self.niching_radius)[0]\n                if len(similar_indices) > 1:\n                    for idx in similar_indices:\n                        if idx != i and fitness[idx] >= fitness[i]:\n                            population[idx] = np.random.uniform(lb, ub, self.dim)\n                            fitness[idx] = func(population[idx])\n                            evaluations += 1\n\n                            if fitness[idx] < best_fitness:\n                                best_individual = population[idx]\n                                best_fitness = fitness[idx]\n\n                            if evaluations >= self.budget:\n                                break\n\n        return best_individual, best_fitness", "name": "Quantum_Inspired_DE_Adaptive_Niching", "description": "Quantum-Inspired Differential Evolution with Adaptive Mutation and Niching for Diverse Exploration and Robust Convergence.", "configspace": "", "generation": 43, "fitness": 0.2733111943805461, "feedback": "The algorithm Quantum_Inspired_DE_Adaptive_Niching got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.27 with standard deviation 0.00.", "error": "", "parent_id": "23a31d7a-9c0f-430c-a37f-f7caeaedc69a", "metadata": {"aucs": [0.2733111943805461, 0.2733111943805461, 0.2733111943805461]}, "mutation_prompt": null}
{"id": "48c6042e-ae3b-4bce-b2cb-2f36efc37cda", "solution": "import numpy as np\n\nclass Adaptive_Evolutionary_Quantum_Swarm_Optimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 30\n        self.c1 = 1.5\n        self.c2 = 1.5\n        self.w = 0.7\n        self.mutation_rate = 0.05\n        self.q_factor = 1.0\n        self.evolutionary_pressure = 0.2\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        np.random.seed(42)\n\n        position = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocity = np.random.uniform(-0.5, 0.5, (self.population_size, self.dim))\n        fitness = np.array([func(p) for p in position])\n        personal_best_position = np.copy(position)\n        personal_best_value = np.copy(fitness)\n        global_best_position = personal_best_position[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n        \n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            # Adaptive Inertia Weight\n            self.w = 0.9 - 0.7 * (evaluations / self.budget)\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(2)\n                velocity[i] = (self.w * velocity[i] +\n                               self.c1 * r1 * (personal_best_position[i] - position[i]) +\n                               self.c2 * r2 * (global_best_position - position[i]))\n\n                # Apply quantum tunneling effect\n                tunneling_effect = self.q_factor * np.random.normal(size=self.dim)\n                position[i] += velocity[i] + tunneling_effect\n                position[i] = np.clip(position[i], lb, ub)\n\n                current_value = func(position[i])\n                evaluations += 1\n\n                if current_value < personal_best_value[i]:\n                    personal_best_position[i] = position[i]\n                    personal_best_value[i] = current_value\n\n                if current_value < global_best_value:\n                    global_best_position = position[i]\n                    global_best_value = current_value\n\n                if evaluations >= self.budget:\n                    break\n\n            # Evolutionary Selection and Mutation\n            sorted_indices = np.argsort(personal_best_value)\n            survivors = sorted_indices[:int(self.population_size * (1 - self.evolutionary_pressure))]\n            offspring_size = self.population_size - len(survivors)\n            offspring = personal_best_position[np.random.choice(survivors, offspring_size)]\n            mutation_mask = np.random.rand(offspring_size, self.dim) < self.mutation_rate\n            offspring += mutation_mask * np.random.laplace(size=(offspring_size, self.dim))\n            offspring = np.clip(offspring, lb, ub)\n\n            # Replace least fit individuals with offspring\n            position[sorted_indices[-offspring_size:]] = offspring\n            fitness[sorted_indices[-offspring_size:]] = [func(p) for p in offspring]\n            evaluations += offspring_size\n\n            for i in range(offspring_size):\n                if fitness[sorted_indices[-i-1]] < personal_best_value[sorted_indices[-i-1]]:\n                    personal_best_position[sorted_indices[-i-1]] = position[sorted_indices[-i-1]]\n                    personal_best_value[sorted_indices[-i-1]] = fitness[sorted_indices[-i-1]]\n\n                if fitness[sorted_indices[-i-1]] < global_best_value:\n                    global_best_position = position[sorted_indices[-i-1]]\n                    global_best_value = fitness[sorted_indices[-i-1]]\n\n        return global_best_position, global_best_value", "name": "Adaptive_Evolutionary_Quantum_Swarm_Optimizer", "description": "Adaptive Evolutionary Quantum Swarm Optimizer: Combines adaptive quantum potential fields with evolutionary strategies for enhanced exploration and exploitation balance.", "configspace": "", "generation": 44, "fitness": 0.27770069271123743, "feedback": "The algorithm Adaptive_Evolutionary_Quantum_Swarm_Optimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28 with standard deviation 0.00.", "error": "", "parent_id": "23a31d7a-9c0f-430c-a37f-f7caeaedc69a", "metadata": {"aucs": [0.27770069271123743, 0.27770069271123743, 0.27770069271123743]}, "mutation_prompt": null}
{"id": "ff273373-c95c-4dc6-aef8-0d467e58d657", "solution": "import numpy as np\n\nclass BioInspired_Random_Walks_Dynamic_Boundary_Adaptation:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 30\n        self.step_size = 0.1\n        self.boundary_factor = 0.1\n        self.mutation_rate = 0.05\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        np.random.seed(42)\n\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_index = np.argmin(fitness)\n        best_position = population[best_index]\n        best_fitness = fitness[best_index]\n\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if np.random.rand() < self.mutation_rate:\n                    new_position = population[i] + np.random.normal(0, self.step_size, self.dim)\n                else:\n                    new_position = population[i] + self.step_size * (np.random.rand(self.dim) - 0.5)\n                \n                new_position = np.clip(new_position, lb, ub)\n                new_fitness = func(new_position)\n                evaluations += 1\n\n                if new_fitness < fitness[i]:\n                    population[i] = new_position\n                    fitness[i] = new_fitness\n\n                if new_fitness < best_fitness:\n                    best_position = new_position\n                    best_fitness = new_fitness\n\n                if evaluations >= self.budget:\n                    break\n\n            # Dynamic boundary adaptation\n            adapt_factor = self.boundary_factor * (1 - evaluations / self.budget)\n            lb = np.maximum(func.bounds.lb, lb - adapt_factor * (ub - lb))\n            ub = np.minimum(func.bounds.ub, ub + adapt_factor * (ub - lb))\n\n        return best_position, best_fitness", "name": "BioInspired_Random_Walks_Dynamic_Boundary_Adaptation", "description": "Bio-inspired Random Walks with Dynamic Boundary Adaptation (BRWDBA): Combines random walk exploration with dynamic boundary adjustments to effectively navigate complex landscapes and maintain diversity.", "configspace": "", "generation": 45, "fitness": 0.23010249974688846, "feedback": "The algorithm BioInspired_Random_Walks_Dynamic_Boundary_Adaptation got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.23 with standard deviation 0.00.", "error": "", "parent_id": "23a31d7a-9c0f-430c-a37f-f7caeaedc69a", "metadata": {"aucs": [0.23010249974688846, 0.23010249974688846, 0.23010249974688846]}, "mutation_prompt": null}
{"id": "4374fbe2-792e-469b-ae31-ca4f456cd671", "solution": "import numpy as np\n\nclass Quantum_Enhanced_Adaptive_Differential_Swarm:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 25\n        self.cr = 0.9  # Crossover rate\n        self.f = 0.8   # Differential weight\n        self.inertia_weight_start = 0.9\n        self.inertia_weight_end = 0.4\n        self.q_factor = 0.1\n        self.epsilon = 1e-8\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        np.random.seed(42)\n\n        # Initialize the population\n        position = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocity = np.zeros((self.population_size, self.dim))\n        personal_best_position = np.copy(position)\n        personal_best_value = np.array([func(p) for p in personal_best_position])\n        global_best_position = personal_best_position[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            inertia_weight = self.inertia_weight_start - (\n                (self.inertia_weight_start - self.inertia_weight_end) * \n                (evaluations / self.budget)\n            )\n\n            for i in range(self.population_size):\n                # Differential mutation\n                indices = np.random.choice(self.population_size, 3, replace=False)\n                a, b, c = position[indices]\n                mutant = a + self.f * (b - c)\n\n                # Quantum-inspired crossover\n                trial = np.copy(position[i])\n                crossover_points = np.random.rand(self.dim) < self.cr\n                trial[crossover_points] = mutant[crossover_points]\n\n                trial_value = func(trial)\n                evaluations += 1\n\n                if trial_value < personal_best_value[i]:\n                    personal_best_position[i] = trial\n                    personal_best_value[i] = trial_value\n\n                if trial_value < global_best_value:\n                    global_best_position = trial\n                    global_best_value = trial_value\n\n                if evaluations >= self.budget:\n                    break\n\n                # Update velocity and position with adaptive inertia\n                velocity[i] = (inertia_weight * velocity[i] +\n                               self.q_factor * np.random.normal(size=self.dim))\n                position[i] = position[i] + velocity[i]\n                position[i] = np.clip(position[i], lb, ub)\n\n        return global_best_position, global_best_value", "name": "Quantum_Enhanced_Adaptive_Differential_Swarm", "description": "Quantum-Enhanced Adaptive Differential Swarm: Incorporates quantum-inspired differential mutation and adaptive inertia to navigate complex landscapes efficiently.", "configspace": "", "generation": 46, "fitness": 0.26066580967227304, "feedback": "The algorithm Quantum_Enhanced_Adaptive_Differential_Swarm got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.26 with standard deviation 0.00.", "error": "", "parent_id": "23a31d7a-9c0f-430c-a37f-f7caeaedc69a", "metadata": {"aucs": [0.26066580967227304, 0.26066580967227304, 0.26066580967227304]}, "mutation_prompt": null}
{"id": "bb2654b2-fc94-41b4-bd84-c83fc667acf8", "solution": "import numpy as np\n\nclass Adaptive_Quantum_Pso_Gauss_Dynamic_Reset_Optimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.w = 0.7  # Starting inertia weight\n        self.q_factor = 0.9\n        self.gaussian_scale = 0.1\n        self.initial_reset_chance = 0.1\n        self.momentum_factor = 1.05\n        self.adaptive_rate = 0.95\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        np.random.seed(42)\n\n        position = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocity = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_position = np.copy(position)\n        personal_best_value = np.array([func(p) for p in personal_best_position])\n        global_best_position = personal_best_position[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n        \n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(2)\n                self.w *= self.adaptive_rate  # Dynamically adjust inertia weight\n                velocity[i] = (self.w * velocity[i] +\n                               self.c1 * r1 * (personal_best_position[i] - position[i]) +\n                               self.c2 * r2 * (global_best_position - position[i]))\n                \n                adaptive_gaussian_scale = self.gaussian_scale * (1 - evaluations / self.budget)\n                position[i] += (velocity[i] + \n                                self.q_factor * np.random.normal(scale=adaptive_gaussian_scale, size=self.dim))\n                position[i] = np.clip(position[i], lb, ub)\n\n                current_value = func(position[i])\n                evaluations += 1\n                \n                if current_value < personal_best_value[i]:\n                    personal_best_position[i] = position[i]\n                    personal_best_value[i] = current_value\n                \n                if current_value < global_best_value:\n                    global_best_position = position[i]\n                    global_best_value = current_value\n\n                if evaluations >= self.budget:\n                    break\n\n            # Adaptive Reset Mechanism based on stagnation\n            reset_chance = self.initial_reset_chance * ((evaluations / self.budget) ** self.momentum_factor)\n            if np.random.rand() < reset_chance:\n                random_index = np.random.randint(self.population_size)\n                position[random_index] = np.random.uniform(lb, ub, self.dim)\n                current_value = func(position[random_index])\n                evaluations += 1\n                \n                if current_value < personal_best_value[random_index]:\n                    personal_best_position[random_index] = position[random_index]\n                    personal_best_value[random_index] = current_value\n                \n                if current_value < global_best_value:\n                    global_best_position = position[random_index]\n                    global_best_value = current_value\n\n        return global_best_position, global_best_value", "name": "Adaptive_Quantum_Pso_Gauss_Dynamic_Reset_Optimizer", "description": "Adaptive Quantum PSO with Gaussian Perturbation and Dynamic Reset Mechanism enhances exploration and exploitation balance for efficient global optimization.", "configspace": "", "generation": 47, "fitness": 0.2780901158458249, "feedback": "The algorithm Adaptive_Quantum_Pso_Gauss_Dynamic_Reset_Optimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28 with standard deviation 0.00.", "error": "", "parent_id": "23a31d7a-9c0f-430c-a37f-f7caeaedc69a", "metadata": {"aucs": [0.2780901158458249, 0.2780901158458249, 0.2780901158458249]}, "mutation_prompt": null}
{"id": "1146bdf2-05bd-43d0-a0ec-4e3a1140cf43", "solution": "import numpy as np\n\nclass Quantum_Inspired_PSO_MultiLeader:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 30\n        self.c1 = 1.5\n        self.c2 = 1.5\n        self.w_min = 0.4\n        self.w_max = 0.9\n        self.q_factor = 0.8\n        self.gaussian_scale = 0.15\n        self.leader_fraction = 0.2\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        np.random.seed(42)\n\n        position = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocity = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_position = np.copy(position)\n        personal_best_value = np.array([func(p) for p in personal_best_position])\n        global_best_position = personal_best_position[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                # Adaptive Inertia Weight\n                w = self.w_max - ((self.w_max - self.w_min) * (evaluations / self.budget))\n                \n                r1, r2 = np.random.rand(2)\n                velocity[i] = (w * velocity[i] +\n                               self.c1 * r1 * (personal_best_position[i] - position[i]) +\n                               self.c2 * r2 * (global_best_position - position[i]))\n                \n                adaptive_gaussian_scale = self.gaussian_scale * (1 - evaluations / self.budget)\n                position[i] += (velocity[i] + \n                                self.q_factor * np.random.normal(scale=adaptive_gaussian_scale, size=self.dim))\n                position[i] = np.clip(position[i], lb, ub)\n\n                current_value = func(position[i])\n                evaluations += 1\n                \n                if current_value < personal_best_value[i]:\n                    personal_best_position[i] = position[i]\n                    personal_best_value[i] = current_value\n                \n                if current_value < global_best_value:\n                    global_best_position = position[i]\n                    global_best_value = current_value\n\n                if evaluations >= self.budget:\n                    break\n\n            # Multi-Leader Approach\n            leader_count = int(self.population_size * self.leader_fraction)\n            sorted_indices = np.argsort(personal_best_value)\n            leaders_positions = personal_best_position[sorted_indices[:leader_count]]\n            leaders_values = personal_best_value[sorted_indices[:leader_count]]\n            \n            # Randomly re-initialize some particles using leaders' influence\n            for j in range(leader_count):\n                random_index = np.random.randint(self.population_size)\n                influence = np.random.choice(leaders_positions)\n                position[random_index] = np.random.uniform(lb, ub, self.dim) * 0.5 + influence * 0.5\n                current_value = func(position[random_index])\n                evaluations += 1\n                \n                if current_value < personal_best_value[random_index]:\n                    personal_best_position[random_index] = position[random_index]\n                    personal_best_value[random_index] = current_value\n                \n                if current_value < global_best_value:\n                    global_best_position = position[random_index]\n                    global_best_value = current_value\n\n        return global_best_position, global_best_value", "name": "Quantum_Inspired_PSO_MultiLeader", "description": "Quantum-Inspired PSO with Adaptive Inertia and Multi-Leader Approach to enhance convergence speed and solution diversity in photonic structure optimization.", "configspace": "", "generation": 48, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('a must be 1-dimensional').", "error": "ValueError('a must be 1-dimensional')", "parent_id": "bb2654b2-fc94-41b4-bd84-c83fc667acf8", "metadata": {}, "mutation_prompt": null}
{"id": "ed9e0e73-f5da-4e4e-b07c-937455b7fc03", "solution": "import numpy as np\n\nclass Enhanced_Adaptive_Quantum_PSO_Levy_Neighbor_Adjust_Optimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.w = 0.7\n        self.q_factor = 0.9\n        self.gaussian_scale = 0.1\n        self.initial_reset_chance = 0.1\n        self.momentum_factor = 1.05\n        self.adaptive_rate = 0.95\n        self.alpha_levy = 1.5  # Lévy flight scale parameter\n\n    def levy_flight(self, size):\n        u = np.random.normal(0, 0.1, size)\n        v = np.random.normal(0, 1, size)\n        step = u / (np.abs(v) ** (1 / self.alpha_levy))\n        return step\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        np.random.seed(42)\n\n        position = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocity = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_position = np.copy(position)\n        personal_best_value = np.array([func(p) for p in personal_best_position])\n        global_best_position = personal_best_position[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(2)\n                self.w *= self.adaptive_rate\n                velocity[i] = (self.w * velocity[i] +\n                               self.c1 * r1 * (personal_best_position[i] - position[i]) +\n                               self.c2 * r2 * (global_best_position - position[i]))\n                \n                adaptive_gaussian_scale = self.gaussian_scale * (1 - evaluations / self.budget)\n                position[i] += (velocity[i] + \n                                self.q_factor * np.random.normal(scale=adaptive_gaussian_scale, size=self.dim) +\n                                self.levy_flight(self.dim))\n                position[i] = np.clip(position[i], lb, ub)\n\n                current_value = func(position[i])\n                evaluations += 1\n\n                if current_value < personal_best_value[i]:\n                    personal_best_position[i] = position[i]\n                    personal_best_value[i] = current_value\n                \n                if current_value < global_best_value:\n                    global_best_position = position[i]\n                    global_best_value = current_value\n\n                if evaluations >= self.budget:\n                    break\n\n            reset_chance = self.initial_reset_chance * ((evaluations / self.budget) ** self.momentum_factor)\n            if np.random.rand() < reset_chance:\n                random_index = np.random.randint(self.population_size)\n                position[random_index] = np.random.uniform(lb, ub, self.dim)\n                current_value = func(position[random_index])\n                evaluations += 1\n                \n                if current_value < personal_best_value[random_index]:\n                    personal_best_position[random_index] = position[random_index]\n                    personal_best_value[random_index] = current_value\n                \n                if current_value < global_best_value:\n                    global_best_position = position[random_index]\n                    global_best_value = current_value\n\n            # Dynamic Neighborhood Adjustment\n            if evaluations > (0.5 * self.budget):\n                for i in range(self.population_size):\n                    local_indices = np.random.choice(self.population_size, size=5, replace=False)\n                    local_best_position = min(local_indices, key=lambda idx: personal_best_value[idx])\n                    if personal_best_value[local_best_position] < personal_best_value[i]:\n                        personal_best_position[i] = personal_best_position[local_best_position]\n                        personal_best_value[i] = personal_best_value[local_best_position]\n\n        return global_best_position, global_best_value", "name": "Enhanced_Adaptive_Quantum_PSO_Levy_Neighbor_Adjust_Optimizer", "description": "Enhanced Adaptive Quantum PSO with Lévy Flight and Dynamic Neighborhood Adjustment for robust exploration and global optimization.", "configspace": "", "generation": 49, "fitness": 0.27807063160875856, "feedback": "The algorithm Enhanced_Adaptive_Quantum_PSO_Levy_Neighbor_Adjust_Optimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28 with standard deviation 0.00.", "error": "", "parent_id": "bb2654b2-fc94-41b4-bd84-c83fc667acf8", "metadata": {"aucs": [0.27807063160875856, 0.27807063160875856, 0.27807063160875856]}, "mutation_prompt": null}
{"id": "a1be1765-432a-4d2b-b381-05819c8ca668", "solution": "import numpy as np\n\nclass Quantum_Inspired_Swarm_Diversity_Optimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.inertia_weight = 0.7\n        self.q_factor = 0.9\n        self.mutation_rate = 0.02\n        self.restart_chance = 0.05\n        self.diversity_threshold = 0.2\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        np.random.seed(42)\n        \n        position = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocity = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_position = np.copy(position)\n        personal_best_value = np.array([func(p) for p in personal_best_position])\n        global_best_position = personal_best_position[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n        \n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(2)\n                velocity[i] = (self.inertia_weight * velocity[i] +\n                               self.c1 * r1 * (personal_best_position[i] - position[i]) +\n                               self.c2 * r2 * (global_best_position - position[i]))\n                \n                position[i] += (velocity[i] +\n                                self.q_factor * np.random.normal(scale=0.1, size=self.dim))\n                position[i] = np.clip(position[i], lb, ub)\n\n                current_value = func(position[i])\n                evaluations += 1\n\n                if current_value < personal_best_value[i]:\n                    personal_best_position[i] = position[i]\n                    personal_best_value[i] = current_value\n                \n                if current_value < global_best_value:\n                    global_best_position = position[i]\n                    global_best_value = current_value\n\n                if evaluations >= self.budget:\n                    break\n\n            # Diversity maintenance through mutation\n            diversity_measure = np.std(position, axis=0).mean()\n            if diversity_measure < self.diversity_threshold:\n                for j in range(self.population_size):\n                    if np.random.rand() < self.mutation_rate:\n                        mutation_vector = np.random.normal(scale=0.1, size=self.dim)\n                        position[j] = np.clip(position[j] + mutation_vector, lb, ub)\n\n            # Restart strategy to escape local optima\n            if np.random.rand() < self.restart_chance:\n                random_index = np.random.randint(self.population_size)\n                position[random_index] = np.random.uniform(lb, ub, self.dim)\n                current_value = func(position[random_index])\n                evaluations += 1\n\n                if current_value < personal_best_value[random_index]:\n                    personal_best_position[random_index] = position[random_index]\n                    personal_best_value[random_index] = current_value\n                \n                if current_value < global_best_value:\n                    global_best_position = position[random_index]\n                    global_best_value = current_value\n\n        return global_best_position, global_best_value", "name": "Quantum_Inspired_Swarm_Diversity_Optimizer", "description": "Quantum-inspired Swarm Intelligence with Dynamic Diversity Maintenance balances convergence and diversity via adaptive mutation and restart strategies for robust global optimization.", "configspace": "", "generation": 50, "fitness": 0.2773280644128564, "feedback": "The algorithm Quantum_Inspired_Swarm_Diversity_Optimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28 with standard deviation 0.00.", "error": "", "parent_id": "bb2654b2-fc94-41b4-bd84-c83fc667acf8", "metadata": {"aucs": [0.2773280644128564, 0.2773280644128564, 0.2773280644128564]}, "mutation_prompt": null}
{"id": "6bdbaa3b-2b4b-40bd-9187-e156bc5c0263", "solution": "import numpy as np\n\nclass Enhanced_Adaptive_Quantum_Pso_Gauss_Dual_Reset_Optimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.w = 0.7  # Starting inertia weight\n        self.q_factor = 0.9\n        self.gaussian_scale = 0.1\n        self.initial_reset_chance = 0.1\n        self.momentum_factor = 1.05\n        self.adaptive_rate = 0.95\n        self.stagnation_threshold = 10  # New parameter for additional reset mechanism\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        np.random.seed(42)\n\n        position = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocity = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_position = np.copy(position)\n        personal_best_value = np.array([func(p) for p in personal_best_position])\n        global_best_position = personal_best_position[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n        \n        evaluations = self.population_size\n        no_improvement_counter = 0  # Counter for tracking stagnation\n        \n        while evaluations < self.budget:\n            improvement_made = False\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(2)\n                self.w *= self.adaptive_rate  # Dynamically adjust inertia weight\n                velocity[i] = (self.w * velocity[i] +\n                               self.c1 * r1 * (personal_best_position[i] - position[i]) +\n                               self.c2 * r2 * (global_best_position - position[i]))\n                \n                adaptive_gaussian_scale = self.gaussian_scale * (1 - evaluations / self.budget)\n                position[i] += (velocity[i] + \n                                self.q_factor * np.random.normal(scale=adaptive_gaussian_scale, size=self.dim))\n                position[i] = np.clip(position[i], lb, ub)\n\n                current_value = func(position[i])\n                evaluations += 1\n                \n                if current_value < personal_best_value[i]:\n                    personal_best_position[i] = position[i]\n                    personal_best_value[i] = current_value\n                    improvement_made = True\n                \n                if current_value < global_best_value:\n                    global_best_position = position[i]\n                    global_best_value = current_value\n                    improvement_made = True\n\n                if evaluations >= self.budget:\n                    break\n\n            # Adaptive Reset Mechanism based on stagnation\n            reset_chance = self.initial_reset_chance * ((evaluations / self.budget) ** self.momentum_factor)\n            if np.random.rand() < reset_chance:\n                random_index = np.random.randint(self.population_size)\n                position[random_index] = np.random.uniform(lb, ub, self.dim)\n                current_value = func(position[random_index])\n                evaluations += 1\n                \n                if current_value < personal_best_value[random_index]:\n                    personal_best_position[random_index] = position[random_index]\n                    personal_best_value[random_index] = current_value\n                    improvement_made = True\n                \n                if current_value < global_best_value:\n                    global_best_position = position[random_index]\n                    global_best_value = current_value\n                    improvement_made = True\n\n            # Additional Reset Mechanism for stagnation\n            if not improvement_made:\n                no_improvement_counter += 1\n            else:\n                no_improvement_counter = 0\n\n            if no_improvement_counter >= self.stagnation_threshold:\n                stagnation_index = np.random.randint(self.population_size)\n                position[stagnation_index] = np.random.uniform(lb, ub, self.dim)\n                current_value = func(position[stagnation_index])\n                evaluations += 1\n                \n                if current_value < personal_best_value[stagnation_index]:\n                    personal_best_position[stagnation_index] = position[stagnation_index]\n                    personal_best_value[stagnation_index] = current_value\n                    no_improvement_counter = 0  # Reset counter on improvement\n                \n                if current_value < global_best_value:\n                    global_best_position = position[stagnation_index]\n                    global_best_value = current_value\n\n        return global_best_position, global_best_value", "name": "Enhanced_Adaptive_Quantum_Pso_Gauss_Dual_Reset_Optimizer", "description": "Enhanced Adaptive Quantum PSO with Gaussian Perturbation and Dual Dynamic Reset Mechanism to improve exploration and convergence by addressing particle stagnation.", "configspace": "", "generation": 51, "fitness": 0.2780901124405072, "feedback": "The algorithm Enhanced_Adaptive_Quantum_Pso_Gauss_Dual_Reset_Optimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28 with standard deviation 0.00.", "error": "", "parent_id": "bb2654b2-fc94-41b4-bd84-c83fc667acf8", "metadata": {"aucs": [0.2780901124405072, 0.2780901124405072, 0.2780901124405072]}, "mutation_prompt": null}
{"id": "9f02530a-e93d-4297-bc22-70355754d2c1", "solution": "import numpy as np\n\nclass Enhanced_Adaptive_Quantum_Pso_Annealing_Gauss_Optimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.w = 0.7  # Starting inertia weight\n        self.q_factor = 0.9\n        self.gaussian_scale = 0.1\n        self.initial_reset_chance = 0.1\n        self.momentum_factor = 1.05\n        self.adaptive_rate = 0.95\n        self.temperature = 1.0  # Starting temperature for simulated annealing\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        np.random.seed(42)\n\n        position = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocity = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_position = np.copy(position)\n        personal_best_value = np.array([func(p) for p in personal_best_position])\n        global_best_position = personal_best_position[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n        \n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(2)\n                self.w *= self.adaptive_rate  # Dynamically adjust inertia weight\n                velocity[i] = (self.w * velocity[i] +\n                               self.c1 * r1 * (personal_best_position[i] - position[i]) +\n                               self.c2 * r2 * (global_best_position - position[i]))\n                \n                adaptive_gaussian_scale = self.gaussian_scale * (1 - evaluations / self.budget)\n                annealing_factor = np.exp(-evaluations / (self.budget * self.temperature))\n                \n                position[i] += (velocity[i] + \n                                self.q_factor * np.random.normal(scale=adaptive_gaussian_scale, size=self.dim) +\n                                annealing_factor * np.random.uniform(-1, 1, self.dim))\n                position[i] = np.clip(position[i], lb, ub)\n\n                current_value = func(position[i])\n                evaluations += 1\n                \n                if current_value < personal_best_value[i]:\n                    personal_best_position[i] = position[i]\n                    personal_best_value[i] = current_value\n                \n                if current_value < global_best_value:\n                    global_best_position = position[i]\n                    global_best_value = current_value\n\n                if evaluations >= self.budget:\n                    break\n\n            # Adaptive Reset Mechanism based on stagnation\n            reset_chance = self.initial_reset_chance * ((evaluations / self.budget) ** self.momentum_factor)\n            if np.random.rand() < reset_chance:\n                random_index = np.random.randint(self.population_size)\n                position[random_index] = np.random.uniform(lb, ub, self.dim)\n                current_value = func(position[random_index])\n                evaluations += 1\n                \n                if current_value < personal_best_value[random_index]:\n                    personal_best_position[random_index] = position[random_index]\n                    personal_best_value[random_index] = current_value\n                \n                if current_value < global_best_value:\n                    global_best_position = position[random_index]\n                    global_best_value = current_value\n\n        return global_best_position, global_best_value", "name": "Enhanced_Adaptive_Quantum_Pso_Annealing_Gauss_Optimizer", "description": "Enhanced Adaptive Quantum PSO incorporates Simulated Annealing-based Exploration with Adaptive Gaussian Perturbation for improved convergence in complex landscapes.", "configspace": "", "generation": 52, "fitness": 0.2781055294317494, "feedback": "The algorithm Enhanced_Adaptive_Quantum_Pso_Annealing_Gauss_Optimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28 with standard deviation 0.00.", "error": "", "parent_id": "bb2654b2-fc94-41b4-bd84-c83fc667acf8", "metadata": {"aucs": [0.2781055294317494, 0.2781055294317494, 0.2781055294317494]}, "mutation_prompt": null}
{"id": "9f58cda6-a6d9-453f-8e43-1ea04b0060a3", "solution": "import numpy as np\n\nclass Hybrid_Dynamic_Firefly_DE_Optimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.alpha_0 = 0.5\n        self.beta_0 = 1.0\n        self.gamma = 1.0\n        self.mutation_factor = 0.5\n        self.crossover_rate = 0.7\n        self.adaptive_decay = 0.99\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        np.random.seed(42)\n\n        # Initialize population\n        position = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        light_intensity = np.array([func(p) for p in position])\n        \n        evaluations = self.population_size\n        global_best_position = position[np.argmin(light_intensity)]\n        global_best_value = np.min(light_intensity)\n\n        while evaluations < self.budget:\n            # Dynamic updating of alpha and mutation factor\n            alpha = self.alpha_0 * (self.adaptive_decay ** (evaluations / self.population_size))\n            mutation_factor = self.mutation_factor * (1 + np.sin(evaluations / self.budget * np.pi))\n\n            for i in range(self.population_size):\n                for j in range(self.population_size):\n                    if light_intensity[i] > light_intensity[j]:\n                        distance = np.linalg.norm(position[i] - position[j])\n                        beta = self.beta_0 * np.exp(-self.gamma * distance ** 2)\n                        attraction = beta * (position[j] - position[i]) + alpha * np.random.uniform(-1, 1, self.dim)\n                        position[i] += attraction\n                        position[i] = np.clip(position[i], lb, ub)\n\n                # Apply Differential Evolution (DE) strategy\n                idxs = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = a + mutation_factor * (b - c)\n                trial = np.where(np.random.rand(self.dim) < self.crossover_rate, mutant, position[i])\n                trial = np.clip(trial, lb, ub)\n\n                trial_value = func(trial)\n                evaluations += 1\n\n                if trial_value < light_intensity[i]:\n                    position[i] = trial\n                    light_intensity[i] = trial_value\n\n                if trial_value < global_best_value:\n                    global_best_position = trial\n                    global_best_value = trial_value\n\n                if evaluations >= self.budget:\n                    break\n\n        return global_best_position, global_best_value", "name": "Hybrid_Dynamic_Firefly_DE_Optimizer", "description": "Hybrid Dynamic Firefly and Differential Evolution Algorithm with Adaptive Attraction and Mutation Rates for Efficient Exploration and Exploitation in Complex Search Spaces.", "configspace": "", "generation": 53, "fitness": -Infinity, "feedback": "An exception occurred: NameError(\"name 'population' is not defined\").", "error": "NameError(\"name 'population' is not defined\")", "parent_id": "9f02530a-e93d-4297-bc22-70355754d2c1", "metadata": {}, "mutation_prompt": null}
{"id": "6e545ef8-5bc8-45d8-9395-9de1840b98af", "solution": "import numpy as np\n\nclass Dynamic_Harmony_Search_Optimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.harmony_memory_size = 30\n        self.hmcr = 0.9  # Harmony Memory Consideration Rate\n        self.par = 0.3  # Pitch Adjustment Rate\n        self.bandwidth = 0.1\n        self.layers = 3\n        self.adaptive_rate = 0.95\n        self.convergence_threshold = 1e-6\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        np.random.seed(42)\n\n        # Initialize harmony memory\n        harmony_memory = np.random.uniform(lb, ub, (self.harmony_memory_size, self.dim))\n        harmony_values = np.array([func(h) for h in harmony_memory])\n        best_harmony = harmony_memory[np.argmin(harmony_values)]\n        best_value = np.min(harmony_values)\n        \n        evaluations = self.harmony_memory_size\n        prev_best_value = best_value\n\n        while evaluations < self.budget:\n            new_harmony = np.empty(self.dim)\n            for d in range(self.dim):\n                if np.random.rand() < self.hmcr:\n                    new_harmony[d] = harmony_memory[np.random.randint(self.harmony_memory_size), d]\n                    if np.random.rand() < self.par:\n                        new_harmony[d] += np.random.uniform(-1, 1) * self.bandwidth\n                else:\n                    new_harmony[d] = np.random.uniform(lb[d], ub[d])\n\n            # Layer-based spatial exploitation\n            for layer in range(self.layers):\n                layer_factor = (self.layers - layer) / self.layers\n                new_harmony += layer_factor * np.random.normal(0, self.bandwidth, self.dim)\n                new_harmony = np.clip(new_harmony, lb, ub)\n            \n            current_value = func(new_harmony)\n            evaluations += 1\n\n            if current_value < np.max(harmony_values):\n                worst_index = np.argmax(harmony_values)\n                harmony_memory[worst_index] = new_harmony\n                harmony_values[worst_index] = current_value\n\n            if current_value < best_value:\n                best_harmony = new_harmony\n                best_value = current_value\n            \n            # Dynamic adaptation\n            if prev_best_value - best_value < self.convergence_threshold:\n                self.bandwidth *= self.adaptive_rate\n            prev_best_value = best_value\n\n            if evaluations >= self.budget:\n                break\n\n        return best_harmony, best_value", "name": "Dynamic_Harmony_Search_Optimizer", "description": "Dynamic Harmony Search Optimizer with Multi-Layer Spatial Exploitation leverages adaptive harmony memory with multi-dimensional decomposition for efficient convergence in complex landscapes.", "configspace": "", "generation": 54, "fitness": 0.27703756026315196, "feedback": "The algorithm Dynamic_Harmony_Search_Optimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28 with standard deviation 0.00.", "error": "", "parent_id": "9f02530a-e93d-4297-bc22-70355754d2c1", "metadata": {"aucs": [0.27703756026315196, 0.27703756026315196, 0.27703756026315196]}, "mutation_prompt": null}
{"id": "2136b061-5486-4e15-bbb2-cfeed8c33128", "solution": "import numpy as np\n\nclass Enhanced_Quantum_Pso_Neighborhood_Informed_Optimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.w = 0.7\n        self.q_factor = 0.9\n        self.gaussian_scale = 0.1\n        self.temperature = 1.0\n        self.neighborhood_size = 5  # New: Local neighborhood size\n        self.adaptive_rate = 0.95\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        np.random.seed(42)\n\n        position = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocity = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_position = np.copy(position)\n        personal_best_value = np.array([func(p) for p in personal_best_position])\n        global_best_position = personal_best_position[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n        \n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                # Get neighborhood best\n                distances = np.linalg.norm(position - position[i], axis=1)\n                neighbors_idx = np.argsort(distances)[:self.neighborhood_size]\n                neighborhood_best_value = np.min(personal_best_value[neighbors_idx])\n                neighborhood_best_position = personal_best_position[neighbors_idx[np.argmin(personal_best_value[neighbors_idx])]]\n                \n                r1, r2 = np.random.rand(2)\n                self.w *= self.adaptive_rate\n                velocity[i] = (self.w * velocity[i] +\n                               self.c1 * r1 * (personal_best_position[i] - position[i]) +\n                               self.c2 * r2 * (neighborhood_best_position - position[i]))\n                \n                adaptive_gaussian_scale = self.gaussian_scale * (1 - evaluations / self.budget)\n                annealing_factor = np.exp(-evaluations / (self.budget * self.temperature))\n                \n                position[i] += (velocity[i] +\n                                self.q_factor * np.random.normal(scale=adaptive_gaussian_scale, size=self.dim) +\n                                annealing_factor * np.random.uniform(-1, 1, self.dim))\n                position[i] = np.clip(position[i], lb, ub)\n\n                current_value = func(position[i])\n                evaluations += 1\n                \n                if current_value < personal_best_value[i]:\n                    personal_best_position[i] = position[i]\n                    personal_best_value[i] = current_value\n                \n                if current_value < global_best_value:\n                    global_best_position = position[i]\n                    global_best_value = current_value\n\n                if evaluations >= self.budget:\n                    break\n\n        return global_best_position, global_best_value", "name": "Enhanced_Quantum_Pso_Neighborhood_Informed_Optimizer", "description": "Enhanced Quantum PSO with Dynamic Neighborhood Informed Search combines local neighborhood information and adaptive Gaussian perturbation for improved convergence in complex landscapes.", "configspace": "", "generation": 55, "fitness": 0.27782318322249455, "feedback": "The algorithm Enhanced_Quantum_Pso_Neighborhood_Informed_Optimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28 with standard deviation 0.00.", "error": "", "parent_id": "9f02530a-e93d-4297-bc22-70355754d2c1", "metadata": {"aucs": [0.27782318322249455, 0.27782318322249455, 0.27782318322249455]}, "mutation_prompt": null}
{"id": "bc7afffc-2082-4e4c-a35f-04420c4db76c", "solution": "import numpy as np\n\nclass Progressive_Multi_Swarm_Quantum_Optimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_swarms = 3\n        self.swarm_size = 10\n        self.c1 = 1.5\n        self.c2 = 1.5\n        self.w = 0.8\n        self.alpha = 0.9\n        self.beta = 0.1\n        self.q_sigma = 0.05\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        np.random.seed(42)\n\n        swarms = [np.random.uniform(lb, ub, (self.swarm_size, self.dim)) for _ in range(self.num_swarms)]\n        velocities = [np.random.uniform(-1, 1, (self.swarm_size, self.dim)) for _ in range(self.num_swarms)]\n        personal_best_positions = [np.copy(swarms[i]) for i in range(self.num_swarms)]\n        personal_best_values = [np.array([func(p) for p in personal_best_positions[i]]) for i in range(self.num_swarms)]\n        global_best_positions = [personal_best_positions[i][np.argmin(personal_best_values[i])] for i in range(self.num_swarms)]\n        global_best_values = [np.min(personal_best_values[i]) for i in range(self.num_swarms)]\n\n        evaluations = self.swarm_size * self.num_swarms\n\n        while evaluations < self.budget:\n            for swarm_index in range(self.num_swarms):\n                for i in range(self.swarm_size):\n                    r1, r2, r3 = np.random.rand(3)\n                    velocities[swarm_index][i] = (\n                        self.w * velocities[swarm_index][i] +\n                        self.c1 * r1 * (personal_best_positions[swarm_index][i] - swarms[swarm_index][i]) +\n                        self.c2 * r2 * (global_best_positions[swarm_index] - swarms[swarm_index][i])\n                    )\n                    \n                    quantum_factor = self.q_sigma * np.random.standard_normal(size=self.dim)\n                    swarms[swarm_index][i] += velocities[swarm_index][i] + quantum_factor\n                    swarms[swarm_index][i] = np.clip(swarms[swarm_index][i], lb, ub)\n\n                    current_value = func(swarms[swarm_index][i])\n                    evaluations += 1\n\n                    if current_value < personal_best_values[swarm_index][i]:\n                        personal_best_positions[swarm_index][i] = swarms[swarm_index][i]\n                        personal_best_values[swarm_index][i] = current_value\n\n                    if current_value < global_best_values[swarm_index]:\n                        global_best_positions[swarm_index] = swarms[swarm_index][i]\n                        global_best_values[swarm_index] = current_value\n\n                    if evaluations >= self.budget:\n                        break\n\n            # Inter-swarm communication and hierarchical update\n            if swarm_index < self.num_swarms - 1:\n                next_swarm_index = swarm_index + 1\n                if global_best_values[swarm_index] < global_best_values[next_swarm_index]:\n                    global_best_positions[next_swarm_index] = global_best_positions[swarm_index]\n                    global_best_values[next_swarm_index] = global_best_values[swarm_index]\n\n            # Update velocities and positions adaptively\n            self.w *= self.alpha\n            self.q_sigma *= self.beta\n\n        best_swarm_index = np.argmin(global_best_values)\n        return global_best_positions[best_swarm_index], global_best_values[best_swarm_index]", "name": "Progressive_Multi_Swarm_Quantum_Optimizer", "description": "Progressive Multi-Swarm Quantum Optimizer uses hierarchically structured swarms with quantum-inspired position updates and adaptive learning rates for efficient global optimization.", "configspace": "", "generation": 56, "fitness": 0.2741095503959825, "feedback": "The algorithm Progressive_Multi_Swarm_Quantum_Optimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.27 with standard deviation 0.00.", "error": "", "parent_id": "9f02530a-e93d-4297-bc22-70355754d2c1", "metadata": {"aucs": [0.2741095503959825, 0.2741095503959825, 0.2741095503959825]}, "mutation_prompt": null}
{"id": "ff37aade-6eba-4b50-bb86-3917c7bd2540", "solution": "import numpy as np\n\nclass Enhanced_Levy_Quantum_Pso_Optimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.w = 0.7  # Starting inertia weight\n        self.q_factor = 0.9\n        self.gaussian_scale = 0.1\n        self.initial_reset_chance = 0.1\n        self.levy_alpha = 1.5\n        self.temperature = 1.0  # Starting temperature for simulated annealing\n\n    def levy_flight(self, scale, size):\n        u = np.random.normal(0, 1, size) * scale\n        v = np.random.normal(0, 1, size)\n        step = u / np.power(np.abs(v), 1 / self.levy_alpha)\n        return step\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        np.random.seed(42)\n\n        position = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocity = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_position = np.copy(position)\n        personal_best_value = np.array([func(p) for p in personal_best_position])\n        global_best_position = personal_best_position[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n        \n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(2)\n                self.w *= 0.99  # Gradually reduce inertia weight\n                velocity[i] = (self.w * velocity[i] +\n                               self.c1 * r1 * (personal_best_position[i] - position[i]) +\n                               self.c2 * r2 * (global_best_position - position[i]))\n                \n                adaptive_quantum_scale = self.q_factor * (1 - evaluations / self.budget)\n                levy_step = self.levy_flight(self.gaussian_scale * adaptive_quantum_scale, self.dim)\n                position[i] += velocity[i] + levy_step\n                position[i] = np.clip(position[i], lb, ub)\n\n                current_value = func(position[i])\n                evaluations += 1\n                \n                if current_value < personal_best_value[i]:\n                    personal_best_position[i] = position[i]\n                    personal_best_value[i] = current_value\n                \n                if current_value < global_best_value:\n                    global_best_position = position[i]\n                    global_best_value = current_value\n\n                if evaluations >= self.budget:\n                    break\n\n            # Adaptive Reset Mechanism based on stagnation\n            reset_chance = self.initial_reset_chance * ((evaluations / self.budget) ** 1.05)\n            if np.random.rand() < reset_chance:\n                random_index = np.random.randint(self.population_size)\n                position[random_index] = np.random.uniform(lb, ub, self.dim)\n                current_value = func(position[random_index])\n                evaluations += 1\n                \n                if current_value < personal_best_value[random_index]:\n                    personal_best_position[random_index] = position[random_index]\n                    personal_best_value[random_index] = current_value\n                \n                if current_value < global_best_value:\n                    global_best_position = position[random_index]\n                    global_best_value = current_value\n\n        return global_best_position, global_best_value", "name": "Enhanced_Levy_Quantum_Pso_Optimizer", "description": "Integrates Levy Flight for enhanced exploration and Dynamic Quantum Perturbation to intensify search in complex landscapes, adjusting adaptively with progress to improve convergence.", "configspace": "", "generation": 57, "fitness": 0.2779516783356534, "feedback": "The algorithm Enhanced_Levy_Quantum_Pso_Optimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28 with standard deviation 0.00.", "error": "", "parent_id": "9f02530a-e93d-4297-bc22-70355754d2c1", "metadata": {"aucs": [0.2779516783356534, 0.2779516783356534, 0.2779516783356534]}, "mutation_prompt": null}
{"id": "fcbc08a7-1009-40f4-bc61-5c48f9810e1d", "solution": "import numpy as np\n\nclass Quantum_Inspired_Hybrid_Evolutionary_Swarm_Optimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 30\n        self.alpha = 0.1  # Quantum potential well depth\n        self.beta = 0.9  # Differential evolution factor\n        self.mutation_rate = 0.8\n        self.crossover_rate = 0.9\n        self.gamma = 0.5  # Chaos influence factor\n        self.delta = 0.2  # Chaos scale factor\n\n    def chaotic_sequence(self, x):\n        return self.gamma * x * (1 - x)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        np.random.seed(42)\n\n        # Initialize population\n        position = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        personal_best_position = np.copy(position)\n        personal_best_value = np.array([func(p) for p in personal_best_position])\n        global_best_position = personal_best_position[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n        \n        evaluations = self.population_size\n        chaotic_var = np.random.rand()\n\n        while evaluations < self.budget:\n            # Quantum potential well influence\n            q_potential = self.alpha * np.random.uniform(-1, 1, (self.population_size, self.dim))\n            q_positions = np.clip(position + q_potential, lb, ub)\n\n            # Differential Evolution mutation and crossover\n            for i in range(self.population_size):\n                indices = np.random.choice(self.population_size, 3, replace=False)\n                x1, x2, x3 = position[indices]\n                mutant_vector = x1 + self.beta * (x2 - x3)\n                trial_vector = np.where(np.random.rand(self.dim) < self.crossover_rate, mutant_vector, position[i])\n                trial_vector = np.clip(trial_vector, lb, ub)\n                \n                # Evaluate candidate solutions\n                q_value = func(q_positions[i])\n                trial_value = func(trial_vector)\n                evaluations += 2\n                \n                # Update personal best\n                if q_value < personal_best_value[i]:\n                    personal_best_position[i] = q_positions[i]\n                    personal_best_value[i] = q_value\n                \n                if trial_value < personal_best_value[i]:\n                    personal_best_position[i] = trial_vector\n                    personal_best_value[i] = trial_value\n\n                # Update global best\n                if personal_best_value[i] < global_best_value:\n                    global_best_position = personal_best_position[i]\n                    global_best_value = personal_best_value[i]\n\n                if evaluations >= self.budget:\n                    break\n\n            # Apply chaotic influence\n            chaotic_var = self.chaotic_sequence(chaotic_var)\n            chaos_impact = self.delta * (chaotic_var - 0.5) * (ub - lb)\n            position += chaos_impact\n            position = np.clip(position, lb, ub)\n\n        return global_best_position, global_best_value", "name": "Quantum_Inspired_Hybrid_Evolutionary_Swarm_Optimizer", "description": "Quantum-Inspired Hybrid Evolutionary Swarm Optimizer combines quantum potential wells with differential evolution and a dynamic chaos scheme for robust exploration and exploitation in high-dimensional spaces.", "configspace": "", "generation": 58, "fitness": 0.23443225633221465, "feedback": "The algorithm Quantum_Inspired_Hybrid_Evolutionary_Swarm_Optimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.23 with standard deviation 0.00.", "error": "", "parent_id": "9f02530a-e93d-4297-bc22-70355754d2c1", "metadata": {"aucs": [0.23443225633221465, 0.23443225633221465, 0.23443225633221465]}, "mutation_prompt": null}
{"id": "81c8e253-c17e-49f1-b5ed-94cb600a5004", "solution": "import numpy as np\n\nclass Quantum_Enhanced_CMA_ES:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + int(3 * np.log(dim))\n        self.sigma = 0.5  # Initial step size\n        self.mu = self.population_size // 2\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights /= np.sum(self.weights)\n        self.mueff = np.sum(self.weights) ** 2 / np.sum(self.weights ** 2)\n        self.c1 = 2 / ((dim + 1.3) ** 2 + self.mueff)\n        self.cmu = min(1 - self.c1, 2 * (self.mueff - 2 + 1 / self.mueff) / ((dim + 2) ** 2 + self.mueff))\n        self.cc = (4 + self.mueff / dim) / (dim + 4 + 2 * self.mueff / dim)\n        self.csigma = (self.mueff + 2) / (dim + self.mueff + 5)\n        self.dsigma = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (dim + 1)) - 1) + self.csigma\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        np.random.seed(42)\n        \n        mean = np.random.uniform(lb, ub, self.dim)\n        covariance_matrix = np.eye(self.dim)\n        path_c = np.zeros(self.dim)\n        path_sigma = np.zeros(self.dim)\n        eigen_decomposition = np.eye(self.dim)\n        eigen_values = np.ones(self.dim)\n        evaluations = 0\n        \n        while evaluations < self.budget:\n            samples = np.random.multivariate_normal(np.zeros(self.dim), covariance_matrix, self.population_size)\n            samples *= self.sigma\n            samples = mean + samples\n            \n            # Quantum-inspired sampling adjustment\n            samples += np.random.uniform(-0.1, 0.1, samples.shape)\n            samples = np.clip(samples, lb, ub)\n            \n            fitness = np.array([func(sample) for sample in samples])\n            evaluations += self.population_size\n            \n            indices = np.argsort(fitness)\n            selected = samples[indices[:self.mu]]\n            selected_weighted = np.dot(self.weights, selected)\n            \n            mean = selected_weighted\n            path_sigma = (1 - self.csigma) * path_sigma + np.sqrt(self.csigma * (2 - self.csigma) * self.mueff) / self.sigma * np.dot(eigen_decomposition, mean - selected_weighted)\n            \n            E_norm = np.sqrt(2) * self.dim / 4\n            hsig = np.linalg.norm(path_sigma) / np.sqrt(1 - (1 - self.csigma) ** (2 * evaluations / self.population_size)) / E_norm < 1.4 + 2 / (self.dim + 1)\n            path_c = (1 - self.cc) * path_c + hsig * np.sqrt(self.cc * (2 - self.cc) * self.mueff) / self.sigma * (selected_weighted - mean)\n            \n            cov_update = np.dot(selected.T, np.dot(np.diag(self.weights), selected)) - np.outer(mean, mean)\n            covariance_matrix = (1 - self.c1 - self.cmu) * covariance_matrix + self.c1 * (np.outer(path_c, path_c) + (1 - hsig) * self.cc * (2 - self.cc) * covariance_matrix) + self.cmu * cov_update\n            \n            self.sigma *= np.exp((self.csigma / self.dsigma) * (np.linalg.norm(path_sigma) / E_norm - 1))\n            \n            if evaluations >= self.budget:\n                break\n            \n            if evaluations % (10 * self.population_size) == 0:\n                eigen_values, eigen_decomposition = np.linalg.eigh(covariance_matrix)\n        \n        best_index = np.argmin(fitness)\n        return samples[best_index], fitness[best_index]", "name": "Quantum_Enhanced_CMA_ES", "description": "Quantum-Enhanced Covariance Matrix Adaptation Evolution Strategy (QECMA-ES) integrates quantum-inspired sampling with dynamic covariance adaptation for efficient navigation of complex search spaces.", "configspace": "", "generation": 59, "fitness": 0.2216339986224085, "feedback": "The algorithm Quantum_Enhanced_CMA_ES got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.22 with standard deviation 0.00.", "error": "", "parent_id": "9f02530a-e93d-4297-bc22-70355754d2c1", "metadata": {"aucs": [0.2216339986224085, 0.2216339986224085, 0.2216339986224085]}, "mutation_prompt": null}
{"id": "e645adf5-0510-4d1b-9415-cb2ba274741a", "solution": "import numpy as np\n\nclass Quantum_Inspired_Genetic_Optimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.mutation_rate = 0.1\n        self.crossover_rate = 0.8\n        self.q_bits = np.random.uniform(-np.pi/4, np.pi/4, (self.population_size, self.dim))\n        self.beta = 0.5  # Learning rate for quantum rotation\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        np.random.seed(42)\n\n        def decode(q_bits):\n            return lb + (ub - lb) * (np.sin(q_bits) ** 2)\n\n        def fitness(individual):\n            return func(decode(individual))\n\n        fitness_values = np.array([fitness(q) for q in self.q_bits])\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            # Selection\n            sorted_indices = np.argsort(fitness_values)\n            self.q_bits = self.q_bits[sorted_indices]\n            fitness_values = fitness_values[sorted_indices]\n\n            new_q_bits = np.copy(self.q_bits)\n\n            # Crossover\n            for i in range(0, self.population_size, 2):\n                if np.random.rand() < self.crossover_rate:\n                    idx1, idx2 = i, i + 1\n                    point = np.random.randint(1, self.dim)\n                    new_q_bits[idx1, point:], new_q_bits[idx2, point:] = (\n                        new_q_bits[idx2, point:].copy(),\n                        new_q_bits[idx1, point:].copy(),\n                    )\n\n            # Mutation\n            for i in range(self.population_size):\n                if np.random.rand() < self.mutation_rate:\n                    mutation_idx = np.random.randint(self.dim)\n                    new_q_bits[i, mutation_idx] += self.beta * np.random.normal()\n\n            # Evaluation\n            fitness_values = np.array([fitness(q) for q in new_q_bits])\n            evaluations += self.population_size\n\n            if evaluations >= self.budget:\n                break\n\n            # Update quantum bits based on selection\n            for i in range(self.population_size):\n                if fitness_values[i] < fitness(decode(self.q_bits[i])):\n                    self.q_bits[i] = new_q_bits[i]\n\n        best_index = np.argmin(fitness_values)\n        best_q_bits = self.q_bits[best_index]\n        best_position = decode(best_q_bits)\n        best_value = fitness(best_q_bits)\n\n        return best_position, best_value", "name": "Quantum_Inspired_Genetic_Optimizer", "description": "Quantum-Inspired Genetic Algorithm utilizes quantum-based encoding and genetic operators with diversity preservation to enhance exploration and exploitation balance in complex optimization landscapes.", "configspace": "", "generation": 60, "fitness": 0.2513484021895457, "feedback": "The algorithm Quantum_Inspired_Genetic_Optimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.25 with standard deviation 0.01.", "error": "", "parent_id": "9f02530a-e93d-4297-bc22-70355754d2c1", "metadata": {"aucs": [0.2514538327513345, 0.2594035774056226, 0.24318779641167998]}, "mutation_prompt": null}
{"id": "23527f91-0839-4554-aae4-a24c49e9c312", "solution": "import numpy as np\n\nclass Enhanced_Adaptive_Quantum_Pso_Annealing_Gauss_Optimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 25  # Increased for better exploration\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.w = 0.7\n        self.q_factor = 0.9\n        self.gaussian_scale = 0.1\n        self.initial_reset_chance = 0.05  # Slightly reduced to focus on convergence\n        self.momentum_factor = 1.05\n        self.adaptive_rate = 0.99  # Slower decrease of inertia weight\n        self.temperature = 1.0\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        np.random.seed(42)\n\n        position = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocity = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_position = np.copy(position)\n        personal_best_value = np.array([func(p) for p in personal_best_position])\n        global_best_position = personal_best_position[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n        \n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            neighborhood_size = int(self.population_size / 2)  # Dynamic restructuring\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(2)\n                self.w *= self.adaptive_rate\n                velocity[i] = (self.w * velocity[i] +\n                               self.c1 * r1 * (personal_best_position[i] - position[i]) +\n                               self.c2 * r2 * (global_best_position - position[i]))\n                \n                adaptive_gaussian_scale = self.gaussian_scale * (1 - evaluations / self.budget)\n                annealing_factor = np.exp(-evaluations / (self.budget * self.temperature))\n                \n                position[i] += (velocity[i] + \n                                self.q_factor * np.random.normal(scale=adaptive_gaussian_scale, size=self.dim) +\n                                annealing_factor * np.random.uniform(-1, 1, self.dim))\n                position[i] = np.clip(position[i], lb, ub)\n\n                current_value = func(position[i])\n                evaluations += 1\n                \n                if current_value < personal_best_value[i]:\n                    personal_best_position[i] = position[i]\n                    personal_best_value[i] = current_value\n                \n                if current_value < global_best_value:\n                    global_best_position = position[i]\n                    global_best_value = current_value\n\n                if evaluations >= self.budget:\n                    break\n\n            # Adaptive Reset Mechanism based on stagnation\n            reset_chance = self.initial_reset_chance * ((evaluations / self.budget) ** self.momentum_factor)\n            if np.random.rand() < reset_chance:\n                random_index = np.random.randint(self.population_size)\n                position[random_index] = np.random.uniform(lb, ub, self.dim)\n                current_value = func(position[random_index])\n                evaluations += 1\n                \n                if current_value < personal_best_value[random_index]:\n                    personal_best_position[random_index] = position[random_index]\n                    personal_best_value[random_index] = current_value\n                \n                if current_value < global_best_value:\n                    global_best_position = position[random_index]\n                    global_best_value = current_value\n\n            # Dynamic Neighborhood Restructuring\n            if evaluations % 10 == 0:  # Adjust neighborhood size dynamically\n                neighborhood_size = max(1, neighborhood_size - 1)\n\n        return global_best_position, global_best_value", "name": "Enhanced_Adaptive_Quantum_Pso_Annealing_Gauss_Optimizer", "description": "Enhanced Particle Swarm Optimizer integrates Quantum-inspired Adaptive Exploration and Dynamic Neighborhood Restructuring for robust global search and convergence.", "configspace": "", "generation": 61, "fitness": 0.27811919428135756, "feedback": "The algorithm Enhanced_Adaptive_Quantum_Pso_Annealing_Gauss_Optimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28 with standard deviation 0.00.", "error": "", "parent_id": "9f02530a-e93d-4297-bc22-70355754d2c1", "metadata": {"aucs": [0.27811919428135756, 0.27811919428135756, 0.27811919428135756]}, "mutation_prompt": null}
{"id": "c64265c0-281b-46aa-81cf-a737cbce3a78", "solution": "import numpy as np\n\nclass Advanced_Quantum_PSO_Adaptive_Restart:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 30  # Increased size for diversity\n        self.c1 = 2.1\n        self.c2 = 2.1\n        self.w_max = 0.9\n        self.w_min = 0.4\n        self.q_factor = 0.85\n        self.gaussian_scale = 0.15\n        self.reset_probability = 0.1\n        self.temperature = 1.0\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        np.random.seed(42)\n\n        position = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocity = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_position = np.copy(position)\n        personal_best_value = np.array([func(p) for p in personal_best_position])\n        global_best_position = personal_best_position[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            inertia_weight = self.w_max - (self.w_max - self.w_min) * (evaluations / self.budget)\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(2)\n                velocity[i] = (inertia_weight * velocity[i] +\n                               self.c1 * r1 * (personal_best_position[i] - position[i]) +\n                               self.c2 * r2 * (global_best_position - position[i]))\n\n                adaptive_gaussian_scale = self.gaussian_scale * (1 - evaluations / self.budget)\n                annealing_factor = np.exp(-evaluations / (self.budget * self.temperature))\n\n                position[i] += (velocity[i] +\n                                self.q_factor * np.random.normal(scale=adaptive_gaussian_scale, size=self.dim) +\n                                annealing_factor * np.random.uniform(-1, 1, self.dim))\n                position[i] = np.clip(position[i], lb, ub)\n\n                current_value = func(position[i])\n                evaluations += 1\n\n                if current_value < personal_best_value[i]:\n                    personal_best_position[i] = position[i]\n                    personal_best_value[i] = current_value\n\n                if current_value < global_best_value:\n                    global_best_position = position[i]\n                    global_best_value = current_value\n\n                if evaluations >= self.budget:\n                    break\n\n            # Adaptive Reset Mechanism based on exploration needs\n            if np.random.rand() < self.reset_probability * (1 - evaluations / self.budget):\n                random_index = np.random.randint(self.population_size)\n                position[random_index] = np.random.uniform(lb, ub, self.dim)\n                current_value = func(position[random_index])\n                evaluations += 1\n\n                if current_value < personal_best_value[random_index]:\n                    personal_best_position[random_index] = position[random_index]\n                    personal_best_value[random_index] = current_value\n\n                if current_value < global_best_value:\n                    global_best_position = position[random_index]\n                    global_best_value = current_value\n\n        return global_best_position, global_best_value", "name": "Advanced_Quantum_PSO_Adaptive_Restart", "description": "Advanced Quantum PSO with Adaptive Inertia and Restart Mechanism for Enhanced Exploration and Convergence.", "configspace": "", "generation": 62, "fitness": 0.27423356552331934, "feedback": "The algorithm Advanced_Quantum_PSO_Adaptive_Restart got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.27 with standard deviation 0.00.", "error": "", "parent_id": "23527f91-0839-4554-aae4-a24c49e9c312", "metadata": {"aucs": [0.27423356552331934, 0.27423356552331934, 0.27423356552331934]}, "mutation_prompt": null}
{"id": "c0cba824-6720-44ed-aa31-c97e00edba3f", "solution": "import numpy as np\n\nclass Quantum_Guided_Adaptive_Swarm_Optimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 30  # Increased for better exploration\n        self.c1 = 1.5\n        self.c2 = 1.5\n        self.w = 0.9  # High initial inertia for broader exploration\n        self.q_factor = 0.8  # Quantum diversity factor\n        self.gaussian_scale = 0.2  # Higher for initial exploration\n        self.reset_chance = 0.02  # Initial lower reset chance\n        self.momentum_factor = 1.2\n        self.adaptive_rate = 0.95  # Moderate decrease of inertia weight\n        self.temperature = 0.5  # Lower temperature for faster convergence\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        np.random.seed(42)\n\n        position = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocity = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_position = np.copy(position)\n        personal_best_value = np.array([func(p) for p in personal_best_position])\n        global_best_position = personal_best_position[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n        \n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(2)\n                self.w *= self.adaptive_rate\n                velocity[i] = (self.w * velocity[i] +\n                               self.c1 * r1 * (personal_best_position[i] - position[i]) +\n                               self.c2 * r2 * (global_best_position - position[i]))\n                \n                adaptive_gaussian_scale = self.gaussian_scale * (1 - evaluations / self.budget)\n                annealing_factor = np.exp(-evaluations / (self.budget * self.temperature))\n                \n                position[i] += (velocity[i] + \n                                self.q_factor * np.random.normal(scale=adaptive_gaussian_scale, size=self.dim) +\n                                annealing_factor * np.random.uniform(-1, 1, self.dim))\n                position[i] = np.clip(position[i], lb, ub)\n\n                current_value = func(position[i])\n                evaluations += 1\n                \n                if current_value < personal_best_value[i]:\n                    personal_best_position[i] = position[i]\n                    personal_best_value[i] = current_value\n                \n                if current_value < global_best_value:\n                    global_best_position = position[i]\n                    global_best_value = current_value\n\n                if evaluations >= self.budget:\n                    break\n\n            # Stochastic Reset Mechanism to escape local optima\n            reset_chance = self.reset_chance * ((evaluations / self.budget) ** self.momentum_factor)\n            if np.random.rand() < reset_chance:\n                random_index = np.random.randint(self.population_size)\n                position[random_index] = np.random.uniform(lb, ub, self.dim)\n                current_value = func(position[random_index])\n                evaluations += 1\n                \n                if current_value < personal_best_value[random_index]:\n                    personal_best_position[random_index] = position[random_index]\n                    personal_best_value[random_index] = current_value\n                \n                if current_value < global_best_value:\n                    global_best_position = position[random_index]\n                    global_best_value = current_value\n\n        return global_best_position, global_best_value", "name": "Quantum_Guided_Adaptive_Swarm_Optimizer", "description": "Quantum-Guided Adaptive Swarm Optimizer enhances PSO with quantum diversity, stochastic reset, and self-adjusting parameters for efficient global convergence.", "configspace": "", "generation": 63, "fitness": 0.2780934175972336, "feedback": "The algorithm Quantum_Guided_Adaptive_Swarm_Optimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28 with standard deviation 0.00.", "error": "", "parent_id": "23527f91-0839-4554-aae4-a24c49e9c312", "metadata": {"aucs": [0.2780934175972336, 0.2780934175972336, 0.2780934175972336]}, "mutation_prompt": null}
{"id": "caf5d22d-59bc-4ecc-9b63-c66620159b74", "solution": "import numpy as np\n\nclass Dynamic_Harmony_Search_Optimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.harmony_memory_size = 30\n        self.harmony_memory_consideration_rate = 0.95\n        self.pitch_adjustment_rate = 0.7\n        self.bandwidth = 0.10\n        self.adaptive_threshold = 0.1\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        np.random.seed(42)\n\n        harmony_memory = np.random.uniform(lb, ub, (self.harmony_memory_size, self.dim))\n        harmony_values = np.array([func(h) for h in harmony_memory])\n        best_harmony = harmony_memory[np.argmin(harmony_values)]\n        best_value = np.min(harmony_values)\n        \n        evaluations = self.harmony_memory_size\n\n        while evaluations < self.budget:\n            new_harmony = np.empty(self.dim)\n            for i in range(self.dim):\n                if np.random.rand() < self.harmony_memory_consideration_rate:\n                    new_harmony[i] = harmony_memory[np.random.randint(self.harmony_memory_size), i]\n                    if np.random.rand() < self.pitch_adjustment_rate:\n                        new_harmony[i] += np.random.uniform(-1, 1) * self.bandwidth\n                else:\n                    new_harmony[i] = np.random.uniform(lb[i], ub[i])\n                    \n            new_harmony = np.clip(new_harmony, lb, ub)\n            new_value = func(new_harmony)\n            evaluations += 1\n\n            if new_value < best_value:\n                best_harmony, best_value = new_harmony, new_value\n            \n            worst_index = np.argmax(harmony_values)\n            if new_value < harmony_values[worst_index]:\n                harmony_memory[worst_index] = new_harmony\n                harmony_values[worst_index] = new_value\n\n            # Adaptive Bandwidth and Threshold Adjustment\n            self.bandwidth *= (1 - evaluations / self.budget)\n            current_threshold = self.adaptive_threshold * (1 - evaluations / self.budget)\n            if np.abs(harmony_values[worst_index] - new_value) < current_threshold:\n                continue\n\n        return best_harmony, best_value", "name": "Dynamic_Harmony_Search_Optimizer", "description": "Nature-inspired Harmony Search integrates Dynamic Convergence Thresholding and Adaptive Bandwidth for efficient exploration and exploitation in black-box optimization.", "configspace": "", "generation": 64, "fitness": 0.2765660861536211, "feedback": "The algorithm Dynamic_Harmony_Search_Optimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28 with standard deviation 0.00.", "error": "", "parent_id": "23527f91-0839-4554-aae4-a24c49e9c312", "metadata": {"aucs": [0.2765660861536211, 0.2765660861536211, 0.2765660861536211]}, "mutation_prompt": null}
{"id": "4f220e9c-4583-4186-b417-7de553462a4f", "solution": "import numpy as np\n\nclass Quantum_Triangular_Mutation_Adaptive_PSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 30\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.w = 0.9\n        self.q_factor = 0.9\n        self.triangular_scale = 0.1  # Scale for triangular distribution\n        self.inertia_adaptive_rate = 0.98  # More dynamic inertia weight adaptation\n        self.temperature = 1.0\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        np.random.seed(42)\n\n        position = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocity = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_position = np.copy(position)\n        personal_best_value = np.array([func(p) for p in personal_best_position])\n        global_best_position = personal_best_position[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n        \n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(2)\n                self.w *= self.inertia_adaptive_rate\n                velocity[i] = (self.w * velocity[i] +\n                               self.c1 * r1 * (personal_best_position[i] - position[i]) +\n                               self.c2 * r2 * (global_best_position - position[i]))\n                \n                triangular_noise = np.random.triangular(-self.triangular_scale, 0, self.triangular_scale, self.dim)\n                annealing_factor = np.exp(-evaluations / (self.budget * self.temperature))\n                \n                position[i] += (velocity[i] + \n                                self.q_factor * triangular_noise +\n                                annealing_factor * np.random.uniform(-1, 1, self.dim))\n                position[i] = np.clip(position[i], lb, ub)\n\n                current_value = func(position[i])\n                evaluations += 1\n                \n                if current_value < personal_best_value[i]:\n                    personal_best_position[i] = position[i]\n                    personal_best_value[i] = current_value\n                \n                if current_value < global_best_value:\n                    global_best_position = position[i]\n                    global_best_value = current_value\n\n                if evaluations >= self.budget:\n                    break\n\n            # Adaptive Reset Mechanism based on stagnation\n            if evaluations % (self.population_size // 2) == 0:\n                random_indices = np.random.choice(self.population_size, 3, replace=False)\n                for idx in random_indices:\n                    position[idx] = np.random.uniform(lb, ub, self.dim)\n                    current_value = func(position[idx])\n                    evaluations += 1\n                    \n                    if current_value < personal_best_value[idx]:\n                        personal_best_position[idx] = position[idx]\n                        personal_best_value[idx] = current_value\n                    \n                    if current_value < global_best_value:\n                        global_best_position = position[idx]\n                        global_best_value = current_value\n\n        return global_best_position, global_best_value", "name": "Quantum_Triangular_Mutation_Adaptive_PSO", "description": "Quantum-Inspired Particle Swarm Optimizer with Triangular Mutation and Adaptive Inertia for improved exploration and convergence in complex search spaces.", "configspace": "", "generation": 65, "fitness": 0.27797882436781585, "feedback": "The algorithm Quantum_Triangular_Mutation_Adaptive_PSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28 with standard deviation 0.00.", "error": "", "parent_id": "23527f91-0839-4554-aae4-a24c49e9c312", "metadata": {"aucs": [0.27797882436781585, 0.27797882436781585, 0.27797882436781585]}, "mutation_prompt": null}
{"id": "c0fdb1b1-d555-4987-8fd1-762d6aefcc9a", "solution": "import numpy as np\n\nclass Quantum_Adaptive_PSO_with_Iterative_Learning:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 30  # Further increased for diverse exploration\n        self.c1 = 1.5\n        self.c2 = 2.5\n        self.w = 0.8\n        self.q_factor = 0.95\n        self.gaussian_scale = 0.05  # Reduced for finer local exploration\n        self.reset_chance = 0.1\n        self.momentum_factor = 1.1\n        self.adaptive_rate = 0.98\n        self.temperature = 1.0\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        np.random.seed(42)\n        \n        position = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocity = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_position = np.copy(position)\n        personal_best_value = np.array([func(p) for p in personal_best_position])\n        global_best_position = personal_best_position[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n        \n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(2)\n                self.w *= self.adaptive_rate\n                velocity[i] = (self.w * velocity[i] +\n                               self.c1 * r1 * (personal_best_position[i] - position[i]) +\n                               self.c2 * r2 * (global_best_position - position[i]))\n                \n                adaptive_gaussian_scale = self.gaussian_scale * (1 - evaluations / self.budget)\n                annealing_factor = np.exp(-evaluations / (self.budget * self.temperature))\n                \n                position[i] += (velocity[i] +\n                                self.q_factor * np.random.normal(scale=adaptive_gaussian_scale, size=self.dim) +\n                                annealing_factor * np.random.uniform(-1, 1, self.dim))\n                position[i] = np.clip(position[i], lb, ub)\n\n                current_value = func(position[i])\n                evaluations += 1\n                \n                if current_value < personal_best_value[i]:\n                    personal_best_position[i] = position[i]\n                    personal_best_value[i] = current_value\n                \n                if current_value < global_best_value:\n                    global_best_position = position[i]\n                    global_best_value = current_value\n\n                if evaluations >= self.budget:\n                    break\n\n            # Momentum-Controlled Restart to escape local optima\n            if evaluations % int(self.budget / 10) == 0:\n                sorted_indices = np.argsort(personal_best_value)\n                for j in range(self.population_size // 3):\n                    idx = sorted_indices[-(j+1)]\n                    position[idx] = np.random.uniform(lb, ub, self.dim)\n                    current_value = func(position[idx])\n                    evaluations += 1\n                    \n                    if current_value < personal_best_value[idx]:\n                        personal_best_position[idx] = position[idx]\n                        personal_best_value[idx] = current_value\n                    \n                    if current_value < global_best_value:\n                        global_best_position = position[idx]\n                        global_best_value = current_value\n\n        return global_best_position, global_best_value", "name": "Quantum_Adaptive_PSO_with_Iterative_Learning", "description": "A Quantum-inspired Adaptive PSO with Enhanced Convergence through Iterative Learning and Momentum-Controlled Restart for effective exploration-exploitation balance.", "configspace": "", "generation": 66, "fitness": 0.2781010011338808, "feedback": "The algorithm Quantum_Adaptive_PSO_with_Iterative_Learning got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28 with standard deviation 0.00.", "error": "", "parent_id": "23527f91-0839-4554-aae4-a24c49e9c312", "metadata": {"aucs": [0.2781010011338808, 0.2781010011338808, 0.2781010011338808]}, "mutation_prompt": null}
{"id": "03ab1ee6-155c-4462-906e-763bd7ed14f4", "solution": "import numpy as np\n\nclass Hybrid_Quantum_Adaptive_Pso_Stochastic_Restarts:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 30  # Further increased for diverse exploration\n        self.c1 = 1.5  # Adjusted cognitive and social factors\n        self.c2 = 2.5\n        self.w = 0.9  # Increased initial inertia for better exploration\n        self.w_min = 0.4  # Minimum inertia weight for better convergence\n        self.alpha = 0.1  # Coefficient for stochastic restarts\n        self.q_factor = 0.8\n        self.temperature = 1.0  # Initial temperature for simulated annealing\n        self.decrement_factor = 0.95  # Temperature decrement factor\n        self.random_seed = 42\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        np.random.seed(self.random_seed)\n\n        position = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocity = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_position = np.copy(position)\n        personal_best_value = np.array([func(p) for p in personal_best_position])\n        global_best_position = personal_best_position[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(2)\n                self.w = self.w_max - (self.w_max - self.w_min) * (evaluations / self.budget)\n                velocity[i] = (self.w * velocity[i] +\n                               self.c1 * r1 * (personal_best_position[i] - position[i]) +\n                               self.c2 * r2 * (global_best_position - position[i]))\n                \n                position[i] += velocity[i] + self.q_factor * np.random.normal(size=self.dim)\n                position[i] = np.clip(position[i], lb, ub)\n\n                current_value = func(position[i])\n                evaluations += 1\n\n                if current_value < personal_best_value[i]:\n                    personal_best_position[i] = position[i]\n                    personal_best_value[i] = current_value\n\n                if current_value < global_best_value:\n                    global_best_position = position[i]\n                    global_best_value = current_value\n\n                if evaluations >= self.budget:\n                    break\n\n            self.temperature *= self.decrement_factor  # Simulated annealing effect\n\n            # Stochastic Restart Mechanism\n            if np.random.rand() < self.alpha:\n                random_index = np.random.randint(self.population_size)\n                position[random_index] = np.random.uniform(lb, ub, self.dim)\n                current_value = func(position[random_index])\n                evaluations += 1\n\n                if current_value < personal_best_value[random_index]:\n                    personal_best_position[random_index] = position[random_index]\n                    personal_best_value[random_index] = current_value\n\n                if current_value < global_best_value:\n                    global_best_position = position[random_index]\n                    global_best_value = current_value\n\n        return global_best_position, global_best_value", "name": "Hybrid_Quantum_Adaptive_Pso_Stochastic_Restarts", "description": "Hybrid Quantum-Enhanced Adaptive Particle Swarm Optimizer with Time-Varying Parameters and Stochastic Restarts for improved exploration-exploitation balance.", "configspace": "", "generation": 67, "fitness": -Infinity, "feedback": "An exception occurred: AttributeError(\"'Hybrid_Quantum_Adaptive_Pso_Stochastic_Restarts' object has no attribute 'w_max'\").", "error": "AttributeError(\"'Hybrid_Quantum_Adaptive_Pso_Stochastic_Restarts' object has no attribute 'w_max'\")", "parent_id": "23527f91-0839-4554-aae4-a24c49e9c312", "metadata": {}, "mutation_prompt": null}
{"id": "dee7334b-f1a0-4e5c-bc74-86007428a9b1", "solution": "import numpy as np\n\nclass Quantum_Adaptive_Mutation_Gradient_Pso_Optimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 30  # Increased for more diversity\n        self.c1 = 1.5\n        self.c2 = 1.5\n        self.w = 0.6\n        self.q_factor = 0.95\n        self.gaussian_scale = 0.1\n        self.mutation_rate = 0.2  # Introduced for adaptive mutation\n        self.mutation_decay = 0.995  # Decay rate of mutation impact\n        self.temperature = 1.0\n        self.gradient_step = 0.01  # Step size for gradient-based exploration\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        np.random.seed(42)\n\n        position = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocity = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_position = np.copy(position)\n        personal_best_value = np.array([func(p) for p in personal_best_position])\n        global_best_position = personal_best_position[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n        \n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(2)\n                self.w *= self.mutation_decay\n                velocity[i] = (self.w * velocity[i] +\n                               self.c1 * r1 * (personal_best_position[i] - position[i]) +\n                               self.c2 * r2 * (global_best_position - position[i]))\n                \n                adaptive_gaussian_scale = self.gaussian_scale * (1 - evaluations / self.budget)\n                annealing_factor = np.exp(-evaluations / (self.budget * self.temperature))\n                \n                position[i] += (velocity[i] +\n                                self.q_factor * np.random.normal(scale=adaptive_gaussian_scale, size=self.dim) +\n                                annealing_factor * np.random.uniform(-1, 1, self.dim))\n\n                # Apply adaptive mutation\n                if np.random.rand() < self.mutation_rate:\n                    mutation_vector = np.random.normal(scale=adaptive_gaussian_scale, size=self.dim)\n                    position[i] += mutation_vector\n                    self.mutation_rate *= self.mutation_decay\n\n                # Gradient-based exploration (simple numerical gradient approximation)\n                gradient = np.zeros(self.dim)\n                eps = 1e-6\n                for d in range(self.dim):\n                    temp_pos = np.copy(position[i])\n                    temp_pos[d] += eps\n                    gradient[d] = (func(temp_pos) - func(position[i])) / eps\n\n                position[i] -= self.gradient_step * gradient\n                position[i] = np.clip(position[i], lb, ub)\n\n                current_value = func(position[i])\n                evaluations += 1\n                \n                if current_value < personal_best_value[i]:\n                    personal_best_position[i] = position[i]\n                    personal_best_value[i] = current_value\n                \n                if current_value < global_best_value:\n                    global_best_position = position[i]\n                    global_best_value = current_value\n\n                if evaluations >= self.budget:\n                    break\n\n        return global_best_position, global_best_value", "name": "Quantum_Adaptive_Mutation_Gradient_Pso_Optimizer", "description": "Quantum-Inspired Hybrid PSO with Adaptive Mutation and Gradient-Guided Search for Enhanced Convergence in High-Dimensional Spaces.", "configspace": "", "generation": 68, "fitness": 0.2743285931671632, "feedback": "The algorithm Quantum_Adaptive_Mutation_Gradient_Pso_Optimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.27 with standard deviation 0.00.", "error": "", "parent_id": "23527f91-0839-4554-aae4-a24c49e9c312", "metadata": {"aucs": [0.2743285931671632, 0.2743285931671632, 0.2743285931671632]}, "mutation_prompt": null}
{"id": "2218b12e-9576-4e41-95a1-4134bd4f41a8", "solution": "import numpy as np\n\nclass Quantum_Dynamic_Exploration_PSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 30  # Slightly increased for more samples\n        self.c1 = 1.5             # Adjusted for balanced exploration-exploitation\n        self.c2 = 1.5\n        self.w = 0.6              # Lower starting inertia weight\n        self.q_factor = 0.85\n        self.gaussian_scale = 0.15\n        self.initial_reset_chance = 0.02\n        self.momentum_factor = 1.2\n        self.adaptive_rate = 0.995\n        self.temperature = 0.9    # Reduced for faster cooling\n        self.velocity_clamp = 0.5 # New: clamp velocity to control exploration\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        np.random.seed(42)\n\n        position = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocity = np.random.uniform(-self.velocity_clamp, self.velocity_clamp, (self.population_size, self.dim))\n        personal_best_position = np.copy(position)\n        personal_best_value = np.array([func(p) for p in personal_best_position])\n        global_best_position = personal_best_position[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n        \n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            neighborhood_size = int(self.population_size / 3)  # Tighter neighborhood\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(2)\n                self.w *= self.adaptive_rate\n                velocity[i] = (self.w * velocity[i] +\n                               self.c1 * r1 * (personal_best_position[i] - position[i]) +\n                               self.c2 * r2 * (global_best_position - position[i]))\n                velocity[i] = np.clip(velocity[i], -self.velocity_clamp, self.velocity_clamp)\n\n                adaptive_gaussian_scale = self.gaussian_scale * (1 - evaluations / self.budget)\n                annealing_factor = np.exp(-evaluations / (self.budget * self.temperature))\n                \n                position[i] += (velocity[i] + \n                                self.q_factor * np.random.normal(scale=adaptive_gaussian_scale, size=self.dim) +\n                                annealing_factor * np.random.uniform(-1, 1, self.dim))\n                position[i] = np.clip(position[i], lb, ub)\n\n                current_value = func(position[i])\n                evaluations += 1\n                \n                if current_value < personal_best_value[i]:\n                    personal_best_position[i] = position[i]\n                    personal_best_value[i] = current_value\n                \n                if current_value < global_best_value:\n                    global_best_position = position[i]\n                    global_best_value = current_value\n\n                if evaluations >= self.budget:\n                    break\n\n            # Adaptive Reset Mechanism based on stagnation\n            reset_chance = self.initial_reset_chance * ((evaluations / self.budget) ** self.momentum_factor)\n            if np.random.rand() < reset_chance:\n                random_index = np.random.randint(self.population_size)\n                position[random_index] = np.random.uniform(lb, ub, self.dim)\n                current_value = func(position[random_index])\n                evaluations += 1\n                \n                if current_value < personal_best_value[random_index]:\n                    personal_best_position[random_index] = position[random_index]\n                    personal_best_value[random_index] = current_value\n                \n                if current_value < global_best_value:\n                    global_best_position = position[random_index]\n                    global_best_value = current_value\n\n            # Dynamic Neighborhood Restructuring\n            if evaluations % 15 == 0:  # Adjust neighborhood size dynamically\n                neighborhood_size = max(1, neighborhood_size - 1)\n\n        return global_best_position, global_best_value", "name": "Quantum_Dynamic_Exploration_PSO", "description": "Quantum-inspired Particle Swarm Optimizer with Dynamic Exploration leveraging Momentum and Adaptive Cooling for enhanced convergence in complex landscapes.", "configspace": "", "generation": 69, "fitness": 0.2687055624764101, "feedback": "The algorithm Quantum_Dynamic_Exploration_PSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.27 with standard deviation 0.00.", "error": "", "parent_id": "23527f91-0839-4554-aae4-a24c49e9c312", "metadata": {"aucs": [0.2687055624764101, 0.2687055624764101, 0.2687055624764101]}, "mutation_prompt": null}
{"id": "24783e92-96cc-4c19-9d83-b587ffa479c8", "solution": "import numpy as np\n\nclass Quantum_Enhanced_PSO_Adaptive_Gaussian:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 30  # Increased for more robust exploration\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.w = 0.8\n        self.q_factor = 0.8\n        self.gaussian_scale = 0.15  # Increased scale for initial exploration\n        self.initial_reset_chance = 0.03  # Adjusted for better exploration\n        self.momentum_factor = 1.1\n        self.adaptive_rate = 0.98  # Slightly faster decrease of inertia weight\n        self.temperature = 1.0\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        np.random.seed(42)\n\n        position = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocity = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_position = np.copy(position)\n        personal_best_value = np.array([func(p) for p in personal_best_position])\n        global_best_position = personal_best_position[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n        \n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            neighborhood_size = int(self.population_size / 3)  # Smaller neighborhoods for diversity\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(2)\n                self.w *= self.adaptive_rate\n                velocity[i] = (self.w * velocity[i] +\n                               self.c1 * r1 * (personal_best_position[i] - position[i]) +\n                               self.c2 * r2 * (global_best_position - position[i]))\n                \n                adaptive_gaussian_scale = self.gaussian_scale * np.exp(-evaluations / self.budget)\n                annealing_factor = np.exp(-evaluations / (self.budget * self.temperature))\n                \n                position[i] += (velocity[i] + \n                                self.q_factor * np.random.normal(scale=adaptive_gaussian_scale, size=self.dim) +\n                                annealing_factor * np.random.uniform(-1, 1, self.dim))\n                position[i] = np.clip(position[i], lb, ub)\n\n                current_value = func(position[i])\n                evaluations += 1\n                \n                if current_value < personal_best_value[i]:\n                    personal_best_position[i] = position[i]\n                    personal_best_value[i] = current_value\n                \n                if current_value < global_best_value:\n                    global_best_position = position[i]\n                    global_best_value = current_value\n\n                if evaluations >= self.budget:\n                    break\n\n            # Stagnation-Triggered Reset Mechanism\n            reset_chance = self.initial_reset_chance * ((evaluations / self.budget) ** self.momentum_factor)\n            if np.random.rand() < reset_chance:\n                stagnated_indices = np.random.choice(self.population_size, neighborhood_size, replace=False)\n                for idx in stagnated_indices:\n                    position[idx] = np.random.uniform(lb, ub, self.dim)\n                    current_value = func(position[idx])\n                    evaluations += 1\n                    \n                    if current_value < personal_best_value[idx]:\n                        personal_best_position[idx] = position[idx]\n                        personal_best_value[idx] = current_value\n                    \n                    if current_value < global_best_value:\n                        global_best_position = position[idx]\n                        global_best_value = current_value\n\n            # Dynamic Neighborhood Restructuring\n            if evaluations % 10 == 0 and neighborhood_size > 2:  # Prevent neighborhood size from becoming too small\n                neighborhood_size = max(2, neighborhood_size - 1)\n\n        return global_best_position, global_best_value", "name": "Quantum_Enhanced_PSO_Adaptive_Gaussian", "description": "Quantum-Enhanced Particle Swarm Optimization with Adaptive Gaussian Mutation and Stagnation-Triggered Neighborhood Restructuring for improved global search and convergence.", "configspace": "", "generation": 70, "fitness": 0.27805810627534444, "feedback": "The algorithm Quantum_Enhanced_PSO_Adaptive_Gaussian got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28 with standard deviation 0.00.", "error": "", "parent_id": "23527f91-0839-4554-aae4-a24c49e9c312", "metadata": {"aucs": [0.27805810627534444, 0.27805810627534444, 0.27805810627534444]}, "mutation_prompt": null}
{"id": "40ea4c67-dcba-499d-9a87-0de0ff02c7e4", "solution": "import numpy as np\n\nclass Quantum_Adaptive_Swarm_Optimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 30  # Slightly increased for better exploration\n        self.c1 = 1.5  # Cognitive coefficient\n        self.c2 = 2.0  # Social coefficient\n        self.w = 0.8  # Initial inertia weight\n        self.q_factor = 0.9\n        self.gaussian_scale = 0.05  # Reduced for precision in local search\n        self.adaptive_rate = 0.995  # More gradual decrease of inertia weight\n        self.temperature = 1.0\n        self.elite_rate = 0.1  # Rate to trigger elite local search\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        np.random.seed(42)\n\n        position = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocity = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_position = np.copy(position)\n        personal_best_value = np.array([func(p) for p in personal_best_position])\n        global_best_position = personal_best_position[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n        \n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(2)\n                self.w *= self.adaptive_rate\n                velocity[i] = (self.w * velocity[i] +\n                               self.c1 * r1 * (personal_best_position[i] - position[i]) +\n                               self.c2 * r2 * (global_best_position - position[i]))\n                \n                adaptive_gaussian_scale = self.gaussian_scale * (1 - evaluations / self.budget)\n                annealing_factor = np.exp(-evaluations / (self.budget * self.temperature))\n                \n                position[i] += (velocity[i] + \n                                self.q_factor * np.random.normal(scale=adaptive_gaussian_scale, size=self.dim) +\n                                annealing_factor * np.random.uniform(-1, 1, self.dim))\n                position[i] = np.clip(position[i], lb, ub)\n\n                current_value = func(position[i])\n                evaluations += 1\n                \n                if current_value < personal_best_value[i]:\n                    personal_best_position[i] = position[i]\n                    personal_best_value[i] = current_value\n                \n                if current_value < global_best_value:\n                    global_best_position = position[i]\n                    global_best_value = current_value\n\n                if evaluations >= self.budget:\n                    break\n\n            # Elite Local Search for top performers\n            elite_indices = np.argsort(personal_best_value)[:int(self.elite_rate * self.population_size)]\n            for idx in elite_indices:\n                perturbation = np.random.normal(0, adaptive_gaussian_scale, self.dim)\n                elite_pos = personal_best_position[idx] + perturbation\n                elite_pos = np.clip(elite_pos, lb, ub)\n                elite_value = func(elite_pos)\n                evaluations += 1\n\n                if elite_value < personal_best_value[idx]:\n                    personal_best_position[idx] = elite_pos\n                    personal_best_value[idx] = elite_value\n                \n                if elite_value < global_best_value:\n                    global_best_position = elite_pos\n                    global_best_value = elite_value\n\n                if evaluations >= self.budget:\n                    break\n\n        return global_best_position, global_best_value", "name": "Quantum_Adaptive_Swarm_Optimizer", "description": "Quantum-inspired Swarm Optimizer with Adaptive Inertia, Gaussian Perturbation, and Elite Local Search for refined global convergence.", "configspace": "", "generation": 71, "fitness": 0.27799167150814585, "feedback": "The algorithm Quantum_Adaptive_Swarm_Optimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28 with standard deviation 0.00.", "error": "", "parent_id": "23527f91-0839-4554-aae4-a24c49e9c312", "metadata": {"aucs": [0.27799167150814585, 0.27799167150814585, 0.27799167150814585]}, "mutation_prompt": null}
{"id": "3989f766-2004-4290-9618-889669c7a3be", "solution": "import numpy as np\n\nclass Adaptive_Hybrid_Swarm_Optimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 25\n        self.c1 = 1.5\n        self.c2 = 1.5\n        self.w = 0.6\n        self.q_factor = 0.9\n        self.initial_reset_chance = 0.04\n        self.momentum_factor = 1.05\n        self.adaptive_rate = 0.98\n        self.leap_scale = 0.1\n        \n    def levy_flight(self, size):\n        beta = 1.5\n        sigma = (np.gamma(1 + beta) * np.sin(np.pi * beta / 2) / \n                 (np.gamma((1 + beta) / 2) * beta * 2 ** ((beta - 1) / 2))) ** (1 / beta)\n        u = np.random.randn(size) * sigma\n        v = np.random.randn(size)\n        return u / np.abs(v) ** (1 / beta)\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        np.random.seed(42)\n\n        position = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocity = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_position = np.copy(position)\n        personal_best_value = np.array([func(p) for p in personal_best_position])\n        global_best_position = personal_best_position[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(2)\n                self.w *= self.adaptive_rate\n                velocity[i] = (self.w * velocity[i] +\n                               self.c1 * r1 * (personal_best_position[i] - position[i]) +\n                               self.c2 * r2 * (global_best_position - position[i]))\n\n                position[i] += velocity[i] + self.q_factor * self.levy_flight(self.dim)\n                position[i] = np.clip(position[i], lb, ub)\n\n                current_value = func(position[i])\n                evaluations += 1\n\n                if current_value < personal_best_value[i]:\n                    personal_best_position[i] = position[i]\n                    personal_best_value[i] = current_value\n\n                if current_value < global_best_value:\n                    global_best_position = position[i]\n                    global_best_value = current_value\n\n                if evaluations >= self.budget:\n                    break\n\n            # Greedy Local Selection\n            local_indices = np.random.choice(self.population_size, 2, replace=False)\n            if personal_best_value[local_indices[0]] < personal_best_value[local_indices[1]]:\n                best_local_position = personal_best_position[local_indices[0]]\n            else:\n                best_local_position = personal_best_position[local_indices[1]]\n\n            if np.random.rand() < self.initial_reset_chance * ((evaluations / self.budget) ** self.momentum_factor):\n                random_index = np.random.randint(self.population_size)\n                position[random_index] = best_local_position + np.random.uniform(lb, ub, self.dim) * self.leap_scale\n                position[random_index] = np.clip(position[random_index], lb, ub)\n                current_value = func(position[random_index])\n                evaluations += 1\n\n                if current_value < personal_best_value[random_index]:\n                    personal_best_position[random_index] = position[random_index]\n                    personal_best_value[random_index] = current_value\n\n                if current_value < global_best_value:\n                    global_best_position = position[random_index]\n                    global_best_value = current_value\n\n        return global_best_position, global_best_value", "name": "Adaptive_Hybrid_Swarm_Optimizer", "description": "Adaptive Hybrid Swarm Optimizer combines Quantum Particle Swarm dynamics with Lévy Flight transition and Greedy Local Selection for enhanced exploration and precision in convergence.", "configspace": "", "generation": 72, "fitness": -Infinity, "feedback": "An exception occurred: AttributeError(\"module 'numpy' has no attribute 'gamma'\").", "error": "AttributeError(\"module 'numpy' has no attribute 'gamma'\")", "parent_id": "23527f91-0839-4554-aae4-a24c49e9c312", "metadata": {}, "mutation_prompt": null}
{"id": "bb548e14-2bb9-4dc4-bb7a-dcc9b16c5785", "solution": "import numpy as np\n\nclass Quantum_Pso_Adaptive_Gauss_Optimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 30  # Increased population size for better coverage\n        self.c1 = 1.5  # Reduced cognitive component for better diversification\n        self.c2 = 2.5  # Increased social component to enhance convergence\n        self.w = 0.9  # Higher initial inertia for exploration\n        self.w_min = 0.4  # Minimum inertia weight to avoid excessive exploration\n        self.q_factor = 0.8  # Adjusted quantum factor for balanced exploration\n        self.gaussian_scale = 0.2  # Increased for more significant mutations\n        self.reset_chance = 0.1  # Initial reset chance to maintain diversity\n        self.adaptive_rate = 0.95  # Gradual decrease in inertia weight\n        self.temperature_scaling = 0.5  # New scaling factor for annealing\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        np.random.seed(42)\n\n        position = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocity = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_position = np.copy(position)\n        personal_best_value = np.array([func(p) for p in personal_best_position])\n        global_best_position = personal_best_position[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n        \n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(2)\n                self.w = max(self.w * self.adaptive_rate, self.w_min)  # Adaptive inertia weight\n                velocity[i] = (self.w * velocity[i] +\n                               self.c1 * r1 * (personal_best_position[i] - position[i]) +\n                               self.c2 * r2 * (global_best_position - position[i]))\n\n                adaptive_gaussian_scale = self.gaussian_scale * (1 - evaluations / self.budget)\n                annealing_factor = np.exp(-self.temperature_scaling * evaluations / self.budget)\n                \n                position[i] += (velocity[i] +\n                                self.q_factor * np.random.normal(scale=adaptive_gaussian_scale, size=self.dim) +\n                                annealing_factor * np.random.uniform(-1, 1, self.dim))\n                position[i] = np.clip(position[i], lb, ub)\n\n                current_value = func(position[i])\n                evaluations += 1\n                \n                if current_value < personal_best_value[i]:\n                    personal_best_position[i] = position[i]\n                    personal_best_value[i] = current_value\n                \n                if current_value < global_best_value:\n                    global_best_position = position[i]\n                    global_best_value = current_value\n\n                if evaluations >= self.budget:\n                    break\n\n            # Reset Mechanism to prevent premature convergence\n            if np.random.rand() < self.reset_chance:\n                random_index = np.random.randint(self.population_size)\n                position[random_index] = np.random.uniform(lb, ub, self.dim)\n                current_value = func(position[random_index])\n                evaluations += 1\n                \n                if current_value < personal_best_value[random_index]:\n                    personal_best_position[random_index] = position[random_index]\n                    personal_best_value[random_index] = current_value\n                \n                if current_value < global_best_value:\n                    global_best_position = position[random_index]\n                    global_best_value = current_value\n\n        return global_best_position, global_best_value", "name": "Quantum_Pso_Adaptive_Gauss_Optimizer", "description": "Quantum Particle Swarm Optimization with Adaptive Gaussian Mutation and Dynamic Parameter Tuning for enhanced exploration and convergence.", "configspace": "", "generation": 73, "fitness": 0.27793263166904236, "feedback": "The algorithm Quantum_Pso_Adaptive_Gauss_Optimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28 with standard deviation 0.00.", "error": "", "parent_id": "23527f91-0839-4554-aae4-a24c49e9c312", "metadata": {"aucs": [0.27793263166904236, 0.27793263166904236, 0.27793263166904236]}, "mutation_prompt": null}
{"id": "f4ab2838-bd7e-4809-b1d0-34b859e0cd18", "solution": "import numpy as np\n\nclass Hybrid_Genetic_Simulated_Annealing_Optimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 30\n        self.crossover_rate = 0.8\n        self.mutation_rate = 0.2\n        self.temperature_decay = 0.95\n        self.initial_temperature = 1.0\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        np.random.seed(42)\n\n        # Initialize population\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_fitness = fitness[best_idx]\n        \n        evaluations = self.population_size\n        temperature = self.initial_temperature\n\n        while evaluations < self.budget:\n            # Selection\n            selected_indices = np.random.choice(self.population_size, self.population_size, replace=True, p=self._softmax(-fitness))\n            selected_population = population[selected_indices]\n\n            # Crossover\n            offspring = np.copy(selected_population)\n            for i in range(0, self.population_size, 2):\n                if np.random.rand() < self.crossover_rate:\n                    crossover_point = np.random.randint(1, self.dim)\n                    offspring[i, crossover_point:], offspring[i+1, crossover_point:] = \\\n                        selected_population[i+1, crossover_point:], selected_population[i, crossover_point:]\n\n            # Mutation\n            for i in range(self.population_size):\n                if np.random.rand() < self.mutation_rate:\n                    mutation_vector = np.random.normal(scale=0.1, size=self.dim)\n                    offspring[i] += mutation_vector\n                    offspring[i] = np.clip(offspring[i], lb, ub)\n\n            # Evaluate offspring\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            evaluations += self.population_size\n\n            # Simulated Annealing: Accept new solutions based on temperature\n            for i in range(self.population_size):\n                delta_fitness = offspring_fitness[i] - fitness[selected_indices[i]]\n                if delta_fitness < 0 or np.random.rand() < np.exp(-delta_fitness / temperature):\n                    population[selected_indices[i]] = offspring[i]\n                    fitness[selected_indices[i]] = offspring_fitness[i]\n                    if fitness[selected_indices[i]] < best_fitness:\n                        best_solution = population[selected_indices[i]]\n                        best_fitness = fitness[selected_indices[i]]\n\n            # Update temperature\n            temperature *= self.temperature_decay\n\n        return best_solution, best_fitness\n\n    def _softmax(self, x):\n        e_x = np.exp(x - np.max(x))\n        return e_x / e_x.sum(axis=0)", "name": "Hybrid_Genetic_Simulated_Annealing_Optimizer", "description": "Hybrid Genetic-Simulated Annealing Optimizer combines genetic algorithm crossover and mutation strategies with simulated annealing for enhanced exploration and exploitation.", "configspace": "", "generation": 74, "fitness": 0.26976069014958437, "feedback": "The algorithm Hybrid_Genetic_Simulated_Annealing_Optimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.27 with standard deviation 0.00.", "error": "", "parent_id": "23527f91-0839-4554-aae4-a24c49e9c312", "metadata": {"aucs": [0.26976069014958437, 0.26976069014958437, 0.26976069014958437]}, "mutation_prompt": null}
{"id": "29f0ba27-df1f-4b40-8dac-25025bf554fa", "solution": "import numpy as np\n\nclass Quantum_Annealing_Adaptive_Mutation_Optimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 30  # Larger for better initial exploration\n        self.initial_temperature = 10.0\n        self.cooling_rate = 0.95\n        self.mutation_scale = 0.1\n        self.q_factor = 0.8\n        self.mutation_adaptive_rate = 0.1\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        np.random.seed(24)\n\n        position = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        personal_best_position = np.copy(position)\n        personal_best_value = np.array([func(p) for p in personal_best_position])\n        global_best_position = personal_best_position[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n        \n        evaluations = self.population_size\n        temperature = self.initial_temperature\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                # Quantum-inspired position update\n                quantum_jump = self.q_factor * np.random.uniform(-1, 1, self.dim)\n                new_position = position[i] + quantum_jump\n                new_position = np.clip(new_position, lb, ub)\n\n                current_value = func(new_position)\n                evaluations += 1\n\n                # Simulated Annealing acceptance criteria\n                if current_value < personal_best_value[i] or \\\n                   np.random.rand() < np.exp((personal_best_value[i] - current_value) / temperature):\n                    position[i] = new_position\n                    personal_best_position[i] = new_position\n                    personal_best_value[i] = current_value\n                \n                if current_value < global_best_value:\n                    global_best_position = new_position\n                    global_best_value = current_value\n\n                # Adaptive mutation\n                if np.random.rand() < self.mutation_adaptive_rate:\n                    mutation = self.mutation_scale * np.random.normal(size=self.dim)\n                    mutated_position = position[i] + mutation\n                    mutated_position = np.clip(mutated_position, lb, ub)\n                    \n                    mutated_value = func(mutated_position)\n                    evaluations += 1\n                    \n                    if mutated_value < personal_best_value[i]:\n                        position[i] = mutated_position\n                        personal_best_position[i] = mutated_position\n                        personal_best_value[i] = mutated_value\n                    \n                    if mutated_value < global_best_value:\n                        global_best_position = mutated_position\n                        global_best_value = mutated_value\n\n                if evaluations >= self.budget:\n                    break\n\n            # Cooling schedule\n            temperature *= self.cooling_rate\n\n        return global_best_position, global_best_value", "name": "Quantum_Annealing_Adaptive_Mutation_Optimizer", "description": "Quantum-inspired Simulated Annealing with Adaptive Mutation integrates quantum behavior and dynamic mutation to enhance global optimization and convergence.", "configspace": "", "generation": 75, "fitness": 0.23772267522127688, "feedback": "The algorithm Quantum_Annealing_Adaptive_Mutation_Optimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.24 with standard deviation 0.00.", "error": "", "parent_id": "23527f91-0839-4554-aae4-a24c49e9c312", "metadata": {"aucs": [0.23772267522127688, 0.23772267522127688, 0.23772267522127688]}, "mutation_prompt": null}
{"id": "d2c79fe7-8159-43d9-93ee-720cd77051f0", "solution": "import numpy as np\n\nclass Hybrid_Quantum_Tunneling_Adaptive_Mutation_Pso_Optimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 30  # Increased to enhance exploration\n        self.c1 = 1.5\n        self.c2 = 1.5\n        self.w = 0.7\n        self.quantum_factor = 0.8\n        self.gaussian_scale = 0.1\n        self.initial_reset_chance = 0.05\n        self.temperature = 1.0\n        self.mutation_factor = 0.5\n        self.crossover_prob = 0.9\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        np.random.seed(42)\n\n        position = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocity = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_position = np.copy(position)\n        personal_best_value = np.array([func(p) for p in personal_best_position])\n        global_best_position = personal_best_position[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n        \n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(2)\n                self.w *= 0.99  # Slight adaptive decrease in inertia\n                \n                # Velocity update with quantum tunneling effect\n                velocity[i] = (self.w * velocity[i] +\n                               self.c1 * r1 * (personal_best_position[i] - position[i]) +\n                               self.c2 * r2 * (global_best_position - position[i]) +\n                               self.quantum_factor * np.random.normal(scale=self.gaussian_scale, size=self.dim))\n                \n                # Position update with adaptive differential mutation\n                mutant_vector = position[i] + self.mutation_factor * (personal_best_position[i] - position[i])\n                trial_vector = np.where(np.random.rand(self.dim) < self.crossover_prob, mutant_vector, position[i])\n                position[i] = np.clip(trial_vector + velocity[i], lb, ub)\n\n                current_value = func(position[i])\n                evaluations += 1\n                \n                if current_value < personal_best_value[i]:\n                    personal_best_position[i] = position[i]\n                    personal_best_value[i] = current_value\n                \n                if current_value < global_best_value:\n                    global_best_position = position[i]\n                    global_best_value = current_value\n\n                if evaluations >= self.budget:\n                    break\n\n            # Random search enhancement based on reset chance\n            if np.random.rand() < self.initial_reset_chance * (1 - (evaluations / self.budget)):\n                random_index = np.random.randint(self.population_size)\n                position[random_index] = np.random.uniform(lb, ub, self.dim)\n                current_value = func(position[random_index])\n                evaluations += 1\n                \n                if current_value < personal_best_value[random_index]:\n                    personal_best_position[random_index] = position[random_index]\n                    personal_best_value[random_index] = current_value\n                \n                if current_value < global_best_value:\n                    global_best_position = position[random_index]\n                    global_best_value = current_value\n        \n        return global_best_position, global_best_value", "name": "Hybrid_Quantum_Tunneling_Adaptive_Mutation_Pso_Optimizer", "description": "Hybridized Particle Swarm Optimizer with Quantum Tunneling and Adaptive Differential Mutation for enhanced balance between exploration and exploitation in global optimization.", "configspace": "", "generation": 76, "fitness": 0.2778093614480879, "feedback": "The algorithm Hybrid_Quantum_Tunneling_Adaptive_Mutation_Pso_Optimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28 with standard deviation 0.00.", "error": "", "parent_id": "23527f91-0839-4554-aae4-a24c49e9c312", "metadata": {"aucs": [0.2778093614480879, 0.2778093614480879, 0.2778093614480879]}, "mutation_prompt": null}
{"id": "b6d36f42-9723-4d60-88f3-54159f854908", "solution": "import numpy as np\n\nclass Quantum_Levy_Pso_Optimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 30\n        self.c1 = 1.5\n        self.c2 = 1.5\n        self.w = 0.5\n        self.levy_alpha = 1.5  # Levy flight exponent\n        self.initial_levy_scale = 0.1\n        self.q_factor = 0.8\n        self.adaptive_rate = 0.98\n        self.temperature = 1.0\n        \n    def levy_flight(self, scale, size):\n        u = np.random.normal(0, 1, size) * scale\n        v = np.random.normal(0, 1, size)\n        step = u / (np.abs(v) ** (1/self.levy_alpha))\n        return step\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        np.random.seed(42)\n\n        position = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocity = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_position = np.copy(position)\n        personal_best_value = np.array([func(p) for p in personal_best_position])\n        global_best_position = personal_best_position[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n        \n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(2)\n                self.w *= self.adaptive_rate\n                velocity[i] = (self.w * velocity[i] +\n                               self.c1 * r1 * (personal_best_position[i] - position[i]) +\n                               self.c2 * r2 * (global_best_position - position[i]))\n                \n                adaptive_levy_scale = self.initial_levy_scale * (1 - evaluations / self.budget)\n                annealing_factor = np.exp(-evaluations / (self.budget * self.temperature))\n                \n                position[i] += (velocity[i] + \n                                self.q_factor * self.levy_flight(adaptive_levy_scale, self.dim) +\n                                annealing_factor * np.random.uniform(-1, 1, self.dim))\n                position[i] = np.clip(position[i], lb, ub)\n\n                current_value = func(position[i])\n                evaluations += 1\n                \n                if current_value < personal_best_value[i]:\n                    personal_best_position[i] = position[i]\n                    personal_best_value[i] = current_value\n                \n                if current_value < global_best_value:\n                    global_best_position = position[i]\n                    global_best_value = current_value\n\n                if evaluations >= self.budget:\n                    break\n\n        return global_best_position, global_best_value", "name": "Quantum_Levy_Pso_Optimizer", "description": "Quantum-Inspired Particle Swarm Optimization with Multi-Scale Adaptive Levy Flight for enhanced exploration and convergence in complex search spaces.", "configspace": "", "generation": 77, "fitness": 0.27811418076864936, "feedback": "The algorithm Quantum_Levy_Pso_Optimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28 with standard deviation 0.00.", "error": "", "parent_id": "23527f91-0839-4554-aae4-a24c49e9c312", "metadata": {"aucs": [0.27811418076864936, 0.27811418076864936, 0.27811418076864936]}, "mutation_prompt": null}
{"id": "b7475708-ffb7-4ce7-819a-49296fdb373b", "solution": "import numpy as np\n\nclass Hybrid_GA_Levy_Optimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 30\n        self.crossover_rate = 0.8\n        self.mutation_rate = 0.2\n        self.alpha = 0.01  # Step size for Levy flight\n        self.beta = 1.5  # Parameter for Levy distribution\n        \n    def levy_flight(self):\n        u = np.random.normal(0, 1, self.dim)\n        v = np.random.normal(0, 1, self.dim)\n        step = u / np.power(np.abs(v), 1/self.beta)\n        return self.alpha * step\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        np.random.seed(42)\n\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            # Selection via tournament\n            new_population = []\n            for _ in range(self.population_size):\n                candidates = np.random.choice(self.population_size, 3, replace=False)\n                best_candidate = candidates[np.argmin(fitness[candidates])]\n                new_population.append(population[best_candidate])\n            population = np.array(new_population)\n\n            # Crossover\n            for i in range(0, self.population_size, 2):\n                if np.random.rand() < self.crossover_rate:\n                    crossover_point = np.random.randint(1, self.dim)\n                    population[i, crossover_point:], population[i+1, crossover_point:] = \\\n                    population[i+1, crossover_point:], population[i, crossover_point:]\n\n            # Mutation with Levy flight\n            for i in range(self.population_size):\n                if np.random.rand() < self.mutation_rate:\n                    population[i] += self.levy_flight()\n                    population[i] = np.clip(population[i], lb, ub)\n\n            # Evaluate new population\n            population_fitness = np.array([func(ind) for ind in population])\n            evaluations += self.population_size\n\n            # Elitism: keep the best individuals\n            combined_population = np.vstack((population, new_population))\n            combined_fitness = np.concatenate((population_fitness, fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n\n        best_index = np.argmin(fitness)\n        return population[best_index], fitness[best_index]", "name": "Hybrid_GA_Levy_Optimizer", "description": "Hybrid Evolutionary Algorithm combining Genetic Algorithm with Lévy Flight Mutation for enhanced exploration and exploitation balance.", "configspace": "", "generation": 78, "fitness": 0.24498689188646028, "feedback": "The algorithm Hybrid_GA_Levy_Optimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.24 with standard deviation 0.00.", "error": "", "parent_id": "23527f91-0839-4554-aae4-a24c49e9c312", "metadata": {"aucs": [0.24498689188646028, 0.24498689188646028, 0.24498689188646028]}, "mutation_prompt": null}
{"id": "c5f0e424-2edb-452b-b35d-6a4d7c60ffc4", "solution": "import numpy as np\n\nclass Synergetic_Quantum_Tunneling_Pso_Optimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 30  # Increased population for better exploration\n        self.c1 = 1.5  # Reduced cognitive factor to balance exploration and exploitation\n        self.c2 = 2.5  # Increased social factor to emphasize global best attraction\n        self.w = 0.9  # Start with higher inertia for exploration\n        self.q_factor = 0.8  # Reduced for focused exploration\n        self.gaussian_scale = 0.05  # Reduced for fine-tuned mutations\n        self.initial_reset_chance = 0.05\n        self.momentum_factor = 1.02  # Slight adjustment to improve reset chance increment\n        self.adaptive_rate = 0.995  # Slower inertia reduction for sustained exploration\n        self.initial_temperature = 5.0  # Higher initial temperature for aggressive exploration\n        self.temperature_decay = 0.95  # Decay temperature slower for smoother transitions\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        np.random.seed(42)\n\n        position = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocity = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_position = np.copy(position)\n        personal_best_value = np.array([func(p) for p in personal_best_position])\n        global_best_position = personal_best_position[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n        \n        evaluations = self.population_size\n        temperature = self.initial_temperature\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(2)\n                velocity[i] = (self.w * velocity[i] +\n                               self.c1 * r1 * (personal_best_position[i] - position[i]) +\n                               self.c2 * r2 * (global_best_position - position[i]))\n\n                # Quantum tunneling and temperature influence\n                adaptive_gaussian_scale = self.gaussian_scale * (1 - evaluations / self.budget)\n                position[i] += (velocity[i] + \n                                self.q_factor * np.random.normal(scale=adaptive_gaussian_scale, size=self.dim) +\n                                temperature * np.random.uniform(-1, 1, self.dim))\n                position[i] = np.clip(position[i], lb, ub)\n\n                current_value = func(position[i])\n                evaluations += 1\n                \n                if current_value < personal_best_value[i]:\n                    personal_best_position[i] = position[i]\n                    personal_best_value[i] = current_value\n                \n                if current_value < global_best_value:\n                    global_best_position = position[i]\n                    global_best_value = current_value\n\n                if evaluations >= self.budget:\n                    break\n\n            # Adaptive Reset Mechanism based on stagnation\n            reset_chance = self.initial_reset_chance * ((evaluations / self.budget) ** self.momentum_factor)\n            if np.random.rand() < reset_chance:\n                random_index = np.random.randint(self.population_size)\n                position[random_index] = np.random.uniform(lb, ub, self.dim)\n                current_value = func(position[random_index])\n                evaluations += 1\n                \n                if current_value < personal_best_value[random_index]:\n                    personal_best_position[random_index] = position[random_index]\n                    personal_best_value[random_index] = current_value\n                \n                if current_value < global_best_value:\n                    global_best_position = position[random_index]\n                    global_best_value = current_value\n\n            # Update inertia and temperature\n            self.w *= self.adaptive_rate\n            temperature *= self.temperature_decay\n\n        return global_best_position, global_best_value", "name": "Synergetic_Quantum_Tunneling_Pso_Optimizer", "description": "Synergetic Particle Swarm with Quantum Tunneling and Adaptive Temperature for enhanced exploration and convergence in photonic structure optimization.", "configspace": "", "generation": 79, "fitness": 0.2763552209336745, "feedback": "The algorithm Synergetic_Quantum_Tunneling_Pso_Optimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28 with standard deviation 0.00.", "error": "", "parent_id": "23527f91-0839-4554-aae4-a24c49e9c312", "metadata": {"aucs": [0.2763552209336745, 0.2763552209336745, 0.2763552209336745]}, "mutation_prompt": null}
{"id": "baf0ede1-d286-4cd8-9a5c-e22b855c24d7", "solution": "import numpy as np\n\nclass Quantum_Levy_Adaptive_Pso_Optimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 30  # Increased for wider exploration\n        self.c1 = 1.5\n        self.c2 = 1.5\n        self.w = 0.9  # Start with higher inertia for exploration\n        self.w_min = 0.4  # Minimum inertia for convergence\n        self.q_factor = 0.8\n        self.levy_scale = 0.1\n        self.reset_chance = 0.1\n        self.adaptive_rate = 0.95  # Adaptive reduction rate for inertia weight\n        \n    def levy_flight(self, size):\n        beta = 1.5\n        sigma = (np.math.gamma(1 + beta) * np.sin(np.pi * beta / 2) /\n                 (np.math.gamma((1 + beta) / 2) * beta * 2 ** ((beta - 1) / 2))) ** (1 / beta)\n        u = np.random.normal(0, sigma, size=size)\n        v = np.random.normal(0, 1, size=size)\n        step = u / abs(v) ** (1 / beta)\n        return step\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        np.random.seed(42)\n\n        position = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocity = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_position = np.copy(position)\n        personal_best_value = np.array([func(p) for p in personal_best_position])\n        global_best_position = personal_best_position[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(2)\n                self.w = max(self.w_min, self.w * self.adaptive_rate)  # Adaptive inertia weight\n                velocity[i] = (self.w * velocity[i] +\n                               self.c1 * r1 * (personal_best_position[i] - position[i]) +\n                               self.c2 * r2 * (global_best_position - position[i]))\n\n                levy_step = self.levy_flight(self.dim) * self.levy_scale\n                position[i] += (velocity[i] + \n                                self.q_factor * np.random.normal(scale=(1 - evaluations / self.budget), size=self.dim) +\n                                levy_step)\n                position[i] = np.clip(position[i], lb, ub)\n\n                current_value = func(position[i])\n                evaluations += 1\n\n                if current_value < personal_best_value[i]:\n                    personal_best_position[i] = position[i]\n                    personal_best_value[i] = current_value\n\n                if current_value < global_best_value:\n                    global_best_position = position[i]\n                    global_best_value = current_value\n\n                if evaluations >= self.budget:\n                    break\n\n            # Stochastic Reset Mechanism to escape local optima\n            if np.random.rand() < self.reset_chance:\n                random_index = np.random.randint(self.population_size)\n                position[random_index] = np.random.uniform(lb, ub, self.dim)\n                current_value = func(position[random_index])\n                evaluations += 1\n\n                if current_value < personal_best_value[random_index]:\n                    personal_best_position[random_index] = position[random_index]\n                    personal_best_value[random_index] = current_value\n\n                if current_value < global_best_value:\n                    global_best_position = position[random_index]\n                    global_best_value = current_value\n\n        return global_best_position, global_best_value", "name": "Quantum_Levy_Adaptive_Pso_Optimizer", "description": "Quantum-Inspired Swarm with Stochastic Levy Flight and Adaptive Hyper-parameters for Enhanced Exploration and Convergence.", "configspace": "", "generation": 80, "fitness": 0.2780970665398418, "feedback": "The algorithm Quantum_Levy_Adaptive_Pso_Optimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28 with standard deviation 0.00.", "error": "", "parent_id": "23527f91-0839-4554-aae4-a24c49e9c312", "metadata": {"aucs": [0.2780970665398418, 0.2780970665398418, 0.2780970665398418]}, "mutation_prompt": null}
{"id": "871d627f-6efe-4e3d-9220-f9692889a8b6", "solution": "import numpy as np\n\nclass Hybrid_GA_DE_SA_Optimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 30  # Suitable for genetic diversity\n        self.crossover_rate = 0.7\n        self.differential_weight = 0.8\n        self.temperature = 1.0\n        self.cooling_rate = 0.995\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        np.random.seed(42)\n        \n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_individual = population[best_idx]\n        best_value = fitness[best_idx]\n        \n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                idxs = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant_vector = a + self.differential_weight * (b - c)\n                mutant_vector = np.clip(mutant_vector, lb, ub)\n                \n                trial_vector = np.where(np.random.rand(self.dim) < self.crossover_rate, mutant_vector, population[i])\n                \n                trial_value = func(trial_vector)\n                evaluations += 1\n                \n                if trial_value < fitness[i]:\n                    population[i] = trial_vector\n                    fitness[i] = trial_value\n                    \n                    if trial_value < best_value:\n                        best_individual = trial_vector\n                        best_value = trial_value\n\n                if evaluations >= self.budget:\n                    break\n\n            # Simulated Annealing component\n            for i in range(self.population_size):\n                perturbed_vector = population[i] + np.random.normal(0, self.temperature, self.dim)\n                perturbed_vector = np.clip(perturbed_vector, lb, ub)\n                perturbed_value = func(perturbed_vector)\n                evaluations += 1\n\n                if perturbed_value < fitness[i] or np.random.rand() < np.exp((fitness[i] - perturbed_value) / self.temperature):\n                    population[i] = perturbed_vector\n                    fitness[i] = perturbed_value\n\n                    if perturbed_value < best_value:\n                        best_individual = perturbed_vector\n                        best_value = perturbed_value\n\n                if evaluations >= self.budget:\n                    break\n        \n            # Update temperature for Simulated Annealing\n            self.temperature *= self.cooling_rate\n\n        return best_individual, best_value", "name": "Hybrid_GA_DE_SA_Optimizer", "description": "Hybrid Genetic Algorithm with Differential Evolution and Simulated Annealing to balance exploration and exploitation in photonic structure optimization.", "configspace": "", "generation": 81, "fitness": 0.26709271181523997, "feedback": "The algorithm Hybrid_GA_DE_SA_Optimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.27 with standard deviation 0.00.", "error": "", "parent_id": "23527f91-0839-4554-aae4-a24c49e9c312", "metadata": {"aucs": [0.26709271181523997, 0.26709271181523997, 0.26709271181523997]}, "mutation_prompt": null}
{"id": "1ce362c4-ef0e-4144-8789-6603c283ac68", "solution": "import numpy as np\n\nclass Dynamic_Multi_Swarm_DE_Optimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_count = 3\n        self.population_size = 20\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.adaptation_factor = 0.99\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        np.random.seed(42)\n\n        swarms = [np.random.uniform(lb, ub, (self.population_size, self.dim)) for _ in range(self.swarm_count)]\n        best_positions = [swarm[np.argmin([func(ind) for ind in swarm])] for swarm in swarms]\n        global_best_position = min(best_positions, key=func)\n        global_best_value = func(global_best_position)\n        \n        evaluations = self.population_size * self.swarm_count\n\n        while evaluations < self.budget:\n            for s in range(self.swarm_count):\n                new_swarm = []\n                for i in range(self.population_size):\n                    a, b, c = swarms[s][np.random.choice(self.population_size, 3, replace=False)]\n                    mutant_vector = np.clip(a + self.F * (b - c), lb, ub)\n                    trial_vector = np.copy(swarms[s][i])\n                    \n                    for j in range(self.dim):\n                        if np.random.rand() < self.CR:\n                            trial_vector[j] = mutant_vector[j]\n                    \n                    trial_value = func(trial_vector)\n                    evaluations += 1\n\n                    if trial_value < func(swarms[s][i]):\n                        new_swarm.append(trial_vector)\n                    else:\n                        new_swarm.append(swarms[s][i])\n\n                    if trial_value < func(best_positions[s]):\n                        best_positions[s] = trial_vector\n\n                    if trial_value < global_best_value:\n                        global_best_position = trial_vector\n                        global_best_value = trial_value\n\n                    if evaluations >= self.budget:\n                        break\n\n                swarms[s] = np.array(new_swarm)\n                self.F *= self.adaptation_factor\n                self.CR *= self.adaptation_factor\n\n                # Adaptive Multi-Swarm Interaction\n                if s < self.swarm_count - 1 and evaluations % 5 == 0:\n                    exchange_ratio = 0.1\n                    exchange_count = int(exchange_ratio * self.population_size)\n                    for _ in range(exchange_count):\n                        idx = np.random.randint(self.population_size)\n                        swarms[s][idx] = swarms[s+1][np.random.randint(self.population_size)]\n\n        return global_best_position, global_best_value", "name": "Dynamic_Multi_Swarm_DE_Optimizer", "description": "Dynamic Multi-Swarm Differential Evolution Algorithm employs adaptive co-evolution among multiple swarms for enhanced exploration and exploitation balance, ensuring robustness against diverse optimization landscapes.", "configspace": "", "generation": 82, "fitness": 0.2640328179165916, "feedback": "The algorithm Dynamic_Multi_Swarm_DE_Optimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.26 with standard deviation 0.00.", "error": "", "parent_id": "23527f91-0839-4554-aae4-a24c49e9c312", "metadata": {"aucs": [0.2640328179165916, 0.2640328179165916, 0.2640328179165916]}, "mutation_prompt": null}
{"id": "588a49a5-1695-4422-97e6-7a17afc2fdf6", "solution": "import numpy as np\n\nclass Stochastic_Quantum_Harmony_Search_Optimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.harmony_memory_size = 30\n        self.harmony_consideration_rate = 0.95\n        self.pitch_adjustment_rate = 0.5\n        self.q_factor = 0.5\n        self.memory_rate_decay = 0.995\n        self.scale_adjustment = 0.2\n        self.adaptive_memory_rate = 0.05\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        np.random.seed(42)\n\n        # Initialize harmony memory randomly\n        harmony_memory = np.random.uniform(lb, ub, (self.harmony_memory_size, self.dim))\n        harmony_values = np.array([func(hm) for hm in harmony_memory])\n        best_harmony = harmony_memory[np.argmin(harmony_values)]\n        best_value = np.min(harmony_values)\n        \n        evaluations = self.harmony_memory_size\n        \n        while evaluations < self.budget:\n            new_harmony = np.copy(best_harmony)\n\n            for i in range(self.dim):\n                if np.random.rand() < self.harmony_consideration_rate:\n                    # Select from harmony memory\n                    new_harmony[i] = harmony_memory[np.random.randint(self.harmony_memory_size)][i]\n                if np.random.rand() < self.pitch_adjustment_rate:\n                    # Adjust pitch with quantum-inspired randomization\n                    new_harmony[i] += self.q_factor * np.random.normal(scale=self.scale_adjustment)\n                    new_harmony[i] = np.clip(new_harmony[i], lb[i], ub[i])\n\n            current_value = func(new_harmony)\n            evaluations += 1\n\n            # Replace worst harmony if new harmony is better\n            if current_value < np.max(harmony_values):\n                worst_index = np.argmax(harmony_values)\n                harmony_memory[worst_index] = new_harmony\n                harmony_values[worst_index] = current_value\n\n            # Update best harmony found\n            if current_value < best_value:\n                best_harmony = new_harmony\n                best_value = current_value\n\n            # Adaptive adjustment of parameters\n            self.harmony_consideration_rate *= self.memory_rate_decay\n            self.q_factor += self.adaptive_memory_rate * (1 - evaluations / self.budget)\n\n        return best_harmony, best_value", "name": "Stochastic_Quantum_Harmony_Search_Optimizer", "description": "Stochastic Quantum Harmony Search combines quantum harmonic exploration with adaptive dynamic harmony memory to enhance convergence speed and solution diversity.", "configspace": "", "generation": 83, "fitness": 0.2723110668745744, "feedback": "The algorithm Stochastic_Quantum_Harmony_Search_Optimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.27 with standard deviation 0.00.", "error": "", "parent_id": "23527f91-0839-4554-aae4-a24c49e9c312", "metadata": {"aucs": [0.2723110668745744, 0.2723110668745744, 0.2723110668745744]}, "mutation_prompt": null}
{"id": "787abef2-21c8-4bff-a013-4e7b3dfc4a30", "solution": "import numpy as np\n\nclass Quantum_Genetic_Bee_Colony_Optimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 30  # Larger initial population for diversity\n        self.exploration_factor = 0.5\n        self.genetic_crossover_rate = 0.8\n        self.mutation_rate = 0.1\n        self.q_population = np.random.uniform(0, 1, (self.population_size, self.dim))\n        self.global_best_value = float('inf')\n        self.scout_bee_rate = 0.2\n\n    def quantum_position(self, q):\n        # Quantum-inspired position transformation\n        return np.random.uniform(-1, 1, self.dim) * q\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        np.random.seed(42)\n\n        position = np.array([self.quantum_position(q) for q in self.q_population])\n        position = np.clip(position, lb, ub)\n        personal_best_position = np.copy(position)\n        personal_best_value = np.array([func(p) for p in personal_best_position])\n        global_best_position = personal_best_position[np.argmin(personal_best_value)]\n        self.global_best_value = np.min(personal_best_value)\n        \n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                # Genetic-inspired crossover and mutation\n                if np.random.rand() < self.genetic_crossover_rate:\n                    partner_index = np.random.randint(self.population_size)\n                    crossover_point = np.random.randint(self.dim)\n                    new_position = np.concatenate((position[i][:crossover_point], \n                                                   position[partner_index][crossover_point:]))\n                    if np.random.rand() < self.mutation_rate:\n                        mutation_vector = np.random.uniform(-self.exploration_factor, self.exploration_factor, self.dim)\n                        new_position += mutation_vector\n                    new_position = np.clip(new_position, lb, ub)\n                else:\n                    new_position = position[i]\n                \n                current_value = func(new_position)\n                evaluations += 1\n\n                if current_value < personal_best_value[i]:\n                    personal_best_position[i] = new_position\n                    personal_best_value[i] = current_value\n\n                if current_value < self.global_best_value:\n                    global_best_position = new_position\n                    self.global_best_value = current_value\n\n                if evaluations >= self.budget:\n                    break\n            \n            # Scout bee behavior for random exploration\n            if np.random.rand() < self.scout_bee_rate:\n                random_index = np.random.randint(self.population_size)\n                new_position = np.random.uniform(lb, ub, self.dim)\n                current_value = func(new_position)\n                evaluations += 1\n\n                if current_value < personal_best_value[random_index]:\n                    personal_best_position[random_index] = new_position\n                    personal_best_value[random_index] = current_value\n                \n                if current_value < self.global_best_value:\n                    global_best_position = new_position\n                    self.global_best_value = current_value\n\n        return global_best_position, self.global_best_value", "name": "Quantum_Genetic_Bee_Colony_Optimizer", "description": "Quantum Genetic Bee Colony Optimizer combines principles from quantum computing, genetic algorithms, and bee colony behavior to enhance exploration and exploitation in global optimization.", "configspace": "", "generation": 84, "fitness": 0.23240780312716544, "feedback": "The algorithm Quantum_Genetic_Bee_Colony_Optimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.23 with standard deviation 0.00.", "error": "", "parent_id": "23527f91-0839-4554-aae4-a24c49e9c312", "metadata": {"aucs": [0.2324078031186948, 0.23240780307732156, 0.23240780318547993]}, "mutation_prompt": null}
{"id": "889065a9-811d-45e1-9d01-e1990c78fd95", "solution": "import numpy as np\n\nclass Hybrid_DE_Adaptive_Quantum_Pso_Neighborhood_Learning:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 30  # Increased for diverse sampling\n        self.c1 = 2.05\n        self.c2 = 2.05\n        self.w = 0.6\n        self.q_factor = 0.8\n        self.gaussian_scale = 0.15\n        self.mutation_factor = 0.8\n        self.crossover_rate = 0.9\n        self.adaptive_rate = 0.98\n        self.temperature = 1.0\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        np.random.seed(42)\n\n        position = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocity = np.random.uniform(-0.5, 0.5, (self.population_size, self.dim))\n        personal_best_position = np.copy(position)\n        personal_best_value = np.array([func(p) for p in personal_best_position])\n        global_best_position = personal_best_position[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n        \n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            neighborhood_size = int(self.population_size / 3)  # Dynamic restructuring\n            for i in range(self.population_size):\n                # Differential Evolution mutation\n                indices = np.random.choice(np.delete(np.arange(self.population_size), i), 3, replace=False)\n                mutant_vector = position[indices[0]] + self.mutation_factor * (position[indices[1]] - position[indices[2]])\n                mutant_vector = np.clip(mutant_vector, lb, ub)\n                \n                # Crossover\n                crossover_mask = np.random.rand(self.dim) < self.crossover_rate\n                trial_vector = np.where(crossover_mask, mutant_vector, position[i])\n\n                # PSO update\n                r1, r2 = np.random.rand(2)\n                self.w *= self.adaptive_rate\n                velocity[i] = (self.w * velocity[i] +\n                               self.c1 * r1 * (personal_best_position[i] - position[i]) +\n                               self.c2 * r2 * (global_best_position - position[i]))\n                \n                adaptive_gaussian_scale = self.gaussian_scale * (1 - evaluations / self.budget)\n                annealing_factor = np.exp(-evaluations / (self.budget * self.temperature))\n                \n                position[i] += (velocity[i] + \n                                self.q_factor * np.random.normal(scale=adaptive_gaussian_scale, size=self.dim) +\n                                annealing_factor * np.random.uniform(-1, 1, self.dim))\n                position[i] = np.clip(position[i], lb, ub)\n\n                # Neighborhood learning\n                neighborhood_indices = np.random.choice(self.population_size, neighborhood_size, replace=False)\n                for neighbor in neighborhood_indices:\n                    local_best = min(personal_best_value[neighbor], personal_best_value[i])\n                    if local_best < personal_best_value[i]:\n                        position[i] = personal_best_position[neighbor]\n                        break\n\n                current_value = func(position[i])\n                evaluations += 1\n                \n                if current_value < personal_best_value[i]:\n                    personal_best_position[i] = position[i]\n                    personal_best_value[i] = current_value\n                \n                if current_value < global_best_value:\n                    global_best_position = position[i]\n                    global_best_value = current_value\n\n                if evaluations >= self.budget:\n                    break\n\n        return global_best_position, global_best_value", "name": "Hybrid_DE_Adaptive_Quantum_Pso_Neighborhood_Learning", "description": "Hybrid Differential Evolution and Adaptive Quantum PSO with Neighborhood Learning for Enhanced Exploration and Exploitation.", "configspace": "", "generation": 85, "fitness": 0.269791080239947, "feedback": "The algorithm Hybrid_DE_Adaptive_Quantum_Pso_Neighborhood_Learning got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.27 with standard deviation 0.00.", "error": "", "parent_id": "23527f91-0839-4554-aae4-a24c49e9c312", "metadata": {"aucs": [0.269791080239947, 0.269791080239947, 0.269791080239947]}, "mutation_prompt": null}
{"id": "4ad10d6a-1ce5-4417-8752-0bfcbccefc59", "solution": "import numpy as np\n\nclass Refined_Enhanced_Quantum_Pso_Optimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 30  # Increased to enhance exploration\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.w = 0.8  # Slightly increased initial inertia weight\n        self.q_factor = 0.7  # Adjusted for better diversification\n        self.gaussian_scale = 0.2  # Increased to enhance perturbation effect\n        self.initial_reset_chance = 0.03  # Reduced to encourage convergence\n        self.momentum_factor = 1.1  # Increased to slow down reset chance\n        self.adaptive_rate = 0.97  # Moderately reduce inertia weight\n        self.temperature = 1.0\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        np.random.seed(42)\n\n        position = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocity = np.random.uniform(-0.5, 0.5, (self.population_size, self.dim))\n        personal_best_position = np.copy(position)\n        personal_best_value = np.array([func(p) for p in personal_best_position])\n        global_best_position = personal_best_position[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n        \n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            neighborhood_size = int(self.population_size / 3)  # Dynamic restructuring\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(2)\n                self.w *= self.adaptive_rate\n                velocity[i] = (self.w * velocity[i] +\n                               self.c1 * r1 * (personal_best_position[i] - position[i]) +\n                               self.c2 * r2 * (global_best_position - position[i]))\n                \n                adaptive_gaussian_scale = self.gaussian_scale * (1 - evaluations / self.budget)\n                annealing_factor = np.exp(-evaluations / (self.budget * self.temperature))\n                \n                position[i] += (velocity[i] + \n                                self.q_factor * np.random.normal(scale=adaptive_gaussian_scale, size=self.dim) +\n                                annealing_factor * np.random.uniform(-1, 1, self.dim))\n                position[i] = np.clip(position[i], lb, ub)\n\n                current_value = func(position[i])\n                evaluations += 1\n                \n                if current_value < personal_best_value[i]:\n                    personal_best_position[i] = position[i]\n                    personal_best_value[i] = current_value\n                \n                if current_value < global_best_value:\n                    global_best_position = position[i]\n                    global_best_value = current_value\n\n                if evaluations >= self.budget:\n                    break\n\n            # Adaptive Reset Mechanism based on stagnation\n            reset_chance = self.initial_reset_chance * ((evaluations / self.budget) ** self.momentum_factor)\n            if np.random.rand() < reset_chance:\n                random_index = np.random.randint(self.population_size)\n                position[random_index] = np.random.uniform(lb, ub, self.dim)\n                current_value = func(position[random_index])\n                evaluations += 1\n                \n                if current_value < personal_best_value[random_index]:\n                    personal_best_position[random_index] = position[random_index]\n                    personal_best_value[random_index] = current_value\n                \n                if current_value < global_best_value:\n                    global_best_position = position[random_index]\n                    global_best_value = current_value\n\n            # Dynamic Neighborhood Restructuring\n            if evaluations % 15 == 0:  # Adjust neighborhood size dynamically less frequently\n                neighborhood_size = max(1, neighborhood_size - 1)\n\n        return global_best_position, global_best_value", "name": "Refined_Enhanced_Quantum_Pso_Optimizer", "description": "Enhanced Quantum-Inspired Particle Swarm Optimizer with Dynamic Gaussian Perturbation and Adaptive Reset for improved exploration and convergence.", "configspace": "", "generation": 86, "fitness": 0.2780569333315852, "feedback": "The algorithm Refined_Enhanced_Quantum_Pso_Optimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28 with standard deviation 0.00.", "error": "", "parent_id": "23527f91-0839-4554-aae4-a24c49e9c312", "metadata": {"aucs": [0.2780569333315852, 0.2780569333315852, 0.2780569333315852]}, "mutation_prompt": null}
{"id": "a65b9411-06d1-4821-a4ce-cba93ed0c551", "solution": "import numpy as np\n\nclass Quantum_Enhanced_Gradient_PSO_Optimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 30  # Increased population for more diverse exploration\n        self.c1 = 1.5\n        self.c2 = 1.5\n        self.w = 0.9\n        self.q_factor = 0.8\n        self.gaussian_scale = 0.05\n        self.initial_reset_chance = 0.1  # Increased to escape local optima\n        self.momentum_factor = 1.1\n        self.adaptive_rate = 0.98\n        self.temperature = 0.9\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        np.random.seed(42)\n\n        position = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocity = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_position = np.copy(position)\n        personal_best_value = np.array([func(p) for p in personal_best_position])\n        global_best_position = personal_best_position[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(2)\n                self.w *= self.adaptive_rate\n                velocity[i] = (self.w * velocity[i] +\n                               self.c1 * r1 * (personal_best_position[i] - position[i]) +\n                               self.c2 * r2 * (global_best_position - position[i]))\n\n                # Gradient-Assisted Exploration\n                gradient_step = np.zeros(self.dim)\n                for d in range(self.dim):\n                    perturbed_position = np.copy(position[i])\n                    epsilon = 1e-8\n                    perturbed_position[d] += epsilon\n                    gradient_step[d] = (func(perturbed_position) - personal_best_value[i]) / epsilon\n                velocity[i] -= gradient_step * 0.01\n\n                adaptive_gaussian_scale = self.gaussian_scale * (1 - evaluations / self.budget)\n                annealing_factor = np.exp(-evaluations / (self.budget * self.temperature))\n\n                position[i] += (velocity[i] +\n                                self.q_factor * np.random.normal(scale=adaptive_gaussian_scale, size=self.dim) +\n                                annealing_factor * np.random.uniform(-1, 1, self.dim))\n                position[i] = np.clip(position[i], lb, ub)\n\n                current_value = func(position[i])\n                evaluations += 1\n\n                if current_value < personal_best_value[i]:\n                    personal_best_position[i] = position[i]\n                    personal_best_value[i] = current_value\n\n                if current_value < global_best_value:\n                    global_best_position = position[i]\n                    global_best_value = current_value\n\n                if evaluations >= self.budget:\n                    break\n\n            # Adaptive Reset Mechanism based on stagnation\n            reset_chance = self.initial_reset_chance * ((evaluations / self.budget) ** self.momentum_factor)\n            if np.random.rand() < reset_chance:\n                random_index = np.random.randint(self.population_size)\n                position[random_index] = np.random.uniform(lb, ub, self.dim)\n                current_value = func(position[random_index])\n                evaluations += 1\n\n                if current_value < personal_best_value[random_index]:\n                    personal_best_position[random_index] = position[random_index]\n                    personal_best_value[random_index] = current_value\n\n                if current_value < global_best_value:\n                    global_best_position = position[random_index]\n                    global_best_value = current_value\n\n        return global_best_position, global_best_value", "name": "Quantum_Enhanced_Gradient_PSO_Optimizer", "description": "Quantum-Enhanced Particle Swarm Optimizer with Gradient-Assisted Exploration and Adaptive Momentum for improved convergence and global optimization.", "configspace": "", "generation": 87, "fitness": 0.2637707206340547, "feedback": "The algorithm Quantum_Enhanced_Gradient_PSO_Optimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.26 with standard deviation 0.00.", "error": "", "parent_id": "23527f91-0839-4554-aae4-a24c49e9c312", "metadata": {"aucs": [0.2637707206340547, 0.2637707206340547, 0.2637707206340547]}, "mutation_prompt": null}
{"id": "8ffed349-9b57-4dd8-841b-8fd817e14196", "solution": "import numpy as np\n\nclass Quantum_Inspired_Gravitational_Swarm_Optimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 30\n        self.G0 = 100  # Initial gravitational constant\n        self.alpha = 20.0  # Decay factor for gravitational constant\n        self.quantum_scale = 0.05\n        self.local_perturbation_strength = 0.1\n        self.inertia_weight = 0.7\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        np.random.seed(42)\n\n        position = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocity = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_position = np.copy(position)\n        personal_best_value = np.array([func(p) for p in personal_best_position])\n        global_best_position = personal_best_position[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            gravitational_constant = self.G0 * np.exp(-self.alpha * evaluations / self.budget)\n            \n            for i in range(self.population_size):\n                for j in range(self.population_size):\n                    if personal_best_value[j] < personal_best_value[i]:\n                        r_ij = np.linalg.norm(personal_best_position[j] - personal_best_position[i])\n                        force = gravitational_constant * (personal_best_position[j] - personal_best_position[i]) / (r_ij + 1e-12)\n                        velocity[i] += force + np.random.normal(scale=self.quantum_scale, size=self.dim)\n                \n                r1 = np.random.rand()\n                velocity[i] = self.inertia_weight * velocity[i] + r1 * (global_best_position - position[i])\n                position[i] += velocity[i]\n                position[i] = np.clip(position[i], lb, ub)\n\n                if np.random.rand() < self.local_perturbation_strength:\n                    perturbation = np.random.normal(scale=self.quantum_scale, size=self.dim)\n                    position[i] += perturbation\n                    position[i] = np.clip(position[i], lb, ub)\n\n                current_value = func(position[i])\n                evaluations += 1\n\n                if current_value < personal_best_value[i]:\n                    personal_best_position[i] = position[i]\n                    personal_best_value[i] = current_value\n\n                if current_value < global_best_value:\n                    global_best_position = position[i]\n                    global_best_value = current_value\n\n                if evaluations >= self.budget:\n                    break\n\n        return global_best_position, global_best_value", "name": "Quantum_Inspired_Gravitational_Swarm_Optimizer", "description": "Quantum-Inspired Gravitational Swarm Optimization with Local Perturbation dynamically balances exploration and exploitation using gravitational attraction and quantum perturbations to enhance convergence.", "configspace": "", "generation": 88, "fitness": 0.25367315252608325, "feedback": "The algorithm Quantum_Inspired_Gravitational_Swarm_Optimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.25 with standard deviation 0.00.", "error": "", "parent_id": "23527f91-0839-4554-aae4-a24c49e9c312", "metadata": {"aucs": [0.25367315252608325, 0.25367315252608325, 0.25367315252608325]}, "mutation_prompt": null}
{"id": "7b9e420c-be1b-4149-aa00-c0ac3f2f80b4", "solution": "import numpy as np\n\nclass Quantum_Evolutionary_Swarm_Optimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 30  # Elevated to enhance diversity\n        self.c1 = 1.5\n        self.c2 = 1.5\n        self.w = 0.6  # Reduced for stability\n        self.q_factor = 0.8\n        self.crossover_rate = 0.4\n        self.mutation_scale = 0.05\n        self.inertia_decay = 0.97  # Gradual decay of inertia\n        self.temperature = 0.9\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        np.random.seed(42)\n\n        position = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocity = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_position = np.copy(position)\n        personal_best_value = np.array([func(p) for p in personal_best_position])\n        global_best_position = personal_best_position[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n        \n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(2)\n                self.w *= self.inertia_decay\n                velocity[i] = (self.w * velocity[i] +\n                               self.c1 * r1 * (personal_best_position[i] - position[i]) +\n                               self.c2 * r2 * (global_best_position - position[i]))\n                \n                quantum_jump = self.q_factor * np.random.normal(scale=self.mutation_scale, size=self.dim)\n                annealing_shift = np.exp(-evaluations / (self.budget * self.temperature)) * np.random.uniform(-0.5, 0.5, self.dim)\n                \n                position[i] += velocity[i] + quantum_jump + annealing_shift\n                position[i] = np.clip(position[i], lb, ub)\n\n                current_value = func(position[i])\n                evaluations += 1\n                \n                if current_value < personal_best_value[i]:\n                    personal_best_position[i] = position[i]\n                    personal_best_value[i] = current_value\n                \n                if current_value < global_best_value:\n                    global_best_position = position[i]\n                    global_best_value = current_value\n\n                if evaluations >= self.budget:\n                    break\n\n            # Genetic-like Crossover operation\n            for i in range(0, self.population_size, 2):\n                if np.random.rand() < self.crossover_rate:\n                    parent1 = personal_best_position[i]\n                    parent2 = personal_best_position[(i + 1) % self.population_size]\n                    offspring1, offspring2 = self.crossover(parent1, parent2)\n                    \n                    # Evaluate offspring and potentially update personal and global bests\n                    self.evaluate_and_update(offspring1, func, personal_best_position, personal_best_value, global_best_position, global_best_value, evaluations, lb, ub)\n                    self.evaluate_and_update(offspring2, func, personal_best_position, personal_best_value, global_best_position, global_best_value, evaluations, lb, ub)\n\n        return global_best_position, global_best_value\n\n    def crossover(self, parent1, parent2):\n        crossover_point = np.random.randint(1, self.dim - 1)\n        offspring1 = np.concatenate((parent1[:crossover_point], parent2[crossover_point:]))\n        offspring2 = np.concatenate((parent2[:crossover_point], parent1[crossover_point:]))\n        return offspring1, offspring2\n\n    def evaluate_and_update(self, position, func, personal_best_position, personal_best_value, global_best_position, global_best_value, evaluations, lb, ub):\n        position = np.clip(position, lb, ub)\n        current_value = func(position)\n        evaluations += 1\n        \n        if current_value < personal_best_value[np.argmin(personal_best_value)]:\n            personal_best_position[np.argmin(personal_best_value)] = position\n            personal_best_value[np.argmin(personal_best_value)] = current_value\n        \n        if current_value < global_best_value:\n            global_best_position = position\n            global_best_value = current_value", "name": "Quantum_Evolutionary_Swarm_Optimizer", "description": "Quantum Evolutionary Swarm Optimizer combines Quantum-inspired Particle Dynamics and Genetic Crossover for enhanced exploration and convergence.", "configspace": "", "generation": 89, "fitness": 0.2781496156244919, "feedback": "The algorithm Quantum_Evolutionary_Swarm_Optimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28 with standard deviation 0.00.", "error": "", "parent_id": "23527f91-0839-4554-aae4-a24c49e9c312", "metadata": {"aucs": [0.2781496156244919, 0.2781496156244919, 0.2781496156244919]}, "mutation_prompt": null}
{"id": "16f7d950-59a5-4b6c-a699-567387a1220e", "solution": "import numpy as np\n\nclass Enhanced_Quantum_Evolutionary_Swarm_Optimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 30  # Start with a fixed size\n        self.c1 = 1.5\n        self.c2 = 1.5\n        self.w = 0.6\n        self.q_factor = 0.8\n        self.crossover_rate = 0.4\n        self.mutation_scale = 0.05\n        self.inertia_decay = 0.97\n        self.temperature = 0.9\n        self.dynamic_pop_size = True  # Enable dynamic population sizing\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        np.random.seed(42)\n\n        position = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocity = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_position = np.copy(position)\n        personal_best_value = np.array([func(p) for p in personal_best_position])\n        global_best_position = personal_best_position[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n        \n        evaluations = self.population_size\n        learning_rate = 0.1  # Initial learning rate for adaptive control\n\n        while evaluations < self.budget:\n            if self.dynamic_pop_size:\n                adaptive_size = min(self.population_size + int(evaluations / self.budget * self.population_size), 100)\n                if adaptive_size != self.population_size:\n                    position = np.vstack((position, np.random.uniform(lb, ub, (adaptive_size - self.population_size, self.dim))))\n                    velocity = np.vstack((velocity, np.random.uniform(-1, 1, (adaptive_size - self.population_size, self.dim))))\n                    personal_best_position = np.vstack((personal_best_position, position[-(adaptive_size - self.population_size):]))\n                    personal_best_value = np.append(personal_best_value, [func(p) for p in personal_best_position[-(adaptive_size - self.population_size):]])\n                    self.population_size = adaptive_size\n                    \n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(2)\n                self.w *= self.inertia_decay\n                velocity[i] = (self.w * velocity[i] +\n                               self.c1 * r1 * (personal_best_position[i] - position[i]) +\n                               self.c2 * r2 * (global_best_position - position[i]))\n                \n                quantum_jump = self.q_factor * np.random.normal(scale=self.mutation_scale, size=self.dim)\n                annealing_shift = np.exp(-evaluations / (self.budget * self.temperature)) * np.random.uniform(-0.5, 0.5, self.dim)\n                \n                position[i] += velocity[i] + quantum_jump + annealing_shift\n                position[i] = np.clip(position[i], lb, ub)\n\n                current_value = func(position[i])\n                evaluations += 1\n                \n                if current_value < personal_best_value[i]:\n                    personal_best_position[i] = position[i]\n                    personal_best_value[i] = current_value\n                \n                if current_value < global_best_value:\n                    global_best_position = position[i]\n                    global_best_value = current_value\n\n                if evaluations >= self.budget:\n                    break\n\n            # Adaptive Learning Rate based Crossover\n            for i in range(0, self.population_size, 2):\n                if np.random.rand() < self.crossover_rate:\n                    parent1 = personal_best_position[i]\n                    parent2 = personal_best_position[(i + 1) % self.population_size]\n                    offspring1, offspring2 = self.crossover(parent1, parent2, learning_rate)\n                    \n                    # Evaluate offspring and potentially update personal and global bests\n                    self.evaluate_and_update(offspring1, func, personal_best_position, personal_best_value, global_best_position, global_best_value, evaluations, lb, ub)\n                    self.evaluate_and_update(offspring2, func, personal_best_position, personal_best_value, global_best_position, global_best_value, evaluations, lb, ub)\n\n            learning_rate *= 1.01  # Gradually adjust learning rate\n\n        return global_best_position, global_best_value\n\n    def crossover(self, parent1, parent2, learning_rate):\n        crossover_point = np.random.randint(1, self.dim - 1)\n        offspring1 = np.concatenate((parent1[:crossover_point], parent2[crossover_point:])) * learning_rate\n        offspring2 = np.concatenate((parent2[:crossover_point], parent1[crossover_point:])) * learning_rate\n        return offspring1, offspring2\n\n    def evaluate_and_update(self, position, func, personal_best_position, personal_best_value, global_best_position, global_best_value, evaluations, lb, ub):\n        position = np.clip(position, lb, ub)\n        current_value = func(position)\n        evaluations += 1\n        \n        if current_value < personal_best_value[np.argmin(personal_best_value)]:\n            personal_best_position[np.argmin(personal_best_value)] = position\n            personal_best_value[np.argmin(personal_best_value)] = current_value\n        \n        if current_value < global_best_value:\n            global_best_position = position\n            global_best_value = current_value", "name": "Enhanced_Quantum_Evolutionary_Swarm_Optimizer", "description": "Enhanced Quantum Evolutionary Swarm Optimizer with Adaptive Learning Rates and Dynamic Population Sizing for Improved Exploration and Exploitation Balance.", "configspace": "", "generation": 90, "fitness": 0.2781479648651122, "feedback": "The algorithm Enhanced_Quantum_Evolutionary_Swarm_Optimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28 with standard deviation 0.00.", "error": "", "parent_id": "7b9e420c-be1b-4149-aa00-c0ac3f2f80b4", "metadata": {"aucs": [0.2781479648651122, 0.2781479648651122, 0.2781479648651122]}, "mutation_prompt": null}
{"id": "8226e8fa-e278-476e-bd62-3ef2c4b99232", "solution": "import numpy as np\n\nclass Hybrid_Quantum_Genetic_Algorithm:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 40  # Increased for better exploration\n        self.q_factor = 0.7  # Quantum exploration factor\n        self.crossover_rate = 0.5\n        self.mutation_rate = 0.1  # Higher mutation rate for diversity\n        self.adaptation_factor = 0.95  # Adaptive control for exploration and exploitation balance\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        np.random.seed(42)\n\n        # Initialize population\n        position = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(p) for p in position])\n        evaluations = self.population_size\n        \n        best_indices = np.argsort(fitness)\n        global_best_position = position[best_indices[0]]\n        global_best_value = fitness[best_indices[0]]\n\n        while evaluations < self.budget:\n            # Quantum-inspired exploration\n            quantum_shift = self.q_factor * np.random.normal(size=(self.population_size, self.dim))\n            position += quantum_shift * np.random.uniform(-1, 1, (self.population_size, self.dim))\n            position = np.clip(position, lb, ub)\n\n            # Evaluate new positions\n            new_fitness = np.array([func(p) for p in position])\n            evaluations += self.population_size\n\n            # Select the best positions for the next generation\n            combined_positions = np.vstack((position, position[best_indices]))\n            combined_fitness = np.hstack((new_fitness, fitness[best_indices]))\n\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            position = combined_positions[best_indices]\n            fitness = combined_fitness[best_indices]\n\n            # Update global best\n            if fitness[0] < global_best_value:\n                global_best_position = position[0]\n                global_best_value = fitness[0]\n\n            # Adaptive crossover and mutation\n            new_population = []\n            for i in range(0, self.population_size, 2):\n                if np.random.rand() < self.crossover_rate:\n                    parent1, parent2 = position[i], position[(i + 1) % self.population_size]\n                    offspring1, offspring2 = self.crossover(parent1, parent2)\n                    new_population.extend([offspring1, offspring2])\n                else:\n                    new_population.extend([position[i], position[(i + 1) % self.population_size]])\n\n            # Apply mutation\n            for i in range(self.population_size):\n                if np.random.rand() < self.mutation_rate:\n                    new_population[i] += self.mutation_rate * np.random.normal(size=self.dim)\n                    new_population[i] = np.clip(new_population[i], lb, ub)\n\n            position = np.array(new_population)\n            fitness = np.array([func(p) for p in position])\n            evaluations += self.population_size\n            \n            # Adapt exploration parameters\n            self.q_factor *= self.adaptation_factor\n\n        return global_best_position, global_best_value\n\n    def crossover(self, parent1, parent2):\n        alpha = np.random.rand(self.dim)\n        offspring1 = alpha * parent1 + (1 - alpha) * parent2\n        offspring2 = alpha * parent2 + (1 - alpha) * parent1\n        return offspring1, offspring2", "name": "Hybrid_Quantum_Genetic_Algorithm", "description": "Hybrid Quantum Genetic Algorithm combines quantum-inspired exploration with adaptive genetic operations for robust global optimization.", "configspace": "", "generation": 91, "fitness": -Infinity, "feedback": "An exception occurred: IndexError('index 40 is out of bounds for axis 0 with size 40').", "error": "IndexError('index 40 is out of bounds for axis 0 with size 40')", "parent_id": "7b9e420c-be1b-4149-aa00-c0ac3f2f80b4", "metadata": {}, "mutation_prompt": null}
{"id": "90218fd9-9a27-432e-b459-86251af00f4b", "solution": "import numpy as np\n\nclass Adaptive_Quantum_Differential_Evolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 40\n        self.F = 0.5  # Mutation factor\n        self.CR = 0.9  # Crossover probability\n        self.q_scale = 0.05  # Quantum mutation scale\n        self.adaptive_factor = 1.05  # Adaptive increase on stagnation\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        np.random.seed(42)\n\n        # Initialize population\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_fitness = fitness[best_idx]\n        \n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            new_population = np.empty_like(population)\n            for i in range(self.population_size):\n                # Mutation using differential evolution strategy\n                candidates = np.random.choice(self.population_size, 3, replace=False)\n                a, b, c = population[candidates]\n                mutant = a + self.F * (b - c)\n                mutant = np.clip(mutant, lb, ub)\n                \n                # Crossover\n                crossover_mask = np.random.rand(self.dim) < self.CR\n                trial = np.where(crossover_mask, mutant, population[i])\n                \n                # Apply quantum mutation for exploration\n                quantum_mutation = self.q_scale * np.random.normal(0, 1, self.dim)\n                trial += quantum_mutation\n                trial = np.clip(trial, lb, ub)\n                \n                # Evaluate trial individual\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                # Selection\n                if trial_fitness < fitness[i]:\n                    new_population[i] = trial\n                    fitness[i] = trial_fitness\n                else:\n                    new_population[i] = population[i]\n                    \n                # Update the best solution found\n                if trial_fitness < best_fitness:\n                    best_solution = trial\n                    best_fitness = trial_fitness\n\n                if evaluations >= self.budget:\n                    break\n\n            population = new_population\n\n            # Adapt the mutation factor if no improvement\n            if np.min(fitness) >= best_fitness:\n                self.F *= self.adaptive_factor\n\n        return best_solution, best_fitness", "name": "Adaptive_Quantum_Differential_Evolution", "description": "Adaptive Quantum Differential Evolution leverages quantum superposition principles and adaptive mutation strategies for robust exploration and exploitation in complex optimization landscapes.", "configspace": "", "generation": 92, "fitness": 0.24930253196022512, "feedback": "The algorithm Adaptive_Quantum_Differential_Evolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.25 with standard deviation 0.00.", "error": "", "parent_id": "7b9e420c-be1b-4149-aa00-c0ac3f2f80b4", "metadata": {"aucs": [0.24930253196022512, 0.24930253196022512, 0.24930253196022512]}, "mutation_prompt": null}
{"id": "f8f47d7d-2ddf-4a6c-877f-275279919034", "solution": "import numpy as np\n\nclass Adaptive_Harmony_Search_Optimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.harmony_memory_size = 30\n        self.harmony_consideration_rate = 0.9\n        self.pitch_adjustment_rate = 0.2\n        self.adaptive_adjustment = 0.01\n        self.bandwidth = 0.1\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        np.random.seed(42)\n\n        harmony_memory = np.random.uniform(lb, ub, (self.harmony_memory_size, self.dim))\n        harmony_values = np.array([func(h) for h in harmony_memory])\n        best_harmony = harmony_memory[np.argmin(harmony_values)]\n        best_value = np.min(harmony_values)\n\n        evaluations = self.harmony_memory_size\n\n        while evaluations < self.budget:\n            new_harmony = np.zeros(self.dim)\n            for i in range(self.dim):\n                if np.random.rand() < self.harmony_consideration_rate:\n                    new_harmony[i] = harmony_memory[np.random.randint(self.harmony_memory_size), i]\n                    if np.random.rand() < self.pitch_adjustment_rate:\n                        new_harmony[i] += self.bandwidth * np.random.uniform(-1, 1)\n                else:\n                    new_harmony[i] = np.random.uniform(lb[i], ub[i])\n            \n            new_harmony = np.clip(new_harmony, lb, ub)\n            new_value = func(new_harmony)\n            evaluations += 1\n\n            if new_value < best_value:\n                best_harmony = new_harmony\n                best_value = new_value\n\n            if new_value < np.max(harmony_values):\n                worst_index = np.argmax(harmony_values)\n                harmony_memory[worst_index] = new_harmony\n                harmony_values[worst_index] = new_value\n\n            # Dynamic adaptation of parameters\n            self.harmony_consideration_rate = max(0.6, self.harmony_consideration_rate - self.adaptive_adjustment)\n            self.pitch_adjustment_rate = min(0.4, self.pitch_adjustment_rate + self.adaptive_adjustment)\n\n        return best_harmony, best_value", "name": "Adaptive_Harmony_Search_Optimizer", "description": "Adaptive Harmony Search Optimizer integrates dynamic parameter tuning and memory consideration for efficient global optimization.", "configspace": "", "generation": 93, "fitness": 0.2755416395059488, "feedback": "The algorithm Adaptive_Harmony_Search_Optimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28 with standard deviation 0.00.", "error": "", "parent_id": "7b9e420c-be1b-4149-aa00-c0ac3f2f80b4", "metadata": {"aucs": [0.2755416395059488, 0.2755416395059488, 0.2755416395059488]}, "mutation_prompt": null}
{"id": "6ed62a81-dc50-427b-9a95-82d0c85c2d3d", "solution": "import numpy as np\n\nclass Adaptive_Quantum_Cultural_Algorithm:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 40\n        self.belief_space = np.zeros((2, self.dim))  # Min and max belief for each dimension\n        self.mutation_scale = 0.1\n        self.inertia = 0.5\n        self.q_factor = 0.9\n        self.crossover_rate = 0.6\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        np.random.seed(42)\n\n        position = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocity = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_position = np.copy(position)\n        personal_best_value = np.array([func(p) for p in personal_best_position])\n        global_best_position = personal_best_position[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        self.belief_space[0] = lb\n        self.belief_space[1] = ub\n\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(2)\n                velocity[i] = (self.inertia * velocity[i] +\n                               r1 * (personal_best_position[i] - position[i]) +\n                               r2 * (global_best_position - position[i]))\n\n                quantum_jump = self.q_factor * np.random.normal(scale=self.mutation_scale, size=self.dim)\n                position[i] += velocity[i] + quantum_jump\n                position[i] = np.clip(position[i], lb, ub)\n\n                current_value = func(position[i])\n                evaluations += 1\n\n                if current_value < personal_best_value[i]:\n                    personal_best_position[i] = position[i]\n                    personal_best_value[i] = current_value\n\n                if current_value < global_best_value:\n                    global_best_position = position[i]\n                    global_best_value = current_value\n\n                if evaluations >= self.budget:\n                    break\n\n            # Adaptive Belief Space Update\n            new_belief = self.update_belief_space(personal_best_position)\n            self.belief_space = new_belief\n\n            # Crossover with belief space influence\n            for i in range(0, self.population_size, 2):\n                if np.random.rand() < self.crossover_rate:\n                    parent1 = personal_best_position[i]\n                    parent2 = personal_best_position[(i + 1) % self.population_size]\n                    offspring1, offspring2 = self.crossover(parent1, parent2)\n\n                    # Evaluate offspring and potentially update personal and global bests\n                    self.evaluate_and_update(offspring1, func, personal_best_position, personal_best_value, global_best_position, global_best_value, evaluations, lb, ub)\n                    self.evaluate_and_update(offspring2, func, personal_best_position, personal_best_value, global_best_position, global_best_value, evaluations, lb, ub)\n\n        return global_best_position, global_best_value\n\n    def crossover(self, parent1, parent2):\n        crossover_point = np.random.randint(1, self.dim - 1)\n        offspring1 = np.concatenate((parent1[:crossover_point], parent2[crossover_point:]))\n        offspring2 = np.concatenate((parent2[:crossover_point], parent1[crossover_point:]))\n        return offspring1, offspring2\n\n    def evaluate_and_update(self, position, func, personal_best_position, personal_best_value, global_best_position, global_best_value, evaluations, lb, ub):\n        position = np.clip(position, lb, ub)\n        current_value = func(position)\n        evaluations += 1\n\n        if current_value < personal_best_value[np.argmin(personal_best_value)]:\n            personal_best_position[np.argmin(personal_best_value)] = position\n            personal_best_value[np.argmin(personal_best_value)] = current_value\n\n        if current_value < global_best_value:\n            global_best_position = position\n            global_best_value = current_value\n\n    def update_belief_space(self, personal_best_position):\n        # Update belief with a simple method based on the best positions\n        new_belief = np.zeros_like(self.belief_space)\n        new_belief[0] = np.min(personal_best_position, axis=0)\n        new_belief[1] = np.max(personal_best_position, axis=0)\n        return new_belief", "name": "Adaptive_Quantum_Cultural_Algorithm", "description": "Adaptive Quantum Cultural Algorithm leverages a dynamic belief space and quantum-inspired search to adaptively balance exploration and exploitation in photonic structure optimization.", "configspace": "", "generation": 94, "fitness": 0.27801905606753075, "feedback": "The algorithm Adaptive_Quantum_Cultural_Algorithm got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28 with standard deviation 0.00.", "error": "", "parent_id": "7b9e420c-be1b-4149-aa00-c0ac3f2f80b4", "metadata": {"aucs": [0.27801905606753075, 0.27801905606753075, 0.27801905606753075]}, "mutation_prompt": null}
{"id": "1f186a0d-5d8b-4729-8929-c3557c72f631", "solution": "import numpy as np\n\nclass Adaptive_Levy_Flight_Optimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 25\n        self.alpha = 0.9  # Influence of local best\n        self.beta = 0.3   # Influence of global best\n        self.mutation_strength = 0.01\n        self.inertia_weight = 0.7\n        self.inertia_decay = 0.95  # Decaying inertia\n        self.levy_scale = 0.1  # Scale factor for Levy flight\n\n    def levy(self):\n        # Generating step from Levy distribution\n        u = np.random.normal(0, 1, size=self.dim)\n        v = np.random.normal(0, 1, size=self.dim)\n        step = u / (np.abs(v) ** (1 / 3))\n        return step * self.levy_scale\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        np.random.seed(42)\n\n        position = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocity = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_pos = np.copy(position)\n        personal_best_val = np.array([func(p) for p in personal_best_pos])\n        global_best_pos = personal_best_pos[np.argmin(personal_best_val)]\n        global_best_val = np.min(personal_best_val)\n\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(2)\n                self.inertia_weight *= self.inertia_decay\n                velocity[i] = (self.inertia_weight * velocity[i] +\n                               self.alpha * r1 * (personal_best_pos[i] - position[i]) +\n                               self.beta * r2 * (global_best_pos - position[i]))\n\n                position[i] += velocity[i]\n                # Levy flight perturbation\n                position[i] += self.levy()\n                position[i] = np.clip(position[i], lb, ub)\n\n                current_val = func(position[i])\n                evaluations += 1\n\n                if current_val < personal_best_val[i]:\n                    personal_best_pos[i] = position[i]\n                    personal_best_val[i] = current_val\n\n                if current_val < global_best_val:\n                    global_best_pos = position[i]\n                    global_best_val = current_val\n\n                if evaluations >= self.budget:\n                    break\n\n        return global_best_pos, global_best_val", "name": "Adaptive_Levy_Flight_Optimizer", "description": "Adaptive Levy Flight Optimizer combines adaptive velocity updates with Levy flight-inspired exploration for efficient global search and local refinement.", "configspace": "", "generation": 95, "fitness": 0.2765490774860374, "feedback": "The algorithm Adaptive_Levy_Flight_Optimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28 with standard deviation 0.00.", "error": "", "parent_id": "7b9e420c-be1b-4149-aa00-c0ac3f2f80b4", "metadata": {"aucs": [0.2765490774860374, 0.2765490774860374, 0.2765490774860374]}, "mutation_prompt": null}
{"id": "733c16a4-44e6-4922-b7f9-954df7d96cc8", "solution": "import numpy as np\n\nclass Adaptive_Memetic_Algorithm:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 40  # Larger pool for diversity\n        self.elite_fraction = 0.2  # Fraction of top performers\n        self.local_search_rate = 0.1  # Initial rate for local search\n        self.mutation_rate = 0.05\n        self.crossover_rate = 0.7\n        self.epsilon_decay = 0.995  # Decay factor for local search rate\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        np.random.seed(42)\n\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(individual) for individual in population])\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            elite_count = int(self.elite_fraction * self.population_size)\n            elite_indices = np.argsort(fitness)[:elite_count]\n            elite_population = population[elite_indices]\n            \n            # Adaptive strategy update\n            self.local_search_rate *= self.epsilon_decay\n            \n            # Local Search on Elite\n            for i in elite_population:\n                if np.random.rand() < self.local_search_rate:\n                    step_size = np.random.normal(0, 0.1, self.dim)\n                    new_solution = np.clip(i + step_size, lb, ub)\n                    new_fitness = func(new_solution)\n                    evaluations += 1\n\n                    if new_fitness < func(i):\n                        i[:] = new_solution\n\n                    if evaluations >= self.budget:\n                        break\n            \n            # Generate offspring through crossover and mutation\n            offspring = []\n            while len(offspring) < self.population_size - elite_count:\n                parents = np.random.choice(elite_population, size=2, replace=False)\n                if np.random.rand() < self.crossover_rate:\n                    child1, child2 = self.crossover(parents[0], parents[1])\n                else:\n                    child1, child2 = parents[0], parents[1]\n                \n                self.mutate(child1)\n                self.mutate(child2)\n                \n                offspring.append(child1)\n                offspring.append(child2)\n                \n            # Evaluate offspring\n            offspring = np.array(offspring)[:self.population_size - elite_count]\n            offspring_fitness = np.array([func(individual) for individual in offspring])\n            evaluations += len(offspring)\n\n            # Update population and fitness\n            population = np.vstack((elite_population, offspring))\n            fitness = np.hstack((fitness[elite_indices], offspring_fitness))\n\n            if evaluations >= self.budget:\n                break\n\n        best_index = np.argmin(fitness)\n        return population[best_index], fitness[best_index]\n\n    def crossover(self, parent1, parent2):\n        crossover_point = np.random.randint(1, self.dim - 1)\n        child1 = np.concatenate((parent1[:crossover_point], parent2[crossover_point:]))\n        child2 = np.concatenate((parent2[:crossover_point], parent1[crossover_point:]))\n        return child1, child2\n\n    def mutate(self, individual):\n        if np.random.rand() < self.mutation_rate:\n            mutation_vector = np.random.normal(0, 0.1, self.dim)\n            individual += mutation_vector", "name": "Adaptive_Memetic_Algorithm", "description": "Adaptive Memetic Algorithm uses localized search with dynamically updated strategies and cooperation between diverse solution pools for robust global optimization.", "configspace": "", "generation": 96, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('a must be 1-dimensional').", "error": "ValueError('a must be 1-dimensional')", "parent_id": "7b9e420c-be1b-4149-aa00-c0ac3f2f80b4", "metadata": {}, "mutation_prompt": null}
{"id": "e9079454-75d8-497a-a9bd-4c311af3e6e8", "solution": "import numpy as np\n\nclass Adaptive_Quantum_Evolutionary_Swarm_Optimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 30\n        self.c1 = 1.5\n        self.c2 = 1.5\n        self.w = 0.6\n        self.q_factor = 0.8\n        self.crossover_rate = 0.4\n        self.mutation_scale = 0.05\n        self.inertia_decay = 0.97\n        self.temperature = 0.9\n        self.learning_rate_decay = 0.99  # New decay parameter for learning rates\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        np.random.seed(42)\n\n        position = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocity = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_position = np.copy(position)\n        personal_best_value = np.array([func(p) for p in personal_best_position])\n        global_best_position = personal_best_position[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        evaluations = self.population_size\n        learning_rate = np.ones((self.population_size, self.dim)) * 0.01  # Initial learning rate\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(2)\n                self.w *= self.inertia_decay\n                learning_rate[i] *= self.learning_rate_decay  # Decaying learning rate\n                velocity[i] = (self.w * velocity[i] +\n                               self.c1 * r1 * (personal_best_position[i] - position[i]) +\n                               self.c2 * r2 * (global_best_position - position[i]))\n\n                quantum_jump = self.q_factor * np.random.normal(scale=self.mutation_scale, size=self.dim)\n                annealing_shift = np.exp(-evaluations / (self.budget * self.temperature)) * np.random.uniform(-0.5, 0.5, self.dim)\n\n                position[i] += learning_rate[i] * velocity[i] + quantum_jump + annealing_shift\n                position[i] = np.clip(position[i], lb, ub)\n\n                current_value = func(position[i])\n                evaluations += 1\n\n                if current_value < personal_best_value[i]:\n                    personal_best_position[i] = position[i]\n                    personal_best_value[i] = current_value\n\n                if current_value < global_best_value:\n                    global_best_position = position[i]\n                    global_best_value = current_value\n\n                if evaluations >= self.budget:\n                    break\n\n            # Genetic-like Crossover operation\n            for i in range(0, self.population_size, 2):\n                if np.random.rand() < self.crossover_rate:\n                    parent1 = personal_best_position[i]\n                    parent2 = personal_best_position[(i + 1) % self.population_size]\n                    offspring1, offspring2 = self.crossover(parent1, parent2)\n\n                    self.evaluate_and_update(offspring1, func, personal_best_position, personal_best_value, global_best_position, global_best_value, evaluations, lb, ub)\n                    self.evaluate_and_update(offspring2, func, personal_best_position, personal_best_value, global_best_position, global_best_value, evaluations, lb, ub)\n\n        return global_best_position, global_best_value\n\n    def crossover(self, parent1, parent2):\n        crossover_point = np.random.randint(1, self.dim - 1)\n        offspring1 = np.concatenate((parent1[:crossover_point], parent2[crossover_point:]))\n        offspring2 = np.concatenate((parent2[:crossover_point], parent1[crossover_point:]))\n        return offspring1, offspring2\n\n    def evaluate_and_update(self, position, func, personal_best_position, personal_best_value, global_best_position, global_best_value, evaluations, lb, ub):\n        position = np.clip(position, lb, ub)\n        current_value = func(position)\n        evaluations += 1\n\n        if current_value < personal_best_value[np.argmin(personal_best_value)]:\n            personal_best_position[np.argmin(personal_best_value)] = position\n            personal_best_value[np.argmin(personal_best_value)] = current_value\n\n        if current_value < global_best_value:\n            global_best_position = position\n            global_best_value = current_value", "name": "Adaptive_Quantum_Evolutionary_Swarm_Optimizer", "description": "Adaptive Quantum Evolutionary Swarm Optimizer integrates adaptive learning rates and momentum with quantum-inspired dynamics and crossover for robust exploration and convergence.", "configspace": "", "generation": 97, "fitness": 0.2527332653488844, "feedback": "The algorithm Adaptive_Quantum_Evolutionary_Swarm_Optimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.25 with standard deviation 0.00.", "error": "", "parent_id": "7b9e420c-be1b-4149-aa00-c0ac3f2f80b4", "metadata": {"aucs": [0.2527332653488844, 0.2527332653488844, 0.2527332653488844]}, "mutation_prompt": null}
{"id": "d5363235-c2f1-4963-8ba5-3a1cea1fc90b", "solution": "import numpy as np\n\nclass Adaptive_Gradient_Based_Hybrid_Optimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 30\n        self.alpha = 0.01  # Learning rate for gradient adjustment\n        self.beta = 0.9  # Momentum factor for gradient-based search\n        self.crossover_rate = 0.5\n        self.mutation_scale = 0.1\n        self.gradient_steps = int(budget * 0.2)  # Fraction of budget for gradient search\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        np.random.seed(42)\n\n        position = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocities = np.zeros((self.population_size, self.dim))\n        personal_best_position = np.copy(position)\n        personal_best_value = np.array([func(p) for p in personal_best_position])\n        global_best_position = personal_best_position[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n        \n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            # Gradient-based local search\n            for _ in range(self.gradient_steps):\n                for i in range(self.population_size):\n                    gradient = self.compute_gradient(func, position[i], lb, ub)\n                    velocities[i] = self.beta * velocities[i] - self.alpha * gradient\n                    position[i] += velocities[i]\n                    position[i] = np.clip(position[i], lb, ub)\n\n                    current_value = func(position[i])\n                    evaluations += 1\n\n                    if current_value < personal_best_value[i]:\n                        personal_best_position[i] = position[i]\n                        personal_best_value[i] = current_value\n\n                    if current_value < global_best_value:\n                        global_best_position = position[i]\n                        global_best_value = current_value\n\n                    if evaluations >= self.budget:\n                        break\n            \n            # Evolutionary operations\n            for i in range(0, self.population_size, 2):\n                if evaluations >= self.budget:\n                    break\n                if np.random.rand() < self.crossover_rate:\n                    parent1 = personal_best_position[i]\n                    parent2 = personal_best_position[(i + 1) % self.population_size]\n                    offspring1, offspring2 = self.crossover(parent1, parent2)\n\n                    self.evaluate_and_update(offspring1, func, personal_best_position, personal_best_value, global_best_position, global_best_value, evaluations, lb, ub)\n                    self.evaluate_and_update(offspring2, func, personal_best_position, personal_best_value, global_best_position, global_best_value, evaluations, lb, ub)\n\n            # Mutation\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n                mutation = self.mutation_scale * np.random.normal(size=self.dim)\n                mutated_position = personal_best_position[i] + mutation\n                mutated_position = np.clip(mutated_position, lb, ub)\n\n                self.evaluate_and_update(mutated_position, func, personal_best_position, personal_best_value, global_best_position, global_best_value, evaluations, lb, ub)\n\n        return global_best_position, global_best_value\n\n    def compute_gradient(self, func, position, lb, ub, epsilon=1e-6):\n        gradient = np.zeros(self.dim)\n        for i in range(self.dim):\n            pos_eps = np.copy(position)\n            neg_eps = np.copy(position)\n            pos_eps[i] += epsilon\n            neg_eps[i] -= epsilon\n            pos_eps = np.clip(pos_eps, lb, ub)\n            neg_eps = np.clip(neg_eps, lb, ub)\n            gradient[i] = (func(pos_eps) - func(neg_eps)) / (2 * epsilon)\n        return gradient\n\n    def crossover(self, parent1, parent2):\n        crossover_point = np.random.randint(1, self.dim - 1)\n        offspring1 = np.concatenate((parent1[:crossover_point], parent2[crossover_point:]))\n        offspring2 = np.concatenate((parent2[:crossover_point], parent1[crossover_point:]))\n        return offspring1, offspring2\n\n    def evaluate_and_update(self, position, func, personal_best_position, personal_best_value, global_best_position, global_best_value, evaluations, lb, ub):\n        current_value = func(position)\n        evaluations += 1\n\n        idx = np.argmin(personal_best_value)\n        if current_value < personal_best_value[idx]:\n            personal_best_position[idx] = position\n            personal_best_value[idx] = current_value\n\n        if current_value < global_best_value:\n            global_best_position = position\n            global_best_value = current_value", "name": "Adaptive_Gradient_Based_Hybrid_Optimizer", "description": "Adaptive Gradient-Based Hybrid Optimizer combines adaptive learning with evolutionary strategies to enhance local search and global exploration capabilities.", "configspace": "", "generation": 98, "fitness": 0.22627794327822814, "feedback": "The algorithm Adaptive_Gradient_Based_Hybrid_Optimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.23 with standard deviation 0.00.", "error": "", "parent_id": "7b9e420c-be1b-4149-aa00-c0ac3f2f80b4", "metadata": {"aucs": [0.22627794327822814, 0.22627794327822814, 0.22627794327822814]}, "mutation_prompt": null}
{"id": "89ff9cd5-920f-47e8-89b2-931cc3c36ad7", "solution": "import numpy as np\n\nclass Quantum_Adaptive_Memetic_Swarm_Optimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 30\n        self.c1 = 1.5\n        self.c2 = 1.5\n        self.w = 0.6\n        self.q_factor = 0.8\n        self.crossover_rate = 0.4\n        self.mutation_scale = 0.05\n        self.inertia_decay = 0.97\n        self.temperature = 0.9\n        self.learning_rate = 0.1  # Adaptive learning rate for local search\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        np.random.seed(42)\n\n        position = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocity = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_position = np.copy(position)\n        personal_best_value = np.array([func(p) for p in personal_best_position])\n        global_best_position = personal_best_position[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n        \n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(2)\n                self.w *= self.inertia_decay\n                velocity[i] = (self.w * velocity[i] +\n                               self.c1 * r1 * (personal_best_position[i] - position[i]) +\n                               self.c2 * r2 * (global_best_position - position[i]))\n                \n                quantum_jump = self.q_factor * np.random.normal(scale=self.mutation_scale, size=self.dim)\n                annealing_shift = np.exp(-evaluations / (self.budget * self.temperature)) * np.random.uniform(-0.5, 0.5, self.dim)\n                \n                position[i] += velocity[i] + quantum_jump + annealing_shift\n                position[i] = np.clip(position[i], lb, ub)\n\n                current_value = func(position[i])\n                evaluations += 1\n                \n                if current_value < personal_best_value[i]:\n                    personal_best_position[i] = position[i]\n                    personal_best_value[i] = current_value\n                \n                if current_value < global_best_value:\n                    global_best_position = position[i]\n                    global_best_value = current_value\n\n                if evaluations >= self.budget:\n                    break\n\n            # Genetic-like Crossover operation\n            for i in range(0, self.population_size, 2):\n                if np.random.rand() < self.crossover_rate:\n                    parent1 = personal_best_position[i]\n                    parent2 = personal_best_position[(i + 1) % self.population_size]\n                    offspring1, offspring2 = self.crossover(parent1, parent2)\n                    \n                    # Evaluate offspring and potentially update personal and global bests\n                    self.evaluate_and_update(offspring1, func, personal_best_position, personal_best_value, global_best_position, global_best_value, evaluations, lb, ub)\n                    self.evaluate_and_update(offspring2, func, personal_best_position, personal_best_value, global_best_position, global_best_value, evaluations, lb, ub)\n\n            # Elite Local Search for top performers\n            elite_indices = np.argsort(personal_best_value)[:self.population_size // 5]\n            for idx in elite_indices:\n                refined_position = self.local_search(personal_best_position[idx], func, lb, ub)\n                refined_value = func(refined_position)\n                if refined_value < personal_best_value[idx]:\n                    personal_best_position[idx] = refined_position\n                    personal_best_value[idx] = refined_value\n                    if refined_value < global_best_value:\n                        global_best_position = refined_position\n                        global_best_value = refined_value\n\n        return global_best_position, global_best_value\n\n    def crossover(self, parent1, parent2):\n        crossover_point = np.random.randint(1, self.dim - 1)\n        offspring1 = np.concatenate((parent1[:crossover_point], parent2[crossover_point:]))\n        offspring2 = np.concatenate((parent2[:crossover_point], parent1[crossover_point:]))\n        return offspring1, offspring2\n\n    def evaluate_and_update(self, position, func, personal_best_position, personal_best_value, global_best_position, global_best_value, evaluations, lb, ub):\n        position = np.clip(position, lb, ub)\n        current_value = func(position)\n        evaluations += 1\n        \n        if current_value < personal_best_value[np.argmin(personal_best_value)]:\n            personal_best_position[np.argmin(personal_best_value)] = position\n            personal_best_value[np.argmin(personal_best_value)] = current_value\n        \n        if current_value < global_best_value:\n            global_best_position = position\n            global_best_value = current_value\n\n    def local_search(self, position, func, lb, ub):\n        for _ in range(5):  # Local search iterations\n            trial_pos = position + self.learning_rate * np.random.uniform(-1, 1, self.dim)\n            trial_pos = np.clip(trial_pos, lb, ub)\n            if func(trial_pos) < func(position):\n                position = trial_pos\n        return position", "name": "Quantum_Adaptive_Memetic_Swarm_Optimizer", "description": "Quantum Adaptive Memetic Swarm Optimizer incorporates adaptive learning rates and elite local search to improve global and local search capabilities.", "configspace": "", "generation": 99, "fitness": 0.2779386154231426, "feedback": "The algorithm Quantum_Adaptive_Memetic_Swarm_Optimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28 with standard deviation 0.00.", "error": "", "parent_id": "7b9e420c-be1b-4149-aa00-c0ac3f2f80b4", "metadata": {"aucs": [0.2779386154231426, 0.2779386154231426, 0.2779386154231426]}, "mutation_prompt": null}
