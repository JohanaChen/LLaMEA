{"id": "8f52be3c-414a-43bc-9b5c-c2fc2736575e", "solution": "import numpy as np\n\nclass AQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.positions = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_values = None\n        self.global_best_position = None\n        self.global_best_value = np.inf\n        self.iteration = 0\n\n    def initialize_particles(self, lb, ub):\n        self.positions = np.random.uniform(lb, ub, (self.budget, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.budget, self.dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_values = np.full(self.budget, np.inf)\n\n    def quantum_behavior(self, position, personal_best, global_best):\n        return position + np.random.uniform(-1, 1, self.dim) * (\n            np.abs(global_best - personal_best) / np.linalg.norm(global_best - personal_best + 1e-10)\n        )\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_particles(lb, ub)\n\n        while self.iteration < self.budget:\n            for i in range(self.budget):\n                # Evaluate the current position\n                current_value = func(self.positions[i])\n                # Update personal best\n                if current_value < self.personal_best_values[i]:\n                    self.personal_best_values[i] = current_value\n                    self.personal_best_positions[i] = self.positions[i].copy()\n                # Update global best\n                if current_value < self.global_best_value:\n                    self.global_best_value = current_value\n                    self.global_best_position = self.positions[i].copy()\n\n            # Update velocities and positions\n            for i in range(self.budget):\n                cognitive = self.quantum_behavior(\n                    self.positions[i], self.personal_best_positions[i], self.global_best_position\n                )\n                self.velocities[i] = cognitive\n                self.positions[i] = np.clip(self.positions[i] + self.velocities[i], lb, ub)\n\n            self.iteration += 1\n\n        return self.global_best_position, self.global_best_value", "name": "AQPSO", "description": "Adaptive Quantum Particle Swarm Optimization (AQPSO) combines quantum computing principles with adaptive behavior to explore the search space efficiently.", "configspace": "", "generation": 0, "fitness": 0.2566240067998336, "feedback": "The algorithm AQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.26 with standard deviation 0.00.", "error": "", "parent_id": null, "metadata": {"aucs": [0.253741388603004, 0.25987239851675226, 0.2562582332797446]}, "mutation_prompt": null}
{"id": "f603702e-bf11-4a00-acc0-807976bd381f", "solution": "import numpy as np\n\nclass QIDE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = max(5, int(budget / 5))\n        self.population = None\n        self.fitness = None\n        self.best_solution = None\n        self.best_value = np.inf\n        self.iteration = 0\n\n    def initialize_population(self, lb, ub):\n        self.population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def quantum_superposition(self, target, best, r1, r2):\n        superposed_state = target + np.random.uniform(-1, 1, self.dim) * (\n            np.abs(best - r1) * np.abs(r2 - best)\n        )\n        return superposed_state\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n\n        while self.iteration < self.budget:\n            F = 0.5 + np.random.rand() * 0.5  # Dynamic scaling factor\n            CR = 0.1 + np.random.rand() * 0.9  # Dynamic crossover rate\n\n            for i in range(self.population_size):\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                r1, r2, r3 = self.population[np.random.choice(indices, 3, replace=False)]\n\n                mutant = r1 + F * (r2 - r3)\n                mutant = np.clip(mutant, lb, ub)\n\n                trial = np.where(np.random.rand(self.dim) < CR, mutant, self.population[i])\n                trial = self.quantum_superposition(trial, self.best_solution or self.population[i], r1, r2)\n                trial = np.clip(trial, lb, ub)\n\n                trial_value = func(trial)\n\n                if trial_value < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_value\n\n                if trial_value < self.best_value:\n                    self.best_solution = trial\n                    self.best_value = trial_value\n\n            self.iteration += self.population_size\n\n        return self.best_solution, self.best_value", "name": "QIDE", "description": "Quantum-Inspired Differential Evolution (QIDE) with Dynamic Parameter Adaptation combines differential evolution with quantum principles and dynamic parameter tuning for efficient exploration and exploitation.", "configspace": "", "generation": 1, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()').", "error": "ValueError('The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()')", "parent_id": "8f52be3c-414a-43bc-9b5c-c2fc2736575e", "metadata": {}, "mutation_prompt": null}
{"id": "c05be72d-5459-4488-a99d-39afd91aca01", "solution": "import numpy as np\n\nclass QIGSA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.positions = None\n        self.velocities = None\n        self.masses = None\n        self.global_best_position = None\n        self.global_best_value = np.inf\n        self.iteration = 0\n\n    def initialize_agents(self, lb, ub):\n        self.positions = np.random.uniform(lb, ub, (self.budget, self.dim))\n        self.velocities = np.zeros((self.budget, self.dim))\n        self.masses = np.ones(self.budget)\n\n    def update_masses(self, values):\n        best_value = np.min(values)\n        worst_value = np.max(values)\n        if best_value == worst_value:\n            self.masses = np.ones(self.budget)\n        else:\n            self.masses = (values - worst_value) / (best_value - worst_value)\n        self.masses /= np.sum(self.masses)\n\n    def quantum_gravitational_attraction(self, position, mass, global_best):\n        direction = np.random.uniform(-1, 1, self.dim)\n        attraction = np.abs(global_best - position) * np.random.rand(self.dim)\n        gravitational_effect = mass * direction * attraction\n        return gravitational_effect\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_agents(lb, ub)\n\n        while self.iteration < self.budget:\n            values = np.array([func(self.positions[i]) for i in range(self.budget)])\n            for i in range(self.budget):\n                # Update global best\n                if values[i] < self.global_best_value:\n                    self.global_best_value = values[i]\n                    self.global_best_position = self.positions[i].copy()\n\n            # Update masses based on current values\n            self.update_masses(values)\n\n            # Update velocities and positions\n            for i in range(self.budget):\n                gravitational_force = self.quantum_gravitational_attraction(\n                    self.positions[i], self.masses[i], self.global_best_position\n                )\n                self.velocities[i] = gravitational_force\n                self.positions[i] = np.clip(self.positions[i] + self.velocities[i], lb, ub)\n\n            self.iteration += 1\n\n        return self.global_best_position, self.global_best_value", "name": "QIGSA", "description": "Quantum-Inspired Gravitational Search Algorithm (QIGSA) merges gravitational attraction principles with quantum behavior to dynamically explore and exploit the search space.", "configspace": "", "generation": 2, "fitness": 0.2566240067998336, "feedback": "The algorithm QIGSA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.26 with standard deviation 0.00.", "error": "", "parent_id": "8f52be3c-414a-43bc-9b5c-c2fc2736575e", "metadata": {"aucs": [0.253741388603004, 0.25987239851675226, 0.2562582332797446]}, "mutation_prompt": null}
{"id": "e45e7047-e77b-4e05-8005-b368edbe647a", "solution": "import numpy as np\n\nclass EQHS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.harmony_memory = None\n        self.harmony_values = None\n        self.best_harmony = None\n        self.best_value = np.inf\n        self.hmcr = 0.9  # Harmony Memory Considering Rate\n        self.par = 0.3   # Pitch Adjusting Rate\n\n    def initialize_harmonies(self, lb, ub):\n        self.harmony_memory = np.random.uniform(lb, ub, (self.budget, self.dim))\n        self.harmony_values = np.full(self.budget, np.inf)\n\n    def quantum_adjustment(self, candidate, best):\n        return candidate + np.random.normal(0, 1, self.dim) * (best - candidate) / 2\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_harmonies(lb, ub)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            for i in range(self.budget):\n                if evaluations >= self.budget:\n                    break\n\n                if np.random.rand() < self.hmcr:\n                    # Consider harmonies in memory\n                    harmony = self.harmony_memory[np.random.randint(self.budget)]\n                else:\n                    # Randomly generate a new harmony\n                    harmony = np.random.uniform(lb, ub, self.dim)\n\n                # Pitch adjustment\n                if np.random.rand() < self.par:\n                    harmony = np.clip(self.quantum_adjustment(harmony, self.best_harmony if self.best_harmony is not None else harmony), lb, ub)\n\n                current_value = func(harmony)\n                evaluations += 1\n\n                # Update harmony memory\n                if current_value < self.harmony_values[i]:\n                    self.harmony_values[i] = current_value\n                    self.harmony_memory[i] = harmony.copy()\n\n                # Update the best harmony\n                if current_value < self.best_value:\n                    self.best_value = current_value\n                    self.best_harmony = harmony.copy()\n\n        return self.best_harmony, self.best_value", "name": "EQHS", "description": "Evolutionary Quantum Harmony Search (EQHS) integrates quantum-inspired principles with harmony search and evolutionary adjustments for enhanced global exploration and exploitation.", "configspace": "", "generation": 3, "fitness": 0.2601416801191483, "feedback": "The algorithm EQHS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.26 with standard deviation 0.00.", "error": "", "parent_id": "8f52be3c-414a-43bc-9b5c-c2fc2736575e", "metadata": {"aucs": [0.2595476293642941, 0.2643587859127735, 0.25651862508037726]}, "mutation_prompt": null}
{"id": "3d33da4f-c0f1-4404-ba7f-62987f7efef1", "solution": "import numpy as np\n\nclass QGDO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(20, budget)\n        self.population = None\n        self.pop_values = None\n        self.best_solution = None\n        self.best_value = np.inf\n        self.mutation_rate = 0.1\n        self.crossover_rate = 0.8\n        self.quantum_step_size = 0.05\n\n    def initialize_population(self, lb, ub):\n        self.population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        self.pop_values = np.full(self.population_size, np.inf)\n\n    def quantum_mutation(self, candidate, lb, ub):\n        step = np.random.normal(0, self.quantum_step_size, self.dim)\n        mutated = candidate + step * np.sign(self.best_solution - candidate)\n        return np.clip(mutated, lb, ub)\n\n    def crossover(self, parent1, parent2):\n        if np.random.rand() < self.crossover_rate:\n            crossover_point = np.random.randint(1, self.dim - 1)\n            child1 = np.concatenate((parent1[:crossover_point], parent2[crossover_point:]))\n            child2 = np.concatenate((parent2[:crossover_point], parent1[crossover_point:]))\n            return child1, child2\n        else:\n            return parent1, parent2\n\n    def select_parents(self):\n        idx = np.random.choice(self.population_size, 2, replace=False)\n        return self.population[idx[0]], self.population[idx[1]]\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            new_population = []\n            for _ in range(self.population_size // 2):\n                parent1, parent2 = self.select_parents()\n                child1, child2 = self.crossover(parent1, parent2)\n\n                if np.random.rand() < self.mutation_rate:\n                    child1 = self.quantum_mutation(child1, lb, ub)\n\n                if np.random.rand() < self.mutation_rate:\n                    child2 = self.quantum_mutation(child2, lb, ub)\n\n                new_population.extend([child1, child2])\n\n            for i, individual in enumerate(new_population):\n                if evaluations >= self.budget:\n                    break\n                current_value = func(individual)\n                evaluations += 1\n\n                if current_value < self.pop_values[i]:\n                    self.pop_values[i] = current_value\n                    self.population[i] = individual.copy()\n\n                if current_value < self.best_value:\n                    self.best_value = current_value\n                    self.best_solution = individual.copy()\n\n            self.population = np.array(new_population)\n\n        return self.best_solution, self.best_value", "name": "QGDO", "description": "Quantum Genetic Diversity Optimization (QGDO) leverages quantum principles and genetic diversity mechanisms to enhance exploration and convergence in complex search spaces.", "configspace": "", "generation": 4, "fitness": -Infinity, "feedback": "An exception occurred: TypeError(\"unsupported operand type(s) for -: 'NoneType' and 'float'\").", "error": "TypeError(\"unsupported operand type(s) for -: 'NoneType' and 'float'\")", "parent_id": "e45e7047-e77b-4e05-8005-b368edbe647a", "metadata": {}, "mutation_prompt": null}
{"id": "374c7c1b-5647-4fb1-8339-ccec9ab98756", "solution": "import numpy as np\n\nclass EQHSPlus:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.harmony_memory = None\n        self.harmony_values = None\n        self.best_harmony = None\n        self.best_value = np.inf\n        self.hmcr_initial = 0.9  # Initial Harmony Memory Considering Rate\n        self.par_initial = 0.3   # Initial Pitch Adjusting Rate\n        self.hmcr_final = 0.95   # Final Harmony Memory Considering Rate\n        self.par_final = 0.1     # Final Pitch Adjusting Rate\n        self.memory_size = min(100, budget)  # Initial size of the harmony memory\n\n    def initialize_harmonies(self, lb, ub):\n        self.harmony_memory = np.random.uniform(lb, ub, (self.memory_size, self.dim))\n        self.harmony_values = np.full(self.memory_size, np.inf)\n\n    def quantum_adjustment(self, candidate, best):\n        return candidate + np.random.normal(0, 1, self.dim) * (best - candidate) / 2\n\n    def adapt_rates(self, evaluations):\n        fraction = evaluations / self.budget\n        self.hmcr = self.hmcr_initial + fraction * (self.hmcr_final - self.hmcr_initial)\n        self.par = self.par_initial + fraction * (self.par_final - self.par_initial)\n\n    def resize_population(self, evaluations):\n        new_size = int(self.memory_size * (1 + evaluations / self.budget))\n        if new_size > self.memory_size:\n            self.harmony_memory = np.concatenate(\n                [self.harmony_memory, np.random.uniform(lb, ub, (new_size - self.memory_size, self.dim))])\n            self.harmony_values = np.concatenate(\n                [self.harmony_values, np.full(new_size - self.memory_size, np.inf)])\n            self.memory_size = new_size\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_harmonies(lb, ub)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            self.adapt_rates(evaluations)\n            self.resize_population(evaluations)\n\n            for i in range(self.memory_size):\n                if evaluations >= self.budget:\n                    break\n\n                if np.random.rand() < self.hmcr:\n                    # Consider harmonies in memory\n                    harmony = self.harmony_memory[np.random.randint(self.memory_size)]\n                else:\n                    # Randomly generate a new harmony\n                    harmony = np.random.uniform(lb, ub, self.dim)\n\n                # Pitch adjustment\n                if np.random.rand() < self.par:\n                    harmony = np.clip(self.quantum_adjustment(harmony, self.best_harmony if self.best_harmony is not None else harmony), lb, ub)\n\n                current_value = func(harmony)\n                evaluations += 1\n\n                # Update harmony memory\n                if current_value < self.harmony_values[i]:\n                    self.harmony_values[i] = current_value\n                    self.harmony_memory[i] = harmony.copy()\n\n                # Update the best harmony\n                if current_value < self.best_value:\n                    self.best_value = current_value\n                    self.best_harmony = harmony.copy()\n\n        return self.best_harmony, self.best_value", "name": "EQHSPlus", "description": "Enhanced Quantum Harmony Search (EQHS+) introduces adaptive memory consideration and pitch adjustment rates, along with dynamic population resizing to improve convergence efficiency and solution quality.", "configspace": "", "generation": 5, "fitness": -Infinity, "feedback": "An exception occurred: NameError(\"name 'lb' is not defined\").", "error": "NameError(\"name 'lb' is not defined\")", "parent_id": "e45e7047-e77b-4e05-8005-b368edbe647a", "metadata": {}, "mutation_prompt": null}
{"id": "280633d7-86f5-4a1f-8d57-6107c43c6280", "solution": "import numpy as np\n\nclass AMQHS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.harmony_memory = None\n        self.harmony_values = None\n        self.best_harmony = None\n        self.best_value = np.inf\n        self.hmcr = 0.9  # Harmony Memory Considering Rate\n        self.par = 0.3   # Pitch Adjusting Rate\n        self.local_search_rate = 0.1  # Probability of applying local search\n        self.memory_size = min(50, budget)  # Fixed memory size for better management\n\n    def initialize_harmonies(self, lb, ub):\n        self.harmony_memory = np.random.uniform(lb, ub, (self.memory_size, self.dim))\n        self.harmony_values = np.full(self.memory_size, np.inf)\n\n    def quantum_adjustment(self, candidate, best):\n        return candidate + np.random.normal(0, 1, self.dim) * (best - candidate) / 2\n\n    def local_search(self, harmony, lb, ub):\n        step_size = 0.1 * (ub - lb)\n        perturbation = np.random.uniform(-step_size, step_size, self.dim)\n        new_harmony = harmony + perturbation\n        return np.clip(new_harmony, lb, ub)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_harmonies(lb, ub)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            for i in range(self.memory_size):\n                if evaluations >= self.budget:\n                    break\n\n                if np.random.rand() < self.hmcr:\n                    # Consider harmonies in memory\n                    harmony = self.harmony_memory[np.random.randint(self.memory_size)]\n                else:\n                    # Randomly generate a new harmony\n                    harmony = np.random.uniform(lb, ub, self.dim)\n\n                # Pitch adjustment\n                if np.random.rand() < self.par:\n                    harmony = np.clip(self.quantum_adjustment(harmony, self.best_harmony if self.best_harmony is not None else harmony), lb, ub)\n\n                # Apply local search with a set probability\n                if np.random.rand() < self.local_search_rate:\n                    harmony = self.local_search(harmony, lb, ub)\n\n                current_value = func(harmony)\n                evaluations += 1\n\n                # Update harmony memory if necessary\n                if current_value < self.harmony_values[i]:\n                    self.harmony_values[i] = current_value\n                    self.harmony_memory[i] = harmony.copy()\n\n                # Update the best harmony found\n                if current_value < self.best_value:\n                    self.best_value = current_value\n                    self.best_harmony = harmony.copy()\n\n        return self.best_harmony, self.best_value", "name": "AMQHS", "description": "Adaptive Memetic Quantum Harmony Search (AMQHS) combines adaptive memory and local search techniques with quantum adjustments for enhanced convergence in complex optimization landscapes.", "configspace": "", "generation": 6, "fitness": 0.2701837141030611, "feedback": "The algorithm AMQHS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.27 with standard deviation 0.00.", "error": "", "parent_id": "e45e7047-e77b-4e05-8005-b368edbe647a", "metadata": {"aucs": [0.2747334077492076, 0.26919777691464397, 0.26661995764533164]}, "mutation_prompt": null}
{"id": "ea8703f2-3c86-439f-b5c0-2673bb1ce9fa", "solution": "import numpy as np\n\nclass EAMQHS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.harmony_memory = None\n        self.harmony_values = None\n        self.best_harmony = None\n        self.best_value = np.inf\n        self.hmcr = 0.95  # Increased Harmony Memory Considering Rate\n        self.par = 0.15   # Reduced Pitch Adjusting Rate for finer adjustments\n        self.local_search_rate = 0.2  # Increased probability of applying local search\n        self.memory_size = min(50, budget)  # Fixed memory size for better management\n        self.dynamic_par_step = 0.01  # Dynamic adjustment for pitch rate\n\n    def initialize_harmonies(self, lb, ub):\n        self.harmony_memory = np.random.uniform(lb, ub, (self.memory_size, self.dim))\n        self.harmony_values = np.full(self.memory_size, np.inf)\n\n    def quantum_adjustment(self, candidate, best):\n        return candidate + np.random.normal(0, 1, self.dim) * (best - candidate) / 2\n\n    def dynamic_pitch_adjustment(self, harmony, lb, ub):\n        adjustment_strength = np.exp(-self.dim / self.memory_size)\n        noise = np.random.uniform(-adjustment_strength, adjustment_strength, self.dim)\n        new_harmony = harmony + noise\n        self.par = min(0.5, self.par + self.dynamic_par_step * np.random.uniform(-1, 1))  # Dynamic adjustment\n        return np.clip(new_harmony, lb, ub)\n\n    def intensified_local_search(self, harmony, lb, ub):\n        step_size = 0.05 * (ub - lb)  # Smaller step size for intensified search\n        perturbation = np.random.normal(0, step_size, self.dim)\n        new_harmony = harmony + perturbation\n        return np.clip(new_harmony, lb, ub)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_harmonies(lb, ub)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            for i in range(self.memory_size):\n                if evaluations >= self.budget:\n                    break\n\n                if np.random.rand() < self.hmcr:\n                    harmony = self.harmony_memory[np.random.randint(self.memory_size)]\n                else:\n                    harmony = np.random.uniform(lb, ub, self.dim)\n\n                if np.random.rand() < self.par:\n                    harmony = self.dynamic_pitch_adjustment(harmony, lb, ub)\n\n                if np.random.rand() < self.local_search_rate:\n                    harmony = self.intensified_local_search(harmony, lb, ub)\n\n                current_value = func(harmony)\n                evaluations += 1\n\n                if current_value < self.harmony_values[i]:\n                    self.harmony_values[i] = current_value\n                    self.harmony_memory[i] = harmony.copy()\n\n                if current_value < self.best_value:\n                    self.best_value = current_value\n                    self.best_harmony = harmony.copy()\n\n        return self.best_harmony, self.best_value", "name": "EAMQHS", "description": "Enhanced Adaptive Memetic Quantum Harmony Search (EAMQHS) incorporates adaptive memory, quantum adjustments, dynamic pitch correction, and intensified local search to achieve improved convergence in complex optimization landscapes.", "configspace": "", "generation": 7, "fitness": 0.27414648692947174, "feedback": "The algorithm EAMQHS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.27 with standard deviation 0.00.", "error": "", "parent_id": "280633d7-86f5-4a1f-8d57-6107c43c6280", "metadata": {"aucs": [0.27548908226292446, 0.2724076828149615, 0.27454269571052925]}, "mutation_prompt": null}
{"id": "de349508-6f34-490e-bf45-c429c5b081f9", "solution": "import numpy as np\n\nclass QHS_AMR:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.harmony_memory = None\n        self.harmony_values = None\n        self.best_harmony = None\n        self.best_value = np.inf\n        self.hmcr = 0.9  # Harmonized memory consideration rate\n        self.par = 0.2   # Pitch adjusting rate\n        self.local_search_rate = 0.3  # Probability of local search\n        self.memory_size = min(50, budget)\n        self.dynamic_par_step = 0.02\n\n    def initialize_harmonies(self, lb, ub):\n        self.harmony_memory = np.random.uniform(lb, ub, (self.memory_size, self.dim))\n        self.harmony_values = np.full(self.memory_size, np.inf)\n\n    def quantum_adjustment(self, candidate, best):\n        return candidate + np.random.normal(0, 1, self.dim) * (best - candidate) / 1.5\n\n    def dynamic_pitch_adjustment(self, harmony, lb, ub):\n        adjustment_strength = np.exp(-self.dim / self.memory_size)\n        noise = np.random.uniform(-adjustment_strength, adjustment_strength, self.dim)\n        new_harmony = harmony + noise\n        self.par = min(0.5, self.par + self.dynamic_par_step * np.random.uniform(-1, 1))\n        return np.clip(new_harmony, lb, ub)\n\n    def intensified_local_search(self, harmony, lb, ub):\n        step_size = 0.03 * (ub - lb)\n        perturbation = np.random.normal(0, step_size, self.dim)\n        new_harmony = harmony + perturbation\n        return np.clip(new_harmony, lb, ub)\n\n    def adaptive_local_search(self, harmony, lb, ub, iteration, max_iterations):\n        step_size = (0.02 + 0.03 * (1 - iteration / max_iterations)) * (ub - lb)\n        perturbation = np.random.normal(0, step_size, self.dim)\n        new_harmony = harmony + perturbation\n        return np.clip(new_harmony, lb, ub)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_harmonies(lb, ub)\n        evaluations = 0\n        max_iterations = self.budget // self.memory_size\n\n        for iteration in range(max_iterations):\n            for i in range(self.memory_size):\n                if evaluations >= self.budget:\n                    break\n\n                if np.random.rand() < self.hmcr:\n                    harmony = self.harmony_memory[np.random.randint(self.memory_size)]\n                else:\n                    harmony = np.random.uniform(lb, ub, self.dim)\n\n                if np.random.rand() < self.par:\n                    harmony = self.dynamic_pitch_adjustment(harmony, lb, ub)\n\n                if np.random.rand() < self.local_search_rate:\n                    if np.random.rand() < 0.5:\n                        harmony = self.intensified_local_search(harmony, lb, ub)\n                    else:\n                        harmony = self.adaptive_local_search(harmony, lb, ub, iteration, max_iterations)\n\n                current_value = func(harmony)\n                evaluations += 1\n\n                if current_value < self.harmony_values[i]:\n                    self.harmony_values[i] = current_value\n                    self.harmony_memory[i] = harmony.copy()\n\n                if current_value < self.best_value:\n                    self.best_value = current_value\n                    self.best_harmony = harmony.copy()\n\n        return self.best_harmony, self.best_value", "name": "QHS_AMR", "description": "Quantum Harmony Search with Adaptive Memetic Refinement (QHS-AMR) enhances convergence by integrating adaptive quantum adjustment, dynamic learning, and global-to-local pitch refinement to tackle complex optimization tasks.", "configspace": "", "generation": 8, "fitness": 0.27347099710086054, "feedback": "The algorithm QHS_AMR got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.27 with standard deviation 0.00.", "error": "", "parent_id": "ea8703f2-3c86-439f-b5c0-2673bb1ce9fa", "metadata": {"aucs": [0.27574546048661164, 0.27222270850597663, 0.27244482230999334]}, "mutation_prompt": null}
{"id": "90e6df98-2d66-47f4-a7ab-b59745f766e0", "solution": "import numpy as np\n\nclass QIDSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particles = None\n        self.velocities = None\n        self.best_personal_positions = None\n        self.best_personal_values = None\n        self.global_best_position = None\n        self.global_best_value = np.inf\n        self.c1 = 1.5  # Cognitive component\n        self.c2 = 1.5  # Social component\n        self.inertia_weight = 0.7\n        self.quantum_prob = 0.2\n        self.swarm_size = min(40, budget)\n    \n    def initialize_particles(self, lb, ub):\n        self.particles = np.random.uniform(lb, ub, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.swarm_size, self.dim)) * (ub - lb) * 0.1\n        self.best_personal_positions = self.particles.copy()\n        self.best_personal_values = np.full(self.swarm_size, np.inf)\n\n    def quantum_position_update(self, position, best_position, lb, ub):\n        direction = np.random.uniform(-1, 1, self.dim) * (best_position - position)\n        new_position = position + direction * np.random.randn(self.dim)\n        return np.clip(new_position, lb, ub)\n\n    def dynamic_velocity_update(self, i):\n        inertia = self.inertia_weight * self.velocities[i]\n        cognitive = self.c1 * np.random.rand(self.dim) * (self.best_personal_positions[i] - self.particles[i])\n        social = self.c2 * np.random.rand(self.dim) * (self.global_best_position - self.particles[i])\n        return inertia + cognitive + social\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_particles(lb, ub)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            for i in range(self.swarm_size):\n                if evaluations >= self.budget:\n                    break\n\n                if np.random.rand() < self.quantum_prob:\n                    new_position = self.quantum_position_update(self.particles[i], self.global_best_position, lb, ub)\n                else:\n                    self.velocities[i] = self.dynamic_velocity_update(i)\n                    new_position = self.particles[i] + self.velocities[i]\n                    new_position = np.clip(new_position, lb, ub)\n\n                current_value = func(new_position)\n                evaluations += 1\n\n                if current_value < self.best_personal_values[i]:\n                    self.best_personal_values[i] = current_value\n                    self.best_personal_positions[i] = new_position.copy()\n\n                if current_value < self.global_best_value:\n                    self.global_best_value = current_value\n                    self.global_best_position = new_position.copy()\n\n        return self.global_best_position, self.global_best_value", "name": "QIDSO", "description": "Quantum-Inspired Dynamic Swarm Optimization (QIDSO) combines quantum-inspired position updates with dynamic swarm behavior to enhance exploration and exploitation in complex optimization landscapes.", "configspace": "", "generation": 9, "fitness": -Infinity, "feedback": "An exception occurred: TypeError(\"unsupported operand type(s) for -: 'NoneType' and 'float'\").", "error": "TypeError(\"unsupported operand type(s) for -: 'NoneType' and 'float'\")", "parent_id": "ea8703f2-3c86-439f-b5c0-2673bb1ce9fa", "metadata": {}, "mutation_prompt": null}
{"id": "1814cb67-e098-46df-a98c-e5254f95bf43", "solution": "import numpy as np\n\nclass QEDE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(100, budget)  # Population size for diverse search\n        self.f_mutation = 0.8  # Mutation factor\n        self.cr_crossover = 0.9  # Crossover rate\n        self.best_solution = None\n        self.best_value = np.inf\n\n    def initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def quantum_position_update(self, position, best, lb, ub):\n        \"\"\"Quantum-inspired position update.\"\"\"\n        shift = np.random.normal(0, 1, self.dim) * (best - position) / 2\n        new_position = position + shift\n        return np.clip(new_position, lb, ub)\n\n    def differential_mutation(self, population, best_idx, lb, ub):\n        idxs = np.random.choice(population.shape[0], 3, replace=False)\n        x1, x2, x3 = population[idxs]\n        mutant_vector = x1 + self.f_mutation * (x2 - x3)\n        mutant_vector = np.clip(mutant_vector, lb, ub)\n        \n        if np.random.rand() < 0.5:\n            mutant_vector = self.quantum_position_update(mutant_vector, population[best_idx], lb, ub)\n        \n        return mutant_vector\n\n    def crossover(self, target, mutant):\n        crossover_mask = np.random.rand(self.dim) < self.cr_crossover\n        trial_vector = np.where(crossover_mask, mutant, target)\n        return trial_vector\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self.initialize_population(lb, ub)\n        population_values = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(population_values)\n        evaluations = self.population_size\n        \n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                mutant_vector = self.differential_mutation(population, best_idx, lb, ub)\n                trial_vector = self.crossover(population[i], mutant_vector)\n\n                trial_value = func(trial_vector)\n                evaluations += 1\n\n                if trial_value < population_values[i]:\n                    population[i] = trial_vector\n                    population_values[i] = trial_value\n                    if trial_value < population_values[best_idx]:\n                        best_idx = i\n\n            self.best_solution, self.best_value = population[best_idx], population_values[best_idx]\n        \n        return self.best_solution, self.best_value", "name": "QEDE", "description": "Quantum-Enhanced Differential Evolution (QEDE) combines quantum-inspired position updates with traditional differential evolution techniques to exploit both global exploration and local refinement capabilities for enhanced convergence.", "configspace": "", "generation": 10, "fitness": 0.26086843955764816, "feedback": "The algorithm QEDE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.26 with standard deviation 0.00.", "error": "", "parent_id": "ea8703f2-3c86-439f-b5c0-2673bb1ce9fa", "metadata": {"aucs": [0.2632196742569908, 0.2581983692008821, 0.26118727521507157]}, "mutation_prompt": null}
{"id": "d68d5acd-c9a2-470c-b9ad-0aa6d1b8553f", "solution": "import numpy as np\n\nclass QILFS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(50, budget // 2)\n        self.alpha = 0.1  # Exploration factor in Levy flight\n        self.beta = 1.5   # Parameter for Levy distribution\n        self.population = None\n        self.population_values = None\n        self.best_solution = None\n        self.best_value = np.inf\n\n    def levy_flight(self, scale=0.1):\n        sigma1 = ((np.math.gamma(1 + self.beta) * np.sin(np.pi * self.beta / 2)) /\n                  (np.math.gamma((1 + self.beta) / 2) * self.beta * 2 ** ((self.beta - 1) / 2))) ** (1 / self.beta)\n        u = np.random.normal(0, sigma1, self.dim)\n        v = np.random.normal(0, 1, self.dim)\n        step = u / abs(v) ** (1 / self.beta)\n        return step * scale\n    \n    def quantum_update(self, current, best):\n        return current + np.random.normal(0, 1, self.dim) * (best - current)\n\n    def initialize_population(self, lb, ub):\n        self.population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        self.population_values = np.full(self.population_size, np.inf)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n                \n                candidate = self.population[i]\n                \n                if np.random.rand() < 0.5:\n                    candidate = self.quantum_update(candidate, self.best_solution if self.best_solution is not None else candidate)\n                else:\n                    candidate += self.levy_flight(self.alpha * (ub - lb))\n                \n                candidate = np.clip(candidate, lb, ub)\n                candidate_value = func(candidate)\n                evaluations += 1\n\n                if candidate_value < self.population_values[i]:\n                    self.population_values[i] = candidate_value\n                    self.population[i] = candidate.copy()\n\n                if candidate_value < self.best_value:\n                    self.best_value = candidate_value\n                    self.best_solution = candidate.copy()\n\n        return self.best_solution, self.best_value", "name": "QILFS", "description": "Quantum-Inspired Levy Flight Search (QILFS) combines quantum-inspired position updates with Levy flight perturbations to enhance exploration and exploitation in complex optimization spaces.", "configspace": "", "generation": 11, "fitness": 0.2481661748683874, "feedback": "The algorithm QILFS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.25 with standard deviation 0.00.", "error": "", "parent_id": "ea8703f2-3c86-439f-b5c0-2673bb1ce9fa", "metadata": {"aucs": [0.24607038038800721, 0.2502392372063814, 0.24818890701077356]}, "mutation_prompt": null}
{"id": "c076badb-61f6-4100-8361-558dd47fab27", "solution": "import numpy as np\n\nclass QEAHS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.harmony_memory = None\n        self.harmony_values = None\n        self.best_harmony = None\n        self.best_value = np.inf\n        self.hmcr = 0.9  # Harmony Memory Considering Rate\n        self.par = 0.25  # Pitch Adjusting Rate\n        self.local_search_rate = 0.3  # Probability of applying local search\n        self.memory_size = min(50, budget)\n        self.beta = 0.5  # Probability of quantum adjustment\n\n    def initialize_harmonies(self, lb, ub):\n        self.harmony_memory = np.random.uniform(lb, ub, (self.memory_size, self.dim))\n        self.harmony_values = np.full(self.memory_size, np.inf)\n\n    def quantum_superposition(self, candidate, best):\n        direction = best - candidate\n        step = np.random.uniform(-self.beta, self.beta, self.dim) * direction\n        return candidate + step\n\n    def dynamic_pitch_adjustment(self, harmony, lb, ub):\n        noise = np.random.uniform(-1, 1, self.dim)\n        new_harmony = harmony + self.par * noise * (ub - lb)\n        return np.clip(new_harmony, lb, ub)\n\n    def intensified_local_search(self, harmony, lb, ub):\n        step_size = 0.1 * (ub - lb)\n        perturbation = np.random.normal(0, step_size, self.dim)\n        new_harmony = harmony + perturbation\n        return np.clip(new_harmony, lb, ub)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_harmonies(lb, ub)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            for i in range(self.memory_size):\n                if evaluations >= self.budget:\n                    break\n\n                if np.random.rand() < self.hmcr:\n                    harmony = self.harmony_memory[np.random.randint(self.memory_size)]\n                else:\n                    harmony = np.random.uniform(lb, ub, self.dim)\n\n                if np.random.rand() < self.beta:\n                    harmony = self.quantum_superposition(harmony, self.best_harmony if self.best_harmony is not None else harmony)\n\n                if np.random.rand() < self.par:\n                    harmony = self.dynamic_pitch_adjustment(harmony, lb, ub)\n\n                if np.random.rand() < self.local_search_rate:\n                    harmony = self.intensified_local_search(harmony, lb, ub)\n\n                current_value = func(harmony)\n                evaluations += 1\n\n                if current_value < self.harmony_values[i]:\n                    self.harmony_values[i] = current_value\n                    self.harmony_memory[i] = harmony.copy()\n\n                if current_value < self.best_value:\n                    self.best_value = current_value\n                    self.best_harmony = harmony.copy()\n\n        return self.best_harmony, self.best_value", "name": "QEAHS", "description": "Quantum-Enhanced Adaptive Harmony Search (QEAHS) leverages quantum superposition and dynamic parameter adjustment for improved convergence efficiency in diverse optimization landscapes.", "configspace": "", "generation": 12, "fitness": 0.2655041914094172, "feedback": "The algorithm QEAHS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.27 with standard deviation 0.00.", "error": "", "parent_id": "ea8703f2-3c86-439f-b5c0-2673bb1ce9fa", "metadata": {"aucs": [0.26045762925448523, 0.2642421822647594, 0.2718127627090071]}, "mutation_prompt": null}
{"id": "d0c0e622-23ff-4544-bc2b-6164276563d4", "solution": "import numpy as np\n\nclass EQHS_ADE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.harmony_memory = None\n        self.harmony_values = None\n        self.best_harmony = None\n        self.best_value = np.inf\n        self.hmcr = 0.95\n        self.par = 0.15\n        self.local_search_rate = 0.2\n        self.memory_size = min(50, budget)\n        self.dynamic_par_step = 0.01\n        self.F = 0.5  # Scaling Factor for DE\n        self.CR = 0.9  # Crossover Rate for DE\n\n    def initialize_harmonies(self, lb, ub):\n        self.harmony_memory = np.random.uniform(lb, ub, (self.memory_size, self.dim))\n        self.harmony_values = np.full(self.memory_size, np.inf)\n\n    def quantum_adjustment(self, candidate, best):\n        return candidate + np.random.normal(0, 1, self.dim) * (best - candidate) / 2\n\n    def dynamic_pitch_adjustment(self, harmony, lb, ub):\n        adjustment_strength = np.exp(-self.dim / self.memory_size)\n        noise = np.random.uniform(-adjustment_strength, adjustment_strength, self.dim)\n        new_harmony = harmony + noise\n        self.par = min(0.5, self.par + self.dynamic_par_step * np.random.uniform(-1, 1))\n        return np.clip(new_harmony, lb, ub)\n\n    def differential_evolution(self, target, lb, ub):\n        indices = np.random.choice(self.memory_size, 3, replace=False)\n        x1, x2, x3 = self.harmony_memory[indices]\n        mutant_vector = np.clip(x1 + self.F * (x2 - x3), lb, ub)\n        crossover = np.random.rand(self.dim) < self.CR\n        trial_vector = np.where(crossover, mutant_vector, target)\n        return trial_vector\n\n    def intensified_local_search(self, harmony, lb, ub):\n        step_size = 0.05 * (ub - lb)\n        perturbation = np.random.normal(0, step_size, self.dim)\n        new_harmony = harmony + perturbation\n        return np.clip(new_harmony, lb, ub)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_harmonies(lb, ub)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            for i in range(self.memory_size):\n                if evaluations >= self.budget:\n                    break\n\n                if np.random.rand() < self.hmcr:\n                    harmony = self.harmony_memory[np.random.randint(self.memory_size)]\n                else:\n                    harmony = np.random.uniform(lb, ub, self.dim)\n\n                if np.random.rand() < self.par:\n                    harmony = self.dynamic_pitch_adjustment(harmony, lb, ub)\n\n                if np.random.rand() < self.local_search_rate:\n                    harmony = self.intensified_local_search(harmony, lb, ub)\n\n                trial_vector = self.differential_evolution(harmony, lb, ub)\n                trial_value = func(trial_vector)\n                evaluations += 1\n\n                if trial_value < self.harmony_values[i]:\n                    self.harmony_values[i] = trial_value\n                    self.harmony_memory[i] = trial_vector.copy()\n\n                if trial_value < self.best_value:\n                    self.best_value = trial_value\n                    self.best_harmony = trial_vector.copy()\n\n        return self.best_harmony, self.best_value", "name": "EQHS_ADE", "description": "Enhanced Quantum Harmony Search with Adaptive Differential Evolution (EQHS-ADE) combines quantum harmony search with adaptive differential evolution to enhance exploration and exploitation through dynamic parameter tuning.", "configspace": "", "generation": 13, "fitness": 0.2755827548129442, "feedback": "The algorithm EQHS_ADE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28 with standard deviation 0.00.", "error": "", "parent_id": "ea8703f2-3c86-439f-b5c0-2673bb1ce9fa", "metadata": {"aucs": [0.27502268967760635, 0.27636729126953274, 0.27535828349169356]}, "mutation_prompt": null}
{"id": "fa0f3b04-2706-47b4-851c-3fc783fdbd02", "solution": "import numpy as np\n\nclass HCS_AOBL:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.nest_size = min(30, budget // 2)\n        self.pa = 0.25  # Abandonment rate\n        self.best_nest = None\n        self.best_value = np.inf\n        self.step_size = 0.01  # Learning step size\n\n    def levy_flight(self):\n        beta = 1.5\n        sigma = (np.gamma(1 + beta) * np.sin(np.pi * beta / 2) / \n                 (np.gamma((1 + beta) / 2) * beta * 2 ** ((beta - 1) / 2))) ** (1 / beta)\n        u = np.random.normal(0, sigma, self.dim)\n        v = np.random.normal(0, 1, self.dim)\n        step = u / abs(v) ** (1 / beta)\n        return step\n\n    def opposite_sol(self, candidate, lb, ub):\n        opposite = lb + (ub - candidate)\n        return np.clip(opposite, lb, ub)\n\n    def init_nests(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.nest_size, self.dim))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        nests = self.init_nests(lb, ub)\n        nest_values = np.full(self.nest_size, np.inf)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            for i in range(self.nest_size):\n                if evaluations >= self.budget:\n                    break\n                \n                candidate = nests[i] + self.step_size * self.levy_flight()\n                candidate = np.clip(candidate, lb, ub)\n                opposite_candidate = self.opposite_sol(candidate, lb, ub)\n\n                candidate_value = func(candidate)\n                opposite_value = func(opposite_candidate)\n                evaluations += 2\n\n                if candidate_value < nest_values[i]:\n                    nest_values[i] = candidate_value\n                    nests[i] = candidate\n                if opposite_value < nest_values[i]:\n                    nest_values[i] = opposite_value\n                    nests[i] = opposite_candidate\n\n                if candidate_value < self.best_value:\n                    self.best_value = candidate_value\n                    self.best_nest = candidate.copy()\n                if opposite_value < self.best_value:\n                    self.best_value = opposite_value\n                    self.best_nest = opposite_candidate.copy()\n\n            abandon = np.random.rand(self.nest_size) < self.pa\n            for j in range(self.nest_size):\n                if abandon[j]:\n                    nests[j] = np.random.uniform(lb, ub, self.dim)\n\n        return self.best_nest, self.best_value", "name": "HCS_AOBL", "description": "Hybrid Cuckoo Search with Adaptive Opposition-Based Learning (HCS-AOBL) integrates Cuckoo Search with adaptive opposition-based learning to enhance diversity, balance exploration and exploitation, and improve convergence speed.", "configspace": "", "generation": 14, "fitness": -Infinity, "feedback": "An exception occurred: AttributeError(\"module 'numpy' has no attribute 'gamma'\").", "error": "AttributeError(\"module 'numpy' has no attribute 'gamma'\")", "parent_id": "d0c0e622-23ff-4544-bc2b-6164276563d4", "metadata": {}, "mutation_prompt": null}
{"id": "cdcb17d4-c069-4990-8786-a2d91383c617", "solution": "import numpy as np\n\nclass EQHS_ADE_DSAL:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.harmony_memory = None\n        self.harmony_values = None\n        self.best_harmony = None\n        self.best_value = np.inf\n        self.hmcr = 0.95\n        self.par = 0.15\n        self.local_search_rate = 0.2\n        self.memory_size = min(50, budget)\n        self.dynamic_par_step = 0.01\n        self.F = 0.5  # Scaling Factor for DE\n        self.CR = 0.9  # Crossover Rate for DE\n        self.learning_rate = 0.1  # Learning rate for self-adaptive adjustments\n        self.diversity_threshold = 1e-5  # Threshold for diversity calculation\n\n    def initialize_harmonies(self, lb, ub):\n        self.harmony_memory = np.random.uniform(lb, ub, (self.memory_size, self.dim))\n        self.harmony_values = np.full(self.memory_size, np.inf)\n\n    def quantum_adjustment(self, candidate, best):\n        return candidate + np.random.normal(0, 1, self.dim) * (best - candidate) / 2\n\n    def dynamic_pitch_adjustment(self, harmony, lb, ub):\n        adjustment_strength = np.exp(-self.dim / self.memory_size)\n        noise = np.random.uniform(-adjustment_strength, adjustment_strength, self.dim)\n        new_harmony = harmony + noise\n        self.par = min(0.5, self.par + self.dynamic_par_step * np.random.uniform(-1, 1))\n        return np.clip(new_harmony, lb, ub)\n\n    def differential_evolution(self, target, lb, ub):\n        indices = np.random.choice(self.memory_size, 3, replace=False)\n        x1, x2, x3 = self.harmony_memory[indices]\n        mutant_vector = np.clip(x1 + self.F * (x2 - x3), lb, ub)\n        crossover = np.random.rand(self.dim) < self.CR\n        trial_vector = np.where(crossover, mutant_vector, target)\n        return trial_vector\n\n    def intensified_local_search(self, harmony, lb, ub):\n        step_size = 0.05 * (ub - lb)\n        perturbation = np.random.normal(0, step_size, self.dim)\n        new_harmony = harmony + perturbation\n        return np.clip(new_harmony, lb, ub)\n\n    def self_adaptive_learning(self):\n        # Increase learning rate if diversity is low\n        diversity = np.std(self.harmony_memory, axis=0).mean()\n        if diversity < self.diversity_threshold:\n            self.learning_rate *= 1.1\n        else:\n            self.learning_rate *= 0.9\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_harmonies(lb, ub)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            self.self_adaptive_learning()\n            for i in range(self.memory_size):\n                if evaluations >= self.budget:\n                    break\n\n                if np.random.rand() < self.hmcr:\n                    harmony = self.harmony_memory[np.random.randint(self.memory_size)]\n                else:\n                    harmony = np.random.uniform(lb, ub, self.dim)\n\n                if np.random.rand() < self.par:\n                    harmony = self.dynamic_pitch_adjustment(harmony, lb, ub)\n\n                if np.random.rand() < self.local_search_rate:\n                    harmony = self.intensified_local_search(harmony, lb, ub)\n\n                trial_vector = self.differential_evolution(harmony, lb, ub)\n                trial_value = func(trial_vector)\n                evaluations += 1\n\n                if trial_value < self.harmony_values[i]:\n                    self.harmony_values[i] = trial_value\n                    self.harmony_memory[i] = trial_vector.copy()\n\n                if trial_value < self.best_value:\n                    self.best_value = trial_value\n                    self.best_harmony = trial_vector.copy()\n\n        return self.best_harmony, self.best_value", "name": "EQHS_ADE_DSAL", "description": "Enhanced Quantum Harmony Search with Adaptive Differential Evolution and Dynamic Self-Adaptive Learning (EQHS-ADE-DSAL) incorporates self-adaptive learning rates and diversity preservation mechanisms to improve exploration and convergence rates in photonic structure optimization.", "configspace": "", "generation": 15, "fitness": 0.2755827548129442, "feedback": "The algorithm EQHS_ADE_DSAL got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28 with standard deviation 0.00.", "error": "", "parent_id": "d0c0e622-23ff-4544-bc2b-6164276563d4", "metadata": {"aucs": [0.27502268967760635, 0.27636729126953274, 0.27535828349169356]}, "mutation_prompt": null}
{"id": "387d942c-6258-4a6f-81fe-b53f9609005d", "solution": "import numpy as np\n\nclass HQHS_ADE_AM:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.harmony_memory = None\n        self.harmony_values = None\n        self.best_harmony = None\n        self.best_value = np.inf\n        self.hmcr = 0.95\n        self.par = 0.15\n        self.local_search_rate = 0.2\n        self.initial_memory_size = min(50, budget)\n        self.dynamic_memory_change = 5\n        self.memory_size = self.initial_memory_size\n        self.dynamic_par_step = 0.01\n        self.F = 0.5  # Scaling Factor for DE\n        self.CR = 0.9  # Crossover Rate for DE\n\n    def initialize_harmonies(self, lb, ub):\n        self.harmony_memory = np.random.uniform(lb, ub, (self.memory_size, self.dim))\n        self.harmony_values = np.full(self.memory_size, np.inf)\n\n    def adaptive_memory_adjustment(self):\n        # Increase memory size dynamically\n        if self.best_value < np.percentile(self.harmony_values, 25):\n            self.memory_size = min(self.memory_size + self.dynamic_memory_change, self.initial_memory_size * 2)\n        # Decrease memory size if improvement stalls\n        elif self.best_value > np.median(self.harmony_values):\n            self.memory_size = max(self.initial_memory_size, self.memory_size - self.dynamic_memory_change)\n\n        self.harmony_memory = self.harmony_memory[:self.memory_size]\n        self.harmony_values = self.harmony_values[:self.memory_size]\n\n    def quantum_adjustment(self, candidate, best):\n        return candidate + np.random.normal(0, 1, self.dim) * (best - candidate) / 2\n\n    def dynamic_pitch_adjustment(self, harmony, lb, ub):\n        adjustment_strength = np.exp(-self.dim / self.memory_size)\n        noise = np.random.uniform(-adjustment_strength, adjustment_strength, self.dim)\n        new_harmony = harmony + noise\n        self.par = min(0.5, self.par + self.dynamic_par_step * np.random.uniform(-1, 1))\n        return np.clip(new_harmony, lb, ub)\n\n    def differential_evolution(self, target, lb, ub):\n        indices = np.random.choice(self.memory_size, 3, replace=False)\n        x1, x2, x3 = self.harmony_memory[indices]\n        mutant_vector = np.clip(x1 + self.F * (x2 - x3), lb, ub)\n        crossover = np.random.rand(self.dim) < self.CR\n        trial_vector = np.where(crossover, mutant_vector, target)\n        return trial_vector\n\n    def intensified_local_search(self, harmony, lb, ub):\n        step_size = 0.05 * (ub - lb)\n        perturbation = np.random.normal(0, step_size, self.dim)\n        new_harmony = harmony + perturbation\n        return np.clip(new_harmony, lb, ub)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_harmonies(lb, ub)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            self.adaptive_memory_adjustment()  # Adjust memory size dynamically\n            for i in range(self.memory_size):\n                if evaluations >= self.budget:\n                    break\n\n                if np.random.rand() < self.hmcr:\n                    harmony = self.harmony_memory[np.random.randint(self.memory_size)]\n                else:\n                    harmony = np.random.uniform(lb, ub, self.dim)\n\n                if np.random.rand() < self.par:\n                    harmony = self.dynamic_pitch_adjustment(harmony, lb, ub)\n\n                if np.random.rand() < self.local_search_rate:\n                    harmony = self.intensified_local_search(harmony, lb, ub)\n\n                trial_vector = self.differential_evolution(harmony, lb, ub)\n                trial_value = func(trial_vector)\n                evaluations += 1\n\n                if trial_value < self.harmony_values[i]:\n                    self.harmony_values[i] = trial_value\n                    self.harmony_memory[i] = trial_vector.copy()\n\n                if trial_value < self.best_value:\n                    self.best_value = trial_value\n                    self.best_harmony = trial_vector.copy()\n\n        return self.best_harmony, self.best_value", "name": "HQHS_ADE_AM", "description": "Hybrid Quantum Harmony Search with Adaptive Differential Evolution and Adaptive Memory (HQHS-ADE-AM) integrates adaptive memory to dynamically adjust harmony memory size and enhance convergence through refined memory and evolutionary mechanisms.", "configspace": "", "generation": 16, "fitness": -Infinity, "feedback": "An exception occurred: IndexError('index 53 is out of bounds for axis 0 with size 50').", "error": "IndexError('index 53 is out of bounds for axis 0 with size 50')", "parent_id": "d0c0e622-23ff-4544-bc2b-6164276563d4", "metadata": {}, "mutation_prompt": null}
{"id": "9b2d1d3e-fa0d-4abf-b5ed-af364bc62bc8", "solution": "import numpy as np\n\nclass QIDHS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.harmony_memory = None\n        self.harmony_values = None\n        self.best_harmony = None\n        self.best_value = np.inf\n        self.hmcr = 0.95  # Harmony Memory Considering Rate\n        self.par = 0.3  # Pitch Adjustment Rate\n        self.memory_size = min(50, budget)\n        self.F = 0.5  # Scaling Factor for DE\n        self.CR = 0.9  # Crossover Rate for DE\n        self.dynamic_par_step = 0.01\n        self.local_search_rate = 0.3\n\n    def initialize_harmonies(self, lb, ub):\n        self.harmony_memory = np.random.uniform(lb, ub, (self.memory_size, self.dim))\n        self.harmony_values = np.full(self.memory_size, np.inf)\n\n    def quantum_adjustment(self, candidate, best):\n        dist = np.linalg.norm(candidate - best)\n        return candidate + np.random.normal(0, 1, self.dim) * (best - candidate) / (1 + dist)\n\n    def dynamic_pitch_adjustment(self, harmony, lb, ub):\n        adjustment_strength = np.exp(-np.sqrt(self.dim) / self.memory_size)\n        noise = np.random.uniform(-adjustment_strength, adjustment_strength, self.dim)\n        new_harmony = harmony + noise\n        self.par = min(0.5, self.par + self.dynamic_par_step * np.random.uniform(-1, 1))\n        return np.clip(new_harmony, lb, ub)\n\n    def differential_evolution(self, target, lb, ub):\n        indices = np.random.choice(self.memory_size, 3, replace=False)\n        x1, x2, x3 = self.harmony_memory[indices]\n        mutant_vector = np.clip(x1 + self.F * (x2 - x3), lb, ub)\n        crossover = np.random.rand(self.dim) < self.CR\n        trial_vector = np.where(crossover, mutant_vector, target)\n        return trial_vector\n\n    def intensified_local_search(self, harmony, lb, ub):\n        step_size = 0.05 * (ub - lb)\n        perturbation = np.random.normal(0, step_size, self.dim)\n        new_harmony = harmony + perturbation\n        return np.clip(new_harmony, lb, ub)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_harmonies(lb, ub)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            for i in range(self.memory_size):\n                if evaluations >= self.budget:\n                    break\n\n                if np.random.rand() < self.hmcr:\n                    harmony = self.harmony_memory[np.random.randint(self.memory_size)]\n                else:\n                    harmony = np.random.uniform(lb, ub, self.dim)\n\n                if np.random.rand() < self.par:\n                    harmony = self.dynamic_pitch_adjustment(harmony, lb, ub)\n\n                if np.random.rand() < self.local_search_rate:\n                    harmony = self.intensified_local_search(harmony, lb, ub)\n\n                quantum_harmony = self.quantum_adjustment(harmony, self.best_harmony or harmony)\n                trial_vector = self.differential_evolution(quantum_harmony, lb, ub)\n                trial_value = func(trial_vector)\n                evaluations += 1\n\n                if trial_value < self.harmony_values[i]:\n                    self.harmony_values[i] = trial_value\n                    self.harmony_memory[i] = trial_vector.copy()\n\n                if trial_value < self.best_value:\n                    self.best_value = trial_value\n                    self.best_harmony = trial_vector.copy()\n\n        return self.best_harmony, self.best_value", "name": "QIDHS", "description": "Quantum-Inspired Dynamic Harmony Search combines quantum principles with dynamic parameter adaptation to enhance global exploration and local exploitation in optimization.", "configspace": "", "generation": 17, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()').", "error": "ValueError('The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()')", "parent_id": "d0c0e622-23ff-4544-bc2b-6164276563d4", "metadata": {}, "mutation_prompt": null}
{"id": "967d0aac-6824-45f5-a074-4366c8d0defe", "solution": "import numpy as np\n\nclass QIMO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(50, budget)\n        self.population = None\n        self.fitness = None\n        self.best_solution = None\n        self.best_fitness = np.inf\n        self.quantum_amplitude = 0.1\n        self.local_search_prob = 0.3\n        self.mutation_rate = 0.05\n\n    def initialize_population(self, lb, ub):\n        self.population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def quantum_search(self, candidate, best):\n        direction = np.random.choice([-1, 1], size=self.dim)\n        step = self.quantum_amplitude * np.random.random(self.dim)\n        new_candidate = candidate + direction * step * (best - candidate)\n        return new_candidate\n\n    def memetic_local_search(self, solution, lb, ub):\n        perturbation = np.random.normal(0, 0.1, self.dim)\n        new_solution = solution + perturbation\n        return np.clip(new_solution, lb, ub)\n\n    def mutate(self, candidate, lb, ub):\n        mutation_vector = candidate + self.mutation_rate * np.random.normal(0, 1, self.dim)\n        return np.clip(mutation_vector, lb, ub)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                # Select a candidate and apply quantum-inspired search\n                candidate = self.population[i]\n                new_candidate = self.quantum_search(candidate, self.best_solution if self.best_solution is not None else candidate)\n\n                # Apply memetic local search with some probability\n                if np.random.rand() < self.local_search_prob:\n                    new_candidate = self.memetic_local_search(new_candidate, lb, ub)\n\n                # Evaluate the new candidate\n                new_fitness = func(new_candidate)\n                evaluations += 1\n\n                # Apply mutation\n                if np.random.rand() < self.mutation_rate:\n                    new_candidate = self.mutate(new_candidate, lb, ub)\n                    new_fitness = func(new_candidate)\n                    evaluations += 1\n\n                # Update population and best solution if applicable\n                if new_fitness < self.fitness[i]:\n                    self.fitness[i] = new_fitness\n                    self.population[i] = new_candidate.copy()\n\n                if new_fitness < self.best_fitness:\n                    self.best_fitness = new_fitness\n                    self.best_solution = new_candidate.copy()\n\n        return self.best_solution, self.best_fitness", "name": "QIMO", "description": "Quantum-Inspired Memetic Optimization Algorithm (QIMO) integrates quantum-inspired search strategies with memetic local enhancement to balance exploration and exploitation in high-dimensional optimization tasks.", "configspace": "", "generation": 18, "fitness": 0.2700985832336732, "feedback": "The algorithm QIMO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.27 with standard deviation 0.00.", "error": "", "parent_id": "d0c0e622-23ff-4544-bc2b-6164276563d4", "metadata": {"aucs": [0.2701947668853498, 0.27049212496608643, 0.2696088578495833]}, "mutation_prompt": null}
{"id": "48bb98a5-36ad-42de-a1d3-ed20cec5a26a", "solution": "import numpy as np\n\nclass QEAHS_CS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.memory_size = min(50, budget)\n        self.harmony_memory = None\n        self.harmony_values = None\n        self.best_harmony = None\n        self.best_value = np.inf\n        self.hmcr = 0.9\n        self.par = 0.3\n        self.F = 0.5\n        self.CR = 0.7\n\n    def initialize_harmonies(self, lb, ub):\n        self.harmony_memory = np.random.uniform(lb, ub, (self.memory_size, self.dim))\n        self.harmony_values = np.array([np.inf] * self.memory_size)\n\n    def quantum_perturbation(self, candidate):\n        distance = np.random.normal(0, 1, self.dim)\n        return candidate + distance * np.random.random(self.dim)\n\n    def adaptive_adjustment(self, harmony, lb, ub):\n        noise = np.random.uniform(-1, 1, self.dim) * (ub - lb) * 0.05\n        return np.clip(harmony + noise, lb, ub)\n\n    def coevolutionary_update(self, lb, ub):\n        indices = np.random.choice(self.memory_size, 2, replace=False)\n        x1, x2 = self.harmony_memory[indices]\n        trial_vector = x1 + self.F * (x2 - x1)\n        trial_vector = np.clip(trial_vector, lb, ub)\n        return trial_vector\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_harmonies(lb, ub)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            for i in range(self.memory_size):\n                if evaluations >= self.budget:\n                    break\n\n                if np.random.rand() < self.hmcr:\n                    harmony = self.harmony_memory[np.random.randint(self.memory_size)]\n                else:\n                    harmony = np.random.uniform(lb, ub, self.dim)\n\n                if np.random.rand() < self.par:\n                    harmony = self.adaptive_adjustment(harmony, lb, ub)\n\n                trial_vector = self.coevolutionary_update(lb, ub)\n                trial_value = func(trial_vector)\n                evaluations += 1\n\n                if trial_value < self.harmony_values[i]:\n                    self.harmony_values[i] = trial_value\n                    self.harmony_memory[i] = trial_vector.copy()\n\n                if trial_value < self.best_value:\n                    self.best_value = trial_value\n                    self.best_harmony = trial_vector.copy()\n\n        return self.best_harmony, self.best_value", "name": "QEAHS_CS", "description": "Quantum-Enhanced Adaptive Harmony Search with Coevolutionary Strategies (QEAHS-CS) integrates quantum-inspired exploration with adaptive harmony memory and coevolutionary updates to enhance diversity and robustness in solution finding.", "configspace": "", "generation": 19, "fitness": 0.2640635690668956, "feedback": "The algorithm QEAHS_CS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.26 with standard deviation 0.01.", "error": "", "parent_id": "d0c0e622-23ff-4544-bc2b-6164276563d4", "metadata": {"aucs": [0.2684819024091113, 0.2711938641743371, 0.25251494061723845]}, "mutation_prompt": null}
{"id": "fb5fcc4d-288d-497d-bfe6-f17499575bb8", "solution": "import numpy as np\n\nclass QDEHS_ADE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.harmony_memory = None\n        self.harmony_values = None\n        self.best_harmony = None\n        self.best_value = np.inf\n        self.hmcr = 0.95\n        self.par = 0.2\n        self.local_search_rate = 0.25\n        self.memory_size = min(50, budget)\n        self.dynamic_par_step = 0.01\n        self.F = 0.5\n        self.CR = 0.9\n        self.quantum_step_factor = 0.1  # Quantum diversity factor\n\n    def initialize_harmonies(self, lb, ub):\n        self.harmony_memory = np.random.uniform(lb, ub, (self.memory_size, self.dim))\n        self.harmony_values = np.full(self.memory_size, np.inf)\n\n    def quantum_adjustment(self, candidate, best, lb, ub):\n        quantum_step = self.quantum_step_factor * np.random.uniform(-1, 1, self.dim)\n        new_candidate = candidate + quantum_step * (best - candidate)\n        return np.clip(new_candidate, lb, ub)\n\n    def dynamic_pitch_adjustment(self, harmony, lb, ub):\n        adjustment_strength = np.exp(-self.dim / self.memory_size)\n        noise = np.random.uniform(-adjustment_strength, adjustment_strength, self.dim)\n        new_harmony = harmony + noise\n        self.par = min(0.5, self.par + self.dynamic_par_step * np.random.uniform(-1, 1))\n        return np.clip(new_harmony, lb, ub)\n\n    def differential_evolution(self, target, lb, ub):\n        indices = np.random.choice(self.memory_size, 3, replace=False)\n        x1, x2, x3 = self.harmony_memory[indices]\n        mutant_vector = np.clip(x1 + self.F * (x2 - x3), lb, ub)\n        crossover = np.random.rand(self.dim) < self.CR\n        trial_vector = np.where(crossover, mutant_vector, target)\n        return trial_vector\n\n    def intensified_local_search(self, harmony, lb, ub):\n        step_size = 0.05 * (ub - lb) * np.random.uniform(0.5, 1.5, self.dim)\n        perturbation = np.random.normal(0, step_size, self.dim)\n        new_harmony = harmony + perturbation\n        return np.clip(new_harmony, lb, ub)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_harmonies(lb, ub)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            for i in range(self.memory_size):\n                if evaluations >= self.budget:\n                    break\n\n                if np.random.rand() < self.hmcr:\n                    harmony = self.harmony_memory[np.random.randint(self.memory_size)]\n                else:\n                    harmony = np.random.uniform(lb, ub, self.dim)\n\n                if np.random.rand() < self.par:\n                    harmony = self.dynamic_pitch_adjustment(harmony, lb, ub)\n\n                if np.random.rand() < self.local_search_rate:\n                    harmony = self.intensified_local_search(harmony, lb, ub)\n\n                harmony = self.quantum_adjustment(harmony, self.best_harmony if self.best_harmony is not None else harmony, lb, ub)\n\n                trial_vector = self.differential_evolution(harmony, lb, ub)\n                trial_value = func(trial_vector)\n                evaluations += 1\n\n                if trial_value < self.harmony_values[i]:\n                    self.harmony_values[i] = trial_value\n                    self.harmony_memory[i] = trial_vector.copy()\n\n                if trial_value < self.best_value:\n                    self.best_value = trial_value\n                    self.best_harmony = trial_vector.copy()\n\n        return self.best_harmony, self.best_value", "name": "QDEHS_ADE", "description": "Quantum Diversity Enhanced Harmony Search with Adaptive Differential Evolution (QDEHS-ADE) introduces a quantum diversity mechanism to maintain solution variety and prevent premature convergence, improving the balance between exploration and exploitation.", "configspace": "", "generation": 20, "fitness": 0.2748114487234253, "feedback": "The algorithm QDEHS_ADE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.27 with standard deviation 0.00.", "error": "", "parent_id": "d0c0e622-23ff-4544-bc2b-6164276563d4", "metadata": {"aucs": [0.27641784621085996, 0.27477134768862, 0.27324515227079593]}, "mutation_prompt": null}
{"id": "29d9d993-c020-42c6-9efc-1f973a9fe983", "solution": "import numpy as np\n\nclass AQHS_EDE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.harmony_memory = None\n        self.harmony_values = None\n        self.best_harmony = None\n        self.best_value = np.inf\n        self.hmcr = 0.95\n        self.par = 0.15\n        self.local_search_rate = 0.2\n        self.memory_size = min(50, budget)\n        self.dynamic_par_step = 0.01\n        self.F = 0.5  # Scaling Factor for DE\n        self.CR = 0.9  # Crossover Rate for DE\n        self.adaptive_F_step = 0.05\n        self.adaptive_CR_step = 0.05\n\n    def initialize_harmonies(self, lb, ub):\n        self.harmony_memory = np.random.uniform(lb, ub, (self.memory_size, self.dim))\n        self.harmony_values = np.full(self.memory_size, np.inf)\n\n    def quantum_adjustment(self, candidate, best):\n        return candidate + np.random.normal(0, 1, self.dim) * (best - candidate) / 2\n\n    def dynamic_pitch_adjustment(self, harmony, lb, ub):\n        adjustment_strength = np.exp(-self.dim / self.memory_size)\n        noise = np.random.uniform(-adjustment_strength, adjustment_strength, self.dim)\n        new_harmony = harmony + noise\n        self.par = min(0.5, self.par + self.dynamic_par_step * np.random.uniform(-1, 1))\n        return np.clip(new_harmony, lb, ub)\n\n    def differential_evolution(self, target, lb, ub):\n        indices = np.random.choice(self.memory_size, 3, replace=False)\n        x1, x2, x3 = self.harmony_memory[indices]\n        mutant_vector = np.clip(x1 + self.F * (x2 - x3), lb, ub)\n        crossover = np.random.rand(self.dim) < self.CR\n        trial_vector = np.where(crossover, mutant_vector, target)\n        return trial_vector\n\n    def intensified_local_search(self, harmony, lb, ub):\n        step_size = 0.05 * (ub - lb)\n        perturbation = np.random.normal(0, step_size, self.dim)\n        new_harmony = harmony + perturbation\n        return np.clip(new_harmony, lb, ub)\n\n    def adaptive_parameter_adjustment(self, success_rate):\n        self.F = min(1.0, self.F + self.adaptive_F_step * (success_rate - 0.5))\n        self.CR = min(1.0, self.CR + self.adaptive_CR_step * (success_rate - 0.5))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_harmonies(lb, ub)\n        evaluations = 0\n        successful_runs = 0\n\n        while evaluations < self.budget:\n            for i in range(self.memory_size):\n                if evaluations >= self.budget:\n                    break\n\n                if np.random.rand() < self.hmcr:\n                    harmony = self.harmony_memory[np.random.randint(self.memory_size)]\n                else:\n                    harmony = np.random.uniform(lb, ub, self.dim)\n\n                if np.random.rand() < self.par:\n                    harmony = self.dynamic_pitch_adjustment(harmony, lb, ub)\n\n                if np.random.rand() < self.local_search_rate:\n                    harmony = self.intensified_local_search(harmony, lb, ub)\n\n                trial_vector = self.differential_evolution(harmony, lb, ub)\n                trial_value = func(trial_vector)\n                evaluations += 1\n\n                if trial_value < self.harmony_values[i]:\n                    self.harmony_values[i] = trial_value\n                    self.harmony_memory[i] = trial_vector.copy()\n                    successful_runs += 1\n\n                if trial_value < self.best_value:\n                    self.best_value = trial_value\n                    self.best_harmony = trial_vector.copy()\n\n            success_rate = successful_runs / self.memory_size\n            self.adaptive_parameter_adjustment(success_rate)\n            successful_runs = 0\n\n        return self.best_harmony, self.best_value", "name": "AQHS_EDE", "description": "Adaptive Quantum Harmony Search with Enhanced Differential Evolution (AQHS-EDE) implements an adaptive adjustment of harmony memory and differential evolution parameters for improved convergence in diverse optimization landscapes.", "configspace": "", "generation": 21, "fitness": 0.2741740317887254, "feedback": "The algorithm AQHS_EDE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.27 with standard deviation 0.00.", "error": "", "parent_id": "d0c0e622-23ff-4544-bc2b-6164276563d4", "metadata": {"aucs": [0.270543616030157, 0.276047638479875, 0.27593084085614417]}, "mutation_prompt": null}
{"id": "2f12f1a5-8810-400d-9a4f-558598d83dbf", "solution": "import numpy as np\n\nclass QIAHS_SLS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.harmony_memory = None\n        self.harmony_values = None\n        self.best_harmony = None\n        self.best_value = np.inf\n        self.hmcr = 0.95\n        self.par = 0.15\n        self.memory_size = min(50, budget)\n        self.dynamic_par_step = 0.01\n        self.F = 0.5  # Scaling Factor for DE\n        self.CR = 0.9  # Crossover Rate for DE\n        self.stochastic_rate = 0.3  # Adjusted Stochastic Rate\n\n    def initialize_harmonies(self, lb, ub):\n        self.harmony_memory = np.random.uniform(lb, ub, (self.memory_size, self.dim))\n        self.harmony_values = np.full(self.memory_size, np.inf)\n\n    def quantum_adjustment(self, candidate, best):\n        q_factor = np.random.rand(self.dim)\n        return candidate + q_factor * (best - candidate)\n\n    def adaptive_pitch_adjustment(self, harmony, lb, ub):\n        adjustment_strength = np.exp(-self.dim / self.memory_size)\n        noise = np.random.uniform(-adjustment_strength, adjustment_strength, self.dim)\n        new_harmony = harmony + noise\n        self.par = min(0.5, self.par + self.dynamic_par_step * np.random.uniform(-1, 1))\n        return np.clip(new_harmony, lb, ub)\n\n    def stochastic_local_search(self, harmony, lb, ub):\n        step_size = np.random.uniform(0.01, 0.1) * (ub - lb)\n        perturbation = np.random.normal(0, step_size, self.dim)\n        new_harmony = harmony + perturbation\n        return np.clip(new_harmony, lb, ub)\n\n    def differential_evolution(self, target, lb, ub):\n        indices = np.random.choice(self.memory_size, 3, replace=False)\n        x1, x2, x3 = self.harmony_memory[indices]\n        mutant_vector = np.clip(x1 + self.F * (x2 - x3), lb, ub)\n        crossover = np.random.rand(self.dim) < self.CR\n        trial_vector = np.where(crossover, mutant_vector, target)\n        return trial_vector\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_harmonies(lb, ub)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            for i in range(self.memory_size):\n                if evaluations >= self.budget:\n                    break\n\n                if np.random.rand() < self.hmcr:\n                    harmony = self.harmony_memory[np.random.randint(self.memory_size)]\n                else:\n                    harmony = np.random.uniform(lb, ub, self.dim)\n\n                if np.random.rand() < self.par:\n                    harmony = self.adaptive_pitch_adjustment(harmony, lb, ub)\n\n                if np.random.rand() < self.stochastic_rate:\n                    harmony = self.stochastic_local_search(harmony, lb, ub)\n\n                trial_vector = self.differential_evolution(harmony, lb, ub)\n                trial_value = func(trial_vector)\n                evaluations += 1\n\n                if trial_value < self.harmony_values[i]:\n                    self.harmony_values[i] = trial_value\n                    self.harmony_memory[i] = trial_vector.copy()\n\n                if trial_value < self.best_value:\n                    self.best_value = trial_value\n                    self.best_harmony = trial_vector.copy()\n\n        return self.best_harmony, self.best_value", "name": "QIAHS_SLS", "description": "Quantum-Inspired Adaptive Harmony Search with Stochastic Local Search (QIAHS-SLS) introduces quantum-based probabilistic exploration with adaptive search intensification through stochastic local refinement to balance search globality and locality.", "configspace": "", "generation": 22, "fitness": 0.2749547168594363, "feedback": "The algorithm QIAHS_SLS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.27 with standard deviation 0.00.", "error": "", "parent_id": "d0c0e622-23ff-4544-bc2b-6164276563d4", "metadata": {"aucs": [0.276870677791357, 0.27548805910797936, 0.2725054136789725]}, "mutation_prompt": null}
{"id": "27ab3ba9-2744-41ce-a521-ba82377a0a5f", "solution": "import numpy as np\n\nclass EQHS_ADE_MF:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.harmony_memory = None\n        self.harmony_values = None\n        self.best_harmony = None\n        self.best_value = np.inf\n        self.hmcr = 0.95\n        self.par = 0.15\n        self.local_search_rate = 0.2\n        self.memory_size = min(50, budget)\n        self.dynamic_par_step = 0.01\n        self.F = 0.5  # Scaling Factor for DE\n        self.CR = 0.9  # Crossover Rate for DE\n        self.feedback_frequency = 10\n        self.feedback_intensity = 0.05\n\n    def initialize_harmonies(self, lb, ub):\n        self.harmony_memory = np.random.uniform(lb, ub, (self.memory_size, self.dim))\n        self.harmony_values = np.full(self.memory_size, np.inf)\n\n    def quantum_adjustment(self, candidate, best):\n        return candidate + np.random.normal(0, 1, self.dim) * (best - candidate) / 2\n\n    def dynamic_pitch_adjustment(self, harmony, lb, ub):\n        adjustment_strength = np.exp(-self.dim / self.memory_size)\n        noise = np.random.uniform(-adjustment_strength, adjustment_strength, self.dim)\n        new_harmony = harmony + noise\n        self.par = min(0.5, self.par + self.dynamic_par_step * np.random.uniform(-1, 1))\n        return np.clip(new_harmony, lb, ub)\n\n    def differential_evolution(self, target, lb, ub):\n        indices = np.random.choice(self.memory_size, 3, replace=False)\n        x1, x2, x3 = self.harmony_memory[indices]\n        mutant_vector = np.clip(x1 + self.F * (x2 - x3), lb, ub)\n        crossover = np.random.rand(self.dim) < self.CR\n        trial_vector = np.where(crossover, mutant_vector, target)\n        return trial_vector\n\n    def intensified_local_search(self, harmony, lb, ub):\n        step_size = 0.05 * (ub - lb)\n        perturbation = np.random.normal(0, step_size, self.dim)\n        new_harmony = harmony + perturbation\n        return np.clip(new_harmony, lb, ub)\n\n    def multi_layer_feedback(self, evaluations):\n        if evaluations % self.feedback_frequency == 0:\n            successful_adaptation = (self.best_value - np.mean(self.harmony_values)) / np.std(self.harmony_values)\n            adjustment_factor = np.tanh(successful_adaptation) * self.feedback_intensity\n            self.F = np.clip(self.F + adjustment_factor, 0.1, 0.9)\n            self.CR = np.clip(self.CR + adjustment_factor, 0.1, 0.9)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_harmonies(lb, ub)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            for i in range(self.memory_size):\n                if evaluations >= self.budget:\n                    break\n\n                if np.random.rand() < self.hmcr:\n                    harmony = self.harmony_memory[np.random.randint(self.memory_size)]\n                else:\n                    harmony = np.random.uniform(lb, ub, self.dim)\n\n                if np.random.rand() < self.par:\n                    harmony = self.dynamic_pitch_adjustment(harmony, lb, ub)\n\n                if np.random.rand() < self.local_search_rate:\n                    harmony = self.intensified_local_search(harmony, lb, ub)\n\n                trial_vector = self.differential_evolution(harmony, lb, ub)\n                trial_value = func(trial_vector)\n                evaluations += 1\n\n                if trial_value < self.harmony_values[i]:\n                    self.harmony_values[i] = trial_value\n                    self.harmony_memory[i] = trial_vector.copy()\n\n                if trial_value < self.best_value:\n                    self.best_value = trial_value\n                    self.best_harmony = trial_vector.copy()\n\n                self.multi_layer_feedback(evaluations)\n\n        return self.best_harmony, self.best_value", "name": "EQHS_ADE_MF", "description": "Enhanced Quantum Harmony Search with Adaptive Differential Evolution and Multi-layer Feedback (EQHS-ADE-MF) introduces a multi-layer feedback mechanism to improve convergence by dynamically updating search parameters based on performance.", "configspace": "", "generation": 23, "fitness": 0.26813452888930406, "feedback": "The algorithm EQHS_ADE_MF got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.27 with standard deviation 0.00.", "error": "", "parent_id": "d0c0e622-23ff-4544-bc2b-6164276563d4", "metadata": {"aucs": [0.27234825711510724, 0.2705225587305947, 0.26153277082221027]}, "mutation_prompt": null}
{"id": "5dc5b40d-5667-4741-8530-03973c3c40d2", "solution": "import numpy as np\n\nclass QIASO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = min(50, budget)\n        self.positions = None\n        self.velocities = None\n        self.best_personal_positions = None\n        self.best_personal_values = None\n        self.best_global_position = None\n        self.best_global_value = np.inf\n        self.w = 0.5  # Inertia weight\n        self.c1 = 1.5  # Cognitive (personal) coefficient\n        self.c2 = 1.5  # Social (global) coefficient\n        self.q_factor = 0.1  # Quantum factor for position update\n\n    def initialize_swarm(self, lb, ub):\n        self.positions = np.random.uniform(lb, ub, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-abs(ub-lb), abs(ub-lb), (self.swarm_size, self.dim))\n        self.best_personal_positions = self.positions.copy()\n        self.best_personal_values = np.full(self.swarm_size, np.inf)\n\n    def quantum_update(self, position, best):\n        delta = np.random.normal(0, self.q_factor, self.dim)\n        quantum_position = position + delta * (best - position)\n        return quantum_position\n\n    def adaptive_parameter_control(self, iteration, max_iterations):\n        self.w = 0.4 + 0.5 * (1 - iteration / max_iterations)\n        self.c1 = 2.0 - 1.5 * (iteration / max_iterations)\n        self.c2 = 0.5 + 1.5 * (iteration / max_iterations)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_swarm(lb, ub)\n        evaluations = 0\n        iteration = 0\n        max_iterations = self.budget // self.swarm_size\n\n        while evaluations < self.budget:\n            self.adaptive_parameter_control(iteration, max_iterations)\n\n            for i in range(self.swarm_size):\n                if evaluations >= self.budget:\n                    break\n\n                # Update velocity\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                cognitive_component = self.c1 * r1 * (self.best_personal_positions[i] - self.positions[i])\n                social_component = self.c2 * r2 * (self.best_global_position - self.positions[i])\n                self.velocities[i] = self.w * self.velocities[i] + cognitive_component + social_component\n\n                # Update position\n                self.positions[i] = self.positions[i] + self.velocities[i]\n                self.positions[i] = self.quantum_update(self.positions[i], self.best_global_position)\n                self.positions[i] = np.clip(self.positions[i], lb, ub)\n\n                # Evaluate new position\n                current_value = func(self.positions[i])\n                evaluations += 1\n\n                # Update personal best\n                if current_value < self.best_personal_values[i]:\n                    self.best_personal_values[i] = current_value\n                    self.best_personal_positions[i] = self.positions[i].copy()\n\n                # Update global best\n                if current_value < self.best_global_value:\n                    self.best_global_value = current_value\n                    self.best_global_position = self.positions[i].copy()\n\n            iteration += 1\n\n        return self.best_global_position, self.best_global_value", "name": "QIASO", "description": "Quantum-Inspired Adaptive Swarm Optimization (QIASO) integrates quantum behavior and adaptive parameter tuning into swarm-based optimization for enhanced global search capability.", "configspace": "", "generation": 24, "fitness": -Infinity, "feedback": "An exception occurred: TypeError(\"unsupported operand type(s) for -: 'NoneType' and 'float'\").", "error": "TypeError(\"unsupported operand type(s) for -: 'NoneType' and 'float'\")", "parent_id": "d0c0e622-23ff-4544-bc2b-6164276563d4", "metadata": {}, "mutation_prompt": null}
{"id": "a227af08-2912-4028-84ce-716798932f6b", "solution": "import numpy as np\n\nclass QAHSDDE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.harmony_memory = None\n        self.harmony_values = None\n        self.best_harmony = None\n        self.best_value = np.inf\n        self.hmcr = 0.95\n        self.par = 0.15\n        self.local_search_rate = 0.25\n        self.memory_size = min(50, budget)\n        self.dynamic_par_step = 0.01\n        self.F = 0.5  # Scaling Factor for DE\n        self.CR = 0.9  # Crossover Rate for DE\n        self.alpha = 0.8  # Quantum adjustment rate\n\n    def initialize_harmonies(self, lb, ub):\n        self.harmony_memory = np.random.uniform(lb, ub, (self.memory_size, self.dim))\n        self.harmony_values = np.full(self.memory_size, np.inf)\n\n    def quantum_adjustment(self, candidate, best):\n        return candidate + np.random.normal(0, self.alpha, self.dim) * (best - candidate) / 2\n\n    def dynamic_pitch_adjustment(self, harmony, lb, ub):\n        adjustment_strength = np.exp(-self.dim / self.memory_size)\n        noise = np.random.uniform(-adjustment_strength, adjustment_strength, self.dim)\n        new_harmony = harmony + noise\n        self.par = min(0.5, self.par + self.dynamic_par_step * np.random.uniform(-1, 1))\n        return np.clip(new_harmony, lb, ub)\n\n    def differential_evolution(self, target, lb, ub):\n        indices = np.random.choice(self.memory_size, 3, replace=False)\n        x1, x2, x3 = self.harmony_memory[indices]\n        mutant_vector = np.clip(x1 + self.F * (x2 - x3), lb, ub)\n        crossover = np.random.rand(self.dim) < self.CR\n        trial_vector = np.where(crossover, mutant_vector, target)\n        return trial_vector\n\n    def intensified_local_search(self, harmony, lb, ub):\n        step_size = 0.1 * (ub - lb)\n        perturbation = np.random.normal(0, step_size, self.dim)\n        new_harmony = harmony + perturbation\n        return np.clip(new_harmony, lb, ub)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_harmonies(lb, ub)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            for i in range(self.memory_size):\n                if evaluations >= self.budget:\n                    break\n\n                if np.random.rand() < self.hmcr:\n                    harmony = self.harmony_memory[np.random.randint(self.memory_size)]\n                else:\n                    harmony = np.random.uniform(lb, ub, self.dim)\n\n                if np.random.rand() < self.par:\n                    harmony = self.dynamic_pitch_adjustment(harmony, lb, ub)\n\n                if np.random.rand() < self.local_search_rate:\n                    harmony = self.intensified_local_search(harmony, lb, ub)\n\n                harmony = self.quantum_adjustment(harmony, self.best_harmony or harmony)\n\n                trial_vector = self.differential_evolution(harmony, lb, ub)\n                trial_value = func(trial_vector)\n                evaluations += 1\n\n                if trial_value < self.harmony_values[i]:\n                    self.harmony_values[i] = trial_value\n                    self.harmony_memory[i] = trial_vector.copy()\n\n                if trial_value < self.best_value:\n                    self.best_value = trial_value\n                    self.best_harmony = trial_vector.copy()\n\n        return self.best_harmony, self.best_value", "name": "QAHSDDE", "description": "The Quantum Adaptive Harmony Search with Dynamic Differential Evolution (QAHSDDE) enhances global search capabilities by integrating adaptive local intensification with dynamic parameter tuning in a quantum-enhanced framework.", "configspace": "", "generation": 25, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()').", "error": "ValueError('The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()')", "parent_id": "d0c0e622-23ff-4544-bc2b-6164276563d4", "metadata": {}, "mutation_prompt": null}
{"id": "11a48b17-3e6d-4721-a6c0-23984b2df14c", "solution": "import numpy as np\n\nclass QE_PSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(50, budget // 2)\n        self.c1 = 2.0  # Cognitive coefficient\n        self.c2 = 2.0  # Social coefficient\n        self.w = 0.7   # Inertia weight\n        self.vel_limit = 0.1\n        self.particles = None\n        self.velocities = None\n        self.pbest_positions = None\n        self.pbest_values = None\n        self.gbest_position = None\n        self.gbest_value = np.inf\n\n    def initialize_particles(self, lb, ub):\n        self.particles = np.random.uniform(lb, ub, (self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-self.vel_limit, self.vel_limit, (self.num_particles, self.dim))\n        self.pbest_positions = self.particles.copy()\n        self.pbest_values = np.full(self.num_particles, np.inf)\n\n    def quantum_update(self, lb, ub):\n        phi = np.random.uniform(0, 2 * np.pi, (self.num_particles, self.dim))\n        radius = np.random.uniform(0, 1, self.num_particles)\n        quantum_positions = (self.gbest_position + radius[:, None] * (\n            self.pbest_positions - self.gbest_position) * np.cos(phi))\n        return np.clip(quantum_positions, lb, ub)\n\n    def update_velocities_and_positions(self, lb, ub):\n        r1 = np.random.rand(self.num_particles, self.dim)\n        r2 = np.random.rand(self.num_particles, self.dim)\n        cognitive = self.c1 * r1 * (self.pbest_positions - self.particles)\n        social = self.c2 * r2 * (self.gbest_position - self.particles)\n        self.velocities = self.w * self.velocities + cognitive + social\n        self.velocities = np.clip(self.velocities, -self.vel_limit, self.vel_limit)\n        self.particles += self.velocities\n        self.particles = np.clip(self.particles, lb, ub)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_particles(lb, ub)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            quantum_positions = self.quantum_update(lb, ub)\n            self.update_velocities_and_positions(lb, ub)\n\n            for i in range(self.num_particles):\n                if evaluations >= self.budget:\n                    break\n\n                # Evaluate quantum positions\n                quantum_value = func(quantum_positions[i])\n                if quantum_value < self.pbest_values[i]:\n                    self.pbest_values[i] = quantum_value\n                    self.pbest_positions[i] = quantum_positions[i].copy()\n\n                # Evaluate particle positions\n                particle_value = func(self.particles[i])\n                evaluations += 2  # counting both quantum and particle evaluations\n\n                if particle_value < self.pbest_values[i]:\n                    self.pbest_values[i] = particle_value\n                    self.pbest_positions[i] = self.particles[i].copy()\n\n                if particle_value < self.gbest_value:\n                    self.gbest_value = particle_value\n                    self.gbest_position = self.particles[i].copy()\n\n        return self.gbest_position, self.gbest_value", "name": "QE_PSO", "description": "Quantum-Enhanced Particle Swarm Optimization (QE-PSO) integrates quantum-inspired particle swarm dynamics with adaptive learning to improve the search efficiency and convergence rate in high-dimensional optimization tasks.", "configspace": "", "generation": 26, "fitness": -Infinity, "feedback": "An exception occurred: TypeError(\"unsupported operand type(s) for -: 'float' and 'NoneType'\").", "error": "TypeError(\"unsupported operand type(s) for -: 'float' and 'NoneType'\")", "parent_id": "d0c0e622-23ff-4544-bc2b-6164276563d4", "metadata": {}, "mutation_prompt": null}
{"id": "47b3aa58-5950-4c8b-99ba-eb9658c1c7cc", "solution": "import numpy as np\n\nclass QIPSO_ANS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = min(50, budget)\n        self.positions = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_values = None\n        self.global_best_position = None\n        self.global_best_value = np.inf\n        self.w = 0.5  # Inertia weight\n        self.c1 = 1.5  # Cognitive coefficient\n        self.c2 = 1.5  # Social coefficient\n        self.adapt_rate = 0.1\n        self.bounds = None\n\n    def initialize_swarm(self, lb, ub):\n        self.positions = np.random.uniform(lb, ub, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.swarm_size, self.dim))\n        self.personal_best_positions = self.positions.copy()\n        self.personal_best_values = np.full(self.swarm_size, np.inf)\n        self.bounds = (lb, ub)\n\n    def quantum_position_update(self, position, global_best):\n        beta = np.random.normal(0, 1, self.dim)\n        delta = np.random.normal(0, 1, self.dim)\n        new_position = position + beta * (global_best - position) + delta * 0.1\n        lb, ub = self.bounds\n        return np.clip(new_position, lb, ub)\n\n    def adaptive_neighborhood_search(self, position, lb, ub):\n        neighborhood_radius = np.exp(-self.dim / self.swarm_size)\n        perturbation = np.random.uniform(-neighborhood_radius, neighborhood_radius, self.dim)\n        new_position = position + perturbation\n        return np.clip(new_position, lb, ub)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_swarm(lb, ub)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            for i in range(self.swarm_size):\n                if evaluations >= self.budget:\n                    break\n\n                current_value = func(self.positions[i])\n                evaluations += 1\n\n                if current_value < self.personal_best_values[i]:\n                    self.personal_best_values[i] = current_value\n                    self.personal_best_positions[i] = self.positions[i].copy()\n\n                if current_value < self.global_best_value:\n                    self.global_best_value = current_value\n                    self.global_best_position = self.positions[i].copy()\n\n            for i in range(self.swarm_size):\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                cognitive_component = self.c1 * r1 * (self.personal_best_positions[i] - self.positions[i])\n                social_component = self.c2 * r2 * (self.global_best_position - self.positions[i])\n                self.velocities[i] = self.w * self.velocities[i] + cognitive_component + social_component\n                self.positions[i] += self.velocities[i]\n\n                # Quantum-inspired position update\n                if np.random.rand() < self.adapt_rate:\n                    self.positions[i] = self.quantum_position_update(self.positions[i], self.global_best_position)\n\n                # Adaptive neighborhood search\n                if np.random.rand() < self.adapt_rate:\n                    self.positions[i] = self.adaptive_neighborhood_search(self.positions[i], lb, ub)\n\n        return self.global_best_position, self.global_best_value", "name": "QIPSO_ANS", "description": "Quantum-Inspired Particle Swarm with Adaptive Neighborhood Search (QIPSO-ANS) leverages quantum computing principles in particle swarm optimization to dynamically balance exploration and exploitation with adaptive neighborhood search.", "configspace": "", "generation": 27, "fitness": 0.2773388409719566, "feedback": "The algorithm QIPSO_ANS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28 with standard deviation 0.00.", "error": "", "parent_id": "d0c0e622-23ff-4544-bc2b-6164276563d4", "metadata": {"aucs": [0.2777418184670877, 0.2775526172259096, 0.2767220872228725]}, "mutation_prompt": null}
{"id": "a4df7157-c05c-46df-b75b-72bb0c75f346", "solution": "import numpy as np\n\nclass QDSADE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(50, budget)\n        self.mutation_factor = 0.5\n        self.crossover_rate = 0.7\n        self.population = None\n        self.bounds = None\n        self.global_best_position = None\n        self.global_best_value = np.inf\n\n    def initialize_population(self, lb, ub):\n        self.population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        self.bounds = (lb, ub)\n\n    def self_adaptive_parameters(self):\n        # Self-adaptive strategy for mutation factor and crossover rate\n        self.mutation_factor = np.random.uniform(0.4, 0.9)\n        self.crossover_rate = np.random.uniform(0.3, 0.9)\n\n    def quantum_mutation(self, target, best, lb, ub):\n        # Quantum-inspired mutation using Gaussian perturbation\n        beta = np.random.normal(0, 1, self.dim)\n        mutant = target + self.mutation_factor * (best - target) + beta * 0.1\n        return np.clip(mutant, lb, ub)\n\n    def crossover(self, target, mutant):\n        # Binomial crossover\n        trial = np.copy(target)\n        for j in range(self.dim):\n            if np.random.rand() < self.crossover_rate:\n                trial[j] = mutant[j]\n        return trial\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        evaluations = 0\n        \n        while evaluations < self.budget:\n            self.self_adaptive_parameters()\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                idxs = list(range(i)) + list(range(i+1, self.population_size))\n                a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n\n                best_idx = np.argmin([func(ind) for ind in self.population])\n                best = self.population[best_idx]\n\n                mutant = self.quantum_mutation(self.population[i], best, lb, ub)\n                trial = self.crossover(self.population[i], mutant)\n\n                target_value = func(self.population[i])\n                trial_value = func(trial)\n                evaluations += 2\n\n                if trial_value < target_value:\n                    self.population[i] = trial\n                    if trial_value < self.global_best_value:\n                        self.global_best_value = trial_value\n                        self.global_best_position = trial.copy()\n\n        return self.global_best_position, self.global_best_value", "name": "QDSADE", "description": "Quantum-Driven Self-Adaptive Differential Evolution (QDSADE) combines quantum computing principles with self-adaptive strategies in differential evolution to enhance search efficiency by dynamically adjusting mutation and crossover parameters.", "configspace": "", "generation": 28, "fitness": 0.26174483343333776, "feedback": "The algorithm QDSADE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.26 with standard deviation 0.01.", "error": "", "parent_id": "47b3aa58-5950-4c8b-99ba-eb9658c1c7cc", "metadata": {"aucs": [0.26421999580578714, 0.2536288440591341, 0.26738566043509204]}, "mutation_prompt": null}
{"id": "fc75795c-2125-4d52-a695-4132c29090f9", "solution": "import numpy as np\n\nclass ADE_QIM:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = min(50, budget)\n        self.mutation_factor = 0.5\n        self.crossover_rate = 0.9\n        self.population = None\n        self.bounds = None\n\n    def initialize_population(self, lb, ub):\n        self.population = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        self.bounds = (lb, ub)\n\n    def differential_mutation(self, target_idx):\n        indices = list(range(self.pop_size))\n        indices.remove(target_idx)\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        lb, ub = self.bounds\n        return np.clip(mutant, lb, ub)\n\n    def quantum_inspired_mutation(self, individual):\n        global_best = self.population[np.argmin([func(ind) for ind in self.population])]\n        beta = np.random.normal(0, 1, self.dim)\n        new_position = individual + beta * (global_best - individual)\n        lb, ub = self.bounds\n        return np.clip(new_position, lb, ub)\n\n    def crossover(self, target, mutant):\n        trial = np.copy(target)\n        crossover_points = np.random.rand(self.dim) < self.crossover_rate\n        trial[crossover_points] = mutant[crossover_points]\n        return trial\n\n    def select(self, target, trial, target_idx, func):\n        target_value = func(target)\n        trial_value = func(trial)\n        if trial_value < target_value:\n            self.population[target_idx] = trial\n            return trial_value\n        else:\n            return target_value\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        evaluations = 0\n        best_value = np.inf\n        best_position = None\n\n        while evaluations < self.budget:\n            for i in range(self.pop_size):\n                if evaluations >= self.budget:\n                    break\n\n                mutant = self.differential_mutation(i)\n                if np.random.rand() < 0.1:\n                    mutant = self.quantum_inspired_mutation(mutant)\n\n                trial = self.crossover(self.population[i], mutant)\n                target_value = self.select(self.population[i], trial, i, func)\n                evaluations += 1\n\n                if target_value < best_value:\n                    best_value = target_value\n                    best_position = trial\n\n        return best_position, best_value", "name": "ADE_QIM", "description": "Adaptive Differential Evolution with Quantum-Inspired Mutations (ADE-QIM) combines differential evolution's adaptive scaling with quantum-inspired mutations to efficiently explore complex landscapes.", "configspace": "", "generation": 29, "fitness": -Infinity, "feedback": "An exception occurred: NameError(\"name 'func' is not defined\").", "error": "NameError(\"name 'func' is not defined\")", "parent_id": "47b3aa58-5950-4c8b-99ba-eb9658c1c7cc", "metadata": {}, "mutation_prompt": null}
{"id": "574c5d66-584f-40cc-8b4a-ff40ee1ecbea", "solution": "import numpy as np\n\nclass QIPSO_ANS_DIA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = min(50, budget)\n        self.positions = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_values = None\n        self.global_best_position = None\n        self.global_best_value = np.inf\n        self.w = 0.9  # Initial inertia weight\n        self.c1 = 1.5  # Cognitive coefficient\n        self.c2 = 1.5  # Social coefficient\n        self.adapt_rate = 0.1\n        self.bounds = None\n\n    def initialize_swarm(self, lb, ub):\n        self.positions = np.random.uniform(lb, ub, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.swarm_size, self.dim))\n        self.personal_best_positions = self.positions.copy()\n        self.personal_best_values = np.full(self.swarm_size, np.inf)\n        self.bounds = (lb, ub)\n\n    def quantum_position_update(self, position, global_best):\n        beta = np.random.normal(0, 1, self.dim)\n        delta = np.random.normal(0, 1, self.dim)\n        new_position = position + beta * (global_best - position) + delta * 0.1\n        lb, ub = self.bounds\n        return np.clip(new_position, lb, ub)\n\n    def adaptive_neighborhood_search(self, position, lb, ub):\n        neighborhood_radius = np.exp(-self.dim / self.swarm_size)\n        perturbation = np.random.uniform(-neighborhood_radius, neighborhood_radius, self.dim)\n        new_position = position + perturbation\n        return np.clip(new_position, lb, ub)\n    \n    def dynamic_inertia_adjustment(self):\n        diversity = np.mean(np.std(self.positions, axis=0))\n        max_diversity = np.sqrt(np.sum((self.bounds[1] - self.bounds[0])**2))\n        self.w = 0.4 + 0.5 * (diversity / max_diversity)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_swarm(lb, ub)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            for i in range(self.swarm_size):\n                if evaluations >= self.budget:\n                    break\n\n                current_value = func(self.positions[i])\n                evaluations += 1\n\n                if current_value < self.personal_best_values[i]:\n                    self.personal_best_values[i] = current_value\n                    self.personal_best_positions[i] = self.positions[i].copy()\n\n                if current_value < self.global_best_value:\n                    self.global_best_value = current_value\n                    self.global_best_position = self.positions[i].copy()\n\n            self.dynamic_inertia_adjustment()\n\n            for i in range(self.swarm_size):\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                cognitive_component = self.c1 * r1 * (self.personal_best_positions[i] - self.positions[i])\n                social_component = self.c2 * r2 * (self.global_best_position - self.positions[i])\n                self.velocities[i] = self.w * self.velocities[i] + cognitive_component + social_component\n                self.positions[i] += self.velocities[i]\n\n                # Quantum-inspired position update\n                if np.random.rand() < self.adapt_rate:\n                    self.positions[i] = self.quantum_position_update(self.positions[i], self.global_best_position)\n\n                # Adaptive neighborhood search\n                if np.random.rand() < self.adapt_rate:\n                    self.positions[i] = self.adaptive_neighborhood_search(self.positions[i], lb, ub)\n\n        return self.global_best_position, self.global_best_value", "name": "QIPSO_ANS_DIA", "description": "Quantum-Inspired Particle Swarm with Adaptive Neighborhood Search and Dynamic Inertia Adjustment (QIPSO-ANS-DIA) enhances convergence by dynamically adjusting inertia weight based on diversity in swarm movement to balance exploration and exploitation.", "configspace": "", "generation": 30, "fitness": 0.27767631502999673, "feedback": "The algorithm QIPSO_ANS_DIA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28 with standard deviation 0.00.", "error": "", "parent_id": "47b3aa58-5950-4c8b-99ba-eb9658c1c7cc", "metadata": {"aucs": [0.2776366751556254, 0.2777663762397198, 0.27762589369464497]}, "mutation_prompt": null}
{"id": "e93089d8-8277-496b-b724-d6777b124d39", "solution": "import numpy as np\n\nclass EQIPSO_ADLS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = min(50, budget)\n        self.positions = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_values = None\n        self.global_best_position = None\n        self.global_best_value = np.inf\n        self.w = 0.9  # Initial inertia weight\n        self.c1 = 1.5  # Cognitive coefficient\n        self.c2 = 1.5  # Social coefficient\n        self.adapt_rate = 0.1\n        self.bounds = None\n        self.learning_rate = 0.1  # Learning rate for dynamic updates\n\n    def initialize_swarm(self, lb, ub):\n        self.positions = np.random.uniform(lb, ub, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.swarm_size, self.dim))\n        self.personal_best_positions = self.positions.copy()\n        self.personal_best_values = np.full(self.swarm_size, np.inf)\n        self.bounds = (lb, ub)\n\n    def quantum_position_update(self, position, global_best):\n        beta = np.random.normal(0, 1, self.dim)\n        delta = np.random.normal(0, 1, self.dim)\n        new_position = position + beta * (global_best - position) + delta * 0.1\n        lb, ub = self.bounds\n        return np.clip(new_position, lb, ub)\n\n    def adaptive_neighborhood_search(self, position, lb, ub):\n        neighborhood_radius = np.exp(-self.dim / self.swarm_size)\n        perturbation = np.random.uniform(-neighborhood_radius, neighborhood_radius, self.dim)\n        new_position = position + perturbation\n        return np.clip(new_position, lb, ub)\n    \n    def dynamic_inertia_adjustment(self):\n        diversity = np.mean(np.std(self.positions, axis=0))\n        max_diversity = np.sqrt(np.sum((self.bounds[1] - self.bounds[0])**2))\n        self.w = 0.4 + 0.5 * (diversity / max_diversity)\n\n    def adaptive_learning_strategy(self):\n        for i in range(self.swarm_size):\n            if np.random.rand() < self.learning_rate:\n                self.velocities[i] += np.random.uniform(-0.1, 0.1, self.dim)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_swarm(lb, ub)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            for i in range(self.swarm_size):\n                if evaluations >= self.budget:\n                    break\n\n                current_value = func(self.positions[i])\n                evaluations += 1\n\n                if current_value < self.personal_best_values[i]:\n                    self.personal_best_values[i] = current_value\n                    self.personal_best_positions[i] = self.positions[i].copy()\n\n                if current_value < self.global_best_value:\n                    self.global_best_value = current_value\n                    self.global_best_position = self.positions[i].copy()\n\n            self.dynamic_inertia_adjustment()\n            self.adaptive_learning_strategy()\n\n            for i in range(self.swarm_size):\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                cognitive_component = self.c1 * r1 * (self.personal_best_positions[i] - self.positions[i])\n                social_component = self.c2 * r2 * (self.global_best_position - self.positions[i])\n                self.velocities[i] = self.w * self.velocities[i] + cognitive_component + social_component\n                self.positions[i] += self.velocities[i]\n\n                # Quantum-inspired position update\n                if np.random.rand() < self.adapt_rate:\n                    self.positions[i] = self.quantum_position_update(self.positions[i], self.global_best_position)\n\n                # Adaptive neighborhood search\n                if np.random.rand() < self.adapt_rate:\n                    self.positions[i] = self.adaptive_neighborhood_search(self.positions[i], lb, ub)\n\n        return self.global_best_position, self.global_best_value", "name": "EQIPSO_ADLS", "description": "Enhanced Quantum-Inspired Particle Swarm with Adaptive Dynamic Learning Strategy (EQIPSO-ADLS) leverages dynamic learning and multi-strategy exploration to optimize convergence and balance exploration-exploitation trade-offs.", "configspace": "", "generation": 31, "fitness": 0.2778184892955932, "feedback": "The algorithm EQIPSO_ADLS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28 with standard deviation 0.00.", "error": "", "parent_id": "574c5d66-584f-40cc-8b4a-ff40ee1ecbea", "metadata": {"aucs": [0.277789037301891, 0.2778810669365619, 0.27778536364832673]}, "mutation_prompt": null}
{"id": "8af7a14e-0ff0-4596-9cc9-c23b9ed25d9c", "solution": "import numpy as np\n\nclass EQIPSO_ALSPM:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = min(50, budget)\n        self.positions = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_values = None\n        self.global_best_position = None\n        self.global_best_value = np.inf\n        self.w = 0.9  # Initial inertia weight\n        self.c1 = 1.5  # Cognitive coefficient\n        self.c2 = 1.5  # Social coefficient\n        self.adapt_rate = 0.1\n        self.bounds = None\n        self.learning_rate = 0.1  # Learning rate for dynamic updates\n        self.memory_factor = 0.5  # Initial memory influence factor\n\n    def initialize_swarm(self, lb, ub):\n        self.positions = np.random.uniform(lb, ub, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.swarm_size, self.dim))\n        self.personal_best_positions = self.positions.copy()\n        self.personal_best_values = np.full(self.swarm_size, np.inf)\n        self.bounds = (lb, ub)\n\n    def quantum_position_update(self, position, global_best):\n        beta = np.random.normal(0, 1, self.dim)\n        delta = np.random.normal(0, 1, self.dim)\n        new_position = position + beta * (global_best - position) + delta * 0.1\n        lb, ub = self.bounds\n        return np.clip(new_position, lb, ub)\n\n    def adaptive_neighborhood_search(self, position, lb, ub):\n        neighborhood_radius = np.exp(-self.dim / self.swarm_size)\n        perturbation = np.random.uniform(-neighborhood_radius, neighborhood_radius, self.dim)\n        new_position = position + perturbation\n        return np.clip(new_position, lb, ub)\n    \n    def dynamic_inertia_adjustment(self):\n        diversity = np.mean(np.std(self.positions, axis=0))\n        max_diversity = np.sqrt(np.sum((self.bounds[1] - self.bounds[0])**2))\n        self.w = 0.4 + 0.5 * (diversity / max_diversity)\n\n    def adaptive_learning_strategy(self):\n        for i in range(self.swarm_size):\n            if np.random.rand() < self.learning_rate:\n                self.velocities[i] += np.random.uniform(-0.1, 0.1, self.dim)\n\n    def self_adaptive_memory_influence(self):\n        diversity = np.mean(np.std(self.positions, axis=0))\n        max_diversity = np.sqrt(np.sum((self.bounds[1] - self.bounds[0])**2))\n        self.memory_factor = 0.3 + 0.7 * (diversity / max_diversity)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_swarm(lb, ub)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            for i in range(self.swarm_size):\n                if evaluations >= self.budget:\n                    break\n\n                current_value = func(self.positions[i])\n                evaluations += 1\n\n                if current_value < self.personal_best_values[i]:\n                    self.personal_best_values[i] = current_value\n                    self.personal_best_positions[i] = self.positions[i].copy()\n\n                if current_value < self.global_best_value:\n                    self.global_best_value = current_value\n                    self.global_best_position = self.positions[i].copy()\n\n            self.dynamic_inertia_adjustment()\n            self.adaptive_learning_strategy()\n            self.self_adaptive_memory_influence()\n\n            for i in range(self.swarm_size):\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                cognitive_component = self.c1 * r1 * (self.personal_best_positions[i] - self.positions[i])\n                social_component = self.c2 * r2 * (self.global_best_position - self.positions[i])\n                memory_component = self.memory_factor * (self.personal_best_positions[i] - self.positions[i])\n                self.velocities[i] = self.w * self.velocities[i] + cognitive_component + social_component + memory_component\n                self.positions[i] += self.velocities[i]\n\n                # Quantum-inspired position update\n                if np.random.rand() < self.adapt_rate:\n                    self.positions[i] = self.quantum_position_update(self.positions[i], self.global_best_position)\n\n                # Adaptive neighborhood search\n                if np.random.rand() < self.adapt_rate:\n                    self.positions[i] = self.adaptive_neighborhood_search(self.positions[i], lb, ub)\n\n        return self.global_best_position, self.global_best_value", "name": "EQIPSO_ALSPM", "description": "Enhanced Quantum-Inspired Particle Swarm with Adaptive Learning Strategy and Self-Adaptive Particle Memory (EQIPSO-ALSPM) introduces self-adaptive particle memory to improve convergence efficiency and solution quality by dynamically adjusting memory influence based on swarm diversity.", "configspace": "", "generation": 32, "fitness": 0.27753319732754844, "feedback": "The algorithm EQIPSO_ALSPM got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28 with standard deviation 0.00.", "error": "", "parent_id": "e93089d8-8277-496b-b724-d6777b124d39", "metadata": {"aucs": [0.2774091216846619, 0.27757633863382325, 0.27761413166416016]}, "mutation_prompt": null}
{"id": "2ed8188c-902b-420d-9375-5bc9e2f81819", "solution": "import numpy as np\n\nclass Enhanced_EQIPSO_ADLS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = min(50, budget)\n        self.positions = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_values = None\n        self.global_best_position = None\n        self.global_best_value = np.inf\n        self.w = 0.9  # Initial inertia weight\n        self.c1 = 1.5  # Cognitive coefficient\n        self.c2 = 1.5  # Social coefficient\n        self.adapt_rate = 0.1\n        self.bounds = None\n        self.learning_rate = 0.1  # Learning rate for dynamic updates\n        self.elite_preservation_rate = 0.1\n        self.stochastic_tunneling_factor = 0.01\n\n    def initialize_swarm(self, lb, ub):\n        self.positions = np.random.uniform(lb, ub, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.swarm_size, self.dim))\n        self.personal_best_positions = self.positions.copy()\n        self.personal_best_values = np.full(self.swarm_size, np.inf)\n        self.bounds = (lb, ub)\n\n    def quantum_position_update(self, position, global_best):\n        beta = np.random.normal(0, 1, self.dim)\n        delta = np.random.normal(0, 1, self.dim)\n        new_position = position + beta * (global_best - position) + delta * 0.1\n        lb, ub = self.bounds\n        return np.clip(new_position, lb, ub)\n\n    def adaptive_neighborhood_search(self, position, lb, ub):\n        neighborhood_radius = np.exp(-self.dim / self.swarm_size)\n        perturbation = np.random.uniform(-neighborhood_radius, neighborhood_radius, self.dim)\n        new_position = position + perturbation\n        return np.clip(new_position, lb, ub)\n    \n    def dynamic_inertia_adjustment(self):\n        diversity = np.mean(np.std(self.positions, axis=0))\n        max_diversity = np.sqrt(np.sum((self.bounds[1] - self.bounds[0])**2))\n        self.w = 0.4 + 0.5 * (diversity / max_diversity)\n\n    def adaptive_learning_strategy(self):\n        for i in range(self.swarm_size):\n            if np.random.rand() < self.learning_rate:\n                self.velocities[i] += np.random.uniform(-0.1, 0.1, self.dim)\n\n    def elite_preservation(self):\n        elite_threshold = np.percentile(self.personal_best_values, 100 * (1 - self.elite_preservation_rate))\n        elite_indices = np.where(self.personal_best_values <= elite_threshold)[0]\n        elite_positions = self.personal_best_positions[elite_indices]\n        return elite_positions\n\n    def stochastic_tunneling(self, position):\n        if np.random.rand() < self.stochastic_tunneling_factor:\n            lb, ub = self.bounds\n            position = np.random.uniform(lb, ub, self.dim)\n        return position\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_swarm(lb, ub)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            for i in range(self.swarm_size):\n                if evaluations >= self.budget:\n                    break\n\n                current_value = func(self.positions[i])\n                evaluations += 1\n\n                if current_value < self.personal_best_values[i]:\n                    self.personal_best_values[i] = current_value\n                    self.personal_best_positions[i] = self.positions[i].copy()\n\n                if current_value < self.global_best_value:\n                    self.global_best_value = current_value\n                    self.global_best_position = self.positions[i].copy()\n\n            self.dynamic_inertia_adjustment()\n            self.adaptive_learning_strategy()\n\n            for i in range(self.swarm_size):\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                cognitive_component = self.c1 * r1 * (self.personal_best_positions[i] - self.positions[i])\n                social_component = self.c2 * r2 * (self.global_best_position - self.positions[i])\n                self.velocities[i] = self.w * self.velocities[i] + cognitive_component + social_component\n                self.positions[i] += self.velocities[i]\n\n                # Quantum-inspired position update\n                if np.random.rand() < self.adapt_rate:\n                    self.positions[i] = self.quantum_position_update(self.positions[i], self.global_best_position)\n\n                # Adaptive neighborhood search\n                if np.random.rand() < self.adapt_rate:\n                    self.positions[i] = self.adaptive_neighborhood_search(self.positions[i], lb, ub)\n\n                # Stochastic tunneling to escape local optima\n                self.positions[i] = self.stochastic_tunneling(self.positions[i])\n\n            # Elite preservation for better convergence\n            elite_positions = self.elite_preservation()\n            for j, elite_position in enumerate(elite_positions):\n                if evaluations >= self.budget:\n                    break\n                elite_value = func(elite_position)\n                evaluations += 1\n                if elite_value < self.global_best_value:\n                    self.global_best_value = elite_value\n                    self.global_best_position = elite_position.copy()\n\n        return self.global_best_position, self.global_best_value", "name": "Enhanced_EQIPSO_ADLS", "description": "The Enhanced Quantum-Inspired Particle Swarm with Adaptive Dynamic Learning Strategy (EQIPSO-ADLS) is refined by incorporating an elite preservation mechanism and a stochastic tunneling strategy to escape local optima and enhance global search capabilities.", "configspace": "", "generation": 33, "fitness": 0.2772248373944432, "feedback": "The algorithm Enhanced_EQIPSO_ADLS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28 with standard deviation 0.00.", "error": "", "parent_id": "e93089d8-8277-496b-b724-d6777b124d39", "metadata": {"aucs": [0.2776761971662388, 0.2768700651473217, 0.27712824986976914]}, "mutation_prompt": null}
{"id": "34466d24-2391-4876-b4b0-7c8910ef389d", "solution": "import numpy as np\n\nclass AQDES:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(50, budget)\n        self.positions = None\n        self.fitness_values = None\n        self.best_position = None\n        self.best_value = np.inf\n        self.mutation_rate = 0.05\n        self.bounds = None\n\n    def initialize_population(self, lb, ub):\n        self.positions = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        self.fitness_values = np.full(self.population_size, np.inf)\n        self.bounds = (lb, ub)\n\n    def quantum_position_update(self, individual):\n        beta = np.random.normal(0, 1, self.dim)\n        delta = np.random.normal(0, 1, self.dim)\n        new_position = individual + beta * (self.best_position - individual) + delta * 0.1\n        lb, ub = self.bounds\n        return np.clip(new_position, lb, ub)\n\n    def adaptive_mutation(self, individual):\n        mutation_vector = np.random.uniform(-1, 1, self.dim) * self.mutation_rate\n        return np.clip(individual + mutation_vector, self.bounds[0], self.bounds[1])\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                current_value = func(self.positions[i])\n                evaluations += 1\n\n                if current_value < self.fitness_values[i]:\n                    self.fitness_values[i] = current_value\n\n                if current_value < self.best_value:\n                    self.best_value = current_value\n                    self.best_position = self.positions[i].copy()\n\n            for i in range(self.population_size):\n                if np.random.rand() < self.mutation_rate:\n                    self.positions[i] = self.adaptive_mutation(self.positions[i])\n\n                # Quantum-inspired position update\n                if np.random.rand() < self.mutation_rate:\n                    self.positions[i] = self.quantum_position_update(self.positions[i])\n\n        return self.best_position, self.best_value", "name": "AQDES", "description": "Adaptive Quantum-Driven Evolutionary Strategy (AQDES) integrates quantum-inspired position updates with an adaptive feedback mechanism to enhance exploration and exploitation in evolutionary search processes.", "configspace": "", "generation": 34, "fitness": 0.25220554565735376, "feedback": "The algorithm AQDES got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.25 with standard deviation 0.00.", "error": "", "parent_id": "e93089d8-8277-496b-b724-d6777b124d39", "metadata": {"aucs": [0.25194939838058583, 0.2545118105254287, 0.25015542806604674]}, "mutation_prompt": null}
{"id": "a4fb5b5f-8002-418d-98c8-c4dd189941a1", "solution": "import numpy as np\n\nclass QEDMA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(50, budget)\n        self.individuals = None\n        self.personal_best_positions = None\n        self.personal_best_values = None\n        self.global_best_position = None\n        self.global_best_value = np.inf\n        self.crossover_rate = 0.8\n        self.mutation_rate = 0.1\n        self.meme_prob = 0.2\n        self.bounds = None\n\n    def initialize_population(self, lb, ub):\n        self.individuals = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        self.personal_best_positions = self.individuals.copy()\n        self.personal_best_values = np.full(self.population_size, np.inf)\n        self.bounds = (lb, ub)\n\n    def quantum_inspired_exploration(self, individual, global_best):\n        beta = np.random.normal(0, 1, self.dim)\n        delta = np.random.normal(0, 1, self.dim)\n        new_individual = individual + beta * (global_best - individual) + delta * 0.1\n        lb, ub = self.bounds\n        return np.clip(new_individual, lb, ub)\n\n    def adaptive_meme_search(self, individual):\n        if np.random.rand() < self.meme_prob:\n            perturbation = (np.random.rand(self.dim) - 0.5) * 0.1\n            individual += perturbation\n        return np.clip(individual, self.bounds[0], self.bounds[1])\n\n    def crossover(self, parent1, parent2):\n        if np.random.rand() < self.crossover_rate:\n            point = np.random.randint(1, self.dim - 1)\n            child1 = np.concatenate((parent1[:point], parent2[point:]))\n            child2 = np.concatenate((parent2[:point], parent1[point:]))\n            return child1, child2\n        return parent1.copy(), parent2.copy()\n\n    def mutation(self, individual):\n        if np.random.rand() < self.mutation_rate:\n            mutation_vector = (np.random.rand(self.dim) - 0.5) * 0.1\n            return np.clip(individual + mutation_vector, self.bounds[0], self.bounds[1])\n        return individual\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                current_value = func(self.individuals[i])\n                evaluations += 1\n\n                if current_value < self.personal_best_values[i]:\n                    self.personal_best_values[i] = current_value\n                    self.personal_best_positions[i] = self.individuals[i].copy()\n\n                if current_value < self.global_best_value:\n                    self.global_best_value = current_value\n                    self.global_best_position = self.individuals[i].copy()\n\n            new_population = []\n\n            for j in range(0, self.population_size, 2):\n                parent1, parent2 = self.individuals[np.random.choice(self.population_size, 2, replace=False)]\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.mutation(child1)\n                child2 = self.mutation(child2)\n                new_population.append(child1)\n                new_population.append(child2)\n\n            self.individuals = np.array(new_population[:self.population_size])\n\n            for k in range(self.population_size):\n                self.individuals[k] = self.quantum_inspired_exploration(self.individuals[k], self.global_best_position)\n                self.individuals[k] = self.adaptive_meme_search(self.individuals[k])\n\n        return self.global_best_position, self.global_best_value", "name": "QEDMA", "description": "Quantum-Enhanced Dynamic Memetic Algorithm (QEDMA) combines quantum-inspired exploration with adaptive meme-based local search for efficient convergence in high-dimensional spaces.", "configspace": "", "generation": 35, "fitness": 0.27402907558777184, "feedback": "The algorithm QEDMA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.27 with standard deviation 0.00.", "error": "", "parent_id": "e93089d8-8277-496b-b724-d6777b124d39", "metadata": {"aucs": [0.27203757736396406, 0.2736242612027049, 0.27642538819664664]}, "mutation_prompt": null}
{"id": "df8b24d3-25ff-46b0-b435-a47b89f7f255", "solution": "import numpy as np\n\nclass AMGA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(100, budget)\n        self.crossover_rate = 0.7\n        self.mutation_rate = 0.1\n        self.elitism_count = 2\n        self.local_search_prob = 0.3\n        self.bounds = None\n\n    def initialize_population(self, lb, ub):\n        self.population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n        self.bounds = (lb, ub)\n\n    def crossover(self, parent1, parent2):\n        if np.random.rand() < self.crossover_rate:\n            crossover_point = np.random.randint(1, self.dim - 1)\n            child1 = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n            child2 = np.concatenate([parent2[:crossover_point], parent1[crossover_point:]])\n            return child1, child2\n        else:\n            return parent1, parent2\n\n    def mutate(self, individual):\n        for i in range(self.dim):\n            if np.random.rand() < self.mutation_rate:\n                mutation_value = np.random.uniform(-0.1, 0.1)\n                individual[i] += mutation_value\n                lb, ub = self.bounds\n                individual[i] = np.clip(individual[i], lb[i], ub[i])\n        return individual\n\n    def local_search(self, individual):\n        perturbation = np.random.uniform(-0.05, 0.05, self.dim)\n        new_individual = individual + perturbation\n        lb, ub = self.bounds\n        return np.clip(new_individual, lb, ub)\n\n    def select_parents(self):\n        fitness_sum = np.sum(1.0 / (self.fitness + 1e-9))\n        selection_probs = (1.0 / (self.fitness + 1e-9)) / fitness_sum\n        parents_indices = np.random.choice(self.population_size, size=2, p=selection_probs)\n        return self.population[parents_indices[0]], self.population[parents_indices[1]]\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            new_population = []\n            new_fitness = []\n\n            for _ in range(self.population_size // 2):\n                parent1, parent2 = self.select_parents()\n                child1, child2 = self.crossover(parent1, parent2)\n\n                child1 = self.mutate(child1)\n                child2 = self.mutate(child2)\n\n                if np.random.rand() < self.local_search_prob:\n                    child1 = self.local_search(child1)\n                if np.random.rand() < self.local_search_prob:\n                    child2 = self.local_search(child2)\n\n                new_population.extend([child1, child2])\n\n                if evaluations < self.budget:\n                    fitness1 = func(child1)\n                    new_fitness.append(fitness1)\n                    evaluations += 1\n                if evaluations < self.budget:\n                    fitness2 = func(child2)\n                    new_fitness.append(fitness2)\n                    evaluations += 1\n\n            # Retain the best solutions (elitism)\n            self.fitness = np.array(new_fitness)\n            sorted_indices = np.argsort(self.fitness)\n            new_population = np.array(new_population)\n            new_population = new_population[sorted_indices]\n            self.population = new_population[:self.population_size]\n            self.fitness = self.fitness[sorted_indices][:self.population_size]\n\n            # Adapt crossover and mutation rates based on diversity\n            diversity = np.mean(np.std(self.population, axis=0))\n            self.crossover_rate = 0.5 + 0.5 * (diversity / np.max(diversity, initial=1e-9))\n            self.mutation_rate = 0.05 + 0.05 * (1 - diversity / np.max(diversity, initial=1e-9))\n\n        best_index = np.argmin(self.fitness)\n        return self.population[best_index], self.fitness[best_index]", "name": "AMGA", "description": "The Adaptive Memetic Genetic Algorithm (AMGA) combines genetic operations with local search heuristics, dynamically adapting crossover and mutation rates to enhance solution diversity and convergence speed.", "configspace": "", "generation": 36, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('probabilities contain NaN').", "error": "ValueError('probabilities contain NaN')", "parent_id": "e93089d8-8277-496b-b724-d6777b124d39", "metadata": {}, "mutation_prompt": null}
{"id": "b26e900f-85e8-47a2-9966-b9a0c9863806", "solution": "import numpy as np\n\nclass QADE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(50, budget)\n        self.positions = None\n        self.fitness = None\n        self.best_position = None\n        self.best_value = np.inf\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.adapt_rate = 0.1\n        self.bounds = None\n\n    def initialize_population(self, lb, ub):\n        self.positions = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n        self.bounds = (lb, ub)\n\n    def quantum_position_update(self, position):\n        alpha = np.random.normal(0, 1, self.dim)\n        beta = np.random.normal(0, 1, self.dim)\n        new_position = position + alpha * (self.best_position - position) + beta * 0.1\n        lb, ub = self.bounds\n        return np.clip(new_position, lb, ub)\n\n    def differential_mutation(self, idx):\n        candidates = list(range(self.population_size))\n        candidates.remove(idx)\n        a, b, c = np.random.choice(candidates, 3, replace=False)\n        donor_vector = self.positions[a] + self.F * (self.positions[b] - self.positions[c])\n        return np.clip(donor_vector, *self.bounds)\n\n    def crossover(self, target, donor):\n        crossover_mask = np.random.rand(self.dim) < self.CR\n        trial_vector = np.where(crossover_mask, donor, target)\n        return trial_vector\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                target = self.positions[i]\n                donor = self.differential_mutation(i)\n                trial = self.crossover(target, donor)\n\n                if np.random.rand() < self.adapt_rate:\n                    trial = self.quantum_position_update(trial)\n\n                trial_value = func(trial)\n                evaluations += 1\n\n                if trial_value < self.fitness[i]:\n                    self.positions[i] = trial\n                    self.fitness[i] = trial_value\n\n                if trial_value < self.best_value:\n                    self.best_value = trial_value\n                    self.best_position = trial\n\n        return self.best_position, self.best_value", "name": "QADE", "description": "Quantum Adaptive Differential Evolution (QADE) combines quantum-inspired position updates with adaptive differential mutation and crossover for robust global optimization in complex search spaces.", "configspace": "", "generation": 37, "fitness": 0.2653429935936437, "feedback": "The algorithm QADE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.27 with standard deviation 0.00.", "error": "", "parent_id": "e93089d8-8277-496b-b724-d6777b124d39", "metadata": {"aucs": [0.26665719943508626, 0.26609153991735124, 0.2632802414284937]}, "mutation_prompt": null}
{"id": "51c42e62-ffbd-463d-bcdc-9cf54a23f661", "solution": "import numpy as np\n\nclass HQESO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = min(50, budget)\n        self.positions = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_values = None\n        self.global_best_position = None\n        self.global_best_value = np.inf\n        self.w = 0.9  # Initial inertia weight\n        self.c1 = 1.5  # Cognitive coefficient\n        self.c2 = 1.5  # Social coefficient\n        self.adapt_rate = 0.2\n        self.bounds = None\n        self.learning_rate = 0.15  # Learning rate for dynamic updates\n\n    def initialize_swarm(self, lb, ub):\n        self.positions = np.random.uniform(lb, ub, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.swarm_size, self.dim))\n        self.personal_best_positions = self.positions.copy()\n        self.personal_best_values = np.full(self.swarm_size, np.inf)\n        self.bounds = (lb, ub)\n\n    def quantum_position_update(self, position, global_best):\n        beta = np.random.normal(0, 1, self.dim)\n        delta = np.random.normal(0, 1, self.dim)\n        new_position = position + beta * (global_best - position) + delta * 0.05\n        lb, ub = self.bounds\n        return np.clip(new_position, lb, ub)\n\n    def adaptive_neighborhood_search(self, position, lb, ub):\n        neighborhood_radius = np.exp(-self.dim / (self.swarm_size + 5))\n        perturbation = np.random.uniform(-neighborhood_radius, neighborhood_radius, self.dim)\n        new_position = position + perturbation\n        return np.clip(new_position, lb, ub)\n    \n    def dynamic_inertia_adjustment(self):\n        diversity = np.mean(np.std(self.positions, axis=0))\n        max_diversity = np.sqrt(np.sum((self.bounds[1] - self.bounds[0])**2))\n        self.w = 0.5 + 0.4 * (diversity / max_diversity)\n\n    def adaptive_learning_strategy(self):\n        for i in range(self.swarm_size):\n            if np.random.rand() < self.learning_rate:\n                self.velocities[i] += np.random.uniform(-0.05, 0.05, self.dim)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_swarm(lb, ub)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            for i in range(self.swarm_size):\n                if evaluations >= self.budget:\n                    break\n\n                current_value = func(self.positions[i])\n                evaluations += 1\n\n                if current_value < self.personal_best_values[i]:\n                    self.personal_best_values[i] = current_value\n                    self.personal_best_positions[i] = self.positions[i].copy()\n\n                if current_value < self.global_best_value:\n                    self.global_best_value = current_value\n                    self.global_best_position = self.positions[i].copy()\n\n            self.dynamic_inertia_adjustment()\n            self.adaptive_learning_strategy()\n\n            for i in range(self.swarm_size):\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                cognitive_component = self.c1 * r1 * (self.personal_best_positions[i] - self.positions[i])\n                social_component = self.c2 * r2 * (self.global_best_position - self.positions[i])\n                self.velocities[i] = self.w * self.velocities[i] + cognitive_component + social_component\n                self.positions[i] += self.velocities[i]\n\n                # Quantum-inspired position update\n                if np.random.rand() < self.adapt_rate:\n                    self.positions[i] = self.quantum_position_update(self.positions[i], self.global_best_position)\n\n                # Adaptive neighborhood search\n                if np.random.rand() < self.adapt_rate:\n                    self.positions[i] = self.adaptive_neighborhood_search(self.positions[i], lb, ub)\n\n        return self.global_best_position, self.global_best_value", "name": "HQESO", "description": "Hybrid Quantum-Enhanced Swarm Optimization (HQESO) integrates quantum position updating with adaptive memory enhancement and multi-modal search to boost convergence efficiency and solution quality.", "configspace": "", "generation": 38, "fitness": 0.27660782167670955, "feedback": "The algorithm HQESO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28 with standard deviation 0.00.", "error": "", "parent_id": "e93089d8-8277-496b-b724-d6777b124d39", "metadata": {"aucs": [0.2760654003778443, 0.27640132062263856, 0.2773567440296457]}, "mutation_prompt": null}
{"id": "81457601-1c55-4519-a560-65254af19a9f", "solution": "import numpy as np\n\nclass EQIPSO_ADLS_Improved:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = min(50, budget)\n        self.positions = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_values = None\n        self.global_best_position = None\n        self.global_best_value = np.inf\n        self.w = 0.9  # Initial inertia weight\n        self.c1 = 1.5  # Cognitive coefficient\n        self.c2 = 1.5  # Social coefficient\n        self.adapt_rate = 0.1\n        self.bounds = None\n        self.learning_rate = 0.1  # Learning rate for dynamic updates\n        self.leader_selection_rate = 0.2\n\n    def initialize_swarm(self, lb, ub):\n        self.positions = np.random.uniform(lb, ub, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.swarm_size, self.dim))\n        self.personal_best_positions = self.positions.copy()\n        self.personal_best_values = np.full(self.swarm_size, np.inf)\n        self.bounds = (lb, ub)\n\n    def quantum_position_update(self, position, global_best):\n        beta = np.random.normal(0, 1, self.dim)\n        delta = np.random.normal(0, 1, self.dim)\n        new_position = position + beta * (global_best - position) + delta * 0.1\n        lb, ub = self.bounds\n        return np.clip(new_position, lb, ub)\n\n    def adaptive_neighborhood_search(self, position, lb, ub):\n        neighborhood_radius = np.exp(-self.dim / self.swarm_size)\n        perturbation = np.random.uniform(-neighborhood_radius, neighborhood_radius, self.dim)\n        new_position = position + perturbation\n        return np.clip(new_position, lb, ub)\n\n    def dynamic_inertia_adjustment(self):\n        diversity = np.mean(np.std(self.positions, axis=0))\n        max_diversity = np.sqrt(np.sum((self.bounds[1] - self.bounds[0])**2))\n        self.w = 0.4 + 0.5 * (diversity / max_diversity)\n\n    def adaptive_learning_strategy(self):\n        for i in range(self.swarm_size):\n            if np.random.rand() < self.learning_rate:\n                self.velocities[i] += np.random.uniform(-0.1, 0.1, self.dim)\n\n    def adaptive_leader_selection(self):\n        leader_indices = np.random.choice(self.swarm_size, int(self.leader_selection_rate * self.swarm_size), replace=False)\n        selected_leader = min(leader_indices, key=lambda i: self.personal_best_values[i])\n        return self.personal_best_positions[selected_leader]\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_swarm(lb, ub)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            for i in range(self.swarm_size):\n                if evaluations >= self.budget:\n                    break\n\n                current_value = func(self.positions[i])\n                evaluations += 1\n\n                if current_value < self.personal_best_values[i]:\n                    self.personal_best_values[i] = current_value\n                    self.personal_best_positions[i] = self.positions[i].copy()\n\n                if current_value < self.global_best_value:\n                    self.global_best_value = current_value\n                    self.global_best_position = self.positions[i].copy()\n\n            self.dynamic_inertia_adjustment()\n            self.adaptive_learning_strategy()\n\n            for i in range(self.swarm_size):\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                cognitive_component = self.c1 * r1 * (self.personal_best_positions[i] - self.positions[i])\n                leader_position = self.adaptive_leader_selection()\n                social_component = self.c2 * r2 * (leader_position - self.positions[i])\n                self.velocities[i] = self.w * self.velocities[i] + cognitive_component + social_component\n                self.positions[i] += self.velocities[i]\n\n                # Quantum-inspired position update\n                if np.random.rand() < self.adapt_rate:\n                    self.positions[i] = self.quantum_position_update(self.positions[i], self.global_best_position)\n\n                # Adaptive neighborhood search\n                if np.random.rand() < self.adapt_rate:\n                    self.positions[i] = self.adaptive_neighborhood_search(self.positions[i], lb, ub)\n\n        return self.global_best_position, self.global_best_value", "name": "EQIPSO_ADLS_Improved", "description": "The algorithm integrates an adaptive leader selection mechanism and quantum information sharing to enhance convergence and diversity in the swarm.", "configspace": "", "generation": 39, "fitness": 0.2753280358151335, "feedback": "The algorithm EQIPSO_ADLS_Improved got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28 with standard deviation 0.00.", "error": "", "parent_id": "e93089d8-8277-496b-b724-d6777b124d39", "metadata": {"aucs": [0.27332667624928586, 0.27521333660106706, 0.2774440945950476]}, "mutation_prompt": null}
{"id": "5c5d0e55-4963-4464-bc3b-1ebb691bd05f", "solution": "import numpy as np\n\nclass QSAA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = min(50, budget)\n        self.positions = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_values = None\n        self.global_best_position = None\n        self.global_best_value = np.inf\n        self.w_min = 0.4  # Minimum inertia weight\n        self.w_max = 0.9  # Maximum inertia weight\n        self.c1 = 2.0  # Cognitive coefficient\n        self.c2 = 2.0  # Social coefficient\n        self.adapt_rate = 0.1\n        self.bounds = None\n\n    def initialize_swarm(self, lb, ub):\n        self.positions = np.random.uniform(lb, ub, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.swarm_size, self.dim))\n        self.personal_best_positions = self.positions.copy()\n        self.personal_best_values = np.full(self.swarm_size, np.inf)\n        self.bounds = (lb, ub)\n\n    def quantum_position_update(self, position, global_best):\n        beta = np.random.normal(0, 1, self.dim)\n        delta = np.random.normal(0, 1, self.dim)\n        new_position = position + beta * (global_best - position) + delta * 0.1\n        lb, ub = self.bounds\n        return np.clip(new_position, lb, ub)\n\n    def adaptive_neighborhood_search(self, position, lb, ub):\n        neighborhood_radius = np.exp(-self.dim / self.swarm_size)\n        perturbation = np.random.uniform(-neighborhood_radius, neighborhood_radius, self.dim)\n        new_position = position + perturbation\n        return np.clip(new_position, lb, ub)\n    \n    def time_varying_inertia(self, current_iter, max_iter):\n        self.w = self.w_max - (self.w_max - self.w_min) * (current_iter / max_iter)\n\n    def opposition_based_learning(self, position, lb, ub):\n        return lb + ub - position\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_swarm(lb, ub)\n        evaluations = 0\n        max_iter = self.budget // self.swarm_size\n\n        for iter in range(max_iter):\n            for i in range(self.swarm_size):\n                if evaluations >= self.budget:\n                    break\n\n                current_value = func(self.positions[i])\n                evaluations += 1\n\n                if current_value < self.personal_best_values[i]:\n                    self.personal_best_values[i] = current_value\n                    self.personal_best_positions[i] = self.positions[i].copy()\n\n                if current_value < self.global_best_value:\n                    self.global_best_value = current_value\n                    self.global_best_position = self.positions[i].copy()\n\n            self.time_varying_inertia(iter, max_iter)\n\n            for i in range(self.swarm_size):\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                cognitive_component = self.c1 * r1 * (self.personal_best_positions[i] - self.positions[i])\n                social_component = self.c2 * r2 * (self.global_best_position - self.positions[i])\n                self.velocities[i] = self.w * self.velocities[i] + cognitive_component + social_component\n                self.positions[i] += self.velocities[i]\n\n                # Quantum-inspired position update\n                if np.random.rand() < self.adapt_rate:\n                    self.positions[i] = self.quantum_position_update(self.positions[i], self.global_best_position)\n\n                # Adaptive neighborhood search\n                if np.random.rand() < self.adapt_rate:\n                    self.positions[i] = self.adaptive_neighborhood_search(self.positions[i], lb, ub)\n\n                # Opposition-based learning\n                if np.random.rand() < self.adapt_rate:\n                    opposed_position = self.opposition_based_learning(self.positions[i], lb, ub)\n                    opposed_value = func(opposed_position)\n                    evaluations += 1\n                    if opposed_value < current_value:\n                        self.positions[i] = opposed_position\n\n        return self.global_best_position, self.global_best_value", "name": "QSAA", "description": "Quantum-Swarm Adaptive Algorithm (QSAA) enhances convergence using a non-linear time-varying inertia weight and stochastic opposition-based learning for dynamic solution refinement.", "configspace": "", "generation": 40, "fitness": 0.2657981522347433, "feedback": "The algorithm QSAA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.27 with standard deviation 0.00.", "error": "", "parent_id": "e93089d8-8277-496b-b724-d6777b124d39", "metadata": {"aucs": [0.2690561225348468, 0.26251658643916276, 0.2658217477302204]}, "mutation_prompt": null}
{"id": "4ed555ea-3aaf-40ac-8b1e-c9361f3ec30e", "solution": "import numpy as np\n\nclass Q_EDE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(50, budget)\n        self.population = None\n        self.fitness = None\n        self.best_solution = None\n        self.best_fitness = np.inf\n        self.bounds = None\n        self.F = 0.5  # Differential weight\n        self.CR = 0.9  # Crossover probability\n\n    def initialize_population(self, lb, ub):\n        self.population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n        self.bounds = (lb, ub)\n\n    def quantum_superposition(self, solutions):\n        mean_position = np.mean(solutions, axis=0)\n        deviation = np.std(solutions, axis=0)\n        new_solutions = mean_position + np.random.normal(0, deviation, solutions.shape)\n        lb, ub = self.bounds\n        return np.clip(new_solutions, lb, ub)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            current_value = func(self.population[i])\n            if current_value < self.fitness[i]:\n                self.fitness[i] = current_value\n            if current_value < self.best_fitness:\n                self.best_fitness = current_value\n                self.best_solution = self.population[i].copy()\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            self.evaluate_population(func)\n            evaluations += self.population_size\n\n            trial_population = np.zeros_like(self.population)\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                # Mutation\n                indices = np.random.choice(self.population_size, 3, replace=False)\n                a, b, c = self.population[indices]\n                mutant_vector = a + self.F * (b - c)\n                mutant_vector = np.clip(mutant_vector, lb, ub)\n\n                # Crossover\n                crossover_mask = np.random.rand(self.dim) < self.CR\n                trial_vector = np.where(crossover_mask, mutant_vector, self.population[i])\n\n                trial_population[i] = trial_vector\n\n                # Selection\n                trial_value = func(trial_vector)\n                evaluations += 1\n\n                if trial_value < self.fitness[i]:\n                    self.fitness[i] = trial_value\n                    self.population[i] = trial_vector\n\n                    if trial_value < self.best_fitness:\n                        self.best_fitness = trial_value\n                        self.best_solution = trial_vector.copy()\n\n            # Quantum-inspired enhancement\n            self.population = self.quantum_superposition(self.population)\n\n        return self.best_solution, self.best_fitness", "name": "Q_EDE", "description": "Quantum-Enhanced Differential Evolution (Q-EDE) integrates quantum-inspired superposition and entanglement concepts into differential evolution to enhance exploration and convergence efficiency.", "configspace": "", "generation": 41, "fitness": 0.26968314298922597, "feedback": "The algorithm Q_EDE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.27 with standard deviation 0.00.", "error": "", "parent_id": "e93089d8-8277-496b-b724-d6777b124d39", "metadata": {"aucs": [0.2696893562669609, 0.2685717334323954, 0.2707883392683216]}, "mutation_prompt": null}
{"id": "d16d81be-8221-4415-bed4-ce03236d430d", "solution": "import numpy as np\n\nclass QGA_Hybrid:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(50, budget // 2)\n        self.population = None\n        self.fitness = None\n        self.best_solution = None\n        self.best_value = np.inf\n        self.bounds = None\n        self.crossover_prob = 0.8\n        self.mutation_prob = 0.1\n\n    def initialize_population(self, lb, ub):\n        self.population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n        self.bounds = (lb, ub)\n\n    def quantum_initialization(self):\n        # Apply quantum random walk for diverse initialization\n        qwalk = np.random.normal(0, 1, (self.population_size, self.dim))\n        self.population = self.population + qwalk * 0.05\n        lb, ub = self.bounds\n        self.population = np.clip(self.population, lb, ub)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            value = func(self.population[i])\n            if value < self.fitness[i]:\n                self.fitness[i] = value\n                if value < self.best_value:\n                    self.best_value = value\n                    self.best_solution = self.population[i].copy()\n\n    def select_parents(self):\n        # Tournament selection\n        idx = np.random.choice(np.arange(self.population_size), size=(self.population_size, 2), replace=True)\n        better_idx = np.argmin(self.fitness[idx], axis=1)\n        parents = self.population[idx[np.arange(self.population_size), better_idx]]\n        return parents\n\n    def crossover(self, parents):\n        # Uniform crossover\n        children = np.empty_like(parents)\n        for i in range(0, self.population_size, 2):\n            if np.random.rand() < self.crossover_prob:\n                mask = np.random.rand(self.dim) > 0.5\n                children[i] = np.where(mask, parents[i], parents[i+1])\n                children[i+1] = np.where(mask, parents[i+1], parents[i])\n            else:\n                children[i], children[i+1] = parents[i], parents[i+1]\n        return children\n\n    def mutate(self, children):\n        # Gaussian mutation\n        for i in range(self.population_size):\n            if np.random.rand() < self.mutation_prob:\n                noise = np.random.normal(0, 0.1, self.dim)\n                children[i] += noise\n                lb, ub = self.bounds\n                children[i] = np.clip(children[i], lb, ub)\n        return children\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        self.quantum_initialization()\n        evaluations = 0\n\n        while evaluations < self.budget:\n            self.evaluate_population(func)\n            evaluations += self.population_size\n\n            if evaluations >= self.budget:\n                break\n\n            parents = self.select_parents()\n            children = self.crossover(parents)\n            children = self.mutate(children)\n            self.population = children\n\n        return self.best_solution, self.best_value", "name": "QGA_Hybrid", "description": "Quantum Genetic Algorithm Hybrid (QGA-Hybrid) combines quantum-inspired initialization with genetic operators to exploit dynamic growth and diversity for robust global optimization.", "configspace": "", "generation": 42, "fitness": 0.257964777997821, "feedback": "The algorithm QGA_Hybrid got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.26 with standard deviation 0.01.", "error": "", "parent_id": "e93089d8-8277-496b-b724-d6777b124d39", "metadata": {"aucs": [0.27015040369187193, 0.24479744458689112, 0.2589464857147]}, "mutation_prompt": null}
{"id": "bb7d3f20-8cd3-4b4d-bc5d-8999374da285", "solution": "import numpy as np\n\nclass EQIPSO_DCA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = min(50, budget)\n        self.positions = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_values = None\n        self.global_best_position = None\n        self.global_best_value = np.inf\n        self.w = 0.9  # Initial inertia weight\n        self.c1 = 2.0  # Cognitive coefficient\n        self.c2 = 2.0  # Social coefficient\n        self.adapt_rate = 0.1\n        self.bounds = None\n        self.learning_rate = 0.1  # Learning rate for dynamic updates\n\n    def initialize_swarm(self, lb, ub):\n        self.positions = np.random.uniform(lb, ub, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.swarm_size, self.dim))\n        self.personal_best_positions = self.positions.copy()\n        self.personal_best_values = np.full(self.swarm_size, np.inf)\n        self.bounds = (lb, ub)\n\n    def quantum_position_update(self, position, global_best):\n        beta = np.random.normal(0, 1, self.dim)\n        delta = np.random.normal(0, 1, self.dim)\n        new_position = position + beta * (global_best - position) + delta * 0.1\n        lb, ub = self.bounds\n        return np.clip(new_position, lb, ub)\n\n    def adaptive_neighborhood_search(self, position, lb, ub):\n        neighborhood_radius = np.exp(-self.dim / self.swarm_size)\n        perturbation = np.random.uniform(-neighborhood_radius, neighborhood_radius, self.dim)\n        new_position = position + perturbation\n        return np.clip(new_position, lb, ub)\n    \n    def dynamic_inertia_adjustment(self):\n        diversity = np.mean(np.std(self.positions, axis=0))\n        max_diversity = np.sqrt(np.sum((self.bounds[1] - self.bounds[0])**2))\n        self.w = 0.4 + 0.5 * (diversity / max_diversity)\n\n    def adaptive_coefficient_tuning(self):\n        diversity = np.mean(np.std(self.positions, axis=0))\n        self.c1 = 1.5 + 0.5 * (diversity / (self.bounds[1] - self.bounds[0]).mean())\n        self.c2 = 1.5 + 0.5 * ((self.bounds[1] - self.bounds[0]).mean() / diversity)\n\n    def adaptive_learning_strategy(self):\n        for i in range(self.swarm_size):\n            if np.random.rand() < self.learning_rate:\n                self.velocities[i] += np.random.uniform(-0.1, 0.1, self.dim)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_swarm(lb, ub)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            for i in range(self.swarm_size):\n                if evaluations >= self.budget:\n                    break\n\n                current_value = func(self.positions[i])\n                evaluations += 1\n\n                if current_value < self.personal_best_values[i]:\n                    self.personal_best_values[i] = current_value\n                    self.personal_best_positions[i] = self.positions[i].copy()\n\n                if current_value < self.global_best_value:\n                    self.global_best_value = current_value\n                    self.global_best_position = self.positions[i].copy()\n\n            self.dynamic_inertia_adjustment()\n            self.adaptive_coefficient_tuning()\n            self.adaptive_learning_strategy()\n\n            for i in range(self.swarm_size):\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                cognitive_component = self.c1 * r1 * (self.personal_best_positions[i] - self.positions[i])\n                social_component = self.c2 * r2 * (self.global_best_position - self.positions[i])\n                self.velocities[i] = self.w * self.velocities[i] + cognitive_component + social_component\n                self.positions[i] += self.velocities[i]\n\n                # Quantum-inspired position update\n                if np.random.rand() < self.adapt_rate:\n                    self.positions[i] = self.quantum_position_update(self.positions[i], self.global_best_position)\n\n                # Adaptive neighborhood search\n                if np.random.rand() < self.adapt_rate:\n                    self.positions[i] = self.adaptive_neighborhood_search(self.positions[i], lb, ub)\n\n        return self.global_best_position, self.global_best_value", "name": "EQIPSO_DCA", "description": "Enhanced Quantum-Inspired Particle Swarm with Dynamic Coefficient Adaptation (EQIPSO-DCA) employs adaptive coefficient tuning based on swarm diversity metrics to improve convergence efficiency and balance exploration-exploitation trade-offs.", "configspace": "", "generation": 43, "fitness": 0.26293824302326224, "feedback": "The algorithm EQIPSO_DCA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.26 with standard deviation 0.01.", "error": "", "parent_id": "e93089d8-8277-496b-b724-d6777b124d39", "metadata": {"aucs": [0.25380151425619, 0.27022776651379954, 0.2647854482997971]}, "mutation_prompt": null}
{"id": "2da81261-4203-4aca-9121-ca2604103493", "solution": "import numpy as np\n\nclass QAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = min(50, budget)\n        self.positions = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_values = None\n        self.global_best_position = None\n        self.global_best_value = np.inf\n        self.w = 0.9  # Initial inertia weight\n        self.c1 = 1.5  # Cognitive coefficient\n        self.c2 = 1.5  # Social coefficient\n        self.adapt_rate = 0.1\n        self.bounds = None\n        self.learning_rate = 0.1  # Learning rate for dynamic updates\n\n    def initialize_swarm(self, lb, ub):\n        self.positions = np.random.uniform(lb, ub, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.swarm_size, self.dim))\n        self.personal_best_positions = self.positions.copy()\n        self.personal_best_values = np.full(self.swarm_size, np.inf)\n        self.bounds = (lb, ub)\n\n    def quantum_position_update(self, position, global_best):\n        beta = np.random.normal(0, 1, self.dim)\n        delta = np.random.normal(0, 1, self.dim)\n        new_position = position + beta * (global_best - position) + delta * 0.05\n        lb, ub = self.bounds\n        return np.clip(new_position, lb, ub)\n\n    def adaptive_neighborhood_search(self, position, lb, ub):\n        neighborhood_radius = np.exp(-self.dim / self.swarm_size)\n        perturbation = np.random.uniform(-neighborhood_radius, neighborhood_radius, self.dim)\n        new_position = position + perturbation * np.random.uniform(0.5, 1.5)\n        return np.clip(new_position, lb, ub)\n    \n    def dynamic_inertia_adjustment(self):\n        diversity = np.mean(np.std(self.positions, axis=0))\n        max_diversity = np.sqrt(np.sum((self.bounds[1] - self.bounds[0])**2))\n        self.w = 0.4 + 0.5 * (diversity / max_diversity)\n\n    def adaptive_velocity_scaling(self):\n        mean_velocity = np.mean(np.linalg.norm(self.velocities, axis=1))\n        scaling_factor = 0.5 + 0.5 * (mean_velocity / np.max([mean_velocity, 1e-8]))\n        self.velocities *= scaling_factor\n\n    def adaptive_learning_strategy(self):\n        for i in range(self.swarm_size):\n            if np.random.rand() < self.learning_rate:\n                directional_influence = np.sign(self.global_best_position - self.positions[i])\n                self.velocities[i] += directional_influence * np.random.uniform(-0.05, 0.05, self.dim)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_swarm(lb, ub)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            for i in range(self.swarm_size):\n                if evaluations >= self.budget:\n                    break\n\n                current_value = func(self.positions[i])\n                evaluations += 1\n\n                if current_value < self.personal_best_values[i]:\n                    self.personal_best_values[i] = current_value\n                    self.personal_best_positions[i] = self.positions[i].copy()\n\n                if current_value < self.global_best_value:\n                    self.global_best_value = current_value\n                    self.global_best_position = self.positions[i].copy()\n\n            self.dynamic_inertia_adjustment()\n            self.adaptive_velocity_scaling()\n            self.adaptive_learning_strategy()\n\n            for i in range(self.swarm_size):\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                cognitive_component = self.c1 * r1 * (self.personal_best_positions[i] - self.positions[i])\n                social_component = self.c2 * r2 * (self.global_best_position - self.positions[i])\n                self.velocities[i] = self.w * self.velocities[i] + cognitive_component + social_component\n                self.positions[i] += self.velocities[i]\n\n                # Quantum-inspired position update\n                if np.random.rand() < self.adapt_rate:\n                    self.positions[i] = self.quantum_position_update(self.positions[i], self.global_best_position)\n\n                # Adaptive neighborhood search\n                if np.random.rand() < self.adapt_rate:\n                    self.positions[i] = self.adaptive_neighborhood_search(self.positions[i], lb, ub)\n\n        return self.global_best_position, self.global_best_value", "name": "QAPSO", "description": "Quantum-Enhanced Adaptive Particle Swarm Optimization (QAPSO) introduces position-dependent learning rates and dynamic velocity scaling to enhance convergence and ensure efficient exploration-exploitation balance.", "configspace": "", "generation": 44, "fitness": 0.277513204852243, "feedback": "The algorithm QAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28 with standard deviation 0.00.", "error": "", "parent_id": "e93089d8-8277-496b-b724-d6777b124d39", "metadata": {"aucs": [0.27774503342839685, 0.27768771796332514, 0.2771068631650071]}, "mutation_prompt": null}
{"id": "8db4ba34-698a-4281-9cd2-99b230728b7c", "solution": "import numpy as np\n\nclass DyQASO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = min(50, budget)\n        self.positions = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_values = None\n        self.global_best_position = None\n        self.global_best_value = np.inf\n        self.w = 0.9  # Initial inertia weight\n        self.c1 = 1.5  # Cognitive coefficient\n        self.c2 = 1.5  # Social coefficient\n        self.adapt_rate = 0.15  # Increased adapt rate\n        self.bounds = None\n        self.learning_rate = 0.15  # Enhanced learning rate for dynamic updates\n\n    def initialize_swarm(self, lb, ub):\n        self.positions = np.random.uniform(lb, ub, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.swarm_size, self.dim))\n        self.personal_best_positions = self.positions.copy()\n        self.personal_best_values = np.full(self.swarm_size, np.inf)\n        self.bounds = (lb, ub)\n\n    def quantum_position_update(self, position, global_best):\n        beta = np.random.normal(0, 1, self.dim)\n        delta = np.random.normal(0, 1, self.dim)\n        new_position = position + beta * (global_best - position) + delta * 0.05  # Reduced delta\n        lb, ub = self.bounds\n        return np.clip(new_position, lb, ub)\n\n    def adaptive_neighborhood_search(self, position, lb, ub):\n        neighborhood_radius = np.exp(-self.dim / self.swarm_size)\n        perturbation = np.random.uniform(-neighborhood_radius, neighborhood_radius, self.dim)\n        new_position = position + perturbation\n        return np.clip(new_position, lb, ub)\n    \n    def dynamic_inertia_adjustment(self):\n        diversity = np.mean(np.std(self.positions, axis=0))\n        max_diversity = np.sqrt(np.sum((self.bounds[1] - self.bounds[0])**2))\n        self.w = 0.5 + 0.4 * (diversity / max_diversity)  # Adjusted inertia bounds\n\n    def adaptive_learning_strategy(self):\n        for i in range(self.swarm_size):\n            if np.random.rand() < self.learning_rate:\n                self.velocities[i] += np.random.uniform(-0.05, 0.05, self.dim)  # More focused perturbation\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_swarm(lb, ub)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            for i in range(self.swarm_size):\n                if evaluations >= self.budget:\n                    break\n\n                current_value = func(self.positions[i])\n                evaluations += 1\n\n                if current_value < self.personal_best_values[i]:\n                    self.personal_best_values[i] = current_value\n                    self.personal_best_positions[i] = self.positions[i].copy()\n\n                if current_value < self.global_best_value:\n                    self.global_best_value = current_value\n                    self.global_best_position = self.positions[i].copy()\n\n            self.dynamic_inertia_adjustment()\n            self.adaptive_learning_strategy()\n\n            for i in range(self.swarm_size):\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                cognitive_component = self.c1 * r1 * (self.personal_best_positions[i] - self.positions[i])\n                social_component = self.c2 * r2 * (self.global_best_position - self.positions[i])\n                self.velocities[i] = self.w * self.velocities[i] + cognitive_component + social_component\n                self.positions[i] += self.velocities[i]\n\n                # Quantum-inspired position update\n                if np.random.rand() < self.adapt_rate:\n                    self.positions[i] = self.quantum_position_update(self.positions[i], self.global_best_position)\n\n                # Adaptive neighborhood search\n                if np.random.rand() < self.adapt_rate:\n                    self.positions[i] = self.adaptive_neighborhood_search(self.positions[i], lb, ub)\n\n        return self.global_best_position, self.global_best_value", "name": "DyQASO", "description": "The Dynamic Quantum Adaptive Swarm Optimization (DyQASO) enhances exploration with dynamic inertia, quantum-inspired position updates, and adaptive neighborhood strategies, refining balance between exploration and exploitation with multi-layered adaptivity.", "configspace": "", "generation": 45, "fitness": 0.27741367459945715, "feedback": "The algorithm DyQASO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28 with standard deviation 0.00.", "error": "", "parent_id": "e93089d8-8277-496b-b724-d6777b124d39", "metadata": {"aucs": [0.27678567010720523, 0.27775377994127803, 0.27770157374988824]}, "mutation_prompt": null}
{"id": "62234ef7-af57-4f0e-99bb-6d028669addb", "solution": "import numpy as np\n\nclass QADE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(50, budget)\n        self.mutation_factor = 0.5\n        self.crossover_probability = 0.9\n        self.positions = None\n        self.bounds = None\n\n    def initialize_population(self, lb, ub):\n        self.positions = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        self.bounds = (lb, ub)\n\n    def quantum_mutation(self, target_idx):\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        selected = np.random.choice(indices, 3, replace=False)\n        a, b, c = self.positions[selected]\n        mutation_vector = a + self.mutation_factor * (b - c)\n        beta = np.random.normal(0, 1, self.dim)\n        delta = np.random.normal(0, 1, self.dim)\n        quantum_mutation_vector = mutation_vector + beta * (self.positions[target_idx] - mutation_vector) + delta * 0.1\n        lb, ub = self.bounds\n        return np.clip(quantum_mutation_vector, lb, ub)\n\n    def crossover(self, target_vector, donor_vector):\n        crossover_mask = np.random.rand(self.dim) < self.crossover_probability\n        trial_vector = np.where(crossover_mask, donor_vector, target_vector)\n        lb, ub = self.bounds\n        return np.clip(trial_vector, lb, ub)\n\n    def adapt_parameters(self, iteration, max_iterations):\n        self.mutation_factor = 0.5 + 0.5 * (iteration / max_iterations)\n        self.crossover_probability = 0.9 - 0.4 * (iteration / max_iterations)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        evaluations = 0\n        best_position = None\n        best_value = np.inf\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                donor_vector = self.quantum_mutation(i)\n                trial_vector = self.crossover(self.positions[i], donor_vector)\n                trial_value = func(trial_vector)\n                evaluations += 1\n\n                if trial_value < func(self.positions[i]):\n                    self.positions[i] = trial_vector\n\n                if trial_value < best_value:\n                    best_value = trial_value\n                    best_position = trial_vector\n\n            self.adapt_parameters(evaluations, self.budget)\n\n        return best_position, best_value", "name": "QADE", "description": "Quantum Adaptive Differential Evolution (QADE) integrates quantum-inspired mutation and dynamic parameter tuning to achieve robust global optimization capabilities.", "configspace": "", "generation": 46, "fitness": 0.2512467904262739, "feedback": "The algorithm QADE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.25 with standard deviation 0.00.", "error": "", "parent_id": "e93089d8-8277-496b-b724-d6777b124d39", "metadata": {"aucs": [0.2519760162913365, 0.25103622319188246, 0.2507281317956028]}, "mutation_prompt": null}
{"id": "c844c40c-1a1a-49a0-b64b-33f2ba22ce0a", "solution": "import numpy as np\n\nclass QI_ADE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = min(50, budget)\n        self.population = None\n        self.fitness = None\n        self.best_individual = None\n        self.best_fitness = np.inf\n        self.cr = 0.9  # Crossover rate\n        self.f = 0.8  # Differential weight\n        self.beta = 0.1  # Quantum influence factor\n        self.bounds = None\n\n    def initialize_population(self, lb, ub):\n        self.population = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        self.fitness = np.full(self.pop_size, np.inf)\n        self.bounds = (lb, ub)\n\n    def quantum_update(self, target, best):\n        direction = best - target\n        quantum_shift = self.beta * np.random.normal(0, 1, self.dim)\n        new_position = target + direction * quantum_shift\n        lb, ub = self.bounds\n        return np.clip(new_position, lb, ub)\n\n    def mutate(self, i):\n        indices = np.random.choice(self.pop_size, 3, replace=False)\n        while i in indices:\n            indices = np.random.choice(self.pop_size, 3, replace=False)\n        a, b, c = self.population[indices]\n        mutant = a + self.f * (b - c)\n        lb, ub = self.bounds\n        return np.clip(mutant, lb, ub)\n\n    def crossover(self, target, mutant):\n        crossover_mask = np.random.rand(self.dim) < self.cr\n        if not np.any(crossover_mask):\n            crossover_mask[np.random.randint(0, self.dim)] = True\n        trial = np.where(crossover_mask, mutant, target)\n        return trial\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            for i in range(self.pop_size):\n                if evaluations >= self.budget:\n                    break\n\n                current_fitness = func(self.population[i])\n                evaluations += 1\n\n                if current_fitness < self.fitness[i]:\n                    self.fitness[i] = current_fitness\n\n                if current_fitness < self.best_fitness:\n                    self.best_fitness = current_fitness\n                    self.best_individual = self.population[i].copy()\n\n            for i in range(self.pop_size):\n                mutant = self.mutate(i)\n                trial = self.crossover(self.population[i], mutant)\n\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_fitness\n\n                    if trial_fitness < self.best_fitness:\n                        self.best_fitness = trial_fitness\n                        self.best_individual = trial.copy()\n\n                # Quantum-inspired update\n                self.population[i] = self.quantum_update(self.population[i], self.best_individual)\n\n        return self.best_individual, self.best_fitness", "name": "QI_ADE", "description": "Quantum-Inspired Adaptive Differential Evolution (QI-ADE) combines quantum behavior with adaptive differential evolution strategies for enhanced global search and convergence.", "configspace": "", "generation": 47, "fitness": 0.2645372089928713, "feedback": "The algorithm QI_ADE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.26 with standard deviation 0.00.", "error": "", "parent_id": "e93089d8-8277-496b-b724-d6777b124d39", "metadata": {"aucs": [0.2679759111736707, 0.26316889476985694, 0.26246682103508634]}, "mutation_prompt": null}
{"id": "1bc08692-5730-48bf-9a0d-1b3c2312bec9", "solution": "import numpy as np\n\nclass Modified_EQIPSO_ADLS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = min(50, budget)\n        self.positions = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_values = None\n        self.global_best_position = None\n        self.global_best_value = np.inf\n        self.w = 0.9  # Initial inertia weight\n        self.c1 = 1.5  # Cognitive coefficient\n        self.c2 = 1.5  # Social coefficient\n        self.adapt_rate = 0.1\n        self.bounds = None\n        self.learning_rate = 0.1  # Learning rate for dynamic updates\n        self.inertia_decay = 0.99  # New inertia weight decay factor\n\n    def initialize_swarm(self, lb, ub):\n        self.positions = np.random.uniform(lb, ub, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.swarm_size, self.dim))\n        self.personal_best_positions = self.positions.copy()\n        self.personal_best_values = np.full(self.swarm_size, np.inf)\n        self.bounds = (lb, ub)\n\n    def quantum_position_update(self, position, global_best):\n        beta = np.random.normal(0, 1, self.dim)\n        delta = np.random.normal(0, 1, self.dim)\n        localized_factor = 0.05 * np.exp(-np.linalg.norm(position - global_best) / self.dim)\n        new_position = position + beta * (global_best - position) * 0.1 + delta * localized_factor\n        lb, ub = self.bounds\n        return np.clip(new_position, lb, ub)\n\n    def adaptive_neighborhood_search(self, position, lb, ub):\n        neighborhood_radius = np.exp(-self.dim / self.swarm_size)\n        perturbation = np.random.uniform(-neighborhood_radius, neighborhood_radius, self.dim)\n        new_position = position + perturbation\n        return np.clip(new_position, lb, ub)\n\n    def dynamic_inertia_adjustment(self):\n        diversity = np.mean(np.std(self.positions, axis=0))\n        max_diversity = np.sqrt(np.sum((self.bounds[1] - self.bounds[0])**2))\n        self.w = max(0.4, 0.9 * (diversity / max_diversity)) * self.inertia_decay\n\n    def adaptive_learning_strategy(self):\n        for i in range(self.swarm_size):\n            if np.random.rand() < self.learning_rate:\n                self.velocities[i] += np.random.uniform(-0.1, 0.1, self.dim)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_swarm(lb, ub)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            for i in range(self.swarm_size):\n                if evaluations >= self.budget:\n                    break\n\n                current_value = func(self.positions[i])\n                evaluations += 1\n\n                if current_value < self.personal_best_values[i]:\n                    self.personal_best_values[i] = current_value\n                    self.personal_best_positions[i] = self.positions[i].copy()\n\n                if current_value < self.global_best_value:\n                    self.global_best_value = current_value\n                    self.global_best_position = self.positions[i].copy()\n\n            self.dynamic_inertia_adjustment()\n            self.adaptive_learning_strategy()\n\n            for i in range(self.swarm_size):\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                cognitive_component = self.c1 * r1 * (self.personal_best_positions[i] - self.positions[i])\n                social_component = self.c2 * r2 * (self.global_best_position - self.positions[i])\n                self.velocities[i] = self.w * self.velocities[i] + cognitive_component + social_component\n                self.positions[i] += self.velocities[i]\n\n                # Quantum-inspired position update\n                if np.random.rand() < self.adapt_rate:\n                    self.positions[i] = self.quantum_position_update(self.positions[i], self.global_best_position)\n\n                # Adaptive neighborhood search\n                if np.random.rand() < self.adapt_rate:\n                    self.positions[i] = self.adaptive_neighborhood_search(self.positions[i], lb, ub)\n\n        return self.global_best_position, self.global_best_value", "name": "Modified_EQIPSO_ADLS", "description": "The Modified EQIPSO-ADLS employs dynamic exploitation-intensification balancing via a novel inertia weight decay and localized quantum perturbation, enhancing convergence speed and solution quality.", "configspace": "", "generation": 48, "fitness": 0.27776723959685506, "feedback": "The algorithm Modified_EQIPSO_ADLS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28 with standard deviation 0.00.", "error": "", "parent_id": "e93089d8-8277-496b-b724-d6777b124d39", "metadata": {"aucs": [0.27790420566234286, 0.2776025897556651, 0.2777949233725573]}, "mutation_prompt": null}
{"id": "2ddd596e-9d9f-4837-9b5a-3c694076e7c3", "solution": "import numpy as np\n\nclass QuantumEnhancedPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = min(50, budget)\n        self.positions = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_values = None\n        self.global_best_position = None\n        self.global_best_value = np.inf\n        self.w = 0.9\n        self.c1 = 1.5\n        self.c2 = 1.5\n        self.adapt_rate = 0.1\n        self.bounds = None\n        self.learning_rate = 0.1\n\n    def initialize_swarm(self, lb, ub):\n        self.positions = np.random.uniform(lb, ub, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.swarm_size, self.dim))\n        self.personal_best_positions = self.positions.copy()\n        self.personal_best_values = np.full(self.swarm_size, np.inf)\n        self.bounds = (lb, ub)\n\n    def quantum_position_update(self, position, global_best):\n        beta = np.random.normal(0, 1, self.dim)\n        delta = np.random.normal(0, 1, self.dim)\n        new_position = position + beta * (global_best - position) / 2 + delta * 0.1\n        lb, ub = self.bounds\n        return np.clip(new_position, lb, ub)\n\n    def adaptive_neighborhood_search(self, position, lb, ub):\n        neighborhood_radius = np.exp(-self.dim / self.swarm_size)\n        perturbation = np.random.uniform(-neighborhood_radius, neighborhood_radius, self.dim)\n        new_position = position + perturbation\n        return np.clip(new_position, lb, ub)\n    \n    def dynamic_inertia_adjustment(self):\n        diversity = np.mean(np.std(self.positions, axis=0))\n        max_diversity = np.sqrt(np.sum((self.bounds[1] - self.bounds[0])**2))\n        self.w = 0.4 + 0.5 * (diversity / max_diversity)\n\n    def diversity_driven_learning(self):\n        diversity = np.mean(np.linalg.norm(self.positions - self.global_best_position, axis=1))\n        learning_factor = np.tanh(diversity)\n        for i in range(self.swarm_size):\n            if np.random.rand() < self.learning_rate * learning_factor:\n                self.velocities[i] += np.random.uniform(-0.1, 0.1, self.dim)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_swarm(lb, ub)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            for i in range(self.swarm_size):\n                if evaluations >= self.budget:\n                    break\n\n                current_value = func(self.positions[i])\n                evaluations += 1\n\n                if current_value < self.personal_best_values[i]:\n                    self.personal_best_values[i] = current_value\n                    self.personal_best_positions[i] = self.positions[i].copy()\n\n                if current_value < self.global_best_value:\n                    self.global_best_value = current_value\n                    self.global_best_position = self.positions[i].copy()\n\n            self.dynamic_inertia_adjustment()\n            self.diversity_driven_learning()\n\n            for i in range(self.swarm_size):\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                cognitive_component = self.c1 * r1 * (self.personal_best_positions[i] - self.positions[i])\n                social_component = self.c2 * r2 * (self.global_best_position - self.positions[i])\n                self.velocities[i] = self.w * self.velocities[i] + cognitive_component + social_component\n                self.positions[i] += self.velocities[i]\n\n                if np.random.rand() < self.adapt_rate:\n                    self.positions[i] = self.quantum_position_update(self.positions[i], self.global_best_position)\n\n                if np.random.rand() < self.adapt_rate:\n                    self.positions[i] = self.adaptive_neighborhood_search(self.positions[i], lb, ub)\n\n        return self.global_best_position, self.global_best_value", "name": "QuantumEnhancedPSO", "description": "Quantum-Enhanced Particle Swarm uses a quantum well strategy for enhanced exploration and a diversity-driven adaptive learning strategy to improve convergence and adaptability.", "configspace": "", "generation": 49, "fitness": 0.27779646991635304, "feedback": "The algorithm QuantumEnhancedPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28 with standard deviation 0.00.", "error": "", "parent_id": "e93089d8-8277-496b-b724-d6777b124d39", "metadata": {"aucs": [0.2777859580924279, 0.27785877060482245, 0.2777446810518087]}, "mutation_prompt": null}
{"id": "947daf14-180c-4ec3-addc-4d25bc20606c", "solution": "import numpy as np\n\nclass HQIPSO_AMF:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = min(50, budget)\n        self.positions = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_values = None\n        self.global_best_position = None\n        self.global_best_value = np.inf\n        self.w = 0.9  # Initial inertia weight\n        self.c1 = 1.5  # Cognitive coefficient\n        self.c2 = 1.5  # Social coefficient\n        self.adapt_rate = 0.1\n        self.bounds = None\n        self.learning_rate = 0.1  # Learning rate for dynamic updates\n        self.memory_coeff = 0.5  # Coefficient for memory influence\n        self.feedback_strength = 0.1  # Feedback influence factor\n\n    def initialize_swarm(self, lb, ub):\n        self.positions = np.random.uniform(lb, ub, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.swarm_size, self.dim))\n        self.personal_best_positions = self.positions.copy()\n        self.personal_best_values = np.full(self.swarm_size, np.inf)\n        self.bounds = (lb, ub)\n\n    def quantum_position_update(self, position, global_best):\n        beta = np.random.normal(0, 1, self.dim)\n        delta = np.random.normal(0, 1, self.dim)\n        new_position = position + beta * (global_best - position) + delta * 0.1\n        lb, ub = self.bounds\n        return np.clip(new_position, lb, ub)\n\n    def adaptive_neighborhood_search(self, position, lb, ub):\n        neighborhood_radius = np.exp(-self.dim / self.swarm_size)\n        perturbation = np.random.uniform(-neighborhood_radius, neighborhood_radius, self.dim)\n        new_position = position + perturbation\n        return np.clip(new_position, lb, ub)\n    \n    def dynamic_inertia_adjustment(self):\n        diversity = np.mean(np.std(self.positions, axis=0))\n        max_diversity = np.sqrt(np.sum((self.bounds[1] - self.bounds[0])**2))\n        self.w = 0.4 + 0.5 * (diversity / max_diversity)\n\n    def adaptive_learning_strategy(self):\n        for i in range(self.swarm_size):\n            if np.random.rand() < self.learning_rate:\n                self.velocities[i] += np.random.uniform(-0.1, 0.1, self.dim)\n\n    def feedback_adjustment(self):\n        for i in range(self.swarm_size):\n            feedback = self.feedback_strength * np.random.uniform(-1, 1, self.dim)\n            self.positions[i] += feedback * (self.global_best_position - self.positions[i])\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_swarm(lb, ub)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            for i in range(self.swarm_size):\n                if evaluations >= self.budget:\n                    break\n\n                current_value = func(self.positions[i])\n                evaluations += 1\n\n                if current_value < self.personal_best_values[i]:\n                    self.personal_best_values[i] = current_value\n                    self.personal_best_positions[i] = self.positions[i].copy()\n\n                if current_value < self.global_best_value:\n                    self.global_best_value = current_value\n                    self.global_best_position = self.positions[i].copy()\n\n            self.dynamic_inertia_adjustment()\n            self.adaptive_learning_strategy()\n            self.feedback_adjustment()\n\n            for i in range(self.swarm_size):\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                cognitive_component = self.c1 * r1 * (self.personal_best_positions[i] - self.positions[i])\n                social_component = self.c2 * r2 * (self.global_best_position - self.positions[i])\n                self.velocities[i] = self.w * self.velocities[i] + cognitive_component + social_component\n                self.positions[i] += self.velocities[i]\n\n                # Quantum-inspired position update\n                if np.random.rand() < self.adapt_rate:\n                    self.positions[i] = self.quantum_position_update(self.positions[i], self.global_best_position)\n\n                # Adaptive neighborhood search\n                if np.random.rand() < self.adapt_rate:\n                    self.positions[i] = self.adaptive_neighborhood_search(self.positions[i], lb, ub)\n\n        return self.global_best_position, self.global_best_value", "name": "HQIPSO_AMF", "description": "Hybrid Quantum-Inspired Particle Swarm Optimization with Adaptive Memory and Feedback Mechanism (HQIPSO-AMF) enhances convergence by incorporating adaptive memory strategies and feedback to fine-tune exploration-exploitation dynamics.", "configspace": "", "generation": 50, "fitness": 0.27762100920997196, "feedback": "The algorithm HQIPSO_AMF got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28 with standard deviation 0.00.", "error": "", "parent_id": "e93089d8-8277-496b-b724-d6777b124d39", "metadata": {"aucs": [0.277784941689584, 0.27787941876321887, 0.27719866717711295]}, "mutation_prompt": null}
{"id": "62c36a23-690e-469c-a45e-351f9f15778a", "solution": "import numpy as np\n\nclass QMHS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.harmony_size = min(50, budget)\n        self.harmonies = None\n        self.harmony_memory = None\n        self.harmony_values = None\n        self.best_harmony = None\n        self.best_value = np.inf\n        self.hmcr = 0.9  # Harmony Memory Consideration Rate\n        self.par = 0.3   # Pitch Adjustment Rate\n        self.fw = 0.02   # Fret Width for pitch adjustment\n        self.adapt_rate = 0.1\n        self.bounds = None\n\n    def initialize_harmonies(self, lb, ub):\n        self.harmonies = np.random.uniform(lb, ub, (self.harmony_size, self.dim))\n        self.harmony_memory = self.harmonies.copy()\n        self.harmony_values = np.full(self.harmony_size, np.inf)\n        self.bounds = (lb, ub)\n\n    def quantum_position_update(self, harmony, best_harmony):\n        beta = np.random.normal(0, 1, self.dim)\n        new_harmony = harmony + beta * (best_harmony - harmony)\n        lb, ub = self.bounds\n        return np.clip(new_harmony, lb, ub)\n\n    def pitch_adjustment(self, harmony, lb, ub):\n        adjustment = np.random.uniform(-self.fw, self.fw, self.dim)\n        return np.clip(harmony + adjustment, lb, ub)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_harmonies(lb, ub)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            for i in range(self.harmony_size):\n                if evaluations >= self.budget:\n                    break\n\n                current_value = func(self.harmonies[i])\n                evaluations += 1\n\n                if current_value < self.harmony_values[i]:\n                    self.harmony_values[i] = current_value\n                    self.harmony_memory[i] = self.harmonies[i].copy()\n\n                if current_value < self.best_value:\n                    self.best_value = current_value\n                    self.best_harmony = self.harmonies[i].copy()\n\n            new_harmony = np.zeros(self.dim)\n            for d in range(self.dim):\n                if np.random.rand() < self.hmcr:\n                    new_harmony[d] = self.harmony_memory[np.random.randint(self.harmony_size)][d]\n                    if np.random.rand() < self.par:\n                        new_harmony[d] = self.pitch_adjustment(new_harmony, lb, ub)[d]\n                else:\n                    new_harmony[d] = np.random.uniform(lb[d], ub[d])\n\n            if np.random.rand() < self.adapt_rate:\n                new_harmony = self.quantum_position_update(new_harmony, self.best_harmony)\n\n            new_value = func(new_harmony)\n            evaluations += 1\n\n            if new_value < self.best_value:\n                self.best_value = new_value\n                self.best_harmony = new_harmony.copy()\n\n        return self.best_harmony, self.best_value", "name": "QMHS", "description": "Quantum Mimetic Harmony Search (QMHS) fuses quantum-inspired position updates with harmony search principles to enhance solution diversity and convergence in optimization tasks.", "configspace": "", "generation": 51, "fitness": 0.24389154242306976, "feedback": "The algorithm QMHS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.24 with standard deviation 0.00.", "error": "", "parent_id": "e93089d8-8277-496b-b724-d6777b124d39", "metadata": {"aucs": [0.2424929155831974, 0.2423156609642203, 0.24686605072179157]}, "mutation_prompt": null}
{"id": "8d9adc12-8700-4747-aff0-cb04273fefcd", "solution": "import numpy as np\n\nclass EQIPSO_TALM:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = min(50, budget)\n        self.positions = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_values = None\n        self.global_best_position = None\n        self.global_best_value = np.inf\n        self.w = 0.9  # Initial inertia weight\n        self.c1 = 1.5  # Cognitive coefficient\n        self.c2 = 1.5  # Social coefficient\n        self.adapt_rate = 0.1\n        self.bounds = None\n        self.learning_rate = 0.1  # Initial learning rate\n        self.memory = np.full((self.swarm_size, self.dim), np.inf)\n\n    def initialize_swarm(self, lb, ub):\n        self.positions = np.random.uniform(lb, ub, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.swarm_size, self.dim))\n        self.personal_best_positions = self.positions.copy()\n        self.personal_best_values = np.full(self.swarm_size, np.inf)\n        self.bounds = (lb, ub)\n\n    def quantum_position_update(self, position, global_best):\n        beta = np.random.normal(0, 1, self.dim)\n        delta = np.random.normal(0, 1, self.dim)\n        new_position = position + beta * (global_best - position) + delta * 0.1\n        lb, ub = self.bounds\n        return np.clip(new_position, lb, ub)\n\n    def dynamic_inertia_adjustment(self):\n        diversity = np.mean(np.std(self.positions, axis=0))\n        max_diversity = np.sqrt(np.sum((self.bounds[1] - self.bounds[0])**2))\n        self.w = 0.4 + 0.5 * (diversity / max_diversity)\n\n    def temporal_adaptive_learning_strategy(self, iteration, max_iterations):\n        factor = iteration / max_iterations\n        self.learning_rate = 0.1 * (1 - factor)\n        for i in range(self.swarm_size):\n            if np.random.rand() < self.learning_rate:\n                self.velocities[i] += np.random.uniform(-0.1, 0.1, self.dim)\n\n    def update_memory(self, i, current_position):\n        self.memory[i] = np.where(current_position < self.memory[i], current_position, self.memory[i])\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_swarm(lb, ub)\n        evaluations = 0\n        iteration = 0\n        max_iterations = self.budget // self.swarm_size\n\n        while evaluations < self.budget:\n            for i in range(self.swarm_size):\n                if evaluations >= self.budget:\n                    break\n\n                current_value = func(self.positions[i])\n                evaluations += 1\n\n                if current_value < self.personal_best_values[i]:\n                    self.personal_best_values[i] = current_value\n                    self.personal_best_positions[i] = self.positions[i].copy()\n\n                if current_value < self.global_best_value:\n                    self.global_best_value = current_value\n                    self.global_best_position = self.positions[i].copy()\n\n                self.update_memory(i, self.positions[i])\n\n            self.dynamic_inertia_adjustment()\n            self.temporal_adaptive_learning_strategy(iteration, max_iterations)\n\n            for i in range(self.swarm_size):\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                cognitive_component = self.c1 * r1 * (self.personal_best_positions[i] - self.positions[i])\n                social_component = self.c2 * r2 * (self.global_best_position - self.positions[i])\n                self.velocities[i] = self.w * self.velocities[i] + cognitive_component + social_component\n                self.positions[i] += self.velocities[i]\n\n                # Quantum-inspired position update\n                if np.random.rand() < self.adapt_rate:\n                    self.positions[i] = self.quantum_position_update(self.positions[i], self.global_best_position)\n\n            iteration += 1\n\n        return self.global_best_position, self.global_best_value", "name": "EQIPSO_TALM", "description": "Enhanced Quantum-Inspired Particle Swarm with Temporal Adaptive Learning and Memory (EQIPSO-TALM) integrates temporal adaptations to learning rates and introduces a memory mechanism for improved convergence in photonic structure optimization.", "configspace": "", "generation": 52, "fitness": 0.2777623662487761, "feedback": "The algorithm EQIPSO_TALM got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28 with standard deviation 0.00.", "error": "", "parent_id": "e93089d8-8277-496b-b724-d6777b124d39", "metadata": {"aucs": [0.277692682427361, 0.27788122476769705, 0.27771319155127017]}, "mutation_prompt": null}
{"id": "df0ee6c0-e51c-4050-b863-8b12b27f014b", "solution": "import numpy as np\n\nclass QuantumDrivenDE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(50, budget)\n        self.mutation_factor = 0.5\n        self.crossover_prob = 0.9\n        self.bounds = None\n        self.population = None\n        self.best_individual = None\n        self.best_value = np.inf\n\n    def initialize_population(self, lb, ub):\n        self.population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        self.bounds = (lb, ub)\n\n    def quantum_update(self, individual, best_individual):\n        beta = np.random.normal(0, 1, self.dim)\n        delta = np.random.normal(0, 1, self.dim)\n        new_position = individual + beta * (best_individual - individual) + delta * 0.1\n        lb, ub = self.bounds\n        return np.clip(new_position, lb, ub)\n\n    def differential_mutation(self, idx):\n        candidates = list(range(self.population_size))\n        candidates.remove(idx)\n        a, b, c = np.random.choice(candidates, 3, replace=False)\n        mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        lb, ub = self.bounds\n        return np.clip(mutant, lb, ub)\n\n    def crossover(self, target, mutant):\n        jrand = np.random.randint(self.dim)\n        trial = np.zeros(self.dim)\n        for j in range(self.dim):\n            if np.random.rand() < self.crossover_prob or j == jrand:\n                trial[j] = mutant[j]\n            else:\n                trial[j] = target[j]\n        return trial\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                target = self.population[i]\n                mutant = self.differential_mutation(i)\n                trial = self.crossover(target, mutant)\n\n                trial_value = func(trial)\n                evaluations += 1\n\n                if trial_value < self.best_value:\n                    self.best_value = trial_value\n                    self.best_individual = trial.copy()\n\n                target_value = func(target)\n                if trial_value < target_value:\n                    self.population[i] = trial\n\n                # Quantum-inspired update\n                if np.random.rand() < 0.2:\n                    self.population[i] = self.quantum_update(self.population[i], self.best_individual)\n\n        return self.best_individual, self.best_value", "name": "QuantumDrivenDE", "description": "Quantum-Driven Differential Evolution (QDE) uses quantum dynamics to guide differential evolution for enhanced global exploration and local exploitation.", "configspace": "", "generation": 53, "fitness": 0.2690841497425079, "feedback": "The algorithm QuantumDrivenDE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.27 with standard deviation 0.00.", "error": "", "parent_id": "e93089d8-8277-496b-b724-d6777b124d39", "metadata": {"aucs": [0.27149569895916714, 0.2670414723955712, 0.2687152778727854]}, "mutation_prompt": null}
{"id": "1610794f-2bf8-42b4-a9ab-5a99cadd851f", "solution": "import numpy as np\n\nclass QEDHS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.harmony_size = min(50, budget)\n        self.positions = None\n        self.energies = None\n        self.personal_best_positions = None\n        self.personal_best_values = None\n        self.global_best_position = None\n        self.global_best_value = np.inf\n        self.hmcr = 0.9  # Harmony memory consideration rate\n        self.par = 0.3  # Pitch adjustment rate\n        self.adapt_rate = 0.1\n        self.bounds = None\n        self.learning_rate = 0.1  # Learning rate for dynamic updates\n\n    def initialize_harmony(self, lb, ub):\n        self.positions = np.random.uniform(lb, ub, (self.harmony_size, self.dim))\n        self.energies = np.full(self.harmony_size, np.inf)\n        self.personal_best_positions = self.positions.copy()\n        self.personal_best_values = np.full(self.harmony_size, np.inf)\n        self.bounds = (lb, ub)\n\n    def quantum_position_update(self, position, global_best):\n        beta = np.random.normal(0, 1, self.dim)\n        delta = np.random.normal(0, 1, self.dim)\n        new_position = position + beta * (global_best - position) + delta * 0.1\n        lb, ub = self.bounds\n        return np.clip(new_position, lb, ub)\n\n    def adaptive_harmony_search(self, position, lb, ub):\n        perturbation = np.random.uniform(-self.par, self.par, self.dim)\n        new_position = position + perturbation\n        return np.clip(new_position, lb, ub)\n\n    def adaptive_learning_strategy(self):\n        for i in range(self.harmony_size):\n            if np.random.rand() < self.learning_rate:\n                self.positions[i] += np.random.uniform(-0.1, 0.1, self.dim)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_harmony(lb, ub)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            for i in range(self.harmony_size):\n                if evaluations >= self.budget:\n                    break\n\n                current_value = func(self.positions[i])\n                evaluations += 1\n\n                if current_value < self.personal_best_values[i]:\n                    self.personal_best_values[i] = current_value\n                    self.personal_best_positions[i] = self.positions[i].copy()\n\n                if current_value < self.global_best_value:\n                    self.global_best_value = current_value\n                    self.global_best_position = self.positions[i].copy()\n\n            self.adaptive_learning_strategy()\n\n            for i in range(self.harmony_size):\n                new_position = self.positions[i].copy()\n\n                if np.random.rand() < self.hmcr:\n                    random_index = np.random.randint(self.harmony_size)\n                    new_position = self.positions[random_index].copy()\n                \n                if np.random.rand() < self.par:\n                    new_position = self.adaptive_harmony_search(new_position, lb, ub)\n\n                if np.random.rand() < self.adapt_rate:\n                    new_position = self.quantum_position_update(new_position, self.global_best_position)\n\n                new_value = func(new_position)\n                if new_value < self.energies[i]:\n                    self.positions[i] = new_position\n                    self.energies[i] = new_value\n\n        return self.global_best_position, self.global_best_value", "name": "QEDHS", "description": "Quantum-Enhanced Dynamic Harmony Search (QEDHS) integrates quantum-inspired position updates with harmony search principles to enhance diversity and convergence in complex search spaces.", "configspace": "", "generation": 54, "fitness": 0.2533031173029068, "feedback": "The algorithm QEDHS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.25 with standard deviation 0.01.", "error": "", "parent_id": "e93089d8-8277-496b-b724-d6777b124d39", "metadata": {"aucs": [0.24692854057025482, 0.2490519021272165, 0.263928909211249]}, "mutation_prompt": null}
{"id": "92eb56ba-3232-49fd-998e-f8e23c3f239b", "solution": "import numpy as np\n\nclass EQIPSO_MMES:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = min(50, budget)\n        self.positions = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_values = None\n        self.global_best_position = None\n        self.global_best_value = np.inf\n        self.w = 0.9  # Initial inertia weight\n        self.c1 = 1.5  # Cognitive coefficient\n        self.c2 = 1.5  # Social coefficient\n        self.adapt_rate = 0.1\n        self.bounds = None\n        self.learning_rate = 0.1  # Learning rate for dynamic updates\n        self.constriction_factor = 0.729  # Constriction factor for velocity\n\n    def initialize_swarm(self, lb, ub):\n        self.positions = np.random.uniform(lb, ub, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.swarm_size, self.dim))\n        self.personal_best_positions = self.positions.copy()\n        self.personal_best_values = np.full(self.swarm_size, np.inf)\n        self.bounds = (lb, ub)\n\n    def quantum_position_update(self, position, global_best):\n        beta = np.random.normal(0, 1, self.dim)\n        delta = np.random.normal(0, 1, self.dim)\n        new_position = position + beta * (global_best - position) + delta * 0.05\n        lb, ub = self.bounds\n        return np.clip(new_position, lb, ub)\n\n    def multi_modal_exploration(self, position, lb, ub):\n        step_size = np.random.uniform(0.05, 0.2, self.dim)\n        perturbation = np.random.uniform(-step_size, step_size, self.dim)\n        new_position = position + perturbation\n        return np.clip(new_position, lb, ub)\n\n    def dynamic_inertia_adjustment(self):\n        diversity = np.mean(np.std(self.positions, axis=0))\n        max_diversity = np.sqrt(np.sum((self.bounds[1] - self.bounds[0])**2))\n        self.w = 0.4 + 0.5 * (diversity / max_diversity)\n\n    def adaptive_learning_strategy(self):\n        for i in range(self.swarm_size):\n            if np.random.rand() < self.learning_rate:\n                self.velocities[i] += np.random.uniform(-0.1, 0.1, self.dim)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_swarm(lb, ub)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            for i in range(self.swarm_size):\n                if evaluations >= self.budget:\n                    break\n\n                current_value = func(self.positions[i])\n                evaluations += 1\n\n                if current_value < self.personal_best_values[i]:\n                    self.personal_best_values[i] = current_value\n                    self.personal_best_positions[i] = self.positions[i].copy()\n\n                if current_value < self.global_best_value:\n                    self.global_best_value = current_value\n                    self.global_best_position = self.positions[i].copy()\n\n            self.dynamic_inertia_adjustment()\n            self.adaptive_learning_strategy()\n\n            for i in range(self.swarm_size):\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                cognitive_component = self.c1 * r1 * (self.personal_best_positions[i] - self.positions[i])\n                social_component = self.c2 * r2 * (self.global_best_position - self.positions[i])\n                self.velocities[i] = self.constriction_factor * (self.w * self.velocities[i] + cognitive_component + social_component)\n                self.positions[i] += self.velocities[i]\n\n                # Quantum-inspired position update\n                if np.random.rand() < self.adapt_rate:\n                    self.positions[i] = self.quantum_position_update(self.positions[i], self.global_best_position)\n\n                # Multi-modal exploration\n                if np.random.rand() < self.adapt_rate:\n                    self.positions[i] = self.multi_modal_exploration(self.positions[i], lb, ub)\n\n        return self.global_best_position, self.global_best_value", "name": "EQIPSO_MMES", "description": "Enhanced Quantum-Inspired Particle Swarm with Multi-Modal Exploration Strategy (EQIPSO-MMES) integrates multi-modal exploration and adaptive convergence mechanisms to improve global search efficiency and solution accuracy.", "configspace": "", "generation": 55, "fitness": 0.2763622035236857, "feedback": "The algorithm EQIPSO_MMES got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28 with standard deviation 0.00.", "error": "", "parent_id": "e93089d8-8277-496b-b724-d6777b124d39", "metadata": {"aucs": [0.27788423718139255, 0.27450085750111153, 0.27670151588855296]}, "mutation_prompt": null}
{"id": "001193fc-e347-4ac0-90fd-0cfb6775d5cb", "solution": "import numpy as np\n\nclass QAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = min(50, budget)\n        self.positions = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_values = None\n        self.global_best_position = None\n        self.global_best_value = np.inf\n        self.w = 0.9  # Initial inertia weight\n        self.c1 = 1.5  # Cognitive coefficient\n        self.c2 = 1.5  # Social coefficient\n        self.adapt_rate = 0.1\n        self.bounds = None\n        self.learning_rate = 0.1  # Learning rate for dynamic updates\n        self.beta_scale = 0.1  # Scale for quantum update\n        self.neighborhood_scale = 0.1  # Scale for neighborhood perturbation\n\n    def initialize_swarm(self, lb, ub):\n        self.positions = np.random.uniform(lb, ub, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.swarm_size, self.dim))\n        self.personal_best_positions = self.positions.copy()\n        self.personal_best_values = np.full(self.swarm_size, np.inf)\n        self.bounds = (lb, ub)\n\n    def quantum_position_update(self, position, global_best):\n        beta = np.random.normal(0, self.beta_scale, self.dim)\n        delta = np.random.normal(0, self.beta_scale, self.dim)\n        new_position = position + beta * (global_best - position) + delta\n        lb, ub = self.bounds\n        return np.clip(new_position, lb, ub)\n\n    def adaptive_neighborhood_search(self, position):\n        neighborhood_radius = np.exp(-self.dim / self.swarm_size) * self.neighborhood_scale\n        perturbation = np.random.uniform(-neighborhood_radius, neighborhood_radius, self.dim)\n        new_position = position + perturbation\n        lb, ub = self.bounds\n        return np.clip(new_position, lb, ub)\n    \n    def dynamic_inertia_adjustment(self):\n        diversity = np.mean(np.std(self.positions, axis=0))\n        max_diversity = np.sqrt(np.sum((self.bounds[1] - self.bounds[0])**2))\n        self.w = 0.4 + 0.5 * (diversity / max_diversity)\n\n    def adaptive_learning_strategy(self):\n        for i in range(self.swarm_size):\n            if np.random.rand() < self.learning_rate:\n                self.velocities[i] += np.random.uniform(-self.beta_scale, self.beta_scale, self.dim)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_swarm(lb, ub)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            for i in range(self.swarm_size):\n                if evaluations >= self.budget:\n                    break\n\n                current_value = func(self.positions[i])\n                evaluations += 1\n\n                if current_value < self.personal_best_values[i]:\n                    self.personal_best_values[i] = current_value\n                    self.personal_best_positions[i] = self.positions[i].copy()\n\n                if current_value < self.global_best_value:\n                    self.global_best_value = current_value\n                    self.global_best_position = self.positions[i].copy()\n\n            self.dynamic_inertia_adjustment()\n            self.adaptive_learning_strategy()\n\n            for i in range(self.swarm_size):\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                cognitive_component = self.c1 * r1 * (self.personal_best_positions[i] - self.positions[i])\n                social_component = self.c2 * r2 * (self.global_best_position - self.positions[i])\n                self.velocities[i] = self.w * self.velocities[i] + cognitive_component + social_component\n                self.positions[i] += self.velocities[i]\n\n                # Quantum-inspired position update\n                if np.random.rand() < self.adapt_rate:\n                    self.positions[i] = self.quantum_position_update(self.positions[i], self.global_best_position)\n\n                # Adaptive neighborhood search\n                if np.random.rand() < self.adapt_rate:\n                    self.positions[i] = self.adaptive_neighborhood_search(self.positions[i])\n\n        return self.global_best_position, self.global_best_value", "name": "QAPSO", "description": "The Quantum Adaptive Particle Swarm Optimization (QAPSO) enhances exploration and exploitation balance through quantum-inspired updates and adaptive learning strategies, with dynamic swarm behavior adjustments.", "configspace": "", "generation": 56, "fitness": 0.27772824304972554, "feedback": "The algorithm QAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28 with standard deviation 0.00.", "error": "", "parent_id": "e93089d8-8277-496b-b724-d6777b124d39", "metadata": {"aucs": [0.27773928737274034, 0.27765757602459207, 0.2777878657518442]}, "mutation_prompt": null}
{"id": "7552df5f-48f3-4967-ab04-56c2f26be785", "solution": "import numpy as np\n\nclass ODE_AC:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(50, budget)\n        self.population = None\n        self.scores = None\n        self.best_solution = None\n        self.best_score = np.inf\n        self.bounds = None\n        self.mutation_factor = 0.5  # Initial mutation factor\n        self.crossover_rate = 0.7  # Crossover rate\n        self.oscillation_factor = 0.05  # Controls mutation factor oscillation\n        self.adapt_rate = 0.3  # Rate at which adaptive control mechanisms are applied\n\n    def initialize_population(self, lb, ub):\n        self.population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        self.scores = np.full(self.population_size, np.inf)\n        self.bounds = (lb, ub)\n\n    def mutate(self, target_idx):\n        idxs = [idx for idx in range(self.population_size) if idx != target_idx]\n        a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n        mutant = a + self.mutation_factor * (b - c)\n        lb, ub = self.bounds\n        return np.clip(mutant, lb, ub)\n\n    def crossover(self, target, mutant):\n        cross_points = np.random.rand(self.dim) < self.crossover_rate\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def adaptive_oscillation(self, generation):\n        self.mutation_factor = 0.5 + self.oscillation_factor * np.sin(2 * np.pi * generation / self.budget)\n\n    def adaptive_mutation_control(self):\n        diversity = np.std(self.population, axis=0).mean()\n        max_diversity = np.sqrt(np.sum((self.bounds[1] - self.bounds[0])**2))\n        self.mutation_factor = 0.4 + 0.3 * (diversity / max_diversity)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        evaluations = 0\n        generation = 0\n\n        while evaluations < self.budget:\n            generation += 1\n            self.adaptive_oscillation(generation)\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                mutant = self.mutate(i)\n                trial = self.crossover(self.population[i], mutant)\n\n                trial_score = func(trial)\n                evaluations += 1\n\n                if trial_score < self.scores[i]:\n                    self.population[i] = trial\n                    self.scores[i] = trial_score\n\n                if trial_score < self.best_score:\n                    self.best_score = trial_score\n                    self.best_solution = trial\n\n            if np.random.rand() < self.adapt_rate:\n                self.adaptive_mutation_control()\n\n        return self.best_solution, self.best_score", "name": "ODE_AC", "description": "Oscillating Differential Evolution with Adaptive Control (ODE-AC) combines oscillating differential mutation rates with adaptive control mechanisms to enhance exploration and convergence efficiency.", "configspace": "", "generation": 57, "fitness": 0.27369956459089145, "feedback": "The algorithm ODE_AC got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.27 with standard deviation 0.00.", "error": "", "parent_id": "e93089d8-8277-496b-b724-d6777b124d39", "metadata": {"aucs": [0.2737902179908319, 0.27339248270808425, 0.27391599307375825]}, "mutation_prompt": null}
{"id": "05181c76-43ad-4440-a616-e610137c95dc", "solution": "import numpy as np\n\nclass DQA_PSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = min(50, budget)\n        self.positions = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_values = None\n        self.global_best_position = None\n        self.global_best_value = np.inf\n        self.w = 0.9  # Initial inertia weight\n        self.c1 = 1.5  # Cognitive coefficient\n        self.c2 = 1.5  # Social coefficient\n        self.adapt_rate = 0.1\n        self.bounds = None\n        self.learning_rate = 0.1  # Learning rate for dynamic updates\n        self.exploration_boost = 0.05  # Exploration boost factor\n\n    def initialize_swarm(self, lb, ub):\n        self.positions = np.random.uniform(lb, ub, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.swarm_size, self.dim))\n        self.personal_best_positions = self.positions.copy()\n        self.personal_best_values = np.full(self.swarm_size, np.inf)\n        self.bounds = (lb, ub)\n\n    def quantum_position_update(self, position, global_best):\n        beta = np.random.normal(0, 1, self.dim)\n        delta = np.random.normal(0, 1, self.dim) * self.exploration_boost\n        new_position = position + beta * (global_best - position) + delta\n        lb, ub = self.bounds\n        return np.clip(new_position, lb, ub)\n\n    def adaptive_neighborhood_search(self, position, lb, ub):\n        neighborhood_radius = np.exp(-self.dim / self.swarm_size)\n        perturbation = np.random.uniform(-neighborhood_radius, neighborhood_radius, self.dim)\n        new_position = position + perturbation\n        return np.clip(new_position, lb, ub)\n    \n    def dynamic_inertia_adjustment(self):\n        diversity = np.mean(np.std(self.positions, axis=0))\n        max_diversity = np.sqrt(np.sum((self.bounds[1] - self.bounds[0])**2))\n        self.w = 0.4 + 0.5 * (diversity / max_diversity)\n\n    def adaptive_learning_strategy(self):\n        for i in range(self.swarm_size):\n            if np.random.rand() < self.learning_rate:\n                self.velocities[i] += np.random.uniform(-0.1, 0.1, self.dim)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_swarm(lb, ub)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            for i in range(self.swarm_size):\n                if evaluations >= self.budget:\n                    break\n\n                current_value = func(self.positions[i])\n                evaluations += 1\n\n                if current_value < self.personal_best_values[i]:\n                    self.personal_best_values[i] = current_value\n                    self.personal_best_positions[i] = self.positions[i].copy()\n\n                if current_value < self.global_best_value:\n                    self.global_best_value = current_value\n                    self.global_best_position = self.positions[i].copy()\n\n            self.dynamic_inertia_adjustment()\n            self.adaptive_learning_strategy()\n\n            for i in range(self.swarm_size):\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                cognitive_component = self.c1 * r1 * (self.personal_best_positions[i] - self.positions[i])\n                social_component = self.c2 * r2 * (self.global_best_position - self.positions[i])\n                self.velocities[i] = self.w * self.velocities[i] + cognitive_component + social_component\n                self.positions[i] += self.velocities[i]\n\n                # Quantum-inspired position update\n                if np.random.rand() < self.adapt_rate:\n                    self.positions[i] = self.quantum_position_update(self.positions[i], self.global_best_position)\n\n                # Adaptive neighborhood search\n                if np.random.rand() < self.adapt_rate:\n                    self.positions[i] = self.adaptive_neighborhood_search(self.positions[i], lb, ub)\n\n        return self.global_best_position, self.global_best_value", "name": "DQA_PSO", "description": "Dynamic Quantum-Adaptive Particle Swarm Optimization (DQA-PSO) enhances exploration and convergence through dynamic parameter tuning and quantum-inspired updates with adaptive neighborhood search.", "configspace": "", "generation": 58, "fitness": 0.2778186794280122, "feedback": "The algorithm DQA_PSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28 with standard deviation 0.00.", "error": "", "parent_id": "e93089d8-8277-496b-b724-d6777b124d39", "metadata": {"aucs": [0.2777900923281865, 0.2778795226277079, 0.2777864233281422]}, "mutation_prompt": null}
{"id": "d4956913-18d4-42b8-94ce-828233358b2e", "solution": "import numpy as np\n\nclass ADPC_DQA_PSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = min(50, budget)\n        self.positions = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_values = None\n        self.global_best_position = None\n        self.global_best_value = np.inf\n        self.w = 0.9  # Initial inertia weight\n        self.c1 = 1.5  # Cognitive coefficient\n        self.c2 = 1.5  # Social coefficient\n        self.adapt_rate = 0.1\n        self.bounds = None\n        self.exploration_boost = 0.05  # Exploration boost factor\n\n    def initialize_swarm(self, lb, ub):\n        self.positions = np.random.uniform(lb, ub, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.swarm_size, self.dim))\n        self.personal_best_positions = self.positions.copy()\n        self.personal_best_values = np.full(self.swarm_size, np.inf)\n        self.bounds = (lb, ub)\n\n    def quantum_position_update(self, position, global_best):\n        beta = np.random.normal(0, 1, self.dim)\n        delta = np.random.normal(0, 1, self.dim) * self.exploration_boost\n        new_position = position + beta * (global_best - position) + delta\n        lb, ub = self.bounds\n        return np.clip(new_position, lb, ub)\n\n    def adaptive_neighborhood_search(self, position, lb, ub):\n        neighborhood_radius = np.exp(-self.dim / self.swarm_size)\n        perturbation = np.random.uniform(-neighborhood_radius, neighborhood_radius, self.dim)\n        new_position = position + perturbation\n        return np.clip(new_position, lb, ub)\n    \n    def dynamic_inertia_adjustment(self):\n        diversity = np.mean(np.std(self.positions, axis=0))\n        max_diversity = np.sqrt(np.sum((self.bounds[1] - self.bounds[0])**2))\n        self.w = 0.4 + 0.5 * (diversity / max_diversity)\n\n    def adaptive_parameter_control(self, global_improvement, local_improvement):\n        if global_improvement > 0:\n            self.c1 = min(2.0, self.c1 + 0.1)\n            self.c2 = max(1.0, self.c2 - 0.1)\n        if local_improvement:\n            self.c1 = max(1.0, self.c1 - 0.05)\n            self.c2 = min(2.0, self.c2 + 0.05)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_swarm(lb, ub)\n        evaluations = 0\n        previous_best_value = np.inf\n\n        while evaluations < self.budget:\n            global_improvement = False\n            local_improvements = [False] * self.swarm_size\n\n            for i in range(self.swarm_size):\n                if evaluations >= self.budget:\n                    break\n\n                current_value = func(self.positions[i])\n                evaluations += 1\n\n                if current_value < self.personal_best_values[i]:\n                    self.personal_best_values[i] = current_value\n                    self.personal_best_positions[i] = self.positions[i].copy()\n                    local_improvements[i] = True\n\n                if current_value < self.global_best_value:\n                    self.global_best_value = current_value\n                    self.global_best_position = self.positions[i].copy()\n                    global_improvement = True\n\n            self.dynamic_inertia_adjustment()\n            self.adaptive_parameter_control(global_improvement, any(local_improvements))\n\n            for i in range(self.swarm_size):\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                cognitive_component = self.c1 * r1 * (self.personal_best_positions[i] - self.positions[i])\n                social_component = self.c2 * r2 * (self.global_best_position - self.positions[i])\n                self.velocities[i] = self.w * self.velocities[i] + cognitive_component + social_component\n                self.positions[i] += self.velocities[i]\n\n                # Quantum-inspired position update\n                if np.random.rand() < self.adapt_rate:\n                    self.positions[i] = self.quantum_position_update(self.positions[i], self.global_best_position)\n\n                # Adaptive neighborhood search\n                if np.random.rand() < self.adapt_rate:\n                    self.positions[i] = self.adaptive_neighborhood_search(self.positions[i], lb, ub)\n\n        return self.global_best_position, self.global_best_value", "name": "ADPC_DQA_PSO", "description": "Enhanced DQA_PSO with Adaptive Dynamic Parameter Control (ADPC-DQA_PSO) introduces adaptive parameter control based on global convergence metrics and local improvements, improving convergence speed and solution quality.", "configspace": "", "generation": 59, "fitness": 0.27764989659485717, "feedback": "The algorithm ADPC_DQA_PSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28 with standard deviation 0.00.", "error": "", "parent_id": "05181c76-43ad-4440-a616-e610137c95dc", "metadata": {"aucs": [0.2776574821624692, 0.2777169939454618, 0.2775752136766405]}, "mutation_prompt": null}
{"id": "9cbd2d70-46eb-4d1b-b8e4-c68c7edae1f5", "solution": "import numpy as np\n\nclass AQGA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(50, budget)\n        self.population = None\n        self.fitness = None\n        self.global_best_position = None\n        self.global_best_value = np.inf\n        self.mutation_rate = 0.1\n        self.crossover_rate = 0.7\n        self.exploration_boost = 0.05\n        self.bounds = None\n\n    def initialize_population(self, lb, ub):\n        self.population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n        self.bounds = (lb, ub)\n\n    def quantum_crossover(self, parent1, parent2):\n        alpha = np.random.uniform(0, 1, self.dim)\n        offspring = alpha * parent1 + (1 - alpha) * parent2\n        return self.apply_quantum_boost(offspring)\n\n    def apply_quantum_boost(self, position):\n        boost = np.random.normal(0, 1, self.dim) * self.exploration_boost\n        new_position = position + boost\n        lb, ub = self.bounds\n        return np.clip(new_position, lb, ub)\n\n    def mutate(self, position):\n        mutation_vector = np.random.uniform(-self.mutation_rate, self.mutation_rate, self.dim)\n        return np.clip(position + mutation_vector, self.bounds[0], self.bounds[1])\n\n    def select_parents(self):\n        # Tournament selection\n        candidates = np.random.choice(self.population_size, 2, replace=False)\n        if self.fitness[candidates[0]] < self.fitness[candidates[1]]:\n            return self.population[candidates[0]], self.population[candidates[1]]\n        else:\n            return self.population[candidates[1]], self.population[candidates[0]]\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            # Evaluate fitness\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                current_value = func(self.population[i])\n                evaluations += 1\n\n                if current_value < self.fitness[i]:\n                    self.fitness[i] = current_value\n\n                if current_value < self.global_best_value:\n                    self.global_best_value = current_value\n                    self.global_best_position = self.population[i].copy()\n\n            # Create next generation\n            new_population = []\n            for _ in range(self.population_size // 2):\n                parent1, parent2 = self.select_parents()\n                if np.random.rand() < self.crossover_rate:\n                    offspring1 = self.quantum_crossover(parent1, parent2)\n                    offspring2 = self.quantum_crossover(parent2, parent1)\n                else:\n                    offspring1, offspring2 = parent1, parent2\n\n                if np.random.rand() < self.mutation_rate:\n                    offspring1 = self.mutate(offspring1)\n                if np.random.rand() < self.mutation_rate:\n                    offspring2 = self.mutate(offspring2)\n\n                new_population.extend([offspring1, offspring2])\n\n            self.population = np.array(new_population)\n\n        return self.global_best_position, self.global_best_value", "name": "AQGA", "description": "Adaptive Quantum Genetic Algorithm (AQGA) combines genetic operations with quantum-inspired enhancements for dynamic exploration and exploitation in optimization.", "configspace": "", "generation": 60, "fitness": 0.26558536715881004, "feedback": "The algorithm AQGA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.27 with standard deviation 0.01.", "error": "", "parent_id": "05181c76-43ad-4440-a616-e610137c95dc", "metadata": {"aucs": [0.2610454206137288, 0.27409172394788595, 0.2616189569148153]}, "mutation_prompt": null}
{"id": "5799d8ee-c533-4e97-ac8f-c3606a16d43d", "solution": "import numpy as np\n\nclass ALPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = min(50, budget)\n        self.positions = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_values = None\n        self.global_best_position = None\n        self.global_best_value = np.inf\n        self.w = 0.9  # Initial inertia weight\n        self.c1 = 2.0  # Cognitive coefficient\n        self.c2 = 2.0  # Social coefficient\n        self.alpha = 1.5  # Levy flight parameter\n        self.bounds = None\n\n    def initialize_swarm(self, lb, ub):\n        self.positions = np.random.uniform(lb, ub, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.swarm_size, self.dim))\n        self.personal_best_positions = self.positions.copy()\n        self.personal_best_values = np.full(self.swarm_size, np.inf)\n        self.bounds = (lb, ub)\n\n    def levy_flight(self, position):\n        step = np.random.standard_normal(self.dim) * self.levy()\n        new_position = position + step\n        lb, ub = self.bounds\n        return np.clip(new_position, lb, ub)\n\n    def levy(self):\n        u = np.random.standard_normal(self.dim) * (np.sqrt(np.pi) / 2) ** (1 / self.alpha)\n        v = np.random.standard_normal(self.dim)\n        z = u / (abs(v) ** (1 / self.alpha))\n        return z\n\n    def adaptive_inertia_weight(self):\n        diversity = np.mean(np.std(self.positions, axis=0))\n        max_diversity = np.sqrt(np.sum((self.bounds[1] - self.bounds[0])**2))\n        self.w = 0.5 + 0.4 * (diversity / max_diversity)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_swarm(lb, ub)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            for i in range(self.swarm_size):\n                if evaluations >= self.budget:\n                    break\n\n                current_value = func(self.positions[i])\n                evaluations += 1\n\n                if current_value < self.personal_best_values[i]:\n                    self.personal_best_values[i] = current_value\n                    self.personal_best_positions[i] = self.positions[i].copy()\n\n                if current_value < self.global_best_value:\n                    self.global_best_value = current_value\n                    self.global_best_position = self.positions[i].copy()\n\n            self.adaptive_inertia_weight()\n\n            for i in range(self.swarm_size):\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                cognitive_component = self.c1 * r1 * (self.personal_best_positions[i] - self.positions[i])\n                social_component = self.c2 * r2 * (self.global_best_position - self.positions[i])\n                self.velocities[i] = self.w * self.velocities[i] + cognitive_component + social_component\n                self.positions[i] += self.velocities[i]\n\n                # Levy flight for enhanced exploration\n                if np.random.rand() < 0.3:\n                    self.positions[i] = self.levy_flight(self.positions[i])\n\n        return self.global_best_position, self.global_best_value", "name": "ALPSO", "description": "Adaptive Levy Particle Swarm Optimization (ALPSO) leverages Levy flight for enhanced exploration and an adaptive inertia weight to dynamically balance exploration and exploitation.", "configspace": "", "generation": 61, "fitness": 0.26906123626078804, "feedback": "The algorithm ALPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.27 with standard deviation 0.01.", "error": "", "parent_id": "05181c76-43ad-4440-a616-e610137c95dc", "metadata": {"aucs": [0.27187248777003514, 0.2768607188998963, 0.2584505021124327]}, "mutation_prompt": null}
{"id": "429ef8f2-3250-46b5-afa4-2b11519c5162", "solution": "import numpy as np\n\nclass QE_APSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = min(50, budget)\n        self.positions = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_values = None\n        self.global_best_position = None\n        self.global_best_value = np.inf\n        self.w_max = 0.9  # Max inertia weight\n        self.w_min = 0.4  # Min inertia weight\n        self.c1 = 1.5  # Cognitive coefficient\n        self.c2 = 1.5  # Social coefficient\n        self.adapt_rate = 0.15\n        self.bounds = None\n        self.tunneling_intensity = 0.1  # Tunneling intensity factor\n        self.diversity_threshold = 0.1  # Diversity threshold for adaptive strategy\n\n    def initialize_swarm(self, lb, ub):\n        self.positions = np.random.uniform(lb, ub, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.swarm_size, self.dim))\n        self.personal_best_positions = self.positions.copy()\n        self.personal_best_values = np.full(self.swarm_size, np.inf)\n        self.global_best_value = np.inf\n        self.bounds = (lb, ub)\n\n    def quantum_tunneling(self, position, global_best):\n        direction = np.random.normal(0, 1, self.dim)\n        intensity = self.tunneling_intensity * np.random.exponential(1.0)\n        new_position = position + direction * intensity * (global_best - position)\n        lb, ub = self.bounds\n        return np.clip(new_position, lb, ub)\n\n    def diversity_control(self):\n        current_diversity = np.mean(np.std(self.positions, axis=0))\n        if current_diversity < self.diversity_threshold:\n            self.velocities += np.random.uniform(-0.5, 0.5, self.velocities.shape)\n\n    def update_inertia_weight(self, evaluations):\n        progress = evaluations / self.budget\n        self.w = self.w_max - (self.w_max - self.w_min) * progress\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_swarm(lb, ub)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            for i in range(self.swarm_size):\n                if evaluations >= self.budget:\n                    break\n\n                current_value = func(self.positions[i])\n                evaluations += 1\n\n                if current_value < self.personal_best_values[i]:\n                    self.personal_best_values[i] = current_value\n                    self.personal_best_positions[i] = self.positions[i].copy()\n\n                if current_value < self.global_best_value:\n                    self.global_best_value = current_value\n                    self.global_best_position = self.positions[i].copy()\n\n            self.update_inertia_weight(evaluations)\n            self.diversity_control()\n\n            for i in range(self.swarm_size):\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                cognitive_component = self.c1 * r1 * (self.personal_best_positions[i] - self.positions[i])\n                social_component = self.c2 * r2 * (self.global_best_position - self.positions[i])\n                self.velocities[i] = self.w * self.velocities[i] + cognitive_component + social_component\n                self.positions[i] += self.velocities[i]\n\n                # Quantum-inspired tunneling\n                if np.random.rand() < self.adapt_rate:\n                    self.positions[i] = self.quantum_tunneling(self.positions[i], self.global_best_position)\n\n        return self.global_best_position, self.global_best_value", "name": "QE_APSO", "description": "Quantum-Enhanced Adaptive Particle Swarm Optimization leverages adaptive quantum-inspired tunneling and diversity-controlled dynamics to enhance global search efficiency and convergence stability.", "configspace": "", "generation": 62, "fitness": 0.27297028320355304, "feedback": "The algorithm QE_APSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.27 with standard deviation 0.00.", "error": "", "parent_id": "05181c76-43ad-4440-a616-e610137c95dc", "metadata": {"aucs": [0.27542992711549996, 0.27471939800831324, 0.268761524486846]}, "mutation_prompt": null}
{"id": "13ae7a9e-0297-47cb-8072-6765944ae694", "solution": "import numpy as np\n\nclass EQA_PSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = min(50, budget)\n        self.positions = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_values = None\n        self.global_best_position = None\n        self.global_best_value = np.inf\n        self.w = 0.9  # Initial inertia weight\n        self.c1 = 2.0  # Cognitive coefficient\n        self.c2 = 2.0  # Social coefficient\n        self.bounds = None\n        self.quantum_chance = 0.2  # Chance to perform quantum tunneling\n        self.elite_learning_rate = 0.05  # Learning rate for elite particles\n        self.elite_fraction = 0.1  # Fraction of elite particles\n\n    def initialize_swarm(self, lb, ub):\n        self.positions = np.random.uniform(lb, ub, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.swarm_size, self.dim))\n        self.personal_best_positions = self.positions.copy()\n        self.personal_best_values = np.full(self.swarm_size, np.inf)\n        self.bounds = (lb, ub)\n\n    def quantum_tunneling(self, position, global_best, lb, ub):\n        new_position = (position + global_best) / 2 + np.random.normal(0, 0.1, self.dim)\n        return np.clip(new_position, lb, ub)\n\n    def elite_learning(self, position, global_best, lb, ub):\n        learning_vector = np.random.uniform(-self.elite_learning_rate, self.elite_learning_rate, self.dim)\n        new_position = position + learning_vector * (global_best - position)\n        return np.clip(new_position, lb, ub)\n\n    def dynamic_inertia_adjustment(self):\n        diversity = np.mean(np.std(self.positions, axis=0))\n        max_diversity = np.sqrt(np.sum((self.bounds[1] - self.bounds[0])**2))\n        self.w = 0.4 + 0.5 * (diversity / max_diversity)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_swarm(lb, ub)\n        evaluations = 0\n        elite_count = int(self.elite_fraction * self.swarm_size)\n\n        while evaluations < self.budget:\n            for i in range(self.swarm_size):\n                if evaluations >= self.budget:\n                    break\n\n                current_value = func(self.positions[i])\n                evaluations += 1\n\n                if current_value < self.personal_best_values[i]:\n                    self.personal_best_values[i] = current_value\n                    self.personal_best_positions[i] = self.positions[i].copy()\n\n                if current_value < self.global_best_value:\n                    self.global_best_value = current_value\n                    self.global_best_position = self.positions[i].copy()\n\n            self.dynamic_inertia_adjustment()\n\n            # Sort particles based on personal best values\n            sorted_indices = np.argsort(self.personal_best_values)\n            for i in range(self.swarm_size):\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                cognitive_component = self.c1 * r1 * (self.personal_best_positions[i] - self.positions[i])\n                social_component = self.c2 * r2 * (self.global_best_position - self.positions[i])\n                self.velocities[i] = self.w * self.velocities[i] + cognitive_component + social_component\n                self.positions[i] += self.velocities[i]\n\n                # Quantum-inspired tunneling for non-elite particles\n                if i >= elite_count and np.random.rand() < self.quantum_chance:\n                    self.positions[i] = self.quantum_tunneling(self.positions[i], self.global_best_position, lb, ub)\n\n                # Elite learning strategy for elite particles\n                if i < elite_count:\n                    self.positions[i] = self.elite_learning(self.positions[i], self.global_best_position, lb, ub)\n\n        return self.global_best_position, self.global_best_value", "name": "EQA_PSO", "description": "Enhanced Quantum-Aware Particle Swarm Optimization (EQA-PSO) boosts diversity and convergence through adaptive quantum tunneling and elite learning strategies.", "configspace": "", "generation": 63, "fitness": 0.2762822853471315, "feedback": "The algorithm EQA_PSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28 with standard deviation 0.00.", "error": "", "parent_id": "05181c76-43ad-4440-a616-e610137c95dc", "metadata": {"aucs": [0.277198402650643, 0.27755888650715577, 0.27408956688359576]}, "mutation_prompt": null}
{"id": "0d2782d0-12f9-44cc-bf55-29d53b9fce81", "solution": "import numpy as np\n\nclass AQEA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(50, budget)\n        self.population = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_values = None\n        self.global_best_position = None\n        self.global_best_value = np.inf\n        self.w = 0.7  # Initial inertia weight\n        self.c1 = 1.3  # Cognitive coefficient\n        self.c2 = 1.7  # Social coefficient\n        self.adaptation_factor = 0.1\n        self.quantum_factor = 0.05\n        self.bounds = None\n\n    def initialize_population(self, lb, ub):\n        self.population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.population_size, self.dim))\n        self.personal_best_positions = self.population.copy()\n        self.personal_best_values = np.full(self.population_size, np.inf)\n        self.bounds = (lb, ub)\n\n    def quantum_superposition(self, position):\n        alpha = np.random.rand(self.dim)\n        superposed = alpha * self.global_best_position + (1 - alpha) * position\n        noise = np.random.normal(0, self.quantum_factor, self.dim)\n        new_position = superposed + noise\n        lb, ub = self.bounds\n        return np.clip(new_position, lb, ub)\n\n    def evolutionary_update(self):\n        for i in range(self.population_size):\n            if np.random.rand() < self.adaptation_factor:\n                mutation = np.random.normal(0, 0.1, self.dim)\n                self.population[i] += mutation\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                current_value = func(self.population[i])\n                evaluations += 1\n\n                if current_value < self.personal_best_values[i]:\n                    self.personal_best_values[i] = current_value\n                    self.personal_best_positions[i] = self.population[i].copy()\n\n                if current_value < self.global_best_value:\n                    self.global_best_value = current_value\n                    self.global_best_position = self.population[i].copy()\n\n            self.evolutionary_update()\n\n            for i in range(self.population_size):\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                cognitive_component = self.c1 * r1 * (self.personal_best_positions[i] - self.population[i])\n                social_component = self.c2 * r2 * (self.global_best_position - self.population[i])\n                self.velocities[i] = self.w * self.velocities[i] + cognitive_component + social_component\n                self.population[i] += self.velocities[i]\n\n                # Quantum-inspired superposition state\n                if np.random.rand() < self.adaptation_factor:\n                    self.population[i] = self.quantum_superposition(self.population[i])\n\n        return self.global_best_position, self.global_best_value", "name": "AQEA", "description": "Adaptive Quantum Evolutionary Algorithm (AQEA) combines evolutionary strategies with quantum-inspired superposition states for enhanced exploration and exploitation in complex optimization landscapes.", "configspace": "", "generation": 64, "fitness": 0.27725418480082553, "feedback": "The algorithm AQEA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28 with standard deviation 0.00.", "error": "", "parent_id": "05181c76-43ad-4440-a616-e610137c95dc", "metadata": {"aucs": [0.27743198459747764, 0.27746312054124567, 0.2768674492637533]}, "mutation_prompt": null}
{"id": "0cec5ff4-e7e3-4739-9ffd-b8a62dae12e0", "solution": "import numpy as np\n\nclass EDQA_PSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = min(60, budget)\n        self.positions = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_values = None\n        self.global_best_position = None\n        self.global_best_value = np.inf\n        self.w = 0.9  # Initial inertia weight\n        self.c1 = 1.5  # Cognitive coefficient\n        self.c2 = 1.5  # Social coefficient\n        self.adapt_rate = 0.1\n        self.bounds = None\n        self.learning_rate = 0.15  # Increased learning rate for dynamic updates\n        self.exploration_boost = 0.07  # Enhanced exploration boost factor\n        self.phase_switch = 0.3  # Probability to switch to a neighborhood exploration phase\n\n    def initialize_swarm(self, lb, ub):\n        self.positions = np.random.uniform(lb, ub, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.swarm_size, self.dim))\n        self.personal_best_positions = self.positions.copy()\n        self.personal_best_values = np.full(self.swarm_size, np.inf)\n        self.bounds = (lb, ub)\n\n    def quantum_position_update(self, position, global_best):\n        beta = np.random.normal(0, 1, self.dim)\n        delta = np.random.normal(0, 1, self.dim) * self.exploration_boost\n        new_position = position + beta * (global_best - position) + delta\n        lb, ub = self.bounds\n        return np.clip(new_position, lb, ub)\n\n    def interactive_neighborhood_search(self, position, lb, ub):\n        neighborhood_radius = np.exp(-self.dim / self.swarm_size)\n        perturbation = np.random.uniform(-neighborhood_radius, neighborhood_radius, self.dim)\n        influence = np.random.uniform(0, 1, self.dim)\n        new_position = position + influence * perturbation\n        return np.clip(new_position, lb, ub)\n    \n    def dynamic_inertia_adjustment(self):\n        diversity = np.mean(np.std(self.positions, axis=0))\n        max_diversity = np.sqrt(np.sum((self.bounds[1] - self.bounds[0])**2))\n        self.w = 0.4 + 0.5 * (diversity / max_diversity)\n\n    def adaptive_learning_strategy(self):\n        for i in range(self.swarm_size):\n            if np.random.rand() < self.learning_rate:\n                self.velocities[i] += np.random.uniform(-0.15, 0.15, self.dim)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_swarm(lb, ub)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            for i in range(self.swarm_size):\n                if evaluations >= self.budget:\n                    break\n\n                current_value = func(self.positions[i])\n                evaluations += 1\n\n                if current_value < self.personal_best_values[i]:\n                    self.personal_best_values[i] = current_value\n                    self.personal_best_positions[i] = self.positions[i].copy()\n\n                if current_value < self.global_best_value:\n                    self.global_best_value = current_value\n                    self.global_best_position = self.positions[i].copy()\n\n            self.dynamic_inertia_adjustment()\n            self.adaptive_learning_strategy()\n\n            for i in range(self.swarm_size):\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                cognitive_component = self.c1 * r1 * (self.personal_best_positions[i] - self.positions[i])\n                social_component = self.c2 * r2 * (self.global_best_position - self.positions[i])\n                self.velocities[i] = self.w * self.velocities[i] + cognitive_component + social_component\n                self.positions[i] += self.velocities[i]\n\n                # Quantum-inspired position update\n                if np.random.rand() < self.adapt_rate:\n                    self.positions[i] = self.quantum_position_update(self.positions[i], self.global_best_position)\n\n                # Interactive neighborhood exploration phase\n                if np.random.rand() < self.phase_switch:\n                    self.positions[i] = self.interactive_neighborhood_search(self.positions[i], lb, ub)\n\n        return self.global_best_position, self.global_best_value", "name": "EDQA_PSO", "description": "Enhanced Dynamic Quantum-Adaptive Particle Swarm Optimization (EDQA-PSO) improves exploration and convergence by incorporating multi-phase learning with stochastic perturbations and interactive feedback from diverse neighborhood strategies.", "configspace": "", "generation": 65, "fitness": 0.2774513725748197, "feedback": "The algorithm EDQA_PSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28 with standard deviation 0.00.", "error": "", "parent_id": "05181c76-43ad-4440-a616-e610137c95dc", "metadata": {"aucs": [0.27726893967566313, 0.27746431039641317, 0.2776208676523828]}, "mutation_prompt": null}
{"id": "6b871c00-0bb5-49a2-94ea-ebe955d4b998", "solution": "import numpy as np\n\nclass QE_PSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = min(50, budget)\n        self.positions = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_values = None\n        self.global_best_position = None\n        self.global_best_value = np.inf\n        self.w = 0.9  # Initial inertia weight\n        self.c1 = 1.5  # Cognitive coefficient\n        self.c2 = 1.5  # Social coefficient\n        self.adapt_rate = 0.1\n        self.bounds = None\n        self.learning_rate = 0.1  # Learning rate for dynamic updates\n        self.exploration_boost = 0.05  # Exploration boost factor\n        self.quantum_tunneling_prob = 0.05  # Probability for quantum tunneling\n\n    def initialize_swarm(self, lb, ub):\n        self.positions = np.random.uniform(lb, ub, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.swarm_size, self.dim))\n        self.personal_best_positions = self.positions.copy()\n        self.personal_best_values = np.full(self.swarm_size, np.inf)\n        self.bounds = (lb, ub)\n\n    def quantum_position_update(self, position, global_best):\n        beta = np.random.normal(0, 1, self.dim)\n        delta = np.random.normal(0, 1, self.dim) * self.exploration_boost\n        new_position = position + beta * (global_best - position) + delta\n        lb, ub = self.bounds\n        return np.clip(new_position, lb, ub)\n\n    def adaptive_neighborhood_search(self, position, lb, ub):\n        neighborhood_radius = np.exp(-self.dim / self.swarm_size)\n        perturbation = np.random.uniform(-neighborhood_radius, neighborhood_radius, self.dim)\n        new_position = position + perturbation\n        return np.clip(new_position, lb, ub)\n\n    def dynamic_inertia_adjustment(self):\n        diversity = np.mean(np.std(self.positions, axis=0))\n        max_diversity = np.sqrt(np.sum((self.bounds[1] - self.bounds[0])**2))\n        self.w = 0.4 + 0.5 * (diversity / max_diversity)\n\n    def adaptive_learning_strategy(self):\n        for i in range(self.swarm_size):\n            if np.random.rand() < self.learning_rate:\n                self.velocities[i] += np.random.uniform(-0.1, 0.1, self.dim)\n\n    def quantum_tunneling(self, position):\n        if np.random.rand() < self.quantum_tunneling_prob:\n            scale = np.random.uniform(0.5, 1.0)\n            lb, ub = self.bounds\n            tunneling_shift = np.random.uniform(lb, ub, self.dim) * scale\n            return np.clip(tunneling_shift, lb, ub)\n        return position\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_swarm(lb, ub)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            for i in range(self.swarm_size):\n                if evaluations >= self.budget:\n                    break\n\n                current_value = func(self.positions[i])\n                evaluations += 1\n\n                if current_value < self.personal_best_values[i]:\n                    self.personal_best_values[i] = current_value\n                    self.personal_best_positions[i] = self.positions[i].copy()\n\n                if current_value < self.global_best_value:\n                    self.global_best_value = current_value\n                    self.global_best_position = self.positions[i].copy()\n\n            self.dynamic_inertia_adjustment()\n            self.adaptive_learning_strategy()\n\n            for i in range(self.swarm_size):\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                cognitive_component = self.c1 * r1 * (self.personal_best_positions[i] - self.positions[i])\n                social_component = self.c2 * r2 * (self.global_best_position - self.positions[i])\n                self.velocities[i] = self.w * self.velocities[i] + cognitive_component + social_component\n                self.positions[i] += self.velocities[i]\n\n                # Quantum-inspired position update\n                if np.random.rand() < self.adapt_rate:\n                    self.positions[i] = self.quantum_position_update(self.positions[i], self.global_best_position)\n\n                # Adaptive neighborhood search\n                if np.random.rand() < self.adapt_rate:\n                    self.positions[i] = self.adaptive_neighborhood_search(self.positions[i], lb, ub)\n\n                # Quantum tunneling for escaping local optima\n                self.positions[i] = self.quantum_tunneling(self.positions[i])\n\n        return self.global_best_position, self.global_best_value", "name": "QE_PSO", "description": "Quantum-Enhanced Particle Swarm Optimization (QE-PSO) incorporates quantum tunneling and multi-layered exploration to enhance convergence and prevent premature stagnation in complex search spaces.", "configspace": "", "generation": 66, "fitness": 0.27764913699535443, "feedback": "The algorithm QE_PSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28 with standard deviation 0.00.", "error": "", "parent_id": "05181c76-43ad-4440-a616-e610137c95dc", "metadata": {"aucs": [0.2777318697140617, 0.2776841278803175, 0.2775314133916841]}, "mutation_prompt": null}
{"id": "57456a66-7fdf-4457-80d2-70a0cee633b1", "solution": "import numpy as np\n\nclass EDQA_PSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = min(50, budget)\n        self.positions = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_values = None\n        self.global_best_position = None\n        self.global_best_value = np.inf\n        self.w = 0.9  # Initial inertia weight\n        self.c1 = 1.5  # Cognitive coefficient\n        self.c2 = 1.5  # Social coefficient\n        self.adapt_rate = 0.1\n        self.bounds = None\n        self.learning_rate = 0.1\n        self.exploration_boost = 0.05\n        self.velocity_clamp_factor = 0.5  # Velocity clamp factor\n        self.crossover_rate = 0.3  # Crossover rate for differential evolution\n\n    def initialize_swarm(self, lb, ub):\n        self.positions = np.random.uniform(lb, ub, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.swarm_size, self.dim))\n        self.personal_best_positions = self.positions.copy()\n        self.personal_best_values = np.full(self.swarm_size, np.inf)\n        self.bounds = (lb, ub)\n\n    def quantum_position_update(self, position, global_best):\n        beta = np.random.normal(0, 1, self.dim)\n        delta = np.random.normal(0, 1, self.dim) * self.exploration_boost\n        new_position = position + beta * (global_best - position) + delta\n        lb, ub = self.bounds\n        return np.clip(new_position, lb, ub)\n\n    def adaptive_neighborhood_search(self, position, lb, ub):\n        neighborhood_radius = np.exp(-self.dim / self.swarm_size)\n        perturbation = np.random.uniform(-neighborhood_radius, neighborhood_radius, self.dim)\n        new_position = position + perturbation\n        return np.clip(new_position, lb, ub)\n    \n    def dynamic_inertia_adjustment(self):\n        diversity = np.mean(np.std(self.positions, axis=0))\n        max_diversity = np.sqrt(np.sum((self.bounds[1] - self.bounds[0])**2))\n        self.w = 0.4 + 0.5 * (diversity / max_diversity)\n\n    def adaptive_learning_strategy(self):\n        for i in range(self.swarm_size):\n            if np.random.rand() < self.learning_rate:\n                self.velocities[i] += np.random.uniform(-0.1, 0.1, self.dim)\n    \n    def velocity_clamping(self):\n        max_velocity = self.velocity_clamp_factor * (self.bounds[1] - self.bounds[0])\n        self.velocities = np.clip(self.velocities, -max_velocity, max_velocity)\n\n    def differential_evolution_crossover(self, parent, target):\n        if np.random.rand() < self.crossover_rate:\n            mutant_vector = target + 0.8 * (self.positions[np.random.randint(self.swarm_size)] - self.positions[np.random.randint(self.swarm_size)])\n            return np.clip(mutant_vector, self.bounds[0], self.bounds[1])\n        return parent\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_swarm(lb, ub)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            for i in range(self.swarm_size):\n                if evaluations >= self.budget:\n                    break\n\n                current_value = func(self.positions[i])\n                evaluations += 1\n\n                if current_value < self.personal_best_values[i]:\n                    self.personal_best_values[i] = current_value\n                    self.personal_best_positions[i] = self.positions[i].copy()\n\n                if current_value < self.global_best_value:\n                    self.global_best_value = current_value\n                    self.global_best_position = self.positions[i].copy()\n\n            self.dynamic_inertia_adjustment()\n            self.adaptive_learning_strategy()\n            self.velocity_clamping()\n\n            for i in range(self.swarm_size):\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                cognitive_component = self.c1 * r1 * (self.personal_best_positions[i] - self.positions[i])\n                social_component = self.c2 * r2 * (self.global_best_position - self.positions[i])\n                self.velocities[i] = self.w * self.velocities[i] + cognitive_component + social_component\n                self.positions[i] += self.velocities[i]\n\n                if np.random.rand() < self.adapt_rate:\n                    self.positions[i] = self.quantum_position_update(self.positions[i], self.global_best_position)\n\n                if np.random.rand() < self.adapt_rate:\n                    self.positions[i] = self.adaptive_neighborhood_search(self.positions[i], lb, ub)\n\n                self.positions[i] = self.differential_evolution_crossover(self.positions[i], self.global_best_position)\n\n        return self.global_best_position, self.global_best_value", "name": "EDQA_PSO", "description": "Enhanced Dynamic Quantum-Adaptive Particle Swarm Optimization (EDQA-PSO) introduces adaptive velocity clamping and differential evolution-inspired crossover to improve convergence and diversity in high-dimensional search spaces.", "configspace": "", "generation": 67, "fitness": 0.27733894269353, "feedback": "The algorithm EDQA_PSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28 with standard deviation 0.00.", "error": "", "parent_id": "05181c76-43ad-4440-a616-e610137c95dc", "metadata": {"aucs": [0.27715177535790236, 0.27732372478032696, 0.27754132794236064]}, "mutation_prompt": null}
{"id": "56cd36cd-319e-42a6-8a1f-babf56631b23", "solution": "import numpy as np\n\nclass EQS_PSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = min(50, budget)\n        self.positions = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_values = None\n        self.global_best_position = None\n        self.global_best_value = np.inf\n        self.w = 0.9  # Initial inertia weight\n        self.c1 = 1.5  # Cognitive coefficient\n        self.c2 = 1.5  # Social coefficient\n        self.adapt_rate = 0.15  # Increased adaptation rate\n        self.bounds = None\n        self.learning_rate = 0.2  # Enhanced learning rate for adaptive strategies\n        self.exploration_boost = 0.1  # Increased exploration boost factor\n\n    def initialize_swarm(self, lb, ub):\n        self.positions = np.random.uniform(lb, ub, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.swarm_size, self.dim))\n        self.personal_best_positions = self.positions.copy()\n        self.personal_best_values = np.full(self.swarm_size, np.inf)\n        self.bounds = (lb, ub)\n\n    def quantum_position_update(self, position, global_best):\n        beta = np.random.normal(0, 1, self.dim)\n        delta = np.random.normal(0, 1, self.dim) * self.exploration_boost\n        new_position = position + beta * (global_best - position) + delta\n        lb, ub = self.bounds\n        return np.clip(new_position, lb, ub)\n\n    def adaptive_neighborhood_search(self, position, lb, ub):\n        neighborhood_radius = np.exp(-self.dim / self.swarm_size)\n        perturbation = np.random.uniform(-neighborhood_radius, neighborhood_radius, self.dim)\n        new_position = position + perturbation\n        return np.clip(new_position, lb, ub)\n    \n    def dynamic_inertia_adjustment(self):\n        diversity = np.mean(np.std(self.positions, axis=0))\n        max_diversity = np.sqrt(np.sum((self.bounds[1] - self.bounds[0])**2))\n        self.w = 0.4 + 0.5 * (diversity / max_diversity)\n\n    def multi_feedback_learning(self):\n        acceleration = np.random.uniform(0.5, 1.5, self.dim)\n        for i in range(self.swarm_size):\n            if np.random.rand() < self.learning_rate:\n                self.velocities[i] *= acceleration\n\n    def stochastic_environment_adaptation(self):\n        if np.random.rand() < self.adapt_rate:\n            self.w *= np.random.uniform(0.8, 1.2)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_swarm(lb, ub)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            for i in range(self.swarm_size):\n                if evaluations >= self.budget:\n                    break\n\n                current_value = func(self.positions[i])\n                evaluations += 1\n\n                if current_value < self.personal_best_values[i]:\n                    self.personal_best_values[i] = current_value\n                    self.personal_best_positions[i] = self.positions[i].copy()\n\n                if current_value < self.global_best_value:\n                    self.global_best_value = current_value\n                    self.global_best_position = self.positions[i].copy()\n\n            self.dynamic_inertia_adjustment()\n            self.multi_feedback_learning()\n            self.stochastic_environment_adaptation()\n\n            for i in range(self.swarm_size):\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                cognitive_component = self.c1 * r1 * (self.personal_best_positions[i] - self.positions[i])\n                social_component = self.c2 * r2 * (self.global_best_position - self.positions[i])\n                self.velocities[i] = self.w * self.velocities[i] + cognitive_component + social_component\n                self.positions[i] += self.velocities[i]\n\n                # Quantum-inspired position update\n                if np.random.rand() < self.adapt_rate:\n                    self.positions[i] = self.quantum_position_update(self.positions[i], self.global_best_position)\n\n                # Adaptive neighborhood search\n                if np.random.rand() < self.adapt_rate:\n                    self.positions[i] = self.adaptive_neighborhood_search(self.positions[i], lb, ub)\n\n        return self.global_best_position, self.global_best_value", "name": "EQS_PSO", "description": "Enhanced Quantum-Social Particle Swarm Optimization (EQS-PSO) improves convergence by integrating multi-feedback learning, adaptive quantum perturbations, and stochastic environment adaptation for robust global optimization.", "configspace": "", "generation": 68, "fitness": 0.2773756392244425, "feedback": "The algorithm EQS_PSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28 with standard deviation 0.00.", "error": "", "parent_id": "05181c76-43ad-4440-a616-e610137c95dc", "metadata": {"aucs": [0.2764206643651794, 0.27785746581134807, 0.2778487874968001]}, "mutation_prompt": null}
{"id": "5ff4399f-3501-40a9-9fc5-a8754f41bb8f", "solution": "import numpy as np\n\nclass QLEPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = min(50, budget)\n        self.positions = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_values = None\n        self.global_best_position = None\n        self.global_best_value = np.inf\n        self.w = 0.5  # Initial inertia weight\n        self.c1 = 1.7  # Cognitive coefficient\n        self.c2 = 1.7  # Social coefficient\n        self.adapt_rate = 0.1\n        self.bounds = None\n        self.levy_alpha = 1.5  # Levy flight parameter\n\n    def initialize_swarm(self, lb, ub):\n        self.positions = np.random.uniform(lb, ub, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.swarm_size, self.dim))\n        self.personal_best_positions = self.positions.copy()\n        self.personal_best_values = np.full(self.swarm_size, np.inf)\n        self.bounds = (lb, ub)\n\n    def levy_flight(self):\n        u = np.random.normal(0, 1, self.dim)\n        v = np.random.normal(0, 1, self.dim)\n        step = u / (np.abs(v)**(1/self.levy_alpha))\n        return step\n\n    def quantum_position_update(self, position, global_best):\n        beta = np.random.normal(0, 1, self.dim)\n        new_position = position + beta * (global_best - position) + self.levy_flight()\n        lb, ub = self.bounds\n        return np.clip(new_position, lb, ub)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_swarm(lb, ub)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            for i in range(self.swarm_size):\n                if evaluations >= self.budget:\n                    break\n\n                current_value = func(self.positions[i])\n                evaluations += 1\n\n                if current_value < self.personal_best_values[i]:\n                    self.personal_best_values[i] = current_value\n                    self.personal_best_positions[i] = self.positions[i].copy()\n\n                if current_value < self.global_best_value:\n                    self.global_best_value = current_value\n                    self.global_best_position = self.positions[i].copy()\n\n            for i in range(self.swarm_size):\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                cognitive_component = self.c1 * r1 * (self.personal_best_positions[i] - self.positions[i])\n                social_component = self.c2 * r2 * (self.global_best_position - self.positions[i])\n                self.velocities[i] = self.w * self.velocities[i] + cognitive_component + social_component\n                self.positions[i] += self.velocities[i]\n\n                # Quantum-inspired position update\n                if np.random.rand() < self.adapt_rate:\n                    self.positions[i] = self.quantum_position_update(self.positions[i], self.global_best_position)\n\n        return self.global_best_position, self.global_best_value", "name": "QLEPSO", "description": "Quantum-Levy Flight Enhanced Particle Swarm Optimization (QLEPSO) integrates quantum behaviors and Levy flight patterns to enhance the exploration and exploitation balance for optimizing complex photonic structures.", "configspace": "", "generation": 69, "fitness": 0.2773211872709853, "feedback": "The algorithm QLEPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28 with standard deviation 0.00.", "error": "", "parent_id": "05181c76-43ad-4440-a616-e610137c95dc", "metadata": {"aucs": [0.2774960377816569, 0.27719644534993804, 0.277271078681361]}, "mutation_prompt": null}
{"id": "8a7f54bc-d409-4324-9ea9-28831ff196c4", "solution": "import numpy as np\n\nclass CAS_PSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = min(50, budget)\n        self.positions = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_values = None\n        self.global_best_position = None\n        self.global_best_value = np.inf\n        self.w = 0.9  # Initial inertia weight\n        self.c1 = 1.5  # Cognitive coefficient\n        self.c2 = 1.5  # Social coefficient\n        self.bounds = None\n        self.exploration_boost = 0.05  # Exploration boost factor\n\n    def initialize_swarm(self, lb, ub):\n        self.positions = np.random.uniform(lb, ub, (self.swarm_size, self.dim))\n        self.velocities = np.zeros((self.swarm_size, self.dim))\n        self.personal_best_positions = self.positions.copy()\n        self.personal_best_values = np.full(self.swarm_size, np.inf)\n        self.bounds = (lb, ub)\n\n    def chaotic_map(self, x):\n        # Using a simple logistic map for chaotic sequence generation\n        r = 3.9\n        return r * x * (1 - x)\n\n    def stochastic_tunneling(self, position, func_value, global_best_value):\n        tunneling_factor = -np.log(1 + np.exp(-0.5 * (func_value - global_best_value)))\n        perturbation = np.random.normal(0, tunneling_factor, self.dim)\n        return position + perturbation\n\n    def update_parameters(self, iteration, max_iterations):\n        chaos = self.chaotic_map(np.random.rand())\n        self.w = 0.4 + chaos * 0.5  # Inertia weight evolves chaotically\n        self.c1 = 1.5 + chaos * 0.5  # Cognitive coefficient evolves chaotically\n        self.c2 = 1.5 + chaos * 0.5  # Social coefficient evolves chaotically\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_swarm(lb, ub)\n        evaluations = 0\n        iteration = 0\n\n        while evaluations < self.budget:\n            for i in range(self.swarm_size):\n                if evaluations >= self.budget:\n                    break\n\n                current_value = func(self.positions[i])\n                evaluations += 1\n\n                if current_value < self.personal_best_values[i]:\n                    self.personal_best_values[i] = current_value\n                    self.personal_best_positions[i] = self.positions[i].copy()\n\n                if current_value < self.global_best_value:\n                    self.global_best_value = current_value\n                    self.global_best_position = self.positions[i].copy()\n\n            self.update_parameters(iteration, self.budget // self.swarm_size)\n\n            for i in range(self.swarm_size):\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                cognitive_component = self.c1 * r1 * (self.personal_best_positions[i] - self.positions[i])\n                social_component = self.c2 * r2 * (self.global_best_position - self.positions[i])\n                self.velocities[i] = self.w * self.velocities[i] + cognitive_component + social_component\n                self.positions[i] += self.velocities[i]\n\n                # Stochastic tunneling to help escape local optima\n                if np.random.rand() < self.exploration_boost:\n                    self.positions[i] = self.stochastic_tunneling(self.positions[i], current_value, self.global_best_value)\n\n                # Ensure positions remain within bounds\n                self.positions[i] = np.clip(self.positions[i], lb, ub)\n\n            iteration += 1\n\n        return self.global_best_position, self.global_best_value", "name": "CAS_PSO", "description": "Particle Swarm Optimization with Chaotic Adaptive Search (CAS-PSO) incorporates chaotic maps for dynamic parameter adaptation and uses stochastic tunneling to escape local optima, enhancing global search efficiency.", "configspace": "", "generation": 70, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('scale < 0').", "error": "ValueError('scale < 0')", "parent_id": "05181c76-43ad-4440-a616-e610137c95dc", "metadata": {}, "mutation_prompt": null}
{"id": "035012bd-2e64-43e3-ac1f-a3d2e703499e", "solution": "import numpy as np\n\nclass EQA_PSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = min(50, budget)\n        self.positions = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_values = None\n        self.global_best_position = None\n        self.global_best_value = np.inf\n        self.w = 0.9  # Initial inertia weight\n        self.c1 = 1.5  # Cognitive coefficient\n        self.c2 = 1.5  # Social coefficient\n        self.c3 = 1.0  # Cooperative coefficient\n        self.adapt_rate = 0.1\n        self.learning_rate = 0.1  # Learning rate for dynamic updates\n        self.exploration_boost = 0.05  # Exploration boost factor\n        self.elite_fraction = 0.1  # Fraction of elite particles\n\n    def initialize_swarm(self, lb, ub):\n        self.positions = np.random.uniform(lb, ub, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.swarm_size, self.dim))\n        self.personal_best_positions = self.positions.copy()\n        self.personal_best_values = np.full(self.swarm_size, np.inf)\n        self.bounds = (lb, ub)\n\n    def quantum_position_update(self, position, global_best):\n        beta = np.random.normal(0, 1, self.dim)\n        delta = np.random.normal(0, 1, self.dim) * self.exploration_boost\n        new_position = position + beta * (global_best - position) + delta\n        lb, ub = self.bounds\n        return np.clip(new_position, lb, ub)\n\n    def adaptive_neighborhood_search(self, position, lb, ub):\n        neighborhood_radius = np.exp(-self.dim / self.swarm_size)\n        perturbation = np.random.uniform(-neighborhood_radius, neighborhood_radius, self.dim)\n        new_position = position + perturbation\n        return np.clip(new_position, lb, ub)\n\n    def dynamic_inertia_adjustment(self):\n        diversity = np.mean(np.std(self.positions, axis=0))\n        max_diversity = np.sqrt(np.sum((self.bounds[1] - self.bounds[0])**2))\n        self.w = 0.4 + 0.5 * (diversity / max_diversity)\n\n    def elite_learning(self, lb, ub):\n        elite_count = int(self.elite_fraction * self.swarm_size)\n        elite_indices = np.argsort(self.personal_best_values)[:elite_count]\n        for idx in elite_indices:\n            self.personal_best_positions[idx] = self.adaptive_neighborhood_search(self.personal_best_positions[idx], lb, ub)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_swarm(lb, ub)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            for i in range(self.swarm_size):\n                if evaluations >= self.budget:\n                    break\n\n                current_value = func(self.positions[i])\n                evaluations += 1\n\n                if current_value < self.personal_best_values[i]:\n                    self.personal_best_values[i] = current_value\n                    self.personal_best_positions[i] = self.positions[i].copy()\n\n                if current_value < self.global_best_value:\n                    self.global_best_value = current_value\n                    self.global_best_position = self.positions[i].copy()\n\n            self.dynamic_inertia_adjustment()\n            self.elite_learning(lb, ub)\n\n            for i in range(self.swarm_size):\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                r3 = np.random.rand(self.dim)\n                cognitive_component = self.c1 * r1 * (self.personal_best_positions[i] - self.positions[i])\n                social_component = self.c2 * r2 * (self.global_best_position - self.positions[i])\n                cooperative_component = self.c3 * r3 * (np.mean(self.personal_best_positions, axis=0) - self.positions[i])\n                self.velocities[i] = self.w * self.velocities[i] + cognitive_component + social_component + cooperative_component\n                self.positions[i] += self.velocities[i]\n\n                # Quantum-inspired position update\n                if np.random.rand() < self.adapt_rate:\n                    self.positions[i] = self.quantum_position_update(self.positions[i], self.global_best_position)\n\n                # Adaptive neighborhood search\n                if np.random.rand() < self.adapt_rate:\n                    self.positions[i] = self.adaptive_neighborhood_search(self.positions[i], lb, ub)\n\n        return self.global_best_position, self.global_best_value", "name": "EQA_PSO", "description": "Enhanced Quantum-Adaptive PSO (EQA-PSO) integrates adaptive inertia scaling, multi-swarm cooperation, and elite learning to improve exploration and convergence rates.", "configspace": "", "generation": 71, "fitness": 0.2778188619889084, "feedback": "The algorithm EQA_PSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28 with standard deviation 0.00.", "error": "", "parent_id": "05181c76-43ad-4440-a616-e610137c95dc", "metadata": {"aucs": [0.2779504041991354, 0.2777172546043267, 0.2777889271632631]}, "mutation_prompt": null}
{"id": "e066af1e-cb87-4319-a144-f51b50d9eab1", "solution": "import numpy as np\n\nclass QIMS_DE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(50, budget)\n        self.mutation_factor = 0.5\n        self.crossover_rate = 0.9\n        self.adapt_rate = 0.1\n        self.positions = None\n        self.best_position = None\n        self.best_value = np.inf\n\n    def quantum_initialize(self, lb, ub):\n        self.positions = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        beta = np.random.normal(0, 1, (self.population_size, self.dim))\n        self.positions += beta * (ub - lb) * 0.1\n        self.positions = np.clip(self.positions, lb, ub)\n\n    def differential_mutation(self, target_idx):\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        mutant = self.positions[a] + self.mutation_factor * (self.positions[b] - self.positions[c])\n        return np.clip(mutant, *self.bounds)\n\n    def crossover(self, target, mutant):\n        j_random = np.random.randint(self.dim)\n        trial = np.where(np.random.rand(self.dim) < self.crossover_rate, mutant, target)\n        trial[j_random] = mutant[j_random]\n        return trial\n\n    def adapt_mutation_factor(self):\n        diversity = np.mean(np.std(self.positions, axis=0))\n        self.mutation_factor = 0.5 + 0.5 * (diversity / np.sqrt(np.sum((self.bounds[1] - self.bounds[0])**2)))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.bounds = (lb, ub)\n        self.quantum_initialize(lb, ub)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                mutant = self.differential_mutation(i)\n                trial = self.crossover(self.positions[i], mutant)\n                \n                trial_value = func(trial)\n                evaluations += 1\n\n                current_value = func(self.positions[i])\n                evaluations += 1\n\n                if trial_value < current_value:\n                    self.positions[i] = trial\n                    current_value = trial_value\n\n                if current_value < self.best_value:\n                    self.best_value = current_value\n                    self.best_position = self.positions[i].copy()\n\n            self.adapt_mutation_factor()\n\n        return self.best_position, self.best_value", "name": "QIMS_DE", "description": "Quantum-Inspired Multi-Stage Differential Evolution (QIMS-DE) combines quantum-inspired initialization, adaptive mutation control, and multi-stage evolution to balance exploration and exploitation for enhanced convergence.", "configspace": "", "generation": 72, "fitness": 0.2719631731497469, "feedback": "The algorithm QIMS_DE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.27 with standard deviation 0.00.", "error": "", "parent_id": "035012bd-2e64-43e3-ac1f-a3d2e703499e", "metadata": {"aucs": [0.2708803737427514, 0.2720711491568861, 0.27293799654960305]}, "mutation_prompt": null}
{"id": "278d70e3-9767-42f4-8a23-6f141c8df82b", "solution": "import numpy as np\n\nclass AQH_PSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = min(50, budget)\n        self.positions = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_values = None\n        self.global_best_position = None\n        self.global_best_value = np.inf\n        self.w = 0.9  # Initial inertia weight\n        self.c1 = 1.5  # Cognitive coefficient\n        self.c2 = 1.5  # Social coefficient\n        self.adapt_rate = 0.1\n        self.scaling_factor = 1.0\n        self.exploration_boost = 0.05  # Exploration boost factor\n        self.elite_fraction = 0.1  # Fraction of elite particles\n\n    def initialize_swarm(self, lb, ub):\n        self.positions = np.random.uniform(lb, ub, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.swarm_size, self.dim))\n        self.personal_best_positions = self.positions.copy()\n        self.personal_best_values = np.full(self.swarm_size, np.inf)\n        self.bounds = (lb, ub)\n\n    def quantum_position_update(self, position, global_best):\n        beta = np.random.normal(0, 1, self.dim)\n        delta = np.random.normal(0, 1, self.dim) * self.exploration_boost\n        scale = self.scaling_factor * (np.random.rand() * 0.5 + 0.75)\n        new_position = position + scale * (beta * (global_best - position) + delta)\n        lb, ub = self.bounds\n        return np.clip(new_position, lb, ub)\n    \n    def stochastic_perturbation(self, position, lb, ub):\n        perturbation_scale = np.random.uniform(0.01, 0.05, self.dim)\n        perturbation = np.random.normal(0, perturbation_scale, self.dim)\n        new_position = position + perturbation\n        return np.clip(new_position, lb, ub)\n\n    def dynamic_inertia_adjustment(self):\n        diversity = np.mean(np.std(self.positions, axis=0))\n        max_diversity = np.sqrt(np.sum((self.bounds[1] - self.bounds[0])**2))\n        self.w = 0.5 + 0.4 * (diversity / max_diversity)\n\n    def elite_learning(self, lb, ub):\n        elite_count = int(self.elite_fraction * self.swarm_size)\n        elite_indices = np.argsort(self.personal_best_values)[:elite_count]\n        for idx in elite_indices:\n            self.personal_best_positions[idx] = self.stochastic_perturbation(self.personal_best_positions[idx], lb, ub)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_swarm(lb, ub)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            for i in range(self.swarm_size):\n                if evaluations >= self.budget:\n                    break\n\n                current_value = func(self.positions[i])\n                evaluations += 1\n\n                if current_value < self.personal_best_values[i]:\n                    self.personal_best_values[i] = current_value\n                    self.personal_best_positions[i] = self.positions[i].copy()\n\n                if current_value < self.global_best_value:\n                    self.global_best_value = current_value\n                    self.global_best_position = self.positions[i].copy()\n\n            self.dynamic_inertia_adjustment()\n            self.elite_learning(lb, ub)\n\n            for i in range(self.swarm_size):\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                cognitive_component = self.c1 * r1 * (self.personal_best_positions[i] - self.positions[i])\n                social_component = self.c2 * r2 * (self.global_best_position - self.positions[i])\n                self.velocities[i] = self.w * self.velocities[i] + cognitive_component + social_component\n                self.positions[i] += self.velocities[i]\n\n                if np.random.rand() < self.adapt_rate:\n                    self.positions[i] = self.quantum_position_update(self.positions[i], self.global_best_position)\n\n                if np.random.rand() < self.adapt_rate:\n                    self.positions[i] = self.stochastic_perturbation(self.positions[i], lb, ub)\n\n        return self.global_best_position, self.global_best_value", "name": "AQH_PSO", "description": "Adaptive Quantum Heuristic PSO (AQH-PSO) enhances EQA-PSO by offering multiscale exploration using adaptive scaling and stochastic perturbations to improve adaptability and convergence precision.", "configspace": "", "generation": 73, "fitness": 0.2775575955063399, "feedback": "The algorithm AQH_PSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28 with standard deviation 0.00.", "error": "", "parent_id": "035012bd-2e64-43e3-ac1f-a3d2e703499e", "metadata": {"aucs": [0.2775855375269003, 0.27779364286872477, 0.2772936061233947]}, "mutation_prompt": null}
{"id": "bf69fe8f-a07c-45c4-b022-d77b51c87743", "solution": "import numpy as np\n\nclass HQGA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(100, budget)\n        self.population = None\n        self.fitness = None\n        self.global_best_position = None\n        self.global_best_value = np.inf\n        self.quantum_prob = 0.1\n        self.crossover_prob = 0.8\n        self.mutation_prob = 0.05\n\n    def initialize_population(self, lb, ub):\n        self.population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            value = func(self.population[i])\n            if value < self.fitness[i]:\n                self.fitness[i] = value\n\n            if value < self.global_best_value:\n                self.global_best_value = value\n                self.global_best_position = self.population[i].copy()\n\n    def quantum_search(self, individual, lb, ub):\n        beta = np.random.normal(0, 1, self.dim)\n        delta = np.random.normal(0, 1, self.dim) * 0.05\n        new_position = individual + beta * (self.global_best_position - individual) + delta\n        return np.clip(new_position, lb, ub)\n\n    def crossover(self, parent1, parent2):\n        mask = np.random.rand(self.dim) < 0.5\n        child = np.where(mask, parent1, parent2)\n        return child\n\n    def mutate(self, individual, lb, ub):\n        mutation_vector = np.random.uniform(-0.1, 0.1, self.dim)\n        new_individual = individual + mutation_vector\n        return np.clip(new_individual, lb, ub)\n\n    def select_parents(self):\n        idx1, idx2 = np.random.choice(self.population_size, 2, replace=False)\n        return self.population[idx1], self.population[idx2]\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            self.evaluate_population(func)\n            evaluations += self.population_size\n\n            new_population = []\n\n            while len(new_population) < self.population_size:\n                if evaluations >= self.budget:\n                    break\n\n                parent1, parent2 = self.select_parents()\n\n                if np.random.rand() < self.quantum_prob:\n                    child1 = self.quantum_search(parent1, lb, ub)\n                    child2 = self.quantum_search(parent2, lb, ub)\n                else:\n                    if np.random.rand() < self.crossover_prob:\n                        child1 = self.crossover(parent1, parent2)\n                        child2 = self.crossover(parent2, parent1)\n                    else:\n                        child1, child2 = parent1, parent2\n\n                if np.random.rand() < self.mutation_prob:\n                    child1 = self.mutate(child1, lb, ub)\n                if np.random.rand() < self.mutation_prob:\n                    child2 = self.mutate(child2, lb, ub)\n\n                new_population.extend([child1, child2])\n\n            self.population = np.array(new_population[:self.population_size])\n\n        return self.global_best_position, self.global_best_value", "name": "HQGA", "description": "Hybrid Quantum Genetic Algorithm (HQGA) combines quantum-inspired search and genetic crossover operations to enhance exploration and exploitation in high-dimensional optimization.", "configspace": "", "generation": 74, "fitness": 0.26423856745458757, "feedback": "The algorithm HQGA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.26 with standard deviation 0.00.", "error": "", "parent_id": "035012bd-2e64-43e3-ac1f-a3d2e703499e", "metadata": {"aucs": [0.2652049994573322, 0.26500301214050204, 0.2625076907659284]}, "mutation_prompt": null}
{"id": "e4d4a69a-1587-487d-afdf-5804fe465eb7", "solution": "import numpy as np\n\nclass QAH_PSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = min(50, budget)\n        self.positions = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_values = None\n        self.global_best_position = None\n        self.global_best_value = np.inf\n        self.w = 0.7  # Adjusted inertia weight for a better balance\n        self.c1 = 1.4  # Cognitive coefficient\n        self.c2 = 1.6  # Social coefficient\n        self.c3 = 1.2  # Hierarchical cooperative coefficient\n        self.adapt_rate = 0.2\n        self.exploration_boost = 0.1  # Increased exploration boost\n        self.elite_fraction = 0.15  # Increased fraction of elite particles\n        self.hierarchy_levels = 3\n\n    def initialize_swarm(self, lb, ub):\n        self.positions = np.random.uniform(lb, ub, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.swarm_size, self.dim))\n        self.personal_best_positions = self.positions.copy()\n        self.personal_best_values = np.full(self.swarm_size, np.inf)\n        self.bounds = (lb, ub)\n\n    def quantum_position_update(self, position, global_best):\n        beta = np.random.normal(0, 1, self.dim)\n        delta = np.random.normal(0, 1, self.dim) * self.exploration_boost\n        new_position = position + beta * (global_best - position) + delta\n        lb, ub = self.bounds\n        return np.clip(new_position, lb, ub)\n\n    def adaptive_neighborhood_search(self, position, lb, ub):\n        neighborhood_radius = np.exp(-self.dim / self.swarm_size)\n        perturbation = np.random.uniform(-neighborhood_radius, neighborhood_radius, self.dim)\n        new_position = position + perturbation\n        return np.clip(new_position, lb, ub)\n\n    def dynamic_inertia_adjustment(self):\n        diversity = np.mean(np.std(self.positions, axis=0))\n        max_diversity = np.sqrt(np.sum((self.bounds[1] - self.bounds[0])**2))\n        self.w = 0.5 + 0.4 * (diversity / max_diversity)\n\n    def hierarchical_cooperation(self):\n        level_count = int(self.swarm_size / self.hierarchy_levels)\n        for level in range(self.hierarchy_levels):\n            start_index = level * level_count\n            end_index = min((level + 1) * level_count, self.swarm_size)\n            local_best_value = np.inf\n            local_best_position = None\n            for i in range(start_index, end_index):\n                if self.personal_best_values[i] < local_best_value:\n                    local_best_value = self.personal_best_values[i]\n                    local_best_position = self.personal_best_positions[i].copy()\n            for i in range(start_index, end_index):\n                r3 = np.random.rand(self.dim)\n                positional_hierarchy_component = self.c3 * r3 * (local_best_position - self.positions[i])\n                self.velocities[i] += positional_hierarchy_component\n\n    def elite_learning(self, lb, ub):\n        elite_count = int(self.elite_fraction * self.swarm_size)\n        elite_indices = np.argsort(self.personal_best_values)[:elite_count]\n        for idx in elite_indices:\n            self.personal_best_positions[idx] = self.adaptive_neighborhood_search(self.personal_best_positions[idx], lb, ub)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_swarm(lb, ub)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            for i in range(self.swarm_size):\n                if evaluations >= self.budget:\n                    break\n\n                current_value = func(self.positions[i])\n                evaluations += 1\n\n                if current_value < self.personal_best_values[i]:\n                    self.personal_best_values[i] = current_value\n                    self.personal_best_positions[i] = self.positions[i].copy()\n\n                if current_value < self.global_best_value:\n                    self.global_best_value = current_value\n                    self.global_best_position = self.positions[i].copy()\n\n            self.dynamic_inertia_adjustment()\n            self.hierarchical_cooperation()\n            self.elite_learning(lb, ub)\n\n            for i in range(self.swarm_size):\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                cognitive_component = self.c1 * r1 * (self.personal_best_positions[i] - self.positions[i])\n                social_component = self.c2 * r2 * (self.global_best_position - self.positions[i])\n                self.velocities[i] = self.w * self.velocities[i] + cognitive_component + social_component\n                self.positions[i] += self.velocities[i]\n\n                # Quantum-inspired position update\n                if np.random.rand() < self.adapt_rate:\n                    self.positions[i] = self.quantum_position_update(self.positions[i], self.global_best_position)\n\n                # Adaptive neighborhood search\n                if np.random.rand() < self.adapt_rate:\n                    self.positions[i] = self.adaptive_neighborhood_search(self.positions[i], lb, ub)\n\n        return self.global_best_position, self.global_best_value", "name": "QAH_PSO", "description": "Quantum Adaptive Hierarchical PSO (QAH-PSO) enhances exploration with hierarchical cooperation and adaptive neighborhood search, improving global and local search balance.", "configspace": "", "generation": 75, "fitness": 0.27737196486164895, "feedback": "The algorithm QAH_PSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28 with standard deviation 0.00.", "error": "", "parent_id": "035012bd-2e64-43e3-ac1f-a3d2e703499e", "metadata": {"aucs": [0.2774983268703741, 0.27708360548152355, 0.27753396223304916]}, "mutation_prompt": null}
{"id": "57c46d13-5c5a-44fb-9920-c23452b2d979", "solution": "import numpy as np\n\nclass QIDL_PSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = min(50, budget)\n        self.positions = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_values = None\n        self.global_best_position = None\n        self.global_best_value = np.inf\n        self.w = 0.9  # Initial inertia weight\n        self.c1 = 1.5  # Cognitive coefficient\n        self.c2 = 1.5  # Social coefficient\n        self.c3 = 1.0  # Cooperative coefficient\n        self.adapt_rate = 0.1\n        self.history = []  # Store historical best values\n\n    def initialize_swarm(self, lb, ub):\n        self.positions = np.random.uniform(lb, ub, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.swarm_size, self.dim))\n        self.personal_best_positions = self.positions.copy()\n        self.personal_best_values = np.full(self.swarm_size, np.inf)\n        self.bounds = (lb, ub)\n\n    def quantum_position_update(self, position, global_best):\n        beta = np.random.normal(0, 1, self.dim)\n        delta = np.random.normal(0, 1, self.dim) * 0.05  # Exploration boost factor\n        new_position = position + beta * (global_best - position) + delta\n        lb, ub = self.bounds\n        return np.clip(new_position, lb, ub)\n\n    def dynamic_inertia_adjustment(self):\n        diversity = np.std(self.positions, axis=0).mean()\n        max_diversity = np.sqrt(np.sum((self.bounds[1] - self.bounds[0])**2))\n        self.w = 0.4 + 0.5 * (diversity / max_diversity)\n        # Historical diversity-driven adjustment\n        if len(self.history) > 1:\n            self.w += 0.1 * (self.history[-1] - self.history[-2]) / max(self.history)\n\n    def elite_learning(self, lb, ub):\n        elite_count = int(0.1 * self.swarm_size)\n        elite_indices = np.argsort(self.personal_best_values)[:elite_count]\n        for idx in elite_indices:\n            neighborhood_radius = np.exp(-self.dim / self.swarm_size)\n            perturbation = np.random.uniform(-neighborhood_radius, neighborhood_radius, self.dim)\n            self.personal_best_positions[idx] = np.clip(self.personal_best_positions[idx] + perturbation, lb, ub)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_swarm(lb, ub)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            for i in range(self.swarm_size):\n                if evaluations >= self.budget:\n                    break\n\n                current_value = func(self.positions[i])\n                evaluations += 1\n\n                if current_value < self.personal_best_values[i]:\n                    self.personal_best_values[i] = current_value\n                    self.personal_best_positions[i] = self.positions[i].copy()\n\n                if current_value < self.global_best_value:\n                    self.global_best_value = current_value\n                    self.global_best_position = self.positions[i].copy()\n\n            self.history.append(self.global_best_value)\n            self.dynamic_inertia_adjustment()\n            self.elite_learning(lb, ub)\n\n            for i in range(self.swarm_size):\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                r3 = np.random.rand(self.dim)\n                cognitive_component = self.c1 * r1 * (self.personal_best_positions[i] - self.positions[i])\n                social_component = self.c2 * r2 * (self.global_best_position - self.positions[i])\n                cooperative_component = self.c3 * r3 * (np.mean(self.personal_best_positions, axis=0) - self.positions[i])\n                self.velocities[i] = self.w * self.velocities[i] + cognitive_component + social_component + cooperative_component\n                self.positions[i] += self.velocities[i]\n\n                if np.random.rand() < self.adapt_rate:\n                    self.positions[i] = self.quantum_position_update(self.positions[i], self.global_best_position)\n\n        return self.global_best_position, self.global_best_value", "name": "QIDL_PSO", "description": "Quantum-Inspired Dynamic Learning PSO (QIDL-PSO) combines quantum-inspired movement with a dynamically adjusted cooperation strategy and a historical diversity-driven exploration mechanism to enhance convergence precision and robustness.", "configspace": "", "generation": 76, "fitness": 0.27788191121498235, "feedback": "The algorithm QIDL_PSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28 with standard deviation 0.00.", "error": "", "parent_id": "035012bd-2e64-43e3-ac1f-a3d2e703499e", "metadata": {"aucs": [0.2779053633971831, 0.2778705717867118, 0.2778697984610521]}, "mutation_prompt": null}
{"id": "2bcc2b07-16a3-484a-af2a-6a8d7b317139", "solution": "import numpy as np\n\nclass ALF_GHO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(50, budget)\n        self.positions = None\n        self.best_position = None\n        self.best_value = np.inf\n        self.history = []  # Store historical best values\n        self.alpha = 0.1   # Step size for gradient-based local search\n        self.beta = 1.5    # Lévy distribution exponent\n        self.sigma = (np.gamma(1 + self.beta) * np.sin(np.pi * self.beta / 2) /\n                      (np.gamma((1 + self.beta) / 2) * self.beta * np.power(2, (self.beta - 1) / 2)))**(1 / self.beta)\n\n    def initialize_population(self, lb, ub):\n        self.positions = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        self.bounds = (lb, ub)\n\n    def levy_flight(self, position, best_position):\n        u = np.random.normal(0, self.sigma, self.dim)\n        v = np.random.normal(0, 1, self.dim)\n        step = u / np.power(np.abs(v), 1/self.beta)\n        new_position = position + step * (position - best_position)\n        lb, ub = self.bounds\n        return np.clip(new_position, lb, ub)\n\n    def gradient_local_search(self, func, position):\n        grad = np.zeros(self.dim)\n        for i in range(self.dim):\n            epsilon = 1e-8\n            step = np.zeros(self.dim)\n            step[i] = epsilon\n            grad[i] = (func(position + step) - func(position - step)) / (2 * epsilon)\n        new_position = position - self.alpha * grad\n        lb, ub = self.bounds\n        return np.clip(new_position, lb, ub)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                current_value = func(self.positions[i])\n                evaluations += 1\n\n                if current_value < self.best_value:\n                    self.best_value = current_value\n                    self.best_position = self.positions[i].copy()\n\n            self.history.append(self.best_value)\n\n            for i in range(self.population_size):\n                self.positions[i] = self.levy_flight(self.positions[i], self.best_position)\n\n                if np.random.rand() < 0.2:  # Perform local search with probability 0.2\n                    self.positions[i] = self.gradient_local_search(func, self.positions[i])\n\n        return self.best_position, self.best_value", "name": "ALF_GHO", "description": "Adaptive Lévy Flight and Gradient-Based Hybrid Optimization (ALF-GHO) integrates adaptive Lévy flights for exploration with a gradient-based local search for exploitation to balance exploration-exploitation dynamics effectively.", "configspace": "", "generation": 77, "fitness": -Infinity, "feedback": "An exception occurred: AttributeError(\"module 'numpy' has no attribute 'gamma'\").", "error": "AttributeError(\"module 'numpy' has no attribute 'gamma'\")", "parent_id": "57c46d13-5c5a-44fb-9920-c23452b2d979", "metadata": {}, "mutation_prompt": null}
{"id": "93196040-9bac-4fcf-950b-e71c53c75380", "solution": "import numpy as np\n\nclass QHDS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(50, budget // 2)\n        self.positions = None\n        self.best_position = None\n        self.best_value = np.inf\n        self.mutation_factor = 0.8\n        self.crossover_rate = 0.9\n        self.randomize_rate = 0.3\n\n    def initialize_population(self, lb, ub):\n        self.positions = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        self.best_position = None\n        self.best_value = np.inf\n        self.bounds = (lb, ub)\n\n    def quantum_harmonic_update(self, position, best_position):\n        alpha = np.random.standard_normal(self.dim)\n        gamma = np.random.normal(0, 0.1, self.dim)\n        new_position = position + alpha * (best_position - position) + gamma\n        lb, ub = self.bounds\n        return np.clip(new_position, lb, ub)\n\n    def differential_mutation(self, idx, population):\n        candidates = list(range(self.population_size))\n        candidates.remove(idx)\n        a, b, c = np.random.choice(candidates, 3, replace=False)\n        mutant_vector = population[a] + self.mutation_factor * (population[b] - population[c])\n        lb, ub = self.bounds\n        return np.clip(mutant_vector, lb, ub)\n\n    def crossover(self, target, mutant):\n        crossover_vector = np.where(np.random.rand(self.dim) < self.crossover_rate, mutant, target)\n        return crossover_vector\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                mutant_vector = self.differential_mutation(i, self.positions)\n                trial_vector = self.crossover(self.positions[i], mutant_vector)\n                \n                if np.random.rand() < self.randomize_rate:\n                    trial_vector = self.quantum_harmonic_update(trial_vector, self.best_position if self.best_position is not None else np.mean(self.positions, axis=0))\n                \n                trial_value = func(trial_vector)\n                evaluations += 1\n\n                if trial_value < func(self.positions[i]):\n                    self.positions[i] = trial_vector\n\n                if trial_value < self.best_value:\n                    self.best_value = trial_value\n                    self.best_position = trial_vector\n\n        return self.best_position, self.best_value", "name": "QHDS", "description": "Quantum Harmony Differential Search (QHDS) integrates quantum harmonics with differential evolution strategies for adaptive global exploration and enhanced convergence in photonic structure optimization.", "configspace": "", "generation": 78, "fitness": 0.25490437901093094, "feedback": "The algorithm QHDS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.25 with standard deviation 0.00.", "error": "", "parent_id": "57c46d13-5c5a-44fb-9920-c23452b2d979", "metadata": {"aucs": [0.25733589719091854, 0.25265088229368815, 0.25472635754818607]}, "mutation_prompt": null}
{"id": "4ccd0b24-f863-4c72-9f35-01b80e5196de", "solution": "import numpy as np\n\nclass QPP_DE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(50, budget)\n        self.population = None\n        self.best_solution = None\n        self.best_value = np.inf\n        self.F = 0.5  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.crowding_factor = 0.1  # Crowding factor for local search\n        self.quantum_perturbation_rate = 0.2\n\n    def initialize_population(self, lb, ub):\n        self.population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        self.bounds = (lb, ub)\n\n    def differential_mutation(self, idx):\n        candidates = list(range(self.population_size))\n        candidates.remove(idx)\n        a, b, c = np.random.choice(candidates, 3, replace=False)\n        mutant = self.population[a] + self.F * (self.population[b] - self.population[c])\n        lb, ub = self.bounds\n        return np.clip(mutant, lb, ub)\n\n    def crossover(self, target, mutant):\n        cross_points = np.random.rand(self.dim) < self.CR\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def quantum_perturbation(self, solution, global_best):\n        if np.random.rand() < self.quantum_perturbation_rate:\n            beta = np.random.normal(0, 1, self.dim)\n            delta = np.random.normal(0, 1, self.dim) * 0.05\n            perturbed = solution + beta * (global_best - solution) + delta\n            lb, ub = self.bounds\n            return np.clip(perturbed, lb, ub)\n        return solution\n\n    def crowding_local_search(self):\n        for idx in np.argsort([np.linalg.norm(ind - self.best_solution) for ind in self.population])[:int(self.crowding_factor * self.population_size)]:\n            perturbation = np.random.uniform(-0.1, 0.1, self.dim)\n            self.population[idx] = np.clip(self.population[idx] + perturbation, *self.bounds)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                mutant = self.differential_mutation(i)\n                trial = self.crossover(self.population[i], mutant)\n\n                trial = self.quantum_perturbation(trial, self.best_solution if self.best_solution is not None else trial)\n\n                trial_value = func(trial)\n                evaluations += 1\n\n                if trial_value < func(self.population[i]):\n                    self.population[i] = trial\n                    if trial_value < self.best_value:\n                        self.best_value = trial_value\n                        self.best_solution = trial\n\n            self.crowding_local_search()\n\n        return self.best_solution, self.best_value", "name": "QPP_DE", "description": "Quantum-Position Perturbation Differential Evolution (QPP-DE) augments differential evolution with quantum-inspired perturbations and crowding-based local search to balance exploration and exploitation.", "configspace": "", "generation": 79, "fitness": 0.27119936290324426, "feedback": "The algorithm QPP_DE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.27 with standard deviation 0.00.", "error": "", "parent_id": "57c46d13-5c5a-44fb-9920-c23452b2d979", "metadata": {"aucs": [0.2725191049349639, 0.26966535938284963, 0.2714136243919193]}, "mutation_prompt": null}
{"id": "048d804b-1dfc-40ed-8ca2-34752e3a77de", "solution": "import numpy as np\n\nclass QSEA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(50, budget)\n        self.positions = None\n        self.fitness = None\n        self.best_position = None\n        self.best_value = np.inf\n        self.mutation_rate = 0.1\n        self.history = []\n\n    def initialize_population(self, lb, ub):\n        self.positions = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n        self.bounds = (lb, ub)\n\n    def quantum_update(self, position, best):\n        phi = np.random.uniform(0, 2 * np.pi, self.dim)\n        radius = np.random.uniform(0, 1, self.dim) * np.abs(best - position)\n        new_position = position + radius * np.cos(phi)\n        lb, ub = self.bounds\n        return np.clip(new_position, lb, ub)\n\n    def adaptive_mutation(self, position):\n        if np.random.rand() < self.mutation_rate:\n            mutation_vector = np.random.normal(0, 0.1, self.dim)\n            position += mutation_vector\n        lb, ub = self.bounds\n        return np.clip(position, lb, ub)\n\n    def evolution_step(self):\n        sorted_indices = np.argsort(self.fitness)\n        elite_count = self.population_size // 5\n        for i in range(elite_count, self.population_size):\n            parent_1, parent_2 = np.random.choice(elite_count, 2, replace=False)\n            crossover_point = np.random.randint(1, self.dim)\n            self.positions[i][:crossover_point] = self.positions[sorted_indices[parent_1]][:crossover_point]\n            self.positions[i][crossover_point:] = self.positions[sorted_indices[parent_2]][crossover_point:]\n            self.positions[i] = self.adaptive_mutation(self.positions[i])\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                current_value = func(self.positions[i])\n                evaluations += 1\n\n                if current_value < self.fitness[i]:\n                    self.fitness[i] = current_value\n\n                if current_value < self.best_value:\n                    self.best_value = current_value\n                    self.best_position = self.positions[i].copy()\n\n            self.history.append(self.best_value)\n            self.evolution_step()\n\n            for i in range(self.population_size):\n                if np.random.rand() < self.mutation_rate:\n                    self.positions[i] = self.quantum_update(self.positions[i], self.best_position)\n\n        return self.best_position, self.best_value", "name": "QSEA", "description": "Quantum Swarm Evolutionary Algorithm (QSEA) leverages evolutionary strategies with quantum-inspired position updates and adaptive mutation for enhanced exploration and convergence in dynamic environments.", "configspace": "", "generation": 80, "fitness": 0.2631095674761575, "feedback": "The algorithm QSEA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.26 with standard deviation 0.00.", "error": "", "parent_id": "57c46d13-5c5a-44fb-9920-c23452b2d979", "metadata": {"aucs": [0.2632041181399559, 0.25876062121259824, 0.26736396307591825]}, "mutation_prompt": null}
{"id": "a35e579c-e65f-4fe9-93e8-8026a1edead0", "solution": "import numpy as np\n\nclass EQIDL_PSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = min(50, budget)\n        self.positions = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_values = None\n        self.global_best_position = None\n        self.global_best_value = np.inf\n        self.w = 0.9  # Initial inertia weight\n        self.c1 = 1.5  # Cognitive coefficient\n        self.c2 = 1.5  # Social coefficient\n        self.c3 = 1.0  # Cooperative coefficient\n        self.adapt_rate = 0.1\n        self.history = []  # Store historical best values\n\n    def initialize_swarm(self, lb, ub):\n        self.positions = np.random.uniform(lb, ub, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.swarm_size, self.dim))\n        self.personal_best_positions = self.positions.copy()\n        self.personal_best_values = np.full(self.swarm_size, np.inf)\n        self.bounds = (lb, ub)\n\n    def quantum_position_update(self, position, global_best):\n        beta = np.random.normal(0, 1, self.dim)\n        delta = np.random.normal(0, 1, self.dim) * 0.05  # Exploration boost factor\n        new_position = position + beta * (global_best - position) + delta\n        lb, ub = self.bounds\n        return np.clip(new_position, lb, ub)\n\n    def dynamic_inertia_adjustment(self):\n        diversity = np.std(self.positions, axis=0).mean()\n        max_diversity = np.sqrt(np.sum((self.bounds[1] - self.bounds[0])**2))\n        self.w = 0.4 + 0.5 * (diversity / max_diversity)\n        if len(self.history) > 1:\n            self.w += 0.1 * (self.history[-1] - self.history[-2]) / max(self.history)\n\n    def adaptive_coefficient_tuning(self):\n        improvement = 0\n        if len(self.history) > 1:\n            improvement = self.history[-2] - self.history[-1]\n        adaptability = np.exp(-improvement)\n        self.c1 = 1.5 * adaptability\n        self.c2 = 1.5 * (1 - adaptability)\n\n    def differential_mutation(self, pos, best_pos, F=0.5):\n        idxs = np.random.choice(range(self.swarm_size), 3, replace=False)\n        x1, x2, x3 = self.positions[idxs]\n        mutant_vector = x1 + F * (x2 - x3)\n        lb, ub = self.bounds\n        return np.clip(mutant_vector, lb, ub)\n\n    def elite_learning(self, lb, ub):\n        elite_count = int(0.1 * self.swarm_size)\n        elite_indices = np.argsort(self.personal_best_values)[:elite_count]\n        for idx in elite_indices:\n            neighborhood_radius = np.exp(-self.dim / self.swarm_size)\n            perturbation = np.random.uniform(-neighborhood_radius, neighborhood_radius, self.dim)\n            self.personal_best_positions[idx] = np.clip(self.personal_best_positions[idx] + perturbation, lb, ub)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_swarm(lb, ub)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            for i in range(self.swarm_size):\n                if evaluations >= self.budget:\n                    break\n\n                current_value = func(self.positions[i])\n                evaluations += 1\n\n                if current_value < self.personal_best_values[i]:\n                    self.personal_best_values[i] = current_value\n                    self.personal_best_positions[i] = self.positions[i].copy()\n\n                if current_value < self.global_best_value:\n                    self.global_best_value = current_value\n                    self.global_best_position = self.positions[i].copy()\n\n            self.history.append(self.global_best_value)\n            self.dynamic_inertia_adjustment()\n            self.adaptive_coefficient_tuning()\n            self.elite_learning(lb, ub)\n\n            for i in range(self.swarm_size):\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                r3 = np.random.rand(self.dim)\n                cognitive_component = self.c1 * r1 * (self.personal_best_positions[i] - self.positions[i])\n                social_component = self.c2 * r2 * (self.global_best_position - self.positions[i])\n                cooperative_component = self.c3 * r3 * (np.mean(self.personal_best_positions, axis=0) - self.positions[i])\n                self.velocities[i] = self.w * self.velocities[i] + cognitive_component + social_component + cooperative_component\n                self.positions[i] += self.velocities[i]\n\n                if np.random.rand() < self.adapt_rate:\n                    self.positions[i] = self.quantum_position_update(self.positions[i], self.global_best_position)\n\n                # Apply differential mutation occasionally\n                if np.random.rand() < 0.1:\n                    self.positions[i] = self.differential_mutation(self.positions[i], self.global_best_position)\n\n        return self.global_best_position, self.global_best_value", "name": "EQIDL_PSO", "description": "Enhanced Quantum-Inspired Dynamic Learning PSO (EQIDL-PSO) introduces a differential evolution-inspired mutation strategy and adaptive coefficient tuning to improve exploration and exploitation balance in dynamic environments.", "configspace": "", "generation": 81, "fitness": 0.2775310763487678, "feedback": "The algorithm EQIDL_PSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28 with standard deviation 0.00.", "error": "", "parent_id": "57c46d13-5c5a-44fb-9920-c23452b2d979", "metadata": {"aucs": [0.27768784424414095, 0.2774765452469453, 0.27742883955521713]}, "mutation_prompt": null}
{"id": "1b05972e-e538-4d01-a90e-8f8287667beb", "solution": "import numpy as np\n\nclass EQIDL_PSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = min(50, budget)\n        self.num_swarms = 3  # Multi-swarm approach\n        self.positions = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_values = None\n        self.global_best_positions = [None] * self.num_swarms\n        self.global_best_values = [np.inf] * self.num_swarms\n        self.w = 0.9\n        self.c1 = 1.5\n        self.c2 = 1.5\n        self.c3 = 1.0\n        self.adapt_rate = 0.1\n        self.history = []\n\n    def initialize_swarm(self, lb, ub):\n        self.positions = np.random.uniform(lb, ub, (self.num_swarms, self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.num_swarms, self.swarm_size, self.dim))\n        self.personal_best_positions = self.positions.copy()\n        self.personal_best_values = np.full((self.num_swarms, self.swarm_size), np.inf)\n        self.bounds = (lb, ub)\n\n    def quantum_position_update(self, position, global_best):\n        beta = np.random.normal(0, 1, self.dim)\n        delta = np.random.normal(0, 1, self.dim) * 0.05\n        new_position = position + beta * (global_best - position) + delta\n        lb, ub = self.bounds\n        return np.clip(new_position, lb, ub)\n\n    def adaptive_boundary_adjustment(self, lb, ub):\n        span = ub - lb\n        min_diversity = 0.1 * np.sqrt(np.sum(span**2))  # 10% of the max possible diversity\n        for swarm_idx, global_best in enumerate(self.global_best_positions):\n            if global_best is not None:\n                diversity = np.std(self.positions[swarm_idx], axis=0).mean()\n                if diversity < min_diversity:\n                    adjust_factor = 0.1 * span\n                    new_lb = np.maximum(lb, global_best - adjust_factor)\n                    new_ub = np.minimum(ub, global_best + adjust_factor)\n                    for i in range(self.swarm_size):\n                        if np.any(self.positions[swarm_idx][i] < new_lb) or np.any(self.positions[swarm_idx][i] > new_ub):\n                            self.positions[swarm_idx][i] = np.random.uniform(new_lb, new_ub, self.dim)\n\n    def elite_learning(self, lb, ub):\n        elite_count = int(0.1 * self.swarm_size)\n        for swarm_idx in range(self.num_swarms):\n            elite_indices = np.argsort(self.personal_best_values[swarm_idx])[:elite_count]\n            for idx in elite_indices:\n                neighborhood_radius = np.exp(-self.dim / self.swarm_size)\n                perturbation = np.random.uniform(-neighborhood_radius, neighborhood_radius, self.dim)\n                self.personal_best_positions[swarm_idx][idx] = np.clip(self.personal_best_positions[swarm_idx][idx] + perturbation, lb, ub)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_swarm(lb, ub)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            for swarm_idx in range(self.num_swarms):\n                for i in range(self.swarm_size):\n                    if evaluations >= self.budget:\n                        break\n\n                    current_value = func(self.positions[swarm_idx][i])\n                    evaluations += 1\n\n                    if current_value < self.personal_best_values[swarm_idx][i]:\n                        self.personal_best_values[swarm_idx][i] = current_value\n                        self.personal_best_positions[swarm_idx][i] = self.positions[swarm_idx][i].copy()\n\n                    if current_value < self.global_best_values[swarm_idx]:\n                        self.global_best_values[swarm_idx] = current_value\n                        self.global_best_positions[swarm_idx] = self.positions[swarm_idx][i].copy()\n\n                self.history.append(min(self.global_best_values))\n                self.adaptive_boundary_adjustment(lb, ub)\n                self.elite_learning(lb, ub)\n\n                for i in range(self.swarm_size):\n                    r1 = np.random.rand(self.dim)\n                    r2 = np.random.rand(self.dim)\n                    r3 = np.random.rand(self.dim)\n                    cognitive_component = self.c1 * r1 * (self.personal_best_positions[swarm_idx][i] - self.positions[swarm_idx][i])\n                    social_component = self.c2 * r2 * (self.global_best_positions[swarm_idx] - self.positions[swarm_idx][i])\n                    cooperative_component = self.c3 * r3 * (np.mean(self.personal_best_positions[swarm_idx], axis=0) - self.positions[swarm_idx][i])\n                    self.velocities[swarm_idx][i] = self.w * self.velocities[swarm_idx][i] + cognitive_component + social_component + cooperative_component\n                    self.positions[swarm_idx][i] += self.velocities[swarm_idx][i]\n\n                    if np.random.rand() < self.adapt_rate:\n                        self.positions[swarm_idx][i] = self.quantum_position_update(self.positions[swarm_idx][i], self.global_best_positions[swarm_idx])\n\n        global_best_idx = np.argmin(self.global_best_values)\n        return self.global_best_positions[global_best_idx], self.global_best_values[global_best_idx]", "name": "EQIDL_PSO", "description": "Enhanced Quantum-Inspired Dynamic Learning PSO (EQIDL-PSO) introduces a multi-swarm strategy and adaptive boundary adjustment to improve exploration and convergence in photonic structure optimization.", "configspace": "", "generation": 82, "fitness": 0.27294550299171766, "feedback": "The algorithm EQIDL_PSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.27 with standard deviation 0.00.", "error": "", "parent_id": "57c46d13-5c5a-44fb-9920-c23452b2d979", "metadata": {"aucs": [0.27110881499692263, 0.27221418084759863, 0.27551351313063166]}, "mutation_prompt": null}
{"id": "e02608f1-42ca-4d15-a287-4dbe99c1b773", "solution": "import numpy as np\n\nclass QE_PSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = min(50, budget)\n        self.positions = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_values = None\n        self.global_best_position = None\n        self.global_best_value = np.inf\n        self.w = 0.7  # Initial inertia weight\n        self.c1 = 2.0  # Cognitive coefficient\n        self.c2 = 2.0  # Social coefficient\n        self.mutation_rate = 0.1\n        self.history = []  # Store historical best values\n\n    def initialize_swarm(self, lb, ub):\n        self.positions = np.random.uniform(lb, ub, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.swarm_size, self.dim))\n        self.personal_best_positions = self.positions.copy()\n        self.personal_best_values = np.full(self.swarm_size, np.inf)\n        self.bounds = (lb, ub)\n\n    def quantum_position_update(self, position, global_best):\n        alpha = np.random.uniform(-1, 1, self.dim)\n        delta = np.random.normal(0, 0.1, self.dim)  # Quantum fluctuation factor\n        new_position = position + alpha * (global_best - position) + delta\n        lb, ub = self.bounds\n        return np.clip(new_position, lb, ub)\n\n    def mutate_positions(self, positions, rate):\n        for i in range(self.swarm_size):\n            if np.random.rand() < rate:\n                mutation_vector = np.random.uniform(-0.1, 0.1, self.dim)\n                positions[i] = np.clip(positions[i] + mutation_vector, *self.bounds)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_swarm(lb, ub)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            for i in range(self.swarm_size):\n                if evaluations >= self.budget:\n                    break\n\n                current_value = func(self.positions[i])\n                evaluations += 1\n\n                if current_value < self.personal_best_values[i]:\n                    self.personal_best_values[i] = current_value\n                    self.personal_best_positions[i] = self.positions[i].copy()\n\n                if current_value < self.global_best_value:\n                    self.global_best_value = current_value\n                    self.global_best_position = self.positions[i].copy()\n\n            self.history.append(self.global_best_value)\n\n            for i in range(self.swarm_size):\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                cognitive_component = self.c1 * r1 * (self.personal_best_positions[i] - self.positions[i])\n                social_component = self.c2 * r2 * (self.global_best_position - self.positions[i])\n                self.velocities[i] = self.w * self.velocities[i] + cognitive_component + social_component\n                self.positions[i] += self.velocities[i]\n\n                if np.random.rand() < self.mutation_rate:\n                    self.positions[i] = self.quantum_position_update(self.positions[i], self.global_best_position)\n\n            # Apply evolutionary selection\n            selected_indices = np.argsort(self.personal_best_values)[:self.swarm_size // 2]\n            self.positions = self.positions[selected_indices]\n            self.velocities = self.velocities[selected_indices]\n            self.mutate_positions(self.positions, self.mutation_rate)\n\n        return self.global_best_position, self.global_best_value", "name": "QE_PSO", "description": "Quantum Evolutionary Particle Swarm Optimization (QE-PSO) integrates evolutionary selection mechanisms with quantum-inspired particle updates to enhance exploration and exploitation balance for robust optimization.", "configspace": "", "generation": 83, "fitness": -Infinity, "feedback": "An exception occurred: IndexError('index 30 is out of bounds for axis 0 with size 25').", "error": "IndexError('index 30 is out of bounds for axis 0 with size 25')", "parent_id": "57c46d13-5c5a-44fb-9920-c23452b2d979", "metadata": {}, "mutation_prompt": null}
{"id": "c04e22b1-a389-41be-81ce-a78d843407a9", "solution": "import numpy as np\n\nclass AQHS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.hms = min(50, budget // 5)  # Harmony memory size\n        self.hmcr = 0.9  # Harmony memory consideration rate\n        self.par = 0.3  # Pitch adjustment rate\n        self.bandwidth = 0.1  # Bandwidth for pitch adjustment\n        self.harmony_memory = None\n        self.fitness_memory = None\n\n    def initialize_harmony_memory(self, lb, ub):\n        self.harmony_memory = np.random.uniform(lb, ub, (self.hms, self.dim))\n        self.fitness_memory = np.full(self.hms, np.inf)\n\n    def quantum_position_update(self, lb, ub):\n        beta = np.random.normal(0, 1, self.dim)\n        position = self.harmony_memory[np.argmin(self.fitness_memory)]\n        new_position = position + beta * (np.random.uniform(lb, ub, self.dim) - position) * 0.1\n        return np.clip(new_position, lb, ub)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_harmony_memory(lb, ub)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            new_harmony = np.zeros(self.dim)\n            for j in range(self.dim):\n                if np.random.rand() < self.hmcr:\n                    new_harmony[j] = self.harmony_memory[np.random.randint(self.hms), j]\n                    if np.random.rand() < self.par:\n                        new_harmony[j] += np.random.uniform(-self.bandwidth, self.bandwidth)\n                else:\n                    new_harmony[j] = np.random.uniform(lb[j], ub[j])\n            \n            new_harmony = np.clip(new_harmony, lb, ub)\n            current_value = func(new_harmony)\n            evaluations += 1\n            \n            if current_value < np.max(self.fitness_memory):\n                worst_index = np.argmax(self.fitness_memory)\n                self.harmony_memory[worst_index] = new_harmony\n                self.fitness_memory[worst_index] = current_value\n\n            if evaluations < self.budget:\n                quantum_harmony = self.quantum_position_update(lb, ub)\n                quantum_value = func(quantum_harmony)\n                evaluations += 1\n\n                if quantum_value < np.max(self.fitness_memory):\n                    worst_index = np.argmax(self.fitness_memory)\n                    self.harmony_memory[worst_index] = quantum_harmony\n                    self.fitness_memory[worst_index] = quantum_value\n\n        best_index = np.argmin(self.fitness_memory)\n        return self.harmony_memory[best_index], self.fitness_memory[best_index]", "name": "AQHS", "description": "Adaptive Quantum Harmony Search (AQHS) synergizes quantum mechanics principles with adaptive harmony consideration to enhance exploration and exploitation for robust optimization convergence.", "configspace": "", "generation": 84, "fitness": 0.2776345517423154, "feedback": "The algorithm AQHS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28 with standard deviation 0.00.", "error": "", "parent_id": "57c46d13-5c5a-44fb-9920-c23452b2d979", "metadata": {"aucs": [0.27743198647971246, 0.27807828867630446, 0.2773933800709293]}, "mutation_prompt": null}
{"id": "2e298f2b-72e9-4b3d-905b-02768099c270", "solution": "import numpy as np\n\nclass AQI_PSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = min(50, budget)\n        self.positions = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_values = None\n        self.global_best_position = None\n        self.global_best_value = np.inf\n        self.w = 0.9\n        self.c1 = 1.5\n        self.c2 = 1.5\n        self.mutation_rate = 0.1  # Adaptive mutation rate\n        self.history = []\n\n    def initialize_swarm(self, lb, ub):\n        self.positions = np.random.uniform(lb, ub, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.swarm_size, self.dim))\n        self.personal_best_positions = self.positions.copy()\n        self.personal_best_values = np.full(self.swarm_size, np.inf)\n        self.bounds = (lb, ub)\n\n    def adaptive_mutation(self, position):\n        mutation_vector = np.random.normal(0, 1, self.dim) * self.mutation_rate\n        lb, ub = self.bounds\n        new_position = position + mutation_vector\n        return np.clip(new_position, lb, ub)\n\n    def dynamic_inertia_and_mutation_adjustment(self):\n        if len(self.history) < 2:\n            return\n        improvement_rate = (self.history[-2] - self.history[-1]) / self.history[-2]\n        max_diversity = np.sqrt(np.sum((self.bounds[1] - self.bounds[0])**2))\n        diversity = np.std(self.positions, axis=0).mean()\n        self.w = 0.4 + 0.5 * (diversity / max_diversity)\n        self.mutation_rate = max(0.05, min(0.2, self.mutation_rate * (1 + improvement_rate)))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_swarm(lb, ub)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            for i in range(self.swarm_size):\n                if evaluations >= self.budget:\n                    break\n\n                current_value = func(self.positions[i])\n                evaluations += 1\n\n                if current_value < self.personal_best_values[i]:\n                    self.personal_best_values[i] = current_value\n                    self.personal_best_positions[i] = self.positions[i].copy()\n\n                if current_value < self.global_best_value:\n                    self.global_best_value = current_value\n                    self.global_best_position = self.positions[i].copy()\n\n            self.history.append(self.global_best_value)\n            self.dynamic_inertia_and_mutation_adjustment()\n\n            for i in range(self.swarm_size):\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                cognitive_component = self.c1 * r1 * (self.personal_best_positions[i] - self.positions[i])\n                social_component = self.c2 * r2 * (self.global_best_position - self.positions[i])\n                self.velocities[i] = self.w * self.velocities[i] + cognitive_component + social_component\n                self.positions[i] += self.velocities[i]\n\n                if np.random.rand() < self.mutation_rate:\n                    self.positions[i] = self.adaptive_mutation(self.positions[i])\n\n        return self.global_best_position, self.global_best_value", "name": "AQI_PSO", "description": "Adaptive Quantum-Inspired PSO (AQI-PSO) enhances convergence by incorporating an adaptive mutation strategy with a historical trend-guided search and dynamic parameter tuning.", "configspace": "", "generation": 85, "fitness": 0.2777468471035765, "feedback": "The algorithm AQI_PSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28 with standard deviation 0.00.", "error": "", "parent_id": "57c46d13-5c5a-44fb-9920-c23452b2d979", "metadata": {"aucs": [0.2778191596540498, 0.27763343091562387, 0.2777879507410559]}, "mutation_prompt": null}
{"id": "7cbcc78d-39e2-4ca2-930e-2d842acdf4d9", "solution": "import numpy as np\n\nclass AQID_PSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = min(50, budget)\n        self.positions = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_values = None\n        self.global_best_position = None\n        self.global_best_value = np.inf\n        self.w = 0.9  # Initial inertia weight\n        self.c1 = 1.5  # Cognitive coefficient\n        self.c2 = 1.5  # Social coefficient\n        self.c3 = 1.0  # Cooperative coefficient\n        self.adapt_rate = 0.1\n        self.history = []  # Store historical best values\n        self.phase_switch_threshold = 0.1  # Threshold to switch exploration phase\n        self.phase = 'exploration'  # Initial phase\n\n    def initialize_swarm(self, lb, ub):\n        self.positions = np.random.uniform(lb, ub, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.swarm_size, self.dim))\n        self.personal_best_positions = self.positions.copy()\n        self.personal_best_values = np.full(self.swarm_size, np.inf)\n        self.bounds = (lb, ub)\n\n    def quantum_position_update(self, position, global_best):\n        beta = np.random.normal(0, 1, self.dim)\n        delta = np.random.normal(0, 1, self.dim) * 0.05  # Exploration boost factor\n        new_position = position + beta * (global_best - position) + delta\n        lb, ub = self.bounds\n        return np.clip(new_position, lb, ub)\n\n    def dynamic_inertia_adjustment(self):\n        diversity = np.std(self.positions, axis=0).mean()\n        max_diversity = np.sqrt(np.sum((self.bounds[1] - self.bounds[0])**2))\n        self.w = 0.4 + 0.5 * (diversity / max_diversity)\n        if len(self.history) > 1:\n            self.w += 0.1 * (self.history[-1] - self.history[-2]) / max(self.history)\n\n    def elite_learning(self, lb, ub):\n        elite_count = int(0.1 * self.swarm_size)\n        elite_indices = np.argsort(self.personal_best_values)[:elite_count]\n        for idx in elite_indices:\n            neighborhood_radius = np.exp(-self.dim / self.swarm_size)\n            perturbation = np.random.uniform(-neighborhood_radius, neighborhood_radius, self.dim)\n            self.personal_best_positions[idx] = np.clip(self.personal_best_positions[idx] + perturbation, lb, ub)\n\n    def memory_driven_phase_adjustment(self):\n        if len(self.history) > 2:\n            improvement = (self.history[-2] - self.history[-1]) / self.history[-2]\n            if improvement < self.phase_switch_threshold:\n                self.phase = 'exploration' if self.phase == 'exploitation' else 'exploitation'\n\n    def adaptive_phase_movement(self, i):\n        if self.phase == 'exploration':\n            self.positions[i] = self.quantum_position_update(self.positions[i], self.global_best_position)\n        else:\n            r1 = np.random.rand(self.dim)\n            r2 = np.random.rand(self.dim)\n            r3 = np.random.rand(self.dim)\n            cognitive_component = self.c1 * r1 * (self.personal_best_positions[i] - self.positions[i])\n            social_component = self.c2 * r2 * (self.global_best_position - self.positions[i])\n            cooperative_component = self.c3 * r3 * (np.mean(self.personal_best_positions, axis=0) - self.positions[i])\n            self.velocities[i] = self.w * self.velocities[i] + cognitive_component + social_component + cooperative_component\n            self.positions[i] += self.velocities[i]\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_swarm(lb, ub)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            for i in range(self.swarm_size):\n                if evaluations >= self.budget:\n                    break\n\n                current_value = func(self.positions[i])\n                evaluations += 1\n\n                if current_value < self.personal_best_values[i]:\n                    self.personal_best_values[i] = current_value\n                    self.personal_best_positions[i] = self.positions[i].copy()\n\n                if current_value < self.global_best_value:\n                    self.global_best_value = current_value\n                    self.global_best_position = self.positions[i].copy()\n\n            self.history.append(self.global_best_value)\n            self.dynamic_inertia_adjustment()\n            self.memory_driven_phase_adjustment()\n            self.elite_learning(lb, ub)\n\n            for i in range(self.swarm_size):\n                self.adaptive_phase_movement(i)\n\n        return self.global_best_position, self.global_best_value", "name": "AQID_PSO", "description": "Enhanced Quantum-Inspired PSO with Adaptive Multi-phase Exploration (AQID-PSO) integrating adaptive phase-based exploration and memory-driven convergence to boost robustness and solution quality.", "configspace": "", "generation": 86, "fitness": 0.2747432032275215, "feedback": "The algorithm AQID_PSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.27 with standard deviation 0.00.", "error": "", "parent_id": "57c46d13-5c5a-44fb-9920-c23452b2d979", "metadata": {"aucs": [0.27598463025732867, 0.27345406409936435, 0.2747909153258714]}, "mutation_prompt": null}
{"id": "fd91d46d-9b2c-4da3-aed2-aaee54bd0259", "solution": "import numpy as np\n\nclass AQEPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = min(50, budget)\n        self.positions = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_values = None\n        self.global_best_position = None\n        self.global_best_value = np.inf\n        self.w = 0.9  # Initial inertia weight\n        self.c1 = 1.5  # Cognitive coefficient\n        self.c2 = 1.5  # Social coefficient\n        self.c3 = 1.0  # Cooperative coefficient\n        self.adapt_rate = 0.1\n        self.history = []  # Store historical best values\n\n    def initialize_swarm(self, lb, ub):\n        self.positions = np.random.uniform(lb, ub, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.swarm_size, self.dim))\n        self.personal_best_positions = self.positions.copy()\n        self.personal_best_values = np.full(self.swarm_size, np.inf)\n        self.bounds = (lb, ub)\n\n    def quantum_position_update(self, position, global_best):\n        beta = np.random.normal(0, 1, self.dim)\n        delta = np.random.normal(0, 1, self.dim) * 0.05  # Exploration boost factor\n        new_position = position + beta * (global_best - position) + delta\n        lb, ub = self.bounds\n        return np.clip(new_position, lb, ub)\n\n    def dynamic_inertia_adjustment(self):\n        diversity = np.std(self.positions, axis=0).mean()\n        max_diversity = np.sqrt(np.sum((self.bounds[1] - self.bounds[0])**2))\n        self.w = 0.4 + 0.5 * (diversity / max_diversity)\n        if len(self.history) > 1:\n            self.w += 0.1 * (self.history[-1] - self.history[-2]) / max(abs(self.history[-1]), 1e-10)\n\n    def adaptive_elite_learning(self, lb, ub):\n        elite_count = int(0.1 * self.swarm_size)\n        elite_indices = np.argsort(self.personal_best_values)[:elite_count]\n        improvement_factor = (self.global_best_value - min(self.history, default=self.global_best_value)) / max(self.global_best_value, 1e-10)\n        perturbation_strength = np.exp(-improvement_factor)  # Stronger perturbation when improvement stalls\n        for idx in elite_indices:\n            perturbation = np.random.uniform(-perturbation_strength, perturbation_strength, self.dim)\n            self.personal_best_positions[idx] = np.clip(self.personal_best_positions[idx] + perturbation, lb, ub)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_swarm(lb, ub)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            for i in range(self.swarm_size):\n                if evaluations >= self.budget:\n                    break\n\n                current_value = func(self.positions[i])\n                evaluations += 1\n\n                if current_value < self.personal_best_values[i]:\n                    self.personal_best_values[i] = current_value\n                    self.personal_best_positions[i] = self.positions[i].copy()\n\n                if current_value < self.global_best_value:\n                    self.global_best_value = current_value\n                    self.global_best_position = self.positions[i].copy()\n\n            self.history.append(self.global_best_value)\n            self.dynamic_inertia_adjustment()\n            self.adaptive_elite_learning(lb, ub)\n\n            for i in range(self.swarm_size):\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                r3 = np.random.rand(self.dim)\n                cognitive_component = self.c1 * r1 * (self.personal_best_positions[i] - self.positions[i])\n                social_component = self.c2 * r2 * (self.global_best_position - self.positions[i])\n                cooperative_component = self.c3 * r3 * (np.mean(self.personal_best_positions, axis=0) - self.positions[i])\n                self.velocities[i] = self.w * self.velocities[i] + cognitive_component + social_component + cooperative_component\n                self.positions[i] += self.velocities[i]\n\n                if np.random.rand() < self.adapt_rate:\n                    self.positions[i] = self.quantum_position_update(self.positions[i], self.global_best_position)\n\n        return self.global_best_position, self.global_best_value", "name": "AQEPSO", "description": "The Adaptive Quantum-Enhanced PSO (AQEPSO) leverages dynamic inertia, quantum-inspired exploration, and adaptive elite perturbation based on performance to enhance convergence while maintaining diversity.", "configspace": "", "generation": 87, "fitness": 0.27783985428542585, "feedback": "The algorithm AQEPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28 with standard deviation 0.00.", "error": "", "parent_id": "57c46d13-5c5a-44fb-9920-c23452b2d979", "metadata": {"aucs": [0.27789593201904195, 0.27775408086359754, 0.277869549973638]}, "mutation_prompt": null}
{"id": "821084e4-9033-484d-b32c-19022dd70f0f", "solution": "import numpy as np\n\nclass EQI_PSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = min(50, budget)\n        self.positions = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_values = None\n        self.global_best_position = None\n        self.global_best_value = np.inf\n        self.w = 0.9\n        self.c1 = 1.5\n        self.c2 = 1.5\n        self.c3 = 1.0\n        self.pheromone_influence = 0.2  # New factor for pheromone-based communication\n        self.adapt_rate = 0.1\n        self.history = []\n\n    def initialize_swarm(self, lb, ub):\n        self.positions = np.random.uniform(lb, ub, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.swarm_size, self.dim))\n        self.personal_best_positions = self.positions.copy()\n        self.personal_best_values = np.full(self.swarm_size, np.inf)\n        self.bounds = (lb, ub)\n\n    def quantum_position_update(self, position, global_best):\n        beta = np.random.normal(0, 1, self.dim)\n        delta = np.random.normal(0, 1, self.dim) * 0.05\n        new_position = position + beta * (global_best - position) + delta\n        lb, ub = self.bounds\n        return np.clip(new_position, lb, ub)\n\n    def dynamic_inertia_adjustment(self):\n        diversity = np.std(self.positions, axis=0).mean()\n        max_diversity = np.sqrt(np.sum((self.bounds[1] - self.bounds[0])**2))\n        self.w = 0.4 + 0.5 * (diversity / max_diversity)\n        if len(self.history) > 1:\n            self.w += 0.1 * (self.history[-1] - self.history[-2]) / max(self.history)\n\n    def pheromone_communication(self):\n        pheromone = np.exp(-np.array(self.personal_best_values))\n        for i in range(self.swarm_size):\n            if np.random.rand() < self.pheromone_influence:\n                influence = np.random.choice(self.swarm_size, p=pheromone/pheromone.sum())\n                self.positions[i] = self.personal_best_positions[influence]\n\n    def elite_search(self, lb, ub):\n        elite_count = int(0.1 * self.swarm_size)\n        elite_indices = np.argsort(self.personal_best_values)[:elite_count]\n        for idx in elite_indices:\n            if np.random.rand() < 0.5:  # Introduce probabilistic search\n                neighborhood_radius = np.exp(-self.dim / self.swarm_size)\n                perturbation = np.random.uniform(-neighborhood_radius, neighborhood_radius, self.dim)\n                self.personal_best_positions[idx] = np.clip(self.personal_best_positions[idx] + perturbation, lb, ub)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_swarm(lb, ub)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            for i in range(self.swarm_size):\n                if evaluations >= self.budget:\n                    break\n\n                current_value = func(self.positions[i])\n                evaluations += 1\n\n                if current_value < self.personal_best_values[i]:\n                    self.personal_best_values[i] = current_value\n                    self.personal_best_positions[i] = self.positions[i].copy()\n\n                if current_value < self.global_best_value:\n                    self.global_best_value = current_value\n                    self.global_best_position = self.positions[i].copy()\n\n            self.history.append(self.global_best_value)\n            self.dynamic_inertia_adjustment()\n            self.pheromone_communication()\n            self.elite_search(lb, ub)\n\n            for i in range(self.swarm_size):\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                r3 = np.random.rand(self.dim)\n                cognitive_component = self.c1 * r1 * (self.personal_best_positions[i] - self.positions[i])\n                social_component = self.c2 * r2 * (self.global_best_position - self.positions[i])\n                cooperative_component = self.c3 * r3 * (np.mean(self.personal_best_positions, axis=0) - self.positions[i])\n                self.velocities[i] = self.w * self.velocities[i] + cognitive_component + social_component + cooperative_component\n                self.positions[i] += self.velocities[i]\n\n                if np.random.rand() < self.adapt_rate:\n                    self.positions[i] = self.quantum_position_update(self.positions[i], self.global_best_position)\n\n        return self.global_best_position, self.global_best_value", "name": "EQI_PSO", "description": "Enhanced Quantum-Inspired PSO (EQI-PSO) utilizes adaptive inertia, pheromone-based communication, and probabilistic elite search to improve convergence speed and solution quality in dynamic environments.", "configspace": "", "generation": 88, "fitness": 0.2765182571844717, "feedback": "The algorithm EQI_PSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28 with standard deviation 0.00.", "error": "", "parent_id": "57c46d13-5c5a-44fb-9920-c23452b2d979", "metadata": {"aucs": [0.2759283336309092, 0.27650184449226545, 0.27712459343024054]}, "mutation_prompt": null}
{"id": "2d226ad7-bb72-4d7f-9091-ec5fd6ffe1bf", "solution": "import numpy as np\n\nclass AQSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = min(50, budget)\n        self.positions = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_values = None\n        self.global_best_position = None\n        self.global_best_value = np.inf\n        self.w = 0.7  # Initial inertia weight\n        self.c1 = 1.4  # Cognitive coefficient\n        self.c2 = 1.4  # Social coefficient\n        self.c3 = 1.2  # Adaptive coefficient for quantum exploration\n        self.adapt_rate = 0.15\n        self.history = []  # Store historical best values\n\n    def initialize_swarm(self, lb, ub):\n        self.positions = np.random.uniform(lb, ub, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.swarm_size, self.dim))\n        self.personal_best_positions = self.positions.copy()\n        self.personal_best_values = np.full(self.swarm_size, np.inf)\n        self.bounds = (lb, ub)\n\n    def quantum_position_adaptive(self, position, global_best):\n        beta = np.random.normal(0, 1, self.dim)\n        exploration_factor = np.random.beta(2, 5)  # Adaptive exploration factor\n        new_position = position + beta * (global_best - position) * exploration_factor\n        lb, ub = self.bounds\n        return np.clip(new_position, lb, ub)\n\n    def dynamic_diversity_control(self):\n        diversity = np.std(self.positions, axis=0).mean()\n        max_diversity = np.sqrt(np.sum((self.bounds[1] - self.bounds[0])**2))\n        self.w = 0.5 + 0.4 * (diversity / max_diversity)\n        if len(self.history) > 1:\n            recent_improvement = (self.history[-1] - self.history[-2]) / max(self.history)\n            self.w += 0.05 * np.clip(recent_improvement, -0.2, 0.2)\n\n    def layer_learning(self, lb, ub):\n        layer_count = int(0.1 * self.swarm_size)\n        best_indices = np.argsort(self.personal_best_values)[:layer_count]\n        for idx in best_indices:\n            learning_rate = np.exp(-self.dim / self.swarm_size)\n            perturbation = np.random.uniform(-learning_rate, learning_rate, self.dim)\n            self.personal_best_positions[idx] = np.clip(self.personal_best_positions[idx] + perturbation, lb, ub)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_swarm(lb, ub)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            for i in range(self.swarm_size):\n                if evaluations >= self.budget:\n                    break\n\n                current_value = func(self.positions[i])\n                evaluations += 1\n\n                if current_value < self.personal_best_values[i]:\n                    self.personal_best_values[i] = current_value\n                    self.personal_best_positions[i] = self.positions[i].copy()\n\n                if current_value < self.global_best_value:\n                    self.global_best_value = current_value\n                    self.global_best_position = self.positions[i].copy()\n\n            self.history.append(self.global_best_value)\n            self.dynamic_diversity_control()\n            self.layer_learning(lb, ub)\n\n            for i in range(self.swarm_size):\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                r3 = np.random.rand(self.dim)\n                cognitive_component = self.c1 * r1 * (self.personal_best_positions[i] - self.positions[i])\n                social_component = self.c2 * r2 * (self.global_best_position - self.positions[i])\n                adaptive_component = self.c3 * r3 * (np.mean(self.personal_best_positions, axis=0) - self.positions[i])\n                self.velocities[i] = self.w * self.velocities[i] + cognitive_component + social_component + adaptive_component\n                self.positions[i] += self.velocities[i]\n\n                if np.random.rand() < self.adapt_rate:\n                    self.positions[i] = self.quantum_position_adaptive(self.positions[i], self.global_best_position)\n\n        return self.global_best_position, self.global_best_value", "name": "AQSO", "description": "Adaptive Quantum Swarm Optimization (AQSO) integrates adaptive quantum-inspired exploration with a multi-layered learning mechanism and dynamic diversity control to enhance global search efficiency and convergence accuracy.", "configspace": "", "generation": 89, "fitness": 0.27779132971071396, "feedback": "The algorithm AQSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28 with standard deviation 0.00.", "error": "", "parent_id": "57c46d13-5c5a-44fb-9920-c23452b2d979", "metadata": {"aucs": [0.2778006883201327, 0.27784004724905387, 0.2777332535629553]}, "mutation_prompt": null}
{"id": "d222dd27-fe45-4e5a-806b-18e378ae6dda", "solution": "import numpy as np\n\nclass AQIEA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(50, budget)\n        self.positions = None\n        self.fitness = None\n        self.best_solution = None\n        self.best_value = np.inf\n        self.mutation_rate = 0.1\n        self.adapt_rate = 0.1\n        self.elite_fraction = 0.2\n\n    def initialize_population(self, lb, ub):\n        self.positions = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n        self.bounds = (lb, ub)\n\n    def adaptive_mutation(self, position):\n        mutation_strength = np.random.normal(0, 0.1, self.dim)\n        new_position = position + self.mutation_rate * mutation_strength * (self.bounds[1] - self.bounds[0])\n        return np.clip(new_position, self.bounds[0], self.bounds[1])\n\n    def quantum_exploration(self, elite_position):\n        beta = np.random.normal(0, 1, self.dim)\n        delta = np.random.normal(0, 1, self.dim) * 0.05\n        new_position = elite_position + beta * (self.best_solution - elite_position) + delta\n        return np.clip(new_position, self.bounds[0], self.bounds[1])\n\n    def select_elites(self):\n        elite_count = int(self.elite_fraction * self.population_size)\n        elite_indices = np.argsort(self.fitness)[:elite_count]\n        return self.positions[elite_indices]\n\n    def evolve(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                current_value = func(self.positions[i])\n                evaluations += 1\n\n                if current_value < self.fitness[i]:\n                    self.fitness[i] = current_value\n\n                if current_value < self.best_value:\n                    self.best_value = current_value\n                    self.best_solution = self.positions[i].copy()\n\n            elites = self.select_elites()\n\n            for i in range(self.population_size):\n                if np.random.rand() < self.adapt_rate:\n                    self.positions[i] = self.adaptive_mutation(self.positions[i])\n                if np.random.rand() < self.mutation_rate:\n                    elite_choice = elites[np.random.randint(len(elites))]\n                    self.positions[i] = self.quantum_exploration(elite_choice)\n\n        return self.best_solution, self.best_value\n\n    def __call__(self, func):\n        return self.evolve(func)", "name": "AQIEA", "description": "Adaptive Quantum-Inspired Evolutionary Algorithm (AQIEA) combines adaptive evolutionary learning with quantum-inspired exploration to enhance diversity and solution precision in global optimization.", "configspace": "", "generation": 90, "fitness": 0.2652717122297858, "feedback": "The algorithm AQIEA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.27 with standard deviation 0.01.", "error": "", "parent_id": "57c46d13-5c5a-44fb-9920-c23452b2d979", "metadata": {"aucs": [0.2555070486490064, 0.26953385079909487, 0.27077423724125604]}, "mutation_prompt": null}
{"id": "df07600c-2982-4385-9c65-31d809ba0732", "solution": "import numpy as np\n\nclass HQE_PSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = min(50, budget)\n        self.positions = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_values = None\n        self.global_best_position = None\n        self.global_best_value = np.inf\n        self.w = 0.9  # Initial inertia weight\n        self.c1 = 1.5  # Cognitive coefficient\n        self.c2 = 1.5  # Social coefficient\n        self.c3 = 1.0  # Cooperative coefficient\n        self.adapt_rate = 0.1\n        self.history = []  # Store historical best values\n        self.memory = np.zeros((self.swarm_size, self.dim))  # Adaptive memory component\n\n    def initialize_swarm(self, lb, ub):\n        self.positions = np.random.uniform(lb, ub, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.swarm_size, self.dim))\n        self.personal_best_positions = self.positions.copy()\n        self.personal_best_values = np.full(self.swarm_size, np.inf)\n        self.bounds = (lb, ub)\n\n    def quantum_position_update(self, position, global_best):\n        beta = np.random.normal(0, 1, self.dim)\n        delta = np.random.normal(0, 1, self.dim) * 0.05  # Exploration boost factor\n        new_position = position + beta * (global_best - position) + delta\n        lb, ub = self.bounds\n        return np.clip(new_position, lb, ub)\n\n    def dynamic_inertia_adjustment(self):\n        diversity = np.std(self.positions, axis=0).mean()\n        max_diversity = np.sqrt(np.sum((self.bounds[1] - self.bounds[0])**2))\n        self.w = 0.4 + 0.5 * (diversity / max_diversity)\n        # Historical diversity-driven adjustment\n        if len(self.history) > 1:\n            self.w += 0.1 * (self.history[-1] - self.history[-2]) / max(self.history)\n\n    def elite_learning(self, lb, ub):\n        elite_count = int(0.1 * self.swarm_size)\n        elite_indices = np.argsort(self.personal_best_values)[:elite_count]\n        for idx in elite_indices:\n            neighborhood_radius = np.exp(-self.dim / self.swarm_size)\n            perturbation = np.random.uniform(-neighborhood_radius, neighborhood_radius, self.dim)\n            self.personal_best_positions[idx] = np.clip(self.personal_best_positions[idx] + perturbation, lb, ub)\n\n    def adaptive_memory_component(self, index):\n        recent_change = self.positions[index] - self.memory[index]\n        self.memory[index] = self.positions[index]\n        return recent_change\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_swarm(lb, ub)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            for i in range(self.swarm_size):\n                if evaluations >= self.budget:\n                    break\n\n                current_value = func(self.positions[i])\n                evaluations += 1\n\n                if current_value < self.personal_best_values[i]:\n                    self.personal_best_values[i] = current_value\n                    self.personal_best_positions[i] = self.positions[i].copy()\n\n                if current_value < self.global_best_value:\n                    self.global_best_value = current_value\n                    self.global_best_position = self.positions[i].copy()\n\n            self.history.append(self.global_best_value)\n            self.dynamic_inertia_adjustment()\n            self.elite_learning(lb, ub)\n\n            for i in range(self.swarm_size):\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                r3 = np.random.rand(self.dim)\n                adaptive_component = self.adaptive_memory_component(i)\n                cognitive_component = self.c1 * r1 * (self.personal_best_positions[i] - self.positions[i])\n                social_component = self.c2 * r2 * (self.global_best_position - self.positions[i])\n                cooperative_component = self.c3 * r3 * (np.mean(self.personal_best_positions, axis=0) - self.positions[i])\n                self.velocities[i] = self.w * self.velocities[i] + cognitive_component + social_component + cooperative_component + 0.5 * adaptive_component\n                self.positions[i] += self.velocities[i]\n\n                if np.random.rand() < self.adapt_rate:\n                    self.positions[i] = self.quantum_position_update(self.positions[i], self.global_best_position)\n\n        return self.global_best_position, self.global_best_value", "name": "HQE_PSO", "description": "Hybrid Quantum-Enhanced PSO with Adaptive Learning and Memory (HQE-PSO) incorporates quantum-enhanced elite perturbation, adaptive memory-based adjustment, and dynamic swarm restructuring to enhance exploration and exploitation balance.", "configspace": "", "generation": 91, "fitness": 0.25714897433369244, "feedback": "The algorithm HQE_PSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.26 with standard deviation 0.00.", "error": "", "parent_id": "57c46d13-5c5a-44fb-9920-c23452b2d979", "metadata": {"aucs": [0.2567497044344741, 0.26323342481095924, 0.251463793755644]}, "mutation_prompt": null}
{"id": "0edb7c4e-dfe2-422a-b62a-f9a004be187c", "solution": "import numpy as np\n\nclass QEDE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(50, budget)\n        self.mutation_factor = 0.8\n        self.crossover_rate = 0.9\n        self.quantum_factor = 0.05\n        self.population = None\n        self.best_solution = None\n        self.best_value = np.inf\n\n    def initialize_population(self, lb, ub):\n        self.population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def quantum_perturbation(self, vector):\n        perturbation = np.random.normal(0, self.quantum_factor, self.dim)\n        return vector + perturbation\n\n    def select_parents(self, idx):\n        candidates = list(range(self.population_size))\n        candidates.remove(idx)\n        parents = np.random.choice(candidates, 3, replace=False)\n        return parents\n\n    def create_offspring(self, idx, lb, ub):\n        a, b, c = self.select_parents(idx)\n        mutant_vector = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        mutant_vector = np.clip(mutant_vector, lb, ub)\n\n        trial_vector = np.copy(self.population[idx])\n        crossover_mask = np.random.rand(self.dim) < self.crossover_rate\n        trial_vector[crossover_mask] = mutant_vector[crossover_mask]\n        \n        quantum_trial = self.quantum_perturbation(trial_vector)\n        return np.clip(quantum_trial, lb, ub)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                trial_vector = self.create_offspring(i, lb, ub)\n                trial_value = func(trial_vector)\n                evaluations += 1\n\n                current_value = func(self.population[i])\n                evaluations += 1\n\n                if trial_value < current_value:\n                    self.population[i] = trial_vector\n                    current_value = trial_value\n\n                if current_value < self.best_value:\n                    self.best_value = current_value\n                    self.best_solution = self.population[i].copy()\n\n        return self.best_solution, self.best_value", "name": "QEDE", "description": "Quantum-Enhanced Differential Evolution (QEDE) integrates quantum-inspired perturbations into Differential Evolution to enhance global exploration and prevent premature convergence.", "configspace": "", "generation": 92, "fitness": 0.2601201659605682, "feedback": "The algorithm QEDE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.26 with standard deviation 0.00.", "error": "", "parent_id": "57c46d13-5c5a-44fb-9920-c23452b2d979", "metadata": {"aucs": [0.2626539074780958, 0.25843430175777227, 0.2592722886458365]}, "mutation_prompt": null}
{"id": "ad57ba64-b6b2-4a9c-a710-65e9b816345d", "solution": "import numpy as np\n\nclass AQES:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(50, budget)\n        self.population = None\n        self.fitness = None\n        self.best_individual = None\n        self.best_fitness = np.inf\n        self.mutation_rate = 0.1\n        self.entanglement_factor = 0.5\n\n    def initialize_population(self, lb, ub):\n        self.population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            current_fitness = func(self.population[i])\n            if current_fitness < self.fitness[i]:\n                self.fitness[i] = current_fitness\n            if current_fitness < self.best_fitness:\n                self.best_fitness = current_fitness\n                self.best_individual = self.population[i].copy()\n\n    def adaptive_mutation(self, lb, ub):\n        mean_fitness = np.mean(self.fitness)\n        mutation_rate = self.mutation_rate * (1 - (self.best_fitness / mean_fitness))\n        for i in range(self.population_size):\n            if np.random.rand() < mutation_rate:\n                mutation_vector = np.random.normal(0, 1, self.dim) * self.entanglement_factor\n                self.population[i] = np.clip(self.population[i] + mutation_vector, lb, ub)\n\n    def quantum_superposition(self, lb, ub):\n        centroid = np.mean(self.population, axis=0)\n        for i in range(self.population_size):\n            superposition_state = np.random.uniform(lb, ub, self.dim)\n            self.population[i] = (self.population[i] + superposition_state + centroid) / 3\n            self.population[i] = np.clip(self.population[i], lb, ub)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            if evaluations >= self.budget:\n                break\n\n            self.evaluate_population(func)\n            evaluations += self.population_size\n\n            self.adaptive_mutation(lb, ub)\n            self.quantum_superposition(lb, ub)\n\n        return self.best_individual, self.best_fitness", "name": "AQES", "description": "Adaptive Quantum Evolutionary Strategy (AQES) leverages quantum-inspired superposition and entanglement principles combined with adaptive mutation rates to enhance exploration and convergence in high-dimensional spaces.", "configspace": "", "generation": 93, "fitness": 0.2747161389348012, "feedback": "The algorithm AQES got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.27 with standard deviation 0.00.", "error": "", "parent_id": "57c46d13-5c5a-44fb-9920-c23452b2d979", "metadata": {"aucs": [0.2752130038399595, 0.2749260709623491, 0.2740093420020949]}, "mutation_prompt": null}
{"id": "247ec53f-a48d-40f5-8019-d1effe90ea9c", "solution": "import numpy as np\n\nclass AQHS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.harmony_memory_size = min(50, budget)\n        self.harmony_memory = None\n        self.harmony_memory_values = None\n        self.best_harmony = None\n        self.best_value = np.inf\n        self.hmcr = 0.9  # Harmony Memory Consideration Rate\n        self.par = 0.3  # Pitch Adjustment Rate\n        self.fret_range = 0.05\n        self.memory_decay = 0.98  # Reduce memory influence over time\n\n    def initialize_harmony_memory(self, lb, ub):\n        self.harmony_memory = np.random.uniform(lb, ub, (self.harmony_memory_size, self.dim))\n        self.harmony_memory_values = np.full(self.harmony_memory_size, np.inf)\n        self.bounds = (lb, ub)\n\n    def quantum_harmony_sampling(self, lb, ub):\n        beta = np.random.normal(0, 1, self.dim)\n        delta = np.random.normal(0, 1, self.dim) * self.fret_range\n        new_harmony = self.best_harmony + beta * (np.mean(self.harmony_memory, axis=0) - self.best_harmony) + delta\n        return np.clip(new_harmony, lb, ub)\n\n    def adaptive_pitch_adjustment(self, harmony, lb, ub):\n        if np.random.rand() < self.par:\n            random_index = np.random.randint(0, self.dim)\n            harmony[random_index] += np.random.uniform(-self.fret_range, self.fret_range)\n        return np.clip(harmony, lb, ub)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_harmony_memory(lb, ub)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            for i in range(self.harmony_memory_size):\n                if evaluations >= self.budget:\n                    break\n\n                current_value = func(self.harmony_memory[i])\n                evaluations += 1\n\n                if current_value < self.harmony_memory_values[i]:\n                    self.harmony_memory_values[i] = current_value\n                    self.harmony_memory[i] = self.harmony_memory[i].copy()\n\n                if current_value < self.best_value:\n                    self.best_value = current_value\n                    self.best_harmony = self.harmony_memory[i].copy()\n\n            new_harmony = np.zeros(self.dim)\n            for j in range(self.dim):\n                if np.random.rand() < self.hmcr:\n                    index = np.random.randint(0, self.harmony_memory_size)\n                    new_harmony[j] = self.harmony_memory[index][j]\n                else:\n                    new_harmony[j] = np.random.uniform(lb[j], ub[j])\n            \n            new_harmony = self.adaptive_pitch_adjustment(new_harmony, lb, ub)\n\n            if np.random.rand() < (1 - self.memory_decay):\n                new_harmony = self.quantum_harmony_sampling(lb, ub)\n\n            new_value = func(new_harmony)\n            evaluations += 1\n\n            if new_value < np.max(self.harmony_memory_values):\n                max_index = np.argmax(self.harmony_memory_values)\n                self.harmony_memory[max_index] = new_harmony\n                self.harmony_memory_values[max_index] = new_value\n\n                if new_value < self.best_value:\n                    self.best_value = new_value\n                    self.best_harmony = new_harmony\n\n        return self.best_harmony, self.best_value", "name": "AQHS", "description": "Adaptive Quantum Harmony Search (AQHS) integrates quantum-inspired harmonic best sampling with adaptive memory and diversity enhancement to optimize convergence speed and solution quality.", "configspace": "", "generation": 94, "fitness": 0.24766924764054463, "feedback": "The algorithm AQHS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.25 with standard deviation 0.00.", "error": "", "parent_id": "57c46d13-5c5a-44fb-9920-c23452b2d979", "metadata": {"aucs": [0.245833532981672, 0.24655727857908694, 0.25061693136087493]}, "mutation_prompt": null}
{"id": "98a81921-8bcc-4031-84f2-a048e879aafd", "solution": "import numpy as np\n\nclass AQGA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(50, budget)\n        self.population = None\n        self.fitness = None\n        self.best_individual = None\n        self.best_fitness = np.inf\n        self.bounds = None\n    \n    def initialize_population(self, lb, ub):\n        self.population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n        self.bounds = (lb, ub)\n    \n    def evaluate_population(self, func):\n        evaluations = 0\n        for i in range(self.population_size):\n            if evaluations >= self.budget:\n                break\n            current_fitness = func(self.population[i])\n            evaluations += 1\n            self.fitness[i] = current_fitness\n            if current_fitness < self.best_fitness:\n                self.best_fitness = current_fitness\n                self.best_individual = self.population[i].copy()\n        return evaluations\n    \n    def quantum_crossover(self):\n        parent1, parent2 = self.select_parents()\n        beta = np.random.uniform(-0.1, 1.1, self.dim)\n        child1 = parent1 * beta + parent2 * (1 - beta)\n        child2 = parent2 * beta + parent1 * (1 - beta)\n        return child1, child2\n    \n    def select_parents(self):\n        indices = np.random.choice(self.population_size, 2, replace=False, p=self.fitness_probs())\n        return self.population[indices[0]], self.population[indices[1]]\n    \n    def fitness_probs(self):\n        inv_fitness = 1.0 / (self.fitness + 1e-10)\n        return inv_fitness / inv_fitness.sum()\n    \n    def mutate(self, individual):\n        mutation_strength = np.random.exponential(0.1, self.dim)\n        mutation_vector = np.random.normal(0, mutation_strength, self.dim)\n        mutated = individual + mutation_vector\n        lb, ub = self.bounds\n        return np.clip(mutated, lb, ub)\n\n    def adaptive_mutation(self):\n        mutation_rate = 0.2 * (1 - (self.best_fitness / (np.mean(self.fitness) + 1e-10)))\n        return mutation_rate\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        evaluations = self.evaluate_population(func)\n\n        while evaluations < self.budget:\n            new_population = []\n            for _ in range(self.population_size // 2):\n                child1, child2 = self.quantum_crossover()\n                if np.random.rand() < self.adaptive_mutation():\n                    child1 = self.mutate(child1)\n                if np.random.rand() < self.adaptive_mutation():\n                    child2 = self.mutate(child2)\n                new_population.extend([child1, child2])\n            self.population = np.array(new_population[:self.population_size])\n            evaluations += self.evaluate_population(func)\n        \n        return self.best_individual, self.best_fitness", "name": "AQGA", "description": "The Adaptive Quantum Genetic Algorithm (AQGA) uses quantum-inspired encoding and an adaptive crossover/mutation mechanism to maintain diversity and convergence efficiency in high-dimensional optimization.", "configspace": "", "generation": 95, "fitness": 0.27052520006573794, "feedback": "The algorithm AQGA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.27 with standard deviation 0.01.", "error": "", "parent_id": "57c46d13-5c5a-44fb-9920-c23452b2d979", "metadata": {"aucs": [0.2759528942336207, 0.26153423933507947, 0.2740884666285136]}, "mutation_prompt": null}
{"id": "352819de-550e-426e-8c53-c7aa72892e33", "solution": "import numpy as np\n\nclass AQL_PSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = min(50, budget)\n        self.positions = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_values = None\n        self.global_best_position = None\n        self.global_best_value = np.inf\n        self.w = 0.9\n        self.c1 = 1.5\n        self.c2 = 1.5\n        self.c3 = 1.2  # Increased cooperative coefficient\n        self.adapt_rate = 0.2  # Increased adaptation rate\n        self.history = []\n\n    def initialize_swarm(self, lb, ub):\n        self.positions = np.random.uniform(lb, ub, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.swarm_size, self.dim))\n        self.personal_best_positions = self.positions.copy()\n        self.personal_best_values = np.full(self.swarm_size, np.inf)\n        self.bounds = (lb, ub)\n\n    def adaptive_quantum_position_update(self, position, global_best):\n        alpha = 0.1 + 0.9 * np.random.rand()  # Adaptive scaling factor\n        beta = np.random.normal(0, 1, self.dim)\n        delta = np.random.normal(0, 1, self.dim) * 0.05\n        new_position = position + alpha * beta * (global_best - position) + delta\n        lb, ub = self.bounds\n        return np.clip(new_position, lb, ub)\n\n    def dynamic_inertia_adjustment(self):\n        diversity = np.std(self.positions, axis=0).mean()\n        max_diversity = np.sqrt(np.sum((self.bounds[1] - self.bounds[0])**2))\n        self.w = 0.4 + 0.5 * (diversity / max_diversity)\n        if len(self.history) > 1:\n            self.w += 0.1 * (self.history[-1] - self.history[-2]) / max(self.history)\n\n    def localized_mutation(self, lb, ub):\n        mutation_chance = 0.1  # Probability to apply mutation\n        for i in range(self.swarm_size):\n            if np.random.rand() < mutation_chance:\n                mutation_radius = 0.05 * (ub - lb)\n                mutation = np.random.uniform(-mutation_radius, mutation_radius, self.dim)\n                self.positions[i] = np.clip(self.positions[i] + mutation, lb, ub)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_swarm(lb, ub)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            for i in range(self.swarm_size):\n                if evaluations >= self.budget:\n                    break\n\n                current_value = func(self.positions[i])\n                evaluations += 1\n\n                if current_value < self.personal_best_values[i]:\n                    self.personal_best_values[i] = current_value\n                    self.personal_best_positions[i] = self.positions[i].copy()\n\n                if current_value < self.global_best_value:\n                    self.global_best_value = current_value\n                    self.global_best_position = self.positions[i].copy()\n\n            self.history.append(self.global_best_value)\n            self.dynamic_inertia_adjustment()\n            self.localized_mutation(lb, ub)\n\n            for i in range(self.swarm_size):\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                r3 = np.random.rand(self.dim)\n                cognitive_component = self.c1 * r1 * (self.personal_best_positions[i] - self.positions[i])\n                social_component = self.c2 * r2 * (self.global_best_position - self.positions[i])\n                cooperative_component = self.c3 * r3 * (np.mean(self.personal_best_positions, axis=0) - self.positions[i])\n                self.velocities[i] = self.w * self.velocities[i] + cognitive_component + social_component + cooperative_component\n                self.positions[i] += self.velocities[i]\n\n                if np.random.rand() < self.adapt_rate:\n                    self.positions[i] = self.adaptive_quantum_position_update(self.positions[i], self.global_best_position)\n\n        return self.global_best_position, self.global_best_value", "name": "AQL_PSO", "description": "AQL-PSO combines adaptive quantum-inspired learning with a localized mutation strategy to balance exploration and exploitation for enhanced convergence in photonic structure optimization.", "configspace": "", "generation": 96, "fitness": 0.27780644121507253, "feedback": "The algorithm AQL_PSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28 with standard deviation 0.00.", "error": "", "parent_id": "57c46d13-5c5a-44fb-9920-c23452b2d979", "metadata": {"aucs": [0.2777916567194023, 0.27768606324532263, 0.2779416036804926]}, "mutation_prompt": null}
{"id": "170da771-71d2-426a-a2d9-a38dc996bd4b", "solution": "import numpy as np\n\nclass EQA_PSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = min(50, budget)\n        self.positions = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_values = None\n        self.global_best_position = None\n        self.global_best_value = np.inf\n        self.w = 0.9  # Inertia weight\n        self.c1 = 1.5  # Cognitive coefficient\n        self.c2 = 1.5  # Social coefficient\n        self.c3 = 1.0  # Cooperative coefficient\n        self.adapt_rate = 0.1\n        self.history = []\n        self.velocity_clamp_factor = 0.1\n        self.evolutionary_pressure = 0.05\n\n    def initialize_swarm(self, lb, ub):\n        self.positions = np.random.uniform(lb, ub, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.swarm_size, self.dim))\n        self.personal_best_positions = self.positions.copy()\n        self.personal_best_values = np.full(self.swarm_size, np.inf)\n        self.bounds = (lb, ub)\n\n    def quantum_position_update(self, position, global_best):\n        beta = np.random.normal(0, 1, self.dim)\n        delta = np.random.normal(0, 1, self.dim) * 0.05\n        new_position = position + beta * (global_best - position) + delta\n        lb, ub = self.bounds\n        return np.clip(new_position, lb, ub)\n\n    def dynamic_inertia_adjustment(self):\n        diversity = np.std(self.positions, axis=0).mean()\n        max_diversity = np.sqrt(np.sum((self.bounds[1] - self.bounds[0])**2))\n        self.w = 0.4 + 0.5 * (diversity / max_diversity)\n        if len(self.history) > 1:\n            self.w += 0.1 * (self.history[-1] - self.history[-2]) / max(self.history)\n\n    def adaptive_velocity_clamping(self):\n        velocity_magnitudes = np.linalg.norm(self.velocities, axis=1)\n        mean_velocity_magnitude = np.mean(velocity_magnitudes)\n        self.velocities = np.clip(self.velocities, -self.velocity_clamp_factor * mean_velocity_magnitude, self.velocity_clamp_factor * mean_velocity_magnitude)\n\n    def evolutionary_constraint_handling(self, lb, ub):\n        for i in range(self.swarm_size):\n            if np.random.rand() < self.evolutionary_pressure:\n                perturbation = np.random.normal(0, 1, self.dim)\n                self.positions[i] = np.clip(self.positions[i] + perturbation, lb, ub)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_swarm(lb, ub)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            for i in range(self.swarm_size):\n                if evaluations >= self.budget:\n                    break\n\n                current_value = func(self.positions[i])\n                evaluations += 1\n\n                if current_value < self.personal_best_values[i]:\n                    self.personal_best_values[i] = current_value\n                    self.personal_best_positions[i] = self.positions[i].copy()\n\n                if current_value < self.global_best_value:\n                    self.global_best_value = current_value\n                    self.global_best_position = self.positions[i].copy()\n\n            self.history.append(self.global_best_value)\n            self.dynamic_inertia_adjustment()\n            self.adaptive_velocity_clamping()\n            self.evolutionary_constraint_handling(lb, ub)\n\n            for i in range(self.swarm_size):\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                r3 = np.random.rand(self.dim)\n                cognitive_component = self.c1 * r1 * (self.personal_best_positions[i] - self.positions[i])\n                social_component = self.c2 * r2 * (self.global_best_position - self.positions[i])\n                cooperative_component = self.c3 * r3 * (np.mean(self.personal_best_positions, axis=0) - self.positions[i])\n                self.velocities[i] = self.w * self.velocities[i] + cognitive_component + social_component + cooperative_component\n                self.positions[i] += self.velocities[i]\n\n                if np.random.rand() < self.adapt_rate:\n                    self.positions[i] = self.quantum_position_update(self.positions[i], self.global_best_position)\n\n        return self.global_best_position, self.global_best_value", "name": "EQA_PSO", "description": "Enhanced Quantum-Inspired Adaptive PSO (EQA-PSO) incorporates adaptive velocity clamping and an evolutionary constraint handling mechanism to improve exploration and exploitation balance for robust global optimization.", "configspace": "", "generation": 97, "fitness": 0.27658078464317054, "feedback": "The algorithm EQA_PSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28 with standard deviation 0.00.", "error": "", "parent_id": "57c46d13-5c5a-44fb-9920-c23452b2d979", "metadata": {"aucs": [0.2772488153449073, 0.2745489481382485, 0.2779445904463559]}, "mutation_prompt": null}
{"id": "69dc1c15-7950-410e-9f78-2abc7764e71a", "solution": "import numpy as np\n\nclass HQAL_PSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = min(50, budget)\n        self.positions = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_values = None\n        self.global_best_position = None\n        self.global_best_value = np.inf\n        self.w = 0.9  # Initial inertia weight\n        self.c1 = 1.5  # Cognitive coefficient\n        self.c2 = 1.5  # Social coefficient\n        self.c3 = 1.0  # Cooperative coefficient\n        self.adapt_rate = 0.1\n        self.history = []  # Historical best values\n        self.mutation_rate = 0.05  # Mutation rate for exploration\n        self.memory_factor = 0.2  # Factor for memory reinforcement\n\n    def initialize_swarm(self, lb, ub):\n        self.positions = np.random.uniform(lb, ub, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.swarm_size, self.dim))\n        self.personal_best_positions = self.positions.copy()\n        self.personal_best_values = np.full(self.swarm_size, np.inf)\n        self.bounds = (lb, ub)\n\n    def quantum_position_update(self, position, global_best):\n        beta = np.random.normal(0, 1, self.dim)\n        delta = np.random.normal(0, 1, self.dim) * self.mutation_rate\n        new_position = position + beta * (global_best - position) + delta\n        lb, ub = self.bounds\n        return np.clip(new_position, lb, ub)\n\n    def dynamic_inertia_adjustment(self):\n        diversity = np.std(self.positions, axis=0).mean()\n        max_diversity = np.sqrt(np.sum((self.bounds[1] - self.bounds[0])**2))\n        self.w = 0.4 + 0.5 * (diversity / max_diversity)\n        if len(self.history) > 1:\n            self.w += 0.1 * (self.history[-1] - self.history[-2]) / max(self.history)\n\n    def elite_learning(self, lb, ub):\n        elite_count = int(0.1 * self.swarm_size)\n        elite_indices = np.argsort(self.personal_best_values)[:elite_count]\n        for idx in elite_indices:\n            neighborhood_radius = np.exp(-self.dim / self.swarm_size)\n            perturbation = np.random.uniform(-neighborhood_radius, neighborhood_radius, self.dim)\n            self.personal_best_positions[idx] = np.clip(self.personal_best_positions[idx] + perturbation, lb, ub)\n\n    def adaptive_memory_reinforcement(self):\n        if len(self.history) > 2:\n            memory_effect = np.mean(self.history[-3:]) * self.memory_factor\n            self.global_best_value -= memory_effect\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_swarm(lb, ub)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            for i in range(self.swarm_size):\n                if evaluations >= self.budget:\n                    break\n\n                current_value = func(self.positions[i])\n                evaluations += 1\n\n                if current_value < self.personal_best_values[i]:\n                    self.personal_best_values[i] = current_value\n                    self.personal_best_positions[i] = self.positions[i].copy()\n\n                if current_value < self.global_best_value:\n                    self.global_best_value = current_value\n                    self.global_best_position = self.positions[i].copy()\n\n            self.history.append(self.global_best_value)\n            self.dynamic_inertia_adjustment()\n            self.elite_learning(lb, ub)\n            self.adaptive_memory_reinforcement()\n\n            for i in range(self.swarm_size):\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                r3 = np.random.rand(self.dim)\n                cognitive_component = self.c1 * r1 * (self.personal_best_positions[i] - self.positions[i])\n                social_component = self.c2 * r2 * (self.global_best_position - self.positions[i])\n                cooperative_component = self.c3 * r3 * (np.mean(self.personal_best_positions, axis=0) - self.positions[i])\n                self.velocities[i] = self.w * self.velocities[i] + cognitive_component + social_component + cooperative_component\n                self.positions[i] += self.velocities[i]\n\n                if np.random.rand() < self.adapt_rate:\n                    self.positions[i] = self.quantum_position_update(self.positions[i], self.global_best_position)\n\n        return self.global_best_position, self.global_best_value", "name": "HQAL_PSO", "description": "Hybrid Quantum-Inspired Adaptive Learning PSO (HQAL-PSO) integrates multi-swarm interaction with adaptive memory reinforcement and quantum-inspired exploration to enhance convergence speed and solution accuracy.", "configspace": "", "generation": 98, "fitness": 0.2750338179806259, "feedback": "The algorithm HQAL_PSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28 with standard deviation 0.00.", "error": "", "parent_id": "57c46d13-5c5a-44fb-9920-c23452b2d979", "metadata": {"aucs": [0.27573437194582484, 0.274387350863164, 0.27497973113288887]}, "mutation_prompt": null}
{"id": "74854539-55c5-41db-b2c5-ce0e97f44a38", "solution": "import numpy as np\n\nclass EQIDL_PSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = min(50, budget)\n        self.positions = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_values = None\n        self.global_best_position = None\n        self.global_best_value = np.inf\n        self.w = 0.9  # Initial inertia weight\n        self.c1 = 1.5  # Cognitive coefficient\n        self.c2 = 1.5  # Social coefficient\n        self.c3 = 1.0  # Cooperative coefficient\n        self.adapt_rate = 0.1\n        self.history = []  # Store historical best values\n        self.learning_rate = 0.5  # Initial learning rate\n\n    def initialize_swarm(self, lb, ub):\n        self.positions = np.random.uniform(lb, ub, (self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.swarm_size, self.dim))\n        self.personal_best_positions = self.positions.copy()\n        self.personal_best_values = np.full(self.swarm_size, np.inf)\n        self.bounds = (lb, ub)\n\n    def quantum_position_update(self, position, global_best):\n        beta = np.random.normal(0, 1, self.dim)\n        delta = np.random.normal(0, 1, self.dim) * 0.05  # Exploration boost factor\n        new_position = position + beta * (global_best - position) + delta\n        lb, ub = self.bounds\n        return np.clip(new_position, lb, ub)\n\n    def adaptive_dynamic_inertia(self):\n        diversity = np.std(self.positions, axis=0).mean()\n        max_diversity = np.sqrt(np.sum((self.bounds[1] - self.bounds[0])**2))\n        self.w = 0.4 + 0.5 * (diversity / max_diversity)\n        if len(self.history) > 1:\n            self.w += 0.1 * (self.history[-1] - self.history[-2]) / max(self.history)\n        self.learning_rate = min(0.8, max(0.2, diversity / max_diversity))  # Adjust learning rate\n\n    def elite_learning(self, lb, ub):\n        elite_count = int(0.1 * self.swarm_size)\n        elite_indices = np.argsort(self.personal_best_values)[:elite_count]\n        for idx in elite_indices:\n            neighborhood_radius = np.exp(-self.dim / self.swarm_size)\n            perturbation = np.random.uniform(-neighborhood_radius, neighborhood_radius, self.dim)\n            self.personal_best_positions[idx] = np.clip(self.personal_best_positions[idx] + perturbation, lb, ub)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_swarm(lb, ub)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            for i in range(self.swarm_size):\n                if evaluations >= self.budget:\n                    break\n\n                current_value = func(self.positions[i])\n                evaluations += 1\n\n                if current_value < self.personal_best_values[i]:\n                    self.personal_best_values[i] = current_value\n                    self.personal_best_positions[i] = self.positions[i].copy()\n\n                if current_value < self.global_best_value:\n                    self.global_best_value = current_value\n                    self.global_best_position = self.positions[i].copy()\n\n            self.history.append(self.global_best_value)\n            self.adaptive_dynamic_inertia()\n            self.elite_learning(lb, ub)\n\n            for i in range(self.swarm_size):\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                r3 = np.random.rand(self.dim)\n                cognitive_component = self.c1 * r1 * (self.personal_best_positions[i] - self.positions[i])\n                social_component = self.c2 * r2 * (self.global_best_position - self.positions[i])\n                cooperative_component = self.c3 * r3 * (np.mean(self.personal_best_positions, axis=0) - self.positions[i])\n                self.velocities[i] = self.w * self.velocities[i] + cognitive_component + social_component + cooperative_component\n                self.positions[i] += self.learning_rate * self.velocities[i]\n\n                if np.random.rand() < self.adapt_rate:\n                    self.positions[i] = self.quantum_position_update(self.positions[i], self.global_best_position)\n\n        return self.global_best_position, self.global_best_value", "name": "EQIDL_PSO", "description": "Enhanced Quantum-Inspired Dynamic Learning PSO (EQIDL-PSO) integrates adaptive learning rates and an exploration-exploitation balancing mechanism to improve convergence speed and accuracy.", "configspace": "", "generation": 99, "fitness": 0.27746868280474474, "feedback": "The algorithm EQIDL_PSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28 with standard deviation 0.00.", "error": "", "parent_id": "57c46d13-5c5a-44fb-9920-c23452b2d979", "metadata": {"aucs": [0.2776727614937742, 0.2779295109160027, 0.27680377600445727]}, "mutation_prompt": null}
