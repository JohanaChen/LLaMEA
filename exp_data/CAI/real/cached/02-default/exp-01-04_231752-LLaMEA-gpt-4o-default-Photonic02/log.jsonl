{"id": "f707e8ee-8bd5-47c8-8af1-07dfd8f8d9a2", "solution": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20 + self.dim\n        self.mutation_factor = 0.8\n        self.crossover_rate = 0.9\n        self.population = None\n        self.scores = None\n\n    def _initialize_population(self, lb, ub):\n        self.population = np.random.rand(self.population_size, self.dim) * (ub - lb) + lb\n        self.scores = np.full(self.population_size, np.inf)\n\n    def _select_parents(self, idx):\n        indices = list(range(self.population_size))\n        indices.remove(idx)\n        return np.random.choice(indices, 3, replace=False)\n\n    def _mutate(self, idx):\n        a, b, c = self._select_parents(idx)\n        mutant_vector = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        return np.clip(mutant_vector, self.lb, self.ub)\n\n    def _crossover(self, target_vector, mutant_vector):\n        crossover_mask = np.random.rand(self.dim) < self.crossover_rate\n        if not np.any(crossover_mask):\n            crossover_mask[np.random.randint(0, self.dim)] = True\n        trial_vector = np.where(crossover_mask, mutant_vector, target_vector)\n        return trial_vector\n\n    def __call__(self, func):\n        self.lb, self.ub = func.bounds.lb, func.bounds.ub\n        self._initialize_population(self.lb, self.ub)\n        \n        eval_count = 0\n        while eval_count < self.budget:\n            for i in range(self.population_size):\n                if eval_count >= self.budget:\n                    break\n                \n                target_vector = self.population[i]\n                mutant_vector = self._mutate(i)\n                trial_vector = self._crossover(target_vector, mutant_vector)\n                \n                trial_score = func(trial_vector)\n                eval_count += 1\n\n                if trial_score < self.scores[i]:\n                    self.population[i] = trial_vector\n                    self.scores[i] = trial_score\n\n        best_idx = np.argmin(self.scores)\n        return self.population[best_idx], self.scores[best_idx]", "name": "AdaptiveDifferentialEvolution", "description": "Adaptive Differential Evolution with Dynamic Strategy Selection optimizes photonic structures by evolving solutions through adaptive crossover, mutation, and selection processes within given function evaluation limits.", "configspace": "", "generation": 0, "fitness": 0.9124971991110242, "feedback": "The algorithm AdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.91 with standard deviation 0.00.", "error": "", "parent_id": null, "metadata": {"aucs": [0.9138499718764008, 0.9111444263456477]}, "mutation_prompt": null}
{"id": "9443ef3c-6400-4110-9cff-89ab22277ba5", "solution": "import numpy as np\n\nclass QuantumInspiredPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20 + self.dim\n        self.particles = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_scores = None\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.omega = 0.5  # Inertia weight\n        self.phi_p = 1.5  # Cognitive coefficient\n        self.phi_g = 1.5  # Social coefficient\n\n    def _initialize_population(self, lb, ub):\n        self.particles = np.random.rand(self.population_size, self.dim) * (ub - lb) + lb\n        self.velocities = np.random.rand(self.population_size, self.dim) * (ub - lb) / 10\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n\n    def _update_particles(self, lb, ub):\n        for i in range(self.population_size):\n            cognitive_component = self.phi_p * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.particles[i])\n            social_component = self.phi_g * np.random.rand(self.dim) * (self.global_best_position - self.particles[i])\n            self.velocities[i] = self.omega * self.velocities[i] + cognitive_component + social_component\n            self.particles[i] += self.velocities[i]\n            self.particles[i] = np.clip(self.particles[i], lb, ub)\n\n    def __call__(self, func):\n        self.lb, self.ub = func.bounds.lb, func.bounds.ub\n        self._initialize_population(self.lb, self.ub)\n\n        eval_count = 0\n        while eval_count < self.budget:\n            for i in range(self.population_size):\n                if eval_count >= self.budget:\n                    break\n\n                score = func(self.particles[i])\n                eval_count += 1\n\n                if score < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = score\n                    self.personal_best_positions[i] = self.particles[i]\n\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.particles[i]\n            \n            self._update_particles(self.lb, self.ub)\n\n        return self.global_best_position, self.global_best_score", "name": "QuantumInspiredPSO", "description": "Quantum-Inspired Particle Swarm Optimization leverages principles of quantum mechanics and classical PSO to explore and exploit the search space effectively within given function evaluation limits.", "configspace": "", "generation": 1, "fitness": 0.9138102766446541, "feedback": "The algorithm QuantumInspiredPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.91 with standard deviation 0.00.", "error": "", "parent_id": "f707e8ee-8bd5-47c8-8af1-07dfd8f8d9a2", "metadata": {"aucs": [0.9110548863952747, 0.9165656668940335]}, "mutation_prompt": null}
{"id": "26302af9-4a6d-4a6b-96b4-9230ea0c59b7", "solution": "import numpy as np\n\nclass HybridQuantumInspiredPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20 + self.dim\n        self.particles = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_scores = None\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.omega_start = 0.9\n        self.omega_end = 0.4\n        self.phi_p = 1.5\n        self.phi_g = 1.5\n        self.memory_probability = 0.1  # Probability to randomize memory\n\n    def _initialize_population(self, lb, ub):\n        self.particles = np.random.rand(self.population_size, self.dim) * (ub - lb) + lb\n        self.velocities = np.random.rand(self.population_size, self.dim) * (ub - lb) / 10\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n\n    def _update_particles(self, lb, ub, eval_count):\n        inertia_weight = self.omega_start - eval_count * (self.omega_start - self.omega_end) / self.budget\n        for i in range(self.population_size):\n            cognitive_component = self.phi_p * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.particles[i])\n            social_component = self.phi_g * np.random.rand(self.dim) * (self.global_best_position - self.particles[i])\n            self.velocities[i] = inertia_weight * self.velocities[i] + cognitive_component + social_component\n            self.particles[i] += self.velocities[i]\n            self.particles[i] = np.clip(self.particles[i], lb, ub)\n\n            # Randomize personal best for exploration\n            if np.random.rand() < self.memory_probability:\n                self.personal_best_positions[i] = np.random.rand(self.dim) * (ub - lb) + lb\n                self.personal_best_scores[i] = np.inf\n\n    def __call__(self, func):\n        self.lb, self.ub = func.bounds.lb, func.bounds.ub\n        self._initialize_population(self.lb, self.ub)\n\n        eval_count = 0\n        while eval_count < self.budget:\n            for i in range(self.population_size):\n                if eval_count >= self.budget:\n                    break\n\n                score = func(self.particles[i])\n                eval_count += 1\n\n                if score < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = score\n                    self.personal_best_positions[i] = self.particles[i]\n\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.particles[i]\n            \n            self._update_particles(self.lb, self.ub, eval_count)\n\n        return self.global_best_position, self.global_best_score", "name": "HybridQuantumInspiredPSO", "description": "Hybrid Quantum-Inspired PSO with Adaptive Inertia and Memory Randomization for Enhanced Exploration and Exploitation.", "configspace": "", "generation": 2, "fitness": 0.7151575002351274, "feedback": "The algorithm HybridQuantumInspiredPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.72 with standard deviation 0.12.", "error": "", "parent_id": "9443ef3c-6400-4110-9cff-89ab22277ba5", "metadata": {"aucs": [0.5962913243946033, 0.8340236760756515]}, "mutation_prompt": null}
{"id": "2bc3d10f-db18-4b5b-bcee-2c8d999ba32b", "solution": "import numpy as np\n\nclass EnhancedQuantumInspiredPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20 + self.dim\n        self.particles = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_scores = None\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.elite_archive = []\n        self.omega_start = 0.9  # Starting inertia weight\n        self.omega_end = 0.4    # Ending inertia weight\n        self.phi_p = 1.5  # Cognitive coefficient\n        self.phi_g = 1.5  # Social coefficient\n\n    def _initialize_population(self, lb, ub):\n        self.particles = np.random.rand(self.population_size, self.dim) * (ub - lb) + lb\n        self.velocities = np.random.rand(self.population_size, self.dim) * (ub - lb) / 10\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n        self.elite_archive = []\n\n    def _update_particles(self, eval_count, lb, ub):\n        omega = self.omega_start - (self.omega_start - self.omega_end) * (eval_count / self.budget)\n        for i in range(self.population_size):\n            cognitive_component = self.phi_p * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.particles[i])\n            social_component = self.phi_g * np.random.rand(self.dim) * (self.global_best_position - self.particles[i])\n            self.velocities[i] = omega * self.velocities[i] + cognitive_component + social_component\n            self.particles[i] += self.velocities[i]\n            self.particles[i] = np.clip(self.particles[i], lb, ub)\n\n    def _update_elite_archive(self, score, particle):\n        if len(self.elite_archive) < 5 or score < max(self.elite_archive, key=lambda x: x[0])[0]:\n            self.elite_archive.append((score, particle))\n            self.elite_archive.sort(key=lambda x: x[0])\n            if len(self.elite_archive) > 5:\n                self.elite_archive.pop()\n\n    def __call__(self, func):\n        self.lb, self.ub = func.bounds.lb, func.bounds.ub\n        self._initialize_population(self.lb, self.ub)\n\n        eval_count = 0\n        while eval_count < self.budget:\n            for i in range(self.population_size):\n                if eval_count >= self.budget:\n                    break\n\n                score = func(self.particles[i])\n                eval_count += 1\n\n                if score < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = score\n                    self.personal_best_positions[i] = self.particles[i]\n\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.particles[i]\n\n                self._update_elite_archive(score, self.particles[i])\n\n            self._update_particles(eval_count, self.lb, self.ub)\n\n        return self.global_best_position, self.global_best_score", "name": "EnhancedQuantumInspiredPSO", "description": "Enhanced Quantum-Inspired PSO incorporates adaptive control parameters and an elite archive to improve convergence and exploration-exploitation balance.", "configspace": "", "generation": 3, "fitness": 0.8900453124499871, "feedback": "The algorithm EnhancedQuantumInspiredPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.89 with standard deviation 0.00.", "error": "", "parent_id": "9443ef3c-6400-4110-9cff-89ab22277ba5", "metadata": {"aucs": [0.8920550977326249, 0.8880355271673492]}, "mutation_prompt": null}
{"id": "5c4ca048-7757-4076-9407-35b1289362f4", "solution": "import numpy as np\n\nclass QuantumFlockingOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20 + self.dim\n        self.particles = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_scores = None\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.omega = 0.5  # Inertia weight\n        self.phi_p = 1.5  # Cognitive coefficient\n        self.phi_g = 1.5  # Social coefficient\n        self.quantum_alpha = 0.2  # Quantum influence\n\n    def _initialize_population(self, lb, ub):\n        self.particles = np.random.rand(self.population_size, self.dim) * (ub - lb) + lb\n        self.velocities = np.random.rand(self.population_size, self.dim) * (ub - lb) / 10\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n\n    def _quantum_flocking(self, lb, ub):\n        for i in range(self.population_size):\n            quantum_step = self.quantum_alpha * (np.random.rand(self.dim) * (ub - lb) - (self.particles[i] - self.global_best_position))\n            self.particles[i] += quantum_step\n            self.particles[i] = np.clip(self.particles[i], lb, ub)\n\n    def _update_particles(self, lb, ub):\n        for i in range(self.population_size):\n            cognitive_component = self.phi_p * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.particles[i])\n            social_component = self.phi_g * np.random.rand(self.dim) * (self.global_best_position - self.particles[i])\n            self.velocities[i] = self.omega * self.velocities[i] + cognitive_component + social_component\n            self.particles[i] += self.velocities[i]\n            self.particles[i] = np.clip(self.particles[i], lb, ub)\n\n    def __call__(self, func):\n        self.lb, self.ub = func.bounds.lb, func.bounds.ub\n        self._initialize_population(self.lb, self.ub)\n\n        eval_count = 0\n        while eval_count < self.budget:\n            for i in range(self.population_size):\n                if eval_count >= self.budget:\n                    break\n\n                score = func(self.particles[i])\n                eval_count += 1\n\n                if score < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = score\n                    self.personal_best_positions[i] = self.particles[i]\n\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.particles[i]\n            \n            self._quantum_flocking(self.lb, self.ub)\n            self._update_particles(self.lb, self.ub)\n\n        return self.global_best_position, self.global_best_score", "name": "QuantumFlockingOptimization", "description": "Quantum Flocking Optimization combines quantum superposition with flocking behavior to enhance global exploration and local exploitation in complex search spaces.", "configspace": "", "generation": 4, "fitness": 0.7702215855593071, "feedback": "The algorithm QuantumFlockingOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.77 with standard deviation 0.01.", "error": "", "parent_id": "9443ef3c-6400-4110-9cff-89ab22277ba5", "metadata": {"aucs": [0.7612932460772728, 0.7791499250413413]}, "mutation_prompt": null}
{"id": "a965a770-c248-4c50-a7a6-94146bb27869", "solution": "import numpy as np\n\nclass AdaptiveQuantumInspiredPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20 + self.dim\n        self.particles = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_scores = None\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.omega = 0.9  # Initial inertia weight\n        self.omega_min = 0.4  # Minimum inertia weight\n        self.omega_decrement = (self.omega - self.omega_min) / self.budget\n        self.phi_p = 1.0  # Initial cognitive coefficient\n        self.phi_g = 2.0  # Initial social coefficient\n        self.max_phi = 2.5  # Maximum value for cognitive and social coefficients\n\n    def _initialize_population(self, lb, ub):\n        self.particles = np.random.rand(self.population_size, self.dim) * (ub - lb) + lb\n        self.velocities = np.random.rand(self.population_size, self.dim) * (ub - lb) / 10\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n\n    def _update_coefficients(self):\n        # Dynamically adjust omega, phi_p, and phi_g based on progress\n        self.omega = max(self.omega_min, self.omega - self.omega_decrement)\n        progress = 1 - (self.global_best_score / (1e-10 + np.mean(self.personal_best_scores)))\n        self.phi_p = 1.0 + progress * (self.max_phi - 1.0)\n        self.phi_g = 2.0 + progress * (self.max_phi - 2.0)\n\n    def _update_particles(self, lb, ub):\n        self._update_coefficients()\n        for i in range(self.population_size):\n            cognitive_component = self.phi_p * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.particles[i])\n            social_component = self.phi_g * np.random.rand(self.dim) * (self.global_best_position - self.particles[i])\n            self.velocities[i] = self.omega * self.velocities[i] + cognitive_component + social_component\n            self.particles[i] += self.velocities[i]\n            self.particles[i] = np.clip(self.particles[i], lb, ub)\n\n    def __call__(self, func):\n        self.lb, self.ub = func.bounds.lb, func.bounds.ub\n        self._initialize_population(self.lb, self.ub)\n\n        eval_count = 0\n        while eval_count < self.budget:\n            for i in range(self.population_size):\n                if eval_count >= self.budget:\n                    break\n\n                score = func(self.particles[i])\n                eval_count += 1\n\n                if score < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = score\n                    self.personal_best_positions[i] = self.particles[i]\n\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.particles[i]\n            \n            self._update_particles(self.lb, self.ub)\n\n        return self.global_best_position, self.global_best_score", "name": "AdaptiveQuantumInspiredPSO", "description": "Adaptive Quantum-Inspired PSO integrates dynamic adjustment of inertia weight and acceleration coefficients to enhance exploration and exploitation balance for improved convergence.", "configspace": "", "generation": 5, "fitness": 0.5949098252903592, "feedback": "The algorithm AdaptiveQuantumInspiredPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.59 with standard deviation 0.00.", "error": "", "parent_id": "9443ef3c-6400-4110-9cff-89ab22277ba5", "metadata": {"aucs": [0.5996287877807559, 0.5901908627999625]}, "mutation_prompt": null}
{"id": "f27c9e0c-af64-452e-8d9a-d9cc5f74e619", "solution": "import numpy as np\n\nclass EnhancedQuantumInspiredPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20 + self.dim\n        self.particles = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_scores = None\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.omega = 0.9  # Initial inertia weight\n        self.phi_p = 2.0  # Cognitive coefficient\n        self.phi_g = 2.0  # Social coefficient\n        self.omega_min = 0.4  # Minimum inertia weight\n        self.local_search_probability = 0.1\n\n    def _initialize_population(self, lb, ub):\n        self.particles = np.random.rand(self.population_size, self.dim) * (ub - lb) + lb\n        self.velocities = np.random.rand(self.population_size, self.dim) * (ub - lb) / 10\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n\n    def _adaptive_parameters(self, eval_count):\n        self.omega = self.omega_min + (0.9 - self.omega_min) * (1 - eval_count / self.budget)\n\n    def _local_search(self, particle, lb, ub):\n        perturbation = np.random.randn(self.dim) * 0.01 * (ub - lb)\n        new_particle = np.clip(particle + perturbation, lb, ub)\n        return new_particle\n\n    def _update_particles(self, lb, ub):\n        for i in range(self.population_size):\n            cognitive_component = self.phi_p * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.particles[i])\n            social_component = self.phi_g * np.random.rand(self.dim) * (self.global_best_position - self.particles[i])\n            self.velocities[i] = self.omega * self.velocities[i] + cognitive_component + social_component\n            self.particles[i] += self.velocities[i]\n            self.particles[i] = np.clip(self.particles[i], lb, ub)\n            if np.random.rand() < self.local_search_probability:\n                self.particles[i] = self._local_search(self.particles[i], lb, ub)\n\n    def __call__(self, func):\n        self.lb, self.ub = func.bounds.lb, func.bounds.ub\n        self._initialize_population(self.lb, self.ub)\n\n        eval_count = 0\n        while eval_count < self.budget:\n            self._adaptive_parameters(eval_count)\n            for i in range(self.population_size):\n                if eval_count >= self.budget:\n                    break\n\n                score = func(self.particles[i])\n                eval_count += 1\n\n                if score < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = score\n                    self.personal_best_positions[i] = self.particles[i]\n\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.particles[i]\n            \n            self._update_particles(self.lb, self.ub)\n\n        return self.global_best_position, self.global_best_score", "name": "EnhancedQuantumInspiredPSO", "description": "Enhanced Quantum-Inspired Particle Swarm Optimization integrates adaptive parameter tuning and local search for improved convergence in optimizing photonic structures.", "configspace": "", "generation": 6, "fitness": 0.8478041767759211, "feedback": "The algorithm EnhancedQuantumInspiredPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.85 with standard deviation 0.02.", "error": "", "parent_id": "9443ef3c-6400-4110-9cff-89ab22277ba5", "metadata": {"aucs": [0.8256780643354674, 0.8699302892163748]}, "mutation_prompt": null}
{"id": "72fdca8f-950e-4d88-a821-27ff34612cec", "solution": "import numpy as np\n\nclass AdaptiveQuantumInspiredPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20 + self.dim\n        self.particles = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_scores = None\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.omega_max = 0.9\n        self.omega_min = 0.4\n        self.phi_p = 2.0  # Cognitive coefficient\n        self.phi_g = 2.0  # Social coefficient\n        self.iteration = 0\n\n    def _initialize_population(self, lb, ub):\n        self.particles = np.random.rand(self.population_size, self.dim) * (ub - lb) + lb\n        self.velocities = np.random.rand(self.population_size, self.dim) * (ub - lb) / 10\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n\n    def _update_parameters(self):\n        # Linearly decrease omega from omega_max to omega_min\n        self.omega = self.omega_max - (self.omega_max - self.omega_min) * (self.iteration / self._max_iter)\n\n    def _update_particles(self, lb, ub):\n        for i in range(self.population_size):\n            cognitive_component = self.phi_p * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.particles[i])\n            social_component = self.phi_g * np.random.rand(self.dim) * (self.global_best_position - self.particles[i])\n            self.velocities[i] = self.omega * self.velocities[i] + cognitive_component + social_component\n            self.particles[i] += self.velocities[i]\n            self.particles[i] = np.clip(self.particles[i], lb, ub)\n\n    def __call__(self, func):\n        self.lb, self.ub = func.bounds.lb, func.bounds.ub\n        self._initialize_population(self.lb, self.ub)\n        self._max_iter = self.budget // self.population_size\n        eval_count = 0\n\n        while eval_count < self.budget:\n            self._update_parameters()\n            for i in range(self.population_size):\n                if eval_count >= self.budget:\n                    break\n\n                score = func(self.particles[i])\n                eval_count += 1\n\n                if score < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = score\n                    self.personal_best_positions[i] = self.particles[i]\n\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.particles[i]\n            \n            self._update_particles(self.lb, self.ub)\n            self.iteration += 1\n\n        return self.global_best_position, self.global_best_score", "name": "AdaptiveQuantumInspiredPSO", "description": "Adaptive Quantum-Inspired PSO incorporates dynamically adjusted parameters for inertia and coefficients to enhance exploration and exploitation in photonic structure optimization.", "configspace": "", "generation": 7, "fitness": 0.771481739983825, "feedback": "The algorithm AdaptiveQuantumInspiredPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.77 with standard deviation 0.05.", "error": "", "parent_id": "9443ef3c-6400-4110-9cff-89ab22277ba5", "metadata": {"aucs": [0.8218490429613943, 0.7211144370062557]}, "mutation_prompt": null}
{"id": "1e5ad416-9f7e-426c-873d-df7e332a4ffb", "solution": "import numpy as np\n\nclass AdaptiveQuantumDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20 + self.dim\n        self.population = None\n        self.scores = None\n        self.best_vector = None\n        self.best_score = np.inf\n        self.F = 0.5  # Mutation factor\n        self.CR = 0.9  # Crossover probability\n\n    def _initialize_population(self, lb, ub):\n        self.population = np.random.rand(self.population_size, self.dim) * (ub - lb) + lb\n        self.scores = np.full(self.population_size, np.inf)\n\n    def _mutate(self, lb, ub, idx):\n        indices = [i for i in range(self.population_size) if i != idx]\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        mutant_vector = self.population[a] + self.F * (self.population[b] - self.population[c])\n        \n        # Quantum-inspired mutation\n        quantum_factor = np.random.rand(self.dim) * (self.best_vector - self.population[idx])\n        mutant_vector += quantum_factor\n        \n        return np.clip(mutant_vector, lb, ub)\n\n    def _crossover(self, target, mutant):\n        crossover_vector = np.copy(target)\n        rand_idx = np.random.randint(self.dim)\n        for j in range(self.dim):\n            if np.random.rand() < self.CR or j == rand_idx:\n                crossover_vector[j] = mutant[j]\n        return crossover_vector\n\n    def __call__(self, func):\n        self.lb, self.ub = func.bounds.lb, func.bounds.ub\n        self._initialize_population(self.lb, self.ub)\n\n        eval_count = 0\n        while eval_count < self.budget:\n            for i in range(self.population_size):\n                if eval_count >= self.budget:\n                    break\n\n                target_vector = self.population[i]\n                mutant_vector = self._mutate(self.lb, self.ub, i)\n                trial_vector = self._crossover(target_vector, mutant_vector)\n\n                trial_score = func(trial_vector)\n                eval_count += 1\n\n                if trial_score < self.scores[i]:\n                    self.population[i] = trial_vector\n                    self.scores[i] = trial_score\n\n                if trial_score < self.best_score:\n                    self.best_score = trial_score\n                    self.best_vector = trial_vector\n\n        return self.best_vector, self.best_score", "name": "AdaptiveQuantumDifferentialEvolution", "description": "Adaptive Differential Evolution with Quantum-Inspired Mutation dynamically adjusts mutation strategies and incorporates quantum mechanics to efficiently explore and exploit the search space.", "configspace": "", "generation": 8, "fitness": -Infinity, "feedback": "An exception occurred: TypeError(\"unsupported operand type(s) for -: 'NoneType' and 'float'\").", "error": "TypeError(\"unsupported operand type(s) for -: 'NoneType' and 'float'\")", "parent_id": "9443ef3c-6400-4110-9cff-89ab22277ba5", "metadata": {}, "mutation_prompt": null}
{"id": "3047ecf0-6d3c-41c3-9919-616bea90ac5f", "solution": "import numpy as np\n\nclass AdaptiveQuantumInspiredPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20 + self.dim\n        self.particles = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_scores = None\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.omega = 0.5  # Initial inertia weight\n        self.omega_min = 0.1\n        self.omega_max = 0.9\n        self.phi_p = 1.5  # Initial cognitive coefficient\n        self.phi_g = 1.5  # Initial social coefficient\n        self.adaptive_rate = 0.1  # Rate of adaptation\n\n    def _initialize_population(self, lb, ub):\n        self.particles = np.random.rand(self.population_size, self.dim) * (ub - lb) + lb\n        self.velocities = np.random.rand(self.population_size, self.dim) * (ub - lb) / 10\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n\n    def _adaptive_parameters(self):\n        # Adapt inertia weight and coefficients based on the improvement ratio\n        improvement_ratio = (np.mean(self.personal_best_scores) - self.global_best_score) / np.mean(self.personal_best_scores)\n        self.omega = max(self.omega_min, self.omega_max * improvement_ratio)\n        if improvement_ratio > 0.5:\n            self.phi_p += self.adaptive_rate\n            self.phi_g -= self.adaptive_rate\n        else:\n            self.phi_p -= self.adaptive_rate\n            self.phi_g += self.adaptive_rate\n\n    def _update_particles(self, lb, ub):\n        for i in range(self.population_size):\n            cognitive_component = self.phi_p * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.particles[i])\n            social_component = self.phi_g * np.random.rand(self.dim) * (self.global_best_position - self.particles[i])\n            self.velocities[i] = self.omega * self.velocities[i] + cognitive_component + social_component\n            self.particles[i] += self.velocities[i]\n            self.particles[i] = np.clip(self.particles[i], lb, ub)\n\n    def __call__(self, func):\n        self.lb, self.ub = func.bounds.lb, func.bounds.ub\n        self._initialize_population(self.lb, self.ub)\n\n        eval_count = 0\n        while eval_count < self.budget:\n            for i in range(self.population_size):\n                if eval_count >= self.budget:\n                    break\n\n                score = func(self.particles[i])\n                eval_count += 1\n\n                if score < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = score\n                    self.personal_best_positions[i] = self.particles[i]\n\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.particles[i]\n            \n            self._adaptive_parameters()  # Adjust parameters based on feedback\n            self._update_particles(self.lb, self.ub)\n\n        return self.global_best_position, self.global_best_score", "name": "AdaptiveQuantumInspiredPSO", "description": "Adaptive Quantum-Inspired PSO dynamically adjusts particle influence and inertia based on fitness landscape feedback to enhance convergence speed and accuracy.", "configspace": "", "generation": 9, "fitness": 0.6360514607854858, "feedback": "The algorithm AdaptiveQuantumInspiredPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.64 with standard deviation 0.02.", "error": "", "parent_id": "9443ef3c-6400-4110-9cff-89ab22277ba5", "metadata": {"aucs": [0.6204032469399876, 0.651699674630984]}, "mutation_prompt": null}
{"id": "1cd44fe7-e603-4e96-80b0-c944ea245e6a", "solution": "import numpy as np\n\nclass QuantumDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20 + self.dim\n        self.mutation_factor = 0.8\n        self.crossover_probability = 0.9\n        self.population = None\n        self.best_solution = None\n        self.best_score = np.inf\n\n    def _initialize_population(self, lb, ub):\n        self.population = np.random.rand(self.population_size, self.dim) * (ub - lb) + lb\n\n    def _quantum_operator(self, target, trial):\n        return target + np.random.uniform(-0.1, 0.1, self.dim) * (trial - target)\n    \n    def _mutate(self, idx):\n        indices = list(range(self.population_size))\n        indices.remove(idx)\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        return self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n\n    def __call__(self, func):\n        self.lb, self.ub = func.bounds.lb, func.bounds.ub\n        self._initialize_population(self.lb, self.ub)\n\n        eval_count = 0\n        while eval_count < self.budget:\n            new_population = np.copy(self.population)\n            for i in range(self.population_size):\n                if eval_count >= self.budget:\n                    break\n\n                mutant = self._mutate(i)\n                trial = np.copy(self.population[i])\n                cross_points = np.random.rand(self.dim) < self.crossover_probability\n                trial[cross_points] = mutant[cross_points]\n                trial = np.clip(trial, self.lb, self.ub)\n\n                trial_quantum = self._quantum_operator(self.population[i], trial)\n                score = func(trial_quantum)\n                eval_count += 1\n\n                if score < self.best_score:\n                    self.best_score = score\n                    self.best_solution = trial_quantum\n\n                if score < func(self.population[i]):\n                    new_population[i] = trial_quantum\n\n            self.population = new_population\n\n        return self.best_solution, self.best_score", "name": "QuantumDifferentialEvolution", "description": "Quantum Differential Evolution (QDE) combines quantum-inspired principles with Differential Evolution to enhance exploration and exploitation capabilities while maintaining diversity.", "configspace": "", "generation": 10, "fitness": 0.7251316649511985, "feedback": "The algorithm QuantumDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.73 with standard deviation 0.05.", "error": "", "parent_id": "9443ef3c-6400-4110-9cff-89ab22277ba5", "metadata": {"aucs": [0.6702450682578434, 0.7800182616445537]}, "mutation_prompt": null}
{"id": "8a57f61d-ef9b-4827-8674-ab504f5b725a", "solution": "import numpy as np\n\nclass AdaptiveDifferentialEvolutionWithLearning:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20 + self.dim\n        self.mutation_factor = 0.5\n        self.crossover_probability = 0.7\n        self.history = []\n        self.population = None\n        self.best_solution = None\n        self.best_score = np.inf\n\n    def _initialize_population(self, lb, ub):\n        self.population = np.random.rand(self.population_size, self.dim) * (ub - lb) + lb\n\n    def _mutate(self, i, lb, ub):\n        indices = [idx for idx in range(self.population_size) if idx != i]\n        a, b, c = self.population[np.random.choice(indices, 3, replace=False)]\n        mutant = np.clip(a + self.mutation_factor * (b - c), lb, ub)\n        return mutant\n\n    def _crossover(self, target, mutant):\n        crossover_mask = np.random.rand(self.dim) < self.crossover_probability\n        offspring = np.where(crossover_mask, mutant, target)\n        return offspring\n\n    def _select(self, target_idx, offspring, func):\n        target = self.population[target_idx]\n        target_score = func(target)\n        offspring_score = func(offspring)\n\n        if offspring_score < target_score:\n            self.population[target_idx] = offspring\n            return offspring_score, True\n        return target_score, False\n\n    def _adapt_parameters(self):\n        success_ratio = sum(self.history[-self.population_size:]) / self.population_size\n        self.mutation_factor = 0.5 + 0.3 * success_ratio\n        self.crossover_probability = 0.7 + 0.2 * (1 - success_ratio)\n\n    def __call__(self, func):\n        self.lb, self.ub = func.bounds.lb, func.bounds.ub\n        self._initialize_population(self.lb, self.ub)\n\n        eval_count = 0\n        while eval_count < self.budget:\n            success_count = 0\n            for i in range(self.population_size):\n                if eval_count >= self.budget:\n                    break\n\n                mutant = self._mutate(i, self.lb, self.ub)\n                offspring = self._crossover(self.population[i], mutant)\n                score, success = self._select(i, offspring, func)\n\n                if score < self.best_score:\n                    self.best_score = score\n                    self.best_solution = np.copy(self.population[i])\n\n                eval_count += 1\n                success_count += success\n            \n            self.history.append(success_count)\n            self._adapt_parameters()\n\n        return self.best_solution, self.best_score", "name": "AdaptiveDifferentialEvolutionWithLearning", "description": "Adaptive Differential Evolution with Learning (ADEL) dynamically tunes mutation and crossover rates based on historical success to efficiently explore and exploit complex search spaces under evaluation constraints.", "configspace": "", "generation": 11, "fitness": 0.6914314002588087, "feedback": "The algorithm AdaptiveDifferentialEvolutionWithLearning got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.69 with standard deviation 0.00.", "error": "", "parent_id": "9443ef3c-6400-4110-9cff-89ab22277ba5", "metadata": {"aucs": [0.6897964462699521, 0.6930663542476652]}, "mutation_prompt": null}
{"id": "efa0984c-76fc-4800-b1a8-4af751e43e5d", "solution": "import numpy as np\n\nclass QuantumTunnelingEnhancedPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20 + self.dim\n        self.particles = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_scores = None\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.omega = 0.5  # Inertia weight\n        self.phi_p = 1.5  # Cognitive coefficient\n        self.phi_g = 1.5  # Social coefficient\n        self.tunneling_prob = 0.1  # Probability of quantum tunneling\n\n    def _initialize_population(self, lb, ub):\n        self.particles = np.random.rand(self.population_size, self.dim) * (ub - lb) + lb\n        self.velocities = np.random.rand(self.population_size, self.dim) * (ub - lb) / 10\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n\n    def _update_particles(self, lb, ub):\n        for i in range(self.population_size):\n            cognitive_component = self.phi_p * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.particles[i])\n            social_component = self.phi_g * np.random.rand(self.dim) * (self.global_best_position - self.particles[i])\n            self.velocities[i] = self.omega * self.velocities[i] + cognitive_component + social_component\n\n            if np.random.rand() < self.tunneling_prob:\n                # Quantum tunneling: randomize particle position with a bias towards the global best\n                self.particles[i] = self.global_best_position + np.random.normal(0, 0.1, self.dim) * (ub - lb)\n            else:\n                self.particles[i] += self.velocities[i]\n\n            self.particles[i] = np.clip(self.particles[i], lb, ub)\n\n    def __call__(self, func):\n        self.lb, self.ub = func.bounds.lb, func.bounds.ub\n        self._initialize_population(self.lb, self.ub)\n\n        eval_count = 0\n        while eval_count < self.budget:\n            for i in range(self.population_size):\n                if eval_count >= self.budget:\n                    break\n\n                score = func(self.particles[i])\n                eval_count += 1\n\n                if score < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = score\n                    self.personal_best_positions[i] = self.particles[i]\n\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.particles[i]\n            \n            self._update_particles(self.lb, self.ub)\n\n        return self.global_best_position, self.global_best_score", "name": "QuantumTunnelingEnhancedPSO", "description": "Quantum-Tunneling Enhanced PSO introduces a quantum-tunneling mechanism to escape local optima and improve global search capabilities, adaptable to varying dimensionality.", "configspace": "", "generation": 12, "fitness": 0.9555605691976563, "feedback": "The algorithm QuantumTunnelingEnhancedPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.96 with standard deviation 0.01.", "error": "", "parent_id": "9443ef3c-6400-4110-9cff-89ab22277ba5", "metadata": {"aucs": [0.9491848505569763, 0.9619362878383364]}, "mutation_prompt": null}
{"id": "c0bdfba9-0393-42c0-85e0-23bac0e48b9d", "solution": "import numpy as np\n\nclass AdaptiveQuantumTunnelingPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20 + dim\n        self.particles = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_scores = None\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.omega = 0.9  # Start with a higher inertia weight\n        self.omega_min = 0.4  # Minimum inertia weight\n        self.phi_p = 1.5  # Cognitive coefficient\n        self.phi_g = 1.5  # Social coefficient\n        self.tunneling_prob = 0.1  # Probability of quantum tunneling\n\n    def _initialize_population(self, lb, ub):\n        self.particles = np.random.rand(self.population_size, self.dim) * (ub - lb) + lb\n        self.velocities = np.random.rand(self.population_size, self.dim) * (ub - lb) / 10\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n\n    def _adapt_inertia_weight(self, eval_count):\n        self.omega = self.omega_min + (0.9 - self.omega_min) * (1 - eval_count / self.budget)\n\n    def _update_particles(self, lb, ub):\n        for i in range(self.population_size):\n            cognitive_component = self.phi_p * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.particles[i])\n            social_component = self.phi_g * np.random.rand(self.dim) * (self.global_best_position - self.particles[i])\n            self.velocities[i] = self.omega * self.velocities[i] + cognitive_component + social_component\n\n            if np.random.rand() < self.tunneling_prob:\n                self.particles[i] = self.global_best_position + np.random.normal(0, 0.1, self.dim) * (ub - lb)\n            else:\n                self.particles[i] += self.velocities[i]\n\n            self.particles[i] = np.clip(self.particles[i], lb, ub)\n\n    def __call__(self, func):\n        self.lb, self.ub = func.bounds.lb, func.bounds.ub\n        self._initialize_population(self.lb, self.ub)\n\n        eval_count = 0\n        while eval_count < self.budget:\n            for i in range(self.population_size):\n                if eval_count >= self.budget:\n                    break\n\n                score = func(self.particles[i])\n                eval_count += 1\n\n                if score < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = score\n                    self.personal_best_positions[i] = self.particles[i]\n\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.particles[i]\n\n            self._adapt_inertia_weight(eval_count)\n            self._update_particles(self.lb, self.ub)\n\n        return self.global_best_position, self.global_best_score", "name": "AdaptiveQuantumTunnelingPSO", "description": "Adaptive Quantum-Tunneling PSO introduces an adaptive inertia mechanism to balance exploration and exploitation dynamically, enhancing convergence speed and accuracy in high-dimensional spaces.", "configspace": "", "generation": 13, "fitness": 0.7498500868783278, "feedback": "The algorithm AdaptiveQuantumTunnelingPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.75 with standard deviation 0.01.", "error": "", "parent_id": "efa0984c-76fc-4800-b1a8-4af751e43e5d", "metadata": {"aucs": [0.7398707929864801, 0.7598293807701756]}, "mutation_prompt": null}
{"id": "710fe4f1-07ac-45fd-9e1a-f0cde52fccf4", "solution": "import numpy as np\n\nclass QuantumEntanglementPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20 + self.dim\n        self.particles = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_scores = None\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.omega = 0.5  # Inertia weight\n        self.phi_p = 1.5  # Cognitive coefficient\n        self.phi_g = 1.5  # Social coefficient\n        self.tunneling_prob = 0.1  # Initial probability of quantum tunneling\n        self.adaptive_factor = 0.1  # Adaptive factor for tunneling probability\n\n    def _initialize_population(self, lb, ub):\n        self.particles = np.random.rand(self.population_size, self.dim) * (ub - lb) + lb\n        self.velocities = np.random.rand(self.population_size, self.dim) * (ub - lb) / 10\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n\n    def _update_particles(self, lb, ub):\n        for i in range(self.population_size):\n            cognitive_component = self.phi_p * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.particles[i])\n            social_component = self.phi_g * np.random.rand(self.dim) * (self.global_best_position - self.particles[i])\n            self.velocities[i] = self.omega * self.velocities[i] + cognitive_component + social_component\n\n            if np.random.rand() < self.tunneling_prob:\n                # Quantum tunneling: randomize particle position with a bias towards the global best\n                self.particles[i] = self.global_best_position + np.random.normal(0, 0.1, self.dim) * (ub - lb)\n            else:\n                self.particles[i] += self.velocities[i]\n\n            # Entangle particles to share global best information\n            if np.random.rand() < 0.05:  # Small probability to entangle\n                entangled_particle = np.random.choice(self.population_size)\n                self.particles[i] = (self.particles[i] + self.particles[entangled_particle]) / 2\n\n            self.particles[i] = np.clip(self.particles[i], lb, ub)\n\n    def __call__(self, func):\n        self.lb, self.ub = func.bounds.lb, func.bounds.ub\n        self._initialize_population(self.lb, self.ub)\n\n        eval_count = 0\n        prev_global_best_score = np.inf\n        while eval_count < self.budget:\n            for i in range(self.population_size):\n                if eval_count >= self.budget:\n                    break\n\n                score = func(self.particles[i])\n                eval_count += 1\n\n                if score < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = score\n                    self.personal_best_positions[i] = self.particles[i]\n\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.particles[i]\n\n            # Adaptive tunneling adjustment\n            if self.global_best_score < prev_global_best_score:\n                self.tunneling_prob = max(0.01, self.tunneling_prob - self.adaptive_factor)\n            else:\n                self.tunneling_prob = min(0.3, self.tunneling_prob + self.adaptive_factor)\n            prev_global_best_score = self.global_best_score\n\n            self._update_particles(self.lb, self.ub)\n\n        return self.global_best_position, self.global_best_score", "name": "QuantumEntanglementPSO", "description": "Quantum Entanglement PSO introduces entangled particles to share information and enhance convergence speed, with adaptive tunneling probability for escaping local optima.", "configspace": "", "generation": 14, "fitness": 0.7423564630366206, "feedback": "The algorithm QuantumEntanglementPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.74 with standard deviation 0.02.", "error": "", "parent_id": "efa0984c-76fc-4800-b1a8-4af751e43e5d", "metadata": {"aucs": [0.7205337078933033, 0.7641792181799381]}, "mutation_prompt": null}
{"id": "86c95907-0c3f-4988-a459-f577ef90b547", "solution": "import numpy as np\n\nclass AdaptiveGenomeRecombination:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20 + self.dim\n        self.population = None\n        self.scores = None\n        self.best_solution = None\n        self.best_score = np.inf\n        self.mutation_rate = 0.1\n    \n    def _initialize_population(self, lb, ub):\n        self.population = np.random.rand(self.population_size, self.dim) * (ub - lb) + lb\n        self.scores = np.full(self.population_size, np.inf)\n    \n    def _evaluate(self, func):\n        for i in range(self.population_size):\n            score = func(self.population[i])\n            if score < self.scores[i]:\n                self.scores[i] = score\n            if score < self.best_score:\n                self.best_score = score\n                self.best_solution = self.population[i]\n    \n    def _select_parents(self):\n        idx = np.argsort(self.scores)\n        return self.population[idx[:self.population_size // 2]]\n    \n    def _recombine(self, parents, lb, ub):\n        offspring = np.empty((self.population_size // 2, self.dim))\n        for i in range(self.population_size // 2):\n            p1, p2 = parents[np.random.choice(parents.shape[0], 2, replace=False)]\n            crossover_point = np.random.randint(1, self.dim)\n            offspring[i, :crossover_point] = p1[:crossover_point]\n            offspring[i, crossover_point:] = p2[crossover_point:]\n            if np.random.rand() < self.mutation_rate:\n                mutation = np.random.normal(0, 0.1, self.dim) * (ub - lb)\n                offspring[i] += mutation\n        offspring = np.clip(offspring, lb, ub)\n        return offspring\n    \n    def _update_mutation_rate(self):\n        self.mutation_rate = np.random.uniform(0.05, 0.2)\n    \n    def __call__(self, func):\n        self.lb, self.ub = func.bounds.lb, func.bounds.ub\n        self._initialize_population(self.lb, self.ub)\n        \n        eval_count = 0\n        while eval_count < self.budget:\n            self._evaluate(func)\n            eval_count += self.population_size\n            \n            parents = self._select_parents()\n            offspring = self._recombine(parents, self.lb, self.ub)\n            \n            if eval_count < self.budget:\n                self.population[:self.population_size // 2] = parents\n                self.population[self.population_size // 2:] = offspring\n            \n            self._update_mutation_rate()\n        \n        return self.best_solution, self.best_score", "name": "AdaptiveGenomeRecombination", "description": "Adaptive Genome Recombination Algorithm leverages genetic recombination and adaptive mutation rates to explore complex search spaces, enhancing diversity and convergence.", "configspace": "", "generation": 15, "fitness": 0.5865753812035069, "feedback": "The algorithm AdaptiveGenomeRecombination got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.59 with standard deviation 0.02.", "error": "", "parent_id": "efa0984c-76fc-4800-b1a8-4af751e43e5d", "metadata": {"aucs": [0.6088460398226476, 0.5643047225843663]}, "mutation_prompt": null}
{"id": "0ad3219f-79bd-41e1-bd22-09adfe67901e", "solution": "import numpy as np\n\nclass AdaptiveQuantumTunnelingPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20 + self.dim\n        self.particles = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_scores = None\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.omega = 0.9  # Initial inertia weight\n        self.phi_p = 1.5  # Cognitive coefficient\n        self.phi_g = 1.5  # Social coefficient\n        self.tunneling_prob = 0.1  # Probability of quantum tunneling\n        self.omega_min = 0.4  # Minimum inertia weight\n\n    def _initialize_population(self, lb, ub):\n        self.particles = np.random.rand(self.population_size, self.dim) * (ub - lb) + lb\n        self.velocities = np.random.rand(self.population_size, self.dim) * (ub - lb) / 10\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n\n    def _update_inertia(self, eval_count):\n        self.omega = self.omega_min + (0.9 - self.omega_min) * (1 - eval_count / self.budget)\n\n    def _update_particles(self, lb, ub, eval_count):\n        self._update_inertia(eval_count)\n\n        for i in range(self.population_size):\n            cognitive_component = self.phi_p * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.particles[i])\n            social_component = self.phi_g * np.random.rand(self.dim) * (self.global_best_position - self.particles[i])\n            self.velocities[i] = self.omega * self.velocities[i] + cognitive_component + social_component\n\n            if np.random.rand() < self.tunneling_prob:\n                # Quantum tunneling: randomize particle position with a bias towards the global best\n                self.particles[i] = self.global_best_position + np.random.normal(0, 0.1, self.dim) * (ub - lb)\n            else:\n                self.particles[i] += self.velocities[i]\n\n            self.particles[i] = np.clip(self.particles[i], lb, ub)\n\n    def __call__(self, func):\n        self.lb, self.ub = func.bounds.lb, func.bounds.ub\n        self._initialize_population(self.lb, self.ub)\n\n        eval_count = 0\n        while eval_count < self.budget:\n            for i in range(self.population_size):\n                if eval_count >= self.budget:\n                    break\n\n                score = func(self.particles[i])\n                eval_count += 1\n\n                if score < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = score\n                    self.personal_best_positions[i] = self.particles[i]\n\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.particles[i]\n            \n            self._update_particles(self.lb, self.ub, eval_count)\n\n        return self.global_best_position, self.global_best_score", "name": "AdaptiveQuantumTunnelingPSO", "description": "Adaptive Quantum-Tunneling PSO with Dynamic Inertia adjusts inertia based on convergence rate to balance exploration and exploitation, enhancing global search in varying dimensions.", "configspace": "", "generation": 16, "fitness": 0.7825372896385467, "feedback": "The algorithm AdaptiveQuantumTunnelingPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.78 with standard deviation 0.03.", "error": "", "parent_id": "efa0984c-76fc-4800-b1a8-4af751e43e5d", "metadata": {"aucs": [0.7485196003661179, 0.8165549789109755]}, "mutation_prompt": null}
{"id": "21912aec-0b60-4547-89a0-d1dc4224d6da", "solution": "import numpy as np\n\nclass QuantumGradientHybridOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20 + self.dim\n        self.particles = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_scores = None\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.omega = 0.5  # Inertia weight\n        self.phi_p = 1.5  # Cognitive coefficient\n        self.phi_g = 1.5  # Social coefficient\n        self.tunneling_prob = 0.1  # Probability of quantum tunneling\n        self.gradient_step = 0.05  # Step size for gradient adjustment\n\n    def _initialize_population(self, lb, ub):\n        self.particles = np.random.rand(self.population_size, self.dim) * (ub - lb) + lb\n        self.velocities = np.random.rand(self.population_size, self.dim) * (ub - lb) / 10\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n\n    def _compute_gradient(self, particle, func, step_size):\n        gradient = np.zeros(self.dim)\n        for d in range(self.dim):\n            step = np.zeros(self.dim)\n            step[d] = step_size\n            f_val_plus = func(np.clip(particle + step, self.lb, self.ub))\n            f_val_minus = func(np.clip(particle - step, self.lb, self.ub))\n            gradient[d] = (f_val_plus - f_val_minus) / (2 * step_size)\n        return gradient\n\n    def _update_particles(self, lb, ub, func):\n        for i in range(self.population_size):\n            cognitive_component = self.phi_p * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.particles[i])\n            social_component = self.phi_g * np.random.rand(self.dim) * (self.global_best_position - self.particles[i])\n            self.velocities[i] = self.omega * self.velocities[i] + cognitive_component + social_component\n\n            if np.random.rand() < self.tunneling_prob:\n                self.particles[i] = self.global_best_position + np.random.normal(0, 0.1, self.dim) * (ub - lb)\n            else:\n                self.particles[i] += self.velocities[i] - self.gradient_step * self._compute_gradient(self.particles[i], func, self.gradient_step)\n\n            self.particles[i] = np.clip(self.particles[i], lb, ub)\n\n    def __call__(self, func):\n        self.lb, self.ub = func.bounds.lb, func.bounds.ub\n        self._initialize_population(self.lb, self.ub)\n\n        eval_count = 0\n        while eval_count < self.budget:\n            for i in range(self.population_size):\n                if eval_count >= self.budget:\n                    break\n\n                score = func(self.particles[i])\n                eval_count += 1\n\n                if score < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = score\n                    self.personal_best_positions[i] = self.particles[i]\n\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.particles[i]\n            \n            self._update_particles(self.lb, self.ub, func)\n\n        return self.global_best_position, self.global_best_score", "name": "QuantumGradientHybridOptimization", "description": "Quantum-Gradient Hybrid Optimization (QGHO) combines quantum-tunneling with a gradient-based adjustment step to enhance both global exploration and local exploitation in varying dimensionalities.", "configspace": "", "generation": 17, "fitness": 0.6956946234722687, "feedback": "The algorithm QuantumGradientHybridOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.70 with standard deviation 0.05.", "error": "", "parent_id": "efa0984c-76fc-4800-b1a8-4af751e43e5d", "metadata": {"aucs": [0.6492383542575146, 0.742150892687023]}, "mutation_prompt": null}
{"id": "e4005ec6-6b04-4978-a37f-78364bb1e8ff", "solution": "import numpy as np\n\nclass QuantumLevyFlightPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20 + self.dim\n        self.particles = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_scores = None\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.omega = 0.5  # Inertia weight\n        self.phi_p = 1.5  # Cognitive coefficient\n        self.phi_g = 1.5  # Social coefficient\n        self.tunneling_prob = 0.1  # Probability of quantum tunneling\n        self.levy_alpha = 1.5  # Levy flight exponent\n\n    def _initialize_population(self, lb, ub):\n        self.particles = np.random.rand(self.population_size, self.dim) * (ub - lb) + lb\n        self.velocities = np.random.rand(self.population_size, self.dim) * (ub - lb) / 10\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n\n    def _levy_flight(self):\n        u = np.random.normal(0, 1, self.dim) * (np.sqrt(np.abs(np.random.normal(0, 1)))) ** (-1/self.levy_alpha)\n        v = np.random.normal(0, 1, self.dim)\n        step = u / np.abs(v) ** (1/self.levy_alpha)\n        return step\n\n    def _update_particles(self, lb, ub):\n        for i in range(self.population_size):\n            cognitive_component = self.phi_p * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.particles[i])\n            social_component = self.phi_g * np.random.rand(self.dim) * (self.global_best_position - self.particles[i])\n            self.velocities[i] = self.omega * self.velocities[i] + cognitive_component + social_component\n\n            if np.random.rand() < self.tunneling_prob:\n                # Quantum tunneling with Levy flight\n                levy_step = self._levy_flight()\n                self.particles[i] = self.global_best_position + levy_step * (ub - lb)\n            else:\n                self.particles[i] += self.velocities[i]\n\n            self.particles[i] = np.clip(self.particles[i], lb, ub)\n\n    def __call__(self, func):\n        self.lb, self.ub = func.bounds.lb, func.bounds.ub\n        self._initialize_population(self.lb, self.ub)\n\n        eval_count = 0\n        while eval_count < self.budget:\n            for i in range(self.population_size):\n                if eval_count >= self.budget:\n                    break\n\n                score = func(self.particles[i])\n                eval_count += 1\n\n                if score < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = score\n                    self.personal_best_positions[i] = self.particles[i]\n\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.particles[i]\n            \n            self._update_particles(self.lb, self.ub)\n\n        return self.global_best_position, self.global_best_score", "name": "QuantumLevyFlightPSO", "description": "Quantum-Levy Flight PSO enhances exploration by integrating Levy flights with quantum tunneling to efficiently escape local optima and improve convergence in complex landscapes.", "configspace": "", "generation": 18, "fitness": 0.8242337158135717, "feedback": "The algorithm QuantumLevyFlightPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.82 with standard deviation 0.10.", "error": "", "parent_id": "efa0984c-76fc-4800-b1a8-4af751e43e5d", "metadata": {"aucs": [0.9206108220136654, 0.727856609613478]}, "mutation_prompt": null}
{"id": "af4460e5-460c-4e29-804b-2d922b8d44d9", "solution": "import numpy as np\n\nclass AdaptiveQuantumDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20 + self.dim\n        self.mutation_factor = 0.5\n        self.crossover_probability = 0.7\n        self.population = None\n        self.best_solution = None\n        self.best_score = np.inf\n\n    def _initialize_population(self, lb, ub):\n        self.population = np.random.rand(self.population_size, self.dim) * (ub - lb) + lb\n\n    def _mutate(self, idx, lb, ub):\n        indices = [i for i in range(self.population_size) if i != idx]\n        a, b, c = self.population[np.random.choice(indices, 3, replace=False)]\n        mutant = a + self.mutation_factor * (b - c)\n        if np.random.rand() < 0.5:\n            # Quantum-inspired mutation\n            mutant += np.random.normal(0, 0.1, self.dim) * (self.best_solution - mutant)\n        return np.clip(mutant, lb, ub)\n\n    def _crossover(self, target, mutant):\n        trial = np.where(np.random.rand(self.dim) < self.crossover_probability, mutant, target)\n        return trial\n\n    def __call__(self, func):\n        self.lb, self.ub = func.bounds.lb, func.bounds.ub\n        self._initialize_population(self.lb, self.ub)\n\n        eval_count = 0\n        while eval_count < self.budget:\n            for i in range(self.population_size):\n                if eval_count >= self.budget:\n                    break\n\n                target = self.population[i]\n                mutant = self._mutate(i, self.lb, self.ub)\n                trial = self._crossover(target, mutant)\n\n                score = func(trial)\n                eval_count += 1\n\n                if score < func(target):\n                    self.population[i] = trial\n                    if score < self.best_score:\n                        self.best_score = score\n                        self.best_solution = trial\n\n                # Adaptive mutation factor adjustment\n                if score < self.best_score:\n                    self.mutation_factor = min(1.0, self.mutation_factor * 1.2)\n                else:\n                    self.mutation_factor = max(0.1, self.mutation_factor * 0.9)\n\n        return self.best_solution, self.best_score", "name": "AdaptiveQuantumDifferentialEvolution", "description": "Adaptive Differential Evolution with Quantum-inspired Adaptive Mutation Strategy adjusts mutation factors dynamically and incorporates quantum-inspired mutations for diverse exploration and exploitation.", "configspace": "", "generation": 19, "fitness": -Infinity, "feedback": "An exception occurred: TypeError(\"unsupported operand type(s) for -: 'NoneType' and 'float'\").", "error": "TypeError(\"unsupported operand type(s) for -: 'NoneType' and 'float'\")", "parent_id": "efa0984c-76fc-4800-b1a8-4af751e43e5d", "metadata": {}, "mutation_prompt": null}
{"id": "886d3ed4-4dda-4108-9bfe-eb231d760865", "solution": "import numpy as np\n\nclass AdaptiveMemorySearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20 + self.dim\n        self.memory_size = 5  # Number of successful strategies to remember\n        self.particles = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_scores = None\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.omega = 0.5  # Inertia weight\n        self.phi_p = 1.5  # Cognitive coefficient\n        self.phi_g = 1.5  # Social coefficient\n        self.memory = []  # Initialize adaptive memory\n\n    def _initialize_population(self, lb, ub):\n        self.particles = np.random.rand(self.population_size, self.dim) * (ub - lb) + lb\n        self.velocities = np.random.rand(self.population_size, self.dim) * (ub - lb) / 10\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n\n    def _update_particles(self, lb, ub):\n        for i in range(self.population_size):\n            # Randomly choose a strategy from memory or use the default\n            if self.memory and np.random.rand() < 0.5:\n                strategy = self.memory[np.random.randint(len(self.memory))]\n                cognitive_component = strategy['phi_p'] * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.particles[i])\n                social_component = strategy['phi_g'] * np.random.rand(self.dim) * (self.global_best_position - self.particles[i])\n                inertia = strategy['omega']\n            else:\n                cognitive_component = self.phi_p * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.particles[i])\n                social_component = self.phi_g * np.random.rand(self.dim) * (self.global_best_position - self.particles[i])\n                inertia = self.omega\n\n            self.velocities[i] = inertia * self.velocities[i] + cognitive_component + social_component\n            self.particles[i] += self.velocities[i]\n            self.particles[i] = np.clip(self.particles[i], lb, ub)\n\n    def __call__(self, func):\n        self.lb, self.ub = func.bounds.lb, func.bounds.ub\n        self._initialize_population(self.lb, self.ub)\n\n        eval_count = 0\n        while eval_count < self.budget:\n            for i in range(self.population_size):\n                if eval_count >= self.budget:\n                    break\n\n                score = func(self.particles[i])\n                eval_count += 1\n\n                if score < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = score\n                    self.personal_best_positions[i] = self.particles[i]\n\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.particles[i]\n                    # Add successful strategy to memory\n                    if len(self.memory) >= self.memory_size:\n                        self.memory.pop(0)  # Remove oldest strategy if memory is full\n                    self.memory.append({'omega': self.omega, 'phi_p': self.phi_p, 'phi_g': self.phi_g})\n                    # Adapt parameters slightly\n                    self.omega = np.clip(self.omega + np.random.uniform(-0.1, 0.1), 0.3, 0.7)\n                    self.phi_p = np.clip(self.phi_p + np.random.uniform(-0.1, 0.1), 1.2, 1.8)\n                    self.phi_g = np.clip(self.phi_g + np.random.uniform(-0.1, 0.1), 1.2, 1.8)\n\n            self._update_particles(self.lb, self.ub)\n\n        return self.global_best_position, self.global_best_score", "name": "AdaptiveMemorySearch", "description": "Adaptive Memory Search (AMS) incorporates an adaptive memory mechanism to balance exploration and exploitation, dynamically adjusting search strategies based on historical information.", "configspace": "", "generation": 20, "fitness": 0.9696325247164075, "feedback": "The algorithm AdaptiveMemorySearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.97 with standard deviation 0.01.", "error": "", "parent_id": "efa0984c-76fc-4800-b1a8-4af751e43e5d", "metadata": {"aucs": [0.9638020214978693, 0.9754630279349458]}, "mutation_prompt": null}
{"id": "8064a7f9-9fc3-4599-ac26-b337a33a469d", "solution": "import numpy as np\n\nclass QuantumInspiredAdaptiveMemorySearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20 + self.dim\n        self.memory_size = 5\n        self.particles = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_scores = None\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.omega = 0.5\n        self.phi_p = 1.5\n        self.phi_g = 1.5\n        self.memory = []\n\n    def _initialize_population(self, lb, ub):\n        self.particles = np.random.rand(self.population_size, self.dim) * (ub - lb) + lb\n        self.velocities = np.random.rand(self.population_size, self.dim) * (ub - lb) / 10\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n\n    def _quantum_behavior(self, particle, lb, ub):\n        return lb + (ub - lb) * np.random.uniform(0.4, 0.6, self.dim) * np.abs(particle - self.global_best_position)\n\n    def _update_particles(self, lb, ub):\n        for i in range(self.population_size):\n            if self.memory and np.random.rand() < 0.5:\n                strategy = self.memory[np.random.randint(len(self.memory))]\n                cognitive_component = strategy['phi_p'] * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.particles[i])\n                social_component = strategy['phi_g'] * np.random.rand(self.dim) * (self.global_best_position - self.particles[i])\n                inertia = strategy['omega']\n            else:\n                cognitive_component = self.phi_p * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.particles[i])\n                social_component = self.phi_g * np.random.rand(self.dim) * (self.global_best_position - self.particles[i])\n                inertia = self.omega\n            \n            if np.random.rand() < 0.1:\n                self.particles[i] = self._quantum_behavior(self.particles[i], lb, ub)\n            else:\n                self.velocities[i] = inertia * self.velocities[i] + cognitive_component + social_component\n                self.particles[i] += self.velocities[i]\n                self.particles[i] = np.clip(self.particles[i], lb, ub)\n\n    def __call__(self, func):\n        self.lb, self.ub = func.bounds.lb, func.bounds.ub\n        self._initialize_population(self.lb, self.ub)\n\n        eval_count = 0\n        while eval_count < self.budget:\n            for i in range(self.population_size):\n                if eval_count >= self.budget:\n                    break\n\n                score = func(self.particles[i])\n                eval_count += 1\n\n                if score < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = score\n                    self.personal_best_positions[i] = self.particles[i]\n\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.particles[i]\n                    if len(self.memory) >= self.memory_size:\n                        self.memory.pop(0)\n                    self.memory.append({'omega': self.omega, 'phi_p': self.phi_p, 'phi_g': self.phi_g})\n                    self.omega = np.clip(self.omega + np.random.uniform(-0.05, 0.05), 0.3, 0.7)\n                    self.phi_p = np.clip(self.phi_p + np.random.uniform(-0.05, 0.05), 1.2, 1.8)\n                    self.phi_g = np.clip(self.phi_g + np.random.uniform(-0.05, 0.05), 1.2, 1.8)\n\n            self._update_particles(self.lb, self.ub)\n\n        return self.global_best_position, self.global_best_score", "name": "QuantumInspiredAdaptiveMemorySearch", "description": "Quantum-Inspired Adaptive Memory Search (QIAMS) enhances exploration with quantum behaviors and adaptive memory to improve global optimization in complex search spaces.", "configspace": "", "generation": 21, "fitness": 0.7338322249950587, "feedback": "The algorithm QuantumInspiredAdaptiveMemorySearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.73 with standard deviation 0.04.", "error": "", "parent_id": "886d3ed4-4dda-4108-9bfe-eb231d760865", "metadata": {"aucs": [0.7727524368476839, 0.6949120131424336]}, "mutation_prompt": null}
{"id": "8b8fd39e-b94b-463e-b809-71ee526a7da0", "solution": "import numpy as np\n\nclass QuantumInspiredEvolutionaryAlgorithm:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20 + self.dim\n        self.qubit_population = None\n        self.best_solution = None\n        self.best_score = np.inf\n        self.alpha = 0.1  # Quantum rotation angle\n\n    def _initialize_population(self):\n        # Initialize qubit population with random angles in radians\n        self.qubit_population = np.random.uniform(0, 2 * np.pi, (self.population_size, self.dim))\n\n    def _qubit_to_solution(self, qubit):\n        # Map qubit states to real-valued solution using probability amplitudes\n        return np.cos(qubit)**2\n\n    def _evaluate_population(self, func):\n        eval_count = 0\n        for i in range(self.population_size):\n            if eval_count >= self.budget:\n                break\n\n            solution = self._qubit_to_solution(self.qubit_population[i])\n            solution = solution * (self.ub - self.lb) + self.lb  # Scale to bounds\n            score = func(solution)\n            eval_count += 1\n\n            if score < self.best_score:\n                self.best_score = score\n                self.best_solution = solution\n\n        return eval_count\n\n    def _update_qubits(self):\n        # Update each qubit using quantum rotation inspired by best-found solution\n        best_solution_qubit = np.arccos(np.sqrt((self.best_solution - self.lb) / (self.ub - self.lb)))\n        for i in range(self.population_size):\n            delta_theta = self.alpha * (best_solution_qubit - self.qubit_population[i])\n            self.qubit_population[i] += delta_theta\n            self.qubit_population[i] = np.mod(self.qubit_population[i], 2 * np.pi)  # Keep within valid range\n\n    def __call__(self, func):\n        self.lb, self.ub = func.bounds.lb, func.bounds.ub\n        self._initialize_population()\n\n        eval_count = self._evaluate_population(func)\n        while eval_count < self.budget:\n            self._update_qubits()\n            eval_count += self._evaluate_population(func)\n\n        return self.best_solution, self.best_score", "name": "QuantumInspiredEvolutionaryAlgorithm", "description": "Quantum-Inspired Evolutionary Algorithm (QIEA) uses quantum mechanics principles like superposition and entanglement to efficiently explore the search space, enhancing both exploration and exploitation.", "configspace": "", "generation": 22, "fitness": 0.5521953944430762, "feedback": "The algorithm QuantumInspiredEvolutionaryAlgorithm got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.55 with standard deviation 0.01.", "error": "", "parent_id": "886d3ed4-4dda-4108-9bfe-eb231d760865", "metadata": {"aucs": [0.5425031299053911, 0.5618876589807613]}, "mutation_prompt": null}
{"id": "61423093-0228-48cf-83cf-0f9be1d3eec0", "solution": "import numpy as np\n\nclass EnhancedAdaptiveMemorySearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20 + self.dim\n        self.memory_size = 5\n        self.particles = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_scores = None\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.omega = 0.6\n        self.phi_p = 1.7\n        self.phi_g = 1.7\n        self.memory = []\n\n    def _initialize_population(self, lb, ub):\n        self.particles = np.random.rand(self.population_size, self.dim) * (ub - lb) + lb\n        self.velocities = np.random.rand(self.population_size, self.dim) * (ub - lb) / 10\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n\n    def _update_particles(self, lb, ub):\n        for i in range(self.population_size):\n            if self.memory and np.random.rand() < 0.7:\n                strategy = self.memory[np.random.randint(len(self.memory))]\n                cognitive_component = strategy['phi_p'] * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.particles[i])\n                social_component = strategy['phi_g'] * np.random.rand(self.dim) * (self.global_best_position - self.particles[i])\n                inertia = strategy['omega']\n            else:\n                cognitive_component = self.phi_p * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.particles[i])\n                social_component = self.phi_g * np.random.rand(self.dim) * (self.global_best_position - self.particles[i])\n                inertia = self.omega\n\n            self.velocities[i] = inertia * self.velocities[i] + cognitive_component + social_component\n            self.particles[i] += self.velocities[i]\n            self.particles[i] = np.clip(self.particles[i], lb, ub)\n\n    def __call__(self, func):\n        self.lb, self.ub = func.bounds.lb, func.bounds.ub\n        self._initialize_population(self.lb, self.ub)\n\n        eval_count = 0\n        while eval_count < self.budget:\n            for i in range(self.population_size):\n                if eval_count >= self.budget:\n                    break\n\n                score = func(self.particles[i])\n                eval_count += 1\n\n                if score < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = score\n                    self.personal_best_positions[i] = self.particles[i]\n\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.particles[i]\n                    if len(self.memory) >= self.memory_size:\n                        survival_chance = np.random.rand()\n                        if survival_chance < 0.5:\n                            self.memory.pop(np.random.randint(len(self.memory)))  # Replace a random strategy in memory\n                    self.memory.append({'omega': self.omega, 'phi_p': self.phi_p, 'phi_g': self.phi_g})\n                    self.omega = np.clip(self.omega + np.random.uniform(-0.1, 0.1), 0.4, 0.8)\n                    self.phi_p = np.clip(self.phi_p + np.random.uniform(-0.1, 0.1), 1.4, 2.0)\n                    self.phi_g = np.clip(self.phi_g + np.random.uniform(-0.1, 0.1), 1.4, 2.0)\n\n            self._update_particles(self.lb, self.ub)\n\n        return self.global_best_position, self.global_best_score", "name": "EnhancedAdaptiveMemorySearch", "description": "Enhanced Adaptive Memory Search (EAMS) utilizes dynamically adjusted inertia, cognitive, and social coefficients with memory-driven competition to refine exploration and exploitation balance.", "configspace": "", "generation": 23, "fitness": 0.9288198463074044, "feedback": "The algorithm EnhancedAdaptiveMemorySearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.93 with standard deviation 0.01.", "error": "", "parent_id": "886d3ed4-4dda-4108-9bfe-eb231d760865", "metadata": {"aucs": [0.943358361429468, 0.9142813311853407]}, "mutation_prompt": null}
{"id": "7101478e-4d6f-4089-a70c-13ba8cdce3a8", "solution": "import numpy as np\n\nclass DynamicStrategyUpdate:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20 + self.dim\n        self.memory_size = 5  # Number of successful strategies to remember\n        self.particles = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_scores = None\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.omega = 0.5  # Inertia weight\n        self.phi_p = 1.5  # Cognitive coefficient\n        self.phi_g = 1.5  # Social coefficient\n        self.memory = []  # Initialize adaptive memory\n        self.success_strategies = []  # Track successful strategies\n\n    def _initialize_population(self, lb, ub):\n        self.particles = np.random.rand(self.population_size, self.dim) * (ub - lb) + lb\n        self.velocities = np.random.rand(self.population_size, self.dim) * (ub - lb) / 10\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n\n    def _update_particles(self, lb, ub):\n        for i in range(self.population_size):\n            # Randomly choose a strategy from memory or use the default\n            if self.memory and np.random.rand() < 0.5:\n                strategy = self.memory[np.random.randint(len(self.memory))]\n                cognitive_component = strategy['phi_p'] * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.particles[i])\n                social_component = strategy['phi_g'] * np.random.rand(self.dim) * (self.global_best_position - self.particles[i])\n                inertia = strategy['omega']\n            else:\n                cognitive_component = self.phi_p * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.particles[i])\n                social_component = self.phi_g * np.random.rand(self.dim) * (self.global_best_position - self.particles[i])\n                inertia = self.omega\n\n            self.velocities[i] = inertia * self.velocities[i] + cognitive_component + social_component\n            self.particles[i] += self.velocities[i]\n            self.particles[i] = np.clip(self.particles[i], lb, ub)\n\n    def _update_strategy(self):\n        if len(self.success_strategies) >= self.memory_size:\n            # Calculate average parameters of recent successful strategies\n            avg_omega = np.mean([s['omega'] for s in self.success_strategies[-self.memory_size:]])\n            avg_phi_p = np.mean([s['phi_p'] for s in self.success_strategies[-self.memory_size:]])\n            avg_phi_g = np.mean([s['phi_g'] for s in self.success_strategies[-self.memory_size:]])\n            # Update parameters with a slight random adjustment\n            self.omega = np.clip(avg_omega + np.random.uniform(-0.05, 0.05), 0.3, 0.7)\n            self.phi_p = np.clip(avg_phi_p + np.random.uniform(-0.05, 0.05), 1.2, 1.8)\n            self.phi_g = np.clip(avg_phi_g + np.random.uniform(-0.05, 0.05), 1.2, 1.8)\n\n    def __call__(self, func):\n        self.lb, self.ub = func.bounds.lb, func.bounds.ub\n        self._initialize_population(self.lb, self.ub)\n\n        eval_count = 0\n        while eval_count < self.budget:\n            for i in range(self.population_size):\n                if eval_count >= self.budget:\n                    break\n\n                score = func(self.particles[i])\n                eval_count += 1\n\n                if score < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = score\n                    self.personal_best_positions[i] = self.particles[i]\n\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.particles[i]\n                    # Add successful strategy to memory\n                    self.success_strategies.append({'omega': self.omega, 'phi_p': self.phi_p, 'phi_g': self.phi_g})\n                    if len(self.memory) >= self.memory_size:\n                        self.memory.pop(0)  # Remove oldest strategy if memory is full\n                    self.memory.append({'omega': self.omega, 'phi_p': self.phi_p, 'phi_g': self.phi_g})\n                    self._update_strategy()\n\n            self._update_particles(self.lb, self.ub)\n\n        return self.global_best_position, self.global_best_score", "name": "DynamicStrategyUpdate", "description": "Dynamic Strategy Update (DSU) enhances the Adaptive Memory Search by integrating a dynamic strategy update mechanism that adjusts based on a rolling window of recent successes.", "configspace": "", "generation": 24, "fitness": 0.8989442009751798, "feedback": "The algorithm DynamicStrategyUpdate got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.90 with standard deviation 0.00.", "error": "", "parent_id": "886d3ed4-4dda-4108-9bfe-eb231d760865", "metadata": {"aucs": [0.8976837196689853, 0.9002046822813743]}, "mutation_prompt": null}
{"id": "1e5bd5fe-b034-4d10-8dfb-52ac437fb6ff", "solution": "import numpy as np\n\nclass EnhancedAdaptiveMemorySearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20 + self.dim\n        self.memory_size = 10  # Increased memory size for better adaptation\n        self.particles = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_scores = None\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.omega = 0.5\n        self.phi_p = 1.5\n        self.phi_g = 1.5\n        self.memory = []\n        self.local_search_prob = 0.1  # Probability of applying local search\n\n    def _initialize_population(self, lb, ub):\n        self.particles = np.random.rand(self.population_size, self.dim) * (ub - lb) + lb\n        self.velocities = np.random.rand(self.population_size, self.dim) * (ub - lb) / 10\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n\n    def _local_search(self, position, lb, ub):\n        # Simple local search strategy to fine-tune\n        perturbation = (np.random.rand(self.dim) - 0.5) * (ub - lb) * 0.1\n        new_position = np.clip(position + perturbation, lb, ub)\n        return new_position\n\n    def _update_particles(self, lb, ub):\n        for i in range(self.population_size):\n            if self.memory and np.random.rand() < 0.5:\n                strategy = self.memory[np.random.randint(len(self.memory))]\n                cognitive_component = strategy['phi_p'] * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.particles[i])\n                social_component = strategy['phi_g'] * np.random.rand(self.dim) * (self.global_best_position - self.particles[i])\n                inertia = strategy['omega']\n            else:\n                cognitive_component = self.phi_p * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.particles[i])\n                social_component = self.phi_g * np.random.rand(self.dim) * (self.global_best_position - self.particles[i])\n                inertia = self.omega\n            \n            self.velocities[i] = inertia * self.velocities[i] + cognitive_component + social_component\n            self.particles[i] += self.velocities[i]\n            self.particles[i] = np.clip(self.particles[i], lb, ub)\n\n            if np.random.rand() < self.local_search_prob:\n                self.particles[i] = self._local_search(self.particles[i], lb, ub)\n\n    def __call__(self, func):\n        self.lb, self.ub = func.bounds.lb, func.bounds.ub\n        self._initialize_population(self.lb, self.ub)\n\n        eval_count = 0\n        while eval_count < self.budget:\n            for i in range(self.population_size):\n                if eval_count >= self.budget:\n                    break\n\n                score = func(self.particles[i])\n                eval_count += 1\n\n                if score < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = score\n                    self.personal_best_positions[i] = self.particles[i]\n\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.particles[i]\n                    if len(self.memory) >= self.memory_size:\n                        self.memory.pop(0)\n                    self.memory.append({'omega': self.omega, 'phi_p': self.phi_p, 'phi_g': self.phi_g})\n                    self.omega = np.clip(self.omega + np.random.uniform(-0.05, 0.05), 0.3, 0.7)\n                    self.phi_p = np.clip(self.phi_p + np.random.uniform(-0.05, 0.05), 1.2, 1.8)\n                    self.phi_g = np.clip(self.phi_g + np.random.uniform(-0.05, 0.05), 1.2, 1.8)\n\n            self._update_particles(self.lb, self.ub)\n\n        return self.global_best_position, self.global_best_score", "name": "EnhancedAdaptiveMemorySearch", "description": "Enhanced Adaptive Memory Search (EAMS) improves convergence by dynamically adjusting parameters based on a larger adaptive memory and introducing a local search phase for fine-tuning.", "configspace": "", "generation": 25, "fitness": 0.8528638019783519, "feedback": "The algorithm EnhancedAdaptiveMemorySearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.85 with standard deviation 0.01.", "error": "", "parent_id": "886d3ed4-4dda-4108-9bfe-eb231d760865", "metadata": {"aucs": [0.846691513177825, 0.8590360907788789]}, "mutation_prompt": null}
{"id": "53eb9e25-b92d-48d0-9436-d1759897aef3", "solution": "import numpy as np\n\nclass EnhancedAdaptiveMemorySearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20 + self.dim\n        self.memory_size = 5\n        self.particles = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_scores = None\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.omega_initial = 0.7\n        self.omega_final = 0.3\n        self.phi_p = 1.5\n        self.phi_g = 1.5\n        self.memory = []\n        self.cross_prob = 0.3\n\n    def _initialize_population(self, lb, ub):\n        self.particles = np.random.rand(self.population_size, self.dim) * (ub - lb) + lb\n        self.velocities = np.random.rand(self.population_size, self.dim) * (ub - lb) / 10\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n\n    def _update_particles(self, lb, ub, eval_count):\n        for i in range(self.population_size):\n            # Reduce inertia linearly over time\n            inertia = (self.omega_final - self.omega_initial) * (eval_count / self.budget) + self.omega_initial\n\n            # Randomly choose a strategy from memory or use the default\n            if self.memory and np.random.rand() < 0.5:\n                strategy = self.memory[np.random.randint(len(self.memory))]\n                cognitive_component = strategy['phi_p'] * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.particles[i])\n                social_component = strategy['phi_g'] * np.random.rand(self.dim) * (self.global_best_position - self.particles[i])\n            else:\n                cognitive_component = self.phi_p * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.particles[i])\n                social_component = self.phi_g * np.random.rand(self.dim) * (self.global_best_position - self.particles[i])\n\n            self.velocities[i] = inertia * self.velocities[i] + cognitive_component + social_component\n            self.particles[i] += self.velocities[i]\n            self.particles[i] = np.clip(self.particles[i], lb, ub)\n\n            # Hybrid crossover\n            if np.random.rand() < self.cross_prob:\n                partner_idx = np.random.randint(self.population_size)\n                crossover_point = np.random.randint(1, self.dim)\n                self.particles[i][:crossover_point] = self.personal_best_positions[partner_idx][:crossover_point]\n\n    def __call__(self, func):\n        self.lb, self.ub = func.bounds.lb, func.bounds.ub\n        self._initialize_population(self.lb, self.ub)\n\n        eval_count = 0\n        while eval_count < self.budget:\n            for i in range(self.population_size):\n                if eval_count >= self.budget:\n                    break\n\n                score = func(self.particles[i])\n                eval_count += 1\n\n                if score < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = score\n                    self.personal_best_positions[i] = self.particles[i]\n\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.particles[i]\n                    if len(self.memory) >= self.memory_size:\n                        self.memory.pop(0)\n                    self.memory.append({'omega': inertia, 'phi_p': self.phi_p, 'phi_g': self.phi_g})\n                    self.phi_p = np.clip(self.phi_p + np.random.uniform(-0.1, 0.1), 1.2, 1.8)\n                    self.phi_g = np.clip(self.phi_g + np.random.uniform(-0.1, 0.1), 1.2, 1.8)\n\n            self._update_particles(self.lb, self.ub, eval_count)\n\n        return self.global_best_position, self.global_best_score", "name": "EnhancedAdaptiveMemorySearch", "description": "Enhanced Adaptive Memory Search (EAMS) leverages a dynamic inertia reduction and hybrid crossover mechanism to balance exploration and exploitation, improving convergence speed and solution accuracy.", "configspace": "", "generation": 26, "fitness": -Infinity, "feedback": "An exception occurred: NameError(\"name 'inertia' is not defined\").", "error": "NameError(\"name 'inertia' is not defined\")", "parent_id": "886d3ed4-4dda-4108-9bfe-eb231d760865", "metadata": {}, "mutation_prompt": null}
{"id": "f1ab02b1-03cf-48b0-8e8c-fdcb7136868a", "solution": "import numpy as np\n\nclass QuantumInspiredDynamicSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20 + self.dim\n        self.particles = None\n        self.personal_best_positions = None\n        self.personal_best_scores = None\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.rotation_angle = np.pi / 4  # Quantum-inspired rotation angle\n        self.dynamic_alpha = 0.1  # Parameter for dynamic adaptation\n\n    def _initialize_population(self, lb, ub):\n        self.particles = np.random.rand(self.population_size, self.dim) * (ub - lb) + lb\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n\n    def _quantum_rotation_update(self, particle, personal_best, global_best):\n        for d in range(self.dim):\n            delta_theta = (np.random.rand() - 0.5) * 2 * self.rotation_angle  # Random delta angle\n            rotation_matrix = np.array([[np.cos(delta_theta), -np.sin(delta_theta)],\n                                        [np.sin(delta_theta), np.cos(delta_theta)]])\n            position_vector = np.array([particle[d], global_best[d]])\n            new_position = rotation_matrix @ position_vector\n            particle[d] = (new_position[0] + personal_best[d]) / 2\n        return particle\n\n    def _dynamic_adjustment(self):\n        diversity = np.std(self.particles, axis=0).mean()\n        if diversity < 1e-5:  # Consider the population has converged\n            self.rotation_angle = min(np.pi / 2, self.rotation_angle + self.dynamic_alpha)\n\n    def __call__(self, func):\n        self.lb, self.ub = func.bounds.lb, func.bounds.ub\n        self._initialize_population(self.lb, self.ub)\n\n        eval_count = 0\n        while eval_count < self.budget:\n            for i in range(self.population_size):\n                if eval_count >= self.budget:\n                    break\n\n                score = func(self.particles[i])\n                eval_count += 1\n\n                if score < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = score\n                    self.personal_best_positions[i] = self.particles[i]\n\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.particles[i]\n\n            for i in range(self.population_size):\n                self.particles[i] = self._quantum_rotation_update(self.particles[i], \n                                                                  self.personal_best_positions[i], \n                                                                  self.global_best_position)\n                self.particles[i] = np.clip(self.particles[i], self.lb, self.ub)\n\n            self._dynamic_adjustment()\n\n        return self.global_best_position, self.global_best_score", "name": "QuantumInspiredDynamicSearch", "description": "Quantum-Inspired Dynamic Search (QIDS) leverages quantum rotation gates for exploration and dynamically adjusts search parameters based on convergence feedback to balance exploration and exploitation.", "configspace": "", "generation": 27, "fitness": 0.7597338141348251, "feedback": "The algorithm QuantumInspiredDynamicSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.76 with standard deviation 0.12.", "error": "", "parent_id": "886d3ed4-4dda-4108-9bfe-eb231d760865", "metadata": {"aucs": [0.8767637553432721, 0.6427038729263781]}, "mutation_prompt": null}
{"id": "cb05175c-07c7-4fe4-857c-c1b533bb368e", "solution": "import numpy as np\n\nclass AdaptiveQuantumInspiredMemorySearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20 + self.dim\n        self.memory_size = 5  # Number of successful strategies to remember\n        self.particles = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_scores = None\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.omega = 0.5  # Inertia weight\n        self.phi_p = 1.5  # Cognitive coefficient\n        self.phi_g = 1.5  # Social coefficient\n        self.memory = []  # Initialize adaptive memory\n        self.quantum_states = None  # Quantum-inspired superposition states\n\n    def _initialize_population(self, lb, ub):\n        self.particles = np.random.rand(self.population_size, self.dim) * (ub - lb) + lb\n        self.velocities = np.random.rand(self.population_size, self.dim) * (ub - lb) / 10\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n        self.quantum_states = np.random.rand(self.population_size, self.dim) * (ub - lb) + lb\n\n    def _update_particles(self, lb, ub):\n        for i in range(self.population_size):\n            # Quantum-inspired update\n            quantum_term = np.sin(self.quantum_states[i] - self.particles[i]) * np.random.rand(self.dim)\n            \n            # Randomly choose a strategy from memory or use the default\n            if self.memory and np.random.rand() < 0.5:\n                strategy = self.memory[np.random.randint(len(self.memory))]\n                cognitive_component = strategy['phi_p'] * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.particles[i])\n                social_component = strategy['phi_g'] * np.random.rand(self.dim) * (self.global_best_position - self.particles[i])\n                inertia = strategy['omega']\n            else:\n                cognitive_component = self.phi_p * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.particles[i])\n                social_component = self.phi_g * np.random.rand(self.dim) * (self.global_best_position - self.particles[i])\n                inertia = self.omega\n\n            self.velocities[i] = inertia * self.velocities[i] + cognitive_component + social_component + quantum_term\n            self.particles[i] += self.velocities[i]\n            self.particles[i] = np.clip(self.particles[i], lb, ub)\n            self.quantum_states[i] = np.clip(self.quantum_states[i] + np.random.uniform(-0.1, 0.1, self.dim), lb, ub)\n\n    def __call__(self, func):\n        self.lb, self.ub = func.bounds.lb, func.bounds.ub\n        self._initialize_population(self.lb, self.ub)\n\n        eval_count = 0\n        while eval_count < self.budget:\n            for i in range(self.population_size):\n                if eval_count >= self.budget:\n                    break\n\n                score = func(self.particles[i])\n                eval_count += 1\n\n                if score < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = score\n                    self.personal_best_positions[i] = self.particles[i]\n\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.particles[i]\n                    # Add successful strategy to memory\n                    if len(self.memory) >= self.memory_size:\n                        self.memory.pop(0)  # Remove oldest strategy if memory is full\n                    self.memory.append({'omega': self.omega, 'phi_p': self.phi_p, 'phi_g': self.phi_g})\n                    # Adapt parameters slightly\n                    self.omega = np.clip(self.omega + np.random.uniform(-0.1, 0.1), 0.3, 0.7)\n                    self.phi_p = np.clip(self.phi_p + np.random.uniform(-0.1, 0.1), 1.2, 1.8)\n                    self.phi_g = np.clip(self.phi_g + np.random.uniform(-0.1, 0.1), 1.2, 1.8)\n\n            self._update_particles(self.lb, self.ub)\n\n        return self.global_best_position, self.global_best_score", "name": "AdaptiveQuantumInspiredMemorySearch", "description": "Adaptive Quantum-Inspired Memory Search (AQIMS) integrates quantum-inspired superposition states with adaptive memory to enhance exploration and exploitation balance for global optimization.", "configspace": "", "generation": 28, "fitness": 0.8284678737555065, "feedback": "The algorithm AdaptiveQuantumInspiredMemorySearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.83 with standard deviation 0.03.", "error": "", "parent_id": "886d3ed4-4dda-4108-9bfe-eb231d760865", "metadata": {"aucs": [0.8563245698627084, 0.8006111776483044]}, "mutation_prompt": null}
{"id": "e7adbce7-4e7c-44e0-a83c-7cc5b3f3a347", "solution": "import numpy as np\n\nclass QuantumInspiredAdaptiveMemorySearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20 + self.dim\n        self.memory_size = 5\n        self.particles = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_scores = None\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.omega = 0.5\n        self.phi_p = 1.5\n        self.phi_g = 1.5\n        self.memory = []\n        self.qp = 0.1  # Quantum probability for superposition\n\n    def _initialize_population(self, lb, ub):\n        self.particles = np.random.rand(self.population_size, self.dim) * (ub - lb) + lb\n        self.velocities = np.random.rand(self.population_size, self.dim) * (ub - lb) / 10\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n\n    def _quantum_superposition(self, lb, ub):\n        for i in range(self.population_size):\n            if np.random.rand() < self.qp:\n                self.particles[i] = np.random.rand(self.dim) * (ub - lb) + lb\n                self.velocities[i] = np.random.rand(self.dim) * (ub - lb) / 10\n\n    def _update_particles(self, lb, ub):\n        for i in range(self.population_size):\n            if self.memory and np.random.rand() < 0.5:\n                strategy = self.memory[np.random.randint(len(self.memory))]\n                cognitive_component = strategy['phi_p'] * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.particles[i])\n                social_component = strategy['phi_g'] * np.random.rand(self.dim) * (self.global_best_position - self.particles[i])\n                inertia = strategy['omega']\n            else:\n                cognitive_component = self.phi_p * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.particles[i])\n                social_component = self.phi_g * np.random.rand(self.dim) * (self.global_best_position - self.particles[i])\n                inertia = self.omega\n                \n            mutation = np.random.normal(0, 0.1, self.dim)\n            self.velocities[i] = inertia * self.velocities[i] + cognitive_component + social_component + mutation\n            self.particles[i] += self.velocities[i]\n            self.particles[i] = np.clip(self.particles[i], lb, ub)\n\n    def __call__(self, func):\n        self.lb, self.ub = func.bounds.lb, func.bounds.ub\n        self._initialize_population(self.lb, self.ub)\n\n        eval_count = 0\n        while eval_count < self.budget:\n            self._quantum_superposition(self.lb, self.ub)\n\n            for i in range(self.population_size):\n                if eval_count >= self.budget:\n                    break\n\n                score = func(self.particles[i])\n                eval_count += 1\n\n                if score < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = score\n                    self.personal_best_positions[i] = self.particles[i]\n\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.particles[i]\n                    if len(self.memory) >= self.memory_size:\n                        self.memory.pop(0)\n                    self.memory.append({'omega': self.omega, 'phi_p': self.phi_p, 'phi_g': self.phi_g})\n                    self.omega = np.clip(self.omega + np.random.uniform(-0.1, 0.1), 0.3, 0.7)\n                    self.phi_p = np.clip(self.phi_p + np.random.uniform(-0.1, 0.1), 1.2, 1.8)\n                    self.phi_g = np.clip(self.phi_g + np.random.uniform(-0.1, 0.1), 1.2, 1.8)\n\n            self._update_particles(self.lb, self.ub)\n\n        return self.global_best_position, self.global_best_score", "name": "QuantumInspiredAdaptiveMemorySearch", "description": "Quantum-Inspired Adaptive Memory Search (QIAMS) integrates quantum-inspired superposition for diversification and enhanced memory mutation mechanisms to improve exploration and exploitation balance.", "configspace": "", "generation": 29, "fitness": 0.6871404348609114, "feedback": "The algorithm QuantumInspiredAdaptiveMemorySearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.69 with standard deviation 0.04.", "error": "", "parent_id": "886d3ed4-4dda-4108-9bfe-eb231d760865", "metadata": {"aucs": [0.6464269565822949, 0.7278539131395279]}, "mutation_prompt": null}
{"id": "3be4bfaa-cbc5-443a-8d2d-a41bba9b2a47", "solution": "import numpy as np\n\nclass EnhancedAdaptiveMemorySearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 20 + self.dim\n        self.max_population_size = 30 + self.dim  # Maximum limit for adaptive population size\n        self.population_size = self.initial_population_size\n        self.memory_size = 5\n        self.particles = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_scores = None\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.omega = 0.5\n        self.phi_p = 1.5\n        self.phi_g = 1.5\n        self.memory = []\n        self.convergence_threshold = 1e-6  # Threshold to detect convergence stagnation\n\n    def _initialize_population(self, lb, ub):\n        self.particles = np.random.rand(self.population_size, self.dim) * (ub - lb) + lb\n        self.velocities = np.random.rand(self.population_size, self.dim) * (ub - lb) / 10\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n\n    def _update_particles(self, lb, ub):\n        for i in range(self.population_size):\n            if self.memory and np.random.rand() < 0.5:\n                strategy = self.memory[np.random.randint(len(self.memory))]\n                cognitive_component = strategy['phi_p'] * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.particles[i])\n                social_component = strategy['phi_g'] * np.random.rand(self.dim) * (self.global_best_position - self.particles[i])\n                inertia = strategy['omega']\n            else:\n                cognitive_component = self.phi_p * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.particles[i])\n                social_component = self.phi_g * np.random.rand(self.dim) * (self.global_best_position - self.particles[i])\n                inertia = self.omega\n\n            self.velocities[i] = inertia * self.velocities[i] + cognitive_component + social_component\n            self.particles[i] += self.velocities[i]\n            self.particles[i] = np.clip(self.particles[i], lb, ub)\n\n    def __call__(self, func):\n        self.lb, self.ub = func.bounds.lb, func.bounds.ub\n        self._initialize_population(self.lb, self.ub)\n\n        eval_count = 0\n        last_global_best_score = self.global_best_score\n        while eval_count < self.budget:\n            for i in range(self.population_size):\n                if eval_count >= self.budget:\n                    break\n\n                score = func(self.particles[i])\n                eval_count += 1\n\n                if score < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = score\n                    self.personal_best_positions[i] = self.particles[i]\n\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.particles[i]\n                    if len(self.memory) >= self.memory_size:\n                        self.memory.pop(0)\n                    self.memory.append({'omega': self.omega, 'phi_p': self.phi_p, 'phi_g': self.phi_g})\n                    self.omega = np.clip(self.omega + np.random.uniform(-0.1, 0.1), 0.3, 0.7)\n                    self.phi_p = np.clip(self.phi_p + np.random.uniform(-0.1, 0.1), 1.2, 1.8)\n                    self.phi_g = np.clip(self.phi_g + np.random.uniform(-0.1, 0.1), 1.2, 1.8)\n                    \n            self._update_particles(self.lb, self.ub)\n\n            # Dynamic population size adjustment\n            if np.abs(self.global_best_score - last_global_best_score) < self.convergence_threshold:\n                self.population_size = min(self.max_population_size, self.population_size + 1)\n            else:\n                self.population_size = max(self.initial_population_size, self.population_size - 1)\n\n            last_global_best_score = self.global_best_score\n\n        return self.global_best_position, self.global_best_score", "name": "EnhancedAdaptiveMemorySearch", "description": "Enhanced Adaptive Memory Search (EAMS) introduces adaptive control of population size and dynamic parameter tuning based on convergence metrics to improve exploration and exploitation balance.", "configspace": "", "generation": 30, "fitness": -Infinity, "feedback": "An exception occurred: IndexError('index 22 is out of bounds for axis 0 with size 22').", "error": "IndexError('index 22 is out of bounds for axis 0 with size 22')", "parent_id": "886d3ed4-4dda-4108-9bfe-eb231d760865", "metadata": {}, "mutation_prompt": null}
{"id": "2b58d4c5-2ec1-4fbe-8eef-a95224aa5348", "solution": "import numpy as np\n\nclass QuantumInspiredDynamicPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20 + self.dim\n        self.particles = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_scores = None\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.omega_initial = 0.9\n        self.omega_final = 0.4\n        self.phi_p = 2.0\n        self.phi_g = 2.0\n        self.qbits = None\n\n    def _initialize_population(self, lb, ub):\n        self.particles = np.random.rand(self.population_size, self.dim) * (ub - lb) + lb\n        self.velocities = np.random.rand(self.population_size, self.dim) * (ub - lb) / 10\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n        self.qbits = np.random.rand(self.population_size, self.dim) < 0.5  # Quantum superposition\n\n    def _update_particles(self, lb, ub, eval_count):\n        # Dynamic inertia weight\n        inertia_weight = self.omega_initial - ((self.omega_initial - self.omega_final) * (eval_count / self.budget))\n        \n        for i in range(self.population_size):\n            cognitive_component = self.phi_p * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.particles[i])\n            social_component = self.phi_g * np.random.rand(self.dim) * (self.global_best_position - self.particles[i])\n\n            self.velocities[i] = inertia_weight * self.velocities[i] + cognitive_component + social_component\n            self.particles[i] += self.velocities[i]\n\n            # Quantum-inspired position update\n            self.particles[i] = np.where(self.qbits[i], self.particles[i], self.global_best_position + np.random.randn(self.dim))\n\n            self.particles[i] = np.clip(self.particles[i], lb, ub)\n            self.qbits[i] = np.random.rand(self.dim) < 0.5  # Update quantum bits\n\n    def __call__(self, func):\n        self.lb, self.ub = func.bounds.lb, func.bounds.ub\n        self._initialize_population(self.lb, self.ub)\n\n        eval_count = 0\n        while eval_count < self.budget:\n            for i in range(self.population_size):\n                if eval_count >= self.budget:\n                    break\n\n                score = func(self.particles[i])\n                eval_count += 1\n\n                if score < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = score\n                    self.personal_best_positions[i] = self.particles[i]\n\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.particles[i]\n\n            self._update_particles(self.lb, self.ub, eval_count)\n\n        return self.global_best_position, self.global_best_score", "name": "QuantumInspiredDynamicPSO", "description": "Quantum-Inspired Dynamic Particle Swarm Optimization (QIDPSO) leverages quantum superposition principles to enhance diversity and employs dynamic parameter tuning for improved exploration and exploitation balance.", "configspace": "", "generation": 31, "fitness": 0.682932744002165, "feedback": "The algorithm QuantumInspiredDynamicPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.68 with standard deviation 0.04.", "error": "", "parent_id": "886d3ed4-4dda-4108-9bfe-eb231d760865", "metadata": {"aucs": [0.6407506306304236, 0.7251148573739064]}, "mutation_prompt": null}
{"id": "ea54994e-5df6-452f-9d18-77e7cfa40226", "solution": "import numpy as np\n\nclass EnhancedAdaptiveMemorySearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20 + self.dim\n        self.memory_size = 5\n        self.particles = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_scores = None\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.omega = 0.5\n        self.phi_p = 1.5\n        self.phi_g = 1.5\n        self.memory = []\n        self.local_search_probability = 0.2\n\n    def _initialize_population(self, lb, ub):\n        self.particles = np.random.rand(self.population_size, self.dim) * (ub - lb) + lb\n        self.velocities = np.random.rand(self.population_size, self.dim) * (ub - lb) / 10\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n\n    def _local_search(self, position, lb, ub):\n        perturbation = np.random.normal(0, 0.1, self.dim)\n        new_position = np.clip(position + perturbation, lb, ub)\n        return new_position\n\n    def _update_particles(self, lb, ub):\n        for i in range(self.population_size):\n            if self.memory and np.random.rand() < 0.5:\n                strategy = self.memory[np.random.randint(len(self.memory))]\n                cognitive_component = strategy['phi_p'] * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.particles[i])\n                social_component = strategy['phi_g'] * np.random.rand(self.dim) * (self.global_best_position - self.particles[i])\n                inertia = strategy['omega']\n            else:\n                cognitive_component = self.phi_p * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.particles[i])\n                social_component = self.phi_g * np.random.rand(self.dim) * (self.global_best_position - self.particles[i])\n                inertia = self.omega\n\n            self.velocities[i] = inertia * self.velocities[i] + cognitive_component + social_component\n            self.particles[i] += self.velocities[i]\n            self.particles[i] = np.clip(self.particles[i], lb, ub)\n\n            if np.random.rand() < self.local_search_probability:\n                local_candidate = self._local_search(self.particles[i], lb, ub)\n                local_score = func(local_candidate)\n                if local_score < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = local_score\n                    self.personal_best_positions[i] = local_candidate\n\n    def __call__(self, func):\n        self.lb, self.ub = func.bounds.lb, func.bounds.ub\n        self._initialize_population(self.lb, self.ub)\n\n        eval_count = 0\n        while eval_count < self.budget:\n            for i in range(self.population_size):\n                if eval_count >= self.budget:\n                    break\n\n                score = func(self.particles[i])\n                eval_count += 1\n\n                if score < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = score\n                    self.personal_best_positions[i] = self.particles[i]\n\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.particles[i]\n                    if len(self.memory) >= self.memory_size:\n                        self.memory.pop(0)\n                    self.memory.append({'omega': self.omega, 'phi_p': self.phi_p, 'phi_g': self.phi_g})\n                    self.omega = np.clip(self.omega + np.random.uniform(-0.05, 0.05), 0.3, 0.7)\n                    self.phi_p = np.clip(self.phi_p + np.random.uniform(-0.05, 0.05), 1.2, 1.8)\n                    self.phi_g = np.clip(self.phi_g + np.random.uniform(-0.05, 0.05), 1.2, 1.8)\n\n            self._update_particles(self.lb, self.ub)\n\n        return self.global_best_position, self.global_best_score", "name": "EnhancedAdaptiveMemorySearch", "description": "Enhanced Adaptive Memory Search (EAMS) integrates an adaptive memory mechanism with a dynamic hyperparameter tuning based on evolutionary strategies and local search intensification to improve convergence and solution quality.", "configspace": "", "generation": 32, "fitness": -Infinity, "feedback": "An exception occurred: NameError(\"name 'func' is not defined\").", "error": "NameError(\"name 'func' is not defined\")", "parent_id": "886d3ed4-4dda-4108-9bfe-eb231d760865", "metadata": {}, "mutation_prompt": null}
{"id": "590d322d-7c4e-416e-a295-250ac67626ce", "solution": "import numpy as np\n\nclass QuantumInspiredAdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20 + self.dim\n        self.population = None\n        self.best_individual = None\n        self.best_score = np.inf\n        self.memory = []\n        self.memory_size = 5\n        self.F = 0.5  # Initial differential weight\n        self.CR = 0.9  # Initial crossover probability\n        self.alpha = 0.1  # Quantum influence factor\n        \n    def _initialize_population(self, lb, ub):\n        # Quantum-inspired initialization\n        self.population = lb + (ub - lb) * np.random.rand(self.population_size, self.dim)\n        quantum_step = self.alpha * np.sin(np.pi * np.random.rand(self.population_size, self.dim))\n        self.population += quantum_step * (ub - lb)\n        self.population = np.clip(self.population, lb, ub)\n\n    def _mutate(self, target_idx, lb, ub):\n        idxs = [idx for idx in range(self.population_size) if idx != target_idx]\n        a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n        mutant = a + self.F * (b - c)\n        return np.clip(mutant, lb, ub)\n\n    def _crossover(self, target, mutant):\n        crossover_mask = np.random.rand(self.dim) < self.CR\n        if not np.any(crossover_mask): \n            crossover_mask[np.random.randint(0, self.dim)] = True\n        return np.where(crossover_mask, mutant, target)\n\n    def __call__(self, func):\n        self.lb, self.ub = func.bounds.lb, func.bounds.ub\n        self._initialize_population(self.lb, self.ub)\n\n        eval_count = 0\n        while eval_count < self.budget:\n            for i in range(self.population_size):\n                if eval_count >= self.budget:\n                    break\n\n                target = self.population[i]\n                mutant = self._mutate(i, self.lb, self.ub)\n                trial = self._crossover(target, mutant)\n\n                score = func(trial)\n                eval_count += 1\n\n                if score < self.best_score:\n                    self.best_score = score\n                    self.best_individual = trial.copy()\n                    # Store successful strategies\n                    if len(self.memory) >= self.memory_size:\n                        self.memory.pop(0)\n                    self.memory.append({'F': self.F, 'CR': self.CR})\n\n                if score < func(target):\n                    self.population[i] = trial\n                    if np.random.rand() < 0.3:  # Occasionally adapt F and CR\n                        self.F = np.clip(self.F + np.random.uniform(-0.1, 0.1), 0.4, 0.9)\n                        self.CR = np.clip(self.CR + np.random.uniform(-0.1, 0.1), 0.6, 1.0)\n\n        return self.best_individual, self.best_score", "name": "QuantumInspiredAdaptiveDifferentialEvolution", "description": "Quantum-Inspired Adaptive Differential Evolution (QIADE) combines quantum-inspired initialization and dynamic differential evolution strategies with adaptive control parameters for efficient exploration and exploitation.", "configspace": "", "generation": 33, "fitness": 0.8782610127599193, "feedback": "The algorithm QuantumInspiredAdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.88 with standard deviation 0.01.", "error": "", "parent_id": "886d3ed4-4dda-4108-9bfe-eb231d760865", "metadata": {"aucs": [0.8684095203317728, 0.8881125051880657]}, "mutation_prompt": null}
{"id": "8139bced-1273-4bf4-af4c-e1e9c79735f1", "solution": "import numpy as np\n\nclass QuantumInspiredEvolutionaryAlgorithm:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20 + self.dim\n        self.alpha = 0.1  # Learning factor\n        self.q_population = None\n        self.observed_population = None\n        self.best_solution = None\n        self.best_score = np.inf\n\n    def _initialize_population(self):\n        # Quantum bit representation: each decision variable in a superposition of 0 and 1\n        self.q_population = np.random.rand(self.population_size, self.dim, 2)\n        self.q_population = self.q_population / np.linalg.norm(self.q_population, axis=2, keepdims=True)\n        self.observed_population = np.zeros((self.population_size, self.dim))\n\n    def _observe(self, lb, ub):\n        # Collapse quantum bit to classical bit (0 or 1) using probabilistic observation\n        for i in range(self.population_size):\n            collapse = np.random.rand(self.dim) < self.q_population[i, :, 0]**2\n            self.observed_population[i] = lb + (ub - lb) * collapse\n\n    def _update_quantum_population(self, guide_position):\n        for i in range(self.population_size):\n            for j in range(self.dim):\n                # Rotation gate for updating quantum bits\n                theta = self.alpha * (2 * (self.observed_population[i, j] != guide_position[j]) - 1)\n                rotation_matrix = np.array([[np.cos(theta), -np.sin(theta)], [np.sin(theta), np.cos(theta)]])\n                self.q_population[i, j] = self.q_population[i, j].dot(rotation_matrix)\n\n    def __call__(self, func):\n        self.lb, self.ub = func.bounds.lb, func.bounds.ub\n        self._initialize_population()\n\n        eval_count = 0\n        while eval_count < self.budget:\n            self._observe(self.lb, self.ub)\n            for i in range(self.population_size):\n                if eval_count >= self.budget:\n                    break\n\n                score = func(self.observed_population[i])\n                eval_count += 1\n\n                if score < self.best_score:\n                    self.best_score = score\n                    self.best_solution = self.observed_population[i].copy()\n\n            # Use the best solution found as the guiding position for quantum population update\n            self._update_quantum_population(self.best_solution)\n\n        return self.best_solution, self.best_score", "name": "QuantumInspiredEvolutionaryAlgorithm", "description": "Quantum-Inspired Evolutionary Algorithm (QIEA) utilizes quantum bit representation and rotation gates to efficiently explore the search space by maintaining diversity and enhancing convergence.", "configspace": "", "generation": 34, "fitness": 0.3367443889227984, "feedback": "The algorithm QuantumInspiredEvolutionaryAlgorithm got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.34 with standard deviation 0.00.", "error": "", "parent_id": "886d3ed4-4dda-4108-9bfe-eb231d760865", "metadata": {"aucs": [0.33675203797817255, 0.33673673986742425]}, "mutation_prompt": null}
{"id": "247ca385-436c-407a-8255-318694aee618", "solution": "import numpy as np\n\nclass EnhancedAdaptiveMemorySearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20 + self.dim\n        self.memory_size = 5\n        self.particles = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_scores = None\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.omega = 0.5\n        self.phi_p = 1.5\n        self.phi_g = 1.5\n        self.memory = []\n        self.mutation_rate = 0.1  # Mutation rate for diversity\n\n    def _initialize_population(self, lb, ub):\n        self.particles = np.random.rand(self.population_size, self.dim) * (ub - lb) + lb\n        self.velocities = np.random.rand(self.population_size, self.dim) * (ub - lb) / 10\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n\n    def _mutate_particle(self, particle, lb, ub):\n        if np.random.rand() < self.mutation_rate:\n            mutation_vector = np.random.normal(0, 0.1, self.dim) * (ub - lb)\n            particle += mutation_vector\n            particle = np.clip(particle, lb, ub)\n        return particle\n\n    def _update_particles(self, lb, ub):\n        for i in range(self.population_size):\n            if self.memory and np.random.rand() < 0.5:\n                strategy = self.memory[np.random.randint(len(self.memory))]\n                cognitive_component = strategy['phi_p'] * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.particles[i])\n                social_component = strategy['phi_g'] * np.random.rand(self.dim) * (self.global_best_position - self.particles[i])\n                inertia = strategy['omega']\n            else:\n                cognitive_component = self.phi_p * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.particles[i])\n                social_component = self.phi_g * np.random.rand(self.dim) * (self.global_best_position - self.particles[i])\n                inertia = self.omega\n\n            self.velocities[i] = inertia * self.velocities[i] + cognitive_component + social_component\n            self.particles[i] += self.velocities[i]\n            self.particles[i] = self._mutate_particle(self.particles[i], lb, ub)\n\n    def __call__(self, func):\n        self.lb, self.ub = func.bounds.lb, func.bounds.ub\n        self._initialize_population(self.lb, self.ub)\n\n        eval_count = 0\n        while eval_count < self.budget:\n            for i in range(self.population_size):\n                if eval_count >= self.budget:\n                    break\n\n                score = func(self.particles[i])\n                eval_count += 1\n\n                if score < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = score\n                    self.personal_best_positions[i] = self.particles[i]\n\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.particles[i]\n                    if len(self.memory) >= self.memory_size:\n                        self.memory.pop(0)\n                    self.memory.append({'omega': self.omega, 'phi_p': self.phi_p, 'phi_g': self.phi_g})\n                    self.omega = np.clip(self.omega + np.random.uniform(-0.05, 0.05), 0.3, 0.7)\n                    self.phi_p = np.clip(self.phi_p + np.random.uniform(-0.05, 0.05), 1.2, 1.8)\n                    self.phi_g = np.clip(self.phi_g + np.random.uniform(-0.05, 0.05), 1.2, 1.8)\n\n            self._update_particles(self.lb, self.ub)\n\n        return self.global_best_position, self.global_best_score", "name": "EnhancedAdaptiveMemorySearch", "description": "Enhanced Adaptive Memory Search (EAMS) introduces diversity-enhancing strategies by incorporating a mutation mechanism and dynamic memory adaptation to improve exploration capabilities and convergence speed.", "configspace": "", "generation": 35, "fitness": 0.873167501057301, "feedback": "The algorithm EnhancedAdaptiveMemorySearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.87 with standard deviation 0.01.", "error": "", "parent_id": "886d3ed4-4dda-4108-9bfe-eb231d760865", "metadata": {"aucs": [0.8624636699907035, 0.8838713321238987]}, "mutation_prompt": null}
{"id": "dcb25542-d279-44f9-8dc3-281702e7e9a8", "solution": "import numpy as np\n\nclass EnhancedAdaptiveMemorySearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20 + self.dim\n        self.memory_size = 5\n        self.particles = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_scores = None\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.omega = 0.5\n        self.phi_p = 1.5\n        self.phi_g = 1.5\n        self.memory = []\n        self.success_rate = 0.0\n        self.adapt_rate = 0.05\n        self.success_threshold = 0.3\n\n    def _initialize_population(self, lb, ub):\n        self.particles = np.random.rand(self.population_size, self.dim) * (ub - lb) + lb\n        self.velocities = np.random.rand(self.population_size, self.dim) * (ub - lb) / 10\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n\n    def _update_particles(self, lb, ub):\n        for i in range(self.population_size):\n            if self.memory and np.random.rand() < 0.5:\n                strategy = self.memory[np.random.randint(len(self.memory))]\n                cognitive_component = strategy['phi_p'] * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.particles[i])\n                social_component = strategy['phi_g'] * np.random.rand(self.dim) * (self.global_best_position - self.particles[i])\n                inertia = strategy['omega']\n            else:\n                cognitive_component = self.phi_p * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.particles[i])\n                social_component = self.phi_g * np.random.rand(self.dim) * (self.global_best_position - self.particles[i])\n                inertia = self.omega\n\n            self.velocities[i] = inertia * self.velocities[i] + cognitive_component + social_component\n            self.particles[i] += self.velocities[i]\n            self.particles[i] = np.clip(self.particles[i], lb, ub)\n\n    def __call__(self, func):\n        self.lb, self.ub = func.bounds.lb, func.bounds.ub\n        self._initialize_population(self.lb, self.ub)\n\n        eval_count = 0\n        successful_updates = 0\n        while eval_count < self.budget:\n            for i in range(self.population_size):\n                if eval_count >= self.budget:\n                    break\n\n                score = func(self.particles[i])\n                eval_count += 1\n\n                if score < self.personal_best_scores[i]:\n                    successful_updates += 1\n                    self.personal_best_scores[i] = score\n                    self.personal_best_positions[i] = self.particles[i]\n\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.particles[i]\n                    if len(self.memory) >= self.memory_size:\n                        self.memory.pop(0)\n                    self.memory.append({'omega': self.omega, 'phi_p': self.phi_p, 'phi_g': self.phi_g})\n                    self._adapt_parameters()\n\n            self.success_rate = successful_updates / self.population_size\n            successful_updates = 0\n            self._update_particles(self.lb, self.ub)\n\n        return self.global_best_position, self.global_best_score\n\n    def _adapt_parameters(self):\n        if self.success_rate > self.success_threshold:\n            self.omega = np.clip(self.omega - self.adapt_rate, 0.3, 0.7)\n            self.phi_p = np.clip(self.phi_p + self.adapt_rate, 1.2, 1.8)\n            self.phi_g = np.clip(self.phi_g + self.adapt_rate, 1.2, 1.8)\n        else:\n            self.omega = np.clip(self.omega + self.adapt_rate, 0.3, 0.7)\n            self.phi_p = np.clip(self.phi_p - self.adapt_rate, 1.2, 1.8)\n            self.phi_g = np.clip(self.phi_g - self.adapt_rate, 1.2, 1.8)", "name": "EnhancedAdaptiveMemorySearch", "description": "Enhanced Adaptive Memory Search (EAMS) integrates a dynamic adjustment of inertia and acceleration coefficients using a success-based adaptive strategy to improve convergence speed and accuracy.", "configspace": "", "generation": 36, "fitness": 0.9268507500153547, "feedback": "The algorithm EnhancedAdaptiveMemorySearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.93 with standard deviation 0.01.", "error": "", "parent_id": "886d3ed4-4dda-4108-9bfe-eb231d760865", "metadata": {"aucs": [0.9342106183516526, 0.9194908816790566]}, "mutation_prompt": null}
{"id": "8a646fa6-79a8-4a88-b551-6ee3cac5f1af", "solution": "import numpy as np\n\nclass DynamicStrategyLearning:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20 + self.dim\n        self.memory_size = 10  # Larger memory buffer\n        self.particles = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_scores = None\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.omega = 0.5  # Initial inertia weight\n        self.phi_p = 1.5  # Initial cognitive coefficient\n        self.phi_g = 1.5  # Initial social coefficient\n        self.memory = []  # Strategy memory\n        self.diversity_threshold = 0.1  # Threshold for diversity metric\n\n    def _initialize_population(self, lb, ub):\n        self.particles = np.random.rand(self.population_size, self.dim) * (ub - lb) + lb\n        self.velocities = np.random.rand(self.population_size, self.dim) * (ub - lb) / 10\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n\n    def _calculate_diversity(self):\n        if len(self.particles) < 2:\n            return 0\n        distances = np.linalg.norm(self.particles - np.mean(self.particles, axis=0), axis=1)\n        return np.std(distances) / np.mean(distances)\n\n    def _update_particles(self, lb, ub):\n        for i in range(self.population_size):\n            if self.memory and np.random.rand() < 0.5:\n                strategy = self.memory[np.random.randint(len(self.memory))]\n                cognitive_component = strategy['phi_p'] * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.particles[i])\n                social_component = strategy['phi_g'] * np.random.rand(self.dim) * (self.global_best_position - self.particles[i])\n                inertia = strategy['omega']\n            else:\n                cognitive_component = self.phi_p * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.particles[i])\n                social_component = self.phi_g * np.random.rand(self.dim) * (self.global_best_position - self.particles[i])\n                inertia = self.omega\n\n            self.velocities[i] = inertia * self.velocities[i] + cognitive_component + social_component\n            self.particles[i] += self.velocities[i]\n            self.particles[i] = np.clip(self.particles[i], lb, ub)\n\n    def __call__(self, func):\n        self.lb, self.ub = func.bounds.lb, func.bounds.ub\n        self._initialize_population(self.lb, self.ub)\n\n        eval_count = 0\n        while eval_count < self.budget:\n            diversity = self._calculate_diversity()\n            for i in range(self.population_size):\n                if eval_count >= self.budget:\n                    break\n\n                score = func(self.particles[i])\n                eval_count += 1\n\n                if score < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = score\n                    self.personal_best_positions[i] = self.particles[i]\n\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.particles[i]\n                    \n                    if len(self.memory) >= self.memory_size:\n                        self.memory.pop(0)\n                    self.memory.append({'omega': self.omega, 'phi_p': self.phi_p, 'phi_g': self.phi_g})\n                    \n                    if diversity > self.diversity_threshold:\n                        self.omega = np.clip(self.omega + np.random.uniform(-0.05, 0.05), 0.4, 0.9)\n                        self.phi_p = np.clip(self.phi_p + np.random.uniform(-0.05, 0.05), 1.4, 2.0)\n                        self.phi_g = np.clip(self.phi_g + np.random.uniform(-0.05, 0.05), 1.4, 2.0)\n                    else:\n                        self.omega = np.clip(self.omega + np.random.uniform(-0.1, 0.1), 0.3, 0.7)\n                        self.phi_p = np.clip(self.phi_p + np.random.uniform(-0.1, 0.1), 1.2, 1.8)\n                        self.phi_g = np.clip(self.phi_g + np.random.uniform(-0.1, 0.1), 1.2, 1.8)\n\n            self._update_particles(self.lb, self.ub)\n\n        return self.global_best_position, self.global_best_score", "name": "DynamicStrategyLearning", "description": "Dynamic Strategy Learning (DSL) enhances exploration by dynamically learning and adapting strategies based on performance and diversity metrics in the population.", "configspace": "", "generation": 37, "fitness": 0.863941210863875, "feedback": "The algorithm DynamicStrategyLearning got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.86 with standard deviation 0.09.", "error": "", "parent_id": "886d3ed4-4dda-4108-9bfe-eb231d760865", "metadata": {"aucs": [0.7755612105293549, 0.9523212111983951]}, "mutation_prompt": null}
{"id": "f58aee58-8226-48e8-8f8f-9da1ddeb34f0", "solution": "import numpy as np\n\nclass DynamicStrategyExploration:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20 + self.dim\n        self.memory_size = 5  # Number of successful strategies to remember\n        self.strategy_pool_size = 10  # Pool of diverse strategies\n        self.particles = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_scores = None\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.omega = 0.5  # Inertia weight\n        self.phi_p = 1.5  # Cognitive coefficient\n        self.phi_g = 1.5  # Social coefficient\n        self.memory = []  # Initialize adaptive memory\n        self.strategy_pool = self._initialize_strategy_pool()\n\n    def _initialize_strategy_pool(self):\n        pool = []\n        for _ in range(self.strategy_pool_size):\n            strategy = {\n                'omega': np.random.uniform(0.3, 0.7),\n                'phi_p': np.random.uniform(1.2, 1.8),\n                'phi_g': np.random.uniform(1.2, 1.8)\n            }\n            pool.append(strategy)\n        return pool\n\n    def _initialize_population(self, lb, ub):\n        self.particles = np.random.rand(self.population_size, self.dim) * (ub - lb) + lb\n        self.velocities = np.random.rand(self.population_size, self.dim) * (ub - lb) / 10\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n\n    def _update_particles(self, lb, ub):\n        for i in range(self.population_size):\n            # Choose a strategy from memory or explore the strategy pool\n            if self.memory and np.random.rand() < 0.5:\n                strategy = self.memory[np.random.randint(len(self.memory))]\n            else:\n                strategy = self.strategy_pool[np.random.randint(len(self.strategy_pool))]\n\n            cognitive_component = strategy['phi_p'] * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.particles[i])\n            social_component = strategy['phi_g'] * np.random.rand(self.dim) * (self.global_best_position - self.particles[i])\n            inertia = strategy['omega']\n\n            self.velocities[i] = inertia * self.velocities[i] + cognitive_component + social_component\n            perturbation = 0.05 * np.random.randn(self.dim)  # Small random perturbation for exploration\n            self.particles[i] = np.clip(self.particles[i] + self.velocities[i] + perturbation, lb, ub)\n\n    def __call__(self, func):\n        self.lb, self.ub = func.bounds.lb, func.bounds.ub\n        self._initialize_population(self.lb, self.ub)\n\n        eval_count = 0\n        while eval_count < self.budget:\n            for i in range(self.population_size):\n                if eval_count >= self.budget:\n                    break\n\n                score = func(self.particles[i])\n                eval_count += 1\n\n                if score < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = score\n                    self.personal_best_positions[i] = self.particles[i]\n\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.particles[i]\n                    # Add successful strategy to memory\n                    if len(self.memory) >= self.memory_size:\n                        self.memory.pop(0)  # Remove oldest strategy if memory is full\n                    self.memory.append({'omega': self.omega, 'phi_p': self.phi_p, 'phi_g': self.phi_g})\n                    # Adapt parameters slightly and update strategy pool\n                    self.omega = np.clip(self.omega + np.random.uniform(-0.1, 0.1), 0.3, 0.7)\n                    self.phi_p = np.clip(self.phi_p + np.random.uniform(-0.1, 0.1), 1.2, 1.8)\n                    self.phi_g = np.clip(self.phi_g + np.random.uniform(-0.1, 0.1), 1.2, 1.8)\n                    self.strategy_pool[np.random.randint(self.strategy_pool_size)] = {'omega': self.omega, 'phi_p': self.phi_p, 'phi_g': self.phi_g}\n\n            self._update_particles(self.lb, self.ub)\n\n        return self.global_best_position, self.global_best_score", "name": "DynamicStrategyExploration", "description": "Dynamic Strategy Exploration (DSE) enhances Adaptive Memory Search by incorporating a dynamic strategy pool with diversity-driven exploration and personal-best perturbation to improve convergence and robustness.", "configspace": "", "generation": 38, "fitness": 0.8777991762693711, "feedback": "The algorithm DynamicStrategyExploration got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.88 with standard deviation 0.02.", "error": "", "parent_id": "886d3ed4-4dda-4108-9bfe-eb231d760865", "metadata": {"aucs": [0.8606411126569659, 0.8949572398817762]}, "mutation_prompt": null}
{"id": "02acac84-3434-4526-b97f-7b3074868c21", "solution": "import numpy as np\n\nclass QuantumInspiredAdaptiveSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20 + self.dim\n        self.memory_size = 5\n        self.particles = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_scores = None\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.omega = 0.5\n        self.phi_p = 1.5\n        self.phi_g = 1.5\n        self.amplitude = 1.0  # Amplitude for quantum effect\n        self.memory = []\n\n    def _initialize_population(self, lb, ub):\n        self.particles = np.random.rand(self.population_size, self.dim) * (ub - lb) + lb\n        self.velocities = np.random.rand(self.population_size, self.dim) * (ub - lb) / 10\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n\n    def _quantum_superposition(self):\n        # Quantum superposition to adjust particle positions\n        phase_shift = np.random.rand(self.population_size, self.dim) - 0.5\n        self.particles += self.amplitude * (self.ub - self.lb) * phase_shift\n\n    def _update_particles(self, lb, ub):\n        for i in range(self.population_size):\n            if self.memory and np.random.rand() < 0.5:\n                strategy = self.memory[np.random.randint(len(self.memory))]\n                cognitive_component = strategy['phi_p'] * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.particles[i])\n                social_component = strategy['phi_g'] * np.random.rand(self.dim) * (self.global_best_position - self.particles[i])\n                inertia = strategy['omega']\n            else:\n                cognitive_component = self.phi_p * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.particles[i])\n                social_component = self.phi_g * np.random.rand(self.dim) * (self.global_best_position - self.particles[i])\n                inertia = self.omega\n\n            self.velocities[i] = inertia * self.velocities[i] + cognitive_component + social_component\n            self.particles[i] += self.velocities[i]\n            self.particles[i] = np.clip(self.particles[i], lb, ub)\n\n    def __call__(self, func):\n        self.lb, self.ub = func.bounds.lb, func.bounds.ub\n        self._initialize_population(self.lb, self.ub)\n\n        eval_count = 0\n        while eval_count < self.budget:\n            self._quantum_superposition()  # Apply quantum superposition\n\n            for i in range(self.population_size):\n                if eval_count >= self.budget:\n                    break\n\n                score = func(self.particles[i])\n                eval_count += 1\n\n                if score < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = score\n                    self.personal_best_positions[i] = self.particles[i]\n\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.particles[i]\n                    if len(self.memory) >= self.memory_size:\n                        self.memory.pop(0)\n                    self.memory.append({'omega': self.omega, 'phi_p': self.phi_p, 'phi_g': self.phi_g})\n                    self.omega = np.clip(self.omega + np.random.uniform(-0.1, 0.1), 0.3, 0.7)\n                    self.phi_p = np.clip(self.phi_p + np.random.uniform(-0.1, 0.1), 1.2, 1.8)\n                    self.phi_g = np.clip(self.phi_g + np.random.uniform(-0.1, 0.1), 1.2, 1.8)\n                    self.amplitude = np.clip(self.amplitude + np.random.uniform(-0.05, 0.05), 0.9, 1.1)  # Adjust amplitude\n\n            self._update_particles(self.lb, self.ub)\n\n        return self.global_best_position, self.global_best_score", "name": "QuantumInspiredAdaptiveSearch", "description": "Quantum-Inspired Adaptive Search (QIAS) leverages quantum superposition principles to explore a wider search space by dynamically adjusting search amplitudes for enhanced global and local search strategies.", "configspace": "", "generation": 39, "fitness": 0.630482318792938, "feedback": "The algorithm QuantumInspiredAdaptiveSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.63 with standard deviation 0.01.", "error": "", "parent_id": "886d3ed4-4dda-4108-9bfe-eb231d760865", "metadata": {"aucs": [0.6383896626767434, 0.6225749749091328]}, "mutation_prompt": null}
{"id": "62e02a0f-488d-446c-9cfb-8358c3610b4b", "solution": "import numpy as np\n\nclass QuantumInspiredAdaptiveMemorySearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20 + dim\n        self.memory_size = 5\n        self.particles = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_scores = None\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.omega = 0.5\n        self.phi_p = 1.5\n        self.phi_g = 1.5\n        self.memory = []\n\n    def _initialize_population(self, lb, ub):\n        self.particles = np.random.rand(self.population_size, self.dim) * (ub - lb) + lb\n        self.velocities = np.random.rand(self.population_size, self.dim) * (ub - lb) / 10\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n\n    def _quantum_superposition(self, lb, ub):\n        quantum_states = np.random.choice([0, 1], size=(self.population_size, self.dim))\n        return lb + quantum_states * (ub - lb)\n\n    def _update_particles(self, lb, ub):\n        for i in range(self.population_size):\n            if self.memory and np.random.rand() < 0.5:\n                strategy = self.memory[np.random.randint(len(self.memory))]\n                cognitive_component = strategy['phi_p'] * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.particles[i])\n                social_component = strategy['phi_g'] * np.random.rand(self.dim) * (self.global_best_position - self.particles[i])\n                inertia = strategy['omega']\n            else:\n                cognitive_component = self.phi_p * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.particles[i])\n                social_component = self.phi_g * np.random.rand(self.dim) * (self.global_best_position - self.particles[i])\n                inertia = self.omega\n\n            self.velocities[i] = inertia * self.velocities[i] + cognitive_component + social_component\n            self.particles[i] += self.velocities[i]\n            self.particles[i] = np.clip(self.particles[i], lb, ub)\n\n    def __call__(self, func):\n        self.lb, self.ub = func.bounds.lb, func.bounds.ub\n        self._initialize_population(self.lb, self.ub)\n\n        eval_count = 0\n        while eval_count < self.budget:\n            if np.random.rand() < 0.1:\n                self.particles = self._quantum_superposition(self.lb, self.ub)\n\n            for i in range(self.population_size):\n                if eval_count >= self.budget:\n                    break\n\n                score = func(self.particles[i])\n                eval_count += 1\n\n                if score < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = score\n                    self.personal_best_positions[i] = self.particles[i]\n\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.particles[i]\n                    if len(self.memory) >= self.memory_size:\n                        self.memory.pop(0)\n                    self.memory.append({'omega': self.omega, 'phi_p': self.phi_p, 'phi_g': self.phi_g})\n                    self.omega = np.clip(self.omega + np.random.uniform(-0.1, 0.1), 0.3, 0.7)\n                    self.phi_p = np.clip(self.phi_p + np.random.uniform(-0.1, 0.1), 1.2, 1.8)\n                    self.phi_g = np.clip(self.phi_g + np.random.uniform(-0.1, 0.1), 1.2, 1.8)\n\n            self._update_particles(self.lb, self.ub)\n\n        return self.global_best_position, self.global_best_score", "name": "QuantumInspiredAdaptiveMemorySearch", "description": "Quantum-Inspired Adaptive Memory Search (QAMS) leverages quantum-inspired superposition states to enhance the exploration capabilities while retaining adaptive memory for dynamic parameter adjustments.", "configspace": "", "generation": 40, "fitness": 0.9389340164890498, "feedback": "The algorithm QuantumInspiredAdaptiveMemorySearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.94 with standard deviation 0.00.", "error": "", "parent_id": "886d3ed4-4dda-4108-9bfe-eb231d760865", "metadata": {"aucs": [0.9400554961688289, 0.9378125368092709]}, "mutation_prompt": null}
{"id": "166c73c6-7d75-4942-bb8d-8c39d6e633e3", "solution": "import numpy as np\n\nclass QuantumInspiredEvolutionaryAlgorithm:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20 + self.dim\n        self.chromosomes = None\n        self.best_chromosome = None\n        self.best_score = np.inf\n        self.alpha = np.pi / 4  # Rotation angle for quantum gates\n\n    def _initialize_chromosomes(self, lb, ub):\n        self.chromosomes = np.random.rand(self.population_size, self.dim) * (ub - lb) + lb\n    \n    def _quantum_rotation(self, chromosome, best_chromosome):\n        for i in range(self.dim):\n            if np.random.rand() < 0.5:\n                theta = self.alpha if chromosome[i] < best_chromosome[i] else -self.alpha\n                chromosome[i] = chromosome[i] + theta * (best_chromosome[i] - chromosome[i])\n        return chromosome\n\n    def __call__(self, func):\n        self.lb, self.ub = func.bounds.lb, func.bounds.ub\n        self._initialize_chromosomes(self.lb, self.ub)\n\n        eval_count = 0\n        while eval_count < self.budget:\n            for i in range(self.population_size):\n                if eval_count >= self.budget:\n                    break\n\n                score = func(self.chromosomes[i])\n                eval_count += 1\n\n                if score < self.best_score:\n                    self.best_score = score\n                    self.best_chromosome = self.chromosomes[i].copy()\n\n            # Perform quantum rotation for exploration and exploitation\n            for i in range(self.population_size):\n                self.chromosomes[i] = self._quantum_rotation(self.chromosomes[i], self.best_chromosome)\n                self.chromosomes[i] = np.clip(self.chromosomes[i], self.lb, self.ub)\n\n        return self.best_chromosome, self.best_score", "name": "QuantumInspiredEvolutionaryAlgorithm", "description": "Quantum-inspired Evolutionary Algorithm (QEA) employs principles of quantum computing, such as superposition and quantum rotation, to enhance exploration and exploitation in optimization.", "configspace": "", "generation": 41, "fitness": 0.46570854952631174, "feedback": "The algorithm QuantumInspiredEvolutionaryAlgorithm got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.47 with standard deviation 0.02.", "error": "", "parent_id": "886d3ed4-4dda-4108-9bfe-eb231d760865", "metadata": {"aucs": [0.44977524561773985, 0.48164185343488364]}, "mutation_prompt": null}
{"id": "e06f4a2c-e4be-4a73-a3ab-d74b8eef4c66", "solution": "import numpy as np\n\nclass QuantumInspiredAdaptiveMemorySearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20 + self.dim\n        self.memory_size = 5\n        self.particles = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_scores = None\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.omega = 0.5\n        self.phi_p = 1.5\n        self.phi_g = 1.5\n        self.memory = []\n        self.alpha = 0.1  # Quantum-inspired rotation angle\n\n    def _initialize_population(self, lb, ub):\n        self.particles = np.random.rand(self.population_size, self.dim) * (ub - lb) + lb\n        self.velocities = np.random.rand(self.population_size, self.dim) * (ub - lb) / 10\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n\n    def _quantum_rotation(self, position, local_best, global_best):\n        theta = np.random.uniform(-self.alpha, self.alpha, self.dim)\n        rotated_position = (np.cos(theta) * (local_best - position) +\n                            np.sin(theta) * (global_best - position))\n        return position + rotated_position\n\n    def _update_particles(self, lb, ub):\n        for i in range(self.population_size):\n            if self.memory and np.random.rand() < 0.5:\n                strategy = self.memory[np.random.randint(len(self.memory))]\n                cognitive_component = strategy['phi_p'] * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.particles[i])\n                social_component = strategy['phi_g'] * np.random.rand(self.dim) * (self.global_best_position - self.particles[i])\n                inertia = strategy['omega']\n            else:\n                cognitive_component = self.phi_p * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.particles[i])\n                social_component = self.phi_g * np.random.rand(self.dim) * (self.global_best_position - self.particles[i])\n                inertia = self.omega\n\n            self.velocities[i] = inertia * self.velocities[i] + cognitive_component + social_component\n            self.particles[i] = self._quantum_rotation(self.particles[i], self.personal_best_positions[i], self.global_best_position)\n            self.particles[i] = np.clip(self.particles[i], lb, ub)\n\n    def __call__(self, func):\n        self.lb, self.ub = func.bounds.lb, func.bounds.ub\n        self._initialize_population(self.lb, self.ub)\n\n        eval_count = 0\n        while eval_count < self.budget:\n            for i in range(self.population_size):\n                if eval_count >= self.budget:\n                    break\n\n                score = func(self.particles[i])\n                eval_count += 1\n\n                if score < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = score\n                    self.personal_best_positions[i] = self.particles[i]\n\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.particles[i]\n                    if len(self.memory) >= self.memory_size:\n                        self.memory.pop(0)\n                    self.memory.append({'omega': self.omega, 'phi_p': self.phi_p, 'phi_g': self.phi_g})\n                    self.omega = np.clip(self.omega + np.random.uniform(-0.1, 0.1), 0.3, 0.7)\n                    self.phi_p = np.clip(self.phi_p + np.random.uniform(-0.1, 0.1), 1.2, 1.8)\n                    self.phi_g = np.clip(self.phi_g + np.random.uniform(-0.1, 0.1), 1.2, 1.8)\n\n            self._update_particles(self.lb, self.ub)\n\n        return self.global_best_position, self.global_best_score", "name": "QuantumInspiredAdaptiveMemorySearch", "description": "Quantum-Inspired Adaptive Memory Search (QIAMS) enhances the adaptive memory mechanism by embedding quantum-inspired operators for improved exploration and exploitation balance.", "configspace": "", "generation": 42, "fitness": 0.6267346459228165, "feedback": "The algorithm QuantumInspiredAdaptiveMemorySearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.63 with standard deviation 0.00.", "error": "", "parent_id": "886d3ed4-4dda-4108-9bfe-eb231d760865", "metadata": {"aucs": [0.6241190268895634, 0.6293502649560695]}, "mutation_prompt": null}
{"id": "36d54311-3d35-459b-8d05-7988b002a0b0", "solution": "import numpy as np\n\nclass EnhancedAdaptiveMemorySearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20 + self.dim\n        self.memory_size = 5\n        self.particles = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_scores = None\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.omega = 0.5\n        self.phi_p = 1.5\n        self.phi_g = 1.5\n        self.memory = []\n        self.diversification_factor = 0.1\n\n    def _initialize_population(self, lb, ub):\n        self.particles = np.random.rand(self.population_size, self.dim) * (ub - lb) + lb\n        self.velocities = np.zeros((self.population_size, self.dim))\n        for i in range(self.population_size):\n            self.velocities[i] = self.diversification_factor * (np.random.rand(self.dim) * (ub - lb))\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n\n    def _update_particles(self, lb, ub):\n        for i in range(self.population_size):\n            if self.memory and np.random.rand() < 0.5:\n                strategy = self.memory[np.random.randint(len(self.memory))]\n                cognitive_component = strategy['phi_p'] * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.particles[i])\n                social_component = strategy['phi_g'] * np.random.rand(self.dim) * (self.global_best_position - self.particles[i])\n                inertia = strategy['omega']\n            else:\n                cognitive_component = self.phi_p * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.particles[i])\n                social_component = self.phi_g * np.random.rand(self.dim) * (self.global_best_position - self.particles[i])\n                inertia = self.omega\n\n            self.velocities[i] = inertia * self.velocities[i] + cognitive_component + social_component\n            adaptive_velocity = self.diversification_factor * (np.random.rand(self.dim) * (ub - lb))\n            self.velocities[i] = (self.velocities[i] + adaptive_velocity) / 2\n            self.particles[i] += self.velocities[i]\n            self.particles[i] = np.clip(self.particles[i], lb, ub)\n\n    def __call__(self, func):\n        self.lb, self.ub = func.bounds.lb, func.bounds.ub\n        self._initialize_population(self.lb, self.ub)\n\n        eval_count = 0\n        while eval_count < self.budget:\n            for i in range(self.population_size):\n                if eval_count >= self.budget:\n                    break\n\n                score = func(self.particles[i])\n                eval_count += 1\n\n                if score < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = score\n                    self.personal_best_positions[i] = self.particles[i]\n\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.particles[i]\n                    if len(self.memory) >= self.memory_size:\n                        self.memory.pop(0)\n                    self.memory.append({'omega': self.omega, 'phi_p': self.phi_p, 'phi_g': self.phi_g})\n                    self.omega = np.clip(self.omega + np.random.uniform(-0.1, 0.1), 0.3, 0.7)\n                    self.phi_p = np.clip(self.phi_p + np.random.uniform(-0.1, 0.1), 1.2, 1.8)\n                    self.phi_g = np.clip(self.phi_g + np.random.uniform(-0.1, 0.1), 1.2, 1.8)\n\n            self._update_particles(self.lb, self.ub)\n\n        return self.global_best_position, self.global_best_score", "name": "EnhancedAdaptiveMemorySearch", "description": "Enhanced Adaptive Memory Search (EAMS) introduces dynamic velocity adjustment and diversified memory strategies to further balance exploration and exploitation for improved convergence.", "configspace": "", "generation": 43, "fitness": 0.590650304525004, "feedback": "The algorithm EnhancedAdaptiveMemorySearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.59 with standard deviation 0.03.", "error": "", "parent_id": "886d3ed4-4dda-4108-9bfe-eb231d760865", "metadata": {"aucs": [0.5567631356394958, 0.6245374734105122]}, "mutation_prompt": null}
{"id": "9677af89-78a4-42b3-9e44-bb14b583c84d", "solution": "import numpy as np\n\nclass QuantumInspiredEvolutionaryAlgorithm:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20 + self.dim\n        self.qubit_population = None\n        self.observed_population = None\n        self.best_solution = None\n        self.best_score = np.inf\n        self.alpha = 0.5  # Quantum rotation angle coefficient\n\n    def _initialize_population(self):\n        self.qubit_population = np.random.rand(self.population_size, self.dim, 2)\n        self.qubit_population /= np.linalg.norm(self.qubit_population, axis=2, keepdims=True)\n        self.observed_population = np.zeros((self.population_size, self.dim))\n\n    def _observe_population(self):\n        for i in range(self.population_size):\n            for j in range(self.dim):\n                prob_0 = self.qubit_population[i, j, 0]**2\n                self.observed_population[i, j] = 0 if np.random.rand() < prob_0 else 1\n\n    def _update_qubits(self, best_qubit):\n        for i in range(self.population_size):\n            for j in range(self.dim):\n                delta_theta = self.alpha * (best_qubit[j] - self.qubit_population[i, j])\n                rotation_matrix = np.array([[np.cos(delta_theta), -np.sin(delta_theta)],\n                                            [np.sin(delta_theta), np.cos(delta_theta)]])\n                self.qubit_population[i, j] = np.dot(rotation_matrix, self.qubit_population[i, j])\n                self.qubit_population[i, j] /= np.linalg.norm(self.qubit_population[i, j])\n\n    def __call__(self, func):\n        self.lb, self.ub = func.bounds.lb, func.bounds.ub\n        self._initialize_population()\n\n        eval_count = 0\n        while eval_count < self.budget:\n            self._observe_population()\n            for i in range(self.population_size):\n                if eval_count >= self.budget:\n                    break\n\n                candidate_solution = self.lb + (self.ub - self.lb) * self.observed_population[i]\n                score = func(candidate_solution)\n                eval_count += 1\n\n                if score < self.best_score:\n                    self.best_score = score\n                    self.best_solution = candidate_solution\n\n            best_qubit = self.qubit_population[np.argmin([func(self.lb + (self.ub - self.lb) * self.observed_population[i]) for i in range(self.population_size)])]\n            self._update_qubits(best_qubit)\n\n        return self.best_solution, self.best_score", "name": "QuantumInspiredEvolutionaryAlgorithm", "description": "Quantum-Inspired Evolutionary Algorithm (QIEA) utilizes quantum principles to explore the search space using qubit representation and probabilistic gate-based evolution, enhancing diversity and convergence.", "configspace": "", "generation": 44, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('could not broadcast input array from shape (2,2) into shape (2,)').", "error": "ValueError('could not broadcast input array from shape (2,2) into shape (2,)')", "parent_id": "886d3ed4-4dda-4108-9bfe-eb231d760865", "metadata": {}, "mutation_prompt": null}
{"id": "bae86734-b0e0-4813-a68b-569ff0da6ee7", "solution": "import numpy as np\n\nclass QuantumInspiredEvolutionarySearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20 + self.dim\n        self.q_population = np.random.rand(self.population_size, self.dim) - 0.5  # Quantum bits: range [-0.5, 0.5]\n        self.solutions = np.zeros((self.population_size, self.dim))\n        self.best_solution = None\n        self.best_score = np.inf\n\n    def _quantum_collapse(self, q_bit):\n        return np.sign(q_bit) * np.floor(np.abs(q_bit) * 2)  # collapse to discrete {-1, 0, 1}\n\n    def _initialize_population(self, lb, ub):\n        for i in range(self.population_size):\n            self.solutions[i] = (ub - lb) * (np.random.rand(self.dim)) + lb\n\n    def _update_population(self, lb, ub):\n        for i in range(self.population_size):\n            q_bits = self.q_population[i] + np.random.normal(0, 0.1, self.dim)\n            solution = self._quantum_collapse(q_bits)\n            self.solutions[i] = (ub - lb) * (solution + 0.5) + lb\n            self.solutions[i] = np.clip(self.solutions[i], lb, ub)\n            self.q_population[i] = q_bits\n\n    def __call__(self, func):\n        self.lb, self.ub = func.bounds.lb, func.bounds.ub\n        self._initialize_population(self.lb, self.ub)\n\n        eval_count = 0\n        while eval_count < self.budget:\n            for i in range(self.population_size):\n                if eval_count >= self.budget:\n                    break\n\n                score = func(self.solutions[i])\n                eval_count += 1\n\n                if score < self.best_score:\n                    self.best_score = score\n                    self.best_solution = self.solutions[i]\n\n            self._update_population(self.lb, self.ub)\n\n        return self.best_solution, self.best_score", "name": "QuantumInspiredEvolutionarySearch", "description": "Quantum-Inspired Evolutionary Search (QIES) utilizes quantum superposition principles to create diverse candidate solutions by encoding them as probability amplitudes, enhancing exploration capabilities in high-dimensional search spaces.", "configspace": "", "generation": 45, "fitness": 0.5005884464535587, "feedback": "The algorithm QuantumInspiredEvolutionarySearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.50 with standard deviation 0.02.", "error": "", "parent_id": "886d3ed4-4dda-4108-9bfe-eb231d760865", "metadata": {"aucs": [0.48543496523142093, 0.5157419276756965]}, "mutation_prompt": null}
{"id": "f962a97f-decf-4206-9292-a7008d405647", "solution": "import numpy as np\n\nclass QuantumInspiredEvolutionaryAlgorithm:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20 + self.dim\n        self.q_population = np.random.uniform(0, np.pi, (self.population_size, self.dim))  # Quantum population using angles\n        self.best_solution = None\n        self.best_score = np.inf\n        self.alpha = 0.02  # Learning rate for quantum rotation\n\n    def _qbit_rotation(self, q, best):\n        # Rotate qbits towards the best solution\n        delta_theta = self.alpha * (best - q)\n        q_new = q + delta_theta\n        q_new = np.clip(q_new, 0, np.pi)  # Keep angles within bounds\n        return q_new\n\n    def _measure_population(self, q_population, lb, ub):\n        # Measure quantum population to get classical solutions\n        population = np.cos(q_population) ** 2 * (ub - lb) + lb\n        return population\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        eval_count = 0\n\n        while eval_count < self.budget:\n            # Measure the quantum population to get classical solutions\n            population = self._measure_population(self.q_population, lb, ub)\n\n            for i in range(self.population_size):\n                if eval_count >= self.budget:\n                    break\n\n                score = func(population[i])\n                eval_count += 1\n\n                if score < self.best_score:\n                    self.best_score = score\n                    self.best_solution = np.copy(population[i])\n\n            # Update the quantum population based on the best solution\n            for i in range(self.population_size):\n                self.q_population[i] = self._qbit_rotation(self.q_population[i], self.best_solution)\n\n        return self.best_solution, self.best_score", "name": "QuantumInspiredEvolutionaryAlgorithm", "description": "Quantum-Inspired Evolutionary Algorithm (QIEA) utilizes quantum bits and superposition to efficiently explore the search space, enhancing diversity and convergence in solving complex optimization problems.", "configspace": "", "generation": 46, "fitness": 0.4581130675293924, "feedback": "The algorithm QuantumInspiredEvolutionaryAlgorithm got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.46 with standard deviation 0.02.", "error": "", "parent_id": "886d3ed4-4dda-4108-9bfe-eb231d760865", "metadata": {"aucs": [0.4371126840937666, 0.47911345096501823]}, "mutation_prompt": null}
{"id": "feaf160b-385e-424c-8dd0-2bebe05da04d", "solution": "import numpy as np\n\nclass DynamicAdaptiveMemorySearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20 + self.dim\n        self.memory_size = 5\n        self.particles = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_scores = None\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.omega = 0.5  # Initial inertia weight\n        self.phi_p = 1.5  # Initial cognitive coefficient\n        self.phi_g = 1.5  # Initial social coefficient\n        self.memory = []\n        self.performance_trend = []  # Track recent performance improvements\n\n    def _initialize_population(self, lb, ub):\n        self.particles = np.random.rand(self.population_size, self.dim) * (ub - lb) + lb\n        self.velocities = np.random.rand(self.population_size, self.dim) * (ub - lb) / 10\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n\n    def _update_particles(self, lb, ub):\n        for i in range(self.population_size):\n            if self.memory and np.random.rand() < 0.5:\n                strategy = self.memory[np.random.randint(len(self.memory))]\n                cognitive_component = strategy['phi_p'] * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.particles[i])\n                social_component = strategy['phi_g'] * np.random.rand(self.dim) * (self.global_best_position - self.particles[i])\n                inertia = strategy['omega']\n            else:\n                cognitive_component = self.phi_p * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.particles[i])\n                social_component = self.phi_g * np.random.rand(self.dim) * (self.global_best_position - self.particles[i])\n                inertia = self.omega\n\n            self.velocities[i] = inertia * self.velocities[i] + cognitive_component + social_component\n            self.particles[i] += self.velocities[i]\n            self.particles[i] = np.clip(self.particles[i], lb, ub)\n\n    def __call__(self, func):\n        self.lb, self.ub = func.bounds.lb, func.bounds.ub\n        self._initialize_population(self.lb, self.ub)\n\n        eval_count = 0\n        while eval_count < self.budget:\n            for i in range(self.population_size):\n                if eval_count >= self.budget:\n                    break\n\n                score = func(self.particles[i])\n                eval_count += 1\n\n                if score < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = score\n                    self.personal_best_positions[i] = self.particles[i]\n\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.particles[i]\n                    if len(self.memory) >= self.memory_size:\n                        self.memory.pop(0)\n                    self.memory.append({'omega': self.omega, 'phi_p': self.phi_p, 'phi_g': self.phi_g})\n\n                    self.performance_trend.append(score)\n                    if len(self.performance_trend) > 10:\n                        self.performance_trend.pop(0)\n\n                    # Dynamic adaptation based on performance trend\n                    if len(self.performance_trend) > 1:\n                        trend = np.mean(np.diff(self.performance_trend))\n                        if trend < 0:\n                            self.omega = np.clip(self.omega + np.random.uniform(-0.05, 0.05), 0.3, 0.7)\n                            self.phi_p = np.clip(self.phi_p + np.random.uniform(-0.05, 0.05), 1.2, 1.8)\n                            self.phi_g = np.clip(self.phi_g + np.random.uniform(-0.05, 0.05), 1.2, 1.8)\n\n            self._update_particles(self.lb, self.ub)\n\n        return self.global_best_position, self.global_best_score", "name": "DynamicAdaptiveMemorySearch", "description": "Dynamic Adaptive Memory Search (DAMS) utilizes a dynamic adaptation of the inertia and learning coefficients based on performance trends, enhancing both convergence speed and global search capabilities.", "configspace": "", "generation": 47, "fitness": 0.9452958663016537, "feedback": "The algorithm DynamicAdaptiveMemorySearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.95 with standard deviation 0.00.", "error": "", "parent_id": "886d3ed4-4dda-4108-9bfe-eb231d760865", "metadata": {"aucs": [0.9439449335998144, 0.9466467990034931]}, "mutation_prompt": null}
{"id": "486d23b6-cda4-4dc7-9a10-7f58c01f15b3", "solution": "import numpy as np\n\nclass QuantumInspiredPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20 + self.dim\n        self.particles = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_scores = None\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.omega = 0.5  # Inertia weight\n        self.phi_p = 1.5  # Cognitive coefficient\n        self.phi_g = 1.5  # Social coefficient\n        self.alpha = 0.1  # Quantum parameter\n\n    def _initialize_population(self, lb, ub):\n        self.particles = np.random.rand(self.population_size, self.dim) * (ub - lb) + lb\n        self.velocities = np.random.rand(self.population_size, self.dim) * (ub - lb) / 10\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n\n    def _update_particles(self, lb, ub):\n        for i in range(self.population_size):\n            r_p = np.random.rand(self.dim)\n            r_g = np.random.rand(self.dim)\n            cognitive_component = self.phi_p * r_p * (self.personal_best_positions[i] - self.particles[i])\n            social_component = self.phi_g * r_g * (self.global_best_position - self.particles[i])\n            quantum_component = self.alpha * np.random.rand(self.dim) * (self.global_best_position - np.random.rand(self.dim) * (ub - lb))\n\n            self.velocities[i] = self.omega * self.velocities[i] + cognitive_component + social_component + quantum_component\n            self.particles[i] += self.velocities[i]\n            self.particles[i] = np.clip(self.particles[i], lb, ub)\n\n    def __call__(self, func):\n        self.lb, self.ub = func.bounds.lb, func.bounds.ub\n        self._initialize_population(self.lb, self.ub)\n\n        eval_count = 0\n        while eval_count < self.budget:\n            for i in range(self.population_size):\n                if eval_count >= self.budget:\n                    break\n\n                score = func(self.particles[i])\n                eval_count += 1\n\n                if score < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = score\n                    self.personal_best_positions[i] = self.particles[i]\n\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.particles[i]\n\n            self._update_particles(self.lb, self.ub)\n\n        return self.global_best_position, self.global_best_score", "name": "QuantumInspiredPSO", "description": "Quantum-Inspired Particle Swarm Optimization (QIPSO) harnesses quantum mechanics principles to enhance exploration, utilizing quantum superposition states and probabilistic particle updates for efficient global optimization.", "configspace": "", "generation": 48, "fitness": 0.7206059112015499, "feedback": "The algorithm QuantumInspiredPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.72 with standard deviation 0.00.", "error": "", "parent_id": "886d3ed4-4dda-4108-9bfe-eb231d760865", "metadata": {"aucs": [0.7182576990754788, 0.722954123327621]}, "mutation_prompt": null}
{"id": "9a4d4e04-bc6f-47a1-ada8-b5e42847904d", "solution": "import numpy as np\n\nclass EnhancedAdaptiveMemorySearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20 + self.dim\n        self.memory_size = 5\n        self.particles = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_scores = None\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.omega = 0.5\n        self.phi_p = 1.5\n        self.phi_g = 1.5\n        self.cross_prob = 0.9  # Crossover probability\n        self.mutation_prob = 0.1  # Mutation probability\n        self.memory = []\n\n    def _initialize_population(self, lb, ub):\n        self.particles = np.random.rand(self.population_size, self.dim) * (ub - lb) + lb\n        self.velocities = np.random.rand(self.population_size, self.dim) * (ub - lb) / 10\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n\n    def _update_particles(self, lb, ub):\n        for i in range(self.population_size):\n            if self.memory and np.random.rand() < 0.5:\n                strategy = self.memory[np.random.randint(len(self.memory))]\n                cognitive_component = strategy['phi_p'] * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.particles[i])\n                social_component = strategy['phi_g'] * np.random.rand(self.dim) * (self.global_best_position - self.particles[i])\n                inertia = strategy['omega']\n            else:\n                cognitive_component = self.phi_p * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.particles[i])\n                social_component = self.phi_g * np.random.rand(self.dim) * (self.global_best_position - self.particles[i])\n                inertia = self.omega\n\n            self.velocities[i] = inertia * self.velocities[i] + cognitive_component + social_component\n            self.particles[i] += self.velocities[i]\n            self.particles[i] = np.clip(self.particles[i], lb, ub)\n\n            # Crossover and mutation\n            if np.random.rand() < self.cross_prob:\n                partner_index = np.random.randint(self.population_size)\n                cross_point = np.random.randint(1, self.dim)\n                self.particles[i][:cross_point] = self.personal_best_positions[partner_index][:cross_point]\n            \n            if np.random.rand() < self.mutation_prob:\n                mutation_vector = np.random.standard_normal(self.dim)\n                mutation_strength = (ub - lb) * 0.1\n                self.particles[i] += mutation_strength * mutation_vector\n                self.particles[i] = np.clip(self.particles[i], lb, ub)\n\n    def __call__(self, func):\n        self.lb, self.ub = func.bounds.lb, func.bounds.ub\n        self._initialize_population(self.lb, self.ub)\n\n        eval_count = 0\n        while eval_count < self.budget:\n            for i in range(self.population_size):\n                if eval_count >= self.budget:\n                    break\n\n                score = func(self.particles[i])\n                eval_count += 1\n\n                if score < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = score\n                    self.personal_best_positions[i] = self.particles[i]\n\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.particles[i]\n                    if len(self.memory) >= self.memory_size:\n                        self.memory.pop(0)\n                    self.memory.append({'omega': self.omega, 'phi_p': self.phi_p, 'phi_g': self.phi_g})\n                    self.omega = np.clip(self.omega + np.random.uniform(-0.1, 0.1), 0.3, 0.7)\n                    self.phi_p = np.clip(self.phi_p + np.random.uniform(-0.1, 0.1), 1.2, 1.8)\n                    self.phi_g = np.clip(self.phi_g + np.random.uniform(-0.1, 0.1), 1.2, 1.8)\n\n            self._update_particles(self.lb, self.ub)\n\n        return self.global_best_position, self.global_best_score", "name": "EnhancedAdaptiveMemorySearch", "description": "Enhanced Adaptive Memory Search (EAMS) uses adaptive hybridization of strategies by incorporating mutation and crossover from evolutionary algorithms to further improve exploration and exploitation balance.", "configspace": "", "generation": 49, "fitness": 0.8574888646352002, "feedback": "The algorithm EnhancedAdaptiveMemorySearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.86 with standard deviation 0.05.", "error": "", "parent_id": "886d3ed4-4dda-4108-9bfe-eb231d760865", "metadata": {"aucs": [0.8099919266174825, 0.904985802652918]}, "mutation_prompt": null}
{"id": "5c2df305-98a9-4dd7-9794-cbf54d391879", "solution": "import numpy as np\n\nclass QuantumInspiredEvolutionaryAlgorithm:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20 + self.dim\n        self.q_population = np.random.rand(self.population_size, self.dim)\n        self.best_position = None\n        self.best_score = np.inf\n        self.alpha = 0.1  # Learning rate for rotation gate adjustments\n\n    def _quantum_measurement(self, lb, ub):\n        return lb + (ub - lb) * (self.q_population > np.random.rand(self.population_size, self.dim))\n\n    def _update_quantum_gates(self, scores, lb, ub):\n        best_indices = np.argsort(scores)[:5]  # Select top 5 solutions\n        for i in best_indices:\n            self.q_population[i] = self.alpha * self.best_position + (1 - self.alpha) * self.q_population[i]\n        self.q_population = np.clip(self.q_population, lb, ub)\n\n    def __call__(self, func):\n        self.lb, self.ub = func.bounds.lb, func.bounds.ub\n\n        eval_count = 0\n        while eval_count < self.budget:\n            solutions = self._quantum_measurement(self.lb, self.ub)\n            scores = np.zeros(self.population_size)\n\n            for i in range(self.population_size):\n                if eval_count >= self.budget:\n                    break\n                scores[i] = func(solutions[i])\n                eval_count += 1\n\n                if scores[i] < self.best_score:\n                    self.best_score = scores[i]\n                    self.best_position = solutions[i]\n\n            self._update_quantum_gates(scores, self.lb, self.ub)\n\n        return self.best_position, self.best_score", "name": "QuantumInspiredEvolutionaryAlgorithm", "description": "Quantum-Inspired Evolutionary Algorithm (QIEA) leverages quantum superposition and entanglement to explore the search space efficiently and improve convergence by dynamically adjusting quantum gates based on performance.", "configspace": "", "generation": 50, "fitness": 0.37468565617630906, "feedback": "The algorithm QuantumInspiredEvolutionaryAlgorithm got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.37 with standard deviation 0.00.", "error": "", "parent_id": "886d3ed4-4dda-4108-9bfe-eb231d760865", "metadata": {"aucs": [0.37466370234684987, 0.37470761000576824]}, "mutation_prompt": null}
{"id": "41d0a339-0e3e-478d-98af-57c5a41f913d", "solution": "import numpy as np\n\nclass EnhancedAdaptiveMemorySearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20 + self.dim\n        self.memory_size = 5\n        self.particles = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_scores = None\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.omega = 0.5\n        self.phi_p = 1.5\n        self.phi_g = 1.5\n        self.memory = []\n        self.convergence_history = []\n\n    def _initialize_population(self, lb, ub):\n        self.particles = np.random.rand(self.population_size, self.dim) * (ub - lb) + lb\n        self.velocities = np.random.rand(self.population_size, self.dim) * (ub - lb) / 10\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n\n    def _update_particles(self, lb, ub):\n        for i in range(self.population_size):\n            if self.memory and np.random.rand() < 0.5:\n                strategy = self.memory[np.random.randint(len(self.memory))]\n                cognitive_component = strategy['phi_p'] * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.particles[i])\n                social_component = strategy['phi_g'] * np.random.rand(self.dim) * (self.global_best_position - self.particles[i])\n                inertia = strategy['omega']\n            else:\n                cognitive_component = self.phi_p * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.particles[i])\n                social_component = self.phi_g * np.random.rand(self.dim) * (self.global_best_position - self.particles[i])\n                inertia = self.omega\n\n            self.velocities[i] = inertia * self.velocities[i] + cognitive_component + social_component\n            self.particles[i] += self.velocities[i]\n            self.particles[i] = np.clip(self.particles[i], lb, ub)\n\n    def _adjust_parameters(self):\n        if len(self.convergence_history) > 2:\n            if self.convergence_history[-1] >= self.convergence_history[-2]:\n                self.phi_p = np.clip(self.phi_p + 0.1, 1.5, 2.0)\n                self.phi_g = np.clip(self.phi_g - 0.1, 1.0, 1.5)\n            else:\n                self.phi_p = np.clip(self.phi_p - 0.1, 1.0, 1.5)\n                self.phi_g = np.clip(self.phi_g + 0.1, 1.5, 2.0)\n\n    def __call__(self, func):\n        self.lb, self.ub = func.bounds.lb, func.bounds.ub\n        self._initialize_population(self.lb, self.ub)\n\n        eval_count = 0\n        while eval_count < self.budget:\n            for i in range(self.population_size):\n                if eval_count >= self.budget:\n                    break\n\n                score = func(self.particles[i])\n                eval_count += 1\n\n                if score < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = score\n                    self.personal_best_positions[i] = self.particles[i]\n\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.particles[i]\n                    if len(self.memory) >= self.memory_size:\n                        self.memory.pop(0)\n                    self.memory.append({'omega': self.omega, 'phi_p': self.phi_p, 'phi_g': self.phi_g})\n\n            self._update_particles(self.lb, self.ub)\n            self.convergence_history.append(self.global_best_score)\n            self._adjust_parameters()\n\n        return self.global_best_position, self.global_best_score", "name": "EnhancedAdaptiveMemorySearch", "description": "Enhanced Adaptive Memory Search (EAMS) incorporates dynamic scaling of cognitive and social coefficients based on convergence history to improve exploration-exploitation balance.", "configspace": "", "generation": 51, "fitness": 0.9352044050103352, "feedback": "The algorithm EnhancedAdaptiveMemorySearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.94 with standard deviation 0.02.", "error": "", "parent_id": "886d3ed4-4dda-4108-9bfe-eb231d760865", "metadata": {"aucs": [0.9530312594908674, 0.9173775505298029]}, "mutation_prompt": null}
{"id": "4a375423-d038-45cb-b73f-ebef444e7636", "solution": "import numpy as np\n\nclass AdaptiveQuantumPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20 + self.dim\n        self.particles = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_scores = None\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.omega = 0.5  # Initial inertia weight\n        self.phi_p = 1.5  # Initial cognitive coefficient\n        self.phi_g = 1.5  # Initial social coefficient\n        self.alpha = 0.5  # Quantum rotation angle for adaptive exploration\n\n    def _initialize_population(self, lb, ub):\n        self.particles = np.random.rand(self.population_size, self.dim) * (ub - lb) + lb\n        self.velocities = np.random.rand(self.population_size, self.dim) * (ub - lb) / 10\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n\n    def _quantum_operator(self):\n        for i in range(self.population_size):\n            for d in range(self.dim):\n                theta = np.random.uniform(0, self.alpha)\n                quantum_shift = np.random.choice([-1, 1])\n                self.particles[i, d] += quantum_shift * np.tan(theta) * (self.global_best_position[d] - self.particles[i, d])\n\n    def _update_particles(self, lb, ub):\n        for i in range(self.population_size):\n            cognitive_component = self.phi_p * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.particles[i])\n            social_component = self.phi_g * np.random.rand(self.dim) * (self.global_best_position - self.particles[i])\n            self.velocities[i] = self.omega * self.velocities[i] + cognitive_component + social_component\n            self.particles[i] += self.velocities[i]\n            self.particles[i] = np.clip(self.particles[i], lb, ub)\n\n    def __call__(self, func):\n        self.lb, self.ub = func.bounds.lb, func.bounds.ub\n        self._initialize_population(self.lb, self.ub)\n\n        eval_count = 0\n        while eval_count < self.budget:\n            for i in range(self.population_size):\n                if eval_count >= self.budget:\n                    break\n\n                score = func(self.particles[i])\n                eval_count += 1\n\n                if score < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = score\n                    self.personal_best_positions[i] = self.particles[i]\n\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.particles[i]\n\n            self._quantum_operator()\n            self._update_particles(self.lb, self.ub)\n\n        return self.global_best_position, self.global_best_score", "name": "AdaptiveQuantumPSO", "description": "Adaptive Quantum Particle Swarm Optimization (AQPSO) leverages quantum-inspired operators and adaptive parameters to enhance exploration and exploitation balance in high-dimensional spaces.", "configspace": "", "generation": 52, "fitness": 0.8692930286489031, "feedback": "The algorithm AdaptiveQuantumPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.87 with standard deviation 0.09.", "error": "", "parent_id": "886d3ed4-4dda-4108-9bfe-eb231d760865", "metadata": {"aucs": [0.9616777041790676, 0.7769083531187387]}, "mutation_prompt": null}
{"id": "bc2bd26e-9bd2-4b62-a3d8-e63671a9fa5d", "solution": "import numpy as np\n\nclass EnhancedAdaptiveMemorySearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20 + self.dim\n        self.memory_size = 10  # Increased memory size for more strategy diversity\n        self.particles = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_scores = None\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.omega = 0.5  # Inertia weight\n        self.phi_p = 1.5  # Cognitive coefficient\n        self.phi_g = 1.5  # Social coefficient\n        self.memory = []  # Initialize adaptive memory\n\n    def _initialize_population(self, lb, ub):\n        self.particles = np.random.rand(self.population_size, self.dim) * (ub - lb) + lb\n        self.velocities = np.random.rand(self.population_size, self.dim) * (ub - lb) / 10\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n\n    def _dynamic_velocity_scaling(self, velocities, eval_ratio):\n        scale_factor = 0.5 + (1.5 * eval_ratio)\n        return velocities * scale_factor\n\n    def _update_particles(self, lb, ub, eval_count):\n        eval_ratio = eval_count / self.budget\n        for i in range(self.population_size):\n            if self.memory and np.random.rand() < 0.5:\n                strategy = self.memory[np.random.randint(len(self.memory))]\n                cognitive_component = strategy['phi_p'] * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.particles[i])\n                social_component = strategy['phi_g'] * np.random.rand(self.dim) * (self.global_best_position - self.particles[i])\n                inertia = strategy['omega']\n            else:\n                cognitive_component = self.phi_p * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.particles[i])\n                social_component = self.phi_g * np.random.rand(self.dim) * (self.global_best_position - self.particles[i])\n                inertia = self.omega\n\n            velocities = inertia * self.velocities[i] + cognitive_component + social_component\n            self.velocities[i] = self._dynamic_velocity_scaling(velocities, eval_ratio)\n            self.particles[i] += self.velocities[i]\n            self.particles[i] = np.clip(self.particles[i], lb, ub)\n\n    def __call__(self, func):\n        self.lb, self.ub = func.bounds.lb, func.bounds.ub\n        self._initialize_population(self.lb, self.ub)\n\n        eval_count = 0\n        while eval_count < self.budget:\n            for i in range(self.population_size):\n                if eval_count >= self.budget:\n                    break\n\n                score = func(self.particles[i])\n                eval_count += 1\n\n                if score < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = score\n                    self.personal_best_positions[i] = self.particles[i]\n\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.particles[i]\n                    if len(self.memory) >= self.memory_size:\n                        self.memory.pop(0)\n                    self.memory.append({'omega': self.omega, 'phi_p': self.phi_p, 'phi_g': self.phi_g})\n                    self.omega = np.clip(self.omega + np.random.uniform(-0.05, 0.05), 0.3, 0.7)\n                    self.phi_p = np.clip(self.phi_p + np.random.uniform(-0.05, 0.05), 1.2, 1.8)\n                    self.phi_g = np.clip(self.phi_g + np.random.uniform(-0.05, 0.05), 1.2, 1.8)\n\n            self._update_particles(self.lb, self.ub, eval_count)\n\n        return self.global_best_position, self.global_best_score", "name": "EnhancedAdaptiveMemorySearch", "description": "Enhanced Adaptive Memory Search (EAMS) introduces dynamic velocity scaling and adaptive memory expansion, improving convergence speed and solution accuracy.", "configspace": "", "generation": 53, "fitness": 0.9592817647428868, "feedback": "The algorithm EnhancedAdaptiveMemorySearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.96 with standard deviation 0.01.", "error": "", "parent_id": "886d3ed4-4dda-4108-9bfe-eb231d760865", "metadata": {"aucs": [0.966844466004918, 0.9517190634808558]}, "mutation_prompt": null}
{"id": "3bfb27de-a068-4cbb-9336-c72c1c5ef313", "solution": "import numpy as np\n\nclass QuantumInspiredPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20 + self.dim\n        self.global_best_score = np.inf\n        self.global_best_position = None\n        self.particles = None\n        self.personal_best_positions = None\n        self.personal_best_scores = None\n        self.qbit_flip_prob = 0.5  # Probability of flipping a qubit\n        self.expansion_coeff = 1.5  # Expansion coefficient for exploration\n\n    def _initialize_population(self, lb, ub):\n        self.particles = np.random.rand(self.population_size, self.dim) * (ub - lb) + lb\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n\n    def _quantum_flip(self, position, lb, ub):\n        new_position = np.copy(position)\n        for j in range(self.dim):\n            if np.random.rand() < self.qbit_flip_prob:\n                new_position[j] = np.random.uniform(lb[j], ub[j])\n        return new_position\n\n    def _update_particles(self, lb, ub):\n        for i in range(self.population_size):\n            # Quantum-inspired position update\n            self.particles[i] = self._quantum_flip(self.particles[i], lb, ub)\n            # Probabilistic expansion towards the global best\n            if np.random.rand() < 0.5:\n                self.particles[i] += self.expansion_coeff * np.random.rand(self.dim) * (self.global_best_position - self.particles[i])\n            self.particles[i] = np.clip(self.particles[i], lb, ub)\n\n    def __call__(self, func):\n        self.lb, self.ub = func.bounds.lb, func.bounds.ub\n        self._initialize_population(self.lb, self.ub)\n\n        eval_count = 0\n        while eval_count < self.budget:\n            for i in range(self.population_size):\n                if eval_count >= self.budget:\n                    break\n\n                score = func(self.particles[i])\n                eval_count += 1\n\n                if score < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = score\n                    self.personal_best_positions[i] = np.copy(self.particles[i])\n\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = np.copy(self.particles[i])\n\n            self._update_particles(self.lb, self.ub)\n\n        return self.global_best_position, self.global_best_score", "name": "QuantumInspiredPSO", "description": "Quantum-Inspired Particle Swarm Optimization (QIPSO) integrates quantum mechanics principles to enhance exploration and convergence by probabilistically updating particle positions.", "configspace": "", "generation": 54, "fitness": 0.7396710701380362, "feedback": "The algorithm QuantumInspiredPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.74 with standard deviation 0.03.", "error": "", "parent_id": "886d3ed4-4dda-4108-9bfe-eb231d760865", "metadata": {"aucs": [0.7118983622305652, 0.7674437780455072]}, "mutation_prompt": null}
{"id": "840727e8-17e4-46b7-8e43-a1256694aede", "solution": "import numpy as np\n\nclass QuantumInspiredEvolutionarySearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20 + self.dim\n        self.particles = None\n        self.best_scores = np.full(self.population_size, np.inf)\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.alpha = 0.99  # Convergence control parameter\n\n    def _initialize_population(self, lb, ub):\n        # Initialize particles using quantum-inspired superposition\n        self.particles = np.random.rand(self.population_size, self.dim) * (ub - lb) + lb\n        self.quantum_states = np.random.rand(self.population_size, self.dim) * (ub - lb) / 10\n\n    def _measure_positions(self, lb, ub):\n        # Simulate quantum measurement to update positions\n        self.particles += self.quantum_states * np.random.normal(0, 1, (self.population_size, self.dim))\n        self.particles = np.clip(self.particles, lb, ub)\n\n    def _update_quantum_states(self):\n        # Entanglement and interaction to update quantum states\n        for i in range(self.population_size):\n            if np.random.rand() < 0.5:\n                partner = np.random.randint(self.population_size)\n                self.quantum_states[i] = (self.quantum_states[i] + self.quantum_states[partner]) / 2\n            self.quantum_states[i] *= self.alpha\n\n    def __call__(self, func):\n        self.lb, self.ub = func.bounds.lb, func.bounds.ub\n        self._initialize_population(self.lb, self.ub)\n\n        eval_count = 0\n        while eval_count < self.budget:\n            self._measure_positions(self.lb, self.ub)\n\n            for i in range(self.population_size):\n                if eval_count >= self.budget:\n                    break\n\n                score = func(self.particles[i])\n                eval_count += 1\n\n                if score < self.best_scores[i]:\n                    self.best_scores[i] = score\n\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.particles[i]\n\n            self._update_quantum_states()\n\n        return self.global_best_position, self.global_best_score", "name": "QuantumInspiredEvolutionarySearch", "description": "Quantum-Inspired Evolutionary Search (QIES) utilizes quantum superposition and entanglement principles to explore multiple potential solutions simultaneously, enhancing diversity and convergence speed.", "configspace": "", "generation": 55, "fitness": 0.5334843141356069, "feedback": "The algorithm QuantumInspiredEvolutionarySearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.53 with standard deviation 0.01.", "error": "", "parent_id": "886d3ed4-4dda-4108-9bfe-eb231d760865", "metadata": {"aucs": [0.547509782484477, 0.5194588457867368]}, "mutation_prompt": null}
{"id": "187e3227-17ca-483e-93df-6a298dec5e74", "solution": "import numpy as np\n\nclass DynamicParameterTuningPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20 + self.dim\n        self.memory_size = 5  \n        self.particles = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_scores = None\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        # Initial PSO parameters\n        self.omega = 0.5 \n        self.phi_p = 1.5 \n        self.phi_g = 1.5 \n        self.memory = []\n\n    def _initialize_population(self, lb, ub):\n        self.particles = np.random.rand(self.population_size, self.dim) * (ub - lb) + lb\n        self.velocities = np.random.rand(self.population_size, self.dim) * (ub - lb) / 10\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n\n    def _update_particles(self, lb, ub):\n        for i in range(self.population_size):\n            if self.memory and np.random.rand() < 0.5:\n                strategy = self.memory[np.random.randint(len(self.memory))]\n                cognitive_component = strategy['phi_p'] * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.particles[i])\n                social_component = strategy['phi_g'] * np.random.rand(self.dim) * (self.global_best_position - self.particles[i])\n                inertia = strategy['omega']\n            else:\n                cognitive_component = self.phi_p * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.particles[i])\n                social_component = self.phi_g * np.random.rand(self.dim) * (self.global_best_position - self.particles[i])\n                inertia = self.omega\n\n            self.velocities[i] = inertia * self.velocities[i] + cognitive_component + social_component\n            self.particles[i] += self.velocities[i]\n            self.particles[i] = np.clip(self.particles[i], lb, ub)\n\n    def __call__(self, func):\n        self.lb, self.ub = func.bounds.lb, func.bounds.ub\n        self._initialize_population(self.lb, self.ub)\n\n        eval_count = 0\n        while eval_count < self.budget:\n            for i in range(self.population_size):\n                if eval_count >= self.budget:\n                    break\n\n                score = func(self.particles[i])\n                eval_count += 1\n\n                if score < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = score\n                    self.personal_best_positions[i] = self.particles[i]\n\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.particles[i]\n                    if len(self.memory) >= self.memory_size:\n                        self.memory.pop(0)\n                    self.memory.append({'omega': self.omega, 'phi_p': self.phi_p, 'phi_g': self.phi_g})\n                    # Dynamic parameter tuning\n                    self.omega = np.clip(self.omega * 0.9 + np.random.uniform(-0.05, 0.05), 0.3, 0.7)\n                    self.phi_p = np.clip(self.phi_p * 1.1 + np.random.uniform(-0.05, 0.05), 1.2, 2.0)\n                    self.phi_g = np.clip(self.phi_g * 1.1 + np.random.uniform(-0.05, 0.05), 1.2, 2.0)\n\n            self._update_particles(self.lb, self.ub)\n\n        return self.global_best_position, self.global_best_score", "name": "DynamicParameterTuningPSO", "description": "Dynamic Parameter Tuning Particle Swarm Optimization (DPT-PSO) enhances optimization by dynamically tuning PSO parameters based on local search successes, balancing exploration and exploitation adaptively.", "configspace": "", "generation": 56, "fitness": 0.937051242367575, "feedback": "The algorithm DynamicParameterTuningPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.94 with standard deviation 0.03.", "error": "", "parent_id": "886d3ed4-4dda-4108-9bfe-eb231d760865", "metadata": {"aucs": [0.9696563010175543, 0.9044461837175958]}, "mutation_prompt": null}
{"id": "e2c67694-f456-4541-a12f-73cf4db4f26e", "solution": "import numpy as np\n\nclass QuantumAdaptiveMemorySearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20 + self.dim\n        self.memory_size = 5  # Number of successful strategies to remember\n        self.particles = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_scores = None\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.omega = 0.5  # Inertia weight\n        self.phi_p = 1.5  # Cognitive coefficient\n        self.phi_g = 1.5  # Social coefficient\n        self.memory = []  # Initialize adaptive memory\n\n    def _initialize_population(self, lb, ub):\n        self.particles = np.random.rand(self.population_size, self.dim) * (ub - lb) + lb\n        self.velocities = np.random.rand(self.population_size, self.dim) * (ub - lb) / 10\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n\n    def _quantum_superposition(self):\n        # Creates a superposition-like state for particles, enhancing exploration.\n        quantum_state = np.random.rand(self.dim)\n        decision_state = np.random.choice([0, 1], size=self.dim, p=[0.5, 0.5])\n        return quantum_state, decision_state\n\n    def _update_particles(self, lb, ub):\n        for i in range(self.population_size):\n            quantum_state, decision_state = self._quantum_superposition()\n\n            # Randomly choose a strategy from memory or use the default\n            if self.memory and np.random.rand() < 0.5:\n                strategy = self.memory[np.random.randint(len(self.memory))]\n                cognitive_component = strategy['phi_p'] * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.particles[i])\n                social_component = strategy['phi_g'] * np.random.rand(self.dim) * (self.global_best_position - self.particles[i])\n                inertia = strategy['omega']\n            else:\n                cognitive_component = self.phi_p * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.particles[i])\n                social_component = self.phi_g * np.random.rand(self.dim) * (self.global_best_position - self.particles[i])\n                inertia = self.omega\n\n            self.velocities[i] = inertia * self.velocities[i] + cognitive_component * decision_state + social_component * (1 - decision_state)\n            self.particles[i] += self.velocities[i] * quantum_state\n            self.particles[i] = np.clip(self.particles[i], lb, ub)\n\n    def __call__(self, func):\n        self.lb, self.ub = func.bounds.lb, func.bounds.ub\n        self._initialize_population(self.lb, self.ub)\n\n        eval_count = 0\n        while eval_count < self.budget:\n            for i in range(self.population_size):\n                if eval_count >= self.budget:\n                    break\n\n                score = func(self.particles[i])\n                eval_count += 1\n\n                if score < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = score\n                    self.personal_best_positions[i] = self.particles[i]\n\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.particles[i]\n                    # Add successful strategy to memory\n                    if len(self.memory) >= self.memory_size:\n                        self.memory.pop(0)  # Remove oldest strategy if memory is full\n                    self.memory.append({'omega': self.omega, 'phi_p': self.phi_p, 'phi_g': self.phi_g})\n                    # Adapt parameters slightly\n                    self.omega = np.clip(self.omega + np.random.uniform(-0.1, 0.1), 0.3, 0.7)\n                    self.phi_p = np.clip(self.phi_p + np.random.uniform(-0.1, 0.1), 1.2, 1.8)\n                    self.phi_g = np.clip(self.phi_g + np.random.uniform(-0.1, 0.1), 1.2, 1.8)\n\n            self._update_particles(self.lb, self.ub)\n\n        return self.global_best_position, self.global_best_score", "name": "QuantumAdaptiveMemorySearch", "description": "Quantum-inspired Adaptive Memory Search (QAMS) leverages quantum superposition-like states for particles, enhancing exploration capabilities and improving convergence speed by probabilistically updating the memory-based strategies.", "configspace": "", "generation": 57, "fitness": 0.9292476925733035, "feedback": "The algorithm QuantumAdaptiveMemorySearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.93 with standard deviation 0.00.", "error": "", "parent_id": "886d3ed4-4dda-4108-9bfe-eb231d760865", "metadata": {"aucs": [0.92492172518371, 0.9335736599628971]}, "mutation_prompt": null}
{"id": "9111fee8-f128-46c5-a25f-23c8314aba7b", "solution": "import numpy as np\n\nclass EnhancedAdaptiveMemorySearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20 + self.dim\n        self.memory_size = 5\n        self.particles = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_scores = None\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.omega = 0.5\n        self.phi_p = 1.5\n        self.phi_g = 1.5\n        self.memory = []\n        self.memory_decay_factor = 0.9\n\n    def _initialize_population(self, lb, ub):\n        self.particles = np.random.rand(self.population_size, self.dim) * (ub - lb) + lb\n        self.velocities = np.random.rand(self.population_size, self.dim) * (ub - lb) / 10\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n\n    def _update_particles(self, lb, ub):\n        for i in range(self.population_size):\n            if self.memory and np.random.rand() < 0.5:\n                strategy = self.memory[np.random.randint(len(self.memory))]\n                cognitive_component = strategy['phi_p'] * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.particles[i])\n                social_component = strategy['phi_g'] * np.random.rand(self.dim) * (self.global_best_position - self.particles[i])\n                inertia = strategy['omega'] * (self.memory_decay_factor ** len(self.memory))\n            else:\n                cognitive_component = self.phi_p * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.particles[i])\n                social_component = self.phi_g * np.random.rand(self.dim) * (self.global_best_position - self.particles[i])\n                inertia = self.omega\n\n            self.velocities[i] = inertia * self.velocities[i] + cognitive_component + social_component\n            self.particles[i] += self.velocities[i]\n            self.particles[i] = np.clip(self.particles[i], lb, ub)\n\n    def __call__(self, func):\n        self.lb, self.ub = func.bounds.lb, func.bounds.ub\n        self._initialize_population(self.lb, self.ub)\n\n        eval_count = 0\n        while eval_count < self.budget:\n            for i in range(self.population_size):\n                if eval_count >= self.budget:\n                    break\n\n                score = func(self.particles[i])\n                eval_count += 1\n\n                if score < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = score\n                    self.personal_best_positions[i] = self.particles[i]\n\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.particles[i]\n                    if len(self.memory) >= self.memory_size:\n                        self.memory.pop(0)\n                    self.memory.append({'omega': self.omega, 'phi_p': self.phi_p, 'phi_g': self.phi_g})\n                    self.omega = np.clip(self.omega + np.random.uniform(-0.05, 0.05), 0.3, 0.7)\n                    self.phi_p = np.clip(self.phi_p + np.random.uniform(-0.05, 0.05), 1.2, 1.8)\n                    self.phi_g = np.clip(self.phi_g + np.random.uniform(-0.05, 0.05), 1.2, 1.8)\n\n            self._update_particles(self.lb, self.ub)\n\n        return self.global_best_position, self.global_best_score", "name": "EnhancedAdaptiveMemorySearch", "description": "Enhanced Adaptive Memory Search (EAMS) optimizes the balance between exploration and exploitation by integrating a dynamic strategy selection mechanism and adaptive memory decay for better handling of diverse optimization landscapes.", "configspace": "", "generation": 58, "fitness": 0.9413982916821102, "feedback": "The algorithm EnhancedAdaptiveMemorySearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.94 with standard deviation 0.01.", "error": "", "parent_id": "886d3ed4-4dda-4108-9bfe-eb231d760865", "metadata": {"aucs": [0.9517825617000053, 0.931014021664215]}, "mutation_prompt": null}
{"id": "e825ab9f-69bc-4f6d-8f6b-3c8f51b3ea0d", "solution": "import numpy as np\n\nclass QuantumInspiredEvolutionaryAlgorithm:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20 + self.dim\n        self.q_population = np.full((self.population_size, self.dim, 2), 1/np.sqrt(2))  # Initialize qubits\n        self.best_solution = None\n        self.best_score = np.inf\n        self.alpha = 0.05  # Rotation angle\n\n    def _measure_population(self, lb, ub):\n        population = np.zeros((self.population_size, self.dim))\n        for i in range(self.population_size):\n            for d in range(self.dim):\n                if np.random.rand() < self.q_population[i, d, 0] ** 2:\n                    population[i, d] = 1\n        scaled_population = lb + (ub - lb) * population\n        return scaled_population\n\n    def _update_quantum_population(self, best_position):\n        for i in range(self.population_size):\n            for d in range(self.dim):\n                theta = self.alpha * (2 * np.random.rand() - 1)\n                if np.random.rand() < self.q_population[i, d, 0] ** 2:\n                    desired_state = best_position[d]\n                else:\n                    desired_state = 1 - best_position[d]\n                if desired_state == 1:\n                    theta = -theta\n                cos_theta, sin_theta = np.cos(theta), np.sin(theta)\n                q0, q1 = self.q_population[i, d]\n                self.q_population[i, d, 0] = cos_theta * q0 - sin_theta * q1\n                self.q_population[i, d, 1] = sin_theta * q0 + cos_theta * q1\n\n    def __call__(self, func):\n        self.lb, self.ub = func.bounds.lb, func.bounds.ub\n        eval_count = 0\n        \n        while eval_count < self.budget:\n            population = self._measure_population(self.lb, self.ub)\n            for i in range(self.population_size):\n                if eval_count >= self.budget:\n                    break\n                score = func(population[i])\n                eval_count += 1\n                \n                if score < self.best_score:\n                    self.best_score = score\n                    self.best_solution = population[i]\n            \n            if self.best_solution is not None:\n                best_binary = ((self.best_solution - self.lb) / (self.ub - self.lb)).round()\n                self._update_quantum_population(best_binary)\n        \n        return self.best_solution, self.best_score", "name": "QuantumInspiredEvolutionaryAlgorithm", "description": "Quantum-Inspired Evolutionary Algorithm (QIEA) leverages quantum principles to enhance diversity and convergence speed by representing individuals as quantum bits, with dynamic rotation of qubits to explore the search space.", "configspace": "", "generation": 59, "fitness": 0.4340439741496952, "feedback": "The algorithm QuantumInspiredEvolutionaryAlgorithm got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.43 with standard deviation 0.00.", "error": "", "parent_id": "886d3ed4-4dda-4108-9bfe-eb231d760865", "metadata": {"aucs": [0.43441807151719625, 0.43366987678219415]}, "mutation_prompt": null}
{"id": "c23b8c38-2155-4ae7-826e-d2d06835a280", "solution": "import numpy as np\n\nclass EnhancedAdaptiveMemorySearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20 + self.dim\n        self.memory_size = 5\n        self.archive_size = 10  # External archive size for diversity\n        self.archive = []  # Archive to store diverse solutions\n        self.particles = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_scores = None\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.omega = 0.5\n        self.phi_p = 1.5\n        self.phi_g = 1.5\n        self.memory = []\n\n    def _initialize_population(self, lb, ub):\n        self.particles = np.random.rand(self.population_size, self.dim) * (ub - lb) + lb\n        self.velocities = np.random.rand(self.population_size, self.dim) * (ub - lb) / 10\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n\n    def _update_particles(self, lb, ub):\n        for i in range(self.population_size):\n            if self.memory and np.random.rand() < 0.5:\n                strategy = self.memory[np.random.randint(len(self.memory))]\n                cognitive_component = strategy['phi_p'] * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.particles[i])\n                social_component = strategy['phi_g'] * np.random.rand(self.dim) * (self.global_best_position - self.particles[i])\n                inertia = strategy['omega']\n            else:\n                cognitive_component = self.phi_p * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.particles[i])\n                social_component = self.phi_g * np.random.rand(self.dim) * (self.global_best_position - self.particles[i])\n                inertia = self.omega\n\n            # Incorporate random archived solution for diversity\n            if self.archive and np.random.rand() < 0.3:\n                archive_solution = self.archive[np.random.randint(len(self.archive))]\n                social_component += np.random.rand(self.dim) * (archive_solution - self.particles[i])\n\n            self.velocities[i] = inertia * self.velocities[i] + cognitive_component + social_component\n            self.particles[i] += self.velocities[i]\n            self.particles[i] = np.clip(self.particles[i], lb, ub)\n\n    def __call__(self, func):\n        self.lb, self.ub = func.bounds.lb, func.bounds.ub\n        self._initialize_population(self.lb, self.ub)\n\n        eval_count = 0\n        while eval_count < self.budget:\n            for i in range(self.population_size):\n                if eval_count >= self.budget:\n                    break\n\n                score = func(self.particles[i])\n                eval_count += 1\n\n                if score < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = score\n                    self.personal_best_positions[i] = self.particles[i]\n\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.particles[i]\n                    if len(self.memory) >= self.memory_size:\n                        self.memory.pop(0)\n                    self.memory.append({'omega': self.omega, 'phi_p': self.phi_p, 'phi_g': self.phi_g})\n                    self.omega = np.clip(self.omega + np.random.uniform(-0.1, 0.1), 0.3, 0.7)\n                    self.phi_p = np.clip(self.phi_p + np.random.uniform(-0.1, 0.1), 1.2, 1.8)\n                    self.phi_g = np.clip(self.phi_g + np.random.uniform(-0.1, 0.1), 1.2, 1.8)\n\n                # Update archive with diverse solutions\n                if len(self.archive) < self.archive_size and score not in self.archive:\n                    self.archive.append(self.particles[i])\n                elif score < min(self.archive, key=lambda x: func(x)):\n                    self.archive[np.argmin([func(x) for x in self.archive])] = self.particles[i]\n\n            self._update_particles(self.lb, self.ub)\n\n        return self.global_best_position, self.global_best_score", "name": "EnhancedAdaptiveMemorySearch", "description": "Enhanced Adaptive Memory Search (EAMS) improves AMS by incorporating an external archive of diverse solutions and adaptive learning rates to boost exploration and exploitation balance.", "configspace": "", "generation": 60, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()').", "error": "ValueError('The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()')", "parent_id": "886d3ed4-4dda-4108-9bfe-eb231d760865", "metadata": {}, "mutation_prompt": null}
{"id": "dbd4310f-63d3-48d8-8d2b-615f0a503128", "solution": "import numpy as np\n\nclass QuantumInspiredEvolutionarySearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20 + self.dim\n        self.population = None\n        self.probability_amplitudes = None\n        self.personal_best_positions = None\n        self.personal_best_scores = None\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.alpha = 0.5  # Probability amplitude factor\n\n    def _initialize_population(self, lb, ub):\n        self.population = np.random.rand(self.population_size, self.dim) * (ub - lb) + lb\n        self.probability_amplitudes = np.full((self.population_size, self.dim), 1 / np.sqrt(self.dim))\n        self.personal_best_positions = np.copy(self.population)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n\n    def _update_population(self, lb, ub):\n        for i in range(self.population_size):\n            # Quantum-inspired update using probability amplitudes\n            quantum_change = self.alpha * (np.random.rand(self.dim) - 0.5) * self.probability_amplitudes[i]\n            mutation_vector = quantum_change\n\n            candidate_solution = self.population[i] + mutation_vector\n            candidate_solution = np.clip(candidate_solution, lb, ub)\n\n            # Evaluate the new candidate solution\n            score = func(candidate_solution)\n            if score < self.personal_best_scores[i]:\n                self.personal_best_scores[i] = score\n                self.personal_best_positions[i] = candidate_solution\n\n            if score < self.global_best_score:\n                self.global_best_score = score\n                self.global_best_position = candidate_solution\n\n            # Update probability amplitudes for diversity\n            self.probability_amplitudes[i] = (1 - self.alpha) * self.probability_amplitudes[i] + \\\n                                              self.alpha * np.abs(candidate_solution - self.global_best_position)\n\n            self.population[i] = candidate_solution\n\n    def __call__(self, func):\n        self.lb, self.ub = func.bounds.lb, func.bounds.ub\n        self._initialize_population(self.lb, self.ub)\n\n        eval_count = 0\n        while eval_count < self.budget:\n            for i in range(self.population_size):\n                if eval_count >= self.budget:\n                    break\n\n                # Evaluate current solution\n                score = func(self.population[i])\n                eval_count += 1\n\n                if score < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = score\n                    self.personal_best_positions[i] = self.population[i]\n\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.population[i]\n\n            self._update_population(self.lb, self.ub)\n\n        return self.global_best_position, self.global_best_score", "name": "QuantumInspiredEvolutionarySearch", "description": "Quantum-Inspired Evolutionary Search (QIES) utilizes quantum superposition principles to enhance exploration by maintaining a probabilistic representation of each solution, encouraging diverse and effective search paths.", "configspace": "", "generation": 61, "fitness": -Infinity, "feedback": "An exception occurred: NameError(\"name 'func' is not defined\").", "error": "NameError(\"name 'func' is not defined\")", "parent_id": "886d3ed4-4dda-4108-9bfe-eb231d760865", "metadata": {}, "mutation_prompt": null}
{"id": "1b861d1f-75ba-4f12-9259-4b3248f6e34f", "solution": "import numpy as np\n\nclass EnhancedAdaptiveMemorySearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20 + self.dim\n        self.memory_size = 5  # Number of successful strategies to remember\n        self.particles = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_scores = None\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.omega = 0.5  # Inertia weight\n        self.phi_p = 1.5  # Cognitive coefficient\n        self.phi_g = 1.5  # Social coefficient\n        self.memory = []  # Initialize adaptive memory\n        self.performance_memory = []\n\n    def _initialize_population(self, lb, ub):\n        self.particles = np.random.rand(self.population_size, self.dim) * (ub - lb) + lb\n        self.velocities = np.random.rand(self.population_size, self.dim) * (ub - lb) / 10\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n\n    def _select_strategy(self):\n        if self.memory and np.random.rand() < 0.7:\n            strategy_idx = np.random.randint(len(self.memory))\n            strategy = self.memory[strategy_idx]\n            performance = self.performance_memory[strategy_idx]\n            # Probabilistic selection biased towards better performance\n            if np.random.rand() < performance:\n                return strategy\n        return {'omega': self.omega, 'phi_p': self.phi_p, 'phi_g': self.phi_g}\n\n    def _update_particles(self, lb, ub):\n        for i in range(self.population_size):\n            strategy = self._select_strategy()\n            cognitive_component = strategy['phi_p'] * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.particles[i])\n            social_component = strategy['phi_g'] * np.random.rand(self.dim) * (self.global_best_position - self.particles[i])\n            inertia = strategy['omega']\n\n            self.velocities[i] = inertia * self.velocities[i] + cognitive_component + social_component\n            self.particles[i] += self.velocities[i]\n            self.particles[i] = np.clip(self.particles[i], lb, ub)\n\n    def __call__(self, func):\n        self.lb, self.ub = func.bounds.lb, func.bounds.ub\n        self._initialize_population(self.lb, self.ub)\n\n        eval_count = 0\n        while eval_count < self.budget:\n            for i in range(self.population_size):\n                if eval_count >= self.budget:\n                    break\n\n                score = func(self.particles[i])\n                eval_count += 1\n\n                if score < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = score\n                    self.personal_best_positions[i] = self.particles[i]\n\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.particles[i]\n                    new_strategy = {'omega': self.omega, 'phi_p': self.phi_p, 'phi_g': self.phi_g}\n                    if len(self.memory) >= self.memory_size:\n                        self.memory.pop(0)\n                        self.performance_memory.pop(0)\n                    self.memory.append(new_strategy)\n                    self.performance_memory.append((self.global_best_score - score) / abs(self.global_best_score) if self.global_best_score != 0 else 1)\n                    self.omega = np.clip(self.omega + np.random.uniform(-0.05, 0.05), 0.3, 0.7)\n                    self.phi_p = np.clip(self.phi_p + np.random.uniform(-0.05, 0.05), 1.2, 1.8)\n                    self.phi_g = np.clip(self.phi_g + np.random.uniform(-0.05, 0.05), 1.2, 1.8)\n\n            self._update_particles(self.lb, self.ub)\n\n        return self.global_best_position, self.global_best_score", "name": "EnhancedAdaptiveMemorySearch", "description": "Enhanced Adaptive Memory Search (EAMS) augments adaptive memory by diversifying strategy selection and dynamically tuning parameters based on performance feedback, improving convergence efficiency.", "configspace": "", "generation": 62, "fitness": 0.9508531615589334, "feedback": "The algorithm EnhancedAdaptiveMemorySearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.95 with standard deviation 0.00.", "error": "", "parent_id": "886d3ed4-4dda-4108-9bfe-eb231d760865", "metadata": {"aucs": [0.9516773757486239, 0.9500289473692429]}, "mutation_prompt": null}
{"id": "48e54e75-6aea-4531-b237-eeb4a67587da", "solution": "import numpy as np\n\nclass QuantumInspiredEvolutionaryAlgorithm:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20 + self.dim\n        self.q_population = np.full((self.population_size, self.dim), 0.5)  # Probabilities of qubits\n        self.solutions = np.zeros((self.population_size, self.dim))\n        self.best_solution = None\n        self.best_score = np.inf\n\n    def _quantum_measurement(self):\n        for i in range(self.population_size):\n            self.solutions[i] = np.where(np.random.rand(self.dim) < self.q_population[i], 1, 0)\n\n    def _initialize_population(self, lb, ub):\n        self.q_population = np.random.rand(self.population_size, self.dim)\n        self._quantum_measurement()\n        self.solutions = self.solutions * (ub - lb) + lb\n\n    def _update_q_population(self, lb, ub):\n        for i in range(self.population_size):\n            candidate_solution = self.solutions[i]\n            new_score = func(candidate_solution)\n            if new_score < self.best_score:\n                self.best_score = new_score\n                self.best_solution = candidate_solution\n\n            # Update quantum population with a simple rotation gate inspired update mechanism\n            for d in range(self.dim):\n                if candidate_solution[d] == self.best_solution[d]:\n                    self.q_population[i][d] = self.q_population[i][d] + 0.01 * (1 - self.q_population[i][d])\n                else:\n                    self.q_population[i][d] = self.q_population[i][d] - 0.01 * self.q_population[i][d]\n\n            self.q_population[i] = np.clip(self.q_population[i], 0, 1)\n\n        self._quantum_measurement()\n        self.solutions = self.solutions * (ub - lb) + lb\n\n    def __call__(self, func):\n        self.lb, self.ub = func.bounds.lb, func.bounds.ub\n        self._initialize_population(self.lb, self.ub)\n\n        eval_count = 0\n        while eval_count < self.budget:\n            for i in range(self.population_size):\n                if eval_count >= self.budget:\n                    break\n                candidate_solution = self.solutions[i]\n                score = func(candidate_solution)\n                eval_count += 1\n\n                if score < self.best_score:\n                    self.best_score = score\n                    self.best_solution = candidate_solution\n\n            self._update_q_population(self.lb, self.ub)\n\n        return self.best_solution, self.best_score", "name": "QuantumInspiredEvolutionaryAlgorithm", "description": "Quantum-Inspired Evolutionary Algorithm (QIEA) leverages quantum computing concepts to represent solutions as quantum bits, enhancing search diversity and convergence speed in high-dimensional optimization problems.", "configspace": "", "generation": 63, "fitness": -Infinity, "feedback": "An exception occurred: NameError(\"name 'func' is not defined\").", "error": "NameError(\"name 'func' is not defined\")", "parent_id": "886d3ed4-4dda-4108-9bfe-eb231d760865", "metadata": {}, "mutation_prompt": null}
{"id": "ae315ea8-a16a-4042-9305-002afd494c5b", "solution": "import numpy as np\n\nclass QuantumInspiredDynamicSelfAdaptiveSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20 + self.dim\n        self.q_bits = np.random.rand(self.population_size, self.dim)  # Quantum bits for superposition\n        self.collapsed_particles = np.zeros((self.population_size, self.dim))\n        self.personal_best_positions = np.zeros((self.population_size, self.dim))\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n        self.global_best_position = None\n        self.global_best_score = np.inf\n\n    def _initialize_population(self, lb, ub):\n        self.collapsed_particles = (self.q_bits > 0.5) * (ub - lb) + lb\n        self.personal_best_positions = np.copy(self.collapsed_particles)\n\n    def _quantum_collapse(self, lb, ub):\n        prob_threshold = np.random.rand(self.population_size, self.dim)\n        self.collapsed_particles = (self.q_bits > prob_threshold) * (ub - lb) + lb\n        self.collapsed_particles = np.clip(self.collapsed_particles, lb, ub)\n\n    def _update_quantum_bits(self, lb, ub):\n        for i in range(self.population_size):\n            delta = (self.personal_best_positions[i] - self.collapsed_particles[i]) / (ub - lb)\n            delta_global = (self.global_best_position - self.collapsed_particles[i]) / (ub - lb)\n            self.q_bits[i] += 0.1 * delta + 0.1 * delta_global  # Dynamic update rule\n            self.q_bits[i] = np.clip(self.q_bits[i], 0, 1)\n\n    def __call__(self, func):\n        self.lb, self.ub = func.bounds.lb, func.bounds.ub\n        self._initialize_population(self.lb, self.ub)\n\n        eval_count = 0\n        while eval_count < self.budget:\n            self._quantum_collapse(self.lb, self.ub)\n            for i in range(self.population_size):\n                if eval_count >= self.budget:\n                    break\n\n                score = func(self.collapsed_particles[i])\n                eval_count += 1\n\n                if score < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = score\n                    self.personal_best_positions[i] = self.collapsed_particles[i]\n\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.collapsed_particles[i]\n\n            self._update_quantum_bits(self.lb, self.ub)\n\n        return self.global_best_position, self.global_best_score", "name": "QuantumInspiredDynamicSelfAdaptiveSearch", "description": "Quantum-inspired Dynamic Self-adaptive Search (QDSS) employs quantum superposition principles to explore multiple solutions simultaneously, dynamically adjusting strategies based on evolving quantum states and local fitness landscapes.", "configspace": "", "generation": 64, "fitness": 0.3424590415988762, "feedback": "The algorithm QuantumInspiredDynamicSelfAdaptiveSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.34 with standard deviation 0.00.", "error": "", "parent_id": "886d3ed4-4dda-4108-9bfe-eb231d760865", "metadata": {"aucs": [0.34244750331592155, 0.34247057988183083]}, "mutation_prompt": null}
{"id": "cdd9d325-baed-470d-8b8e-4f3f72826549", "solution": "import numpy as np\n\nclass QuantumInspiredEvolutionaryAlgorithm:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20 + self.dim\n        self.q_population = np.random.rand(self.population_size, self.dim, 2)  # Quantum bits\n        self.best_position = None\n        self.best_score = np.inf\n\n    def _initialize_population(self, lb, ub):\n        self.p_real = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        self.best_position = np.copy(self.p_real[0])\n    \n    def _measure_population(self):\n        # Convert quantum bits to real values\n        return np.array([[(0 if np.random.rand() < q[0] else 1) for q in individual] for individual in self.q_population])\n\n    def _update_population(self, lb, ub):\n        # Apply quantum gates for updating\n        for i in range(self.population_size):\n            if np.random.rand() < 0.5:\n                # Quantum NOT gate\n                self.q_population[i] = 1 - self.q_population[i]\n            else:\n                # Quantum rotation\n                rotation_angle = np.random.rand() * np.pi / 4\n                self.q_population[i] = np.array([self._apply_rotation(q, rotation_angle) for q in self.q_population[i]])\n            \n            # Measure the new positions\n            self.p_real[i] = lb + self._measure_population()[i] * (ub - lb)\n\n    def _apply_rotation(self, q, angle):\n        # Apply a quantum rotation gate\n        cos_angle = np.cos(angle)\n        sin_angle = np.sin(angle)\n        new_q0 = cos_angle * q[0] - sin_angle * q[1]\n        new_q1 = sin_angle * q[0] + cos_angle * q[1]\n        return np.array([new_q0, new_q1])\n\n    def __call__(self, func):\n        self.lb, self.ub = func.bounds.lb, func.bounds.ub\n        self._initialize_population(self.lb, self.ub)\n\n        eval_count = 0\n        while eval_count < self.budget:\n            for i in range(self.population_size):\n                if eval_count >= self.budget:\n                    break\n\n                score = func(self.p_real[i])\n                eval_count += 1\n\n                if score < self.best_score:\n                    self.best_score = score\n                    self.best_position = self.p_real[i]\n\n            self._update_population(self.lb, self.ub)\n\n        return self.best_position, self.best_score", "name": "QuantumInspiredEvolutionaryAlgorithm", "description": "Quantum-Inspired Evolutionary Algorithm (QIEA) leverages quantum bit representation and quantum gates to maintain a diverse population, enhancing exploration and adaptability in the search for optimal solutions.", "configspace": "", "generation": 65, "fitness": 0.5341826628947376, "feedback": "The algorithm QuantumInspiredEvolutionaryAlgorithm got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.53 with standard deviation 0.03.", "error": "", "parent_id": "886d3ed4-4dda-4108-9bfe-eb231d760865", "metadata": {"aucs": [0.5045250327986626, 0.5638402929908126]}, "mutation_prompt": null}
{"id": "ccc53f0f-50e7-4009-b04c-3d68991c17b8", "solution": "import numpy as np\n\nclass QuantumInspiredSwarmOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20 + self.dim\n        self.particles = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_scores = None\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.alpha = 0.5  # Exploration weight\n        self.beta = 0.5   # Exploitation weight\n        self.quantum_operator = 0.01  # Quantum operation factor\n\n    def _initialize_population(self, lb, ub):\n        self.particles = np.random.rand(self.population_size, self.dim) * (ub - lb) + lb\n        self.velocities = np.random.rand(self.population_size, self.dim) * (ub - lb) / 10\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n\n    def _quantum_update(self, particle, lb, ub):\n        # Apply quantum-inspired superposition\n        quantum_state = particle + self.quantum_operator * (np.random.rand(self.dim) - 0.5)\n        return np.clip(quantum_state, lb, ub)\n\n    def _update_particles(self, lb, ub):\n        for i in range(self.population_size):\n            r1, r2 = np.random.rand(), np.random.rand()\n            cognitive_component = self.alpha * r1 * (self.personal_best_positions[i] - self.particles[i])\n            social_component = self.beta * r2 * (self.global_best_position - self.particles[i])\n            self.velocities[i] = cognitive_component + social_component\n            quantum_component = self._quantum_update(self.particles[i], lb, ub)\n\n            # Combine classical and quantum updates\n            self.particles[i] += self.velocities[i] + quantum_component - self.particles[i]\n            self.particles[i] = np.clip(self.particles[i], lb, ub)\n\n    def __call__(self, func):\n        self.lb, self.ub = func.bounds.lb, func.bounds.ub\n        self._initialize_population(self.lb, self.ub)\n\n        eval_count = 0\n        while eval_count < self.budget:\n            for i in range(self.population_size):\n                if eval_count >= self.budget:\n                    break\n\n                score = func(self.particles[i])\n                eval_count += 1\n\n                if score < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = score\n                    self.personal_best_positions[i] = self.particles[i]\n\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.particles[i]\n\n            self._update_particles(self.lb, self.ub)\n\n        return self.global_best_position, self.global_best_score", "name": "QuantumInspiredSwarmOptimization", "description": "Quantum-inspired Swarm Optimization (QISO) leverages quantum superposition and entanglement principles to enhance exploration and exploitation, dynamically adjusting particle states and interactions.", "configspace": "", "generation": 66, "fitness": 0.9334103460121839, "feedback": "The algorithm QuantumInspiredSwarmOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.93 with standard deviation 0.01.", "error": "", "parent_id": "886d3ed4-4dda-4108-9bfe-eb231d760865", "metadata": {"aucs": [0.9213059385400885, 0.9455147534842792]}, "mutation_prompt": null}
{"id": "7ef09f68-b2f0-43c5-bd6f-ab546caad257", "solution": "import numpy as np\n\nclass EnhancedAdaptiveMemorySearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20 + self.dim\n        self.memory_size = 5\n        self.particles = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_scores = None\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.omega = 0.5\n        self.phi_p = 1.5\n        self.phi_g = 1.5\n        self.memory = []\n        self.dynamic_friction = 0.9  # Friction coefficient to gradually reduce velocity\n\n    def _initialize_population(self, lb, ub):\n        self.particles = np.random.rand(self.population_size, self.dim) * (ub - lb) + lb\n        self.velocities = np.random.rand(self.population_size, self.dim) * (ub - lb) / 10\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n\n    def _update_particles(self, lb, ub):\n        for i in range(self.population_size):\n            if self.memory and np.random.rand() < 0.5:\n                strategy = self.memory[np.random.randint(len(self.memory))]\n                cognitive_component = strategy['phi_p'] * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.particles[i])\n                social_component = strategy['phi_g'] * np.random.rand(self.dim) * (self.global_best_position - self.particles[i])\n                inertia = strategy['omega']\n            else:\n                cognitive_component = self.phi_p * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.particles[i])\n                social_component = self.phi_g * np.random.rand(self.dim) * (self.global_best_position - self.particles[i])\n                inertia = self.omega\n\n            self.velocities[i] = inertia * self.velocities[i] + cognitive_component + social_component\n            self.velocities[i] *= self.dynamic_friction  # Apply dynamic friction\n            self.particles[i] += self.velocities[i]\n            self.particles[i] = np.clip(self.particles[i], lb, ub)\n\n    def __call__(self, func):\n        self.lb, self.ub = func.bounds.lb, func.bounds.ub\n        self._initialize_population(self.lb, self.ub)\n\n        eval_count = 0\n        elite_preservation_prob = 0.05  # Probability to preserve the best particle\n        while eval_count < self.budget:\n            for i in range(self.population_size):\n                if eval_count >= self.budget:\n                    break\n\n                score = func(self.particles[i])\n                eval_count += 1\n\n                if score < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = score\n                    self.personal_best_positions[i] = self.particles[i]\n\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.particles[i]\n                    if len(self.memory) >= self.memory_size:\n                        self.memory.pop(0)\n                    self.memory.append({'omega': self.omega, 'phi_p': self.phi_p, 'phi_g': self.phi_g})\n                    self.omega = np.clip(self.omega + np.random.uniform(-0.1, 0.1), 0.3, 0.7)\n                    self.phi_p = np.clip(self.phi_p + np.random.uniform(-0.1, 0.1), 1.2, 1.8)\n                    self.phi_g = np.clip(self.phi_g + np.random.uniform(-0.1, 0.1), 1.2, 1.8)\n\n            if np.random.rand() < elite_preservation_prob:\n                # Preserve the best particle by pertubating slightly\n                perturbation = np.random.uniform(-0.01, 0.01, self.dim)\n                self.global_best_position = np.clip(self.global_best_position + perturbation, self.lb, self.ub)\n            \n            self._update_particles(self.lb, self.ub)\n        \n        return self.global_best_position, self.global_best_score", "name": "EnhancedAdaptiveMemorySearch", "description": "Enhanced Adaptive Memory Search (EAMS) introduces a dynamic friction coefficient and a stochastic elite preservation mechanism to improve convergence speed and solution quality by maintaining a balance between exploration and exploitation.", "configspace": "", "generation": 67, "fitness": 0.9525472328629414, "feedback": "The algorithm EnhancedAdaptiveMemorySearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.95 with standard deviation 0.01.", "error": "", "parent_id": "886d3ed4-4dda-4108-9bfe-eb231d760865", "metadata": {"aucs": [0.9610069355400832, 0.9440875301857997]}, "mutation_prompt": null}
{"id": "332d47ef-c1e7-4490-9bd7-e84ba7be06f2", "solution": "import numpy as np\n\nclass QuantumInspiredPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20 + self.dim\n        self.particles = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_scores = None\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.omega = 0.5  # Inertia weight\n        self.phi_p = 1.5  # Cognitive coefficient\n        self.phi_g = 1.5  # Social coefficient\n        self.delta = 0.1  # Quantum tunneling effect strength\n\n    def _initialize_population(self, lb, ub):\n        self.particles = np.random.rand(self.population_size, self.dim) * (ub - lb) + lb\n        self.velocities = np.random.rand(self.population_size, self.dim) * (ub - lb) / 10\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n\n    def _update_particles(self, lb, ub):\n        for i in range(self.population_size):\n            cognitive_component = self.phi_p * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.particles[i])\n            social_component = self.phi_g * np.random.rand(self.dim) * (self.global_best_position - self.particles[i])\n            self.velocities[i] = self.omega * self.velocities[i] + cognitive_component + social_component\n            \n            # Quantum tunneling\n            if np.random.rand() < self.delta:\n                self.particles[i] = np.random.rand(self.dim) * (ub - lb) + lb\n            else:\n                self.particles[i] += self.velocities[i]\n\n            self.particles[i] = np.clip(self.particles[i], lb, ub)\n\n    def __call__(self, func):\n        self.lb, self.ub = func.bounds.lb, func.bounds.ub\n        self._initialize_population(self.lb, self.ub)\n\n        eval_count = 0\n        while eval_count < self.budget:\n            for i in range(self.population_size):\n                if eval_count >= self.budget:\n                    break\n\n                score = func(self.particles[i])\n                eval_count += 1\n\n                if score < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = score\n                    self.personal_best_positions[i] = self.particles[i]\n\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.particles[i]\n\n            self._update_particles(self.lb, self.ub)\n\n        return self.global_best_position, self.global_best_score", "name": "QuantumInspiredPSO", "description": "Quantum-Inspired Particle Swarm Optimization (QIPSO) utilizes quantum computing principles to explore search spaces through quantum superposition and tunneling effects for enhanced convergence.", "configspace": "", "generation": 68, "fitness": 0.8638533186628438, "feedback": "The algorithm QuantumInspiredPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.86 with standard deviation 0.02.", "error": "", "parent_id": "886d3ed4-4dda-4108-9bfe-eb231d760865", "metadata": {"aucs": [0.8819468848645412, 0.8457597524611463]}, "mutation_prompt": null}
{"id": "50bd094c-e00e-4b43-a8f9-9db975f176df", "solution": "import numpy as np\n\nclass QuantumInspiredGeneticAlgorithm:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20 + self.dim\n        self.population = None\n        self.best_solution = None\n        self.best_score = np.inf\n        self.quantum_chromosomes = None\n        self.rotation_angle = np.pi / 4\n\n    def _initialize_population(self, lb, ub):\n        self.quantum_chromosomes = np.random.rand(self.population_size, self.dim, 2)  # Quantum representation\n        self.population = self._quantum_to_real(self.quantum_chromosomes, lb, ub)\n\n    def _quantum_to_real(self, quantum_chromosomes, lb, ub):\n        # Convert quantum representation to real values using probabilistic interpretation\n        angles = np.arctan2(quantum_chromosomes[:,:,1], quantum_chromosomes[:,:,0])\n        probabilities = (np.sin(angles)**2)\n        return lb + probabilities * (ub - lb)\n\n    def _quantum_rotation(self, idx, best_idx):\n        # Apply quantum rotation gate based on the best solution found\n        angle_diff = np.arctan2(self.quantum_chromosomes[best_idx,:,1], self.quantum_chromosomes[best_idx,:,0]) - \\\n                     np.arctan2(self.quantum_chromosomes[idx,:,1], self.quantum_chromosomes[idx,:,0])\n        rotation = np.clip(angle_diff, -self.rotation_angle, self.rotation_angle)\n        new_angles = np.arctan2(self.quantum_chromosomes[idx,:,1], self.quantum_chromosomes[idx,:,0]) + rotation\n        self.quantum_chromosomes[idx,:] = np.array([np.cos(new_angles), np.sin(new_angles)]).T\n\n    def _quantum_crossover(self, parent1, parent2):\n        # Perform quantum crossover by averaging the angles\n        angles1 = np.arctan2(parent1[:,:,1], parent1[:,:,0])\n        angles2 = np.arctan2(parent2[:,:,1], parent2[:,:,0])\n        new_angles = (angles1 + angles2) / 2\n        return np.array([np.cos(new_angles), np.sin(new_angles)]).T\n\n    def _quantum_mutation(self, chromosome):\n        # Perform quantum mutation by adding a small random angle\n        mutation_angle = np.random.uniform(-self.rotation_angle/10, self.rotation_angle/10, self.dim)\n        angles = np.arctan2(chromosome[:,1], chromosome[:,0]) + mutation_angle\n        chromosome[:] = np.array([np.cos(angles), np.sin(angles)]).T\n\n    def __call__(self, func):\n        self.lb, self.ub = func.bounds.lb, func.bounds.ub\n        self._initialize_population(self.lb, self.ub)\n\n        eval_count = 0\n        while eval_count < self.budget:\n            scores = np.array([func(ind) for ind in self.population])\n            eval_count += self.population_size\n\n            best_idx = np.argmin(scores)\n            if scores[best_idx] < self.best_score:\n                self.best_score = scores[best_idx]\n                self.best_solution = self.population[best_idx]\n\n            # Evolutionary process\n            new_quantum_chromosomes = np.zeros_like(self.quantum_chromosomes)\n            for i in range(self.population_size):\n                self._quantum_rotation(i, best_idx)\n                partner_idx = np.random.randint(self.population_size)\n                new_quantum_chromosomes[i] = self._quantum_crossover(self.quantum_chromosomes[i:i+1], \n                                                                     self.quantum_chromosomes[partner_idx:partner_idx+1])\n                self._quantum_mutation(new_quantum_chromosomes[i])\n\n            self.quantum_chromosomes = new_quantum_chromosomes\n            self.population = self._quantum_to_real(self.quantum_chromosomes, self.lb, self.ub)\n\n        return self.best_solution, self.best_score", "name": "QuantumInspiredGeneticAlgorithm", "description": "Quantum-Inspired Genetic Algorithm (QIGA) leverages quantum rotations for population evolution, incorporating quantum gates into traditional genetic operators for enhanced exploration and exploitation.", "configspace": "", "generation": 69, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('could not broadcast input array from shape (2,1,2) into shape (2,2)').", "error": "ValueError('could not broadcast input array from shape (2,1,2) into shape (2,2)')", "parent_id": "886d3ed4-4dda-4108-9bfe-eb231d760865", "metadata": {}, "mutation_prompt": null}
{"id": "ef638629-db8f-4f5a-9498-b1de07bbafdf", "solution": "import numpy as np\n\nclass QuantumInspiredEvolutionaryAlgorithm:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 20 + self.dim\n        self.alpha = 0.5  # Coefficient for updating quantum rotation gates\n        self.probability_real = None\n        self.probability_imaginary = None\n        self.best_solution = None\n        self.best_score = np.inf\n\n    def _initialize_population(self):\n        # Initialize quantum states with equal probability\n        self.probability_real = np.full((self.pop_size, self.dim), 1/np.sqrt(2))\n        self.probability_imaginary = np.full((self.pop_size, self.dim), 1/np.sqrt(2))\n\n    def _measure_population(self, lb, ub):\n        # Collapse quantum states to a solution vector\n        collapse_real = np.random.rand(self.pop_size, self.dim) < (self.probability_real ** 2)\n        collapse_imaginary = np.random.rand(self.pop_size, self.dim) < (self.probability_imaginary ** 2)\n        solutions = np.where(collapse_real, lb, ub) + np.where(collapse_imaginary, 0, 1) * (ub - lb)\n        return solutions\n\n    def _update_quantum_states(self, best_solution, lb, ub):\n        # Update quantum states based on the best solution\n        for i in range(self.pop_size):\n            for j in range(self.dim):\n                delta_theta = self.alpha * ((best_solution[j] - lb[j]) / (ub[j] - lb[j]) - 0.5)\n                cos_theta = np.cos(delta_theta)\n                sin_theta = np.sin(delta_theta)\n                real_temp = self.probability_real[i, j]\n                self.probability_real[i, j] = cos_theta * self.probability_real[i, j] - sin_theta * self.probability_imaginary[i, j]\n                self.probability_imaginary[i, j] = sin_theta * real_temp + cos_theta * self.probability_imaginary[i, j]\n\n    def __call__(self, func):\n        self.lb, self.ub = func.bounds.lb, func.bounds.ub\n        self._initialize_population()\n\n        eval_count = 0\n        while eval_count < self.budget:\n            solutions = self._measure_population(self.lb, self.ub)\n\n            for sol in solutions:\n                if eval_count >= self.budget:\n                    break\n\n                score = func(sol)\n                eval_count += 1\n\n                if score < self.best_score:\n                    self.best_score = score\n                    self.best_solution = sol\n\n            self._update_quantum_states(self.best_solution, self.lb, self.ub)\n\n        return self.best_solution, self.best_score", "name": "QuantumInspiredEvolutionaryAlgorithm", "description": "Quantum-inspired Evolutionary Algorithm (QEA) leverages quantum superposition and entanglement principles to explore the solution space through probabilistic representation and evolutionary operators.", "configspace": "", "generation": 70, "fitness": 0.45968068642990834, "feedback": "The algorithm QuantumInspiredEvolutionaryAlgorithm got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.46 with standard deviation 0.00.", "error": "", "parent_id": "886d3ed4-4dda-4108-9bfe-eb231d760865", "metadata": {"aucs": [0.45955094021396814, 0.45981043264584853]}, "mutation_prompt": null}
{"id": "373cce69-4a53-4810-ad46-90bf8a3bc9eb", "solution": "import numpy as np\n\nclass QuantumInspiredPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20 + self.dim\n        self.particles = None\n        self.personal_best_positions = None\n        self.personal_best_scores = None\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.beta = 1.5  # Attraction strength\n        self.alpha = 0.75 # Step size for quantum-based update\n\n    def _initialize_population(self, lb, ub):\n        self.particles = np.random.rand(self.population_size, self.dim) * (ub - lb) + lb\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n\n    def _update_particles(self, lb, ub):\n        for i in range(self.population_size):\n            mean_best = (self.personal_best_positions[i] + self.global_best_position) / 2\n            u = np.random.rand(self.dim)\n            b = self.beta * np.abs(self.personal_best_positions[i] - self.global_best_position)\n            delta = np.random.uniform(-self.alpha, self.alpha, self.dim) * b / np.log(1 + u)\n            self.particles[i] = mean_best + delta\n            self.particles[i] = np.clip(self.particles[i], lb, ub)\n\n    def __call__(self, func):\n        self.lb, self.ub = func.bounds.lb, func.bounds.ub\n        self._initialize_population(self.lb, self.ub)\n\n        eval_count = 0\n        while eval_count < self.budget:\n            for i in range(self.population_size):\n                if eval_count >= self.budget:\n                    break\n\n                score = func(self.particles[i])\n                eval_count += 1\n\n                if score < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = score\n                    self.personal_best_positions[i] = self.particles[i]\n\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.particles[i]\n\n            self._update_particles(self.lb, self.ub)\n\n        return self.global_best_position, self.global_best_score", "name": "QuantumInspiredPSO", "description": "Quantum-inspired Particle Swarm Optimization (QPSO) integrates quantum mechanics principles to enhance exploration by probabilistically sampling new positions, maintaining a balance between exploration and exploitation.", "configspace": "", "generation": 71, "fitness": 0.6840634086711693, "feedback": "The algorithm QuantumInspiredPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.68 with standard deviation 0.23.", "error": "", "parent_id": "886d3ed4-4dda-4108-9bfe-eb231d760865", "metadata": {"aucs": [0.4528149085779054, 0.9153119087644332]}, "mutation_prompt": null}
{"id": "e7c9ee24-cc1d-40da-a660-faab95e25adf", "solution": "import numpy as np\n\nclass MultiStrategyDynamicSwarmOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20 + self.dim\n        self.strategies = [\n            {'name': 'default', 'omega': 0.5, 'phi_p': 1.5, 'phi_g': 1.5},\n            {'name': 'aggressive', 'omega': 0.7, 'phi_p': 2.0, 'phi_g': 2.0},\n            {'name': 'conservative', 'omega': 0.3, 'phi_p': 1.2, 'phi_g': 1.2},\n        ]\n        self.strategy_scores = np.zeros(len(self.strategies))\n        self.particles = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_scores = None\n        self.global_best_position = None\n        self.global_best_score = np.inf\n\n    def _initialize_population(self, lb, ub):\n        self.particles = np.random.rand(self.population_size, self.dim) * (ub - lb) + lb\n        self.velocities = np.random.rand(self.population_size, self.dim) * (ub - lb) / 10\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n\n    def _update_particles(self, lb, ub):\n        for i in range(self.population_size):\n            # Select strategy based on performance feedback\n            prob_distribution = self.strategy_scores + 1  # Avoid zero probability\n            prob_distribution /= prob_distribution.sum()\n            strategy_idx = np.random.choice(len(self.strategies), p=prob_distribution)\n            strategy = self.strategies[strategy_idx]\n\n            cognitive_component = strategy['phi_p'] * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.particles[i])\n            social_component = strategy['phi_g'] * np.random.rand(self.dim) * (self.global_best_position - self.particles[i])\n            inertia = strategy['omega']\n\n            self.velocities[i] = inertia * self.velocities[i] + cognitive_component + social_component\n            self.particles[i] += self.velocities[i]\n            self.particles[i] = np.clip(self.particles[i], lb, ub)\n\n    def __call__(self, func):\n        self.lb, self.ub = func.bounds.lb, func.bounds.ub\n        self._initialize_population(self.lb, self.ub)\n\n        eval_count = 0\n        while eval_count < self.budget:\n            for i in range(self.population_size):\n                if eval_count >= self.budget:\n                    break\n\n                score = func(self.particles[i])\n                eval_count += 1\n\n                if score < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = score\n                    self.personal_best_positions[i] = self.particles[i]\n\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.particles[i]\n                    # Update strategy score positively\n                    self.strategy_scores[strategy_idx] += 1\n\n            self._update_particles(self.lb, self.ub)\n\n        return self.global_best_position, self.global_best_score", "name": "MultiStrategyDynamicSwarmOptimization", "description": "Multi-Strategy Dynamic Swarm Optimization (MDSO) employs multiple search strategies with dynamic adaptation based on performance feedback to enhance adaptive exploration and exploitation in optimization.", "configspace": "", "generation": 72, "fitness": -Infinity, "feedback": "An exception occurred: NameError(\"name 'strategy_idx' is not defined\").", "error": "NameError(\"name 'strategy_idx' is not defined\")", "parent_id": "886d3ed4-4dda-4108-9bfe-eb231d760865", "metadata": {}, "mutation_prompt": null}
{"id": "d755725e-0f43-43ac-9656-83a885ae7536", "solution": "import numpy as np\n\nclass QuantumInspiredDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20 + self.dim\n        self.q_population = None  # Quantum population\n        self.population = None  # Real population\n        self.best_solution = None\n        self.best_score = np.inf\n        self.F = 0.5  # Differential weight\n        self.CR = 0.9  # Crossover probability\n\n    def _initialize_population(self, lb, ub):\n        self.q_population = np.random.rand(self.population_size, self.dim) * (ub - lb) + lb\n        self.population = np.random.rand(self.population_size, self.dim) * (ub - lb) + lb\n\n    def _quantum_rotation_gate(self, current, best, lb, ub):\n        delta = (best - current) * np.random.rand(self.dim)\n        theta = np.arctan2(delta, 1.0)\n        qrotation = np.cos(theta) * current + np.sin(theta) * delta\n        return np.clip(qrotation, lb, ub)\n\n    def _mutate(self, idx, lb, ub):\n        indices = [i for i in range(self.population_size) if i != idx]\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        mutant = self.population[a] + self.F * (self.population[b] - self.population[c])\n        return np.clip(mutant, lb, ub)\n\n    def _crossover(self, target, mutant):\n        trial = np.where(np.random.rand(self.dim) < self.CR, mutant, target)\n        return trial\n\n    def __call__(self, func):\n        self.lb, self.ub = func.bounds.lb, func.bounds.ub\n        self._initialize_population(self.lb, self.ub)\n\n        eval_count = 0\n        while eval_count < self.budget:\n            for i in range(self.population_size):\n                if eval_count >= self.budget:\n                    break\n\n                # Quantum-inspired update\n                self.q_population[i] = self._quantum_rotation_gate(self.q_population[i], self.best_solution or self.q_population[i], self.lb, self.ub)\n                self.population[i] = self.q_population[i]\n\n                # Apply differential evolution mutation and crossover\n                mutant = self._mutate(i, self.lb, self.ub)\n                trial = self._crossover(self.population[i], mutant)\n\n                # Evaluate trial and update the best solution\n                score = func(trial)\n                eval_count += 1\n\n                if score < self.best_score:\n                    self.best_score = score\n                    self.best_solution = trial\n\n                # Select between trial and current population\n                if score < func(self.population[i]):\n                    self.population[i] = trial\n\n        return self.best_solution, self.best_score", "name": "QuantumInspiredDifferentialEvolution", "description": "Quantum-Inspired Differential Evolution (QIDE) utilizes principles of quantum computing to enhance individual diversity and convergence through quantum rotation gates and adaptive differential evolution mutation strategies.", "configspace": "", "generation": 73, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()').", "error": "ValueError('The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()')", "parent_id": "886d3ed4-4dda-4108-9bfe-eb231d760865", "metadata": {}, "mutation_prompt": null}
{"id": "f8e955f5-b33f-45d1-9764-4bb909cd0fd3", "solution": "import numpy as np\n\nclass EnhancedAdaptiveMemorySearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20 + self.dim\n        self.memory_size = 5\n        self.particles = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_scores = None\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.omega = 0.5\n        self.phi_p = 1.5\n        self.phi_g = 1.5\n        self.memory = []\n        self.learning_rate = 0.05  # Adaptive learning rate for exploration\n        self.de_mutation_factor = 0.8\n        self.de_crossover_prob = 0.9\n\n    def _initialize_population(self, lb, ub):\n        self.particles = np.random.rand(self.population_size, self.dim) * (ub - lb) + lb\n        self.velocities = np.random.rand(self.population_size, self.dim) * (ub - lb) / 10\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n\n    def _update_particles(self, lb, ub):\n        for i in range(self.population_size):\n            if self.memory and np.random.rand() < 0.5:\n                strategy = self.memory[np.random.randint(len(self.memory))]\n                cognitive_component = strategy['phi_p'] * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.particles[i])\n                social_component = strategy['phi_g'] * np.random.rand(self.dim) * (self.global_best_position - self.particles[i])\n                inertia = strategy['omega']\n            else:\n                cognitive_component = self.phi_p * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.particles[i])\n                social_component = self.phi_g * np.random.rand(self.dim) * (self.global_best_position - self.particles[i])\n                inertia = self.omega\n\n            self.velocities[i] = inertia * self.velocities[i] + cognitive_component + social_component\n            self.velocities[i] *= (1.0 + self.learning_rate * np.random.randn(self.dim))  # Adaptive learning\n            self.particles[i] += self.velocities[i]\n            self.particles[i] = np.clip(self.particles[i], lb, ub)\n            \n            # Local search using Differential Evolution (DE)\n            new_solution = self._differential_evolution(lb, ub, i)\n            self.particles[i] = np.where(np.random.rand(self.dim) < self.de_crossover_prob, new_solution, self.particles[i])\n\n    def _differential_evolution(self, lb, ub, idx):\n        idxs = [i for i in range(self.population_size) if i != idx]\n        a, b, c = np.random.choice(idxs, 3, replace=False)\n        candidate = self.particles[idx] + self.de_mutation_factor * (self.particles[a] - self.particles[b] + self.particles[c] - self.particles[idx])\n        return np.clip(candidate, lb, ub)\n\n    def __call__(self, func):\n        self.lb, self.ub = func.bounds.lb, func.bounds.ub\n        self._initialize_population(self.lb, self.ub)\n\n        eval_count = 0\n        while eval_count < self.budget:\n            for i in range(self.population_size):\n                if eval_count >= self.budget:\n                    break\n\n                score = func(self.particles[i])\n                eval_count += 1\n\n                if score < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = score\n                    self.personal_best_positions[i] = self.particles[i]\n\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.particles[i]\n                    if len(self.memory) >= self.memory_size:\n                        self.memory.pop(0)\n                    self.memory.append({'omega': self.omega, 'phi_p': self.phi_p, 'phi_g': self.phi_g})\n                    self.omega = np.clip(self.omega + np.random.uniform(-0.1, 0.1), 0.3, 0.7)\n                    self.phi_p = np.clip(self.phi_p + np.random.uniform(-0.1, 0.1), 1.2, 1.8)\n                    self.phi_g = np.clip(self.phi_g + np.random.uniform(-0.1, 0.1), 1.2, 1.8)\n\n            self._update_particles(self.lb, self.ub)\n\n        return self.global_best_position, self.global_best_score", "name": "EnhancedAdaptiveMemorySearch", "description": "Enhanced Adaptive Memory Search (EAMS) leverages dynamic strategy adaptation by incorporating local search via Differential Evolution (DE) and adaptive learning rates, enhancing exploration and exploitation balance for improved convergence.", "configspace": "", "generation": 74, "fitness": 0.6262426623985968, "feedback": "The algorithm EnhancedAdaptiveMemorySearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.63 with standard deviation 0.06.", "error": "", "parent_id": "886d3ed4-4dda-4108-9bfe-eb231d760865", "metadata": {"aucs": [0.6831154648504947, 0.5693698599466989]}, "mutation_prompt": null}
{"id": "121c83d8-12b5-455a-a3a7-ca35731b8aab", "solution": "import numpy as np\n\nclass QuantumInspiredStochasticOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20 + self.dim\n        self.quantum_bits = np.random.rand(self.population_size, self.dim)  # Quantum bit representation\n        self.observed_states = np.zeros((self.population_size, self.dim))  # Observed states from quantum bits\n        self.best_solution = None\n        self.best_score = np.inf\n        self.rotation_angle = np.pi / 4  # Rotation angle for quantum gates\n\n    def _observe_states(self, lb, ub):\n        \"\"\" Observe the quantum states to generate actual solution candidates \"\"\"\n        self.observed_states = lb + (ub - lb) * (self.quantum_bits > 0.5).astype(float)\n\n    def _apply_quantum_rotation(self):\n        \"\"\" Apply a quantum-inspired rotation gate to the quantum bits \"\"\"\n        for i in range(self.population_size):\n            for d in range(self.dim):\n                if np.random.rand() < 0.5:\n                    # Quantum rotation\n                    self.quantum_bits[i, d] = (self.quantum_bits[i, d] + self.rotation_angle) % 1.0\n                else:\n                    # Quantum rotation in the opposite direction\n                    self.quantum_bits[i, d] = (self.quantum_bits[i, d] - self.rotation_angle) % 1.0\n\n    def __call__(self, func):\n        self.lb, self.ub = func.bounds.lb, func.bounds.ub\n        eval_count = 0\n\n        while eval_count < self.budget:\n            self._observe_states(self.lb, self.ub)\n            for i in range(self.population_size):\n                if eval_count >= self.budget:\n                    break\n                score = func(self.observed_states[i])\n                eval_count += 1\n\n                if score < self.best_score:\n                    self.best_score = score\n                    self.best_solution = np.copy(self.observed_states[i])\n\n            self._apply_quantum_rotation()\n\n        return self.best_solution, self.best_score", "name": "QuantumInspiredStochasticOptimization", "description": "Quantum-Inspired Stochastic Optimization (QISO) leverages quantum superposition principles to explore multiple states simultaneously, enhancing exploration and convergence efficiency with quantum-inspired rotation gates.", "configspace": "", "generation": 75, "fitness": 0.32231840525168587, "feedback": "The algorithm QuantumInspiredStochasticOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.32 with standard deviation 0.00.", "error": "", "parent_id": "886d3ed4-4dda-4108-9bfe-eb231d760865", "metadata": {"aucs": [0.3223036038748829, 0.32233320662848886]}, "mutation_prompt": null}
{"id": "c0ad111e-7da2-479a-93cb-e8b942145335", "solution": "import numpy as np\n\nclass QuantumParticleSwarmOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20 + self.dim\n        self.particles = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_scores = None\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.alpha = 0.5  # Contraction-expansion coefficient\n        self.beta = 1.5  # Attraction strength\n\n    def _initialize_population(self, lb, ub):\n        self.particles = np.random.rand(self.population_size, self.dim) * (ub - lb) + lb\n        self.velocities = np.random.rand(self.population_size, self.dim) * (ub - lb) / 10\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n        self.global_best_position = np.random.rand(self.dim) * (ub - lb) + lb\n\n    def _quantum_update(self, lb, ub):\n        for i in range(self.population_size):\n            phi = np.random.rand(self.dim)\n            p = phi * self.personal_best_positions[i] + (1 - phi) * self.global_best_position\n            u = np.random.rand(self.dim) - 0.5\n            self.particles[i] = p + self.alpha * np.abs(self.global_best_position - self.particles[i]) * np.log(1/u)\n            self.particles[i] = np.clip(self.particles[i], lb, ub)\n\n    def __call__(self, func):\n        self.lb, self.ub = func.bounds.lb, func.bounds.ub\n        self._initialize_population(self.lb, self.ub)\n\n        eval_count = 0\n        while eval_count < self.budget:\n            for i in range(self.population_size):\n                if eval_count >= self.budget:\n                    break\n\n                score = func(self.particles[i])\n                eval_count += 1\n\n                if score < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = score\n                    self.personal_best_positions[i] = self.particles[i]\n\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.particles[i]\n\n            self._quantum_update(self.lb, self.ub)\n\n        return self.global_best_position, self.global_best_score", "name": "QuantumParticleSwarmOptimization", "description": "Quantum Particle Swarm Optimization (QPSO) leverages quantum mechanics principles by incorporating a probabilistic update mechanism, improving exploration and avoiding local optima.", "configspace": "", "generation": 76, "fitness": 0.5707517379226714, "feedback": "The algorithm QuantumParticleSwarmOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.57 with standard deviation 0.05.", "error": "", "parent_id": "886d3ed4-4dda-4108-9bfe-eb231d760865", "metadata": {"aucs": [0.6210013425445406, 0.5205021333008021]}, "mutation_prompt": null}
{"id": "b1fc0811-497c-45c1-bee2-880a1bb99915", "solution": "import numpy as np\n\nclass EnhancedAdaptiveMemorySearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20 + self.dim\n        self.memory_size = 5  # Number of successful strategies to remember\n        self.particles = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_scores = None\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.omega = 0.5  # Initial inertia weight\n        self.phi_p = 1.5  # Cognitive coefficient\n        self.phi_g = 1.5  # Social coefficient\n        self.memory = []  # Initialize adaptive memory\n\n    def _initialize_population(self, lb, ub):\n        self.particles = np.random.rand(self.population_size, self.dim) * (ub - lb) + lb\n        self.velocities = np.random.rand(self.population_size, self.dim) * (ub - lb) / 10\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n\n    def _update_particles(self, lb, ub, eval_ratio):\n        dynamic_omega = np.clip(self.omega * (1 - eval_ratio), 0.3, 0.7)\n        for i in range(self.population_size):\n            if self.memory and np.random.rand() < 0.5:\n                strategy = self.memory[np.random.randint(len(self.memory))]\n                cognitive_component = strategy['phi_p'] * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.particles[i])\n                social_component = strategy['phi_g'] * np.random.rand(self.dim) * (self.global_best_position - self.particles[i])\n                inertia = strategy['omega']\n            else:\n                cognitive_component = self.phi_p * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.particles[i])\n                social_component = self.phi_g * np.random.rand(self.dim) * (self.global_best_position - self.particles[i])\n                inertia = dynamic_omega\n\n            velocity_scale = 1 + 0.1 * np.random.randn()\n            self.velocities[i] = inertia * self.velocities[i] * velocity_scale + cognitive_component + social_component\n            self.particles[i] += self.velocities[i]\n            self.particles[i] = np.clip(self.particles[i], lb, ub)\n\n    def __call__(self, func):\n        self.lb, self.ub = func.bounds.lb, func.bounds.ub\n        self._initialize_population(self.lb, self.ub)\n\n        eval_count = 0\n        while eval_count < self.budget:\n            eval_ratio = eval_count / self.budget\n            for i in range(self.population_size):\n                if eval_count >= self.budget:\n                    break\n\n                score = func(self.particles[i])\n                eval_count += 1\n\n                if score < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = score\n                    self.personal_best_positions[i] = self.particles[i]\n\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.particles[i]\n                    if len(self.memory) >= self.memory_size:\n                        self.memory.pop(0)\n                    self.memory.append({'omega': self.omega, 'phi_p': self.phi_p, 'phi_g': self.phi_g})\n                    self.omega = np.clip(self.omega + np.random.uniform(-0.1, 0.1), 0.3, 0.7)\n                    self.phi_p = np.clip(self.phi_p + np.random.uniform(-0.1, 0.1), 1.2, 1.8)\n                    self.phi_g = np.clip(self.phi_g + np.random.uniform(-0.1, 0.1), 1.2, 1.8)\n\n            self._update_particles(self.lb, self.ub, eval_ratio)\n\n        return self.global_best_position, self.global_best_score", "name": "EnhancedAdaptiveMemorySearch", "description": "Enhanced Adaptive Memory Search (EAMS) introduces adaptive velocity scaling and dynamic inertia adjustment to improve exploration and convergence in high-dimensional spaces.", "configspace": "", "generation": 77, "fitness": 0.9322236912223445, "feedback": "The algorithm EnhancedAdaptiveMemorySearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.93 with standard deviation 0.01.", "error": "", "parent_id": "886d3ed4-4dda-4108-9bfe-eb231d760865", "metadata": {"aucs": [0.9397425694504392, 0.9247048129942498]}, "mutation_prompt": null}
{"id": "2bc374f8-c68e-4a41-b685-00f518f6db4b", "solution": "import numpy as np\n\nclass EnhancedAdaptiveMemorySearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20 + self.dim\n        self.memory_size = 5\n        self.subpopulation_size = self.population_size // 2\n        self.particles = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_scores = None\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.omega = 0.5\n        self.phi_p = 1.5\n        self.phi_g = 1.5\n        self.memory = []\n\n    def _initialize_population(self, lb, ub):\n        self.particles = np.random.rand(self.population_size, self.dim) * (ub - lb) + lb\n        self.velocities = np.random.rand(self.population_size, self.dim) * (ub - lb) / 10\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n\n    def _update_particles(self, lb, ub):\n        for i in range(self.population_size):\n            # Select strategy with mutation\n            if self.memory and np.random.rand() < 0.5:\n                strategy = self.memory[np.random.randint(len(self.memory))]\n                omega = strategy['omega'] + np.random.uniform(-0.05, 0.05)\n                phi_p = strategy['phi_p'] + np.random.uniform(-0.05, 0.05)\n                phi_g = strategy['phi_g'] + np.random.uniform(-0.05, 0.05)\n            else:\n                omega = self.omega\n                phi_p = self.phi_p\n                phi_g = self.phi_g\n\n            cognitive_component = phi_p * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.particles[i])\n            social_component = phi_g * np.random.rand(self.dim) * (self.global_best_position - self.particles[i])\n            self.velocities[i] = omega * self.velocities[i] + cognitive_component + social_component\n            self.particles[i] += self.velocities[i]\n            self.particles[i] = np.clip(self.particles[i], lb, ub)\n\n    def _manage_subpopulations(self):\n        subpopulations = np.array_split(self.particles, 2)\n        for subpopulation in subpopulations:\n            # Shuffle strategies within subpopulations\n            np.random.shuffle(subpopulation)\n\n    def __call__(self, func):\n        self.lb, self.ub = func.bounds.lb, func.bounds.ub\n        self._initialize_population(self.lb, self.ub)\n\n        eval_count = 0\n        while eval_count < self.budget:\n            self._manage_subpopulations()\n\n            for i in range(self.population_size):\n                if eval_count >= self.budget:\n                    break\n\n                score = func(self.particles[i])\n                eval_count += 1\n\n                if score < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = score\n                    self.personal_best_positions[i] = self.particles[i]\n\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.particles[i]\n                    if len(self.memory) >= self.memory_size:\n                        self.memory.pop(0)\n                    self.memory.append({'omega': self.omega, 'phi_p': self.phi_p, 'phi_g': self.phi_g})\n                    self.omega = np.clip(self.omega + np.random.uniform(-0.05, 0.05), 0.3, 0.7)\n                    self.phi_p = np.clip(self.phi_p + np.random.uniform(-0.05, 0.05), 1.2, 1.8)\n                    self.phi_g = np.clip(self.phi_g + np.random.uniform(-0.05, 0.05), 1.2, 1.8)\n\n            self._update_particles(self.lb, self.ub)\n\n        return self.global_best_position, self.global_best_score", "name": "EnhancedAdaptiveMemorySearch", "description": "Enhanced Adaptive Memory Search (EAMS) introduces diversity into the search process by incorporating dynamic subpopulation management and strategy mutation, aiming to improve convergence speed and solution quality.", "configspace": "", "generation": 78, "fitness": 0.6900848613089714, "feedback": "The algorithm EnhancedAdaptiveMemorySearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.69 with standard deviation 0.02.", "error": "", "parent_id": "886d3ed4-4dda-4108-9bfe-eb231d760865", "metadata": {"aucs": [0.7138640477446807, 0.6663056748732621]}, "mutation_prompt": null}
{"id": "dd09a210-3884-4b97-81f4-7ce8fa9f9205", "solution": "import numpy as np\n\nclass DynamicStrategyPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20 + self.dim\n        self.memory_size = 5\n        self.particles = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_scores = None\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.omega = 0.7\n        self.phi_p = 1.4\n        self.phi_g = 1.6\n        self.memory = []\n        self.neighborhood_size = max(3, self.dim // 5)  # Neighborhood size for local exploration\n\n    def _initialize_population(self, lb, ub):\n        self.particles = np.random.rand(self.population_size, self.dim) * (ub - lb) + lb\n        self.velocities = np.random.rand(self.population_size, self.dim) * (ub - lb) / 10\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n\n    def _update_particles(self, lb, ub):\n        for i in range(self.population_size):\n            if self.memory and np.random.rand() < 0.5:\n                strategy = self.memory[np.random.randint(len(self.memory))]\n                inertia = strategy['omega']\n                phi_p = strategy['phi_p']\n                phi_g = strategy['phi_g']\n            else:\n                inertia = self.omega\n                phi_p = self.phi_p\n                phi_g = self.phi_g\n            \n            neighbors = self._get_neighbors(i)\n            local_best_position = min(neighbors, key=lambda n: self.personal_best_scores[n])\n            local_best_position = self.personal_best_positions[local_best_position]\n\n            cognitive_component = phi_p * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.particles[i])\n            social_component = phi_g * np.random.rand(self.dim) * (local_best_position - self.particles[i])\n\n            self.velocities[i] = inertia * self.velocities[i] + cognitive_component + social_component\n            self.particles[i] += self.velocities[i]\n            self.particles[i] = np.clip(self.particles[i], lb, ub)\n\n    def _get_neighbors(self, index):\n        half_size = self.neighborhood_size // 2\n        indices = np.arange(index - half_size, index + half_size + 1)\n        return indices % self.population_size\n\n    def __call__(self, func):\n        self.lb, self.ub = func.bounds.lb, func.bounds.ub\n        self._initialize_population(self.lb, self.ub)\n\n        eval_count = 0\n        while eval_count < self.budget:\n            for i in range(self.population_size):\n                if eval_count >= self.budget:\n                    break\n\n                score = func(self.particles[i])\n                eval_count += 1\n\n                if score < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = score\n                    self.personal_best_positions[i] = self.particles[i]\n\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.particles[i]\n                    if len(self.memory) >= self.memory_size:\n                        self.memory.pop(0)\n                    self.memory.append({'omega': self.omega, 'phi_p': self.phi_p, 'phi_g': self.phi_g})\n                    self.omega = np.clip(self.omega + np.random.uniform(-0.05, 0.05), 0.3, 0.9)\n                    self.phi_p = np.clip(self.phi_p + np.random.uniform(-0.05, 0.05), 1.0, 2.0)\n                    self.phi_g = np.clip(self.phi_g + np.random.uniform(-0.05, 0.05), 1.0, 2.0)\n\n            self._update_particles(self.lb, self.ub)\n\n        return self.global_best_position, self.global_best_score", "name": "DynamicStrategyPSO", "description": "Dynamic Strategy PSO enhances Adaptive Memory Search by introducing dynamic strategy adaptation, utilizing neighborhood-based communication to improve convergence on complex landscapes.", "configspace": "", "generation": 79, "fitness": 0.8423086370852098, "feedback": "The algorithm DynamicStrategyPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.84 with standard deviation 0.06.", "error": "", "parent_id": "886d3ed4-4dda-4108-9bfe-eb231d760865", "metadata": {"aucs": [0.778610009348897, 0.9060072648215225]}, "mutation_prompt": null}
{"id": "9378fc00-cc0f-474d-93a9-25a24e85d678", "solution": "import numpy as np\n\nclass DynamicAdaptiveParticleSwarmOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20 + self.dim\n        self.memory_size = 5\n        self.particles = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_scores = None\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.omega = 0.5\n        self.phi_p = 1.5\n        self.phi_g = 1.5\n        self.memory = []\n        self.no_improvement_count = 0\n        self.diversity_threshold = 10\n\n    def _initialize_population(self, lb, ub):\n        self.particles = np.random.rand(self.population_size, self.dim) * (ub - lb) + lb\n        self.velocities = np.random.rand(self.population_size, self.dim) * (ub - lb) / 10\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n\n    def _update_particles(self, lb, ub):\n        for i in range(self.population_size):\n            if self.memory and np.random.rand() < 0.5:\n                strategy = self.memory[np.random.randint(len(self.memory))]\n                cognitive_component = strategy['phi_p'] * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.particles[i])\n                social_component = strategy['phi_g'] * np.random.rand(self.dim) * (self.global_best_position - self.particles[i])\n                inertia = strategy['omega']\n            else:\n                cognitive_component = self.phi_p * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.particles[i])\n                social_component = self.phi_g * np.random.rand(self.dim) * (self.global_best_position - self.particles[i])\n                inertia = self.omega\n\n            self.velocities[i] = inertia * self.velocities[i] + cognitive_component + social_component\n            self.particles[i] += self.velocities[i]\n            self.particles[i] = np.clip(self.particles[i], lb, ub)\n\n    def _restore_diversity(self, lb, ub):\n        diversity = np.mean(np.std(self.particles, axis=0))\n        if diversity < self.diversity_threshold:\n            indices = np.random.choice(self.population_size, self.population_size // 2, replace=False)\n            self.particles[indices] = np.random.rand(len(indices), self.dim) * (ub - lb) + lb\n\n    def __call__(self, func):\n        self.lb, self.ub = func.bounds.lb, func.bounds.ub\n        self._initialize_population(self.lb, self.ub)\n\n        eval_count = 0\n        while eval_count < self.budget:\n            for i in range(self.population_size):\n                if eval_count >= self.budget:\n                    break\n\n                score = func(self.particles[i])\n                eval_count += 1\n\n                if score < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = score\n                    self.personal_best_positions[i] = self.particles[i]\n\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.particles[i]\n                    self.no_improvement_count = 0\n                    if len(self.memory) >= self.memory_size:\n                        self.memory.pop(0)\n                    self.memory.append({'omega': self.omega, 'phi_p': self.phi_p, 'phi_g': self.phi_g})\n                    self.omega = np.clip(self.omega + np.random.uniform(-0.1, 0.1), 0.3, 0.7)\n                    self.phi_p = np.clip(self.phi_p + np.random.uniform(-0.1, 0.1), 1.2, 1.8)\n                    self.phi_g = np.clip(self.phi_g + np.random.uniform(-0.1, 0.1), 1.2, 1.8)\n                else:\n                    self.no_improvement_count += 1\n\n            if self.no_improvement_count > self.population_size:\n                self._restore_diversity(self.lb, self.ub)\n                self.no_improvement_count = 0\n\n            self._update_particles(self.lb, self.ub)\n\n        return self.global_best_position, self.global_best_score", "name": "DynamicAdaptiveParticleSwarmOptimization", "description": "Dynamic Adaptive Particle Swarm Optimization (DAPSO) enhances AMS by introducing dynamic parameter adaptation and diversity restoration to prevent premature convergence and improve exploration.", "configspace": "", "generation": 80, "fitness": 0.9272696970942154, "feedback": "The algorithm DynamicAdaptiveParticleSwarmOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.93 with standard deviation 0.00.", "error": "", "parent_id": "886d3ed4-4dda-4108-9bfe-eb231d760865", "metadata": {"aucs": [0.9281794905831127, 0.9263599036053181]}, "mutation_prompt": null}
{"id": "0c8c7356-284b-4273-9e0f-85d2c0efec6b", "solution": "import numpy as np\n\nclass QuantumInspiredEvolutionaryAlgorithm:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20 + self.dim\n        self.qubit_population = None\n        self.best_solution = None\n        self.best_score = np.inf\n        self.rotation_angle = 0.05  # Rotation angle for quantum gates\n\n    def _initialize_qubit_population(self):\n        # Initialize qubit representation with equal probabilities\n        self.qubit_population = np.full((self.population_size, self.dim, 2), 1/np.sqrt(2))\n\n    def _measure_population(self, lb, ub):\n        # Convert qubit representation to real values\n        real_values = np.zeros((self.population_size, self.dim))\n        for i in range(self.population_size):\n            for j in range(self.dim):\n                # Measure qubit and obtain real value\n                prob_zero = self.qubit_population[i, j, 0]**2\n                real_values[i, j] = lb[j] if np.random.rand() < prob_zero else ub[j]\n        return real_values\n\n    def _update_qubit_population(self, real_values, scores, lb, ub):\n        # Update qubit states using a quantum-inspired rotation gate\n        for i in range(self.population_size):\n            if scores[i] < self.best_score:\n                self.best_score = scores[i]\n                self.best_solution = real_values[i]\n\n            for j in range(self.dim):\n                best_bit = (self.best_solution[j] - lb[j]) / (ub[j] - lb[j])\n                current_bit = (real_values[i, j] - lb[j]) / (ub[j] - lb[j])\n                # Calculate rotation direction and apply rotation\n                rotation_direction = np.sign(best_bit - current_bit)\n                theta = rotation_direction * self.rotation_angle\n                cos_theta = np.cos(theta)\n                sin_theta = np.sin(theta)\n\n                a, b = self.qubit_population[i, j, 0], self.qubit_population[i, j, 1]\n                self.qubit_population[i, j, 0] = cos_theta * a - sin_theta * b\n                self.qubit_population[i, j, 1] = sin_theta * a + cos_theta * b\n\n                # Normalize qubits to ensure valid probability distribution\n                norm = np.linalg.norm(self.qubit_population[i, j])\n                self.qubit_population[i, j] /= norm\n\n    def __call__(self, func):\n        self.lb, self.ub = func.bounds.lb, func.bounds.ub\n        self._initialize_qubit_population()\n\n        eval_count = 0\n        while eval_count < self.budget:\n            real_values = self._measure_population(self.lb, self.ub)\n\n            scores = np.zeros(self.population_size)\n            for i in range(self.population_size):\n                if eval_count >= self.budget:\n                    break\n\n                scores[i] = func(real_values[i])\n                eval_count += 1\n\n            self._update_qubit_population(real_values, scores, self.lb, self.ub)\n\n        return self.best_solution, self.best_score", "name": "QuantumInspiredEvolutionaryAlgorithm", "description": "Quantum-inspired Evolutionary Algorithm (QEA) leverages quantum superposition principles to enhance population diversity and accelerate convergence through unique rotation gates and probabilistic selection.", "configspace": "", "generation": 81, "fitness": 0.3620051136037932, "feedback": "The algorithm QuantumInspiredEvolutionaryAlgorithm got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.36 with standard deviation 0.00.", "error": "", "parent_id": "886d3ed4-4dda-4108-9bfe-eb231d760865", "metadata": {"aucs": [0.3621690802411124, 0.36184114696647407]}, "mutation_prompt": null}
{"id": "40326841-1c60-4348-ae8e-9b854d721e62", "solution": "import numpy as np\n\nclass QuantumInspiredDynamicSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20 + self.dim\n        self.memory_size = 5\n        self.particles = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_scores = None\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.alpha = 0.9  # Learning parameter\n        self.beta = np.pi / 4  # Phase shift in quantum context\n        self.memory = []\n\n    def _initialize_population(self, lb, ub):\n        self.particles = np.random.rand(self.population_size, self.dim) * (ub - lb) + lb\n        self.velocities = np.random.rand(self.population_size, self.dim) * (ub - lb) / 10\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n\n    def _update_particles(self, lb, ub):\n        for i in range(self.population_size):\n            if self.memory and np.random.rand() < 0.5:\n                strategy = self.memory[np.random.randint(len(self.memory))]\n                alpha = strategy['alpha']\n                beta = strategy['beta']\n            else:\n                alpha = self.alpha\n                beta = self.beta\n\n            phase_shift = np.cos(beta) * np.random.rand(self.dim)\n            velocity_update = alpha * (self.personal_best_positions[i] - self.particles[i]) + phase_shift * (self.global_best_position - self.particles[i])\n            self.velocities[i] = self.velocities[i] + velocity_update\n            self.particles[i] += self.velocities[i]\n            self.particles[i] = np.clip(self.particles[i], lb, ub)\n\n    def __call__(self, func):\n        self.lb, self.ub = func.bounds.lb, func.bounds.ub\n        self._initialize_population(self.lb, self.ub)\n        \n        eval_count = 0\n        while eval_count < self.budget:\n            for i in range(self.population_size):\n                if eval_count >= self.budget:\n                    break\n\n                score = func(self.particles[i])\n                eval_count += 1\n\n                if score < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = score\n                    self.personal_best_positions[i] = self.particles[i]\n\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.particles[i]\n\n                    if len(self.memory) >= self.memory_size:\n                        self.memory.pop(0)\n                    self.memory.append({'alpha': self.alpha, 'beta': self.beta})\n\n                    self.alpha = np.clip(self.alpha + np.random.uniform(-0.05, 0.05), 0.8, 1.0)\n                    self.beta = np.clip(self.beta + np.random.uniform(-np.pi/16, np.pi/16), np.pi/8, np.pi/2)\n\n            self._update_particles(self.lb, self.ub)\n\n        return self.global_best_position, self.global_best_score", "name": "QuantumInspiredDynamicSearch", "description": "Quantum-Inspired Dynamic Search (QIDS) utilizes quantum-inspired principles such as superposition and entanglement to enhance exploration capacities, dynamically adjusting search directions based on quantum states.", "configspace": "", "generation": 82, "fitness": 0.6957932894005583, "feedback": "The algorithm QuantumInspiredDynamicSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.70 with standard deviation 0.01.", "error": "", "parent_id": "886d3ed4-4dda-4108-9bfe-eb231d760865", "metadata": {"aucs": [0.7038208644880001, 0.6877657143131166]}, "mutation_prompt": null}
{"id": "29d78332-980a-4832-a10c-5421717f4f25", "solution": "import numpy as np\n\nclass QuantumInspiredDynamicSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20 + self.dim\n        self.qubits = None\n        self.population = None\n        self.best_position = None\n        self.best_score = np.inf\n        self.alpha = 0.5  # Quantum exploration weight\n        self.beta = 1.0  # Exploitation weight\n        self.adaptive_control = 0.1  # Adaptation rate\n\n    def _initialize_qubits(self):\n        self.qubits = np.random.rand(self.population_size, self.dim, 2)\n        self.qubits /= np.linalg.norm(self.qubits, axis=2, keepdims=True)\n\n    def _quantum_measurement(self, lb, ub):\n        self.population = np.argmax(np.random.rand(self.population_size, self.dim, 1) < self.qubits[:, :, :1], axis=2)\n        self.population = self.population * (ub - lb) + lb\n\n    def _update_qubits(self, scores):\n        for i in range(self.population_size):\n            if scores[i] < self.best_score:\n                self.best_score = scores[i]\n                self.best_position = self.population[i]\n\n            # Quantum rotation gates\n            theta = self.alpha * np.random.rand(self.dim) * (self.population[i] - self.best_position) / (ub - lb)\n            self.qubits[i, :, 0] = self.qubits[i, :, 0] * np.cos(theta) - self.qubits[i, :, 1] * np.sin(theta)\n            self.qubits[i, :, 1] = self.qubits[i, :, 0] * np.sin(theta) + self.qubits[i, :, 1] * np.cos(theta)\n            self.qubits[i] /= np.linalg.norm(self.qubits[i], axis=1, keepdims=True)\n\n    def _adaptive_exploitation(self):\n        self.alpha = np.clip(self.alpha + self.adaptive_control * np.random.uniform(-0.2, 0.2), 0.3, 0.7)\n        self.beta = np.clip(self.beta + self.adaptive_control * np.random.uniform(-0.2, 0.2), 0.8, 1.2)\n\n    def __call__(self, func):\n        self.lb, self.ub = func.bounds.lb, func.bounds.ub\n        self._initialize_qubits()\n\n        eval_count = 0\n        while eval_count < self.budget:\n            self._quantum_measurement(self.lb, self.ub)\n            scores = np.array([func(self.population[i]) for i in range(self.population_size)])\n            eval_count += self.population_size\n\n            self._update_qubits(scores)\n            self._adaptive_exploitation()\n\n        return self.best_position, self.best_score", "name": "QuantumInspiredDynamicSearch", "description": "Quantum-Inspired Dynamic Search (QIDS) utilizes quantum superposition principles to enhance global exploration with dynamic adaptive control for local exploitation, aiming to converge effectively on complex landscapes.", "configspace": "", "generation": 83, "fitness": -Infinity, "feedback": "An exception occurred: NameError(\"name 'ub' is not defined\").", "error": "NameError(\"name 'ub' is not defined\")", "parent_id": "886d3ed4-4dda-4108-9bfe-eb231d760865", "metadata": {}, "mutation_prompt": null}
{"id": "a3046736-2107-4620-8839-c9af09a1a025", "solution": "import numpy as np\n\nclass QuantumInspiredPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20 + self.dim\n        self.particles = None\n        self.personal_best_positions = None\n        self.personal_best_scores = None\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.beta = 1.5  # Quantum potential impact\n\n    def _initialize_population(self, lb, ub):\n        self.particles = np.random.rand(self.population_size, self.dim) * (ub - lb) + lb\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n\n    def _update_particles(self, lb, ub):\n        mean_best_position = np.mean(self.personal_best_positions, axis=0)\n        for i in range(self.population_size):\n            quantum_potential = self.beta * np.random.rand(self.dim) * (mean_best_position - self.particles[i])\n            self.particles[i] = self.personal_best_positions[i] + quantum_potential\n            self.particles[i] = np.clip(self.particles[i], lb, ub)\n\n    def __call__(self, func):\n        self.lb, self.ub = func.bounds.lb, func.bounds.ub\n        self._initialize_population(self.lb, self.ub)\n\n        eval_count = 0\n        while eval_count < self.budget:\n            for i in range(self.population_size):\n                if eval_count >= self.budget:\n                    break\n\n                score = func(self.particles[i])\n                eval_count += 1\n\n                if score < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = score\n                    self.personal_best_positions[i] = self.particles[i]\n\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.particles[i]\n\n            self._update_particles(self.lb, self.ub)\n\n        return self.global_best_position, self.global_best_score", "name": "QuantumInspiredPSO", "description": "Quantum-inspired Particle Swarm Optimization (QPSO) leverages quantum mechanics principles to enhance convergence and exploration by using quantum potential fields to guide the swarm movement.", "configspace": "", "generation": 84, "fitness": 0.8247414169501206, "feedback": "The algorithm QuantumInspiredPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.82 with standard deviation 0.13.", "error": "", "parent_id": "886d3ed4-4dda-4108-9bfe-eb231d760865", "metadata": {"aucs": [0.9521286954498459, 0.6973541384503954]}, "mutation_prompt": null}
{"id": "4e9eb8f9-53df-4061-be8a-12826fbb31e3", "solution": "import numpy as np\n\nclass EnhancedAdaptiveMemorySearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20 + self.dim\n        self.memory_size = 5  # Number of successful strategies to remember\n        self.elite_size = max(1, self.population_size // 10)  # Elite selection size\n        self.particles = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_scores = None\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.omega = 0.5  # Inertia weight\n        self.phi_p = 1.5  # Cognitive coefficient\n        self.phi_g = 1.5  # Social coefficient\n        self.memory = []  # Initialize adaptive memory\n\n    def _initialize_population(self, lb, ub):\n        self.particles = np.random.rand(self.population_size, self.dim) * (ub - lb) + lb\n        self.velocities = np.random.rand(self.population_size, self.dim) * (ub - lb) / 10\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n\n    def _update_particles(self, lb, ub):\n        for i in range(self.population_size):\n            if self.memory and np.random.rand() < 0.5:\n                strategy = self.memory[np.random.randint(len(self.memory))]\n                cognitive_component = strategy['phi_p'] * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.particles[i])\n                social_component = strategy['phi_g'] * np.random.rand(self.dim) * (self.global_best_position - self.particles[i])\n                inertia = strategy['omega']\n            else:\n                cognitive_component = self.phi_p * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.particles[i])\n                social_component = self.phi_g * np.random.rand(self.dim) * (self.global_best_position - self.particles[i])\n                inertia = self.omega\n\n            self.velocities[i] = inertia * self.velocities[i] + cognitive_component + social_component\n            self.particles[i] += self.velocities[i]\n            self.particles[i] = np.clip(self.particles[i], lb, ub)\n\n    def _elite_selection(self):\n        elite_indices = np.argsort(self.personal_best_scores)[:self.elite_size]\n        return self.particles[elite_indices]\n\n    def __call__(self, func):\n        self.lb, self.ub = func.bounds.lb, func.bounds.ub\n        self._initialize_population(self.lb, self.ub)\n\n        eval_count = 0\n        while eval_count < self.budget:\n            for i in range(self.population_size):\n                if eval_count >= self.budget:\n                    break\n\n                score = func(self.particles[i])\n                eval_count += 1\n\n                if score < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = score\n                    self.personal_best_positions[i] = self.particles[i]\n\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.particles[i]\n                    if len(self.memory) >= self.memory_size:\n                        self.memory.pop(0)\n                    self.memory.append({'omega': self.omega, 'phi_p': self.phi_p, 'phi_g': self.phi_g})\n                    self.omega = np.clip(self.omega + np.random.uniform(-0.1, 0.1), 0.3, 0.7)\n                    self.phi_p = np.clip(self.phi_p + np.random.uniform(-0.1, 0.1), 1.2, 1.8)\n                    self.phi_g = np.clip(self.phi_g + np.random.uniform(-0.1, 0.1), 1.2, 1.8)\n\n            elites = self._elite_selection()\n            for particle in elites:\n                if eval_count >= self.budget:\n                    break\n                self.particles = np.vstack((self.particles, particle))\n                self.particles = self.particles[-self.population_size:]\n\n            self._update_particles(self.lb, self.ub)\n\n        return self.global_best_position, self.global_best_score", "name": "EnhancedAdaptiveMemorySearch", "description": "Enhanced Adaptive Memory Search (EAMS) incorporates multilevel adaptive memory with dynamic parameter tuning and elite selection to improve convergence speed and solution accuracy.", "configspace": "", "generation": 85, "fitness": 0.6579725066935389, "feedback": "The algorithm EnhancedAdaptiveMemorySearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.66 with standard deviation 0.00.", "error": "", "parent_id": "886d3ed4-4dda-4108-9bfe-eb231d760865", "metadata": {"aucs": [0.661211079672533, 0.6547339337145449]}, "mutation_prompt": null}
{"id": "3ce51772-5443-4d12-8ba4-8829fd927e07", "solution": "import numpy as np\n\nclass EnhancedAdaptiveMemorySearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20 + self.dim\n        self.memory_size = 5\n        self.particles = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_scores = None\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.omega = 0.5\n        self.phi_p = 1.5\n        self.phi_g = 1.5\n        self.memory = []\n        self.local_memory = []  # Additional layer for local memory\n\n    def _initialize_population(self, lb, ub):\n        self.particles = np.random.rand(self.population_size, self.dim) * (ub - lb) + lb\n        self.velocities = np.random.rand(self.population_size, self.dim) * (ub - lb) / 10\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n\n    def _update_particles(self, lb, ub):\n        for i in range(self.population_size):\n            if self.memory and np.random.rand() < 0.5:\n                strategy = self.memory[np.random.randint(len(self.memory))]\n            elif self.local_memory and np.random.rand() < 0.5:\n                strategy = self.local_memory[np.random.randint(len(self.local_memory))]\n            else:\n                strategy = {'omega': self.omega, 'phi_p': self.phi_p, 'phi_g': self.phi_g}\n\n            cognitive_component = strategy['phi_p'] * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.particles[i])\n            social_component = strategy['phi_g'] * np.random.rand(self.dim) * (self.global_best_position - self.particles[i])\n            inertia = strategy['omega']\n\n            self.velocities[i] = inertia * self.velocities[i] + cognitive_component + social_component\n            self.particles[i] += self.velocities[i]\n            self.particles[i] = np.clip(self.particles[i], lb, ub)\n\n    def __call__(self, func):\n        self.lb, self.ub = func.bounds.lb, func.bounds.ub\n        self._initialize_population(self.lb, self.ub)\n\n        eval_count = 0\n        while eval_count < self.budget:\n            for i in range(self.population_size):\n                if eval_count >= self.budget:\n                    break\n\n                score = func(self.particles[i])\n                eval_count += 1\n\n                if score < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = score\n                    self.personal_best_positions[i] = self.particles[i]\n\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.particles[i]\n                    if len(self.memory) >= self.memory_size:\n                        self.memory.pop(0)\n                    self.memory.append({'omega': self.omega, 'phi_p': self.phi_p, 'phi_g': self.phi_g})\n                    if len(self.local_memory) >= self.memory_size:\n                        self.local_memory.pop(0)\n                    self.local_memory.append({'omega': self.omega + np.random.uniform(-0.05, 0.05), \n                                              'phi_p': self.phi_p + np.random.uniform(-0.05, 0.05), \n                                              'phi_g': self.phi_g + np.random.uniform(-0.05, 0.05)})\n                    self.omega = np.clip(self.omega + np.random.uniform(-0.05, 0.05), 0.3, 0.7)\n                    self.phi_p = np.clip(self.phi_p + np.random.uniform(-0.05, 0.05), 1.2, 1.8)\n                    self.phi_g = np.clip(self.phi_g + np.random.uniform(-0.05, 0.05), 1.2, 1.8)\n\n            self._update_particles(self.lb, self.ub)\n\n        return self.global_best_position, self.global_best_score", "name": "EnhancedAdaptiveMemorySearch", "description": "Enhanced Adaptive Memory Search (EAMS) introduces a multi-layer adaptive memory strategy and dynamic neighborhood search to improve convergence rates and solution quality.", "configspace": "", "generation": 86, "fitness": 0.9178263357246012, "feedback": "The algorithm EnhancedAdaptiveMemorySearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.92 with standard deviation 0.01.", "error": "", "parent_id": "886d3ed4-4dda-4108-9bfe-eb231d760865", "metadata": {"aucs": [0.9264680649776319, 0.9091846064715704]}, "mutation_prompt": null}
{"id": "46b35456-a923-46fb-be84-bb154149f8f3", "solution": "import numpy as np\n\nclass QuantumInspiredAdaptivePSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20 + self.dim\n        self.memory_size = 5\n        self.particles = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_scores = None\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.omega = 0.5\n        self.phi_p = 1.5\n        self.phi_g = 1.5\n        self.memory = []\n        self.q_population = None  # Quantum population\n        self.q_velocity = None  # Quantum velocity\n\n    def _initialize_population(self, lb, ub):\n        self.particles = np.random.rand(self.population_size, self.dim) * (ub - lb) + lb\n        self.velocities = np.random.rand(self.population_size, self.dim) * (ub - lb) / 10\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n        self.q_population = np.random.rand(self.population_size, self.dim)\n        self.q_velocity = np.random.rand(self.population_size, self.dim)\n\n    def _quantum_update(self):\n        for i in range(self.population_size):\n            self.q_velocity[i] = np.random.rand(self.dim)\n            self.q_population[i] = np.sin(self.q_velocity[i] * np.pi / 2) * self.personal_best_positions[i] + \\\n                                   np.cos(self.q_velocity[i] * np.pi / 2) * self.global_best_position\n            self.particles[i] = 0.5 * (self.particles[i] + self.q_population[i])\n\n    def _update_particles(self, lb, ub):\n        for i in range(self.population_size):\n            if self.memory and np.random.rand() < 0.5:\n                strategy = self.memory[np.random.randint(len(self.memory))]\n                cognitive_component = strategy['phi_p'] * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.particles[i])\n                social_component = strategy['phi_g'] * np.random.rand(self.dim) * (self.global_best_position - self.particles[i])\n                inertia = strategy['omega']\n            else:\n                cognitive_component = self.phi_p * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.particles[i])\n                social_component = self.phi_g * np.random.rand(self.dim) * (self.global_best_position - self.particles[i])\n                inertia = self.omega\n\n            self.velocities[i] = inertia * self.velocities[i] + cognitive_component + social_component\n            self.particles[i] += self.velocities[i]\n            self.particles[i] = np.clip(self.particles[i], lb, ub)\n\n    def __call__(self, func):\n        self.lb, self.ub = func.bounds.lb, func.bounds.ub\n        self._initialize_population(self.lb, self.ub)\n\n        eval_count = 0\n        while eval_count < self.budget:\n            for i in range(self.population_size):\n                if eval_count >= self.budget:\n                    break\n\n                score = func(self.particles[i])\n                eval_count += 1\n\n                if score < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = score\n                    self.personal_best_positions[i] = self.particles[i]\n\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.particles[i]\n                    if len(self.memory) >= self.memory_size:\n                        self.memory.pop(0)\n                    self.memory.append({'omega': self.omega, 'phi_p': self.phi_p, 'phi_g': self.phi_g})\n                    self.omega = np.clip(self.omega + np.random.uniform(-0.1, 0.1), 0.3, 0.7)\n                    self.phi_p = np.clip(self.phi_p + np.random.uniform(-0.1, 0.1), 1.2, 1.8)\n                    self.phi_g = np.clip(self.phi_g + np.random.uniform(-0.1, 0.1), 1.2, 1.8)\n\n            self._quantum_update()\n            self._update_particles(self.lb, self.ub)\n\n        return self.global_best_position, self.global_best_score", "name": "QuantumInspiredAdaptivePSO", "description": "Quantum-Inspired Adaptive Particle Swarm Optimization (QIAPSO) utilizes quantum superposition states to enhance exploration, combined with adaptive parameters for efficient convergence in complex search spaces.", "configspace": "", "generation": 87, "fitness": 0.6530181045455111, "feedback": "The algorithm QuantumInspiredAdaptivePSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.65 with standard deviation 0.03.", "error": "", "parent_id": "886d3ed4-4dda-4108-9bfe-eb231d760865", "metadata": {"aucs": [0.6871878243980225, 0.6188483846929997]}, "mutation_prompt": null}
{"id": "6c623782-df3c-45c0-ac18-979f52836eb5", "solution": "import numpy as np\n\nclass QuantumInspiredPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20 + self.dim\n        self.alpha = 0.05  # Mutation rate in quantum state\n        self.betas = np.random.uniform(0.5, 1.0, (self.population_size, self.dim))  # Probabilistic weights\n        self.particles = None\n        self.best_positions = None\n        self.best_scores = None\n        self.global_best_position = None\n        self.global_best_score = np.inf\n\n    def _initialize_population(self, lb, ub):\n        self.particles = np.random.rand(self.population_size, self.dim) * (ub - lb) + lb\n        self.best_positions = np.copy(self.particles)\n        self.best_scores = np.full(self.population_size, np.inf)\n\n    def _quantum_update(self, lb, ub):\n        for i in range(self.population_size):\n            quantum_state = np.random.rand(self.dim) < self.betas[i]\n            mutation = np.random.randn(self.dim) * self.alpha\n            if np.random.rand() < 0.5:  # Explore\n                self.particles[i] = quantum_state * (self.best_positions[i] + mutation) + (1 - quantum_state) * (self.global_best_position + mutation)\n            else:  # Exploit\n                self.particles[i] = quantum_state * (self.global_best_position + mutation) + (1 - quantum_state) * (self.particles[i] + mutation)\n            \n            self.particles[i] = np.clip(self.particles[i], lb, ub)\n\n    def __call__(self, func):\n        self.lb, self.ub = func.bounds.lb, func.bounds.ub\n        self._initialize_population(self.lb, self.ub)\n\n        eval_count = 0\n        while eval_count < self.budget:\n            for i in range(self.population_size):\n                if eval_count >= self.budget:\n                    break\n\n                score = func(self.particles[i])\n                eval_count += 1\n\n                if score < self.best_scores[i]:\n                    self.best_scores[i] = score\n                    self.best_positions[i] = self.particles[i]\n\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.particles[i]\n                    # Adapt probabilistic weights slightly\n                    self.betas[i] = np.clip(self.betas[i] + np.random.uniform(-0.05, 0.05, self.dim), 0.5, 1.0)\n\n            self._quantum_update(self.lb, self.ub)\n\n        return self.global_best_position, self.global_best_score", "name": "QuantumInspiredPSO", "description": "Quantum-Inspired Particle Swarm Optimization (QIPSO) uses quantum superposition principles to enhance diversity and convergence, leveraging probabilistic solutions and recombination for global optimization.", "configspace": "", "generation": 88, "fitness": 0.6261385505917851, "feedback": "The algorithm QuantumInspiredPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.63 with standard deviation 0.06.", "error": "", "parent_id": "886d3ed4-4dda-4108-9bfe-eb231d760865", "metadata": {"aucs": [0.6898997241339242, 0.5623773770496462]}, "mutation_prompt": null}
{"id": "93e16b2d-57f5-4681-a35c-696a718f37df", "solution": "import numpy as np\n\nclass EnhancedAdaptiveMemorySearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20 + self.dim\n        self.memory_size = 5  # Number of successful strategies to remember\n        self.particles = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_scores = None\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.learning_rate = 0.05  # Learning rate for adaptive memory updates\n        self.omega = 0.5  # Inertia weight\n        self.phi_p = 1.5  # Cognitive coefficient\n        self.phi_g = 1.5  # Social coefficient\n        self.memory = []  # Initialize adaptive memory\n\n    def _initialize_population(self, lb, ub):\n        self.particles = np.random.rand(self.population_size, self.dim) * (ub - lb) + lb\n        self.velocities = np.random.rand(self.population_size, self.dim) * (ub - lb) / 10\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n\n    def _update_particles(self, lb, ub):\n        for i in range(self.population_size):\n            if self.memory and np.random.rand() < 0.5:\n                strategy = self.memory[np.random.randint(len(self.memory))]\n                cognitive_component = strategy['phi_p'] * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.particles[i])\n                social_component = strategy['phi_g'] * np.random.rand(self.dim) * (self.global_best_position - self.particles[i])\n                inertia = strategy['omega']\n            else:\n                cognitive_component = self.phi_p * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.particles[i])\n                social_component = self.phi_g * np.random.rand(self.dim) * (self.global_best_position - self.particles[i])\n                inertia = self.omega\n\n            self.velocities[i] = inertia * self.velocities[i] + cognitive_component + social_component\n            self.particles[i] += self.velocities[i]\n            self.particles[i] = np.clip(self.particles[i], lb, ub)\n\n    def __call__(self, func):\n        self.lb, self.ub = func.bounds.lb, func.bounds.ub\n        self._initialize_population(self.lb, self.ub)\n\n        eval_count = 0\n        while eval_count < self.budget:\n            for i in range(self.population_size):\n                if eval_count >= self.budget:\n                    break\n\n                score = func(self.particles[i])\n                eval_count += 1\n\n                if score < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = score\n                    self.personal_best_positions[i] = self.particles[i]\n\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.particles[i]\n                    # Add successful strategy to memory\n                    if len(self.memory) >= self.memory_size:\n                        self.memory.pop(0)  # Remove oldest strategy if memory is full\n                    self.memory.append({'omega': self.omega, 'phi_p': self.phi_p, 'phi_g': self.phi_g})\n                    # Adapt parameters using a learning rate\n                    self.omega = np.clip(self.omega + self.learning_rate * np.random.uniform(-0.1, 0.1), 0.3, 0.7)\n                    self.phi_p = np.clip(self.phi_p + self.learning_rate * np.random.uniform(-0.1, 0.1), 1.2, 1.8)\n                    self.phi_g = np.clip(self.phi_g + self.learning_rate * np.random.uniform(-0.1, 0.1), 1.2, 1.8)\n\n            self._update_particles(self.lb, self.ub)\n\n        return self.global_best_position, self.global_best_score", "name": "EnhancedAdaptiveMemorySearch", "description": "Enhanced Adaptive Memory Search (EAMS) incorporates dynamic strategy adaptation using a learning rate to refine memory updates and increase exploration efficiency.", "configspace": "", "generation": 89, "fitness": 0.9075096913637938, "feedback": "The algorithm EnhancedAdaptiveMemorySearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.91 with standard deviation 0.04.", "error": "", "parent_id": "886d3ed4-4dda-4108-9bfe-eb231d760865", "metadata": {"aucs": [0.9443345171274576, 0.8706848656001299]}, "mutation_prompt": null}
{"id": "4d0a7c15-1546-41bd-b574-8286708b9216", "solution": "import numpy as np\n\nclass QuantumInspiredSwarmOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 30 + self.dim\n        self.particles = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_scores = None\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.alpha = 0.2  # Quantum-inspired exploration parameter\n        self.beta = 2.0  # Quantum-inspired exploitation parameter\n\n    def _initialize_population(self, lb, ub):\n        self.particles = np.random.rand(self.population_size, self.dim) * (ub - lb) + lb\n        self.velocities = np.random.rand(self.population_size, self.dim) * (ub - lb) / 10\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n\n    def _quantum_update(self, lb, ub):\n        for i in range(self.population_size):\n            quantum_exploration = self.alpha * np.random.rand(self.dim) * (self.global_best_position - self.particles[i])\n            quantum_exploitation = self.beta * np.sin(self.velocities[i])\n            self.velocities[i] *= np.cos(np.random.rand(self.dim) * np.pi)\n            self.velocities[i] += quantum_exploration + quantum_exploitation\n            self.particles[i] += self.velocities[i]\n            self.particles[i] = np.clip(self.particles[i], lb, ub)\n\n    def __call__(self, func):\n        self.lb, self.ub = func.bounds.lb, func.bounds.ub\n        self._initialize_population(self.lb, self.ub)\n\n        eval_count = 0\n        while eval_count < self.budget:\n            for i in range(self.population_size):\n                if eval_count >= self.budget:\n                    break\n\n                score = func(self.particles[i])\n                eval_count += 1\n\n                if score < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = score\n                    self.personal_best_positions[i] = self.particles[i]\n\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.particles[i]\n\n            self._quantum_update(self.lb, self.ub)\n\n        return self.global_best_position, self.global_best_score", "name": "QuantumInspiredSwarmOptimization", "description": "Quantum-Inspired Swarm Optimization (QISO) integrates quantum-inspired principles, leveraging quantum superposition and interference to enhance exploration and exploitation in global optimization tasks.", "configspace": "", "generation": 90, "fitness": 0.6670930725605403, "feedback": "The algorithm QuantumInspiredSwarmOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.67 with standard deviation 0.03.", "error": "", "parent_id": "886d3ed4-4dda-4108-9bfe-eb231d760865", "metadata": {"aucs": [0.6326518156933256, 0.701534329427755]}, "mutation_prompt": null}
{"id": "6da14f85-a02e-407d-b5f9-497f962af50d", "solution": "import numpy as np\n\nclass EnhancedAdaptiveMemorySearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20 + self.dim\n        self.memory_size = 5\n        self.particles = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_scores = None\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.omega = 0.5\n        self.phi_p = 1.5\n        self.phi_g = 1.5\n        self.memory = []\n        self.dynamic_lr = 0.1\n        self.diversity_coefficient = 0.1\n\n    def _initialize_population(self, lb, ub):\n        self.particles = np.random.rand(self.population_size, self.dim) * (ub - lb) + lb\n        self.velocities = np.random.rand(self.population_size, self.dim) * (ub - lb) / 10\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n\n    def _update_particles(self, lb, ub):\n        for i in range(self.population_size):\n            if self.memory and np.random.rand() < 0.5:\n                strategy = self.memory[np.random.randint(len(self.memory))]\n                cognitive_component = strategy['phi_p'] * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.particles[i])\n                social_component = strategy['phi_g'] * np.random.rand(self.dim) * (self.global_best_position - self.particles[i])\n                inertia = strategy['omega']\n            else:\n                cognitive_component = self.phi_p * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.particles[i])\n                social_component = self.phi_g * np.random.rand(self.dim) * (self.global_best_position - self.particles[i])\n                inertia = self.omega\n\n            diversity_component = self.diversity_coefficient * (np.random.rand(self.dim) - 0.5) * (ub - lb)\n            self.velocities[i] = inertia * self.velocities[i] + cognitive_component + social_component + diversity_component\n            self.particles[i] += self.velocities[i]\n            self.particles[i] = np.clip(self.particles[i], lb, ub)\n\n    def __call__(self, func):\n        self.lb, self.ub = func.bounds.lb, func.bounds.ub\n        self._initialize_population(self.lb, self.ub)\n\n        eval_count = 0\n        while eval_count < self.budget:\n            for i in range(self.population_size):\n                if eval_count >= self.budget:\n                    break\n\n                score = func(self.particles[i])\n                eval_count += 1\n\n                if score < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = score\n                    self.personal_best_positions[i] = self.particles[i]\n\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.particles[i]\n                    if len(self.memory) >= self.memory_size:\n                        self.memory.pop(0)\n                    self.memory.append({'omega': self.omega, 'phi_p': self.phi_p, 'phi_g': self.phi_g})\n                    self.omega = np.clip(self.omega + np.random.uniform(-0.1, 0.1), 0.3, 0.7)\n                    self.phi_p = np.clip(self.phi_p + np.random.uniform(-0.1, 0.1), 1.2, 1.8)\n                    self.phi_g = np.clip(self.phi_g + np.random.uniform(-0.1, 0.1), 1.2, 1.8)\n                    self.dynamic_lr = np.clip(self.dynamic_lr * (1 + np.random.uniform(-0.05, 0.05)), 0.05, 0.2)\n\n            self._update_particles(self.lb, self.ub)\n\n        return self.global_best_position, self.global_best_score", "name": "EnhancedAdaptiveMemorySearch", "description": "Enhanced Adaptive Memory Search (EAMS) introduces dynamic learning rates and diversity preservation strategies to improve exploration while maintaining robust convergence.", "configspace": "", "generation": 91, "fitness": 0.6585153858372347, "feedback": "The algorithm EnhancedAdaptiveMemorySearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.66 with standard deviation 0.03.", "error": "", "parent_id": "886d3ed4-4dda-4108-9bfe-eb231d760865", "metadata": {"aucs": [0.624619539138157, 0.6924112325363123]}, "mutation_prompt": null}
{"id": "687dc177-c273-418c-8e65-690fdfa042af", "solution": "import numpy as np\n\nclass DynamicMultiSwarmOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_swarms = 3\n        self.swarm_size = 10 + self.dim\n        self.swarms = [[] for _ in range(self.num_swarms)]\n        self.velocities = [[] for _ in range(self.num_swarms)]\n        self.personal_best_positions = [[] for _ in range(self.num_swarms)]\n        self.personal_best_scores = [[] for _ in range(self.num_swarms)]\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.omega = 0.5\n        self.phi_p = 1.5\n        self.phi_g = 1.5\n        self.interaction_strengths = np.linspace(0.1, 0.5, self.num_swarms)\n\n    def _initialize_swarms(self, lb, ub):\n        for swarm in range(self.num_swarms):\n            self.swarms[swarm] = np.random.rand(self.swarm_size, self.dim) * (ub - lb) + lb\n            self.velocities[swarm] = np.random.rand(self.swarm_size, self.dim) * (ub - lb) / 10\n            self.personal_best_positions[swarm] = np.copy(self.swarms[swarm])\n            self.personal_best_scores[swarm] = np.full(self.swarm_size, np.inf)\n\n    def _update_particles(self, lb, ub):\n        for swarm in range(self.num_swarms):\n            for i in range(self.swarm_size):\n                cognitive_component = self.phi_p * np.random.rand(self.dim) * (self.personal_best_positions[swarm][i] - self.swarms[swarm][i])\n                social_component = self.phi_g * np.random.rand(self.dim) * (self.global_best_position - self.swarms[swarm][i])\n                inertia = self.omega\n\n                interaction_component = np.zeros(self.dim)\n                for other_swarm in range(self.num_swarms):\n                    if other_swarm != swarm:\n                        interaction_component += self.interaction_strengths[swarm] * np.random.rand(self.dim) * (np.mean(self.swarms[other_swarm], axis=0) - self.swarms[swarm][i])\n                \n                self.velocities[swarm][i] = inertia * self.velocities[swarm][i] + cognitive_component + social_component + interaction_component\n                self.swarms[swarm][i] += self.velocities[swarm][i]\n                self.swarms[swarm][i] = np.clip(self.swarms[swarm][i], lb, ub)\n\n    def __call__(self, func):\n        self.lb, self.ub = func.bounds.lb, func.bounds.ub\n        self._initialize_swarms(self.lb, self.ub)\n\n        eval_count = 0\n        while eval_count < self.budget:\n            for swarm in range(self.num_swarms):\n                for i in range(self.swarm_size):\n                    if eval_count >= self.budget:\n                        break\n\n                    score = func(self.swarms[swarm][i])\n                    eval_count += 1\n\n                    if score < self.personal_best_scores[swarm][i]:\n                        self.personal_best_scores[swarm][i] = score\n                        self.personal_best_positions[swarm][i] = self.swarms[swarm][i]\n\n                    if score < self.global_best_score:\n                        self.global_best_score = score\n                        self.global_best_position = self.swarms[swarm][i]\n\n            self._update_particles(self.lb, self.ub)\n\n        return self.global_best_position, self.global_best_score", "name": "DynamicMultiSwarmOptimization", "description": "Dynamic Multi-Swarm Optimization (DMSO) utilizes multiple interacting swarms with varying interaction strengths to enhance global search capability while maintaining local refinement.", "configspace": "", "generation": 92, "fitness": 0.8731810217171538, "feedback": "The algorithm DynamicMultiSwarmOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.87 with standard deviation 0.02.", "error": "", "parent_id": "886d3ed4-4dda-4108-9bfe-eb231d760865", "metadata": {"aucs": [0.8561362103065948, 0.890225833127713]}, "mutation_prompt": null}
{"id": "46b0ef98-c9a0-4124-9614-c425225e5034", "solution": "import numpy as np\n\nclass QuantumInspiredEvolutionarySearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20 + self.dim\n        self.alpha = 0.05  # Step size for updating\n        self.quantum_population = None\n        self.best_solution = None\n        self.best_score = np.inf\n\n    def _initialize_population(self, lb, ub):\n        # Quantum states initialized to represent superposition of all possible states\n        self.quantum_population = np.random.rand(self.population_size, self.dim, 2)\n        self.quantum_population[:, :, 0] = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        self.quantum_population[:, :, 1] = np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def _measure_population(self, lb, ub):\n        # Collapse quantum states to classical solutions using probabilistic sampling\n        measured_population = np.zeros((self.population_size, self.dim))\n        for i in range(self.population_size):\n            probabilities = np.random.rand(self.dim)\n            measured_population[i] = np.where(probabilities < 0.5, self.quantum_population[i, :, 0], self.quantum_population[i, :, 1])\n        measured_population = np.clip(measured_population, lb, ub)\n        return measured_population\n\n    def _update_quantum_states(self, population, best_solution):\n        # Quantum interference and superposition update mechanism\n        for i in range(self.population_size):\n            for d in range(self.dim):\n                theta = np.random.rand() * np.pi\n                self.quantum_population[i, d, 0] += self.alpha * np.cos(theta) * (best_solution[d] - population[i, d])\n                self.quantum_population[i, d, 1] += self.alpha * np.sin(theta) * (best_solution[d] - population[i, d])\n\n    def __call__(self, func):\n        self.lb, self.ub = func.bounds.lb, func.bounds.ub\n        self._initialize_population(self.lb, self.ub)\n\n        eval_count = 0\n        while eval_count < self.budget:\n            population = self._measure_population(self.lb, self.ub)\n\n            for i in range(self.population_size):\n                if eval_count >= self.budget:\n                    break\n\n                score = func(population[i])\n                eval_count += 1\n\n                if score < self.best_score:\n                    self.best_score = score\n                    self.best_solution = population[i]\n\n            self._update_quantum_states(population, self.best_solution)\n\n        return self.best_solution, self.best_score", "name": "QuantumInspiredEvolutionarySearch", "description": "Quantum-Inspired Evolutionary Search (QIES) integrates quantum superposition and interference concepts to enhance exploration and convergence in optimization by probabilistically updating solutions.", "configspace": "", "generation": 93, "fitness": 0.5510900446199236, "feedback": "The algorithm QuantumInspiredEvolutionarySearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.55 with standard deviation 0.01.", "error": "", "parent_id": "886d3ed4-4dda-4108-9bfe-eb231d760865", "metadata": {"aucs": [0.5563504928600581, 0.5458295963797892]}, "mutation_prompt": null}
{"id": "e3c31fc3-c757-4009-b4db-3c8f1da3e3d6", "solution": "import numpy as np\n\nclass QuantumInspiredEvolutionaryAlgorithm:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + self.dim\n        self.alpha = 0.05  # Quantum rotation angle\n        self.mutations = 0.1  # Probability of mutation\n        self.q_population = None\n        self.population = None\n        self.best_solution = None\n        self.best_score = np.inf\n\n    def _initialize_population(self, lb, ub):\n        self.q_population = np.random.rand(self.population_size, self.dim) * 2 - 1  # Q-bits initialized\n        self.population = np.random.rand(self.population_size, self.dim) * (ub - lb) + lb\n\n    def _update_q_population(self, individual, best_individual):\n        # Q-bit update with rotation gate\n        delta = np.arccos(np.dot(individual, best_individual) / (np.linalg.norm(individual) * np.linalg.norm(best_individual)))\n        rotation_angle = self.alpha * delta\n        rotation_matrix = np.array([[np.cos(rotation_angle), -np.sin(rotation_angle)],\n                                    [np.sin(rotation_angle), np.cos(rotation_angle)]])\n        return np.dot(rotation_matrix, individual)\n\n    def _apply_mutation(self, individual, lb, ub):\n        if np.random.rand() < self.mutations:\n            mutation_vector = np.random.randn(self.dim) * 0.1 * (ub - lb)\n            individual += mutation_vector\n            individual = np.clip(individual, lb, ub)\n        return individual\n\n    def __call__(self, func):\n        self.lb, self.ub = func.bounds.lb, func.bounds.ub\n        self._initialize_population(self.lb, self.ub)\n\n        eval_count = 0\n        while eval_count < self.budget:\n            for i in range(self.population_size):\n                if eval_count >= self.budget:\n                    break\n\n                score = func(self.population[i])\n                eval_count += 1\n\n                if score < self.best_score:\n                    self.best_score = score\n                    self.best_solution = self.population[i]\n\n            # Update quantum population based on the best solution\n            for i in range(self.population_size):\n                self.q_population[i] = self._update_q_population(self.q_population[i], self.best_solution)\n                self.population[i] = self._apply_mutation(self.q_population[i] * (self.ub - self.lb) / 2 + (self.lb + self.ub) / 2, self.lb, self.ub)\n\n        return self.best_solution, self.best_score", "name": "QuantumInspiredEvolutionaryAlgorithm", "description": "Quantum-Inspired Evolutionary Algorithm (QIEA) combines quantum computing principles with evolutionary strategies to explore the search space efficiently and adaptively manage exploration-exploitation balance.", "configspace": "", "generation": 94, "fitness": 0.5696198185218513, "feedback": "The algorithm QuantumInspiredEvolutionaryAlgorithm got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.57 with standard deviation 0.01.", "error": "", "parent_id": "886d3ed4-4dda-4108-9bfe-eb231d760865", "metadata": {"aucs": [0.5602223478455495, 0.5790172891981531]}, "mutation_prompt": null}
{"id": "8284cf76-2265-4bba-89cd-e33f7dadacc7", "solution": "import numpy as np\n\nclass EnhancedAdaptiveMemorySearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20 + self.dim\n        self.short_memory_size = 5  # Short-term memory size\n        self.long_memory_size = 10  # Long-term memory size\n        self.particles = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_scores = None\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.omega = 0.5  # Inertia weight\n        self.phi_p = 1.5  # Cognitive coefficient\n        self.phi_g = 1.5  # Social coefficient\n        self.short_memory = []  # Short-term memory\n        self.long_memory = []   # Long-term memory\n\n    def _initialize_population(self, lb, ub):\n        self.particles = np.random.rand(self.population_size, self.dim) * (ub - lb) + lb\n        self.velocities = np.random.rand(self.population_size, self.dim) * (ub - lb) / 10\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n\n    def _update_particles(self, lb, ub):\n        for i in range(self.population_size):\n            # Choose strategy from short or long-term memory\n            if self.short_memory and np.random.rand() < 0.3:\n                strategy = self.short_memory[np.random.randint(len(self.short_memory))]\n            elif self.long_memory and np.random.rand() < 0.3:\n                strategy = self.long_memory[np.random.randint(len(self.long_memory))]\n            else:\n                strategy = {'omega': self.omega, 'phi_p': self.phi_p, 'phi_g': self.phi_g}\n                \n            cognitive_component = strategy['phi_p'] * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.particles[i])\n            social_component = strategy['phi_g'] * np.random.rand(self.dim) * (self.global_best_position - self.particles[i])\n            inertia = strategy['omega']\n\n            self.velocities[i] = inertia * self.velocities[i] + cognitive_component + social_component\n            self.particles[i] += self.velocities[i]\n            self.particles[i] = np.clip(self.particles[i], lb, ub)\n\n    def __call__(self, func):\n        self.lb, self.ub = func.bounds.lb, func.bounds.ub\n        self._initialize_population(self.lb, self.ub)\n\n        eval_count = 0\n        while eval_count < self.budget:\n            for i in range(self.population_size):\n                if eval_count >= self.budget:\n                    break\n\n                score = func(self.particles[i])\n                eval_count += 1\n\n                if score < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = score\n                    self.personal_best_positions[i] = self.particles[i]\n\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.particles[i]\n                    \n                    # Add successful strategy to both memories\n                    if len(self.short_memory) >= self.short_memory_size:\n                        self.short_memory.pop(0)\n                    self.short_memory.append({'omega': self.omega, 'phi_p': self.phi_p, 'phi_g': self.phi_g})\n\n                    if len(self.long_memory) >= self.long_memory_size:\n                        self.long_memory.pop(0)\n                    self.long_memory.append({'omega': self.omega, 'phi_p': self.phi_p, 'phi_g': self.phi_g})\n\n                    # Adapt parameters slightly\n                    self.omega = np.clip(self.omega + np.random.uniform(-0.05, 0.05), 0.3, 0.7)\n                    self.phi_p = np.clip(self.phi_p + np.random.uniform(-0.05, 0.05), 1.2, 1.8)\n                    self.phi_g = np.clip(self.phi_g + np.random.uniform(-0.05, 0.05), 1.2, 1.8)\n\n            if eval_count / self.budget > 0.5:  # Increase population size after half of the budget\n                self.population_size = min(self.population_size + 5, int(1.5 * (20 + self.dim)))\n                self._initialize_population(self.lb, self.ub)\n\n            self._update_particles(self.lb, self.ub)\n\n        return self.global_best_position, self.global_best_score", "name": "EnhancedAdaptiveMemorySearch", "description": "Enhanced Adaptive Memory Search (EAMS) leverages a dual-memory structure for short and long-term memory to effectively balance exploration and exploitation, alongside adaptive parameter tuning and dynamic population scaling.", "configspace": "", "generation": 95, "fitness": 0.8639642037754296, "feedback": "The algorithm EnhancedAdaptiveMemorySearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.86 with standard deviation 0.02.", "error": "", "parent_id": "886d3ed4-4dda-4108-9bfe-eb231d760865", "metadata": {"aucs": [0.8439012924978854, 0.8840271150529737]}, "mutation_prompt": null}
{"id": "4d472033-837c-4d2a-8201-6ae68afa368e", "solution": "import numpy as np\n\nclass QuantumInspiredPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20 + self.dim\n        self.particles = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_scores = None\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.c1 = 1.5  # Cognitive coefficient\n        self.c2 = 1.5  # Social coefficient\n        self.alpha = 0.5  # Attraction parameter\n        self.beta = 0.5   # Quantum parameter\n\n    def _initialize_population(self, lb, ub):\n        self.particles = np.random.rand(self.population_size, self.dim) * (ub - lb) + lb\n        self.velocities = np.random.rand(self.population_size, self.dim) * (ub - lb) / 10\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n\n    def _quantum_update(self, position, lb, ub):\n        # Quantum-inspired update using wave function model\n        mbest = (self.alpha * self.global_best_position + (1 - self.alpha) * np.mean(self.personal_best_positions, axis=0))\n        u = np.random.rand(self.dim)\n        b = self.beta * np.random.uniform(lb, ub, self.dim)\n        return mbest + b * np.sign(u - 0.5) * np.log(1.0 / (1.0 - u))\n\n    def __call__(self, func):\n        self.lb, self.ub = func.bounds.lb, func.bounds.ub\n        self._initialize_population(self.lb, self.ub)\n\n        eval_count = 0\n        while eval_count < self.budget:\n            for i in range(self.population_size):\n                if eval_count >= self.budget:\n                    break\n\n                score = func(self.particles[i])\n                eval_count += 1\n\n                if score < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = score\n                    self.personal_best_positions[i] = self.particles[i]\n\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.particles[i]\n\n            for i in range(self.population_size):\n                # Apply quantum-inspired update\n                self.particles[i] = self._quantum_update(self.particles[i], self.lb, self.ub)\n                self.particles[i] = np.clip(self.particles[i], self.lb, self.ub)\n\n        return self.global_best_position, self.global_best_score", "name": "QuantumInspiredPSO", "description": "Quantum-Inspired Particle Swarm Optimization (QPSO) leverages principles of quantum mechanics to enhance exploration capabilities, embedding wave function models to adaptively navigate the search space.", "configspace": "", "generation": 96, "fitness": 0.6653645937838912, "feedback": "The algorithm QuantumInspiredPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.67 with standard deviation 0.01.", "error": "", "parent_id": "886d3ed4-4dda-4108-9bfe-eb231d760865", "metadata": {"aucs": [0.6545972538641864, 0.676131933703596]}, "mutation_prompt": null}
{"id": "7c52b736-b42a-42c0-ae0d-61943060307b", "solution": "import numpy as np\n\nclass QuantumInspiredEvolutionaryOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 30\n        self.gamma = 0.03  # Quantum interference factor\n        self.elitism_rate = 0.2  # Rate for selecting elite individuals\n        self.positions = None\n        self.best_position = None\n        self.best_score = np.inf\n\n    def _initialize_population(self, lb, ub):\n        self.positions = np.random.rand(self.population_size, self.dim) * (ub - lb) + lb\n        self.best_position = None\n        self.best_score = np.inf\n\n    def _quantum_interference(self, lb, ub):\n        for i in range(self.population_size):\n            if np.random.rand() < 0.5:\n                interference = np.sin(self.gamma * np.pi * (ub - lb))\n                self.positions[i] = self.positions[i] + interference * (np.random.rand(self.dim) - 0.5)\n                self.positions[i] = np.clip(self.positions[i], lb, ub)\n\n    def _select_elite(self, scores):\n        elite_count = int(self.elitism_rate * self.population_size)\n        elite_indices = np.argsort(scores)[:elite_count]\n        return self.positions[elite_indices]\n\n    def _generate_offspring(self, elites, lb, ub):\n        offspring_size = self.population_size - len(elites)\n        offspring = np.empty((offspring_size, self.dim))\n        for i in range(offspring_size):\n            parent1, parent2 = np.random.choice(len(elites), 2, replace=False)\n            crossover_point = np.random.randint(1, self.dim)\n            offspring[i, :crossover_point] = elites[parent1, :crossover_point]\n            offspring[i, crossover_point:] = elites[parent2, crossover_point:]\n            mutation = (np.random.rand(self.dim) - 0.5) * (ub - lb) * 0.05\n            offspring[i] = np.clip(offspring[i] + mutation, lb, ub)\n        return offspring\n\n    def __call__(self, func):\n        self.lb, self.ub = func.bounds.lb, func.bounds.ub\n        self._initialize_population(self.lb, self.ub)\n\n        eval_count = 0\n        while eval_count < self.budget:\n            scores = np.array([func(pos) for pos in self.positions])\n            eval_count += self.population_size\n\n            if np.min(scores) < self.best_score:\n                self.best_score = np.min(scores)\n                self.best_position = self.positions[np.argmin(scores)]\n\n            elites = self._select_elite(scores)\n            offspring = self._generate_offspring(elites, self.lb, self.ub)\n            self.positions = np.vstack((elites, offspring))\n            self._quantum_interference(self.lb, self.ub)\n\n        return self.best_position, self.best_score", "name": "QuantumInspiredEvolutionaryOptimization", "description": "Quantum-Inspired Evolutionary Optimization (QIEO) leverages quantum superposition and interference principles to maintain a diverse solution space, enhancing exploration while applying evolutionary selection to refine potential solutions.", "configspace": "", "generation": 97, "fitness": 0.9049767970374074, "feedback": "The algorithm QuantumInspiredEvolutionaryOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.90 with standard deviation 0.00.", "error": "", "parent_id": "886d3ed4-4dda-4108-9bfe-eb231d760865", "metadata": {"aucs": [0.9046705514676504, 0.9052830426071643]}, "mutation_prompt": null}
{"id": "c7d9aaf3-77fd-4547-ae30-cacaeb1ccafb", "solution": "import numpy as np\n\nclass DynamicAdaptiveMemoryPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20 + self.dim\n        self.memory_size = 5\n        self.particles = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_scores = None\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.omega = 0.5\n        self.phi_p = 1.5\n        self.phi_g = 1.5\n        self.memory = []\n\n    def _initialize_population(self, lb, ub):\n        self.particles = np.random.rand(self.population_size, self.dim) * (ub - lb) + lb\n        self.velocities = np.random.rand(self.population_size, self.dim) * (ub - lb) / 10\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n\n    def _update_particles(self, lb, ub):\n        for i in range(self.population_size):\n            if self.memory and np.random.rand() < 0.5:\n                strategy = self.memory[np.random.randint(len(self.memory))]\n                inertia = strategy['omega']\n                cognitive_component = strategy['phi_p'] * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.particles[i])\n                social_component = strategy['phi_g'] * np.random.rand(self.dim) * (self.global_best_position - self.particles[i])\n            else:\n                inertia = self.omega\n                cognitive_component = self.phi_p * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.particles[i])\n                social_component = self.phi_g * np.random.rand(self.dim) * (self.global_best_position - self.particles[i])\n\n            local_best_positions = np.mean(self.personal_best_positions, axis=0)\n            local_social_component = np.random.rand(self.dim) * (local_best_positions - self.particles[i])\n\n            self.velocities[i] = inertia * self.velocities[i] + cognitive_component + social_component + 0.2 * local_social_component\n            self.particles[i] += self.velocities[i]\n            self.particles[i] = np.clip(self.particles[i], lb, ub)\n\n    def __call__(self, func):\n        self.lb, self.ub = func.bounds.lb, func.bounds.ub\n        self._initialize_population(self.lb, self.ub)\n\n        eval_count = 0\n        while eval_count < self.budget:\n            for i in range(self.population_size):\n                if eval_count >= self.budget:\n                    break\n\n                score = func(self.particles[i])\n                eval_count += 1\n\n                if score < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = score\n                    self.personal_best_positions[i] = self.particles[i]\n\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.particles[i]\n                    if len(self.memory) >= self.memory_size:\n                        self.memory.pop(0)\n                    self.memory.append({'omega': self.omega, 'phi_p': self.phi_p, 'phi_g': self.phi_g})\n                    self.omega = np.clip(self.omega + np.random.uniform(-0.05, 0.05), 0.3, 0.7)\n                    self.phi_p = np.clip(self.phi_p + np.random.uniform(-0.05, 0.05), 1.2, 1.8)\n                    self.phi_g = np.clip(self.phi_g + np.random.uniform(-0.05, 0.05), 1.2, 1.8)\n\n            self._update_particles(self.lb, self.ub)\n\n        return self.global_best_position, self.global_best_score", "name": "DynamicAdaptiveMemoryPSO", "description": "Dynamic Adaptive Memory PSO (DA-PSO) dynamically adjusts inertia and learning coefficients based on local success, refining the balance between exploration and exploitation.", "configspace": "", "generation": 98, "fitness": 0.9164949600929673, "feedback": "The algorithm DynamicAdaptiveMemoryPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.92 with standard deviation 0.00.", "error": "", "parent_id": "886d3ed4-4dda-4108-9bfe-eb231d760865", "metadata": {"aucs": [0.9162348242432616, 0.9167550959426729]}, "mutation_prompt": null}
{"id": "68f55b03-17ed-4450-9043-e6a28ce3f1a6", "solution": "import numpy as np\n\nclass QuantumInspiredPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20 + dim\n        self.particles = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_scores = None\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.alpha = 0.75  # Alpha coefficient for quantum behavior\n        self.beta = 0.25   # Beta coefficient for quantum behavior\n\n    def _initialize_population(self, lb, ub):\n        self.particles = np.random.rand(self.population_size, self.dim) * (ub - lb) + lb\n        self.velocities = np.zeros((self.population_size, self.dim))\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n\n    def _quantum_update(self, lb, ub):\n        # Quantum-inspired update using probabilistic superposition\n        mean_position = np.mean(self.particles, axis=0)\n        for i in range(self.population_size):\n            r1, r2 = np.random.rand(), np.random.rand()\n            quantum_move = np.exp(-self.beta * r1) * (mean_position - self.particles[i])\n            random_move = self.alpha * (r2 - 0.5)\n            self.particles[i] += self.velocities[i] + quantum_move + random_move\n            self.particles[i] = np.clip(self.particles[i], lb, ub)\n\n    def __call__(self, func):\n        self.lb, self.ub = func.bounds.lb, func.bounds.ub\n        self._initialize_population(self.lb, self.ub)\n\n        eval_count = 0\n        while eval_count < self.budget:\n            for i in range(self.population_size):\n                if eval_count >= self.budget:\n                    break\n\n                score = func(self.particles[i])\n                eval_count += 1\n\n                if score < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = score\n                    self.personal_best_positions[i] = self.particles[i]\n\n                if score < self.global_best_score:\n                    self.global_best_score = score\n                    self.global_best_position = self.particles[i]\n\n            self._quantum_update(self.lb, self.ub)\n\n        return self.global_best_position, self.global_best_score", "name": "QuantumInspiredPSO", "description": "Quantum-inspired Particle Swarm Optimization (QPSO) utilizes principles from quantum computing to enhance global exploration with quantum superposition and wavefunction collapse, allowing for a probabilistic search space traversal for improved convergence.", "configspace": "", "generation": 99, "fitness": 0.49958597559506274, "feedback": "The algorithm QuantumInspiredPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.50 with standard deviation 0.01.", "error": "", "parent_id": "886d3ed4-4dda-4108-9bfe-eb231d760865", "metadata": {"aucs": [0.5121390679212772, 0.4870328832688483]}, "mutation_prompt": null}
