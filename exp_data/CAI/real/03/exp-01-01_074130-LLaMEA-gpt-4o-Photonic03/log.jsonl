{"id": "2e96e5f1-5478-42ea-88cc-dc85b634c00b", "solution": "import numpy as np\n\nclass QIPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particles = 50  # Number of quantum particles\n        self.position = np.random.uniform(0, 1, (self.particles, self.dim))\n        self.velocity = np.zeros((self.particles, self.dim))\n        self.personal_best_position = np.copy(self.position)\n        self.global_best_position = None\n        self.personal_best_value = np.full(self.particles, np.inf)\n        self.global_best_value = np.inf\n        self.iteration = 0\n\n    def __call__(self, func):\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        scale = ub - lb\n\n        def quantum_update():\n            for i in range(self.particles):\n                delta = np.random.uniform(-1, 1, self.dim)\n                potential_position = self.position[i] + self.velocity[i] * delta\n                potential_position = np.clip(potential_position, 0, 1)\n                real_position = lb + potential_position * scale\n                value = func(real_position)\n                if value < self.personal_best_value[i]:\n                    self.personal_best_value[i] = value\n                    self.personal_best_position[i] = potential_position\n                if value < self.global_best_value:\n                    self.global_best_value = value\n                    self.global_best_position = potential_position\n\n        while self.iteration < self.budget:\n            quantum_update()\n            for i in range(self.particles):\n                cognitive = np.random.random(self.dim)\n                social = np.random.random(self.dim)\n                self.velocity[i] = (0.5 * self.velocity[i] +\n                                    cognitive * (self.personal_best_position[i] - self.position[i]) +\n                                    social * (self.global_best_position - self.position[i]))\n                self.position[i] += self.velocity[i]\n                self.position[i] = np.clip(self.position[i], 0, 1)\n            self.iteration += self.particles\n\n        return lb + self.global_best_position * scale", "name": "QIPSO", "description": "Quantum-Inspired Particle Swarm Optimization (QIPSO) utilizes quantum superposition and entanglement concepts to enhance global exploration and convergence in optimization tasks.", "configspace": "", "generation": 0, "fitness": 0.28832340847405213, "feedback": "The algorithm QIPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.29 with standard deviation 0.00.", "error": "", "parent_id": null, "metadata": {"aucs": [0.28832340847405213]}, "mutation_prompt": null}
{"id": "ffbdf83f-a8a1-4b35-8d3b-22349eef4971", "solution": "import numpy as np\n\nclass QIPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particles = 50  # Number of quantum particles\n        self.position = np.random.uniform(0, 1, (self.particles, self.dim))\n        self.velocity = np.zeros((self.particles, self.dim))\n        self.personal_best_position = np.copy(self.position)\n        self.global_best_position = None\n        self.personal_best_value = np.full(self.particles, np.inf)\n        self.global_best_value = np.inf\n        self.iteration = 0\n\n    def __call__(self, func):\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        scale = ub - lb\n\n        def quantum_update():\n            for i in range(self.particles):\n                delta = np.random.uniform(-1, 1, self.dim)\n                potential_position = self.position[i] + self.velocity[i] * delta\n                potential_position = np.clip(potential_position, 0, 1)\n                real_position = lb + potential_position * scale\n                value = func(real_position)\n                if value < self.personal_best_value[i]:\n                    self.personal_best_value[i] = value\n                    self.personal_best_position[i] = potential_position\n                if value < self.global_best_value:\n                    self.global_best_value = value\n                    self.global_best_position = potential_position\n\n        while self.iteration < self.budget:\n            quantum_update()\n            inertia_weight = 0.9 - (0.8 * self.iteration / self.budget)  # Dynamic inertia weight\n            for i in range(self.particles):\n                cognitive = np.random.random(self.dim)\n                social = np.random.random(self.dim)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive * (self.personal_best_position[i] - self.position[i]) +\n                                    social * (self.global_best_position - self.position[i]))\n                self.position[i] += self.velocity[i]\n                self.position[i] = np.clip(self.position[i], 0, 1)\n            self.iteration += self.particles\n\n        return lb + self.global_best_position * scale", "name": "QIPSO", "description": "Introducing a dynamic inertia weight to balance exploration and exploitation in QIPSO.", "configspace": "", "generation": 1, "fitness": 0.28581344386227236, "feedback": "The algorithm QIPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.29 with standard deviation 0.00.", "error": "", "parent_id": "2e96e5f1-5478-42ea-88cc-dc85b634c00b", "metadata": {"aucs": [0.28581344386227236]}, "mutation_prompt": null}
{"id": "fff09930-28c5-474d-a604-6532123bc94c", "solution": "import numpy as np\n\nclass QIPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particles = 50  # Number of quantum particles\n        self.position = np.random.uniform(0, 1, (self.particles, self.dim))\n        self.velocity = np.zeros((self.particles, self.dim))\n        self.personal_best_position = np.copy(self.position)\n        self.global_best_position = None\n        self.personal_best_value = np.full(self.particles, np.inf)\n        self.global_best_value = np.inf\n        self.iteration = 0\n\n    def __call__(self, func):\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        scale = ub - lb\n\n        def quantum_update():\n            for i in range(self.particles):\n                delta = np.random.uniform(-1, 1, self.dim)\n                potential_position = self.position[i] + self.velocity[i] * delta\n                potential_position = np.clip(potential_position, 0, 1)\n                real_position = lb + potential_position * scale\n                value = func(real_position)\n                if value < self.personal_best_value[i]:\n                    self.personal_best_value[i] = value\n                    self.personal_best_position[i] = potential_position\n                if value < self.global_best_value:\n                    self.global_best_value = value\n                    self.global_best_position = potential_position\n\n        while self.iteration < self.budget:\n            quantum_update()\n            inertia_weight = 0.9 - 0.7 * (self.iteration / self.budget)  # Adaptive inertia weight\n            for i in range(self.particles):\n                cognitive = np.random.random(self.dim)\n                social = np.random.random(self.dim)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive * (self.personal_best_position[i] - self.position[i]) +\n                                    social * (self.global_best_position - self.position[i]))\n                self.position[i] += self.velocity[i]\n                self.position[i] = np.clip(self.position[i], 0, 1)\n            self.iteration += self.particles\n\n        return lb + self.global_best_position * scale", "name": "QIPSO", "description": "Quantum-Inspired Particle Swarm Optimization with adaptive inertia weight to balance exploration and exploitation dynamically.", "configspace": "", "generation": 2, "fitness": 0.28474321845130257, "feedback": "The algorithm QIPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28 with standard deviation 0.00.", "error": "", "parent_id": "2e96e5f1-5478-42ea-88cc-dc85b634c00b", "metadata": {"aucs": [0.28474321845130257]}, "mutation_prompt": null}
{"id": "bb88ff32-4df5-4f23-933c-6f6937e2d8e4", "solution": "import numpy as np\n\nclass QIPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particles = 50  # Number of quantum particles\n        self.position = np.random.uniform(0, 1, (self.particles, self.dim))\n        self.velocity = np.zeros((self.particles, self.dim))\n        self.personal_best_position = np.copy(self.position)\n        self.global_best_position = None\n        self.personal_best_value = np.full(self.particles, np.inf)\n        self.global_best_value = np.inf\n        self.iteration = 0\n\n    def __call__(self, func):\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        scale = ub - lb\n\n        def quantum_update():\n            for i in range(self.particles):\n                delta = np.random.uniform(-1, 1, self.dim)\n                potential_position = self.position[i] + self.velocity[i] * delta\n                potential_position = np.clip(potential_position, 0, 1)\n                real_position = lb + potential_position * scale\n                value = func(real_position)\n                if value < self.personal_best_value[i]:\n                    self.personal_best_value[i] = value\n                    self.personal_best_position[i] = potential_position\n                if value < self.global_best_value:\n                    self.global_best_value = value\n                    self.global_best_position = potential_position\n\n        while self.iteration < self.budget:\n            quantum_update()\n            for i in range(self.particles):\n                cognitive = np.random.random(self.dim)\n                social = np.random.random(self.dim)\n                self.velocity[i] = (0.5 * self.velocity[i] +\n                                    cognitive * (self.personal_best_position[i] - self.position[i]) +\n                                    social * (self.global_best_position - self.position[i]))\n                self.position[i] += 0.9 * self.velocity[i]  # Adaptive velocity scaling\n                self.position[i] = np.clip(self.position[i], 0, 1)\n            self.iteration += self.particles\n\n        return lb + self.global_best_position * scale", "name": "QIPSO", "description": "Enhanced Quantum-Inspired Particle Swarm Optimization (EQIPSO) integrates adaptive velocity scaling for improved convergence.", "configspace": "", "generation": 3, "fitness": 0.2885712014995727, "feedback": "The algorithm QIPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.29 with standard deviation 0.00.", "error": "", "parent_id": "2e96e5f1-5478-42ea-88cc-dc85b634c00b", "metadata": {"aucs": [0.2885712014995727]}, "mutation_prompt": null}
{"id": "0fffa727-3296-4bc2-9973-fb0059735f34", "solution": "import numpy as np\n\nclass QIPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particles = 50  # Number of quantum particles\n        self.position = np.random.uniform(0, 1, (self.particles, self.dim))\n        self.velocity = np.zeros((self.particles, self.dim))\n        self.personal_best_position = np.copy(self.position)\n        self.global_best_position = None\n        self.personal_best_value = np.full(self.particles, np.inf)\n        self.global_best_value = np.inf\n        self.iteration = 0\n\n    def __call__(self, func):\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        scale = ub - lb\n\n        def quantum_update():\n            for i in range(self.particles):\n                delta = np.random.uniform(-1, 1, self.dim)\n                potential_position = self.position[i] + self.velocity[i] * delta\n                potential_position = np.clip(potential_position, 0, 1)\n                real_position = lb + potential_position * scale\n                value = func(real_position)\n                if value < self.personal_best_value[i]:\n                    self.personal_best_value[i] = value\n                    self.personal_best_position[i] = potential_position\n                if value < self.global_best_value:\n                    self.global_best_value = value\n                    self.global_best_position = potential_position\n\n        while self.iteration < self.budget:\n            quantum_update()\n            inertia_weight = 0.5 + np.random.rand() * 0.4  # Change: Introduce random inertia weight\n            for i in range(self.particles):\n                cognitive = np.random.random(self.dim)\n                social = np.random.random(self.dim)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive * (self.personal_best_position[i] - self.position[i]) +\n                                    social * (self.global_best_position - self.position[i]))\n                self.position[i] += 0.9 * self.velocity[i]  # Adaptive velocity scaling\n                self.position[i] = np.clip(self.position[i], 0, 1)\n            self.iteration += self.particles\n\n        return lb + self.global_best_position * scale", "name": "QIPSO", "description": "Introduce random inertia weight to enhance exploration and exploitation balance in QIPSO.", "configspace": "", "generation": 4, "fitness": 0.2881342634266376, "feedback": "The algorithm QIPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.29 with standard deviation 0.00.", "error": "", "parent_id": "bb88ff32-4df5-4f23-933c-6f6937e2d8e4", "metadata": {"aucs": [0.2881342634266376]}, "mutation_prompt": null}
{"id": "fa232f87-9d56-450c-a532-021638d1dbf8", "solution": "import numpy as np\n\nclass QIPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particles = 50  # Number of quantum particles\n        self.position = np.random.uniform(0, 1, (self.particles, self.dim))\n        self.velocity = np.zeros((self.particles, self.dim))\n        self.personal_best_position = np.copy(self.position)\n        self.global_best_position = None\n        self.personal_best_value = np.full(self.particles, np.inf)\n        self.global_best_value = np.inf\n        self.iteration = 0\n\n    def __call__(self, func):\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        scale = ub - lb\n\n        def quantum_update():\n            for i in range(self.particles):\n                delta = np.random.uniform(-1, 1, self.dim)\n                potential_position = self.position[i] + self.velocity[i] * delta\n                potential_position = np.clip(potential_position, 0, 1)\n                real_position = lb + potential_position * scale\n                value = func(real_position)\n                if value < self.personal_best_value[i]:\n                    self.personal_best_value[i] = value\n                    self.personal_best_position[i] = potential_position\n                if value < self.global_best_value:\n                    self.global_best_value = value\n                    self.global_best_position = potential_position\n\n        while self.iteration < self.budget:\n            quantum_update()\n            for i in range(self.particles):\n                cognitive = np.random.random(self.dim) * (0.5 + 0.5 * np.sin(self.iteration))\n                social = np.random.random(self.dim) * (0.5 + 0.5 * np.cos(self.iteration))\n                self.velocity[i] = (0.5 * self.velocity[i] +\n                                    cognitive * (self.personal_best_position[i] - self.position[i]) +\n                                    social * (self.global_best_position - self.position[i]))\n                self.position[i] += 0.9 * self.velocity[i]  # Adaptive velocity scaling\n                self.position[i] = np.clip(self.position[i], 0, 1)\n            self.iteration += self.particles\n\n        return lb + self.global_best_position * scale", "name": "QIPSO", "description": "Improved EQIPSO adjusts cognitive and social component influence dynamically to enhance exploration and exploitation balance.", "configspace": "", "generation": 5, "fitness": 0.2845552134543772, "feedback": "The algorithm QIPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28 with standard deviation 0.00.", "error": "", "parent_id": "bb88ff32-4df5-4f23-933c-6f6937e2d8e4", "metadata": {"aucs": [0.2845552134543772]}, "mutation_prompt": null}
{"id": "93909956-8e88-4205-b4af-3887e7cc796e", "solution": "import numpy as np\n\nclass QIPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particles = min(50, budget // 10)  # Number of quantum particles\n        self.position = np.random.uniform(0, 1, (self.particles, self.dim))\n        self.velocity = np.zeros((self.particles, self.dim))\n        self.personal_best_position = np.copy(self.position)\n        self.global_best_position = None\n        self.personal_best_value = np.full(self.particles, np.inf)\n        self.global_best_value = np.inf\n        self.iteration = 0\n\n    def __call__(self, func):\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        scale = ub - lb\n\n        def quantum_update():\n            for i in range(self.particles):\n                delta = np.random.uniform(-1, 1, self.dim)\n                potential_position = self.position[i] + self.velocity[i] * delta\n                potential_position = np.clip(potential_position, 0, 1)\n                real_position = lb + potential_position * scale\n                value = func(real_position)\n                if value < self.personal_best_value[i]:\n                    self.personal_best_value[i] = value\n                    self.personal_best_position[i] = potential_position\n                if value < self.global_best_value:\n                    self.global_best_value = value\n                    self.global_best_position = potential_position\n        \n        while self.iteration < self.budget:\n            quantum_update()\n            for i in range(self.particles):\n                cognitive = np.random.random(self.dim)\n                social = np.random.random(self.dim)\n                self.velocity[i] = (0.5 * self.velocity[i] +\n                                    cognitive * (self.personal_best_position[i] - self.position[i]) +\n                                    social * (self.global_best_position - self.position[i]))\n                self.position[i] += 0.9 * self.velocity[i]  # Adaptive velocity scaling\n                self.position[i] = np.clip(self.position[i], 0, 1)\n            self.iteration += self.particles\n\n        return lb + self.global_best_position * scale", "name": "QIPSO", "description": "Improved EQIPSO with dynamic particle count for enhanced exploration-exploitation balance.", "configspace": "", "generation": 6, "fitness": 0.2885712014995727, "feedback": "The algorithm QIPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.29 with standard deviation 0.00.", "error": "", "parent_id": "bb88ff32-4df5-4f23-933c-6f6937e2d8e4", "metadata": {"aucs": [0.2885712014995727]}, "mutation_prompt": null}
{"id": "ebb8fdea-6ec8-477d-9fd3-9a16afc7bef1", "solution": "import numpy as np\n\nclass QIPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particles = 50  # Number of quantum particles\n        self.position = np.random.uniform(0, 1, (self.particles, self.dim))\n        self.velocity = np.zeros((self.particles, self.dim))\n        self.personal_best_position = np.copy(self.position)\n        self.global_best_position = None\n        self.personal_best_value = np.full(self.particles, np.inf)\n        self.global_best_value = np.inf\n        self.iteration = 0\n\n    def __call__(self, func):\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        scale = ub - lb\n\n        def quantum_update():\n            for i in range(self.particles):\n                delta = np.random.uniform(-1, 1, self.dim)\n                potential_position = self.position[i] + self.velocity[i] * delta\n                potential_position = np.clip(potential_position, 0, 1)\n                real_position = lb + potential_position * scale\n                value = func(real_position)\n                if value < self.personal_best_value[i]:\n                    self.personal_best_value[i] = value\n                    self.personal_best_position[i] = potential_position\n                if value < self.global_best_value:\n                    self.global_best_value = value\n                    self.global_best_position = potential_position\n\n        while self.iteration < self.budget:\n            quantum_update()\n            for i in range(self.particles):\n                cognitive = np.random.uniform(0.5, 1.5, self.dim)  # Modified cognitive component\n                social = np.random.random(self.dim)\n                self.velocity[i] = (0.5 * self.velocity[i] +\n                                    cognitive * (self.personal_best_position[i] - self.position[i]) +\n                                    social * (self.global_best_position - self.position[i]))\n                self.position[i] += 0.9 * self.velocity[i]  # Adaptive velocity scaling\n                self.position[i] = np.clip(self.position[i], 0, 1)\n            self.iteration += self.particles\n\n        return lb + self.global_best_position * scale", "name": "QIPSO", "description": "Adjusted cognitive component to enhance convergence and population diversity.", "configspace": "", "generation": 7, "fitness": 0.28824191660533516, "feedback": "The algorithm QIPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.29 with standard deviation 0.00.", "error": "", "parent_id": "bb88ff32-4df5-4f23-933c-6f6937e2d8e4", "metadata": {"aucs": [0.28824191660533516]}, "mutation_prompt": null}
{"id": "c75e9fd1-438b-4ad8-8bd4-7399b2a82a26", "solution": "import numpy as np\n\nclass QIPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particles = 50  # Number of quantum particles\n        self.position = np.random.uniform(0, 1, (self.particles, self.dim))\n        self.velocity = np.zeros((self.particles, self.dim))\n        self.personal_best_position = np.copy(self.position)\n        self.global_best_position = None\n        self.personal_best_value = np.full(self.particles, np.inf)\n        self.global_best_value = np.inf\n        self.iteration = 0\n\n    def __call__(self, func):\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        scale = ub - lb\n\n        def quantum_update():\n            for i in range(self.particles):\n                delta = np.random.uniform(-1, 1, self.dim)\n                potential_position = self.position[i] + self.velocity[i] * delta\n                potential_position = np.clip(potential_position, 0, 1)\n                real_position = lb + potential_position * scale\n                value = func(real_position)\n                if value < self.personal_best_value[i]:\n                    self.personal_best_value[i] = value\n                    self.personal_best_position[i] = potential_position\n                if value < self.global_best_value:\n                    self.global_best_value = value\n                    self.global_best_position = potential_position\n\n        while self.iteration < self.budget:\n            quantum_update()\n            for i in range(self.particles):\n                cognitive = np.random.random(self.dim) * 1.2  # Amplified cognitive component\n                social = np.random.random(self.dim)\n                self.velocity[i] = (0.5 * self.velocity[i] +\n                                    cognitive * (self.personal_best_position[i] - self.position[i]) +\n                                    social * (self.global_best_position - self.position[i]))\n                self.position[i] += 0.9 * self.velocity[i]  # Adaptive velocity scaling\n                self.position[i] = np.clip(self.position[i], 0, 1)\n            self.iteration += self.particles\n\n        return lb + self.global_best_position * scale", "name": "QIPSO", "description": "Enhanced Quantum-Inspired Particle Swarm Optimization (EQIPSO) with amplified cognitive component for accelerated convergence.", "configspace": "", "generation": 8, "fitness": 0.2879019118205721, "feedback": "The algorithm QIPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.29 with standard deviation 0.00.", "error": "", "parent_id": "bb88ff32-4df5-4f23-933c-6f6937e2d8e4", "metadata": {"aucs": [0.2879019118205721]}, "mutation_prompt": null}
{"id": "8b289413-1e3e-48ac-8364-2d3c41df0371", "solution": "import numpy as np\n\nclass QIPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particles = 50  # Number of quantum particles\n        self.position = np.random.uniform(0, 1, (self.particles, self.dim))\n        self.velocity = np.zeros((self.particles, self.dim))\n        self.personal_best_position = np.copy(self.position)\n        self.global_best_position = None\n        self.personal_best_value = np.full(self.particles, np.inf)\n        self.global_best_value = np.inf\n        self.iteration = 0\n\n    def __call__(self, func):\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        scale = ub - lb\n\n        def quantum_update():\n            for i in range(self.particles):\n                delta = np.random.uniform(-1, 1, self.dim)\n                potential_position = self.position[i] + self.velocity[i] * delta\n                potential_position = np.clip(potential_position, 0, 1)\n                real_position = lb + potential_position * scale\n                value = func(real_position)\n                if value < self.personal_best_value[i]:\n                    self.personal_best_value[i] = value\n                    self.personal_best_position[i] = potential_position\n                if value < self.global_best_value:\n                    self.global_best_value = value\n                    self.global_best_position = potential_position\n\n        while self.iteration < self.budget:\n            quantum_update()\n            inertia_weight = 0.5 + 0.4 * (1 - self.iteration / self.budget)  # Dynamic inertia weight\n            for i in range(self.particles):\n                cognitive = np.random.random(self.dim)\n                social = np.random.random(self.dim)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive * (self.personal_best_position[i] - self.position[i]) +\n                                    social * (self.global_best_position - self.position[i]))\n                self.position[i] += 0.9 * self.velocity[i]  # Adaptive velocity scaling\n                self.position[i] = np.clip(self.position[i], 0, 1)\n            self.iteration += self.particles\n\n        return lb + self.global_best_position * scale", "name": "QIPSO", "description": "Enhanced Quantum-Inspired Particle Swarm Optimization (EQIPSO) with dynamic inertia weight for improved exploration-exploitation balance.", "configspace": "", "generation": 9, "fitness": 0.2857357541300547, "feedback": "The algorithm QIPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.29 with standard deviation 0.00.", "error": "", "parent_id": "bb88ff32-4df5-4f23-933c-6f6937e2d8e4", "metadata": {"aucs": [0.2857357541300547]}, "mutation_prompt": null}
{"id": "7d292af4-a935-4d39-8388-a8b4e55c53da", "solution": "import numpy as np\n\nclass QIPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particles = 50  # Number of quantum particles\n        self.position = np.random.uniform(0, 1, (self.particles, self.dim))\n        self.velocity = np.zeros((self.particles, self.dim))\n        self.personal_best_position = np.copy(self.position)\n        self.global_best_position = None\n        self.personal_best_value = np.full(self.particles, np.inf)\n        self.global_best_value = np.inf\n        self.iteration = 0\n\n    def __call__(self, func):\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        scale = ub - lb\n\n        def quantum_update():\n            for i in range(self.particles):\n                delta = np.random.uniform(-1, 1, self.dim)\n                potential_position = self.position[i] + self.velocity[i] * delta\n                potential_position = np.clip(potential_position, 0, 1)\n                real_position = lb + potential_position * scale\n                value = func(real_position)\n                if value < self.personal_best_value[i]:\n                    self.personal_best_value[i] = value\n                    self.personal_best_position[i] = potential_position\n                if value < self.global_best_value:\n                    self.global_best_value = value\n                    self.global_best_position = potential_position\n\n        while self.iteration < self.budget:\n            quantum_update()\n            for i in range(self.particles):\n                cognitive = np.random.random(self.dim)\n                social = np.random.random(self.dim)\n                inertia_weight = 0.9 - 0.5 * (self.iteration / self.budget)  # Dynamic inertia weight adjustment\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive * (self.personal_best_position[i] - self.position[i]) +\n                                    social * (self.global_best_position - self.position[i]))\n                self.position[i] += self.velocity[i]  # Adaptive velocity scaling\n                self.position[i] = np.clip(self.position[i], 0, 1)\n            self.iteration += self.particles\n\n        return lb + self.global_best_position * scale", "name": "QIPSO", "description": "Introduce dynamic inertia weight adjustment to enhance exploration and exploitation balance in EQIPSO.", "configspace": "", "generation": 10, "fitness": 0.2844562016826331, "feedback": "The algorithm QIPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28 with standard deviation 0.00.", "error": "", "parent_id": "bb88ff32-4df5-4f23-933c-6f6937e2d8e4", "metadata": {"aucs": [0.2844562016826331]}, "mutation_prompt": null}
{"id": "5455cf80-267f-428c-986a-551d36615374", "solution": "import numpy as np\n\nclass QIPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particles = 50  # Number of quantum particles\n        self.position = np.random.uniform(0, 1, (self.particles, self.dim))\n        self.velocity = np.zeros((self.particles, self.dim))\n        self.personal_best_position = np.copy(self.position)\n        self.global_best_position = None\n        self.personal_best_value = np.full(self.particles, np.inf)\n        self.global_best_value = np.inf\n        self.iteration = 0\n\n    def __call__(self, func):\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        scale = ub - lb\n\n        def quantum_update():\n            for i in range(self.particles):\n                delta = np.random.uniform(-1, 1, self.dim)\n                potential_position = self.position[i] + self.velocity[i] * delta\n                potential_position = np.clip(potential_position, 0, 1)\n                real_position = lb + potential_position * scale\n                value = func(real_position)\n                if value < self.personal_best_value[i]:\n                    self.personal_best_value[i] = value\n                    self.personal_best_position[i] = potential_position\n                if value < self.global_best_value:\n                    self.global_best_value = value\n                    self.global_best_position = potential_position\n\n        while self.iteration < self.budget:\n            quantum_update()\n            inertia_weight = 0.9 - 0.7 * (self.iteration / self.budget)  # Nonlinear inertia weight\n            for i in range(self.particles):\n                cognitive = np.random.random(self.dim)\n                social = np.random.random(self.dim)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive * (self.personal_best_position[i] - self.position[i]) +\n                                    social * (self.global_best_position - self.position[i]))\n                self.position[i] += inertia_weight * self.velocity[i]  # Adaptive velocity scaling\n                self.position[i] = np.clip(self.position[i], 0, 1)\n            self.iteration += self.particles\n\n        return lb + self.global_best_position * scale", "name": "QIPSO", "description": "Introduced a nonlinear inertia weight to improve exploration-exploitation balance in the adaptive velocity scaling.", "configspace": "", "generation": 11, "fitness": 0.2851170630235593, "feedback": "The algorithm QIPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.29 with standard deviation 0.00.", "error": "", "parent_id": "bb88ff32-4df5-4f23-933c-6f6937e2d8e4", "metadata": {"aucs": [0.2851170630235593]}, "mutation_prompt": null}
{"id": "339b6d0f-ee4f-45ae-8875-803a9fd3283e", "solution": "import numpy as np\n\nclass QIPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particles = 50  # Number of quantum particles\n        self.position = np.random.uniform(0, 1, (self.particles, self.dim))\n        self.velocity = np.zeros((self.particles, self.dim))\n        self.personal_best_position = np.copy(self.position)\n        self.global_best_position = None\n        self.personal_best_value = np.full(self.particles, np.inf)\n        self.global_best_value = np.inf\n        self.iteration = 0\n\n    def __call__(self, func):\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        scale = ub - lb\n\n        def quantum_update():\n            for i in range(self.particles):\n                delta = np.random.uniform(-1, 1, self.dim)\n                potential_position = self.position[i] + self.velocity[i] * delta\n                potential_position = np.clip(potential_position, 0, 1)\n                real_position = lb + potential_position * scale\n                value = func(real_position)\n                if value < self.personal_best_value[i]:\n                    self.personal_best_value[i] = value\n                    self.personal_best_position[i] = potential_position\n                if value < self.global_best_value:\n                    self.global_best_value = value\n                    self.global_best_position = potential_position\n\n        while self.iteration < self.budget:\n            quantum_update()\n            inertia = 0.5 + 0.4 * (1 - self.iteration / self.budget)  # Dynamic inertia factor\n            for i in range(self.particles):\n                cognitive = np.random.random(self.dim)\n                social = np.random.random(self.dim)\n                self.velocity[i] = (inertia * self.velocity[i] +\n                                    cognitive * (self.personal_best_position[i] - self.position[i]) +\n                                    social * (self.global_best_position - self.position[i]))\n                self.position[i] += 0.9 * self.velocity[i]  # Adaptive velocity scaling\n                self.position[i] = np.clip(self.position[i], 0, 1)\n            self.iteration += self.particles\n\n        return lb + self.global_best_position * scale", "name": "QIPSO", "description": "Improved Quantum-Inspired PSO (IQIPSO) with dynamic inertia to enhance exploration-exploitation balance.", "configspace": "", "generation": 12, "fitness": 0.2857357541300547, "feedback": "The algorithm QIPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.29 with standard deviation 0.00.", "error": "", "parent_id": "bb88ff32-4df5-4f23-933c-6f6937e2d8e4", "metadata": {"aucs": [0.2857357541300547]}, "mutation_prompt": null}
{"id": "5b0f81e2-2b15-4701-a87f-e06c5885887d", "solution": "import numpy as np\n\nclass QIPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particles = 50  # Number of quantum particles\n        self.position = np.random.uniform(0, 1, (self.particles, self.dim))\n        self.velocity = np.zeros((self.particles, self.dim))\n        self.personal_best_position = np.copy(self.position)\n        self.global_best_position = None\n        self.personal_best_value = np.full(self.particles, np.inf)\n        self.global_best_value = np.inf\n        self.iteration = 0\n\n    def __call__(self, func):\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        scale = ub - lb\n\n        def quantum_update():\n            for i in range(self.particles):\n                delta = np.random.uniform(-1, 1, self.dim)\n                potential_position = self.position[i] + self.velocity[i] * delta\n                potential_position = np.clip(potential_position, 0, 1)\n                real_position = lb + potential_position * scale\n                value = func(real_position)\n                if value < self.personal_best_value[i]:\n                    self.personal_best_value[i] = value\n                    self.personal_best_position[i] = potential_position\n                if value < self.global_best_value:\n                    self.global_best_value = value\n                    self.global_best_position = potential_position\n\n        while self.iteration < self.budget:\n            quantum_update()\n            for i in range(self.particles):\n                cognitive = np.random.random(self.dim)\n                social = np.random.random(self.dim)\n                self.velocity[i] = (0.5 * self.velocity[i] +\n                                    cognitive * (self.personal_best_position[i] - self.position[i]) +\n                                    social * (self.global_best_position - self.position[i]) +\n                                    np.random.uniform(-0.1, 0.1, self.dim))  # Random perturbation\n                self.position[i] += 0.9 * self.velocity[i]  # Adaptive velocity scaling\n                self.position[i] = np.clip(self.position[i], 0, 1)\n            self.iteration += self.particles\n\n        return lb + self.global_best_position * scale", "name": "QIPSO", "description": "Introduce random perturbation in velocity update to enhance exploration and prevent particle stagnation.", "configspace": "", "generation": 13, "fitness": 0.2848180949789707, "feedback": "The algorithm QIPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28 with standard deviation 0.00.", "error": "", "parent_id": "bb88ff32-4df5-4f23-933c-6f6937e2d8e4", "metadata": {"aucs": [0.2848180949789707]}, "mutation_prompt": null}
{"id": "68bec999-b125-46fd-96f2-e95f1dbb39a1", "solution": "import numpy as np\n\nclass QIPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particles = 50\n        self.position = np.random.uniform(0, 1, (self.particles, self.dim))\n        self.velocity = np.zeros((self.particles, self.dim))\n        self.personal_best_position = np.copy(self.position)\n        self.global_best_position = None\n        self.personal_best_value = np.full(self.particles, np.inf)\n        self.global_best_value = np.inf\n        self.iteration = 0\n        self.chaotic_sequence = np.random.rand(self.particles)  # Chaotic sequence initialization\n\n    def __call__(self, func):\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        scale = ub - lb\n\n        def logistic_map(x):\n            return 4 * x * (1 - x)  # Chaotic logistic map\n\n        def quantum_update():\n            for i in range(self.particles):\n                delta = np.random.uniform(-1, 1, self.dim) * self.chaotic_sequence[i]\n                potential_position = self.position[i] + self.velocity[i] * delta\n                potential_position = np.clip(potential_position, 0, 1)\n                real_position = lb + potential_position * scale\n                value = func(real_position)\n                if value < self.personal_best_value[i]:\n                    self.personal_best_value[i] = value\n                    self.personal_best_position[i] = potential_position\n                if value < self.global_best_value:\n                    self.global_best_value = value\n                    self.global_best_position = potential_position\n            self.chaotic_sequence = logistic_map(self.chaotic_sequence)  # Update chaotic sequence\n\n        while self.iteration < self.budget:\n            quantum_update()\n            for i in range(self.particles):\n                cognitive = np.random.random(self.dim)\n                social = np.random.random(self.dim)\n                self.velocity[i] = (0.5 * self.velocity[i] +\n                                    cognitive * (self.personal_best_position[i] - self.position[i]) +\n                                    social * (self.global_best_position - self.position[i]))\n                self.position[i] += 0.9 * self.velocity[i]\n                self.position[i] = np.clip(self.position[i], 0, 1)\n            self.iteration += self.particles\n\n        return lb + self.global_best_position * scale", "name": "QIPSO", "description": "EQIPSO with Chaotic Quantum Update (EQIPSO-CQU) embeds chaotic sequences for enhanced exploration and convergence.", "configspace": "", "generation": 14, "fitness": 0.2885075117024948, "feedback": "The algorithm QIPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.29 with standard deviation 0.00.", "error": "", "parent_id": "bb88ff32-4df5-4f23-933c-6f6937e2d8e4", "metadata": {"aucs": [0.2885075117024948]}, "mutation_prompt": null}
{"id": "c178e4eb-a4a2-428a-9953-a05c0e235135", "solution": "import numpy as np\n\nclass QIPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particles = 50  # Number of quantum particles\n        self.position = np.random.uniform(0, 1, (self.particles, self.dim))\n        self.velocity = np.zeros((self.particles, self.dim))\n        self.personal_best_position = np.copy(self.position)\n        self.global_best_position = None\n        self.personal_best_value = np.full(self.particles, np.inf)\n        self.global_best_value = np.inf\n        self.iteration = 0\n\n    def __call__(self, func):\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        scale = ub - lb\n\n        def quantum_update():\n            for i in range(self.particles):\n                delta = np.random.uniform(-1, 1, self.dim)\n                potential_position = self.position[i] + self.velocity[i] * delta\n                potential_position = np.clip(potential_position, 0, 1)\n                real_position = lb + potential_position * scale\n                value = func(real_position)\n                if value < self.personal_best_value[i]:\n                    self.personal_best_value[i] = value\n                    self.personal_best_position[i] = potential_position\n                if value < self.global_best_value:\n                    self.global_best_value = value\n                    self.global_best_position = potential_position\n\n        while self.iteration < self.budget:\n            quantum_update()\n            for i in range(self.particles):\n                cognitive = np.random.random(self.dim)\n                social = np.random.random(self.dim)\n                self.velocity[i] = (0.5 * self.velocity[i] +\n                                    cognitive * (self.personal_best_position[i] - self.position[i]) +\n                                    social * (self.global_best_position - self.position[i]))\n                self.position[i] += 0.5 * self.velocity[i]  # Dynamic velocity damping\n                self.position[i] = np.clip(self.position[i], 0, 1)\n            self.iteration += self.particles\n\n        return lb + self.global_best_position * scale", "name": "QIPSO", "description": "Improved QIPSO with Dynamic Velocity Damping to enhance convergence stability.", "configspace": "", "generation": 15, "fitness": 0.28834711433213467, "feedback": "The algorithm QIPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.29 with standard deviation 0.00.", "error": "", "parent_id": "bb88ff32-4df5-4f23-933c-6f6937e2d8e4", "metadata": {"aucs": [0.28834711433213467]}, "mutation_prompt": null}
{"id": "433dd7bf-eda8-409b-9c70-6edd73905d19", "solution": "import numpy as np\n\nclass QIPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particles = 50  # Number of quantum particles\n        self.position = np.random.uniform(0, 1, (self.particles, self.dim))\n        self.velocity = np.zeros((self.particles, self.dim))\n        self.personal_best_position = np.copy(self.position)\n        self.global_best_position = None\n        self.personal_best_value = np.full(self.particles, np.inf)\n        self.global_best_value = np.inf\n        self.iteration = 0\n\n    def __call__(self, func):\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        scale = ub - lb\n\n        def quantum_update():\n            for i in range(self.particles):\n                delta = np.random.uniform(-1, 1, self.dim)\n                potential_position = self.position[i] + self.velocity[i] * delta\n                potential_position = np.clip(potential_position, 0, 1)\n                real_position = lb + potential_position * scale\n                value = func(real_position)\n                if value < self.personal_best_value[i]:\n                    self.personal_best_value[i] = value\n                    self.personal_best_position[i] = potential_position\n                if value < self.global_best_value:\n                    self.global_best_value = value\n                    self.global_best_position = potential_position\n\n        while self.iteration < self.budget:\n            quantum_update()\n            inertia_weight = 0.9 - 0.5 * (self.iteration / self.budget)  # Dynamic inertia weight\n            for i in range(self.particles):\n                cognitive = np.random.random(self.dim)\n                social = np.random.random(self.dim)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive * (self.personal_best_position[i] - self.position[i]) +\n                                    social * (self.global_best_position - self.position[i]))\n                self.position[i] += 0.9 * self.velocity[i]  # Adaptive velocity scaling\n                self.position[i] = np.clip(self.position[i], 0, 1)\n            self.iteration += self.particles\n\n        return lb + self.global_best_position * scale", "name": "QIPSO", "description": "Improved EQIPSO with dynamic inertia weight for better exploration-exploitation balance.", "configspace": "", "generation": 16, "fitness": 0.2861613722481009, "feedback": "The algorithm QIPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.29 with standard deviation 0.00.", "error": "", "parent_id": "bb88ff32-4df5-4f23-933c-6f6937e2d8e4", "metadata": {"aucs": [0.2861613722481009]}, "mutation_prompt": null}
{"id": "3a4d0731-cf00-4fa0-ae3e-752fedb0be0e", "solution": "import numpy as np\n\nclass QIPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particles = 50  # Number of quantum particles\n        self.position = np.random.uniform(0, 1, (self.particles, self.dim))\n        self.velocity = np.zeros((self.particles, self.dim))\n        self.personal_best_position = np.copy(self.position)\n        self.global_best_position = None\n        self.personal_best_value = np.full(self.particles, np.inf)\n        self.global_best_value = np.inf\n        self.iteration = 0\n\n    def __call__(self, func):\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        scale = ub - lb\n\n        def quantum_update():\n            for i in range(self.particles):\n                delta = np.random.uniform(-1, 1, self.dim)\n                potential_position = self.position[i] + self.velocity[i] * delta\n                potential_position = np.clip(potential_position, 0, 1)\n                real_position = lb + potential_position * scale\n                value = func(real_position)\n                if value < self.personal_best_value[i]:\n                    self.personal_best_value[i] = value\n                    self.personal_best_position[i] = potential_position\n                if value < self.global_best_value:\n                    self.global_best_value = value\n                    self.global_best_position = potential_position\n\n        inertia_weight = 0.9  # Added inertia weight adjustment\n        while self.iteration < self.budget:\n            quantum_update()\n            for i in range(self.particles):\n                cognitive = np.random.random(self.dim)\n                social = np.random.random(self.dim)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive * (self.personal_best_position[i] - self.position[i]) +\n                                    social * (self.global_best_position - self.position[i]))\n                self.position[i] += 0.9 * self.velocity[i]  # Adaptive velocity scaling\n                self.position[i] = np.clip(self.position[i], 0, 1)\n            self.iteration += self.particles\n\n        return lb + self.global_best_position * scale", "name": "QIPSO", "description": "Introduce inertia weight adjustment to enhance exploration and exploitation balance in EQIPSO.", "configspace": "", "generation": 17, "fitness": 0.2855047050587448, "feedback": "The algorithm QIPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.29 with standard deviation 0.00.", "error": "", "parent_id": "bb88ff32-4df5-4f23-933c-6f6937e2d8e4", "metadata": {"aucs": [0.2855047050587448]}, "mutation_prompt": null}
{"id": "4b142fe9-ef8b-48b4-8d69-8f47d53f2e7c", "solution": "import numpy as np\n\nclass QIPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particles = 50  # Number of quantum particles\n        self.position = np.random.uniform(0, 1, (self.particles, self.dim))\n        self.velocity = np.zeros((self.particles, self.dim))\n        self.personal_best_position = np.copy(self.position)\n        self.global_best_position = None\n        self.personal_best_value = np.full(self.particles, np.inf)\n        self.global_best_value = np.inf\n        self.iteration = 0\n\n    def __call__(self, func):\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        scale = ub - lb\n\n        def quantum_update():\n            for i in range(self.particles):\n                delta = np.random.uniform(-1, 1, self.dim)\n                potential_position = self.position[i] + self.velocity[i] * delta\n                potential_position = np.clip(potential_position, 0, 1)\n                real_position = lb + potential_position * scale\n                value = func(real_position)\n                if value < self.personal_best_value[i]:\n                    self.personal_best_value[i] = value\n                    self.personal_best_position[i] = potential_position\n                if value < self.global_best_value:\n                    self.global_best_value = value\n                    self.global_best_position = potential_position\n\n        while self.iteration < self.budget:\n            quantum_update()\n            inertia_weight = 0.9 - 0.7 * (self.iteration / self.budget)  # Dynamic inertia weight\n            for i in range(self.particles):\n                cognitive = np.random.random(self.dim)\n                social = np.random.random(self.dim)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive * (self.personal_best_position[i] - self.position[i]) +\n                                    social * (self.global_best_position - self.position[i]))\n                self.position[i] += 0.9 * self.velocity[i]  # Adaptive velocity scaling\n                self.position[i] = np.clip(self.position[i], 0, 1)\n            self.iteration += self.particles\n\n        return lb + self.global_best_position * scale", "name": "QIPSO", "description": "Introduce dynamic inertia weighting to balance exploration and exploitation for improved convergence.", "configspace": "", "generation": 18, "fitness": 0.28622895476058907, "feedback": "The algorithm QIPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.29 with standard deviation 0.00.", "error": "", "parent_id": "bb88ff32-4df5-4f23-933c-6f6937e2d8e4", "metadata": {"aucs": [0.28622895476058907]}, "mutation_prompt": null}
{"id": "114a804b-1a1e-4e82-89fe-4fb6488740b6", "solution": "import numpy as np\n\nclass QIPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particles = 50  # Number of quantum particles\n        self.position = np.random.uniform(0, 1, (self.particles, self.dim))\n        self.velocity = np.zeros((self.particles, self.dim))\n        self.personal_best_position = np.copy(self.position)\n        self.global_best_position = None\n        self.personal_best_value = np.full(self.particles, np.inf)\n        self.global_best_value = np.inf\n        self.iteration = 0\n\n    def __call__(self, func):\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        scale = ub - lb\n\n        def quantum_update():\n            for i in range(self.particles):\n                delta = np.random.uniform(-1, 1, self.dim)\n                potential_position = self.position[i] + self.velocity[i] * delta\n                potential_position = np.clip(potential_position, 0, 1)\n                real_position = lb + potential_position * scale\n                value = func(real_position)\n                if value < self.personal_best_value[i]:\n                    self.personal_best_value[i] = value\n                    self.personal_best_position[i] = potential_position\n                if value < self.global_best_value:\n                    self.global_best_value = value\n                    self.global_best_position = potential_position\n\n        inertia_weight = 0.7  # Added inertia weight for velocity update\n        while self.iteration < self.budget:\n            quantum_update()\n            for i in range(self.particles):\n                cognitive = np.random.random(self.dim)\n                social = np.random.random(self.dim)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive * (self.personal_best_position[i] - self.position[i]) +\n                                    social * (self.global_best_position - self.position[i]))\n                self.position[i] += 0.9 * self.velocity[i]  # Adaptive velocity scaling\n                self.position[i] = np.clip(self.position[i], 0, 1)\n            self.iteration += self.particles\n\n        return lb + self.global_best_position * scale", "name": "QIPSO", "description": "Improved velocity updating by introducing inertia weight to balance exploration and exploitation.", "configspace": "", "generation": 19, "fitness": 0.2859296199244209, "feedback": "The algorithm QIPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.29 with standard deviation 0.00.", "error": "", "parent_id": "bb88ff32-4df5-4f23-933c-6f6937e2d8e4", "metadata": {"aucs": [0.2859296199244209]}, "mutation_prompt": null}
{"id": "0087cbd0-fc42-4469-be54-c3896ea343fd", "solution": "import numpy as np\n\nclass QIPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particles = max(30, int(50 * (dim/10)))  # Number of quantum particles scaled dynamically\n        self.position = np.random.uniform(0, 1, (self.particles, self.dim))\n        self.velocity = np.zeros((self.particles, self.dim))\n        self.personal_best_position = np.copy(self.position)\n        self.global_best_position = None\n        self.personal_best_value = np.full(self.particles, np.inf)\n        self.global_best_value = np.inf\n        self.iteration = 0\n\n    def __call__(self, func):\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        scale = ub - lb\n\n        def quantum_update():\n            for i in range(self.particles):\n                delta = np.random.uniform(-1, 1, self.dim)\n                potential_position = self.position[i] + self.velocity[i] * delta\n                potential_position = np.clip(potential_position, 0, 1)\n                real_position = lb + potential_position * scale\n                value = func(real_position)\n                if value < self.personal_best_value[i]:\n                    self.personal_best_value[i] = value\n                    self.personal_best_position[i] = potential_position\n                if value < self.global_best_value:\n                    self.global_best_value = value\n                    self.global_best_position = potential_position\n\n        while self.iteration < self.budget:\n            quantum_update()\n            for i in range(self.particles):\n                cognitive = np.random.random(self.dim)\n                social = np.random.random(self.dim)\n                self.velocity[i] = (0.5 * self.velocity[i] +\n                                    cognitive * (self.personal_best_position[i] - self.position[i]) +\n                                    social * (self.global_best_position - self.position[i]))\n                self.position[i] += 0.9 * self.velocity[i]  # Adaptive velocity scaling\n                self.position[i] = np.clip(self.position[i], 0, 1)\n            self.iteration += self.particles\n\n        return lb + self.global_best_position * scale", "name": "QIPSO", "description": "EQIPSO with dynamic particle count adjustment for enhanced exploration-exploitation balance.", "configspace": "", "generation": 20, "fitness": 0.2885712014995727, "feedback": "The algorithm QIPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.29 with standard deviation 0.00.", "error": "", "parent_id": "bb88ff32-4df5-4f23-933c-6f6937e2d8e4", "metadata": {"aucs": [0.2885712014995727]}, "mutation_prompt": null}
{"id": "22d3a134-8a26-4f4f-bb75-982382b52ffb", "solution": "import numpy as np\n\nclass QIPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particles = 50  # Number of quantum particles\n        self.position = np.random.uniform(0, 1, (self.particles, self.dim))\n        self.velocity = np.zeros((self.particles, self.dim))\n        self.personal_best_position = np.copy(self.position)\n        self.global_best_position = None\n        self.personal_best_value = np.full(self.particles, np.inf)\n        self.global_best_value = np.inf\n        self.iteration = 0\n\n    def __call__(self, func):\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        scale = ub - lb\n\n        def quantum_update():\n            for i in range(self.particles):\n                delta = np.random.uniform(-1, 1, self.dim)\n                potential_position = self.position[i] + self.velocity[i] * delta\n                potential_position = np.clip(potential_position, 0, 1)\n                real_position = lb + potential_position * scale\n                value = func(real_position)\n                if value < self.personal_best_value[i]:\n                    self.personal_best_value[i] = value\n                    self.personal_best_position[i] = potential_position\n                if value < self.global_best_value:\n                    self.global_best_value = value\n                    self.global_best_position = potential_position\n\n        while self.iteration < self.budget:\n            quantum_update()\n            for i in range(self.particles):\n                cognitive = np.random.random(self.dim)\n                social = np.random.random(self.dim) * np.random.uniform(1.5, 2.0)  # Added random weight factor\n                self.velocity[i] = (0.5 * self.velocity[i] +\n                                    cognitive * (self.personal_best_position[i] - self.position[i]) +\n                                    social * (self.global_best_position - self.position[i]))\n                self.position[i] += 0.9 * self.velocity[i]  # Adaptive velocity scaling\n                self.position[i] = np.clip(self.position[i], 0, 1)\n            self.iteration += self.particles\n\n        return lb + self.global_best_position * scale", "name": "QIPSO", "description": "Introduced a random weight factor to adjust social influence dynamically for enhanced convergence.", "configspace": "", "generation": 21, "fitness": 0.288355692181, "feedback": "The algorithm QIPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.29 with standard deviation 0.00.", "error": "", "parent_id": "bb88ff32-4df5-4f23-933c-6f6937e2d8e4", "metadata": {"aucs": [0.288355692181]}, "mutation_prompt": null}
{"id": "fc92078f-2389-45a4-be18-1c75937a5cf0", "solution": "import numpy as np\n\nclass QIPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particles = 50  # Number of quantum particles\n        self.position = np.random.uniform(0, 1, (self.particles, self.dim))\n        self.velocity = np.zeros((self.particles, self.dim))\n        self.personal_best_position = np.copy(self.position)\n        self.global_best_position = None\n        self.personal_best_value = np.full(self.particles, np.inf)\n        self.global_best_value = np.inf\n        self.iteration = 0\n\n    def __call__(self, func):\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        scale = ub - lb\n\n        def quantum_update():\n            for i in range(self.particles):\n                delta = np.random.uniform(-1, 1, self.dim)\n                potential_position = self.position[i] + self.velocity[i] * delta\n                potential_position = np.clip(potential_position, 0, 1)\n                real_position = lb + potential_position * scale\n                value = func(real_position)\n                if value < self.personal_best_value[i]:\n                    self.personal_best_value[i] = value\n                    self.personal_best_position[i] = potential_position\n                if value < self.global_best_value:\n                    self.global_best_value = value\n                    self.global_best_position = potential_position\n\n        while self.iteration < self.budget:\n            quantum_update()\n            inertia_weight = 0.9 - 0.5 * (self.iteration / self.budget)  # Dynamic inertia weight\n            for i in range(self.particles):\n                cognitive = np.random.random(self.dim)\n                social = np.random.random(self.dim)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive * (self.personal_best_position[i] - self.position[i]) +\n                                    social * (self.global_best_position - self.position[i]))\n                self.position[i] += self.velocity[i]\n                self.position[i] = np.clip(self.position[i], 0, 1)\n                self.position[i] += 0.01 * np.sin(np.pi * self.position[i])  # Chaos-driven local search\n            self.iteration += self.particles\n\n        return lb + self.global_best_position * scale", "name": "QIPSO", "description": "Enhanced Quantum-Inspired Particle Swarm Optimization (EQIPSO) with dynamic inertia weight and chaos-driven local search for improved global exploration.", "configspace": "", "generation": 22, "fitness": 0.28548322568828277, "feedback": "The algorithm QIPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.29 with standard deviation 0.00.", "error": "", "parent_id": "bb88ff32-4df5-4f23-933c-6f6937e2d8e4", "metadata": {"aucs": [0.28548322568828277]}, "mutation_prompt": null}
{"id": "8dcd1af7-4915-4820-82b6-0ccd8bc18753", "solution": "import numpy as np\n\nclass QIPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particles = 50  # Number of quantum particles\n        self.position = np.random.uniform(0, 1, (self.particles, self.dim))\n        self.velocity = np.zeros((self.particles, self.dim))\n        self.personal_best_position = np.copy(self.position)\n        self.global_best_position = None\n        self.personal_best_value = np.full(self.particles, np.inf)\n        self.global_best_value = np.inf\n        self.iteration = 0\n\n    def __call__(self, func):\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        scale = ub - lb\n\n        def quantum_update():\n            for i in range(self.particles):\n                delta = np.random.uniform(-1, 1, self.dim)\n                potential_position = self.position[i] + self.velocity[i] * delta\n                potential_position = np.clip(potential_position, 0, 1)\n                real_position = lb + potential_position * scale\n                value = func(real_position)\n                if value < self.personal_best_value[i]:\n                    self.personal_best_value[i] = value\n                    self.personal_best_position[i] = potential_position\n                if value < self.global_best_value:\n                    self.global_best_value = value\n                    self.global_best_position = potential_position\n\n        while self.iteration < self.budget:\n            quantum_update()\n            for i in range(self.particles):\n                cognitive = np.random.random(self.dim)\n                social = np.random.random(self.dim)\n                self.velocity[i] = (0.4 * self.velocity[i] +  # Modified line\n                                    cognitive * (self.personal_best_position[i] - self.position[i]) +\n                                    social * (self.global_best_position - self.position[i]))\n                self.position[i] += 0.9 * self.velocity[i]  # Adaptive velocity scaling\n                self.position[i] = np.clip(self.position[i], 0, 1)\n            self.iteration += self.particles\n\n        return lb + self.global_best_position * scale", "name": "QIPSO", "description": "Enhanced EQIPSO with improved cognitive-swarm balance by adjusting the weight factor.", "configspace": "", "generation": 23, "fitness": 0.2877520651631036, "feedback": "The algorithm QIPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.29 with standard deviation 0.00.", "error": "", "parent_id": "bb88ff32-4df5-4f23-933c-6f6937e2d8e4", "metadata": {"aucs": [0.2877520651631036]}, "mutation_prompt": null}
{"id": "8df5dbf8-c483-451e-bfb1-c6494dc8638e", "solution": "import numpy as np\n\nclass QIPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particles = 55  # Number of quantum particles increased from 50 to 55\n        self.position = np.random.uniform(0, 1, (self.particles, self.dim))\n        self.velocity = np.zeros((self.particles, self.dim))\n        self.personal_best_position = np.copy(self.position)\n        self.global_best_position = None\n        self.personal_best_value = np.full(self.particles, np.inf)\n        self.global_best_value = np.inf\n        self.iteration = 0\n\n    def __call__(self, func):\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        scale = ub - lb\n\n        def quantum_update():\n            for i in range(self.particles):\n                delta = np.random.uniform(-1, 1, self.dim)\n                potential_position = self.position[i] + self.velocity[i] * delta\n                potential_position = np.clip(potential_position, 0, 1)\n                real_position = lb + potential_position * scale\n                value = func(real_position)\n                if value < self.personal_best_value[i]:\n                    self.personal_best_value[i] = value\n                    self.personal_best_position[i] = potential_position\n                if value < self.global_best_value:\n                    self.global_best_value = value\n                    self.global_best_position = potential_position\n\n        while self.iteration < self.budget:\n            quantum_update()\n            for i in range(self.particles):\n                cognitive = np.random.random(self.dim)\n                social = np.random.random(self.dim)\n                self.velocity[i] = (0.5 * self.velocity[i] +\n                                    cognitive * (self.personal_best_position[i] - self.position[i]) +\n                                    social * (self.global_best_position - self.position[i]))\n                self.position[i] += 0.9 * self.velocity[i]  # Adaptive velocity scaling\n                self.position[i] = np.clip(self.position[i], 0, 1)\n            self.iteration += self.particles\n\n        return lb + self.global_best_position * scale", "name": "QIPSO", "description": "Enhanced Quantum-Inspired Particle Swarm Optimization (EQIPSO) integrates adaptive velocity scaling for improved convergence, with increased particle count for greater exploration.", "configspace": "", "generation": 24, "fitness": 0.2867702374785296, "feedback": "The algorithm QIPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.29 with standard deviation 0.00.", "error": "", "parent_id": "bb88ff32-4df5-4f23-933c-6f6937e2d8e4", "metadata": {"aucs": [0.2867702374785296]}, "mutation_prompt": null}
{"id": "814325f1-3a96-4d30-8114-cdb13ff3e3d8", "solution": "import numpy as np\n\nclass QIPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particles = 50  # Number of quantum particles\n        self.position = np.random.uniform(0, 1, (self.particles, self.dim))\n        self.velocity = np.zeros((self.particles, self.dim))\n        self.personal_best_position = np.copy(self.position)\n        self.global_best_position = None\n        self.personal_best_value = np.full(self.particles, np.inf)\n        self.global_best_value = np.inf\n        self.iteration = 0\n\n    def __call__(self, func):\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        scale = ub - lb\n\n        def quantum_update():\n            for i in range(self.particles):\n                delta = np.random.uniform(-1, 1, self.dim)\n                potential_position = self.position[i] + self.velocity[i] * delta\n                potential_position = np.clip(potential_position, 0, 1)\n                real_position = lb + potential_position * scale\n                value = func(real_position)\n                if value < self.personal_best_value[i]:\n                    self.personal_best_value[i] = value\n                    self.personal_best_position[i] = potential_position\n                if value < self.global_best_value:\n                    self.global_best_value = value\n                    self.global_best_position = potential_position\n\n        while self.iteration < self.budget:\n            quantum_update()\n            for i in range(self.particles):\n                cognitive = np.random.uniform(0.5, 1.5, self.dim)  # Changed line: stochastic cognitive component\n                social = np.random.uniform(0.5, 1.5, self.dim)     # Changed line: stochastic social component\n                self.velocity[i] = (0.5 * self.velocity[i] +\n                                    cognitive * (self.personal_best_position[i] - self.position[i]) +\n                                    social * (self.global_best_position - self.position[i]))\n                self.position[i] += 0.9 * self.velocity[i]  # Adaptive velocity scaling\n                self.position[i] = np.clip(self.position[i], 0, 1)\n            self.iteration += self.particles\n\n        return lb + self.global_best_position * scale", "name": "QIPSO", "description": "Improved EQIPSO with stochastic cognitive and social parameters for enhanced diversity.", "configspace": "", "generation": 25, "fitness": 0.28725188022436354, "feedback": "The algorithm QIPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.29 with standard deviation 0.00.", "error": "", "parent_id": "bb88ff32-4df5-4f23-933c-6f6937e2d8e4", "metadata": {"aucs": [0.28725188022436354]}, "mutation_prompt": null}
{"id": "f37ad111-c51b-4fed-968c-8e259b8b9aa4", "solution": "import numpy as np\n\nclass QIPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particles = 50  # Number of quantum particles\n        self.position = np.random.uniform(0, 1, (self.particles, self.dim))\n        self.velocity = np.zeros((self.particles, self.dim))\n        self.personal_best_position = np.copy(self.position)\n        self.global_best_position = None\n        self.personal_best_value = np.full(self.particles, np.inf)\n        self.global_best_value = np.inf\n        self.iteration = 0\n\n    def __call__(self, func):\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        scale = ub - lb\n\n        def quantum_update():\n            for i in range(self.particles):\n                delta = np.random.uniform(-1, 1, self.dim)\n                potential_position = self.position[i] + self.velocity[i] * delta\n                potential_position = np.clip(potential_position, 0, 1)\n                real_position = lb + potential_position * scale\n                value = func(real_position)\n                if value < self.personal_best_value[i]:\n                    self.personal_best_value[i] = value\n                    self.personal_best_position[i] = potential_position\n                if value < self.global_best_value:\n                    self.global_best_value = value\n                    self.global_best_position = potential_position\n\n        while self.iteration < self.budget:\n            quantum_update()\n            inertia_weight = 0.9 - (0.4 * self.iteration / self.budget)  # Dynamic inertia weight\n            for i in range(self.particles):\n                cognitive = np.random.random(self.dim)\n                social = np.random.random(self.dim)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive * (self.personal_best_position[i] - self.position[i]) +\n                                    social * (self.global_best_position - self.position[i]))\n                self.position[i] += 0.9 * self.velocity[i]  # Adaptive velocity scaling\n                self.position[i] = np.clip(self.position[i], 0, 1)\n            self.iteration += self.particles\n\n        return lb + self.global_best_position * scale", "name": "QIPSO", "description": "Enhanced Quantum-Inspired Particle Swarm Optimization (EQIPSO) with dynamic inertia weight for improved balance between exploration and exploitation.", "configspace": "", "generation": 26, "fitness": 0.2857357541300547, "feedback": "The algorithm QIPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.29 with standard deviation 0.00.", "error": "", "parent_id": "bb88ff32-4df5-4f23-933c-6f6937e2d8e4", "metadata": {"aucs": [0.2857357541300547]}, "mutation_prompt": null}
{"id": "32615095-2243-4ff0-a411-6f66f2e01c88", "solution": "import numpy as np\n\nclass QIPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particles = 50  # Number of quantum particles\n        self.position = np.random.uniform(0, 1, (self.particles, self.dim))\n        self.velocity = np.zeros((self.particles, self.dim))\n        self.personal_best_position = np.copy(self.position)\n        self.global_best_position = None\n        self.personal_best_value = np.full(self.particles, np.inf)\n        self.global_best_value = np.inf\n        self.iteration = 0\n\n    def __call__(self, func):\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        scale = ub - lb\n\n        def quantum_update():\n            for i in range(self.particles):\n                delta = np.random.uniform(-1, 1, self.dim)\n                potential_position = self.position[i] + self.velocity[i] * delta\n                potential_position = np.clip(potential_position, 0, 1)\n                real_position = lb + potential_position * scale\n                value = func(real_position)\n                if value < self.personal_best_value[i]:\n                    self.personal_best_value[i] = value\n                    self.personal_best_position[i] = potential_position\n                if value < self.global_best_value:\n                    self.global_best_value = value\n                    self.global_best_position = potential_position\n\n        while self.iteration < self.budget:\n            quantum_update()\n            for i in range(self.particles):\n                cognitive = np.random.random(self.dim)\n                social = np.random.random(self.dim)\n                self.velocity[i] = (0.6 * self.velocity[i] +  # Changed from 0.5 to 0.6\n                                    cognitive * (self.personal_best_position[i] - self.position[i]) +\n                                    social * (self.global_best_position - self.position[i]))\n                self.position[i] += 0.9 * self.velocity[i]  # Adaptive velocity scaling\n                self.position[i] = np.clip(self.position[i], 0, 1)\n            self.iteration += self.particles\n\n        return lb + self.global_best_position * scale", "name": "QIPSO", "description": "Improved velocity update strategy in EQIPSO for enhanced exploration and exploitation balance.", "configspace": "", "generation": 27, "fitness": 0.2879700821301593, "feedback": "The algorithm QIPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.29 with standard deviation 0.00.", "error": "", "parent_id": "bb88ff32-4df5-4f23-933c-6f6937e2d8e4", "metadata": {"aucs": [0.2879700821301593]}, "mutation_prompt": null}
{"id": "70f68be6-728c-4190-a215-78638bb4bbd8", "solution": "import numpy as np\n\nclass QIPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particles = 50  # Number of quantum particles\n        self.position = np.random.uniform(0, 1, (self.particles, self.dim))\n        self.velocity = np.zeros((self.particles, self.dim))\n        self.personal_best_position = np.copy(self.position)\n        self.global_best_position = None\n        self.personal_best_value = np.full(self.particles, np.inf)\n        self.global_best_value = np.inf\n        self.iteration = 0\n\n    def __call__(self, func):\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        scale = ub - lb\n\n        def quantum_update():\n            for i in range(self.particles):\n                delta = np.random.uniform(-1, 1, self.dim)\n                potential_position = self.position[i] + self.velocity[i] * delta\n                potential_position = np.clip(potential_position, 0, 1)\n                real_position = lb + potential_position * scale\n                value = func(real_position)\n                if value < self.personal_best_value[i]:\n                    self.personal_best_value[i] = value\n                    self.personal_best_position[i] = potential_position\n                if value < self.global_best_value:\n                    self.global_best_value = value\n                    self.global_best_position = potential_position\n\n        while self.iteration < self.budget:\n            quantum_update()\n            for i in range(self.particles):\n                cognitive = np.random.random(self.dim)\n                social = np.random.random(self.dim)\n                interaction_factor = 0.7  # Changed line: Enhanced interaction factor\n                self.velocity[i] = (0.5 * self.velocity[i] +\n                                    cognitive * (self.personal_best_position[i] - self.position[i]) +\n                                    interaction_factor * social * (self.global_best_position - self.position[i]))\n                self.position[i] += 0.9 * self.velocity[i]  # Adaptive velocity scaling\n                self.position[i] = np.clip(self.position[i], 0, 1)\n            self.iteration += self.particles\n\n        return lb + self.global_best_position * scale", "name": "QIPSO", "description": "Enhanced interaction between cognitive and social components to improve convergence in EQIPSO.", "configspace": "", "generation": 28, "fitness": 0.2884769924231978, "feedback": "The algorithm QIPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.29 with standard deviation 0.00.", "error": "", "parent_id": "bb88ff32-4df5-4f23-933c-6f6937e2d8e4", "metadata": {"aucs": [0.2884769924231978]}, "mutation_prompt": null}
{"id": "cb882c58-0d61-4118-82c7-5f3ab5db7f09", "solution": "import numpy as np\n\nclass QIPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particles = 50  # Number of quantum particles\n        self.position = np.random.uniform(0, 1, (self.particles, self.dim))\n        self.velocity = np.zeros((self.particles, self.dim))\n        self.personal_best_position = np.copy(self.position)\n        self.global_best_position = None\n        self.personal_best_value = np.full(self.particles, np.inf)\n        self.global_best_value = np.inf\n        self.iteration = 0\n\n    def __call__(self, func):\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        scale = ub - lb\n\n        def quantum_update():\n            for i in range(self.particles):\n                delta = np.random.uniform(-1, 1, self.dim)\n                potential_position = self.position[i] + self.velocity[i] * delta\n                potential_position = np.clip(potential_position, 0, 1)\n                real_position = lb + potential_position * scale\n                value = func(real_position)\n                if value < self.personal_best_value[i]:\n                    self.personal_best_value[i] = value\n                    self.personal_best_position[i] = potential_position\n                if value < self.global_best_value:\n                    self.global_best_value = value\n                    self.global_best_position = potential_position\n\n        while self.iteration < self.budget:\n            quantum_update()\n            for i in range(self.particles):\n                cognitive = np.random.random(self.dim)\n                social = np.random.random(self.dim)\n                self.velocity[i] = (0.7 * self.velocity[i] +  # Adjusted inertia weight\n                                    cognitive * (self.personal_best_position[i] - self.position[i]) +\n                                    social * (self.global_best_position - self.position[i]))\n                self.position[i] += 0.9 * self.velocity[i]  # Adaptive velocity scaling\n                self.position[i] = np.clip(self.position[i], 0, 1)\n            self.iteration += self.particles\n\n        return lb + self.global_best_position * scale", "name": "QIPSO", "description": "Refined EQIPSO with calibrated inertia weight for enhanced exploration and convergence.", "configspace": "", "generation": 29, "fitness": 0.2859296199244209, "feedback": "The algorithm QIPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.29 with standard deviation 0.00.", "error": "", "parent_id": "bb88ff32-4df5-4f23-933c-6f6937e2d8e4", "metadata": {"aucs": [0.2859296199244209]}, "mutation_prompt": null}
{"id": "91a6df14-7e4a-4619-be84-d22c7614b15e", "solution": "import numpy as np\n\nclass QIPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particles = 50  # Number of quantum particles\n        self.position = np.random.uniform(0, 1, (self.particles, self.dim))\n        self.velocity = np.zeros((self.particles, self.dim))\n        self.personal_best_position = np.copy(self.position)\n        self.global_best_position = None\n        self.personal_best_value = np.full(self.particles, np.inf)\n        self.global_best_value = np.inf\n        self.iteration = 0\n\n    def __call__(self, func):\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        scale = ub - lb\n\n        def quantum_update():\n            for i in range(self.particles):\n                delta = np.random.uniform(-1, 1, self.dim)\n                potential_position = self.position[i] + self.velocity[i] * delta\n                potential_position = np.clip(potential_position, 0, 1)\n                real_position = lb + potential_position * scale\n                value = func(real_position)\n                if value < self.personal_best_value[i]:\n                    self.personal_best_value[i] = value\n                    self.personal_best_position[i] = potential_position\n                if value < self.global_best_value:\n                    self.global_best_value = value\n                    self.global_best_position = potential_position\n\n        while self.iteration < self.budget:\n            quantum_update()\n            for i in range(self.particles):\n                cognitive = np.random.random(self.dim)\n                social = np.random.random(self.dim)\n                inertia_weight = 0.7  # Adding inertia weight for velocity calculation\n                self.velocity[i] = (inertia_weight * self.velocity[i] +  # Hybrid velocity update\n                                    cognitive * (self.personal_best_position[i] - self.position[i]) +\n                                    social * (self.global_best_position - self.position[i]))\n                self.position[i] += 0.9 * self.velocity[i]  # Adaptive velocity scaling\n                self.position[i] = np.clip(self.position[i], 0, 1)\n            self.iteration += self.particles\n\n        return lb + self.global_best_position * scale", "name": "QIPSO", "description": "Enhanced Quantum-Inspired Particle Swarm Optimization (EQIPSO) with hybrid velocity update rule for better exploration and exploitation balance.", "configspace": "", "generation": 30, "fitness": 0.2859296199244209, "feedback": "The algorithm QIPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.29 with standard deviation 0.00.", "error": "", "parent_id": "bb88ff32-4df5-4f23-933c-6f6937e2d8e4", "metadata": {"aucs": [0.2859296199244209]}, "mutation_prompt": null}
{"id": "184d8292-2c19-43c0-9700-2d16f2159b95", "solution": "import numpy as np\n\nclass QIPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particles = min(100, max(10, dim * 2))  # Dynamic particle count\n        self.position = np.random.uniform(0, 1, (self.particles, self.dim))\n        self.velocity = np.zeros((self.particles, self.dim))\n        self.personal_best_position = np.copy(self.position)\n        self.global_best_position = None\n        self.personal_best_value = np.full(self.particles, np.inf)\n        self.global_best_value = np.inf\n        self.iteration = 0\n\n    def __call__(self, func):\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        scale = ub - lb\n\n        def quantum_update():\n            for i in range(self.particles):\n                delta = np.random.uniform(-1, 1, self.dim)\n                potential_position = self.position[i] + self.velocity[i] * delta\n                potential_position = np.clip(potential_position, 0, 1)\n                real_position = lb + potential_position * scale\n                value = func(real_position)\n                if value < self.personal_best_value[i]:\n                    self.personal_best_value[i] = value\n                    self.personal_best_position[i] = potential_position\n                if value < self.global_best_value:\n                    self.global_best_value = value\n                    self.global_best_position = potential_position\n\n        while self.iteration < self.budget:\n            quantum_update()\n            for i in range(self.particles):\n                cognitive = np.random.random(self.dim)\n                social = np.random.random(self.dim)\n                self.velocity[i] = (0.5 * self.velocity[i] +\n                                    cognitive * (self.personal_best_position[i] - self.position[i]) +\n                                    social * (self.global_best_position - self.position[i]))\n                self.position[i] += 0.9 * self.velocity[i]  # Adaptive velocity scaling\n                self.position[i] = np.clip(self.position[i], 0, 1)\n            self.iteration += self.particles\n\n        return lb + self.global_best_position * scale", "name": "QIPSO", "description": "Improved Enhanced Quantum-Inspired Particle Swarm Optimization (IEQIPSO) with dynamic particle count adjustment for better exploration-exploitation balance.", "configspace": "", "generation": 31, "fitness": 0.2921440963152311, "feedback": "The algorithm QIPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.29 with standard deviation 0.00.", "error": "", "parent_id": "bb88ff32-4df5-4f23-933c-6f6937e2d8e4", "metadata": {"aucs": [0.2921440963152311]}, "mutation_prompt": null}
{"id": "5cc1eae1-65b3-49b2-a667-865b2a4d07b4", "solution": "import numpy as np\n\nclass QIPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particles = min(100, max(10, dim * 2))  # Dynamic particle count\n        self.position = np.random.uniform(0, 1, (self.particles, self.dim))\n        self.velocity = np.zeros((self.particles, self.dim))\n        self.personal_best_position = np.copy(self.position)\n        self.global_best_position = None\n        self.personal_best_value = np.full(self.particles, np.inf)\n        self.global_best_value = np.inf\n        self.iteration = 0\n\n    def __call__(self, func):\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        scale = ub - lb\n\n        def quantum_update():\n            for i in range(self.particles):\n                delta = np.random.uniform(-1, 1, self.dim)\n                potential_position = self.position[i] + self.velocity[i] * delta\n                potential_position = np.clip(potential_position, 0, 1)\n                real_position = lb + potential_position * scale\n                value = func(real_position)\n                if value < self.personal_best_value[i]:\n                    self.personal_best_value[i] = value\n                    self.personal_best_position[i] = potential_position\n                if value < self.global_best_value:\n                    self.global_best_value = value\n                    self.global_best_position = potential_position\n\n        while self.iteration < self.budget:\n            quantum_update()\n            for i in range(self.particles):\n                cognitive = np.random.random(self.dim)\n                social = np.random.random(self.dim) * 1.5  # Increase social influence\n                self.velocity[i] = (0.5 * self.velocity[i] +\n                                    cognitive * (self.personal_best_position[i] - self.position[i]) +\n                                    social * (self.global_best_position - self.position[i]))\n                self.position[i] += 0.9 * self.velocity[i]  # Adaptive velocity scaling\n                self.position[i] = np.clip(self.position[i], 0, 1)\n            self.iteration += self.particles\n\n        return lb + self.global_best_position * scale", "name": "QIPSO", "description": "Enhanced social component scaling in QIPSO to improve the convergence speed and accuracy.", "configspace": "", "generation": 32, "fitness": 0.29088425965136, "feedback": "The algorithm QIPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.29 with standard deviation 0.00.", "error": "", "parent_id": "184d8292-2c19-43c0-9700-2d16f2159b95", "metadata": {"aucs": [0.29088425965136]}, "mutation_prompt": null}
{"id": "d730bd02-62be-40cb-b13f-94bead5ce14a", "solution": "import numpy as np\n\nclass QIPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particles = min(100, max(10, dim * 2))  # Dynamic particle count\n        self.position = np.random.uniform(0, 1, (self.particles, self.dim))\n        self.velocity = np.zeros((self.particles, self.dim))\n        self.personal_best_position = np.copy(self.position)\n        self.global_best_position = None\n        self.personal_best_value = np.full(self.particles, np.inf)\n        self.global_best_value = np.inf\n        self.iteration = 0\n\n    def __call__(self, func):\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        scale = ub - lb\n\n        def quantum_update():\n            for i in range(self.particles):\n                delta = np.random.uniform(-1, 1, self.dim)\n                potential_position = self.position[i] + self.velocity[i] * delta\n                potential_position = np.clip(potential_position, 0, 1)\n                real_position = lb + potential_position * scale\n                value = func(real_position)\n                if value < self.personal_best_value[i]:\n                    self.personal_best_value[i] = value\n                    self.personal_best_position[i] = potential_position\n                if value < self.global_best_value:\n                    self.global_best_value = value\n                    self.global_best_position = potential_position\n\n        while self.iteration < self.budget:\n            quantum_update()\n            for i in range(self.particles):\n                cognitive = np.random.random(self.dim)\n                social = np.random.random(self.dim)\n                inertia_weight = 0.9 - (0.7 * (self.iteration / self.budget))  # Adaptive inertia weight\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive * (self.personal_best_position[i] - self.position[i]) +\n                                    social * (self.global_best_position - self.position[i]))\n                self.position[i] += 0.9 * self.velocity[i]  # Adaptive velocity scaling\n                self.position[i] = np.clip(self.position[i], 0, 1)\n            self.iteration += self.particles\n\n        return lb + self.global_best_position * scale", "name": "QIPSO", "description": "Enhanced IEQIPSO with adaptive inertia weight for improved exploration-exploitation trade-off.", "configspace": "", "generation": 33, "fitness": 0.2889305893160351, "feedback": "The algorithm QIPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.29 with standard deviation 0.00.", "error": "", "parent_id": "184d8292-2c19-43c0-9700-2d16f2159b95", "metadata": {"aucs": [0.2889305893160351]}, "mutation_prompt": null}
{"id": "a5ddb84c-5523-4a26-addd-065f0a513228", "solution": "import numpy as np\n\nclass QIPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particles = min(100, max(10, dim * 2))  # Dynamic particle count\n        self.position = np.random.uniform(0, 1, (self.particles, self.dim))\n        self.velocity = np.zeros((self.particles, self.dim))\n        self.personal_best_position = np.copy(self.position)\n        self.global_best_position = None\n        self.personal_best_value = np.full(self.particles, np.inf)\n        self.global_best_value = np.inf\n        self.iteration = 0\n\n    def __call__(self, func):\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        scale = ub - lb\n\n        def quantum_update():\n            for i in range(self.particles):\n                delta = np.random.uniform(-1, 1, self.dim)\n                potential_position = self.position[i] + self.velocity[i] * delta\n                potential_position = np.clip(potential_position, 0, 1)\n                real_position = lb + potential_position * scale\n                value = func(real_position)\n                if value < self.personal_best_value[i]:\n                    self.personal_best_value[i] = value\n                    self.personal_best_position[i] = potential_position\n                if value < self.global_best_value:\n                    self.global_best_value = value\n                    self.global_best_position = potential_position\n\n        while self.iteration < self.budget:\n            quantum_update()\n            for i in range(self.particles):\n                cognitive = np.random.random(self.dim)\n                social = np.random.random(self.dim)\n                self.velocity[i] = (0.5 * self.velocity[i] + \n                                    cognitive * (self.personal_best_position[i] - self.position[i]) +\n                                    social * (self.global_best_position - self.position[i]))\n                self.position[i] += 0.95 * self.velocity[i]  # Adjusted momentum coefficient\n                self.position[i] = np.clip(self.position[i], 0, 1)\n            self.iteration += self.particles\n\n        return lb + self.global_best_position * scale", "name": "QIPSO", "description": "Enhanced velocity scaling by adjusting momentum coefficient for better convergence.", "configspace": "", "generation": 34, "fitness": 0.29173289706942684, "feedback": "The algorithm QIPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.29 with standard deviation 0.00.", "error": "", "parent_id": "184d8292-2c19-43c0-9700-2d16f2159b95", "metadata": {"aucs": [0.29173289706942684]}, "mutation_prompt": null}
{"id": "a40b1610-e7e8-4c4e-8a0c-e8fd4e70946a", "solution": "import numpy as np\n\nclass QIPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particles = min(100, max(10, dim * 2))  # Dynamic particle count\n        self.position = np.random.uniform(0, 1, (self.particles, self.dim))\n        self.velocity = np.zeros((self.particles, self.dim))\n        self.personal_best_position = np.copy(self.position)\n        self.global_best_position = None\n        self.personal_best_value = np.full(self.particles, np.inf)\n        self.global_best_value = np.inf\n        self.iteration = 0\n\n    def __call__(self, func):\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        scale = ub - lb\n\n        def quantum_update():\n            for i in range(self.particles):\n                delta = np.random.uniform(-1, 1, self.dim)\n                potential_position = self.position[i] + self.velocity[i] * delta\n                potential_position = np.clip(potential_position, 0, 1)\n                real_position = lb + potential_position * scale\n                value = func(real_position)\n                if value < self.personal_best_value[i]:\n                    self.personal_best_value[i] = value\n                    self.personal_best_position[i] = potential_position\n                if value < self.global_best_value:\n                    self.global_best_value = value\n                    self.global_best_position = potential_position\n\n        while self.iteration < self.budget:\n            quantum_update()\n            inertia_weight = 0.9 - (0.5 * self.iteration / self.budget)  # Adaptive inertia weight\n            for i in range(self.particles):\n                cognitive = np.random.random(self.dim)\n                social = np.random.random(self.dim)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive * (self.personal_best_position[i] - self.position[i]) +\n                                    social * (self.global_best_position - self.position[i]))\n                self.position[i] += 0.9 * self.velocity[i]  # Adaptive velocity scaling\n                self.position[i] = np.clip(self.position[i], 0, 1)\n            self.iteration += self.particles\n\n        return lb + self.global_best_position * scale", "name": "QIPSO", "description": "Incorporate adaptive inertia weight in quantum-inspired PSO to enhance convergence speed and solution quality.", "configspace": "", "generation": 35, "fitness": 0.28901388415293616, "feedback": "The algorithm QIPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.29 with standard deviation 0.00.", "error": "", "parent_id": "184d8292-2c19-43c0-9700-2d16f2159b95", "metadata": {"aucs": [0.28901388415293616]}, "mutation_prompt": null}
{"id": "aa15ff29-751a-47ae-beb1-cb1b57e4e727", "solution": "import numpy as np\n\nclass QIPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particles = min(100, max(10, dim * 2))  # Dynamic particle count\n        self.position = np.random.uniform(0, 1, (self.particles, self.dim))\n        self.velocity = np.zeros((self.particles, self.dim))\n        self.personal_best_position = np.copy(self.position)\n        self.global_best_position = None\n        self.personal_best_value = np.full(self.particles, np.inf)\n        self.global_best_value = np.inf\n        self.iteration = 0\n\n    def __call__(self, func):\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        scale = ub - lb\n\n        def quantum_update():\n            for i in range(self.particles):\n                delta = np.random.uniform(-1, 1, self.dim)\n                potential_position = self.position[i] + self.velocity[i] * delta\n                potential_position = np.clip(potential_position, 0, 1)\n                real_position = lb + potential_position * scale\n                value = func(real_position)\n                if value < self.personal_best_value[i]:\n                    self.personal_best_value[i] = value\n                    self.personal_best_position[i] = potential_position\n                if value < self.global_best_value:\n                    self.global_best_value = value\n                    self.global_best_position = potential_position\n\n        while self.iteration < self.budget:\n            quantum_update()\n            for i in range(self.particles):\n                cognitive = np.random.random(self.dim)\n                social = np.random.random(self.dim)\n                self.velocity[i] = (0.5 * self.velocity[i] +\n                                    cognitive * (self.personal_best_position[i] - self.position[i]) +\n                                    social * (self.global_best_position - self.position[i]))\n                self.position[i] += 1.1 * self.velocity[i]  # Enhanced velocity scaling\n                self.position[i] = np.clip(self.position[i], 0, 1)\n            self.iteration += self.particles\n\n        return lb + self.global_best_position * scale", "name": "QIPSO", "description": "Enhanced dynamic scaling of velocities to boost convergence efficiency in IEQIPSO.", "configspace": "", "generation": 36, "fitness": 0.2920594611410712, "feedback": "The algorithm QIPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.29 with standard deviation 0.00.", "error": "", "parent_id": "184d8292-2c19-43c0-9700-2d16f2159b95", "metadata": {"aucs": [0.2920594611410712]}, "mutation_prompt": null}
{"id": "f9fca765-4631-463f-b0cd-4f6befb7013e", "solution": "import numpy as np\n\nclass QIPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particles = min(100, max(10, dim * 2))  # Dynamic particle count\n        self.position = np.random.uniform(0, 1, (self.particles, self.dim))\n        self.velocity = np.zeros((self.particles, self.dim))\n        self.personal_best_position = np.copy(self.position)\n        self.global_best_position = None\n        self.personal_best_value = np.full(self.particles, np.inf)\n        self.global_best_value = np.inf\n        self.iteration = 0\n\n    def __call__(self, func):\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        scale = ub - lb\n\n        def quantum_update():\n            for i in range(self.particles):\n                delta = np.random.normal(0, 1, self.dim)  # Changed from uniform to normal distribution\n                potential_position = self.position[i] + self.velocity[i] * delta\n                potential_position = np.clip(potential_position, 0, 1)\n                real_position = lb + potential_position * scale\n                value = func(real_position)\n                if value < self.personal_best_value[i]:\n                    self.personal_best_value[i] = value\n                    self.personal_best_position[i] = potential_position\n                if value < self.global_best_value:\n                    self.global_best_value = value\n                    self.global_best_position = potential_position\n\n        while self.iteration < self.budget:\n            quantum_update()\n            for i in range(self.particles):\n                cognitive = np.random.random(self.dim)\n                social = np.random.random(self.dim)\n                self.velocity[i] = (0.5 * self.velocity[i] +\n                                    cognitive * (self.personal_best_position[i] - self.position[i]) +\n                                    social * (self.global_best_position - self.position[i]))\n                self.position[i] += 0.9 * self.velocity[i]  # Adaptive velocity scaling\n                self.position[i] = np.clip(self.position[i], 0, 1)\n            self.iteration += self.particles\n\n        return lb + self.global_best_position * scale", "name": "QIPSO", "description": "Enhanced Quantum-Inspired PSO with improved quantum update mechanism for better convergence.", "configspace": "", "generation": 37, "fitness": 0.29032579986979257, "feedback": "The algorithm QIPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.29 with standard deviation 0.00.", "error": "", "parent_id": "184d8292-2c19-43c0-9700-2d16f2159b95", "metadata": {"aucs": [0.29032579986979257]}, "mutation_prompt": null}
{"id": "f41962d8-9954-4d0a-83f7-8d57fd6f7970", "solution": "import numpy as np\n\nclass QIPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particles = min(100, max(10, dim * 2))  # Dynamic particle count\n        self.position = np.random.uniform(0, 1, (self.particles, self.dim))\n        self.velocity = np.zeros((self.particles, self.dim))\n        self.personal_best_position = np.copy(self.position)\n        self.global_best_position = None\n        self.personal_best_value = np.full(self.particles, np.inf)\n        self.global_best_value = np.inf\n        self.iteration = 0\n\n    def __call__(self, func):\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        scale = ub - lb\n\n        def quantum_update():\n            for i in range(self.particles):\n                delta = np.random.uniform(-1, 1, self.dim)\n                potential_position = self.position[i] + self.velocity[i] * delta\n                potential_position = np.clip(potential_position, 0, 1)\n                real_position = lb + potential_position * scale\n                value = func(real_position)\n                if value < self.personal_best_value[i]:\n                    self.personal_best_value[i] = value\n                    self.personal_best_position[i] = potential_position\n                if value < self.global_best_value:\n                    self.global_best_value = value\n                    self.global_best_position = potential_position\n\n        while self.iteration < self.budget:\n            quantum_update()\n            inertia_weight = 0.9 - (0.5 * self.iteration / self.budget)  # Dynamic inertia weight\n            for i in range(self.particles):\n                cognitive = np.random.random(self.dim)\n                social = np.random.random(self.dim)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive * (self.personal_best_position[i] - self.position[i]) +\n                                    social * (self.global_best_position - self.position[i]))\n                self.position[i] += 0.9 * self.velocity[i]  # Adaptive velocity scaling\n                self.position[i] = np.clip(self.position[i], 0, 1)\n            self.iteration += self.particles\n\n        return lb + self.global_best_position * scale", "name": "QIPSO", "description": "Enhanced Quantum-Inspired Particle Swarm Optimization with dynamic inertia weighting for improved convergence stability.", "configspace": "", "generation": 38, "fitness": 0.28901388415293616, "feedback": "The algorithm QIPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.29 with standard deviation 0.00.", "error": "", "parent_id": "184d8292-2c19-43c0-9700-2d16f2159b95", "metadata": {"aucs": [0.28901388415293616]}, "mutation_prompt": null}
{"id": "fead7981-6726-489a-ac4d-56ae5ce8bdf8", "solution": "import numpy as np\n\nclass QIPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particles = min(100, max(10, dim * 2))  # Dynamic particle count\n        self.position = np.random.uniform(0, 1, (self.particles, self.dim))\n        self.velocity = np.zeros((self.particles, self.dim))\n        self.personal_best_position = np.copy(self.position)\n        self.global_best_position = None\n        self.personal_best_value = np.full(self.particles, np.inf)\n        self.global_best_value = np.inf\n        self.iteration = 0\n\n    def __call__(self, func):\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        scale = ub - lb\n\n        def quantum_update():\n            for i in range(self.particles):\n                delta = np.random.uniform(-1, 1, self.dim)\n                potential_position = self.position[i] + self.velocity[i] * delta\n                potential_position = np.clip(potential_position, 0, 1)\n                real_position = lb + potential_position * scale\n                value = func(real_position)\n                if value < self.personal_best_value[i]:\n                    self.personal_best_value[i] = value\n                    self.personal_best_position[i] = potential_position\n                if value < self.global_best_value:\n                    self.global_best_value = value\n                    self.global_best_position = potential_position\n\n        while self.iteration < self.budget:\n            quantum_update()\n            for i in range(self.particles):\n                cognitive = np.random.random(self.dim)\n                social = np.random.random(self.dim)\n                self.velocity[i] = (0.5 * self.velocity[i] +\n                                    cognitive * (self.personal_best_position[i] - self.position[i]) +\n                                    social * (self.global_best_position - self.position[i]) +\n                                    np.random.normal(0, 0.01, self.dim))  # Added random perturbation\n                self.position[i] += 0.9 * self.velocity[i]  # Adaptive velocity scaling\n                self.position[i] = np.clip(self.position[i], 0, 1)\n            self.iteration += self.particles\n\n        return lb + self.global_best_position * scale", "name": "QIPSO", "description": "Enhanced search space exploration by introducing small random perturbations in particle velocities.", "configspace": "", "generation": 39, "fitness": 0.28396164617025876, "feedback": "The algorithm QIPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28 with standard deviation 0.00.", "error": "", "parent_id": "184d8292-2c19-43c0-9700-2d16f2159b95", "metadata": {"aucs": [0.28396164617025876]}, "mutation_prompt": null}
{"id": "2c15660a-4246-41e7-ab0d-d3e9b9bb229d", "solution": "import numpy as np\n\nclass QIPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particles = min(100, max(10, dim * 2))\n        self.position = np.random.uniform(0, 1, (self.particles, self.dim))\n        self.velocity = np.zeros((self.particles, self.dim))\n        self.personal_best_position = np.copy(self.position)\n        self.global_best_position = None\n        self.personal_best_value = np.full(self.particles, np.inf)\n        self.global_best_value = np.inf\n        self.iteration = 0\n        self.inertia_weight = 0.9  # Initial inertia weight\n\n    def __call__(self, func):\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        scale = ub - lb\n\n        def quantum_update():\n            for i in range(self.particles):\n                delta = np.random.uniform(-1, 1, self.dim)\n                potential_position = self.position[i] + self.velocity[i] * delta\n                potential_position = np.clip(potential_position, 0, 1)\n                real_position = lb + potential_position * scale\n                value = func(real_position)\n                if value < self.personal_best_value[i]:\n                    self.personal_best_value[i] = value\n                    self.personal_best_position[i] = potential_position\n                if value < self.global_best_value:\n                    self.global_best_value = value\n                    self.global_best_position = potential_position\n\n        while self.iteration < self.budget:\n            quantum_update()\n            for i in range(self.particles):\n                cognitive = np.random.random(self.dim)\n                social = np.random.random(self.dim)\n                self.velocity[i] = (self.inertia_weight * self.velocity[i] +  # Adaptive inertia\n                                    cognitive * (self.personal_best_position[i] - self.position[i]) +\n                                    social * (self.global_best_position - self.position[i]))\n                self.position[i] += 0.9 * self.velocity[i]\n                self.position[i] = np.clip(self.position[i], 0, 1)\n                # Local search perturbation\n                if np.random.random() < 0.1:\n                    self.position[i] += np.random.uniform(-0.05, 0.05, self.dim)\n                    self.position[i] = np.clip(self.position[i], 0, 1)\n            self.inertia_weight *= 0.99  # Decrease inertia weight\n            self.iteration += self.particles\n            \n        return lb + self.global_best_position * scale", "name": "QIPSO", "description": "Enhanced Quantum-Inspired Particle Swarm Optimization (EQIPSO) using adaptive inertia weight and local search perturbations for improved convergence.", "configspace": "", "generation": 40, "fitness": 0.28811267663244133, "feedback": "The algorithm QIPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.29 with standard deviation 0.00.", "error": "", "parent_id": "184d8292-2c19-43c0-9700-2d16f2159b95", "metadata": {"aucs": [0.28811267663244133]}, "mutation_prompt": null}
{"id": "4470783e-5035-45d7-b270-49a7fba7e7ac", "solution": "import numpy as np\n\nclass QIPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particles = min(100, max(10, dim * 2))  # Dynamic particle count\n        self.position = np.random.uniform(0, 1, (self.particles, self.dim))\n        self.velocity = np.zeros((self.particles, self.dim))\n        self.personal_best_position = np.copy(self.position)\n        self.global_best_position = None\n        self.personal_best_value = np.full(self.particles, np.inf)\n        self.global_best_value = np.inf\n        self.iteration = 0\n\n    def __call__(self, func):\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        scale = ub - lb\n\n        def quantum_update():\n            for i in range(self.particles):\n                delta = np.random.uniform(-1, 1, self.dim)\n                potential_position = self.position[i] + self.velocity[i] * delta\n                potential_position = np.clip(potential_position, 0, 1)\n                real_position = lb + potential_position * scale\n                value = func(real_position)\n                if value < self.personal_best_value[i]:\n                    self.personal_best_value[i] = value\n                    self.personal_best_position[i] = potential_position\n                if value < self.global_best_value:\n                    self.global_best_value = value\n                    self.global_best_position = potential_position\n\n        while self.iteration < self.budget:\n            quantum_update()\n            decay = 0.99  # New decay factor for velocity\n            for i in range(self.particles):\n                cognitive = np.random.random(self.dim)\n                social = np.random.random(self.dim)\n                self.velocity[i] = (decay * self.velocity[i] +  # Changed line\n                                    cognitive * (self.personal_best_position[i] - self.position[i]) +\n                                    social * (self.global_best_position - self.position[i]))\n                self.position[i] += 0.9 * self.velocity[i]  # Adaptive velocity scaling\n                self.position[i] = np.clip(self.position[i], 0, 1)\n            self.iteration += self.particles\n\n        return lb + self.global_best_position * scale", "name": "QIPSO", "description": "Enhanced velocity update rule in IEQIPSO for better convergence by introducing a decay factor.", "configspace": "", "generation": 41, "fitness": 0.28520343558396566, "feedback": "The algorithm QIPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.29 with standard deviation 0.00.", "error": "", "parent_id": "184d8292-2c19-43c0-9700-2d16f2159b95", "metadata": {"aucs": [0.28520343558396566]}, "mutation_prompt": null}
{"id": "e8e95a05-07ad-4606-9832-6156cde584a1", "solution": "import numpy as np\n\nclass QIPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particles = min(100, max(10, dim * 2))  # Dynamic particle count\n        self.position = np.random.uniform(0, 1, (self.particles, self.dim))\n        self.velocity = np.zeros((self.particles, self.dim))\n        self.personal_best_position = np.copy(self.position)\n        self.global_best_position = None\n        self.personal_best_value = np.full(self.particles, np.inf)\n        self.global_best_value = np.inf\n        self.iteration = 0\n\n    def __call__(self, func):\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        scale = ub - lb\n\n        def quantum_update():\n            for i in range(self.particles):\n                delta = np.random.uniform(-1, 1, self.dim)\n                potential_position = self.position[i] + self.velocity[i] * delta\n                potential_position = np.clip(potential_position, 0, 1)\n                real_position = lb + potential_position * scale\n                value = func(real_position)\n                if value < self.personal_best_value[i]:\n                    self.personal_best_value[i] = value\n                    self.personal_best_position[i] = potential_position\n                if value < self.global_best_value:\n                    self.global_best_value = value\n                    self.global_best_position = potential_position\n\n        while self.iteration < self.budget:\n            quantum_update()\n            for i in range(self.particles):\n                cognitive = np.random.random(self.dim)\n                social = np.random.random(self.dim)\n                self.velocity[i] = (0.5 * self.velocity[i] +\n                                    cognitive * (self.personal_best_position[i] - self.position[i]) +\n                                    social * (self.global_best_position - self.position[i]))\n                self.position[i] += 1.0 * self.velocity[i]  # Enhanced velocity scaling\n                self.position[i] = np.clip(self.position[i], 0, 1)\n            self.iteration += self.particles\n\n        return lb + self.global_best_position * scale", "name": "QIPSO", "description": "Quantum-Inspired Particle Swarm Optimization with enhanced velocity scaling for improved convergence.", "configspace": "", "generation": 42, "fitness": 0.2911362774027022, "feedback": "The algorithm QIPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.29 with standard deviation 0.00.", "error": "", "parent_id": "184d8292-2c19-43c0-9700-2d16f2159b95", "metadata": {"aucs": [0.2911362774027022]}, "mutation_prompt": null}
{"id": "7117e5cb-a0b9-4587-a0a5-4b4b593089ed", "solution": "import numpy as np\n\nclass QIPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particles = min(100, max(10, dim * 2))  # Dynamic particle count\n        self.position = np.random.uniform(0, 1, (self.particles, self.dim))\n        self.velocity = np.zeros((self.particles, self.dim))\n        self.personal_best_position = np.copy(self.position)\n        self.global_best_position = None\n        self.personal_best_value = np.full(self.particles, np.inf)\n        self.global_best_value = np.inf\n        self.iteration = 0\n\n    def __call__(self, func):\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        scale = ub - lb\n\n        def quantum_update():\n            for i in range(self.particles):\n                delta = np.random.uniform(-1, 1, self.dim)\n                potential_position = self.position[i] + self.velocity[i] * delta\n                potential_position = np.clip(potential_position, 0, 1)\n                real_position = lb + potential_position * scale\n                value = func(real_position)\n                if value < self.personal_best_value[i]:\n                    self.personal_best_value[i] = value\n                    self.personal_best_position[i] = potential_position\n                if value < self.global_best_value:\n                    self.global_best_value = value\n                    self.global_best_position = potential_position\n\n        while self.iteration < self.budget:\n            quantum_update()\n            for i in range(self.particles):\n                cognitive = np.random.random(self.dim)\n                social = np.random.random(self.dim)\n                random_factor = np.random.uniform(0.8, 1.2, self.dim)\n                self.velocity[i] = (0.5 * self.velocity[i] +\n                                    cognitive * (self.personal_best_position[i] - self.position[i]) +\n                                    social * (self.global_best_position - self.position[i])) * random_factor\n                self.position[i] += 0.9 * self.velocity[i]  # Adaptive velocity scaling\n                self.position[i] = np.clip(self.position[i], 0, 1)\n            self.iteration += self.particles\n\n        return lb + self.global_best_position * scale", "name": "QIPSO", "description": "Introduced a random factor to the velocity update equation to enhance exploration capabilities.", "configspace": "", "generation": 43, "fitness": 0.29317792560156997, "feedback": "The algorithm QIPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.29 with standard deviation 0.00.", "error": "", "parent_id": "184d8292-2c19-43c0-9700-2d16f2159b95", "metadata": {"aucs": [0.29317792560156997]}, "mutation_prompt": null}
{"id": "b309b50c-87c2-4302-be03-1a72a9c85374", "solution": "import numpy as np\n\nclass QIPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particles = min(100, max(10, dim * 2))  # Dynamic particle count\n        self.position = np.random.uniform(0, 1, (self.particles, self.dim))\n        self.velocity = np.zeros((self.particles, self.dim))\n        self.personal_best_position = np.copy(self.position)\n        self.global_best_position = None\n        self.personal_best_value = np.full(self.particles, np.inf)\n        self.global_best_value = np.inf\n        self.iteration = 0\n        self.history_best_position = np.copy(self.position)  # New line to store past bests\n\n    def __call__(self, func):\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        scale = ub - lb\n\n        def quantum_update():\n            for i in range(self.particles):\n                delta = np.random.uniform(-1, 1, self.dim)\n                potential_position = self.position[i] + self.velocity[i] * delta\n                potential_position = np.clip(potential_position, 0, 1)\n                real_position = lb + potential_position * scale\n                value = func(real_position)\n                if value < self.personal_best_value[i]:\n                    self.personal_best_value[i] = value\n                    self.personal_best_position[i] = potential_position\n                if value < self.global_best_value:\n                    self.global_best_value = value\n                    self.global_best_position = potential_position\n                if value < func(self.history_best_position[i]):  # New condition to update history\n                    self.history_best_position[i] = potential_position\n\n        while self.iteration < self.budget:\n            quantum_update()\n            for i in range(self.particles):\n                cognitive = np.random.random(self.dim)\n                social = np.random.random(self.dim)\n                random_factor = np.random.uniform(0.8, 1.2, self.dim)\n                memory_influence = np.random.random(self.dim)  # New memory factor\n                self.velocity[i] = (0.5 * self.velocity[i] +\n                                    cognitive * (self.personal_best_position[i] - self.position[i]) +\n                                    social * (self.global_best_position - self.position[i]) +\n                                    memory_influence * (self.history_best_position[i] - self.position[i])) * random_factor\n                self.position[i] += 0.9 * self.velocity[i]  # Adaptive velocity scaling\n                self.position[i] = np.clip(self.position[i], 0, 1)\n            self.iteration += self.particles\n\n        return lb + self.global_best_position * scale", "name": "QIPSO", "description": "Introduced memory-based exploration by incorporating historical bests in velocity updates to improve convergence speed.", "configspace": "", "generation": 44, "fitness": 0.2891178908735693, "feedback": "The algorithm QIPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.29 with standard deviation 0.00.", "error": "", "parent_id": "7117e5cb-a0b9-4587-a0a5-4b4b593089ed", "metadata": {"aucs": [0.2891178908735693]}, "mutation_prompt": null}
{"id": "83bc2f46-dc9c-4ad1-83e2-5d400cb61570", "solution": "import numpy as np\n\nclass QIPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particles = min(100, max(10, dim * 2))  # Dynamic particle count\n        self.position = np.random.uniform(0, 1, (self.particles, self.dim))\n        self.velocity = np.zeros((self.particles, self.dim))\n        self.personal_best_position = np.copy(self.position)\n        self.global_best_position = None\n        self.personal_best_value = np.full(self.particles, np.inf)\n        self.global_best_value = np.inf\n        self.iteration = 0\n\n    def __call__(self, func):\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        scale = ub - lb\n\n        def quantum_update():\n            for i in range(self.particles):\n                delta = np.random.uniform(-1, 1, self.dim)\n                potential_position = self.position[i] + self.velocity[i] * delta\n                potential_position = np.clip(potential_position, 0, 1)\n                real_position = lb + potential_position * scale\n                value = func(real_position)\n                if value < self.personal_best_value[i]:\n                    self.personal_best_value[i] = value\n                    self.personal_best_position[i] = potential_position\n                if value < self.global_best_value:\n                    self.global_best_value = value\n                    self.global_best_position = potential_position\n\n        while self.iteration < self.budget:\n            quantum_update()\n            inertia_weight = 0.9 - (0.5 * self.iteration / self.budget)\n            for i in range(self.particles):\n                cognitive = np.random.random(self.dim)\n                social = np.random.random(self.dim)\n                random_factor = np.random.uniform(0.5, 1.5, self.dim) * (1 - self.iteration / self.budget)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive * (self.personal_best_position[i] - self.position[i]) +\n                                    social * (self.global_best_position - self.position[i])) * random_factor\n                self.position[i] += self.velocity[i]  # Adaptive velocity scaling\n                self.position[i] = np.clip(self.position[i], 0, 1)\n            self.iteration += self.particles\n\n        return lb + self.global_best_position * scale", "name": "QIPSO", "description": "Enhanced velocity update by incorporating a dynamic inertia weight and time-varying random factor for better exploration and exploitation balance.", "configspace": "", "generation": 45, "fitness": 0.29107891768597816, "feedback": "The algorithm QIPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.29 with standard deviation 0.00.", "error": "", "parent_id": "7117e5cb-a0b9-4587-a0a5-4b4b593089ed", "metadata": {"aucs": [0.29107891768597816]}, "mutation_prompt": null}
{"id": "9dabbc5a-e5a9-4908-81a8-9a84aafb050a", "solution": "import numpy as np\n\nclass QIPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particles = min(100, max(10, dim * 2))  # Dynamic particle count\n        self.position = np.random.uniform(0, 1, (self.particles, self.dim))\n        self.velocity = np.zeros((self.particles, self.dim))\n        self.personal_best_position = np.copy(self.position)\n        self.global_best_position = None\n        self.personal_best_value = np.full(self.particles, np.inf)\n        self.global_best_value = np.inf\n        self.iteration = 0\n\n    def __call__(self, func):\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        scale = ub - lb\n\n        def quantum_update():\n            for i in range(self.particles):\n                delta = np.random.uniform(-1, 1, self.dim)\n                potential_position = self.position[i] + self.velocity[i] * delta\n                potential_position = np.clip(potential_position, 0, 1)\n                real_position = lb + potential_position * scale\n                value = func(real_position)\n                if value < self.personal_best_value[i]:\n                    self.personal_best_value[i] = value\n                    self.personal_best_position[i] = potential_position\n                if value < self.global_best_value:\n                    self.global_best_value = value\n                    self.global_best_position = potential_position\n\n        while self.iteration < self.budget:\n            quantum_update()\n            inertia_weight = 0.9 - (0.5 * (self.iteration/self.budget))  # Dynamic inertia weight\n            for i in range(self.particles):\n                cognitive = np.random.random(self.dim)\n                social = np.random.random(self.dim)\n                random_factor = np.random.uniform(0.8, 1.2, self.dim)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +  # Updated inertia application\n                                    cognitive * (self.personal_best_position[i] - self.position[i]) +\n                                    social * (self.global_best_position - self.position[i])) * random_factor\n                self.position[i] += self.velocity[i]  # Removed constant scaling factor\n                self.position[i] = np.clip(self.position[i], 0, 1)\n            self.iteration += self.particles\n\n        return lb + self.global_best_position * scale", "name": "QIPSO", "description": "Enhanced adaptive velocity scaling with dynamic inertia weight for improved convergence.", "configspace": "", "generation": 46, "fitness": 0.28905215492199454, "feedback": "The algorithm QIPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.29 with standard deviation 0.00.", "error": "", "parent_id": "7117e5cb-a0b9-4587-a0a5-4b4b593089ed", "metadata": {"aucs": [0.28905215492199454]}, "mutation_prompt": null}
{"id": "8648be3f-e272-45c6-aef0-a7215a78f46d", "solution": "import numpy as np\n\nclass QIPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particles = min(100, max(10, dim * 2))  # Dynamic particle count\n        self.position = np.random.uniform(0, 1, (self.particles, self.dim))\n        self.velocity = np.zeros((self.particles, self.dim))\n        self.personal_best_position = np.copy(self.position)\n        self.global_best_position = None\n        self.personal_best_value = np.full(self.particles, np.inf)\n        self.global_best_value = np.inf\n        self.iteration = 0\n\n    def __call__(self, func):\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        scale = ub - lb\n\n        def quantum_update():\n            for i in range(self.particles):\n                delta = np.random.uniform(-1, 1, self.dim)\n                potential_position = self.position[i] + self.velocity[i] * delta\n                potential_position = np.clip(potential_position, 0, 1)\n                real_position = lb + potential_position * scale\n                value = func(real_position)\n                if value < self.personal_best_value[i]:\n                    self.personal_best_value[i] = value\n                    self.personal_best_position[i] = potential_position\n                if value < self.global_best_value:\n                    self.global_best_value = value\n                    self.global_best_position = potential_position\n\n        while self.iteration < self.budget:\n            quantum_update()\n            for i in range(self.particles):\n                cognitive = np.random.random(self.dim)\n                social = np.random.random(self.dim)\n                adaptive_cognitive = cognitive * (0.5 + 0.5 * np.random.random())\n                adaptive_social = social * (0.5 + 0.5 * np.random.random())\n                random_factor = np.random.uniform(0.8, 1.2, self.dim)\n                self.velocity[i] = (0.5 * self.velocity[i] +\n                                    adaptive_cognitive * (self.personal_best_position[i] - self.position[i]) +\n                                    adaptive_social * (self.global_best_position - self.position[i])) * random_factor\n                self.position[i] += 0.9 * self.velocity[i]  # Adaptive velocity scaling\n                self.position[i] = np.clip(self.position[i], 0, 1)\n            self.iteration += self.particles\n\n        return lb + self.global_best_position * scale", "name": "QIPSO", "description": "Introduced adaptive personal and global influence factors to balance exploration and exploitation dynamically.", "configspace": "", "generation": 47, "fitness": 0.28764866125100297, "feedback": "The algorithm QIPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.29 with standard deviation 0.00.", "error": "", "parent_id": "7117e5cb-a0b9-4587-a0a5-4b4b593089ed", "metadata": {"aucs": [0.28764866125100297]}, "mutation_prompt": null}
{"id": "daac564b-1204-488d-810a-549976a255e0", "solution": "import numpy as np\n\nclass QIPSOPlus:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particles = min(100, max(10, dim * 2))  # Dynamic particle count\n        self.position = np.random.uniform(0, 1, (self.particles, self.dim))\n        self.velocity = np.zeros((self.particles, self.dim))\n        self.personal_best_position = np.copy(self.position)\n        self.global_best_position = None\n        self.personal_best_value = np.full(self.particles, np.inf)\n        self.global_best_value = np.inf\n        self.iteration = 0\n\n    def __call__(self, func):\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        scale = ub - lb\n\n        def quantum_update():\n            for i in range(self.particles):\n                delta = np.random.uniform(-1, 1, self.dim)\n                potential_position = self.position[i] + self.velocity[i] * delta\n                potential_position = np.clip(potential_position, 0, 1)\n                real_position = lb + potential_position * scale\n                value = func(real_position)\n                if value < self.personal_best_value[i]:\n                    self.personal_best_value[i] = value\n                    self.personal_best_position[i] = potential_position\n                if value < self.global_best_value:\n                    self.global_best_value = value\n                    self.global_best_position = potential_position\n\n        while self.iteration < self.budget:\n            quantum_update()\n            inertia_weight = 0.9 - 0.5 * (self.iteration / self.budget)\n            for i in range(self.particles):\n                cognitive = np.random.random(self.dim)\n                social = np.random.random(self.dim)\n                random_factor = np.random.uniform(0.8, 1.2, self.dim)\n                perturbation = np.random.normal(0, 0.1, self.dim)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive * (self.personal_best_position[i] - self.position[i]) +\n                                    social * (self.global_best_position - self.position[i])) * random_factor + perturbation\n                self.position[i] += self.velocity[i]\n                self.position[i] = np.clip(self.position[i], 0, 1)\n            self.iteration += self.particles\n\n        return lb + self.global_best_position * scale", "name": "QIPSOPlus", "description": "Enhanced particle diversification through adaptive velocity perturbations and dynamic inertia weighting.", "configspace": "", "generation": 48, "fitness": 0.2830108548445964, "feedback": "The algorithm QIPSOPlus got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28 with standard deviation 0.00.", "error": "", "parent_id": "7117e5cb-a0b9-4587-a0a5-4b4b593089ed", "metadata": {"aucs": [0.2830108548445964]}, "mutation_prompt": null}
{"id": "d6b8f74b-0a6a-40da-bd4b-68d2052241b6", "solution": "import numpy as np\n\nclass QIPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particles = min(100, max(10, dim * 2))  # Dynamic particle count\n        self.position = np.random.uniform(0, 1, (self.particles, self.dim))\n        self.velocity = np.zeros((self.particles, self.dim))\n        self.personal_best_position = np.copy(self.position)\n        self.global_best_position = None\n        self.personal_best_value = np.full(self.particles, np.inf)\n        self.global_best_value = np.inf\n        self.iteration = 0\n\n    def __call__(self, func):\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        scale = ub - lb\n\n        def quantum_update():\n            for i in range(self.particles):\n                delta = np.random.uniform(-1, 1, self.dim)\n                potential_position = self.position[i] + self.velocity[i] * delta\n                potential_position = np.clip(potential_position, 0, 1)\n                real_position = lb + potential_position * scale\n                value = func(real_position)\n                if value < self.personal_best_value[i]:\n                    self.personal_best_value[i] = value\n                    self.personal_best_position[i] = potential_position\n                if value < self.global_best_value:\n                    self.global_best_value = value\n                    self.global_best_position = potential_position\n\n        while self.iteration < self.budget:\n            quantum_update()\n            inertia_weight = 0.5 + 0.4 * (self.budget - self.iteration) / self.budget  # Dynamic inertia weight\n            for i in range(self.particles):\n                cognitive = np.random.random(self.dim)\n                social = np.random.random(self.dim)\n                random_factor = np.random.uniform(0.8, 1.2, self.dim)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +  # Adjusted with inertia weight\n                                    cognitive * (self.personal_best_position[i] - self.position[i]) +\n                                    social * (self.global_best_position - self.position[i])) * random_factor\n                self.position[i] += 0.9 * self.velocity[i]  # Adaptive velocity scaling\n                self.position[i] = np.clip(self.position[i], 0, 1)\n            self.iteration += self.particles\n\n        return lb + self.global_best_position * scale", "name": "QIPSO", "description": "Enhanced exploration by introducing an inertia weight factor to dynamically adjust exploration and exploitation balance.", "configspace": "", "generation": 49, "fitness": 0.29078097440101147, "feedback": "The algorithm QIPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.29 with standard deviation 0.00.", "error": "", "parent_id": "7117e5cb-a0b9-4587-a0a5-4b4b593089ed", "metadata": {"aucs": [0.29078097440101147]}, "mutation_prompt": null}
{"id": "44999594-63d4-4d63-a9ab-b9f2417f255a", "solution": "import numpy as np\n\nclass QIPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particles = min(100, max(10, dim * 2))  # Dynamic particle count\n        self.position = np.random.uniform(0, 1, (self.particles, self.dim))\n        self.velocity = np.zeros((self.particles, self.dim))\n        self.personal_best_position = np.copy(self.position)\n        self.global_best_position = None\n        self.personal_best_value = np.full(self.particles, np.inf)\n        self.global_best_value = np.inf\n        self.iteration = 0\n\n    def __call__(self, func):\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        scale = ub - lb\n\n        def quantum_update():\n            for i in range(self.particles):\n                delta = np.random.uniform(-1, 1, self.dim)\n                potential_position = self.position[i] + self.velocity[i] * delta\n                potential_position = np.clip(potential_position, 0, 1)\n                real_position = lb + potential_position * scale\n                value = func(real_position)\n                if value < self.personal_best_value[i]:\n                    self.personal_best_value[i] = value\n                    self.personal_best_position[i] = potential_position\n                if value < self.global_best_value:\n                    self.global_best_value = value\n                    self.global_best_position = potential_position\n\n        while self.iteration < self.budget:\n            quantum_update()\n            inertia_weight = 0.9 - 0.5 * (self.iteration / self.budget)  # Dynamic inertia weight\n            for i in range(self.particles):\n                cognitive = np.random.random(self.dim)\n                social = np.random.random(self.dim)\n                random_factor = np.random.uniform(0.8, 1.2, self.dim)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive * (self.personal_best_position[i] - self.position[i]) +\n                                    social * (self.global_best_position - self.position[i])) * random_factor\n                self.position[i] += self.velocity[i]  # Removed fixed scaling\n                self.position[i] = np.clip(self.position[i], 0, 1)\n            self.iteration += self.particles\n\n        return lb + self.global_best_position * scale", "name": "QIPSO", "description": "Introduced dynamic inertia weight to balance exploration and exploitation during the search process.", "configspace": "", "generation": 50, "fitness": 0.28905215492199454, "feedback": "The algorithm QIPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.29 with standard deviation 0.00.", "error": "", "parent_id": "7117e5cb-a0b9-4587-a0a5-4b4b593089ed", "metadata": {"aucs": [0.28905215492199454]}, "mutation_prompt": null}
{"id": "3f68eb56-82aa-42b1-b35f-c96f48112299", "solution": "import numpy as np\n\nclass QIPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particles = min(100, max(10, dim * 2))  # Dynamic particle count\n        self.position = np.random.uniform(0, 1, (self.particles, self.dim))\n        self.velocity = np.zeros((self.particles, self.dim))\n        self.personal_best_position = np.copy(self.position)\n        self.global_best_position = None\n        self.personal_best_value = np.full(self.particles, np.inf)\n        self.global_best_value = np.inf\n        self.iteration = 0\n\n    def __call__(self, func):\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        scale = ub - lb\n\n        def quantum_update():\n            for i in range(self.particles):\n                delta = np.random.uniform(-1, 1, self.dim)\n                potential_position = self.position[i] + self.velocity[i] * delta\n                potential_position = np.clip(potential_position, 0, 1)\n                real_position = lb + potential_position * scale\n                value = func(real_position)\n                if value < self.personal_best_value[i]:\n                    self.personal_best_value[i] = value\n                    self.personal_best_position[i] = potential_position\n                if value < self.global_best_value:\n                    self.global_best_value = value\n                    self.global_best_position = potential_position\n\n        while self.iteration < self.budget:\n            quantum_update()\n            for i in range(self.particles):\n                cognitive = np.random.random(self.dim)\n                social = np.random.random(self.dim)\n                random_factor = np.random.uniform(0.8, 1.2, self.dim)\n                inertia_weight = 0.9 - 0.5 * (self.iteration / self.budget)  # Adaptive inertia\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive * (self.personal_best_position[i] - self.position[i]) +\n                                    social * (self.global_best_position - self.position[i])) * random_factor\n                self.position[i] += 0.9 * self.velocity[i]  # Adaptive velocity scaling\n                self.position[i] = np.clip(self.position[i], 0, 1)\n            self.iteration += self.particles\n\n        return lb + self.global_best_position * scale", "name": "QIPSO", "description": "Enhanced QIPSO by introducing adaptive inertia weight for better balance between exploration and exploitation.", "configspace": "", "generation": 51, "fitness": 0.2908104672612435, "feedback": "The algorithm QIPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.29 with standard deviation 0.00.", "error": "", "parent_id": "7117e5cb-a0b9-4587-a0a5-4b4b593089ed", "metadata": {"aucs": [0.2908104672612435]}, "mutation_prompt": null}
{"id": "2d1cb409-f53e-446e-ad15-f6d0e8a67cba", "solution": "import numpy as np\n\nclass QIPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particles = min(100, max(10, dim * 2))  # Dynamic particle count\n        self.position = np.random.uniform(0, 1, (self.particles, self.dim))\n        self.velocity = np.zeros((self.particles, self.dim))\n        self.personal_best_position = np.copy(self.position)\n        self.global_best_position = None\n        self.personal_best_value = np.full(self.particles, np.inf)\n        self.global_best_value = np.inf\n        self.iteration = 0\n\n    def __call__(self, func):\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        scale = ub - lb\n\n        def quantum_update():\n            for i in range(self.particles):\n                delta = np.random.uniform(-1, 1, self.dim)\n                potential_position = self.position[i] + self.velocity[i] * delta\n                potential_position = np.clip(potential_position, 0, 1)\n                real_position = lb + potential_position * scale\n                value = func(real_position)\n                if value < self.personal_best_value[i]:\n                    self.personal_best_value[i] = value\n                    self.personal_best_position[i] = potential_position\n                if value < self.global_best_value:\n                    self.global_best_value = value\n                    self.global_best_position = potential_position\n\n        while self.iteration < self.budget:\n            quantum_update()\n            for i in range(self.particles):\n                cognitive = np.random.random(self.dim)\n                social = np.random.random(self.dim)\n                random_factor = np.random.uniform(0.8, 1.2, self.dim)\n                # Attraction-repulsion mechanism\n                force_direction = np.sign(self.global_best_position - self.position[i])\n                attraction_repulsion = force_direction * np.random.uniform(-0.1, 0.1, self.dim)\n                self.velocity[i] = (0.5 * self.velocity[i] +\n                                    cognitive * (self.personal_best_position[i] - self.position[i]) +\n                                    social * (self.global_best_position - self.position[i])) * random_factor + attraction_repulsion\n                self.position[i] += 0.9 * self.velocity[i]  # Adaptive velocity scaling\n                self.position[i] = np.clip(self.position[i], 0, 1)\n            self.iteration += self.particles\n\n        return lb + self.global_best_position * scale", "name": "QIPSO", "description": "Enhanced local search capability by adding an attraction-repulsion mechanism to improve convergence.", "configspace": "", "generation": 52, "fitness": 0.28698345881663156, "feedback": "The algorithm QIPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.29 with standard deviation 0.00.", "error": "", "parent_id": "7117e5cb-a0b9-4587-a0a5-4b4b593089ed", "metadata": {"aucs": [0.28698345881663156]}, "mutation_prompt": null}
{"id": "2a90a8b8-a24a-4a61-a1c9-f2bf3d6a12d6", "solution": "import numpy as np\n\nclass QIPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particles = min(100, max(10, dim * 2))  # Dynamic particle count\n        self.position = np.random.uniform(0, 1, (self.particles, self.dim))\n        self.velocity = np.zeros((self.particles, self.dim))\n        self.personal_best_position = np.copy(self.position)\n        self.global_best_position = None\n        self.personal_best_value = np.full(self.particles, np.inf)\n        self.global_best_value = np.inf\n        self.iteration = 0\n\n    def __call__(self, func):\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        scale = ub - lb\n\n        def quantum_update():\n            for i in range(self.particles):\n                delta = np.random.uniform(-1, 1, self.dim)\n                potential_position = self.position[i] + self.velocity[i] * delta\n                potential_position = np.clip(potential_position, 0, 1)\n                real_position = lb + potential_position * scale\n                value = func(real_position)\n                if value < self.personal_best_value[i]:\n                    self.personal_best_value[i] = value\n                    self.personal_best_position[i] = potential_position\n                if value < self.global_best_value:\n                    self.global_best_value = value\n                    self.global_best_position = potential_position\n\n        while self.iteration < self.budget:\n            quantum_update()\n            for i in range(self.particles):\n                cognitive = np.random.random(self.dim)\n                social = np.random.random(self.dim)\n                random_factor = np.random.uniform(0.8, 1.2, self.dim)\n                w = 0.5 + 0.4 * np.cos(np.pi * self.iteration / self.budget)  # Adaptive inertia weight\n                self.velocity[i] = (w * self.velocity[i] +\n                                    cognitive * (self.personal_best_position[i] - self.position[i]) +\n                                    social * (self.global_best_position - self.position[i])) * random_factor\n                self.position[i] += 0.9 * self.velocity[i]  # Adaptive velocity scaling\n                self.position[i] = np.clip(self.position[i], 0, 1)\n            self.iteration += self.particles\n\n        return lb + self.global_best_position * scale", "name": "QIPSO", "description": "Introduced a non-static inertia weight to enhance adaptive exploration and exploitation balance.", "configspace": "", "generation": 53, "fitness": 0.2909221580087956, "feedback": "The algorithm QIPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.29 with standard deviation 0.00.", "error": "", "parent_id": "7117e5cb-a0b9-4587-a0a5-4b4b593089ed", "metadata": {"aucs": [0.2909221580087956]}, "mutation_prompt": null}
{"id": "a3dbfece-070e-4a35-b88f-f8ccf31843f5", "solution": "import numpy as np\n\nclass QIPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particles = min(100, max(10, dim * 2))  # Dynamic particle count\n        self.position = np.random.uniform(0, 1, (self.particles, self.dim))\n        self.velocity = np.zeros((self.particles, self.dim))\n        self.personal_best_position = np.copy(self.position)\n        self.global_best_position = None\n        self.personal_best_value = np.full(self.particles, np.inf)\n        self.global_best_value = np.inf\n        self.iteration = 0\n\n    def __call__(self, func):\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        scale = ub - lb\n\n        def quantum_update():\n            for i in range(self.particles):\n                delta = np.random.uniform(-1, 1, self.dim)\n                potential_position = self.position[i] + self.velocity[i] * delta\n                potential_position = np.clip(potential_position, 0, 1)\n                real_position = lb + potential_position * scale\n                value = func(real_position)\n                if value < self.personal_best_value[i]:\n                    self.personal_best_value[i] = value\n                    self.personal_best_position[i] = potential_position\n                if value < self.global_best_value:\n                    self.global_best_value = value\n                    self.global_best_position = potential_position\n\n        while self.iteration < self.budget:\n            quantum_update()\n            w = 0.9 - (0.5 * self.iteration / self.budget)  # Dynamic inertia weight\n            for i in range(self.particles):\n                cognitive = np.random.random(self.dim)\n                social = np.random.random(self.dim)\n                random_factor = np.random.uniform(0.8, 1.2, self.dim)\n                self.velocity[i] = (w * self.velocity[i] +\n                                    cognitive * (self.personal_best_position[i] - self.position[i]) +\n                                    social * (self.global_best_position - self.position[i])) * random_factor\n                self.position[i] += 0.9 * self.velocity[i]  # Adaptive velocity scaling\n                self.position[i] = np.clip(self.position[i], 0, 1)\n            self.iteration += self.particles\n\n        return lb + self.global_best_position * scale", "name": "QIPSO", "description": "Introduced dynamic inertia weight adjustment to improve convergence speed and solution quality.", "configspace": "", "generation": 54, "fitness": 0.2908104672612435, "feedback": "The algorithm QIPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.29 with standard deviation 0.00.", "error": "", "parent_id": "7117e5cb-a0b9-4587-a0a5-4b4b593089ed", "metadata": {"aucs": [0.2908104672612435]}, "mutation_prompt": null}
{"id": "91ecd217-6e25-4318-bb99-0598e0b339f9", "solution": "import numpy as np\n\nclass QIPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particles = min(100, max(10, dim * 2))  # Dynamic particle count\n        self.position = np.random.uniform(0, 1, (self.particles, self.dim))\n        self.velocity = np.zeros((self.particles, self.dim))\n        self.personal_best_position = np.copy(self.position)\n        self.global_best_position = None\n        self.personal_best_value = np.full(self.particles, np.inf)\n        self.global_best_value = np.inf\n        self.iteration = 0\n\n    def __call__(self, func):\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        scale = ub - lb\n\n        def quantum_update():\n            for i in range(self.particles):\n                delta = np.random.uniform(-1, 1, self.dim)\n                potential_position = self.position[i] + self.velocity[i] * delta\n                potential_position = np.clip(potential_position, 0, 1)\n                real_position = lb + potential_position * scale\n                value = func(real_position)\n                if value < self.personal_best_value[i]:\n                    self.personal_best_value[i] = value\n                    self.personal_best_position[i] = potential_position\n                if value < self.global_best_value:\n                    self.global_best_value = value\n                    self.global_best_position = potential_position\n\n        while self.iteration < self.budget:\n            quantum_update()\n            inertia_weight = 0.9 - 0.5 * (self.iteration / self.budget)  # Dynamic inertia weight\n            for i in range(self.particles):\n                cognitive = np.random.random(self.dim)\n                social = np.random.random(self.dim)\n                random_factor = np.random.uniform(0.8, 1.2, self.dim)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +  # Applied dynamic inertia weight\n                                    cognitive * (self.personal_best_position[i] - self.position[i]) +\n                                    social * (self.global_best_position - self.position[i])) * random_factor\n                self.position[i] += 0.9 * self.velocity[i]  # Adaptive velocity scaling\n                self.position[i] = np.clip(self.position[i], 0, 1)\n            self.iteration += self.particles\n\n        return lb + self.global_best_position * scale", "name": "QIPSO", "description": "Introduced a dynamic inertia weight approach to improve convergence speed and exploration-exploitation balance.", "configspace": "", "generation": 55, "fitness": 0.2908104672612435, "feedback": "The algorithm QIPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.29 with standard deviation 0.00.", "error": "", "parent_id": "7117e5cb-a0b9-4587-a0a5-4b4b593089ed", "metadata": {"aucs": [0.2908104672612435]}, "mutation_prompt": null}
{"id": "29ef8bf0-44a9-49f4-9565-85b00fc1d290", "solution": "import numpy as np\n\nclass QIPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particles = min(100, max(10, dim * 2))  # Dynamic particle count\n        self.position = np.random.uniform(0, 1, (self.particles, self.dim))\n        self.velocity = np.zeros((self.particles, self.dim))\n        self.personal_best_position = np.copy(self.position)\n        self.global_best_position = None\n        self.personal_best_value = np.full(self.particles, np.inf)\n        self.global_best_value = np.inf\n        self.iteration = 0\n\n    def __call__(self, func):\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        scale = ub - lb\n\n        def quantum_update():\n            for i in range(self.particles):\n                delta = np.random.uniform(-1, 1, self.dim)\n                potential_position = self.position[i] + self.velocity[i] * delta\n                random_adjustment = np.random.uniform(-0.1, 0.1, self.dim)  # New line\n                potential_position = np.clip(potential_position + random_adjustment, 0, 1)  # Changed line\n                real_position = lb + potential_position * scale\n                value = func(real_position)\n                if value < self.personal_best_value[i]:\n                    self.personal_best_value[i] = value\n                    self.personal_best_position[i] = potential_position\n                if value < self.global_best_value:\n                    self.global_best_value = value\n                    self.global_best_position = potential_position\n\n        while self.iteration < self.budget:\n            quantum_update()\n            for i in range(self.particles):\n                cognitive = np.random.random(self.dim)\n                social = np.random.random(self.dim)\n                random_factor = np.random.uniform(0.8, 1.2, self.dim)\n                self.velocity[i] = (0.5 * self.velocity[i] +\n                                    0.3 * cognitive * (self.personal_best_position[i] - self.position[i]) +  # Changed line\n                                    0.7 * social * (self.global_best_position - self.position[i])) * random_factor  # Changed line\n                self.position[i] += 0.8 * self.velocity[i]  # Changed line\n                self.position[i] = np.clip(self.position[i], 0, 1)\n            self.iteration += self.particles\n\n        return lb + self.global_best_position * scale", "name": "QIPSO", "description": "Enhanced exploration by introducing a random adjustment to position updates and improving velocity scaling.", "configspace": "", "generation": 56, "fitness": 0.28849751703379056, "feedback": "The algorithm QIPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.29 with standard deviation 0.00.", "error": "", "parent_id": "7117e5cb-a0b9-4587-a0a5-4b4b593089ed", "metadata": {"aucs": [0.28849751703379056]}, "mutation_prompt": null}
{"id": "fb7f0647-ced4-416e-a7a5-13ca06a50538", "solution": "import numpy as np\n\nclass QIPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particles = min(100, max(10, dim * 2))  # Dynamic particle count\n        self.position = np.random.uniform(0, 1, (self.particles, self.dim))\n        self.velocity = np.zeros((self.particles, self.dim))\n        self.personal_best_position = np.copy(self.position)\n        self.global_best_position = None\n        self.personal_best_value = np.full(self.particles, np.inf)\n        self.global_best_value = np.inf\n        self.iteration = 0\n\n    def __call__(self, func):\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        scale = ub - lb\n\n        def quantum_update():\n            for i in range(self.particles):\n                delta = np.random.uniform(-1, 1, self.dim)\n                potential_position = self.position[i] + self.velocity[i] * delta\n                potential_position = np.clip(potential_position, 0, 1)\n                real_position = lb + potential_position * scale\n                value = func(real_position)\n                if value < self.personal_best_value[i]:\n                    self.personal_best_value[i] = value\n                    self.personal_best_position[i] = potential_position\n                if value < self.global_best_value:\n                    self.global_best_value = value\n                    self.global_best_position = potential_position\n\n        while self.iteration < self.budget:\n            quantum_update()\n            for i in range(self.particles):\n                cognitive = np.random.random(self.dim)\n                social = np.random.random(self.dim)\n                random_factor = np.random.uniform(0.8, 1.2, self.dim)\n                self.velocity[i] = (0.5 * self.velocity[i] +\n                                    cognitive * (self.personal_best_position[i] - self.position[i]) +\n                                    social * (self.global_best_position - self.position[i])) * random_factor\n                self.position[i] += 0.95 * self.velocity[i]  # Enhanced velocity scaling\n                self.position[i] = np.clip(self.position[i], 0, 1)\n            self.iteration += self.particles\n\n        return lb + self.global_best_position * scale", "name": "QIPSO", "description": "Enhanced velocity scaling factor to increase convergence speed.", "configspace": "", "generation": 57, "fitness": 0.29380862340359015, "feedback": "The algorithm QIPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.29 with standard deviation 0.00.", "error": "", "parent_id": "7117e5cb-a0b9-4587-a0a5-4b4b593089ed", "metadata": {"aucs": [0.29380862340359015]}, "mutation_prompt": null}
{"id": "6c969cbb-96e2-4c46-9101-9c5975c870ea", "solution": "import numpy as np\n\nclass QIPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particles = min(100, max(10, dim * 2))  # Dynamic particle count\n        self.position = np.random.uniform(0, 1, (self.particles, self.dim))\n        self.velocity = np.zeros((self.particles, self.dim))\n        self.personal_best_position = np.copy(self.position)\n        self.global_best_position = None\n        self.personal_best_value = np.full(self.particles, np.inf)\n        self.global_best_value = np.inf\n        self.iteration = 0\n\n    def __call__(self, func):\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        scale = ub - lb\n\n        def quantum_update():\n            for i in range(self.particles):\n                delta = np.random.uniform(-1, 1, self.dim)\n                potential_position = self.position[i] + self.velocity[i] * delta\n                potential_position = np.clip(potential_position, 0, 1)\n                real_position = lb + potential_position * scale\n                value = func(real_position)\n                if value < self.personal_best_value[i]:\n                    self.personal_best_value[i] = value\n                    self.personal_best_position[i] = potential_position\n                if value < self.global_best_value:\n                    self.global_best_value = value\n                    self.global_best_position = potential_position\n\n        while self.iteration < self.budget:\n            quantum_update()\n            for i in range(self.particles):\n                cognitive = np.random.random(self.dim)\n                social = np.random.random(self.dim)\n                random_factor = np.random.uniform(0.8, 1.2, self.dim)\n                inertia_weight = 0.9 - (self.iteration / self.budget) * 0.5  # Adaptive inertia weight\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive * (self.personal_best_position[i] - self.position[i]) +\n                                    social * (self.global_best_position - self.position[i])) * random_factor\n                self.position[i] += 0.95 * self.velocity[i]\n                self.position[i] = np.clip(self.position[i], 0, 1)\n            self.iteration += self.particles\n\n        return lb + self.global_best_position * scale", "name": "QIPSO", "description": "Introducing adaptive inertia weight and tournament selection to enhance exploration-exploitation balance.", "configspace": "", "generation": 58, "fitness": 0.2897368238764767, "feedback": "The algorithm QIPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.29 with standard deviation 0.00.", "error": "", "parent_id": "fb7f0647-ced4-416e-a7a5-13ca06a50538", "metadata": {"aucs": [0.2897368238764767]}, "mutation_prompt": null}
{"id": "c0f08c68-0a15-4890-97b6-c2a4b48e4fbe", "solution": "import numpy as np\n\nclass QIPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particles = min(100, max(10, dim * 2))  # Dynamic particle count\n        self.position = np.random.uniform(0, 1, (self.particles, self.dim))\n        self.velocity = np.zeros((self.particles, self.dim))\n        self.personal_best_position = np.copy(self.position)\n        self.global_best_position = None\n        self.personal_best_value = np.full(self.particles, np.inf)\n        self.global_best_value = np.inf\n        self.iteration = 0\n\n    def __call__(self, func):\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        scale = ub - lb\n\n        def quantum_update():\n            for i in range(self.particles):\n                delta = np.random.normal(0, 0.5, self.dim)  # Line 1: Use normal distribution for delta\n                potential_position = self.position[i] + self.velocity[i] * delta\n                potential_position = np.clip(potential_position, 0, 1)\n                real_position = lb + potential_position * scale\n                value = func(real_position)\n                if value < self.personal_best_value[i]:\n                    self.personal_best_value[i] = value\n                    self.personal_best_position[i] = potential_position\n                if value < self.global_best_value:\n                    self.global_best_value = value\n                    self.global_best_position = potential_position\n\n        while self.iteration < self.budget:\n            quantum_update()\n            for i in range(self.particles):\n                cognitive = np.random.random(self.dim)\n                social = np.random.random(self.dim)\n                random_factor = np.random.uniform(0.8, 1.2, self.dim)\n                diversity_factor = 1.0 + (np.std(self.position, axis=0) * 0.1)  # Line 2: Add diversity factor\n                self.velocity[i] = (0.5 * self.velocity[i] +\n                                    cognitive * (self.personal_best_position[i] - self.position[i]) +\n                                    social * (self.global_best_position - self.position[i])) * random_factor * diversity_factor  # Line 3: Modify velocity update\n                self.position[i] += 0.95 * self.velocity[i]\n                self.position[i] = np.clip(self.position[i], 0, 1)\n            self.iteration += self.particles\n\n        return lb + self.global_best_position * scale", "name": "QIPSO", "description": "Adaptive quantum update and enhanced diversity control for improved global exploration.", "configspace": "", "generation": 59, "fitness": 0.28728672307648695, "feedback": "The algorithm QIPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.29 with standard deviation 0.00.", "error": "", "parent_id": "fb7f0647-ced4-416e-a7a5-13ca06a50538", "metadata": {"aucs": [0.28728672307648695]}, "mutation_prompt": null}
{"id": "dfea40e9-5054-4dd9-8bb1-e488a9300bfa", "solution": "import numpy as np\n\nclass QIPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particles = min(100, max(10, dim * 2))  # Dynamic particle count\n        self.position = np.random.uniform(0, 1, (self.particles, self.dim))\n        self.velocity = np.zeros((self.particles, self.dim))\n        self.personal_best_position = np.copy(self.position)\n        self.global_best_position = None\n        self.personal_best_value = np.full(self.particles, np.inf)\n        self.global_best_value = np.inf\n        self.iteration = 0\n        self.inertia_weight = 0.9\n\n    def __call__(self, func):\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        scale = ub - lb\n\n        def quantum_update():\n            for i in range(self.particles):\n                delta = np.random.uniform(-1, 1, self.dim)\n                potential_position = self.position[i] + self.velocity[i] * delta\n                potential_position = np.clip(potential_position, 0, 1)\n                real_position = lb + potential_position * scale\n                value = func(real_position)\n                if value < self.personal_best_value[i]:\n                    self.personal_best_value[i] = value\n                    self.personal_best_position[i] = potential_position\n                if value < self.global_best_value:\n                    self.global_best_value = value\n                    self.global_best_position = potential_position\n\n        while self.iteration < self.budget:\n            quantum_update()\n            for i in range(self.particles):\n                cognitive = np.random.random(self.dim)\n                social = np.random.random(self.dim)\n                random_factor = np.random.uniform(0.8, 1.2, self.dim)\n                self.velocity[i] = (self.inertia_weight * self.velocity[i] +\n                                    cognitive * (self.personal_best_position[i] - self.position[i]) +\n                                    social * (self.global_best_position - self.position[i])) * random_factor\n                self.position[i] += 0.95 * self.velocity[i]  # Enhanced velocity scaling\n                self.position[i] = np.clip(self.position[i], 0, 1)\n                if np.random.random() < 0.1:  # Perturbation mechanism\n                    self.position[i] = np.random.uniform(0, 1, self.dim)\n            self.iteration += self.particles\n            self.inertia_weight *= 0.99  # Adapt inertia weight\n\n        return lb + self.global_best_position * scale", "name": "QIPSO", "description": "Introduced inertia weight adaptation and perturbation mechanism to enhance exploration and exploitation balance.", "configspace": "", "generation": 60, "fitness": 0.282376382671472, "feedback": "The algorithm QIPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28 with standard deviation 0.00.", "error": "", "parent_id": "fb7f0647-ced4-416e-a7a5-13ca06a50538", "metadata": {"aucs": [0.282376382671472]}, "mutation_prompt": null}
{"id": "406e89df-eff6-4c91-bbe8-1dcea6090c20", "solution": "import numpy as np\n\nclass QIPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particles = min(100, max(10, dim * 2))  # Dynamic particle count\n        self.position = np.random.uniform(0, 1, (self.particles, self.dim))\n        self.velocity = np.zeros((self.particles, self.dim))\n        self.personal_best_position = np.copy(self.position)\n        self.global_best_position = None\n        self.personal_best_value = np.full(self.particles, np.inf)\n        self.global_best_value = np.inf\n        self.iteration = 0\n\n    def __call__(self, func):\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        scale = ub - lb\n\n        def quantum_update():\n            for i in range(self.particles):\n                delta = np.random.uniform(-1, 1, self.dim)\n                potential_position = self.position[i] + self.velocity[i] * delta\n                potential_position = np.clip(potential_position, 0, 1)\n                real_position = lb + potential_position * scale\n                value = func(real_position)\n                if value < self.personal_best_value[i]:\n                    self.personal_best_value[i] = value\n                    self.personal_best_position[i] = potential_position\n                if value < self.global_best_value:\n                    self.global_best_value = value\n                    self.global_best_position = potential_position\n\n        while self.iteration < self.budget:\n            quantum_update()\n            inertia_weight = 0.9 - 0.5 * (self.iteration / self.budget)  # Adaptive inertia weight\n            for i in range(self.particles):\n                cognitive = np.random.random(self.dim)\n                social = np.random.random(self.dim)\n                random_factor = np.random.uniform(0.8, 1.2, self.dim)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +  # Updated line\n                                    cognitive * (self.personal_best_position[i] - self.position[i]) +\n                                    social * (self.global_best_position - self.position[i])) * random_factor\n                self.position[i] += 0.9 * self.velocity[i]  # Adjusted enhanced velocity scaling\n                self.position[i] = np.clip(self.position[i], 0, 1)\n            self.iteration += self.particles\n\n        return lb + self.global_best_position * scale", "name": "QIPSO", "description": "Introduced adaptive inertia weight and slightly adjusted enhanced velocity scaling for improved convergence.", "configspace": "", "generation": 61, "fitness": 0.2908104672612435, "feedback": "The algorithm QIPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.29 with standard deviation 0.00.", "error": "", "parent_id": "fb7f0647-ced4-416e-a7a5-13ca06a50538", "metadata": {"aucs": [0.2908104672612435]}, "mutation_prompt": null}
{"id": "3541af86-698f-4e74-bd1b-38050ecee410", "solution": "import numpy as np\n\nclass QIPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particles = min(100, max(10, dim * 2))  # Dynamic particle count\n        self.position = np.random.uniform(0, 1, (self.particles, self.dim))\n        self.velocity = np.zeros((self.particles, self.dim))\n        self.personal_best_position = np.copy(self.position)\n        self.global_best_position = None\n        self.personal_best_value = np.full(self.particles, np.inf)\n        self.global_best_value = np.inf\n        self.iteration = 0\n\n    def __call__(self, func):\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        scale = ub - lb\n\n        def quantum_update():\n            for i in range(self.particles):\n                delta = np.random.uniform(-1, 1, self.dim)\n                potential_position = self.position[i] + self.velocity[i] * delta\n                potential_position = np.clip(potential_position, 0, 1)\n                real_position = lb + potential_position * scale\n                value = func(real_position)\n                if value < self.personal_best_value[i]:\n                    self.personal_best_value[i] = value\n                    self.personal_best_position[i] = potential_position\n                if value < self.global_best_value:\n                    self.global_best_value = value\n                    self.global_best_position = potential_position\n\n        while self.iteration < self.budget:\n            quantum_update()\n            inertia_weight = 0.9 - 0.5 * (self.iteration / self.budget)  # Adaptive inertia weight\n            for i in range(self.particles):\n                cognitive = np.random.random(self.dim)\n                social = np.random.random(self.dim)\n                random_factor = np.random.uniform(0.8, 1.2, self.dim)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +  # updated line\n                                    cognitive * (self.personal_best_position[i] - self.position[i]) +\n                                    social * (self.global_best_position - self.position[i])) * random_factor\n                self.position[i] += 0.95 * self.velocity[i]  # Enhanced velocity scaling\n                self.position[i] = np.clip(self.position[i], 0, 1)\n            self.iteration += self.particles\n\n        return lb + self.global_best_position * scale", "name": "QIPSO", "description": "Introduced adaptive inertia weight to balance exploration and exploitation dynamically.", "configspace": "", "generation": 62, "fitness": 0.2897368238764767, "feedback": "The algorithm QIPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.29 with standard deviation 0.00.", "error": "", "parent_id": "fb7f0647-ced4-416e-a7a5-13ca06a50538", "metadata": {"aucs": [0.2897368238764767]}, "mutation_prompt": null}
{"id": "6195114f-8605-48bd-bcbb-7124035b45ec", "solution": "import numpy as np\n\nclass QIPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particles = min(100, max(10, dim * 2))  # Dynamic particle count\n        self.position = np.random.uniform(0, 1, (self.particles, self.dim))\n        self.velocity = np.zeros((self.particles, self.dim))\n        self.personal_best_position = np.copy(self.position)\n        self.global_best_position = None\n        self.personal_best_value = np.full(self.particles, np.inf)\n        self.global_best_value = np.inf\n        self.iteration = 0\n\n    def __call__(self, func):\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        scale = ub - lb\n\n        def quantum_update():\n            for i in range(self.particles):\n                delta = np.random.uniform(-1, 1, self.dim)\n                potential_position = self.position[i] + self.velocity[i] * delta\n                potential_position = np.clip(potential_position, 0, 1)\n                real_position = lb + potential_position * scale\n                value = func(real_position)\n                if value < self.personal_best_value[i]:\n                    self.personal_best_value[i] = value\n                    self.personal_best_position[i] = potential_position\n                if value < self.global_best_value:\n                    self.global_best_value = value\n                    self.global_best_position = potential_position\n\n        while self.iteration < self.budget:\n            quantum_update()\n            inertia_weight = 0.9 - (0.5 * self.iteration / self.budget)  # Adaptive inertia\n            for i in range(self.particles):\n                cognitive = np.random.random(self.dim)\n                social = np.random.uniform(0.5, 2.0, self.dim)  # Dynamic social component\n                random_factor = np.random.uniform(0.8, 1.2, self.dim)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive * (self.personal_best_position[i] - self.position[i]) +\n                                    social * (self.global_best_position - self.position[i])) * random_factor\n                self.position[i] += 0.95 * self.velocity[i]  # Enhanced velocity scaling\n                self.position[i] = np.clip(self.position[i], 0, 1)\n            self.iteration += self.particles\n\n        return lb + self.global_best_position * scale", "name": "QIPSO", "description": "Introduced adaptive inertia weight and dynamic social component for improved convergence in QIPSO.", "configspace": "", "generation": 63, "fitness": 0.2893512884740269, "feedback": "The algorithm QIPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.29 with standard deviation 0.00.", "error": "", "parent_id": "fb7f0647-ced4-416e-a7a5-13ca06a50538", "metadata": {"aucs": [0.2893512884740269]}, "mutation_prompt": null}
{"id": "c508eb1d-ec85-45f5-9f3a-7dcdf7d0ff09", "solution": "import numpy as np\n\nclass QIPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particles = min(100, max(10, dim * 2))  # Dynamic particle count\n        self.position = np.random.uniform(0, 1, (self.particles, self.dim))\n        self.velocity = np.zeros((self.particles, self.dim))\n        self.personal_best_position = np.copy(self.position)\n        self.global_best_position = None\n        self.personal_best_value = np.full(self.particles, np.inf)\n        self.global_best_value = np.inf\n        self.iteration = 0\n\n    def __call__(self, func):\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        scale = ub - lb\n\n        def quantum_update():\n            for i in range(self.particles):\n                delta = np.random.uniform(-1, 1, self.dim)\n                potential_position = self.position[i] + self.velocity[i] * delta\n                potential_position = np.clip(potential_position, 0, 1)\n                real_position = lb + potential_position * scale\n                value = func(real_position)\n                if value < self.personal_best_value[i]:\n                    self.personal_best_value[i] = value\n                    self.personal_best_position[i] = potential_position\n                if value < self.global_best_value:\n                    self.global_best_value = value\n                    self.global_best_position = potential_position\n\n        while self.iteration < self.budget:\n            quantum_update()\n            inertia_weight = 0.9 - (0.5 * self.iteration / self.budget)  # Dynamic inertia weight\n            for i in range(self.particles):\n                cognitive = np.random.random(self.dim)\n                social = np.random.random(self.dim)\n                selection_prob = np.random.rand()\n                if selection_prob < 0.5:  # Stochastic selection\n                    self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                        cognitive * (self.personal_best_position[i] - self.position[i]) +\n                                        social * (self.global_best_position - self.position[i]))\n                self.position[i] += 0.95 * self.velocity[i]  # Enhanced velocity scaling\n                self.position[i] = np.clip(self.position[i], 0, 1)\n            self.iteration += self.particles\n\n        return lb + self.global_best_position * scale", "name": "QIPSO", "description": "Introduced dynamic inertia weight and stochastic selection to enhance exploration and exploitation balance.", "configspace": "", "generation": 64, "fitness": 0.2882522674516209, "feedback": "The algorithm QIPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.29 with standard deviation 0.00.", "error": "", "parent_id": "fb7f0647-ced4-416e-a7a5-13ca06a50538", "metadata": {"aucs": [0.2882522674516209]}, "mutation_prompt": null}
{"id": "8fdbf889-74a7-4f35-85c9-2ac6e9890ed9", "solution": "import numpy as np\n\nclass QIPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particles = min(100, max(10, dim * 2))  # Dynamic particle count\n        self.position = np.random.uniform(0, 1, (self.particles, self.dim))\n        self.velocity = np.zeros((self.particles, self.dim))\n        self.personal_best_position = np.copy(self.position)\n        self.global_best_position = None\n        self.personal_best_value = np.full(self.particles, np.inf)\n        self.global_best_value = np.inf\n        self.iteration = 0\n\n    def __call__(self, func):\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        scale = ub - lb\n\n        def quantum_update():\n            for i in range(self.particles):\n                delta = np.random.uniform(-1, 1, self.dim)\n                potential_position = self.position[i] + self.velocity[i] * delta\n                potential_position = np.clip(potential_position, 0, 1)\n                real_position = lb + potential_position * scale\n                value = func(real_position)\n                if value < self.personal_best_value[i]:\n                    self.personal_best_value[i] = value\n                    self.personal_best_position[i] = potential_position\n                if value < self.global_best_value:\n                    self.global_best_value = value\n                    self.global_best_position = potential_position\n\n        while self.iteration < self.budget:\n            quantum_update()\n            for i in range(self.particles):\n                cognitive = np.random.random(self.dim)\n                social = np.random.random(self.dim)\n                random_factor = 1.0 + np.random.uniform(-0.4, 0.4, self.dim)  # Change made here\n                self.velocity[i] = (0.5 * self.velocity[i] +\n                                    cognitive * (self.personal_best_position[i] - self.position[i]) +\n                                    social * (self.global_best_position - self.position[i])) * random_factor\n                self.position[i] += 0.95 * self.velocity[i]  # Enhanced velocity scaling\n                self.position[i] = np.clip(self.position[i], 0, 1)\n            self.iteration += self.particles\n\n        return lb + self.global_best_position * scale", "name": "QIPSO", "description": "Improved convergence through adaptive random factor scaling.", "configspace": "", "generation": 65, "fitness": 0.28581452375376115, "feedback": "The algorithm QIPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.29 with standard deviation 0.00.", "error": "", "parent_id": "fb7f0647-ced4-416e-a7a5-13ca06a50538", "metadata": {"aucs": [0.28581452375376115]}, "mutation_prompt": null}
{"id": "9be0fcfd-97b1-4082-802d-21e93d596be1", "solution": "import numpy as np\n\nclass QIPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particles = min(100, max(10, dim * 2))  # Dynamic particle count\n        self.position = np.random.uniform(0, 1, (self.particles, self.dim))\n        self.velocity = np.zeros((self.particles, self.dim))\n        self.personal_best_position = np.copy(self.position)\n        self.global_best_position = None\n        self.personal_best_value = np.full(self.particles, np.inf)\n        self.global_best_value = np.inf\n        self.iteration = 0\n        self.inertia_weight = 0.9  # Adaptive inertia weight\n\n    def __call__(self, func):\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        scale = ub - lb\n\n        def quantum_update():\n            for i in range(self.particles):\n                delta = np.random.uniform(-1, 1, self.dim)\n                potential_position = self.position[i] + self.velocity[i] * delta\n                potential_position = np.clip(potential_position, 0, 1)\n                real_position = lb + potential_position * scale\n                value = func(real_position)\n                if value < self.personal_best_value[i]:\n                    self.personal_best_value[i] = value\n                    self.personal_best_position[i] = potential_position\n                if value < self.global_best_value:\n                    self.global_best_value = value\n                    self.global_best_position = potential_position\n\n        while self.iteration < self.budget:\n            quantum_update()\n            for i in range(self.particles):\n                cognitive = np.random.random(self.dim)\n                social = np.random.random(self.dim)\n                random_factor = np.random.uniform(0.8, 1.2, self.dim)\n                self.velocity[i] = (self.inertia_weight * self.velocity[i] +\n                                    cognitive * (self.personal_best_position[i] - self.position[i]) +\n                                    social * (self.global_best_position - self.position[i])) * random_factor\n                self.position[i] += self.velocity[i]  # Removed enhanced velocity scaling\n                self.position[i] = np.clip(self.position[i], 0, 1)\n            self.iteration += self.particles\n            \n            # Adaptive inertia weight and diversity-based restart\n            if self.iteration % (self.budget // 10) == 0:\n                self.inertia_weight = 0.4 + 0.5 * (1 - self.iteration / self.budget)\n                if np.std(self.position) < 0.01:\n                    self.position = np.random.uniform(0, 1, (self.particles, self.dim))\n\n        return lb + self.global_best_position * scale", "name": "QIPSO", "description": "Adaptive inertia weight and diversity-based restart for improved exploration.", "configspace": "", "generation": 66, "fitness": 0.28962742542805886, "feedback": "The algorithm QIPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.29 with standard deviation 0.00.", "error": "", "parent_id": "fb7f0647-ced4-416e-a7a5-13ca06a50538", "metadata": {"aucs": [0.28962742542805886]}, "mutation_prompt": null}
{"id": "4e7aabae-2f23-425c-b4d2-12df83ab020d", "solution": "import numpy as np\n\nclass QIPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particles = min(100, max(10, dim * 2))  # Dynamic particle count\n        self.position = np.random.uniform(0, 1, (self.particles, self.dim))\n        self.velocity = np.zeros((self.particles, self.dim))\n        self.personal_best_position = np.copy(self.position)\n        self.global_best_position = None\n        self.personal_best_value = np.full(self.particles, np.inf)\n        self.global_best_value = np.inf\n        self.iteration = 0\n\n    def __call__(self, func):\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        scale = ub - lb\n\n        def quantum_update():\n            for i in range(self.particles):\n                delta = np.random.uniform(-1, 1, self.dim)\n                potential_position = self.position[i] + self.velocity[i] * delta\n                potential_position = np.clip(potential_position, 0, 1)\n                real_position = lb + potential_position * scale\n                value = func(real_position)\n                if value < self.personal_best_value[i]:\n                    self.personal_best_value[i] = value\n                    self.personal_best_position[i] = potential_position\n                if value < self.global_best_value:\n                    self.global_best_value = value\n                    self.global_best_position = potential_position\n\n        while self.iteration < self.budget:\n            quantum_update()\n            inertia_weight = 0.8 - 0.6 * (self.iteration / self.budget)  # Adaptive inertia weight\n            for i in range(self.particles):\n                cognitive = np.random.random(self.dim)\n                social = np.random.random(self.dim)\n                random_factor = np.random.uniform(0.7, 1.3, self.dim)  # Broadened random factor range\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive * (self.personal_best_position[i] - self.position[i]) +\n                                    social * (self.global_best_position - self.position[i])) * random_factor\n                self.position[i] += self.velocity[i]  # Removed fixed scaling factor\n                self.position[i] = np.clip(self.position[i], 0, 1)\n            self.iteration += self.particles\n\n        return lb + self.global_best_position * scale", "name": "QIPSO", "description": "Adaptive velocity scaling and dynamic particle influence to enhance convergence in diverse landscapes.", "configspace": "", "generation": 67, "fitness": 0.29091378842178395, "feedback": "The algorithm QIPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.29 with standard deviation 0.00.", "error": "", "parent_id": "fb7f0647-ced4-416e-a7a5-13ca06a50538", "metadata": {"aucs": [0.29091378842178395]}, "mutation_prompt": null}
{"id": "6cd255a7-6497-487d-a22b-ffdb34c9f17b", "solution": "import numpy as np\n\nclass QIPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particles = min(100, max(10, dim * 2))  # Dynamic particle count\n        self.position = np.random.uniform(0, 1, (self.particles, self.dim))\n        self.velocity = np.zeros((self.particles, self.dim))\n        self.personal_best_position = np.copy(self.position)\n        self.global_best_position = None\n        self.personal_best_value = np.full(self.particles, np.inf)\n        self.global_best_value = np.inf\n        self.iteration = 0\n\n    def __call__(self, func):\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        scale = ub - lb\n\n        def quantum_update():\n            for i in range(self.particles):\n                delta = np.random.uniform(-1, 1, self.dim)\n                potential_position = self.position[i] + self.velocity[i] * delta\n                potential_position = np.clip(potential_position, 0, 1)\n                real_position = lb + potential_position * scale\n                value = func(real_position)\n                if value < self.personal_best_value[i]:\n                    self.personal_best_value[i] = value\n                    self.personal_best_position[i] = potential_position\n                if value < self.global_best_value:\n                    self.global_best_value = value\n                    self.global_best_position = potential_position\n\n        while self.iteration < self.budget:\n            quantum_update()\n            inertia_weight = 0.9 - 0.5 * (self.iteration / self.budget)  # Adaptive inertia weight\n            for i in range(self.particles):\n                cognitive = np.random.random(self.dim)\n                social = np.random.random(self.dim)\n                random_factor = np.random.uniform(0.8, 1.2, self.dim)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +  # Adjusted inertia weight usage\n                                    cognitive * (self.personal_best_position[i] - self.position[i]) +\n                                    social * (self.global_best_position - self.position[i])) * random_factor\n                self.position[i] += 0.95 * self.velocity[i]  # Enhanced velocity scaling\n                self.position[i] = np.clip(self.position[i], 0, 1)\n            self.iteration += self.particles\n\n        return lb + self.global_best_position * scale", "name": "QIPSO", "description": "Introduced adaptive inertia weight to balance exploration and exploitation dynamically.", "configspace": "", "generation": 68, "fitness": 0.2897368238764767, "feedback": "The algorithm QIPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.29 with standard deviation 0.00.", "error": "", "parent_id": "fb7f0647-ced4-416e-a7a5-13ca06a50538", "metadata": {"aucs": [0.2897368238764767]}, "mutation_prompt": null}
{"id": "d0d4c7e9-6579-4982-9efc-1032b328a289", "solution": "import numpy as np\n\nclass QIPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particles = min(100, max(10, dim * 2))  # Dynamic particle count\n        self.position = np.random.uniform(0, 1, (self.particles, self.dim))\n        self.velocity = np.zeros((self.particles, self.dim))\n        self.personal_best_position = np.copy(self.position)\n        self.global_best_position = None\n        self.personal_best_value = np.full(self.particles, np.inf)\n        self.global_best_value = np.inf\n        self.iteration = 0\n\n    def __call__(self, func):\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        scale = ub - lb\n\n        def quantum_update():\n            for i in range(self.particles):\n                delta = np.random.uniform(-1, 1, self.dim)\n                potential_position = self.position[i] + self.velocity[i] * delta\n                potential_position = np.clip(potential_position, 0, 1)\n                real_position = lb + potential_position * scale\n                value = func(real_position)\n                if value < self.personal_best_value[i]:\n                    self.personal_best_value[i] = value\n                    self.personal_best_position[i] = potential_position\n                if value < self.global_best_value:\n                    self.global_best_value = value\n                    self.global_best_position = potential_position\n\n        while self.iteration < self.budget:\n            quantum_update()\n            inertia_weight = 0.9 - (0.5 * (self.iteration / self.budget))  # Adaptive inertia weight\n            for i in range(self.particles):\n                cognitive = np.random.random(self.dim)\n                social = np.random.random(self.dim)\n                random_factor = np.random.uniform(0.8, 1.2, self.dim)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive * (self.personal_best_position[i] - self.position[i]) +\n                                    social * (self.global_best_position - self.position[i])) * random_factor\n                self.position[i] += 0.95 * self.velocity[i]  # Enhanced velocity scaling\n                self.position[i] = np.clip(self.position[i], 0, 1)\n            self.iteration += self.particles\n\n        return lb + self.global_best_position * scale", "name": "QIPSO", "description": "Introduce adaptive inertia weight to balance exploration and exploitation dynamically.", "configspace": "", "generation": 69, "fitness": 0.2897368238764767, "feedback": "The algorithm QIPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.29 with standard deviation 0.00.", "error": "", "parent_id": "fb7f0647-ced4-416e-a7a5-13ca06a50538", "metadata": {"aucs": [0.2897368238764767]}, "mutation_prompt": null}
{"id": "45d5a7f0-c388-4836-b726-608869f87af4", "solution": "import numpy as np\n\nclass QIPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particles = min(100, max(10, dim * 2))  # Dynamic particle count\n        self.position = np.random.uniform(0, 1, (self.particles, self.dim))\n        self.velocity = np.zeros((self.particles, self.dim))\n        self.personal_best_position = np.copy(self.position)\n        self.global_best_position = None\n        self.personal_best_value = np.full(self.particles, np.inf)\n        self.global_best_value = np.inf\n        self.iteration = 0\n\n    def __call__(self, func):\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        scale = ub - lb\n\n        def quantum_update():\n            for i in range(self.particles):\n                delta = np.random.uniform(-1, 1, self.dim)\n                potential_position = self.position[i] + self.velocity[i] * delta\n                potential_position = np.clip(potential_position, 0, 1)\n                real_position = lb + potential_position * scale\n                value = func(real_position)\n                if value < self.personal_best_value[i]:\n                    self.personal_best_value[i] = value\n                    self.personal_best_position[i] = potential_position\n                if value < self.global_best_value:\n                    self.global_best_value = value\n                    self.global_best_position = potential_position\n\n        while self.iteration < self.budget:\n            quantum_update()\n            inertia_weight = 0.9 - 0.5 * (self.iteration / self.budget)  # Adaptive inertia weight\n            for i in range(self.particles):\n                cognitive = np.random.random(self.dim)\n                social = np.random.random(self.dim)\n                random_factor = np.random.uniform(0.8, 1.2, self.dim)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive * (self.personal_best_position[i] - self.position[i]) +\n                                    social * (self.global_best_position - self.position[i])) * random_factor\n                self.position[i] += self.velocity[i]  # Modified exploitation-exploration balance\n                self.position[i] = np.clip(self.position[i], 0, 1)\n            self.iteration += self.particles\n\n        return lb + self.global_best_position * scale", "name": "QIPSO", "description": "Introduced adaptive inertia weight and exploitation-exploration balance for improved convergence.", "configspace": "", "generation": 70, "fitness": 0.28905215492199454, "feedback": "The algorithm QIPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.29 with standard deviation 0.00.", "error": "", "parent_id": "fb7f0647-ced4-416e-a7a5-13ca06a50538", "metadata": {"aucs": [0.28905215492199454]}, "mutation_prompt": null}
{"id": "0fe66e53-f617-4689-b10d-e6897fc2e694", "solution": "import numpy as np\n\nclass QIPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particles = min(100, max(10, dim * 2))  # Dynamic particle count\n        self.position = np.random.uniform(0, 1, (self.particles, self.dim))\n        self.velocity = np.zeros((self.particles, self.dim))\n        self.personal_best_position = np.copy(self.position)\n        self.global_best_position = None\n        self.personal_best_value = np.full(self.particles, np.inf)\n        self.global_best_value = np.inf\n        self.iteration = 0\n\n    def __call__(self, func):\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        scale = ub - lb\n\n        def quantum_update():\n            for i in range(self.particles):\n                delta = np.random.uniform(-1, 1, self.dim)\n                potential_position = self.position[i] + self.velocity[i] * delta\n                adaptive_factor = np.random.uniform(0.9, 1.1)  # Adaptive scaling\n                potential_position = np.clip(potential_position, 0, 1)\n                real_position = lb + potential_position * scale\n                value = func(real_position)\n                if value < self.personal_best_value[i]:\n                    self.personal_best_value[i] = value\n                    self.personal_best_position[i] = potential_position\n                if value < self.global_best_value:\n                    self.global_best_value = value\n                    self.global_best_position = potential_position\n\n        while self.iteration < self.budget:\n            quantum_update()\n            for i in range(self.particles):\n                cognitive = np.random.random(self.dim)\n                social = np.random.random(self.dim)\n                random_factor = np.random.uniform(0.8, 1.2, self.dim)\n                self.velocity[i] = (0.5 * self.velocity[i] +\n                                    cognitive * (self.personal_best_position[i] - self.position[i]) +\n                                    social * (self.global_best_position - self.position[i])) * random_factor\n                self.position[i] += adaptive_factor * self.velocity[i]  # Adaptive scaling\n                self.position[i] = np.clip(self.position[i], 0, 1)\n            self.iteration += self.particles\n\n        return lb + self.global_best_position * scale", "name": "QIPSO", "description": "Improved convergence by adaptive quantum tunneling and modified scaling.", "configspace": "", "generation": 71, "fitness": -Infinity, "feedback": "An exception occurred: NameError(\"name 'adaptive_factor' is not defined\").", "error": "NameError(\"name 'adaptive_factor' is not defined\")", "parent_id": "fb7f0647-ced4-416e-a7a5-13ca06a50538", "metadata": {}, "mutation_prompt": null}
{"id": "9a40862c-69b1-4a5b-a2fb-8684d6ad5289", "solution": "import numpy as np\n\nclass QIPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particles = min(100, max(10, dim * 2))  # Dynamic particle count\n        self.position = np.random.uniform(0, 1, (self.particles, self.dim))\n        self.velocity = np.zeros((self.particles, self.dim))\n        self.personal_best_position = np.copy(self.position)\n        self.global_best_position = None\n        self.personal_best_value = np.full(self.particles, np.inf)\n        self.global_best_value = np.inf\n        self.iteration = 0\n\n    def __call__(self, func):\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        scale = ub - lb\n\n        def quantum_update():\n            for i in range(self.particles):\n                delta = np.random.uniform(-1, 1, self.dim)\n                potential_position = self.position[i] + self.velocity[i] * delta\n                potential_position = np.clip(potential_position, 0, 1)\n                real_position = lb + potential_position * scale\n                value = func(real_position)\n                if value < self.personal_best_value[i]:\n                    self.personal_best_value[i] = value\n                    self.personal_best_position[i] = potential_position\n                if value < self.global_best_value:\n                    self.global_best_value = value\n                    self.global_best_position = potential_position\n\n        while self.iteration < self.budget:\n            quantum_update()\n            for i in range(self.particles):\n                cognitive = np.random.random(self.dim)\n                social = np.random.random(self.dim)\n                random_factor = np.random.uniform(0.8, 1.2, self.dim)\n                self.velocity[i] = (0.5 * self.velocity[i] +\n                                    cognitive * (self.personal_best_position[i] - self.position[i]) +\n                                    social * (self.global_best_position - self.position[i])) * random_factor\n                self.position[i] += (0.9 + 0.1 * np.sin(self.iteration/10)) * self.velocity[i]  # Adaptive velocity scaling\n                self.position[i] = np.clip(self.position[i], 0, 1)\n            self.iteration += self.particles\n\n        return lb + self.global_best_position * scale", "name": "QIPSO", "description": "Introduce adaptive velocity scaling to enhance convergence adaptability.", "configspace": "", "generation": 72, "fitness": 0.2924178639704914, "feedback": "The algorithm QIPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.29 with standard deviation 0.00.", "error": "", "parent_id": "fb7f0647-ced4-416e-a7a5-13ca06a50538", "metadata": {"aucs": [0.2924178639704914]}, "mutation_prompt": null}
{"id": "f08c7089-2b59-4c31-9e27-7ee598e65cbb", "solution": "import numpy as np\n\nclass QIPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particles = min(100, max(10, dim * 2))\n        self.position = np.random.uniform(0, 1, (self.particles, self.dim))\n        self.velocity = np.zeros((self.particles, self.dim))\n        self.personal_best_position = np.copy(self.position)\n        self.global_best_position = None\n        self.personal_best_value = np.full(self.particles, np.inf)\n        self.global_best_value = np.inf\n        self.iteration = 0\n\n    def __call__(self, func):\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        scale = ub - lb\n\n        def quantum_update():\n            for i in range(self.particles):\n                delta = np.random.uniform(-1, 1, self.dim)\n                potential_position = self.position[i] + self.velocity[i] * delta\n                potential_position = np.clip(potential_position, 0, 1)\n                real_position = lb + potential_position * scale\n                value = func(real_position)\n                if value < self.personal_best_value[i]:\n                    self.personal_best_value[i] = value\n                    self.personal_best_position[i] = potential_position\n                if value < self.global_best_value:\n                    self.global_best_value = value\n                    self.global_best_position = potential_position\n\n        while self.iteration < self.budget:\n            quantum_update()\n            for i in range(self.particles):\n                inertia_weight = 0.9 - (self.iteration / self.budget) * 0.5  # Adaptive inertia weight\n                cognitive = np.random.random(self.dim)\n                social = np.random.random(self.dim)\n                random_factor = np.random.uniform(0.8, 1.2, self.dim)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive * (self.personal_best_position[i] - self.position[i]) +\n                                    social * (self.global_best_position - self.position[i])) * random_factor\n                self.position[i] += self.velocity[i]  # Remove enhanced scaling\n                self.position[i] = np.clip(self.position[i], 0, 1)\n            self.iteration += self.particles\n            if self.iteration % (self.budget // 4) == 0:  # Dynamic particle count adjustment\n                self.particles = min(self.particles + 5, 100)\n\n        return lb + self.global_best_position * scale", "name": "QIPSO", "description": "Introduce adaptive inertia weight and dynamic particle count adjustment based on iterations to improve exploration-exploitation balance.", "configspace": "", "generation": 73, "fitness": -Infinity, "feedback": "An exception occurred: IndexError('index 20 is out of bounds for axis 0 with size 20').", "error": "IndexError('index 20 is out of bounds for axis 0 with size 20')", "parent_id": "fb7f0647-ced4-416e-a7a5-13ca06a50538", "metadata": {}, "mutation_prompt": null}
{"id": "511586b4-b145-4421-980b-04c60023ce0c", "solution": "import numpy as np\n\nclass QIPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particles = min(100, max(10, dim * 2))\n        self.position = np.random.uniform(0, 1, (self.particles, self.dim))\n        self.velocity = np.zeros((self.particles, self.dim))\n        self.personal_best_position = np.copy(self.position)\n        self.global_best_position = None\n        self.personal_best_value = np.full(self.particles, np.inf)\n        self.global_best_value = np.inf\n        self.iteration = 0\n\n    def __call__(self, func):\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        scale = ub - lb\n\n        def quantum_update():\n            for i in range(self.particles):\n                delta = np.random.uniform(-1, 1, self.dim)\n                potential_position = self.position[i] + self.velocity[i] * delta\n                potential_position = np.clip(potential_position, 0, 1)\n                real_position = lb + potential_position * scale\n                value = func(real_position)\n                if value < self.personal_best_value[i]:\n                    self.personal_best_value[i] = value\n                    self.personal_best_position[i] = potential_position\n                if value < self.global_best_value:\n                    self.global_best_value = value\n                    self.global_best_position = potential_position\n\n        while self.iteration < self.budget:\n            quantum_update()\n            inertia_weight = 0.9 - (0.5 * (self.iteration / self.budget))  # Adaptive inertia\n            chaos_factor = 0.5 + 0.5 * np.sin(3.14 * self.iteration / self.budget)  # Chaos-based factor\n            for i in range(self.particles):\n                cognitive = np.random.random(self.dim)\n                social = np.random.random(self.dim)\n                random_factor = np.random.uniform(0.8, 1.2, self.dim)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive * (self.personal_best_position[i] - self.position[i]) +\n                                    social * (self.global_best_position - self.position[i])) * random_factor\n                self.position[i] += chaos_factor * self.velocity[i]  # Adjusted velocity update\n                self.position[i] = np.clip(self.position[i], 0, 1)\n            self.iteration += self.particles\n\n        return lb + self.global_best_position * scale", "name": "QIPSO", "description": "Introduce adaptive inertia and chaos-based velocity update to balance exploration and exploitation.", "configspace": "", "generation": 74, "fitness": 0.288864744787551, "feedback": "The algorithm QIPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.29 with standard deviation 0.00.", "error": "", "parent_id": "fb7f0647-ced4-416e-a7a5-13ca06a50538", "metadata": {"aucs": [0.288864744787551]}, "mutation_prompt": null}
{"id": "66e8f284-412d-4416-9d1f-3c5b299ca610", "solution": "import numpy as np\n\nclass QIPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particles = min(100, max(10, dim * 2))  # Dynamic particle count\n        self.position = np.random.uniform(0, 1, (self.particles, self.dim))\n        self.velocity = np.zeros((self.particles, self.dim))\n        self.personal_best_position = np.copy(self.position)\n        self.global_best_position = None\n        self.personal_best_value = np.full(self.particles, np.inf)\n        self.global_best_value = np.inf\n        self.iteration = 0\n\n    def __call__(self, func):\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        scale = ub - lb\n\n        def quantum_update():\n            for i in range(self.particles):\n                delta = np.random.uniform(-1, 1, self.dim)\n                potential_position = self.position[i] + self.velocity[i] * delta\n                potential_position = np.clip(potential_position, 0, 1)\n                real_position = lb + potential_position * scale\n                value = func(real_position)\n                if value < self.personal_best_value[i]:\n                    self.personal_best_value[i] = value\n                    self.personal_best_position[i] = potential_position\n                if value < self.global_best_value:\n                    self.global_best_value = value\n                    self.global_best_position = potential_position\n\n        while self.iteration < self.budget:\n            quantum_update()\n            inertia_weight = 0.9 - (0.5 * self.iteration / self.budget)  # Dynamic inertia\n            for i in range(self.particles):\n                cognitive = np.random.random(self.dim)\n                social = np.random.random(self.dim)\n                random_factor = np.random.uniform(0.8, 1.2, self.dim)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    2.0 * cognitive * (self.personal_best_position[i] - self.position[i]) +\n                                    2.0 * social * (self.global_best_position - self.position[i])) * random_factor\n                self.position[i] += 0.95 * self.velocity[i]  # Enhanced velocity scaling\n                self.position[i] = np.clip(self.position[i], 0, 1)\n            self.iteration += self.particles\n\n        return lb + self.global_best_position * scale", "name": "QIPSO", "description": "Introduced a dynamic inertia weight and adaptive learning factors to balance exploration and exploitation.", "configspace": "", "generation": 75, "fitness": 0.28560321541929934, "feedback": "The algorithm QIPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.29 with standard deviation 0.00.", "error": "", "parent_id": "fb7f0647-ced4-416e-a7a5-13ca06a50538", "metadata": {"aucs": [0.28560321541929934]}, "mutation_prompt": null}
{"id": "2d6bf250-c871-4ee9-8845-fa57b98faac5", "solution": "import numpy as np\n\nclass QIPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particles = min(100, max(10, dim * 2))  # Dynamic particle count\n        self.position = np.random.uniform(0, 1, (self.particles, self.dim))\n        self.velocity = np.zeros((self.particles, self.dim))\n        self.personal_best_position = np.copy(self.position)\n        self.global_best_position = None\n        self.personal_best_value = np.full(self.particles, np.inf)\n        self.global_best_value = np.inf\n        self.iteration = 0\n\n    def __call__(self, func):\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        scale = ub - lb\n\n        def quantum_update():\n            for i in range(self.particles):\n                delta = np.random.uniform(-1, 1, self.dim)\n                potential_position = self.position[i] + self.velocity[i] * delta\n                potential_position = np.clip(potential_position, 0, 1)\n                real_position = lb + potential_position * scale\n                value = func(real_position)\n                if value < self.personal_best_value[i]:\n                    self.personal_best_value[i] = value\n                    self.personal_best_position[i] = potential_position\n                if value < self.global_best_value:\n                    self.global_best_value = value\n                    self.global_best_position = potential_position\n\n        while self.iteration < self.budget:\n            quantum_update()\n            inertia_weight = 0.9 - 0.5 * (self.iteration / self.budget)  # Adaptive inertia weight\n            for i in range(self.particles):\n                cognitive = np.random.random(self.dim)\n                social = np.random.random(self.dim)\n                random_factor = np.random.uniform(0.8, 1.2, self.dim)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +  # Adaptive inertia applied\n                                    cognitive * (self.personal_best_position[i] - self.position[i]) +\n                                    social * (self.global_best_position - self.position[i])) * random_factor\n                self.position[i] += 0.95 * self.velocity[i]  # Enhanced velocity scaling\n                self.position[i] = np.clip(self.position[i], 0, 1)\n            self.iteration += self.particles\n\n        return lb + self.global_best_position * scale", "name": "QIPSO", "description": "Introduce adaptive inertia weight to enhance convergence and exploration balance.", "configspace": "", "generation": 76, "fitness": 0.2897368238764767, "feedback": "The algorithm QIPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.29 with standard deviation 0.00.", "error": "", "parent_id": "fb7f0647-ced4-416e-a7a5-13ca06a50538", "metadata": {"aucs": [0.2897368238764767]}, "mutation_prompt": null}
{"id": "6092af2a-3295-4dce-ab26-ae3b7e969fda", "solution": "import numpy as np\n\nclass QIPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particles = min(100, max(10, dim * 2))  # Dynamic particle count\n        self.position = np.random.uniform(0, 1, (self.particles, self.dim))\n        self.velocity = np.zeros((self.particles, self.dim))\n        self.personal_best_position = np.copy(self.position)\n        self.global_best_position = None\n        self.personal_best_value = np.full(self.particles, np.inf)\n        self.global_best_value = np.inf\n        self.iteration = 0\n\n    def __call__(self, func):\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        scale = ub - lb\n\n        def quantum_update():\n            for i in range(self.particles):\n                delta = np.random.uniform(-1, 1, self.dim)\n                potential_position = self.position[i] + self.velocity[i] * delta\n                potential_position = np.clip(potential_position, 0, 1)\n                real_position = lb + potential_position * scale\n                value = func(real_position)\n                if value < self.personal_best_value[i]:\n                    self.personal_best_value[i] = value\n                    self.personal_best_position[i] = potential_position\n                if value < self.global_best_value:\n                    self.global_best_value = value\n                    self.global_best_position = potential_position\n\n        while self.iteration < self.budget:\n            quantum_update()\n            inertia = 0.9 - (0.5 * self.iteration / self.budget)  # Adaptive inertia weight\n            for i in range(self.particles):\n                cognitive = np.random.random(self.dim)\n                social = np.random.random(self.dim)\n                random_factor = np.random.uniform(0.8, 1.2, self.dim)\n                self.velocity[i] = (inertia * self.velocity[i] +\n                                    cognitive * (self.personal_best_position[i] - self.position[i]) +\n                                    social * (self.global_best_position - self.position[i])) * random_factor\n                self.position[i] += 0.95 * self.velocity[i]\n                self.position[i] = np.clip(self.position[i], 0.1, 0.9)  # Adjusted boundary clipping for exploration\n            self.iteration += self.particles\n\n        return lb + self.global_best_position * scale", "name": "QIPSO", "description": "Introduced adaptive inertia weights and boundary adjustments for enhanced diversity and exploration.", "configspace": "", "generation": 77, "fitness": 0.2875933212249254, "feedback": "The algorithm QIPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.29 with standard deviation 0.00.", "error": "", "parent_id": "fb7f0647-ced4-416e-a7a5-13ca06a50538", "metadata": {"aucs": [0.2875933212249254]}, "mutation_prompt": null}
{"id": "26bda604-8d68-4e9a-adac-d7de7ffb9e21", "solution": "import numpy as np\n\nclass QIPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particles = min(100, max(10, dim * 2))  # Dynamic particle count\n        self.position = np.random.uniform(0, 1, (self.particles, self.dim))\n        self.velocity = np.zeros((self.particles, self.dim))\n        self.personal_best_position = np.copy(self.position)\n        self.global_best_position = None\n        self.personal_best_value = np.full(self.particles, np.inf)\n        self.global_best_value = np.inf\n        self.iteration = 0\n\n    def __call__(self, func):\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        scale = ub - lb\n\n        def quantum_update():\n            for i in range(self.particles):\n                delta = np.random.uniform(-1, 1, self.dim)\n                potential_position = self.position[i] + self.velocity[i] * delta\n                potential_position = np.clip(potential_position, 0, 1)\n                real_position = lb + potential_position * scale\n                value = func(real_position)\n                if value < self.personal_best_value[i]:\n                    self.personal_best_value[i] = value\n                    self.personal_best_position[i] = potential_position\n                if value < self.global_best_value:\n                    self.global_best_value = value\n                    self.global_best_position = potential_position\n\n        while self.iteration < self.budget:\n            inertia_weight = 0.9 - (0.5 * self.iteration / self.budget)  # Adaptive inertia weight\n            quantum_update()\n            for i in range(self.particles):\n                cognitive = np.random.random(self.dim)\n                social = np.random.random(self.dim)\n                random_factor = np.random.uniform(0.8, 1.2, self.dim)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive * (self.personal_best_position[i] - self.position[i]) +\n                                    social * (self.global_best_position - self.position[i])) * random_factor\n                self.position[i] += 0.95 * self.velocity[i]  # Enhanced velocity scaling\n                self.position[i] = np.clip(self.position[i], 0, 1)\n            self.iteration += self.particles\n\n        return lb + self.global_best_position * scale", "name": "QIPSO", "description": "Introducing adaptive inertia weight to balance exploration and exploitation in QIPSO.", "configspace": "", "generation": 78, "fitness": 0.2897368238764767, "feedback": "The algorithm QIPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.29 with standard deviation 0.00.", "error": "", "parent_id": "fb7f0647-ced4-416e-a7a5-13ca06a50538", "metadata": {"aucs": [0.2897368238764767]}, "mutation_prompt": null}
{"id": "c6359edb-f6e0-46ce-a812-5658f3b7d586", "solution": "import numpy as np\n\nclass QIPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particles = min(100, max(10, dim * 2))  # Dynamic particle count\n        self.position = np.random.uniform(0, 1, (self.particles, self.dim))\n        self.velocity = np.zeros((self.particles, self.dim))\n        self.personal_best_position = np.copy(self.position)\n        self.global_best_position = None\n        self.personal_best_value = np.full(self.particles, np.inf)\n        self.global_best_value = np.inf\n        self.iteration = 0\n\n    def __call__(self, func):\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        scale = ub - lb\n\n        def quantum_update():\n            for i in range(self.particles):\n                delta = np.random.uniform(-1, 1, self.dim)\n                potential_position = self.position[i] + self.velocity[i] * delta\n                potential_position = np.clip(potential_position, 0, 1)\n                real_position = lb + potential_position * scale\n                value = func(real_position)\n                if value < self.personal_best_value[i]:\n                    self.personal_best_value[i] = value\n                    self.personal_best_position[i] = potential_position\n                if value < self.global_best_value:\n                    self.global_best_value = value\n                    self.global_best_position = potential_position\n\n        stagnation_counter = 0  # New line: Counter for stagnation\n        while self.iteration < self.budget:\n            quantum_update()\n            for i in range(self.particles):\n                cognitive = np.random.random(self.dim)\n                social = np.random.random(self.dim)\n                random_factor = np.random.uniform(0.8, 1.2, self.dim)\n                if np.random.random() < 0.1:  # Modified line: Random direction change\n                    self.velocity[i] = np.random.uniform(-1, 1, self.dim)  # Completely random direction\n                else:\n                    self.velocity[i] = (0.5 * self.velocity[i] +\n                                        cognitive * (self.personal_best_position[i] - self.position[i]) +\n                                        social * (self.global_best_position - self.position[i])) * random_factor\n                self.position[i] += 0.95 * self.velocity[i]  # Enhanced velocity scaling\n                self.position[i] = np.clip(self.position[i], 0, 1)\n            if np.allclose(self.global_best_value, self.personal_best_value, atol=1e-5):  # New line: Stagnation check\n                stagnation_counter += 1\n                if stagnation_counter > 5:  # New line: Reset positions if stagnation persists\n                    self.position = np.random.uniform(0, 1, (self.particles, self.dim))\n                    stagnation_counter = 0\n            self.iteration += self.particles\n\n        return lb + self.global_best_position * scale", "name": "QIPSO", "description": "Introduce adaptive velocity direction and stagnation handling to enhance exploration and convergence.", "configspace": "", "generation": 79, "fitness": 0.28908116162130804, "feedback": "The algorithm QIPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.29 with standard deviation 0.00.", "error": "", "parent_id": "fb7f0647-ced4-416e-a7a5-13ca06a50538", "metadata": {"aucs": [0.28908116162130804]}, "mutation_prompt": null}
{"id": "ea468d4d-1615-4536-9564-816469b2f952", "solution": "import numpy as np\n\nclass QIPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particles = min(100, max(10, dim * 2))  # Dynamic particle count\n        self.position = np.random.uniform(0, 1, (self.particles, self.dim))\n        self.velocity = np.zeros((self.particles, self.dim))\n        self.personal_best_position = np.copy(self.position)\n        self.global_best_position = None\n        self.personal_best_value = np.full(self.particles, np.inf)\n        self.global_best_value = np.inf\n        self.iteration = 0\n\n    def __call__(self, func):\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        scale = ub - lb\n\n        def quantum_update():\n            for i in range(self.particles):\n                delta = np.random.uniform(-1, 1, self.dim)\n                potential_position = self.position[i] + self.velocity[i] * delta\n                potential_position = np.clip(potential_position, 0, 1)\n                real_position = lb + potential_position * scale\n                value = func(real_position)\n                if value < self.personal_best_value[i]:\n                    self.personal_best_value[i] = value\n                    self.personal_best_position[i] = potential_position\n                if value < self.global_best_value:\n                    self.global_best_value = value\n                    self.global_best_position = potential_position\n\n        while self.iteration < self.budget:\n            quantum_update()\n            inertia_weight = 0.9 - 0.8 * (self.iteration / self.budget)  # Adaptive inertia weight\n            for i in range(self.particles):\n                cognitive = np.random.random(self.dim)\n                social = np.random.random(self.dim)\n                random_factor = np.random.uniform(0.8, 1.2, self.dim)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive * (self.personal_best_position[i] - self.position[i]) +\n                                    social * (self.global_best_position - self.position[i])) * random_factor\n                self.position[i] += 0.95 * self.velocity[i]  # Enhanced velocity scaling\n                self.position[i] = np.clip(self.position[i], 0, 1)\n            self.iteration += self.particles\n\n        return lb + self.global_best_position * scale", "name": "QIPSO", "description": "Introduce adaptive inertia weight for dynamic balance between exploration and exploitation.", "configspace": "", "generation": 80, "fitness": 0.2900586010440238, "feedback": "The algorithm QIPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.29 with standard deviation 0.00.", "error": "", "parent_id": "fb7f0647-ced4-416e-a7a5-13ca06a50538", "metadata": {"aucs": [0.2900586010440238]}, "mutation_prompt": null}
{"id": "d8598065-3559-41c8-963e-f477f7cb4ef6", "solution": "import numpy as np\n\nclass QIPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particles = min(100, max(10, dim * 2))  # Dynamic particle count\n        self.position = np.random.uniform(0, 1, (self.particles, self.dim))\n        self.velocity = np.zeros((self.particles, self.dim))\n        self.personal_best_position = np.copy(self.position)\n        self.global_best_position = None\n        self.personal_best_value = np.full(self.particles, np.inf)\n        self.global_best_value = np.inf\n        self.iteration = 0\n\n    def __call__(self, func):\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        scale = ub - lb\n\n        def quantum_update():\n            for i in range(self.particles):\n                delta = np.random.uniform(-1, 1, self.dim)\n                potential_position = self.position[i] + self.velocity[i] * delta\n                potential_position = np.clip(potential_position, 0, 1)\n                real_position = lb + potential_position * scale\n                value = func(real_position)\n                if value < self.personal_best_value[i]:\n                    self.personal_best_value[i] = value\n                    self.personal_best_position[i] = potential_position\n                if value < self.global_best_value:\n                    self.global_best_value = value\n                    self.global_best_position = potential_position\n\n        while self.iteration < self.budget:\n            quantum_update()\n            inertia_weight = 0.9 - 0.5 * (self.iteration / self.budget)  # Adaptive inertia weight\n            for i in range(self.particles):\n                cognitive = np.random.random(self.dim)\n                social = np.random.random(self.dim)\n                random_factor = np.random.uniform(0.8, 1.2, self.dim)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +  # Modified line\n                                    cognitive * (self.personal_best_position[i] - self.position[i]) +\n                                    social * (self.global_best_position - self.position[i])) * random_factor\n                perturbation = np.random.normal(0, 0.1, self.dim)  # New perturbation factor\n                self.position[i] += 0.95 * self.velocity[i] + perturbation  # Enhanced velocity scaling with perturbation\n                self.position[i] = np.clip(self.position[i], 0, 1)\n            self.iteration += self.particles\n\n        return lb + self.global_best_position * scale", "name": "QIPSO", "description": "Introduce adaptive inertia weight and perturbation factor to enhance exploration and exploitation balance.", "configspace": "", "generation": 81, "fitness": 0.2868297847454735, "feedback": "The algorithm QIPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.29 with standard deviation 0.00.", "error": "", "parent_id": "fb7f0647-ced4-416e-a7a5-13ca06a50538", "metadata": {"aucs": [0.2868297847454735]}, "mutation_prompt": null}
{"id": "5943bc76-fd7c-49f0-9b3b-ee9680a1deea", "solution": "import numpy as np\n\nclass QIPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particles = min(100, max(10, dim * 2))  # Dynamic particle count\n        self.position = np.random.uniform(0, 1, (self.particles, self.dim))\n        self.velocity = np.zeros((self.particles, self.dim))\n        self.personal_best_position = np.copy(self.position)\n        self.global_best_position = None\n        self.personal_best_value = np.full(self.particles, np.inf)\n        self.global_best_value = np.inf\n        self.iteration = 0\n\n    def __call__(self, func):\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        scale = ub - lb\n\n        def quantum_update():\n            for i in range(self.particles):\n                delta = np.random.uniform(-1, 1, self.dim)\n                potential_position = self.position[i] + self.velocity[i] * delta\n                potential_position = np.clip(potential_position, 0, 1)\n                real_position = lb + potential_position * scale\n                value = func(real_position)\n                if value < self.personal_best_value[i]:\n                    self.personal_best_value[i] = value\n                    self.personal_best_position[i] = potential_position\n                if value < self.global_best_value:\n                    self.global_best_value = value\n                    self.global_best_position = potential_position\n\n        while self.iteration < self.budget:\n            quantum_update()\n            for i in range(self.particles):\n                cognitive = np.random.random(self.dim)\n                social = np.random.random(self.dim)\n                random_factor = np.random.uniform(0.8, 1.2, self.dim)\n                inertia_weight = 0.9 - 0.5 * (self.iteration / self.budget)  # Adaptive inertia weight\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive * (self.personal_best_position[i] - self.position[i]) +\n                                    social * (self.global_best_position - self.position[i])) * random_factor\n                self.position[i] += 0.95 * self.velocity[i]  # Enhanced velocity scaling\n                self.position[i] = np.clip(self.position[i], 0, 1)\n            self.iteration += self.particles\n\n        return lb + self.global_best_position * scale", "name": "QIPSO", "description": "Introduce adaptive inertia weight for better exploration and exploitation balance.", "configspace": "", "generation": 82, "fitness": 0.2897368238764767, "feedback": "The algorithm QIPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.29 with standard deviation 0.00.", "error": "", "parent_id": "fb7f0647-ced4-416e-a7a5-13ca06a50538", "metadata": {"aucs": [0.2897368238764767]}, "mutation_prompt": null}
{"id": "2cd38c61-0227-4aa0-89c6-b8b3c5239d44", "solution": "import numpy as np\n\nclass QIPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particles = min(100, max(10, dim * 2))  # Dynamic particle count\n        self.position = np.random.uniform(0, 1, (self.particles, self.dim))\n        self.velocity = np.zeros((self.particles, self.dim))\n        self.personal_best_position = np.copy(self.position)\n        self.global_best_position = None\n        self.personal_best_value = np.full(self.particles, np.inf)\n        self.global_best_value = np.inf\n        self.iteration = 0\n\n    def __call__(self, func):\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        scale = ub - lb\n\n        def quantum_update():\n            for i in range(self.particles):\n                delta = np.random.uniform(-1, 1, self.dim)\n                potential_position = self.position[i] + self.velocity[i] * delta\n                potential_position = np.clip(potential_position, 0, 1)\n                real_position = lb + potential_position * scale\n                value = func(real_position)\n                if value < self.personal_best_value[i]:\n                    self.personal_best_value[i] = value\n                    self.personal_best_position[i] = potential_position\n                if value < self.global_best_value:\n                    self.global_best_value = value\n                    self.global_best_position = potential_position\n\n        def levy_flight(Lambda=1.5):  # Added Lévy flight mechanism\n            sigma1 = np.power((np.math.gamma(1 + Lambda) * np.sin(np.pi * Lambda / 2)) /\n                              (np.math.gamma((1 + Lambda) / 2) * Lambda * np.power(2, (Lambda - 1) / 2)), 1 / Lambda)\n            return np.random.normal(0, sigma1, size=self.dim)\n\n        while self.iteration < self.budget:\n            quantum_update()\n            for i in range(self.particles):\n                cognitive = np.random.random(self.dim)\n                social = np.random.random(self.dim)\n                random_factor = np.random.uniform(0.8, 1.2, self.dim)\n                self.velocity[i] = (0.5 * self.velocity[i] +\n                                    cognitive * (self.personal_best_position[i] - self.position[i]) +\n                                    social * (self.global_best_position - self.position[i])) * random_factor\n                self.position[i] += 0.95 * self.velocity[i] + levy_flight()  # Apply Lévy flight for exploration\n                self.position[i] = np.clip(self.position[i], 0, 1)\n            self.iteration += self.particles\n\n        return lb + self.global_best_position * scale", "name": "QIPSO", "description": "Introduced a Lévy flight mechanism to enhance exploration in the search space.", "configspace": "", "generation": 83, "fitness": 0.2766166352461188, "feedback": "The algorithm QIPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28 with standard deviation 0.00.", "error": "", "parent_id": "fb7f0647-ced4-416e-a7a5-13ca06a50538", "metadata": {"aucs": [0.2766166352461188]}, "mutation_prompt": null}
{"id": "e7e73c90-a998-46e3-8bde-b72bc02cba52", "solution": "import numpy as np\n\nclass QIPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particles = min(100, max(10, dim * 2))  # Dynamic particle count\n        self.position = np.random.uniform(0, 1, (self.particles, self.dim))\n        self.velocity = np.zeros((self.particles, self.dim))\n        self.personal_best_position = np.copy(self.position)\n        self.global_best_position = None\n        self.personal_best_value = np.full(self.particles, np.inf)\n        self.global_best_value = np.inf\n        self.iteration = 0\n\n    def __call__(self, func):\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        scale = ub - lb\n\n        def quantum_update():\n            for i in range(self.particles):\n                delta = np.random.uniform(-1, 1, self.dim)\n                potential_position = self.position[i] + self.velocity[i] * delta\n                potential_position = np.clip(potential_position, 0, 1)\n                real_position = lb + potential_position * scale\n                value = func(real_position)\n                if value < self.personal_best_value[i]:\n                    self.personal_best_value[i] = value\n                    self.personal_best_position[i] = potential_position\n                if value < self.global_best_value:\n                    self.global_best_value = value\n                    self.global_best_position = potential_position\n\n        while self.iteration < self.budget:\n            quantum_update()\n            inertia_weight = 0.9 - 0.5 * (self.iteration / self.budget)  # Adaptive inertia\n            for i in range(self.particles):\n                cognitive = np.random.random(self.dim)\n                social = np.random.random(self.dim)\n                neighborhood_best_position = self.personal_best_position[np.random.randint(self.particles)]\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive * (self.personal_best_position[i] - self.position[i]) +\n                                    social * (neighborhood_best_position - self.position[i]))  # Neighborhood attraction\n                self.position[i] += 0.95 * self.velocity[i]  # Enhanced velocity scaling\n                self.position[i] = np.clip(self.position[i], 0, 1)\n            self.iteration += self.particles\n\n        return lb + self.global_best_position * scale", "name": "QIPSO", "description": "Introduce adaptive inertia weight and dynamic neighborhood attraction to balance exploration and exploitation.", "configspace": "", "generation": 84, "fitness": 0.2803764378053384, "feedback": "The algorithm QIPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28 with standard deviation 0.00.", "error": "", "parent_id": "fb7f0647-ced4-416e-a7a5-13ca06a50538", "metadata": {"aucs": [0.2803764378053384]}, "mutation_prompt": null}
{"id": "40419250-1fc2-4884-9624-8ec96e3b4655", "solution": "import numpy as np\n\nclass QIPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particles = min(100, max(10, dim * 2))  # Dynamic particle count\n        self.position = np.random.uniform(0, 1, (self.particles, self.dim))\n        self.velocity = np.zeros((self.particles, self.dim))\n        self.personal_best_position = np.copy(self.position)\n        self.global_best_position = None\n        self.personal_best_value = np.full(self.particles, np.inf)\n        self.global_best_value = np.inf\n        self.iteration = 0\n\n    def __call__(self, func):\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        scale = ub - lb\n\n        def quantum_update():\n            for i in range(self.particles):\n                delta = np.random.uniform(-1, 1, self.dim)\n                potential_position = self.position[i] + self.velocity[i] * delta\n                potential_position = np.clip(potential_position, 0, 1)\n                real_position = lb + potential_position * scale\n                value = func(real_position)\n                if value < self.personal_best_value[i]:\n                    self.personal_best_value[i] = value\n                    self.personal_best_position[i] = potential_position\n                if value < self.global_best_value:\n                    self.global_best_value = value\n                    self.global_best_position = potential_position\n\n        while self.iteration < self.budget:\n            quantum_update()\n            for i in range(self.particles):\n                cognitive = np.random.random(self.dim)\n                social = np.random.random(self.dim)\n                random_factor = np.random.uniform(0.8, 1.2, self.dim)\n                self.velocity[i] = (0.5 * self.velocity[i] +\n                                    cognitive * (self.personal_best_position[i] - self.position[i]) +\n                                    1.5 * social * (self.global_best_position - self.position[i])) * random_factor  # Adjusted scaling\n                self.position[i] += 0.95 * self.velocity[i]  # Enhanced velocity scaling\n                self.position[i] = np.clip(self.position[i], 0, 1)\n            self.iteration += self.particles\n\n        return lb + self.global_best_position * scale", "name": "QIPSO", "description": "Adjusted social component scaling to enhance exploration without compromising convergence speed.", "configspace": "", "generation": 85, "fitness": 0.2878617303059542, "feedback": "The algorithm QIPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.29 with standard deviation 0.00.", "error": "", "parent_id": "fb7f0647-ced4-416e-a7a5-13ca06a50538", "metadata": {"aucs": [0.2878617303059542]}, "mutation_prompt": null}
{"id": "0b5e7d41-306c-4eee-87a2-2ad122189cf0", "solution": "import numpy as np\n\nclass QIPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particles = min(100, max(10, dim * 2))  # Dynamic particle count\n        self.position = np.random.uniform(0, 1, (self.particles, self.dim))\n        self.velocity = np.zeros((self.particles, self.dim))\n        self.personal_best_position = np.copy(self.position)\n        self.global_best_position = None\n        self.personal_best_value = np.full(self.particles, np.inf)\n        self.global_best_value = np.inf\n        self.iteration = 0\n\n    def __call__(self, func):\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        scale = ub - lb\n\n        def quantum_update():\n            for i in range(self.particles):\n                delta = np.random.uniform(-1, 1, self.dim)\n                potential_position = self.position[i] + self.velocity[i] * delta\n                potential_position = np.clip(potential_position, 0, 1)\n                real_position = lb + potential_position * scale\n                value = func(real_position)\n                if value < self.personal_best_value[i]:\n                    self.personal_best_value[i] = value\n                    self.personal_best_position[i] = potential_position\n                if value < self.global_best_value:\n                    self.global_best_value = value\n                    self.global_best_position = potential_position\n\n        while self.iteration < self.budget:\n            quantum_update()\n            inertia_weight = 0.9 - (self.iteration / self.budget) * 0.5  # Adaptive inertia weight\n            for i in range(self.particles):\n                cognitive = np.random.random(self.dim)\n                social = np.random.random(self.dim) * (1 + np.exp(-self.iteration / self.budget))  # Dynamic social influence\n                random_factor = np.random.uniform(0.8, 1.2, self.dim)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +  # Use adaptive inertia weight\n                                    cognitive * (self.personal_best_position[i] - self.position[i]) +\n                                    social * (self.global_best_position - self.position[i])) * random_factor\n                self.position[i] += 0.95 * self.velocity[i]  # Enhanced velocity scaling\n                self.position[i] = np.clip(self.position[i], 0, 1)\n            self.iteration += self.particles\n\n        return lb + self.global_best_position * scale", "name": "QIPSO", "description": "Introduced adaptive inertia weight and dynamic social influence to enhance convergence.", "configspace": "", "generation": 86, "fitness": 0.2887597004298822, "feedback": "The algorithm QIPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.29 with standard deviation 0.00.", "error": "", "parent_id": "fb7f0647-ced4-416e-a7a5-13ca06a50538", "metadata": {"aucs": [0.2887597004298822]}, "mutation_prompt": null}
{"id": "7371f2d1-3700-43ef-a5bb-3ebd25a56d01", "solution": "import numpy as np\n\nclass QIPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particles = min(100, max(10, dim * 2))  # Dynamic particle count\n        self.position = np.random.uniform(0, 1, (self.particles, self.dim))\n        self.velocity = np.zeros((self.particles, self.dim))\n        self.personal_best_position = np.copy(self.position)\n        self.global_best_position = None\n        self.personal_best_value = np.full(self.particles, np.inf)\n        self.global_best_value = np.inf\n        self.iteration = 0\n\n    def __call__(self, func):\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        scale = ub - lb\n\n        def quantum_update():\n            for i in range(self.particles):\n                delta = np.random.uniform(-1, 1, self.dim)\n                potential_position = self.position[i] + self.velocity[i] * delta\n                potential_position = np.clip(potential_position, 0, 1)\n                real_position = lb + potential_position * scale\n                value = func(real_position)\n                if value < self.personal_best_value[i]:\n                    self.personal_best_value[i] = value\n                    self.personal_best_position[i] = potential_position\n                if value < self.global_best_value:\n                    self.global_best_value = value\n                    self.global_best_position = potential_position\n\n        while self.iteration < self.budget:\n            quantum_update()\n            for i in range(self.particles):\n                cognitive = np.random.random(self.dim)\n                social = np.random.random(self.dim)\n                random_factor = np.random.uniform(0.8, 1.2, self.dim)\n                self.velocity[i] = (0.5 * self.velocity[i] +\n                                    cognitive * (self.personal_best_position[i] - self.position[i]) +\n                                    social * (self.global_best_position - self.position[i])) * random_factor\n                self.position[i] += self.velocity[i] * (0.9 + 0.1 * np.random.random())  # Adaptive velocity scaling\n                self.position[i] = np.clip(self.position[i], 0, 1)\n            self.iteration += self.particles\n\n        return lb + self.global_best_position * scale", "name": "QIPSO", "description": "Introduced adaptive velocity scaling to exploit exploration-exploitation balance in dynamic environments.", "configspace": "", "generation": 87, "fitness": 0.292591695923779, "feedback": "The algorithm QIPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.29 with standard deviation 0.00.", "error": "", "parent_id": "fb7f0647-ced4-416e-a7a5-13ca06a50538", "metadata": {"aucs": [0.292591695923779]}, "mutation_prompt": null}
{"id": "1609b1e8-c8d9-4dff-ad99-406582944aaa", "solution": "import numpy as np\n\nclass QIPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particles = min(100, max(10, dim * 2))  # Dynamic particle count\n        self.position = np.random.uniform(0, 1, (self.particles, self.dim))\n        self.velocity = np.zeros((self.particles, self.dim))\n        self.personal_best_position = np.copy(self.position)\n        self.global_best_position = None\n        self.personal_best_value = np.full(self.particles, np.inf)\n        self.global_best_value = np.inf\n        self.iteration = 0\n\n    def __call__(self, func):\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        scale = ub - lb\n\n        def quantum_update():\n            for i in range(self.particles):\n                delta = np.random.uniform(-1, 1, self.dim)\n                potential_position = self.position[i] + self.velocity[i] * delta\n                potential_position = np.clip(potential_position, 0, 1)\n                real_position = lb + potential_position * scale\n                value = func(real_position)\n                if value < self.personal_best_value[i]:\n                    self.personal_best_value[i] = value\n                    self.personal_best_position[i] = potential_position\n                if value < self.global_best_value:\n                    self.global_best_value = value\n                    self.global_best_position = potential_position\n\n        while self.iteration < self.budget:\n            quantum_update()\n            w = 0.9 - 0.5 * (self.iteration / self.budget)  # Adaptive inertia weight\n            for i in range(self.particles):\n                cognitive = np.random.random(self.dim)\n                social = np.random.random(self.dim)\n                self.velocity[i] = (w * self.velocity[i] +\n                                    cognitive * (self.personal_best_position[i] - self.position[i]) +\n                                    social * (self.global_best_position - self.position[i]))\n                self.position[i] += self.velocity[i] * np.sin(chaotic_map(self.iteration))  # Chaotic mapping\n                self.position[i] = np.clip(self.position[i], 0, 1)\n            self.iteration += self.particles\n\n        return lb + self.global_best_position * scale\n\ndef chaotic_map(iteration):\n    r = 3.9  # Logistic map constant for chaos\n    return r * iteration * (1 - iteration)", "name": "QIPSO", "description": "Introduce adaptive inertia weight and chaotic mapping to enhance exploration and convergence.", "configspace": "", "generation": 88, "fitness": 0.28150666985475525, "feedback": "The algorithm QIPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28 with standard deviation 0.00.", "error": "", "parent_id": "fb7f0647-ced4-416e-a7a5-13ca06a50538", "metadata": {"aucs": [0.28150666985475525]}, "mutation_prompt": null}
{"id": "f2519711-95cb-4949-8c87-0fcb735a540b", "solution": "import numpy as np\n\nclass QIPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particles = min(100, max(10, dim * 2))  # Dynamic particle count\n        self.position = np.random.uniform(0, 1, (self.particles, self.dim))\n        self.velocity = np.zeros((self.particles, self.dim))\n        self.personal_best_position = np.copy(self.position)\n        self.global_best_position = None\n        self.personal_best_value = np.full(self.particles, np.inf)\n        self.global_best_value = np.inf\n        self.iteration = 0\n\n    def __call__(self, func):\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        scale = ub - lb\n\n        def quantum_update():\n            for i in range(self.particles):\n                delta = np.random.uniform(-1, 1, self.dim)\n                potential_position = self.position[i] + self.velocity[i] * delta\n                potential_position = np.clip(potential_position, 0, 1)\n                real_position = lb + potential_position * scale\n                value = func(real_position)\n                if value < self.personal_best_value[i]:\n                    self.personal_best_value[i] = value\n                    self.personal_best_position[i] = potential_position\n                if value < self.global_best_value:\n                    self.global_best_value = value\n                    self.global_best_position = potential_position\n\n        while self.iteration < self.budget:\n            inertia_weight = 0.9 - (0.5 * self.iteration / self.budget)  # Dynamic inertia weight\n            quantum_update()\n            for i in range(self.particles):\n                cognitive = np.random.random(self.dim)\n                social = np.random.random(self.dim)\n                random_factor = np.random.uniform(0.8, 1.2, self.dim)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive * (self.personal_best_position[i] - self.position[i]) +\n                                    social * (self.global_best_position - self.position[i])) * random_factor\n                self.position[i] += 0.95 * self.velocity[i]  # Enhanced velocity scaling\n                self.position[i] = np.clip(self.position[i], 0, 1)\n            self.iteration += self.particles\n\n        return lb + self.global_best_position * scale", "name": "QIPSO", "description": "QIPSO with dynamic inertia weight for adaptive exploration and exploitation.", "configspace": "", "generation": 89, "fitness": 0.2897368238764767, "feedback": "The algorithm QIPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.29 with standard deviation 0.00.", "error": "", "parent_id": "fb7f0647-ced4-416e-a7a5-13ca06a50538", "metadata": {"aucs": [0.2897368238764767]}, "mutation_prompt": null}
{"id": "d1a7948b-be25-4341-92a9-9a7c51ff80e7", "solution": "import numpy as np\n\nclass QIPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particles = min(100, max(10, dim * 2))  # Dynamic particle count\n        self.position = np.random.uniform(0, 1, (self.particles, self.dim))\n        self.velocity = np.zeros((self.particles, self.dim))\n        self.personal_best_position = np.copy(self.position)\n        self.global_best_position = None\n        self.personal_best_value = np.full(self.particles, np.inf)\n        self.global_best_value = np.inf\n        self.iteration = 0\n\n    def __call__(self, func):\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        scale = ub - lb\n\n        def quantum_update():\n            for i in range(self.particles):\n                delta = np.random.uniform(-0.5, 0.5, self.dim)  # Enhanced local exploration\n                potential_position = self.position[i] + self.velocity[i] * delta\n                potential_position = np.clip(potential_position, 0, 1)\n                real_position = lb + potential_position * scale\n                value = func(real_position)\n                if value < self.personal_best_value[i]:\n                    self.personal_best_value[i] = value\n                    self.personal_best_position[i] = potential_position\n                if value < self.global_best_value:\n                    self.global_best_value = value\n                    self.global_best_position = potential_position\n\n        inertia_weight = 0.9  # Adaptive inertia weight\n        while self.iteration < self.budget:\n            quantum_update()\n            for i in range(self.particles):\n                cognitive = np.random.random(self.dim)\n                social = np.random.random(self.dim)\n                random_factor = np.random.uniform(0.8, 1.2, self.dim)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive * (self.personal_best_position[i] - self.position[i]) +\n                                    social * (self.global_best_position - self.position[i])) * random_factor\n                self.position[i] += self.velocity[i]  # Removed enhanced velocity scaling\n                self.position[i] = np.clip(self.position[i], 0, 1)\n            self.iteration += self.particles\n            inertia_weight -= 0.5 / self.budget  # Decreasing inertia weight\n\n        return lb + self.global_best_position * scale", "name": "QIPSO", "description": "Introduced adaptive inertia weight and enhanced local exploration to improve convergence speed and accuracy.", "configspace": "", "generation": 90, "fitness": 0.28789176621965895, "feedback": "The algorithm QIPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.29 with standard deviation 0.00.", "error": "", "parent_id": "fb7f0647-ced4-416e-a7a5-13ca06a50538", "metadata": {"aucs": [0.28789176621965895]}, "mutation_prompt": null}
{"id": "ccb8d2a5-42a9-4003-b11b-dab4d63fad81", "solution": "import numpy as np\n\nclass QIPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particles = min(100, max(10, dim * 2))  # Dynamic particle count\n        self.position = np.random.uniform(0, 1, (self.particles, self.dim))\n        self.velocity = np.zeros((self.particles, self.dim))\n        self.personal_best_position = np.copy(self.position)\n        self.global_best_position = None\n        self.personal_best_value = np.full(self.particles, np.inf)\n        self.global_best_value = np.inf\n        self.iteration = 0\n\n    def __call__(self, func):\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        scale = ub - lb\n\n        def quantum_update():\n            for i in range(self.particles):\n                delta = np.random.uniform(-1, 1, self.dim)\n                potential_position = self.position[i] + self.velocity[i] * delta\n                potential_position = np.clip(potential_position, 0, 1)\n                real_position = lb + potential_position * scale\n                value = func(real_position)\n                if value < self.personal_best_value[i]:\n                    self.personal_best_value[i] = value\n                    self.personal_best_position[i] = potential_position\n                if value < self.global_best_value:\n                    self.global_best_value = value\n                    self.global_best_position = potential_position\n\n        while self.iteration < self.budget:\n            quantum_update()\n            inertia_weight = 0.9 - (self.iteration / self.budget) * 0.5  # Adaptive inertia weight\n            for i in range(self.particles):\n                cognitive = np.random.random(self.dim)\n                social = np.random.random(self.dim)\n                random_factor = np.random.uniform(0.9, 1.3, self.dim)  # Adjusted random factor range\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive * (self.personal_best_position[i] - self.position[i]) +\n                                    social * (self.global_best_position - self.position[i])) * random_factor\n                self.position[i] += self.velocity[i]  # Removed fixed scaling to allow dynamic adjustment\n                self.position[i] = np.clip(self.position[i], 0, 1)\n            self.iteration += self.particles\n\n        return lb + self.global_best_position * scale", "name": "QIPSO", "description": "Introduced adaptive velocity scaling and inertia weights to enhance exploration and exploitation balance.", "configspace": "", "generation": 91, "fitness": 0.2899647289335373, "feedback": "The algorithm QIPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.29 with standard deviation 0.00.", "error": "", "parent_id": "fb7f0647-ced4-416e-a7a5-13ca06a50538", "metadata": {"aucs": [0.2899647289335373]}, "mutation_prompt": null}
{"id": "0b8f86fe-e9f8-4b68-93e1-35148cfe77e1", "solution": "import numpy as np\n\nclass QIPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particles = min(100, max(10, dim * 2))  # Dynamic particle count\n        self.position = np.random.uniform(0, 1, (self.particles, self.dim))\n        self.velocity = np.zeros((self.particles, self.dim))\n        self.personal_best_position = np.copy(self.position)\n        self.global_best_position = None\n        self.personal_best_value = np.full(self.particles, np.inf)\n        self.global_best_value = np.inf\n        self.iteration = 0\n\n    def __call__(self, func):\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        scale = ub - lb\n\n        def quantum_update():\n            for i in range(self.particles):\n                delta = np.random.uniform(-1, 1, self.dim)\n                potential_position = self.position[i] + self.velocity[i] * delta\n                potential_position = np.clip(potential_position, 0, 1)\n                real_position = lb + potential_position * scale\n                value = func(real_position)\n                if value < self.personal_best_value[i]:\n                    self.personal_best_value[i] = value\n                    self.personal_best_position[i] = potential_position\n                if value < self.global_best_value:\n                    self.global_best_value = value\n                    self.global_best_position = potential_position\n\n        while self.iteration < self.budget:\n            quantum_update()\n            inertia_weight = 0.9 - 0.7 * (self.iteration / self.budget)  # Adaptive inertia weight\n            for i in range(self.particles):\n                cognitive = np.random.random(self.dim)\n                social = np.random.random(self.dim)\n                random_factor = np.random.uniform(0.8, 1.2, self.dim)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +  # Updated inertia weight usage\n                                    cognitive * (self.personal_best_position[i] - self.position[i]) +\n                                    social * (self.global_best_position - self.position[i])) * random_factor\n                self.position[i] += 0.9 * self.velocity[i]  # Updated velocity scaling\n                self.position[i] = np.clip(self.position[i], 0, 1)\n            self.iteration += self.particles\n\n        return lb + self.global_best_position * scale", "name": "QIPSO", "description": "Introduced adaptive inertia weight and updated velocity scaling for better convergence.", "configspace": "", "generation": 92, "fitness": 0.2908813233901588, "feedback": "The algorithm QIPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.29 with standard deviation 0.00.", "error": "", "parent_id": "fb7f0647-ced4-416e-a7a5-13ca06a50538", "metadata": {"aucs": [0.2908813233901588]}, "mutation_prompt": null}
{"id": "14c85dae-6f90-4227-aa69-be98e4051334", "solution": "import numpy as np\n\nclass QIPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particles = min(100, max(10, dim * 2))  # Dynamic particle count\n        self.position = np.random.uniform(0, 1, (self.particles, self.dim))\n        self.velocity = np.zeros((self.particles, self.dim))\n        self.personal_best_position = np.copy(self.position)\n        self.global_best_position = None\n        self.personal_best_value = np.full(self.particles, np.inf)\n        self.global_best_value = np.inf\n        self.iteration = 0\n\n    def __call__(self, func):\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        scale = ub - lb\n\n        def quantum_update():\n            for i in range(self.particles):\n                delta = np.random.uniform(-1, 1, self.dim)\n                potential_position = self.position[i] + self.velocity[i] * delta\n                potential_position = np.clip(potential_position, 0, 1)\n                real_position = lb + potential_position * scale\n                value = func(real_position)\n                if value < self.personal_best_value[i]:\n                    self.personal_best_value[i] = value\n                    self.personal_best_position[i] = potential_position\n                if value < self.global_best_value:\n                    self.global_best_value = value\n                    self.global_best_position = potential_position\n\n        while self.iteration < self.budget:\n            inertia_weight = 0.9 - (self.iteration / self.budget) * 0.5  # Adaptive inertia weight\n            quantum_update()\n            for i in range(self.particles):\n                cognitive = np.random.random(self.dim)\n                social = np.random.random(self.dim)\n                neighborhood = np.random.uniform(0, 1, self.dim)  # Stochastic neighborhood influence\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive * (self.personal_best_position[i] - self.position[i]) +\n                                    social * (self.global_best_position - self.position[i]) +\n                                    neighborhood * (self.global_best_position - self.position[i]))  # Added neighborhood influence\n                self.position[i] += 0.95 * self.velocity[i]  # Enhanced velocity scaling\n                self.position[i] = np.clip(self.position[i], 0, 1)\n            self.iteration += self.particles\n\n        return lb + self.global_best_position * scale", "name": "QIPSO", "description": "Introduced adaptive inertia weight and stochastic neighborhood influence to enhance convergence.", "configspace": "", "generation": 93, "fitness": 0.2897113656641095, "feedback": "The algorithm QIPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.29 with standard deviation 0.00.", "error": "", "parent_id": "fb7f0647-ced4-416e-a7a5-13ca06a50538", "metadata": {"aucs": [0.2897113656641095]}, "mutation_prompt": null}
{"id": "dd386ae8-2e95-444f-8d29-92cc231b7264", "solution": "import numpy as np\n\nclass QIPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particles = min(100, max(10, dim * 2))  # Dynamic particle count\n        self.position = np.random.uniform(0, 1, (self.particles, self.dim))\n        self.velocity = np.zeros((self.particles, self.dim))\n        self.personal_best_position = np.copy(self.position)\n        self.global_best_position = None\n        self.personal_best_value = np.full(self.particles, np.inf)\n        self.global_best_value = np.inf\n        self.iteration = 0\n\n    def __call__(self, func):\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        scale = ub - lb\n\n        def quantum_update():\n            for i in range(self.particles):\n                if np.random.rand() < 0.05:  # Stochastic position reset\n                    self.position[i] = np.random.uniform(0, 1, self.dim)\n                delta = np.random.uniform(-1, 1, self.dim)\n                potential_position = self.position[i] + self.velocity[i] * delta\n                potential_position = np.clip(potential_position, 0, 1)\n                real_position = lb + potential_position * scale\n                value = func(real_position)\n                if value < self.personal_best_value[i]:\n                    self.personal_best_value[i] = value\n                    self.personal_best_position[i] = potential_position\n                if value < self.global_best_value:\n                    self.global_best_value = value\n                    self.global_best_position = potential_position\n\n        while self.iteration < self.budget:\n            quantum_update()\n            inertia_weight = 0.9 - (0.5 * (self.iteration / self.budget))  # Adaptive inertia\n            for i in range(self.particles):\n                cognitive = np.random.random(self.dim)\n                social = np.random.random(self.dim)\n                random_factor = np.random.uniform(0.8, 1.2, self.dim)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive * (self.personal_best_position[i] - self.position[i]) +\n                                    social * (self.global_best_position - self.position[i])) * random_factor\n                self.position[i] += 0.95 * self.velocity[i]  # Enhanced velocity scaling\n                self.position[i] = np.clip(self.position[i], 0, 1)\n            self.iteration += self.particles\n\n        return lb + self.global_best_position * scale", "name": "QIPSO", "description": "Introduce adaptive inertia and stochastic position reset to enhance exploration and exploitation balance.", "configspace": "", "generation": 94, "fitness": 0.28701477765011774, "feedback": "The algorithm QIPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.29 with standard deviation 0.00.", "error": "", "parent_id": "fb7f0647-ced4-416e-a7a5-13ca06a50538", "metadata": {"aucs": [0.28701477765011774]}, "mutation_prompt": null}
{"id": "a59f3352-393b-4eb6-b109-61a0d03de157", "solution": "import numpy as np\n\nclass QIPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particles = min(100, max(10, dim * 2))  # Dynamic particle count\n        self.position = np.random.uniform(0, 1, (self.particles, self.dim))\n        self.velocity = np.zeros((self.particles, self.dim))\n        self.personal_best_position = np.copy(self.position)\n        self.global_best_position = None\n        self.personal_best_value = np.full(self.particles, np.inf)\n        self.global_best_value = np.inf\n        self.iteration = 0\n\n    def __call__(self, func):\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        scale = ub - lb\n\n        def quantum_update():\n            for i in range(self.particles):\n                delta = np.random.uniform(-1, 1, self.dim)\n                potential_position = self.position[i] + self.velocity[i] * delta\n                potential_position = np.clip(potential_position, 0, 1)\n                real_position = lb + potential_position * scale\n                value = func(real_position)\n                if value < self.personal_best_value[i]:\n                    self.personal_best_value[i] = value\n                    self.personal_best_position[i] = potential_position\n                if value < self.global_best_value:\n                    self.global_best_value = value\n                    self.global_best_position = potential_position\n\n        while self.iteration < self.budget:\n            quantum_update()\n            inertia_weight = 0.9 - (0.5 * self.iteration / self.budget)  # Adaptive inertia weight\n            for i in range(self.particles):\n                cognitive = np.random.random(self.dim)\n                social = np.random.random(self.dim)\n                random_factor = np.random.uniform(0.8, 1.2, self.dim)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +  # Modified line\n                                    cognitive * (self.personal_best_position[i] - self.position[i]) +\n                                    social * (self.global_best_position - self.position[i])) * random_factor\n                self.position[i] += 0.95 * self.velocity[i]  # Enhanced velocity scaling\n                self.position[i] = np.clip(self.position[i], 0, 1)\n                if np.random.random() < 0.1:  # Local search improvement\n                    local_search = np.random.uniform(-0.1, 0.1, self.dim)\n                    self.position[i] = np.clip(self.position[i] + local_search, 0, 1)\n            self.iteration += self.particles\n\n        return lb + self.global_best_position * scale", "name": "QIPSO", "description": "Introduce adaptive inertia and local search strategy to enhance exploration and exploitation balance.", "configspace": "", "generation": 95, "fitness": 0.28419663036522, "feedback": "The algorithm QIPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28 with standard deviation 0.00.", "error": "", "parent_id": "fb7f0647-ced4-416e-a7a5-13ca06a50538", "metadata": {"aucs": [0.28419663036522]}, "mutation_prompt": null}
{"id": "1c7ce727-f88f-4abf-bf49-75fa860a836e", "solution": "import numpy as np\n\nclass QIPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particles = min(100, max(10, dim * 2))  # Dynamic particle count\n        self.position = np.random.uniform(0, 1, (self.particles, self.dim))\n        self.velocity = np.zeros((self.particles, self.dim))\n        self.personal_best_position = np.copy(self.position)\n        self.global_best_position = None\n        self.personal_best_value = np.full(self.particles, np.inf)\n        self.global_best_value = np.inf\n        self.iteration = 0\n\n    def __call__(self, func):\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        scale = ub - lb\n\n        def quantum_update():\n            for i in range(self.particles):\n                delta = np.random.uniform(-1, 1, self.dim)\n                potential_position = self.position[i] + self.velocity[i] * delta\n                potential_position = np.clip(potential_position, 0, 1)\n                real_position = lb + potential_position * scale\n                value = func(real_position)\n                if value < self.personal_best_value[i]:\n                    self.personal_best_value[i] = value\n                    self.personal_best_position[i] = potential_position\n                if value < self.global_best_value:\n                    self.global_best_value = value\n                    self.global_best_position = potential_position\n\n        while self.iteration < self.budget:\n            w = 0.9 - 0.5 * (self.iteration / self.budget)  # Adaptive inertia weight\n            quantum_update()\n            for i in range(self.particles):\n                cognitive = np.random.random(self.dim)\n                social = np.random.random(self.dim)\n                random_factor = np.random.uniform(0.8, 1.2, self.dim)\n                self.velocity[i] = (w * self.velocity[i] +  # Applied adaptive inertia weight\n                                    cognitive * (self.personal_best_position[i] - self.position[i]) +\n                                    social * (self.global_best_position - self.position[i])) * random_factor\n                self.position[i] += 0.95 * self.velocity[i]  # Enhanced velocity scaling\n                self.position[i] = np.clip(self.position[i], 0, 1)\n            self.iteration += self.particles\n\n        return lb + self.global_best_position * scale", "name": "QIPSO", "description": "Integrate adaptive inertia weight for dynamic velocity adjustment.", "configspace": "", "generation": 96, "fitness": 0.2897368238764767, "feedback": "The algorithm QIPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.29 with standard deviation 0.00.", "error": "", "parent_id": "fb7f0647-ced4-416e-a7a5-13ca06a50538", "metadata": {"aucs": [0.2897368238764767]}, "mutation_prompt": null}
{"id": "d6698949-7a70-4e56-abdb-4c85125c2c77", "solution": "import numpy as np\n\nclass QIPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particles = min(100, max(10, dim * 2))  # Dynamic particle count\n        self.position = np.random.uniform(0, 1, (self.particles, self.dim))\n        self.velocity = np.zeros((self.particles, self.dim))\n        self.personal_best_position = np.copy(self.position)\n        self.global_best_position = None\n        self.personal_best_value = np.full(self.particles, np.inf)\n        self.global_best_value = np.inf\n        self.iteration = 0\n\n    def __call__(self, func):\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        scale = ub - lb\n\n        def quantum_update():\n            for i in range(self.particles):\n                delta = np.random.uniform(-1, 1, self.dim)\n                potential_position = self.position[i] + self.velocity[i] * delta\n                potential_position = np.clip(potential_position, 0, 1)\n                real_position = lb + potential_position * scale\n                value = func(real_position)\n                if value < self.personal_best_value[i]:\n                    self.personal_best_value[i] = value\n                    self.personal_best_position[i] = potential_position\n                if value < self.global_best_value:\n                    self.global_best_value = value\n                    self.global_best_position = potential_position\n\n        while self.iteration < self.budget:\n            quantum_update()\n            inertia_weight = 0.9 - (0.5 * self.iteration / self.budget)  # Adaptive inertia weight\n            for i in range(self.particles):\n                cognitive = np.random.random(self.dim)\n                social = np.random.random(self.dim)\n                random_factor = np.random.uniform(0.8, 1.2, self.dim)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +  # Modified inertia weight application\n                                    cognitive * (self.personal_best_position[i] - self.position[i]) +\n                                    social * (self.global_best_position - self.position[i])) * random_factor\n                self.position[i] += 0.95 * self.velocity[i]  # Enhanced velocity scaling\n                self.position[i] = np.clip(self.position[i], 0, 1)\n            self.iteration += self.particles\n\n        return lb + self.global_best_position * scale", "name": "QIPSO", "description": "Adaptive inertia weight enhances exploration-exploitation balance by dynamically adjusting based on iteration progress.", "configspace": "", "generation": 97, "fitness": 0.2897368238764767, "feedback": "The algorithm QIPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.29 with standard deviation 0.00.", "error": "", "parent_id": "fb7f0647-ced4-416e-a7a5-13ca06a50538", "metadata": {"aucs": [0.2897368238764767]}, "mutation_prompt": null}
{"id": "8f8b70d9-cb11-4dd9-86af-ac2f8d427431", "solution": "import numpy as np\n\nclass QIPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particles = min(100, max(10, dim * 2))  # Dynamic particle count\n        self.position = np.random.uniform(0, 1, (self.particles, self.dim))\n        self.velocity = np.zeros((self.particles, self.dim))\n        self.personal_best_position = np.copy(self.position)\n        self.global_best_position = None\n        self.personal_best_value = np.full(self.particles, np.inf)\n        self.global_best_value = np.inf\n        self.iteration = 0\n\n    def __call__(self, func):\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        scale = ub - lb\n\n        def quantum_update():\n            for i in range(self.particles):\n                delta = np.random.uniform(-1, 1, self.dim)\n                potential_position = self.position[i] + self.velocity[i] * delta\n                potential_position = np.clip(potential_position, 0, 1)\n                real_position = lb + potential_position * scale\n                value = func(real_position)\n                if value < self.personal_best_value[i]:\n                    self.personal_best_value[i] = value\n                    self.personal_best_position[i] = potential_position\n                if value < self.global_best_value:\n                    self.global_best_value = value\n                    self.global_best_position = potential_position\n\n        while self.iteration < self.budget:\n            quantum_update()\n            inertia_weight = 0.9 - 0.5 * (self.iteration / self.budget)  # Adaptive inertia weight\n            for i in range(self.particles):\n                cognitive = np.random.random(self.dim)\n                social = np.random.random(self.dim)\n                random_factor = np.random.uniform(0.8, 1.2, self.dim)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +  # Adaptive inertia weight applied\n                                    cognitive * (self.personal_best_position[i] - self.position[i]) +\n                                    social * (self.global_best_position - self.position[i])) * random_factor\n                self.position[i] += self.velocity[i]  # Removed the enhanced velocity scaling\n                self.position[i] = np.clip(self.position[i], 0, 1)\n            self.iteration += self.particles\n\n        return lb + self.global_best_position * scale", "name": "QIPSO", "description": "Introduced adaptive inertia weight and improved position update to enhance convergence and exploration.", "configspace": "", "generation": 98, "fitness": 0.28905215492199454, "feedback": "The algorithm QIPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.29 with standard deviation 0.00.", "error": "", "parent_id": "fb7f0647-ced4-416e-a7a5-13ca06a50538", "metadata": {"aucs": [0.28905215492199454]}, "mutation_prompt": null}
{"id": "48edc29a-f599-40c9-a16a-5bd34ccd4236", "solution": "import numpy as np\n\nclass QIPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particles = min(100, max(10, dim * 2))  # Dynamic particle count\n        self.position = np.random.uniform(0, 1, (self.particles, self.dim))\n        self.velocity = np.zeros((self.particles, self.dim))\n        self.personal_best_position = np.copy(self.position)\n        self.global_best_position = None\n        self.personal_best_value = np.full(self.particles, np.inf)\n        self.global_best_value = np.inf\n        self.iteration = 0\n\n    def __call__(self, func):\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        scale = ub - lb\n\n        def quantum_update():\n            for i in range(self.particles):\n                delta = np.random.uniform(-1, 1, self.dim)\n                potential_position = self.position[i] + self.velocity[i] * delta\n                potential_position = np.clip(potential_position, 0, 1)\n                real_position = lb + potential_position * scale\n                value = func(real_position)\n                if value < self.personal_best_value[i]:\n                    self.personal_best_value[i] = value\n                    self.personal_best_position[i] = potential_position\n                if value < self.global_best_value:\n                    self.global_best_value = value\n                    self.global_best_position = potential_position\n\n        while self.iteration < self.budget:\n            quantum_update()\n            inertia_weight = 0.9 - (0.9 - 0.4) * (self.iteration / self.budget)  # Adaptive inertia weight\n            for i in range(self.particles):\n                cognitive = np.random.random(self.dim)\n                social = np.random.random(self.dim)\n                random_factor = np.random.uniform(0.5, 1.5, self.dim)  # Adjusted random factor range\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive * (self.personal_best_position[i] - self.position[i]) +\n                                    social * (self.global_best_position - self.position[i])) * random_factor\n                self.position[i] += 0.95 * self.velocity[i]\n                self.position[i] = np.clip(self.position[i], 0, 1)\n            self.iteration += self.particles\n\n        return lb + self.global_best_position * scale", "name": "QIPSO", "description": "Adaptive inertia weight and dynamic velocity update to enhance exploration and exploitation balance.", "configspace": "", "generation": 99, "fitness": 0.28882017549729977, "feedback": "The algorithm QIPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.29 with standard deviation 0.00.", "error": "", "parent_id": "fb7f0647-ced4-416e-a7a5-13ca06a50538", "metadata": {"aucs": [0.28882017549729977]}, "mutation_prompt": null}
