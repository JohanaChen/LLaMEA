{"id": "f41e160a-479b-41a4-a520-2d3497890115", "solution": "import numpy as np\n\nclass HybridPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = min(50, budget // 10)  # Adaptive population size\n        self.w = 0.5  # Inertia weight\n        self.c1 = 1.5  # Cognitive coefficient\n        self.c2 = 1.5  # Social coefficient\n        self.mutation_factor = 0.8  # Differential mutation factor\n        self.best_global_position = None\n        self.best_global_fitness = float('inf')\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop_position = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        pop_velocity = np.random.uniform(-abs(ub-lb), abs(ub-lb), (self.pop_size, self.dim))\n        personal_best_positions = np.copy(pop_position)\n        personal_best_fitness = np.full(self.pop_size, float('inf'))\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            for i in range(self.pop_size):\n                fitness = func(pop_position[i])\n                evaluations += 1\n\n                if fitness < personal_best_fitness[i]:\n                    personal_best_fitness[i] = fitness\n                    personal_best_positions[i] = pop_position[i]\n\n                if fitness < self.best_global_fitness:\n                    self.best_global_fitness = fitness\n                    self.best_global_position = pop_position[i]\n\n                if evaluations >= self.budget:\n                    break\n\n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                cognitive_velocity = self.c1 * r1 * (personal_best_positions[i] - pop_position[i])\n                social_velocity = self.c2 * r2 * (self.best_global_position - pop_position[i])\n                pop_velocity[i] = self.w * pop_velocity[i] + cognitive_velocity + social_velocity\n\n                # Adaptive differential mutation\n                if np.random.rand() < 0.1:\n                    indices = np.random.choice(self.pop_size, 3, replace=False)\n                    donor_vector = (pop_position[indices[0]] +\n                                    self.mutation_factor * (pop_position[indices[1]] - pop_position[indices[2]]))\n                    np.clip(donor_vector, lb, ub, out=donor_vector)\n                    pop_position[i] = donor_vector\n                else:\n                    pop_position[i] += pop_velocity[i]\n                    np.clip(pop_position[i], lb, ub, out=pop_position[i])\n\n        return self.best_global_position, self.best_global_fitness", "name": "HybridPSO", "description": "A hybrid metaheuristic combining Particle Swarm Optimization (PSO) with adaptive differential mutation for enhanced exploration and exploitation in optimizing photonic structures.", "configspace": "", "generation": 0, "fitness": 0.2707118184287542, "feedback": "The algorithm HybridPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.27 with standard deviation 0.00.", "error": "", "parent_id": null, "metadata": {"aucs": [0.27094736091524774, 0.2687923352533351, 0.27239575911767966]}, "mutation_prompt": null}
{"id": "8c4ff505-7184-435a-ae89-4340e146f609", "solution": "import numpy as np\n\nclass EnhancedHybridPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = min(50, budget // 10)\n        self.w = 0.5\n        self.c1 = 1.5\n        self.c2 = 1.5\n        self.mutation_factor = 0.8\n        self.best_global_position = None\n        self.best_global_fitness = float('inf')\n\n    def levy_flight(self, L):\n        beta = 1.5\n        sigma = (np.gamma(1 + beta) * np.sin(np.pi * beta / 2) / \n                 (np.gamma((1 + beta) / 2) * beta * 2**((beta - 1) / 2)))**(1 / beta)\n        u = np.random.normal(0, sigma, size=self.dim)\n        v = np.random.normal(0, 1, size=self.dim)\n        step = u / np.abs(v)**(1 / beta)\n        return L * step\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop_position = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        pop_velocity = np.random.uniform(-abs(ub-lb), abs(ub-lb), (self.pop_size, self.dim))\n        personal_best_positions = np.copy(pop_position)\n        personal_best_fitness = np.full(self.pop_size, float('inf'))\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            for i in range(self.pop_size):\n                fitness = func(pop_position[i])\n                evaluations += 1\n\n                if fitness < personal_best_fitness[i]:\n                    personal_best_fitness[i] = fitness\n                    personal_best_positions[i] = pop_position[i]\n\n                if fitness < self.best_global_fitness:\n                    self.best_global_fitness = fitness\n                    self.best_global_position = pop_position[i]\n\n                if evaluations >= self.budget:\n                    break\n\n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                cognitive_velocity = self.c1 * r1 * (personal_best_positions[i] - pop_position[i])\n                social_velocity = self.c2 * r2 * (self.best_global_position - pop_position[i])\n                pop_velocity[i] = self.w * pop_velocity[i] + cognitive_velocity + social_velocity\n\n                if np.random.rand() < 0.1:\n                    indices = np.random.choice(self.pop_size, 3, replace=False)\n                    donor_vector = (pop_position[indices[0]] +\n                                    self.mutation_factor * (pop_position[indices[1]] - pop_position[indices[2]]))\n                    np.clip(donor_vector, lb, ub, out=donor_vector)\n                    pop_position[i] = donor_vector\n                else:\n                    pop_position[i] += pop_velocity[i]\n                    np.clip(pop_position[i], lb, ub, out=pop_position[i])\n\n                # Lévy flight step\n                if np.random.rand() < 0.05:\n                    levy_step = self.levy_flight(0.01 * (ub - lb))\n                    pop_position[i] += levy_step\n                    np.clip(pop_position[i], lb, ub, out=pop_position[i])\n\n        return self.best_global_position, self.best_global_fitness", "name": "EnhancedHybridPSO", "description": "An enhanced HybridPSO that integrates a Lévy flight mechanism for escaping local optima and improving convergence in photonic structure optimization.", "configspace": "", "generation": 1, "fitness": -Infinity, "feedback": "An exception occurred: AttributeError(\"module 'numpy' has no attribute 'gamma'\").", "error": "AttributeError(\"module 'numpy' has no attribute 'gamma'\")", "parent_id": "f41e160a-479b-41a4-a520-2d3497890115", "metadata": {}, "mutation_prompt": null}
{"id": "ab9b461f-147d-4567-a6f0-17fb6911d60c", "solution": "import numpy as np\n\nclass QuantumInspiredEA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = min(50, budget // 10)\n        self.alpha = 0.5  # Quantum rotation angle\n        self.best_fitness = float('inf')\n        self.best_position = None\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        # Initialize the quantum bit population in the superposition state\n        q_population = np.random.uniform(0, 1, (self.pop_size, self.dim, 2))\n        population = self.qbit_to_real(q_population, lb, ub)\n        fitness = np.full(self.pop_size, float('inf'))\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            # Evaluate fitness and update bests\n            for i in range(self.pop_size):\n                fitness[i] = func(population[i])\n                evaluations += 1\n\n                if fitness[i] < self.best_fitness:\n                    self.best_fitness = fitness[i]\n                    self.best_position = population[i]\n\n                if evaluations >= self.budget:\n                    break\n\n            # Update quantum particles\n            for i in range(self.pop_size):\n                for d in range(self.dim):\n                    theta = self.alpha * (fitness[i] - self.best_fitness)\n                    if np.random.rand() < 0.5:  # Quantum rotation operator\n                        q_population[i, d, 0] = q_population[i, d, 0] * np.cos(theta) - q_population[i, d, 1] * np.sin(theta)\n                        q_population[i, d, 1] = q_population[i, d, 0] * np.sin(theta) + q_population[i, d, 1] * np.cos(theta)\n                    else:\n                        q_population[i, d, 0] = q_population[i, d, 0] * np.cos(-theta) - q_population[i, d, 1] * np.sin(-theta)\n                        q_population[i, d, 1] = q_population[i, d, 0] * np.sin(-theta) + q_population[i, d, 1] * np.cos(-theta)\n\n            # Collapse quantum bits to generate new real-valued individuals\n            population = self.qbit_to_real(q_population, lb, ub)\n\n        return self.best_position, self.best_fitness\n\n    def qbit_to_real(self, q_population, lb, ub):\n        # Convert quantum bits to real values in the search space\n        real_population = np.zeros((self.pop_size, self.dim))\n        for i in range(self.pop_size):\n            for d in range(self.dim):\n                real_population[i, d] = lb[d] + (ub[d] - lb[d]) * (q_population[i, d, 0] ** 2 + q_population[i, d, 1] ** 2)\n        return real_population", "name": "QuantumInspiredEA", "description": "Quantum-Inspired Evolutionary Algorithm (QIEA) that leverages quantum bit representation and superposition for efficient exploration and exploitation in photonic structure optimization.", "configspace": "", "generation": 2, "fitness": 0.2344535825432398, "feedback": "The algorithm QuantumInspiredEA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.23 with standard deviation 0.01.", "error": "", "parent_id": "f41e160a-479b-41a4-a520-2d3497890115", "metadata": {"aucs": [0.2396521492211774, 0.22450849249238558, 0.23920010591615648]}, "mutation_prompt": null}
{"id": "ecd24d13-1d0d-4171-9235-54efafd28bce", "solution": "import numpy as np\n\nclass EnhancedHybridPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = min(50, budget // 10)  # Adaptive population size\n        self.c1 = 1.5  # Cognitive coefficient\n        self.c2 = 1.5  # Social coefficient\n        self.mutation_factor = 0.8  # Differential mutation factor\n        self.best_global_position = None\n        self.best_global_fitness = float('inf')\n        self.inertia_initial = 0.9\n        self.inertia_final = 0.4\n\n    def chaotic_initialization(self, lb, ub):\n        # Chaotic logistic map for initialization\n        chaotic_seq = np.zeros((self.pop_size, self.dim))\n        chaotic_seq[0] = np.random.rand(self.dim)\n        for i in range(1, self.pop_size):\n            chaotic_seq[i] = 4 * chaotic_seq[i - 1] * (1 - chaotic_seq[i - 1])\n        return lb + chaotic_seq * (ub - lb)\n\n    def dynamic_inertia(self, evaluations):\n        return self.inertia_final + (self.inertia_initial - self.inertia_final) * \\\n               ((self.budget - evaluations) / self.budget)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop_position = self.chaotic_initialization(lb, ub)\n        pop_velocity = np.random.uniform(-abs(ub-lb), abs(ub-lb), (self.pop_size, self.dim))\n        personal_best_positions = np.copy(pop_position)\n        personal_best_fitness = np.full(self.pop_size, float('inf'))\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            for i in range(self.pop_size):\n                fitness = func(pop_position[i])\n                evaluations += 1\n\n                if fitness < personal_best_fitness[i]:\n                    personal_best_fitness[i] = fitness\n                    personal_best_positions[i] = pop_position[i]\n\n                if fitness < self.best_global_fitness:\n                    self.best_global_fitness = fitness\n                    self.best_global_position = pop_position[i]\n\n                if evaluations >= self.budget:\n                    break\n\n            inertia_weight = self.dynamic_inertia(evaluations)\n\n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                cognitive_velocity = self.c1 * r1 * (personal_best_positions[i] - pop_position[i])\n                social_velocity = self.c2 * r2 * (self.best_global_position - pop_position[i])\n                pop_velocity[i] = inertia_weight * pop_velocity[i] + cognitive_velocity + social_velocity\n\n                # Adaptive differential mutation\n                if np.random.rand() < 0.1:\n                    indices = np.random.choice(self.pop_size, 3, replace=False)\n                    donor_vector = (pop_position[indices[0]] +\n                                    self.mutation_factor * (pop_position[indices[1]] - pop_position[indices[2]]))\n                    np.clip(donor_vector, lb, ub, out=donor_vector)\n                    pop_position[i] = donor_vector\n                else:\n                    pop_position[i] += pop_velocity[i]\n                    np.clip(pop_position[i], lb, ub, out=pop_position[i])\n\n        return self.best_global_position, self.best_global_fitness", "name": "EnhancedHybridPSO", "description": "An improved hybrid PSO integrating chaotic initialization for diverse exploration and dynamic adaptive inertia for balancing exploration-exploitation in photonic structures optimization.", "configspace": "", "generation": 3, "fitness": 0.2573598806552747, "feedback": "The algorithm EnhancedHybridPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.26 with standard deviation 0.00.", "error": "", "parent_id": "f41e160a-479b-41a4-a520-2d3497890115", "metadata": {"aucs": [0.2577201852398724, 0.2560385759825835, 0.25832088074336823]}, "mutation_prompt": null}
{"id": "1cdcd21b-cf9c-48cf-826d-58e38de726d9", "solution": "import numpy as np\n\nclass QuantumInspiredEA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = min(50, budget // 10)\n        self.q_population = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        self.best_solution = None\n        self.best_fitness = float('inf')\n        self.alpha = 0.01  # Learning rate for updating quantum bits\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        evaluations = 0\n        \n        while evaluations < self.budget:\n            positions = np.clip(self.quantum_to_real(self.q_population), lb, ub)\n            fitness_values = np.array([func(pos) for pos in positions])\n            evaluations += self.pop_size\n\n            min_idx = np.argmin(fitness_values)\n            \n            if fitness_values[min_idx] < self.best_fitness:\n                self.best_fitness = fitness_values[min_idx]\n                self.best_solution = positions[min_idx]\n            \n            mean_fitness = np.mean(fitness_values)\n            self.update_quantum_bits(fitness_values, mean_fitness)\n\n        return self.best_solution, self.best_fitness\n\n    def quantum_to_real(self, q_population):\n        return (ub + lb) / 2 + (ub - lb) / 2 * np.tanh(q_population)\n\n    def update_quantum_bits(self, fitness_values, mean_fitness):\n        for i in range(self.pop_size):\n            if fitness_values[i] < mean_fitness:\n                rotation_angle = self.alpha\n            else:\n                rotation_angle = -self.alpha\n\n            # Update quantum bits using a rotation gate concept\n            self.q_population[i] += rotation_angle * np.sign(self.q_population[i])\n            np.clip(self.q_population[i], -1, 1, out=self.q_population[i])", "name": "QuantumInspiredEA", "description": "Quantum-inspired Evolutionary Algorithm (QIEA) leveraging quantum bit representation and rotation gates for global optimization of photonic structures.", "configspace": "", "generation": 4, "fitness": -Infinity, "feedback": "An exception occurred: NameError(\"name 'ub' is not defined\").", "error": "NameError(\"name 'ub' is not defined\")", "parent_id": "f41e160a-479b-41a4-a520-2d3497890115", "metadata": {}, "mutation_prompt": null}
{"id": "5dcd7e14-3456-497b-818e-5cbfa32140dd", "solution": "import numpy as np\n\nclass AdaptiveQuantumSwarm:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = min(50, budget // 10)\n        self.alpha = 0.5  # Quantum probability control\n        self.beta = 0.1  # Local enhancement factor\n        self.best_global_position = None\n        self.best_global_fitness = float('inf')\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop_position = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        pop_fitness = np.full(self.pop_size, float('inf'))\n        \n        evaluations = 0\n\n        while evaluations < self.budget:\n            for i in range(self.pop_size):\n                fitness = func(pop_position[i])\n                evaluations += 1\n\n                if fitness < pop_fitness[i]:\n                    pop_fitness[i] = fitness\n\n                if fitness < self.best_global_fitness:\n                    self.best_global_fitness = fitness\n                    self.best_global_position = pop_position[i]\n\n                if evaluations >= self.budget:\n                    break\n\n            # Quantum-inspired update\n            for i in range(self.pop_size):\n                if np.random.rand() < self.alpha:\n                    # Generate superposition state\n                    superposed_state = np.random.uniform(lb, ub, self.dim)\n                    pop_position[i] = superposed_state\n                else:\n                    # Local enhancement using a Gaussian perturbation\n                    perturbation = np.random.normal(0, self.beta, self.dim)\n                    pop_position[i] += perturbation\n                    np.clip(pop_position[i], lb, ub, out=pop_position[i])\n\n        return self.best_global_position, self.best_global_fitness", "name": "AdaptiveQuantumSwarm", "description": "An Adaptive Quantum Swarm Optimization (AQSO) algorithm utilizing quantum-inspired superposition states and local enhancement for improved search efficiency in optimizing photonic structures.", "configspace": "", "generation": 5, "fitness": 0.25356211979798887, "feedback": "The algorithm AdaptiveQuantumSwarm got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.25 with standard deviation 0.00.", "error": "", "parent_id": "f41e160a-479b-41a4-a520-2d3497890115", "metadata": {"aucs": [0.25313640039689966, 0.25441284994961166, 0.25313710904745523]}, "mutation_prompt": null}
{"id": "1fdb012f-db81-4b31-8238-3a70eab74450", "solution": "import numpy as np\n\nclass QPDE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = min(50, budget // 10)  # Adaptive population size\n        self.w = 0.5  # Inertia weight\n        self.c1 = 1.5  # Cognitive coefficient\n        self.c2 = 1.5  # Social coefficient\n        self.mutation_factor = 0.8  # Differential mutation factor\n        self.best_global_position = None\n        self.best_global_fitness = float('inf')\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop_position = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        pop_velocity = np.random.uniform(-abs(ub-lb), abs(ub-lb), (self.pop_size, self.dim))\n        personal_best_positions = np.copy(pop_position)\n        personal_best_fitness = np.full(self.pop_size, float('inf'))\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            for i in range(self.pop_size):\n                fitness = func(pop_position[i])\n                evaluations += 1\n\n                if fitness < personal_best_fitness[i]:\n                    personal_best_fitness[i] = fitness\n                    personal_best_positions[i] = pop_position[i]\n\n                if fitness < self.best_global_fitness:\n                    self.best_global_fitness = fitness\n                    self.best_global_position = pop_position[i]\n\n                if evaluations >= self.budget:\n                    break\n\n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                cognitive_velocity = self.c1 * r1 * (personal_best_positions[i] - pop_position[i])\n                social_velocity = self.c2 * r2 * (self.best_global_position - pop_position[i])\n                pop_velocity[i] = self.w * pop_velocity[i] + cognitive_velocity + social_velocity\n\n                # Quantum behavior\n                if np.random.rand() < 0.3:\n                    theta = np.random.rand(self.dim) * (ub - lb) + lb\n                    quantum_position = self.best_global_position + 0.5 * (np.sin(theta) - 0.5)\n                    np.clip(quantum_position, lb, ub, out=quantum_position)\n                    pop_position[i] = quantum_position\n                else:\n                    # Differential mutation\n                    indices = np.random.choice(self.pop_size, 3, replace=False)\n                    donor_vector = (pop_position[indices[0]] +\n                                    self.mutation_factor * (pop_position[indices[1]] - pop_position[indices[2]]))\n                    np.clip(donor_vector, lb, ub, out=donor_vector)\n                    if np.random.rand() < 0.5:\n                        pop_position[i] = donor_vector\n                    else:\n                        pop_position[i] += pop_velocity[i]\n                        np.clip(pop_position[i], lb, ub, out=pop_position[i])\n\n        return self.best_global_position, self.best_global_fitness", "name": "QPDE", "description": "A novel Quantum Particles with Differential Evolution (QPDE) algorithm that integrates quantum behavior and differential evolution for efficient global exploration in photonic structure optimization.", "configspace": "", "generation": 6, "fitness": 0.260760527413628, "feedback": "The algorithm QPDE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.26 with standard deviation 0.01.", "error": "", "parent_id": "f41e160a-479b-41a4-a520-2d3497890115", "metadata": {"aucs": [0.25870020191476173, 0.25486494129071036, 0.26871643903541187]}, "mutation_prompt": null}
{"id": "e2a9f833-9b74-4860-936b-f1ba15aab50f", "solution": "import numpy as np\n\nclass QuantumInspiredOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = min(50, budget // 10)  # Adaptive population size\n        self.best_global_position = None\n        self.best_global_fitness = float('inf')\n        self.alpha = 0.8  # Influence coefficient of the best solution\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        # Initialize population in quantum states (superposition of lb and ub)\n        q_population = np.random.uniform(0, 1, (self.pop_size, self.dim))\n        # Classical positions derived from quantum states\n        pop_position = lb + (ub - lb) * q_population\n        personal_best_positions = np.copy(pop_position)\n        personal_best_fitness = np.full(self.pop_size, float('inf'))\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            for i in range(self.pop_size):\n                fitness = func(pop_position[i])\n                evaluations += 1\n\n                if fitness < personal_best_fitness[i]:\n                    personal_best_fitness[i] = fitness\n                    personal_best_positions[i] = pop_position[i]\n\n                if fitness < self.best_global_fitness:\n                    self.best_global_fitness = fitness\n                    self.best_global_position = pop_position[i]\n\n                if evaluations >= self.budget:\n                    break\n\n            # Update quantum states using best solutions\n            for i in range(self.pop_size):\n                # Quantum-inspired update mechanism\n                q_population[i] = self.alpha * q_population[i] + (1 - self.alpha) * np.random.rand(self.dim)\n                pop_position[i] = lb + (ub - lb) * q_population[i]\n\n                # Include a random entanglement-like behavior\n                if np.random.rand() < 0.1:\n                    indices = np.random.choice(self.pop_size, 2, replace=False)\n                    q_population[i] = (q_population[indices[0]] + q_population[indices[1]]) / 2\n\n        return self.best_global_position, self.best_global_fitness", "name": "QuantumInspiredOptimization", "description": "Quantum-inspired optimization algorithm leveraging quantum superposition and entanglement principles for enhanced exploration and exploitation in black box optimization.", "configspace": "", "generation": 7, "fitness": 0.274412529817285, "feedback": "The algorithm QuantumInspiredOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.27 with standard deviation 0.00.", "error": "", "parent_id": "f41e160a-479b-41a4-a520-2d3497890115", "metadata": {"aucs": [0.2754706540967069, 0.27382863130062585, 0.27393830405452213]}, "mutation_prompt": null}
{"id": "43a56bf1-0302-4fb6-b44e-e1a5e0961eb2", "solution": "import numpy as np\n\nclass EnhancedQuantumInspiredOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = min(50, budget // 10)\n        self.best_global_position = None\n        self.best_global_fitness = float('inf')\n        self.alpha = 0.8\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        q_population = np.random.uniform(0, 1, (self.pop_size, self.dim))\n        pop_position = lb + (ub - lb) * q_population\n        personal_best_positions = np.copy(pop_position)\n        personal_best_fitness = np.full(self.pop_size, float('inf'))\n\n        evaluations = 0\n        adapt_rate = 0.1  # Adaptive rate for entanglement\n\n        while evaluations < self.budget:\n            for i in range(self.pop_size):\n                fitness = func(pop_position[i])\n                evaluations += 1\n\n                if fitness < personal_best_fitness[i]:\n                    personal_best_fitness[i] = fitness\n                    personal_best_positions[i] = pop_position[i]\n\n                if fitness < self.best_global_fitness:\n                    self.best_global_fitness = fitness\n                    self.best_global_position = pop_position[i]\n\n                if evaluations >= self.budget:\n                    break\n\n            for i in range(self.pop_size):\n                q_population[i] = self.alpha * q_population[i] + (1 - self.alpha) * np.random.rand(self.dim)\n                pop_position[i] = lb + (ub - lb) * q_population[i]\n\n                if np.random.rand() < adapt_rate:\n                    indices = np.random.choice(self.pop_size, 3, replace=False)\n                    q_population[i] = (q_population[indices[0]] + q_population[indices[1]] + q_population[indices[2]]) / 3\n\n            adapt_rate = max(0.05, adapt_rate * 0.99)  # Gradually reduce adaptation rate\n\n        return self.best_global_position, self.best_global_fitness", "name": "EnhancedQuantumInspiredOptimization", "description": "An enhanced quantum-inspired algorithm incorporating adaptive quantum potential and dynamic entanglement to improve the exploration and exploitation balance in black box optimization.", "configspace": "", "generation": 8, "fitness": 0.2744613149312877, "feedback": "The algorithm EnhancedQuantumInspiredOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.27 with standard deviation 0.00.", "error": "", "parent_id": "e2a9f833-9b74-4860-936b-f1ba15aab50f", "metadata": {"aucs": [0.2745291828548596, 0.27451878086674775, 0.27433598107225576]}, "mutation_prompt": null}
{"id": "d2f62da0-2e6a-4954-b663-00802d3c143c", "solution": "import numpy as np\n\nclass CooperativeParticleSwarmOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = min(50, budget // 10)\n        self.best_global_position = None\n        self.best_global_fitness = float('inf')\n        self.inertia_weight = 0.7\n        self.cognitive_coeff = 1.5\n        self.social_coeff = 1.5\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        velocity = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        pop_position = lb + (ub - lb) * np.random.rand(self.pop_size, self.dim)\n        personal_best_positions = np.copy(pop_position)\n        personal_best_fitness = np.full(self.pop_size, float('inf'))\n\n        evaluations = 0\n        turbulence_intensity = 0.1  # Initial turbulence intensity\n\n        while evaluations < self.budget:\n            for i in range(self.pop_size):\n                fitness = func(pop_position[i])\n                evaluations += 1\n\n                if fitness < personal_best_fitness[i]:\n                    personal_best_fitness[i] = fitness\n                    personal_best_positions[i] = pop_position[i]\n\n                if fitness < self.best_global_fitness:\n                    self.best_global_fitness = fitness\n                    self.best_global_position = pop_position[i]\n\n                if evaluations >= self.budget:\n                    break\n\n            for i in range(self.pop_size):\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n\n                # Update velocity\n                velocity[i] = (self.inertia_weight * velocity[i] +\n                               self.cognitive_coeff * r1 * (personal_best_positions[i] - pop_position[i]) +\n                               self.social_coeff * r2 * (self.best_global_position - pop_position[i]))\n\n                # Add turbulence\n                if np.random.rand() < turbulence_intensity:\n                    velocity[i] += np.random.normal(0, 0.1, self.dim)\n\n                # Update position\n                pop_position[i] = pop_position[i] + velocity[i]\n                pop_position[i] = np.clip(pop_position[i], lb, ub)\n\n            # Adaptive turbulence reduction\n            turbulence_intensity = max(0.01, turbulence_intensity * 0.99)\n\n        return self.best_global_position, self.best_global_fitness", "name": "CooperativeParticleSwarmOptimization", "description": "A cooperative particle swarm optimization with adaptive turbulence and social reinforcement mechanisms to enhance convergence speed and solution diversity.", "configspace": "", "generation": 9, "fitness": 0.2754510166175672, "feedback": "The algorithm CooperativeParticleSwarmOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28 with standard deviation 0.00.", "error": "", "parent_id": "43a56bf1-0302-4fb6-b44e-e1a5e0961eb2", "metadata": {"aucs": [0.27466709134592004, 0.27642627024095934, 0.27525968826582226]}, "mutation_prompt": null}
{"id": "64ebe4d3-ff1a-45b1-8be4-0193ee43596c", "solution": "import numpy as np\n\nclass RefinedCooperativePSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = min(50, budget // 10)\n        self.best_global_position = None\n        self.best_global_fitness = float('inf')\n        self.inertia_weight = 0.7\n        self.cognitive_coeff = 1.5\n        self.social_coeff = 1.5\n        self.quantum_coeff = 0.05  # Quantum-inspired coefficient for perturbation\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        velocity = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        pop_position = lb + (ub - lb) * np.random.rand(self.pop_size, self.dim)\n        personal_best_positions = np.copy(pop_position)\n        personal_best_fitness = np.full(self.pop_size, float('inf'))\n\n        evaluations = 0\n        turbulence_intensity = 0.1\n\n        while evaluations < self.budget:\n            # Divide swarm into subgroups dynamically based on convergence behavior\n            subgroups = self._divide_swarm(pop_position, num_groups=2)\n\n            for group in subgroups:\n                for i in group:\n                    fitness = func(pop_position[i])\n                    evaluations += 1\n\n                    if fitness < personal_best_fitness[i]:\n                        personal_best_fitness[i] = fitness\n                        personal_best_positions[i] = pop_position[i]\n\n                    if fitness < self.best_global_fitness:\n                        self.best_global_fitness = fitness\n                        self.best_global_position = pop_position[i]\n\n                    if evaluations >= self.budget:\n                        break\n\n            for group in subgroups:\n                for i in group:\n                    r1 = np.random.rand(self.dim)\n                    r2 = np.random.rand(self.dim)\n\n                    velocity[i] = (self.inertia_weight * velocity[i] +\n                                   self.cognitive_coeff * r1 * (personal_best_positions[i] - pop_position[i]) +\n                                   self.social_coeff * r2 * (self.best_global_position - pop_position[i]))\n\n                    if np.random.rand() < turbulence_intensity:\n                        velocity[i] += np.random.normal(0, 0.1, self.dim)\n\n                    pop_position[i] = pop_position[i] + velocity[i]\n                    pop_position[i] = np.clip(pop_position[i], lb, ub)\n\n                    # Quantum-inspired perturbation to enhance exploration\n                    if evaluations % (self.budget // 4) == 0:\n                        pop_position[i] += self.quantum_coeff * np.random.normal(0, 1, self.dim)\n                        pop_position[i] = np.clip(pop_position[i], lb, ub)\n\n            turbulence_intensity = max(0.01, turbulence_intensity * 0.99)\n\n        return self.best_global_position, self.best_global_fitness\n\n    def _divide_swarm(self, positions, num_groups=2):\n        distances = np.linalg.norm(positions - self.best_global_position, axis=1)\n        sorted_indices = np.argsort(distances)\n        return np.array_split(sorted_indices, num_groups)", "name": "RefinedCooperativePSO", "description": "A refined cooperative particle swarm optimization utilizing dynamic swarm division and quantum-inspired perturbations to significantly enhance convergence precision and diversity.", "configspace": "", "generation": 10, "fitness": -Infinity, "feedback": "An exception occurred: TypeError(\"unsupported operand type(s) for -: 'float' and 'NoneType'\").", "error": "TypeError(\"unsupported operand type(s) for -: 'float' and 'NoneType'\")", "parent_id": "d2f62da0-2e6a-4954-b663-00802d3c143c", "metadata": {}, "mutation_prompt": null}
{"id": "2ae9f866-b765-4a3b-9bdd-0e4048b003f9", "solution": "import numpy as np\n\nclass EnhancedCooperativePSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = min(50, budget // 10)\n        self.best_global_position = None\n        self.best_global_fitness = float('inf')\n        \n        # Initialize parameters for dynamic adaptation\n        self.inertia_weight_max = 0.9\n        self.inertia_weight_min = 0.4\n        self.cognitive_coeff = 1.5\n        self.social_coeff = 1.5\n        self.mutation_factor = 0.8  # Factor for DE-based mutation\n        self.crossover_rate = 0.9   # Crossover rate for DE\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        velocity = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        pop_position = lb + (ub - lb) * np.random.rand(self.pop_size, self.dim)\n        personal_best_positions = np.copy(pop_position)\n        personal_best_fitness = np.full(self.pop_size, float('inf'))\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            for i in range(self.pop_size):\n                fitness = func(pop_position[i])\n                evaluations += 1\n\n                if fitness < personal_best_fitness[i]:\n                    personal_best_fitness[i] = fitness\n                    personal_best_positions[i] = pop_position[i]\n\n                if fitness < self.best_global_fitness:\n                    self.best_global_fitness = fitness\n                    self.best_global_position = pop_position[i]\n\n                if evaluations >= self.budget:\n                    break\n\n            inertia_weight = self.inertia_weight_max - (\n                (self.inertia_weight_max - self.inertia_weight_min) * (evaluations / self.budget)\n            )\n\n            for i in range(self.pop_size):\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n\n                # Update velocity with dynamic inertia\n                velocity[i] = (inertia_weight * velocity[i] +\n                               self.cognitive_coeff * r1 * (personal_best_positions[i] - pop_position[i]) +\n                               self.social_coeff * r2 * (self.best_global_position - pop_position[i]))\n\n                # Apply DE-like mutation and crossover\n                if np.random.rand() < self.crossover_rate:\n                    a, b, c = np.random.choice(self.pop_size, 3, replace=False)\n                    mutant_vector = personal_best_positions[a] + self.mutation_factor * (personal_best_positions[b] - personal_best_positions[c])\n                    mutant_vector = np.clip(mutant_vector, lb, ub)\n                    pop_position[i] = np.where(np.random.rand(self.dim) < 0.5, mutant_vector, pop_position[i])\n                else:\n                    pop_position[i] = pop_position[i] + velocity[i]\n\n                # Ensure positions are within bounds\n                pop_position[i] = np.clip(pop_position[i], lb, ub)\n\n        return self.best_global_position, self.best_global_fitness", "name": "EnhancedCooperativePSO", "description": "A cooperative particle swarm optimization with dynamic adaptive inertia and differential evolution-inspired synergistic mutations for enhanced exploration and exploitation.", "configspace": "", "generation": 11, "fitness": 0.25406237981375945, "feedback": "The algorithm EnhancedCooperativePSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.25 with standard deviation 0.00.", "error": "", "parent_id": "d2f62da0-2e6a-4954-b663-00802d3c143c", "metadata": {"aucs": [0.25527123761685055, 0.2524331634986644, 0.2544827383257634]}, "mutation_prompt": null}
{"id": "7f6444f5-bf53-4e47-a658-38e6b7bf2d57", "solution": "import numpy as np\n\nclass HybridDESA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = min(50, budget // 10)\n        self.mutation_factor = 0.5\n        self.crossover_rate = 0.7\n        self.temperature = 1.0\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop_position = lb + (ub - lb) * np.random.rand(self.pop_size, self.dim)\n        pop_fitness = np.array([func(ind) for ind in pop_position])\n\n        evaluations = self.pop_size\n        best_idx = np.argmin(pop_fitness)\n        best_global_position = pop_position[best_idx]\n        best_global_fitness = pop_fitness[best_idx]\n\n        while evaluations < self.budget:\n            for i in range(self.pop_size):\n                indices = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = pop_position[np.random.choice(indices, 3, replace=False)]\n\n                mutant_vector = np.clip(a + self.mutation_factor * (b - c), lb, ub)\n                crossover = np.random.rand(self.dim) < self.crossover_rate\n                trial_vector = np.where(crossover, mutant_vector, pop_position[i])\n\n                trial_fitness = func(trial_vector)\n                evaluations += 1\n\n                if (trial_fitness < pop_fitness[i] or \n                    np.exp((pop_fitness[i] - trial_fitness) / self.temperature) > np.random.rand()):\n                    pop_position[i] = trial_vector\n                    pop_fitness[i] = trial_fitness\n\n                if trial_fitness < best_global_fitness:\n                    best_global_fitness = trial_fitness\n                    best_global_position = trial_vector\n\n                if evaluations >= self.budget:\n                    break\n\n            # Adaptive temperature reduction\n            self.temperature *= 0.99\n\n        return best_global_position, best_global_fitness", "name": "HybridDESA", "description": "A hybrid differential evolution and simulated annealing algorithm incorporating adaptive temperature control to balance exploration and exploitation for enhanced convergence.", "configspace": "", "generation": 12, "fitness": 0.26278604686208146, "feedback": "The algorithm HybridDESA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.26 with standard deviation 0.00.", "error": "", "parent_id": "d2f62da0-2e6a-4954-b663-00802d3c143c", "metadata": {"aucs": [0.26110900487496935, 0.26411637592570747, 0.2631327597855675]}, "mutation_prompt": null}
{"id": "ba14a99d-d984-4cd2-b283-47a0be19958b", "solution": "import numpy as np\n\nclass QuantumInspiredParticleSwarmOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = min(50, budget // 10)\n        self.best_global_position = None\n        self.best_global_fitness = float('inf')\n        self.alpha = 0.5  # Quantum cloud influence\n        self.beta = 0.5   # Memory influence\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop_position = lb + (ub - lb) * np.random.rand(self.pop_size, self.dim)\n        personal_best_positions = np.copy(pop_position)\n        personal_best_fitness = np.full(self.pop_size, float('inf'))\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            for i in range(self.pop_size):\n                fitness = func(pop_position[i])\n                evaluations += 1\n\n                if fitness < personal_best_fitness[i]:\n                    personal_best_fitness[i] = fitness\n                    personal_best_positions[i] = pop_position[i]\n\n                if fitness < self.best_global_fitness:\n                    self.best_global_fitness = fitness\n                    self.best_global_position = pop_position[i]\n\n                if evaluations >= self.budget:\n                    break\n\n            # Quantum-inspired position update\n            for i in range(self.pop_size):\n                # Generate a probability cloud around the best global position\n                q_cloud = self.alpha * np.random.normal(0, 1, self.dim)\n                # Memory component encourages retention of old information\n                memory_component = self.beta * (personal_best_positions[i] - pop_position[i])\n                # Compute new position\n                pop_position[i] = self.best_global_position + q_cloud + memory_component\n                pop_position[i] = np.clip(pop_position[i], lb, ub)\n\n        return self.best_global_position, self.best_global_fitness", "name": "QuantumInspiredParticleSwarmOptimization", "description": "A Quantum-inspired Particle Swarm Optimization with quantum-behavior-based position updates and probability clouds to enhance exploration and convergence.", "configspace": "", "generation": 13, "fitness": 0.2675516215088895, "feedback": "The algorithm QuantumInspiredParticleSwarmOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.27 with standard deviation 0.01.", "error": "", "parent_id": "d2f62da0-2e6a-4954-b663-00802d3c143c", "metadata": {"aucs": [0.2569114916346372, 0.27162108601606383, 0.2741222868759674]}, "mutation_prompt": null}
{"id": "1ec8ef51-9ead-4f30-9290-74c06f62803f", "solution": "import numpy as np\n\nclass EnhancedCooperativeParticleSwarmOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = min(50, budget // 10)\n        self.best_global_position = None\n        self.best_global_fitness = float('inf')\n        self.inertia_weight = 0.7\n        self.cognitive_coeff = 1.5\n        self.social_coeff = 1.5\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        velocity = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        pop_position = lb + (ub - lb) * np.random.rand(self.pop_size, self.dim)\n        personal_best_positions = np.copy(pop_position)\n        personal_best_fitness = np.full(self.pop_size, float('inf'))\n\n        evaluations = 0\n        turbulence_intensity = 0.1  # Initial turbulence intensity\n        group_size = max(2, self.pop_size // 5)  # Dynamic sub-group size\n\n        while evaluations < self.budget:\n            for i in range(self.pop_size):\n                fitness = func(pop_position[i])\n                evaluations += 1\n\n                if fitness < personal_best_fitness[i]:\n                    personal_best_fitness[i] = fitness\n                    personal_best_positions[i] = pop_position[i]\n\n                if fitness < self.best_global_fitness:\n                    self.best_global_fitness = fitness\n                    self.best_global_position = pop_position[i]\n\n                if evaluations >= self.budget:\n                    break\n\n            # Dynamic group reshuffling\n            np.random.shuffle(pop_position)\n\n            for i in range(self.pop_size):\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n\n                # Determine local best position within group\n                group_indices = range((i // group_size) * group_size, min((i // group_size + 1) * group_size, self.pop_size))\n                local_best_position = min(personal_best_positions[group_indices], key=lambda pos: func(pos))\n\n                # Update velocity using local best\n                velocity[i] = (self.inertia_weight * velocity[i] +\n                               self.cognitive_coeff * r1 * (personal_best_positions[i] - pop_position[i]) +\n                               self.social_coeff * r2 * (local_best_position - pop_position[i]))\n\n                # Add turbulence\n                if np.random.rand() < turbulence_intensity:\n                    velocity[i] += np.random.normal(0, 0.1, self.dim)\n\n                # Update position\n                pop_position[i] = pop_position[i] + velocity[i]\n                pop_position[i] = np.clip(pop_position[i], lb, ub)\n\n            # Adaptive parameter control\n            self.inertia_weight = 0.9 - (0.5 * (evaluations / self.budget))\n            turbulence_intensity = max(0.01, turbulence_intensity * 0.99)\n\n        return self.best_global_position, self.best_global_fitness", "name": "EnhancedCooperativeParticleSwarmOptimization", "description": "An enhanced cooperative particle swarm optimizer with dynamic group reshuffling and adaptive parameter control to improve exploration and exploitation balance.", "configspace": "", "generation": 14, "fitness": 0.24344188622387522, "feedback": "The algorithm EnhancedCooperativeParticleSwarmOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.24 with standard deviation 0.00.", "error": "", "parent_id": "d2f62da0-2e6a-4954-b663-00802d3c143c", "metadata": {"aucs": [0.2422018712990862, 0.24507361397650607, 0.2430501733960334]}, "mutation_prompt": null}
{"id": "f88ae28d-1faf-4f42-8395-6e3efa185b05", "solution": "import numpy as np\n\nclass EnhancedCooperativeParticleSwarmOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = min(50, budget // 10)\n        self.best_global_position = None\n        self.best_global_fitness = float('inf')\n        self.inertia_weight = 0.7\n        self.cognitive_coeff = 1.5\n        self.social_coeff = 1.5\n        self.mutation_prob = 0.1  # Probability of mutation\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        velocity = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        pop_position = lb + (ub - lb) * np.random.rand(self.pop_size, self.dim)\n        personal_best_positions = np.copy(pop_position)\n        personal_best_fitness = np.full(self.pop_size, float('inf'))\n\n        evaluations = 0\n        turbulence_intensity = 0.1  # Initial turbulence intensity\n\n        while evaluations < self.budget:\n            social_network = self._create_social_network()\n\n            for i in range(self.pop_size):\n                fitness = func(pop_position[i])\n                evaluations += 1\n\n                if fitness < personal_best_fitness[i]:\n                    personal_best_fitness[i] = fitness\n                    personal_best_positions[i] = pop_position[i]\n\n                if fitness < self.best_global_fitness:\n                    self.best_global_fitness = fitness\n                    self.best_global_position = pop_position[i]\n\n                if evaluations >= self.budget:\n                    break\n\n            for i in range(self.pop_size):\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n\n                # Select a social leader from the network\n                social_leader = self._select_social_leader(i, social_network, personal_best_positions)\n\n                # Update velocity\n                velocity[i] = (self.inertia_weight * velocity[i] +\n                               self.cognitive_coeff * r1 * (personal_best_positions[i] - pop_position[i]) +\n                               self.social_coeff * r2 * (social_leader - pop_position[i]))\n\n                # Add turbulence\n                if np.random.rand() < turbulence_intensity:\n                    velocity[i] += np.random.normal(0, 0.1, self.dim)\n\n                # Apply mutation\n                if np.random.rand() < self.mutation_prob:\n                    mutation_vector = np.random.normal(0, 0.1, self.dim)\n                    pop_position[i] = pop_position[i] + mutation_vector\n\n                # Update position\n                pop_position[i] = pop_position[i] + velocity[i]\n                pop_position[i] = np.clip(pop_position[i], lb, ub)\n\n            # Adaptive turbulence reduction\n            turbulence_intensity = max(0.01, turbulence_intensity * 0.99)\n\n        return self.best_global_position, self.best_global_fitness\n\n    def _create_social_network(self):\n        # Create a random social network with connections\n        social_network = [np.random.choice(range(self.pop_size), size=3, replace=False)\n                          for _ in range(self.pop_size)]\n        return social_network\n\n    def _select_social_leader(self, index, social_network, personal_best_positions):\n        # Select the leader based on the network connectivity\n        connected_indices = social_network[index]\n        leader_fitness = float('inf')\n        leader_position = None\n        for idx in connected_indices:\n            if personal_best_fitness[idx] < leader_fitness:\n                leader_fitness = personal_best_fitness[idx]\n                leader_position = personal_best_positions[idx]\n        return leader_position", "name": "EnhancedCooperativeParticleSwarmOptimization", "description": "An enhanced cooperative particle swarm optimization incorporating dynamic social network structures and mutation-based exploration to achieve a balance between exploration and exploitation.", "configspace": "", "generation": 15, "fitness": -Infinity, "feedback": "An exception occurred: NameError(\"name 'personal_best_fitness' is not defined\").", "error": "NameError(\"name 'personal_best_fitness' is not defined\")", "parent_id": "d2f62da0-2e6a-4954-b663-00802d3c143c", "metadata": {}, "mutation_prompt": null}
{"id": "133a2aca-0ab0-4675-96c6-22688827e14c", "solution": "import numpy as np\n\nclass QuantumInspiredGeneticAlgorithm:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = min(50, budget // 10)\n        self.population = None\n        self.best_solution = None\n        self.best_fitness = float('inf')\n        self.mutation_rate = 0.1\n        self.entanglement_factor = 0.5\n\n    def initialize_population(self, lb, ub):\n        self.population = lb + (ub - lb) * np.random.rand(self.pop_size, self.dim)\n\n    def evaluate_population(self, func):\n        fitness = np.array([func(ind) for ind in self.population])\n        best_index = np.argmin(fitness)\n        if fitness[best_index] < self.best_fitness:\n            self.best_fitness = fitness[best_index]\n            self.best_solution = self.population[best_index]\n        return fitness\n\n    def select_parents(self, fitness):\n        total_fitness = np.sum(1 / (fitness + 1e-9))\n        selection_prob = (1 / (fitness + 1e-9)) / total_fitness\n        indices = np.random.choice(self.pop_size, size=self.pop_size, p=selection_prob)\n        parents = self.population[indices]\n        return parents\n\n    def crossover(self, parents):\n        offspring = np.empty_like(parents)\n        for i in range(0, self.pop_size, 2):\n            if i+1 >= self.pop_size:\n                break\n            alpha = np.random.rand(self.dim)\n            offspring[i] = alpha * parents[i] + (1 - alpha) * parents[i+1]\n            offspring[i+1] = alpha * parents[i+1] + (1 - alpha) * parents[i]\n        return offspring\n\n    def mutate(self, offspring, lb, ub):\n        mutation_matrix = np.random.rand(*offspring.shape) < self.mutation_rate\n        random_values = lb + (ub - lb) * np.random.rand(*offspring.shape)\n        offspring = np.where(mutation_matrix, random_values, offspring)\n        return np.clip(offspring, lb, ub)\n\n    def apply_quantum_entanglement(self, offspring):\n        for i in range(self.pop_size):\n            if np.random.rand() < self.entanglement_factor:\n                partner_idx = np.random.randint(self.pop_size)\n                qubit_superposition = 0.5 * (offspring[i] + offspring[partner_idx])\n                offspring[i] = qubit_superposition + np.random.normal(0, 0.1, self.dim)\n        return offspring\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            fitness = self.evaluate_population(func)\n            evaluations += self.pop_size\n\n            if evaluations >= self.budget:\n                break\n\n            parents = self.select_parents(fitness)\n            offspring = self.crossover(parents)\n            offspring = self.mutate(offspring, lb, ub)\n            offspring = self.apply_quantum_entanglement(offspring)\n\n            self.population = offspring\n\n        return self.best_solution, self.best_fitness", "name": "QuantumInspiredGeneticAlgorithm", "description": "Quantum-inspired Genetic Algorithm with adaptive mutation and entanglement mechanisms for enhanced exploration and exploitation.", "configspace": "", "generation": 16, "fitness": 0.276552808810145, "feedback": "The algorithm QuantumInspiredGeneticAlgorithm got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28 with standard deviation 0.00.", "error": "", "parent_id": "d2f62da0-2e6a-4954-b663-00802d3c143c", "metadata": {"aucs": [0.27657138616214705, 0.27669035551054366, 0.27639668475774426]}, "mutation_prompt": null}
{"id": "8483163f-efa8-443e-837a-048bb70639f0", "solution": "import numpy as np\n\nclass EnhancedQuantumInspiredGeneticAlgorithm:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_pop_size = min(50, budget // 10)\n        self.population = None\n        self.best_solution = None\n        self.best_fitness = float('inf')\n        self.mutation_rate = 0.1\n        self.entanglement_factor = 0.5\n        self.generational_progress = 0\n\n    def initialize_population(self, lb, ub):\n        self.population = lb + (ub - lb) * np.random.rand(self.initial_pop_size, self.dim)\n\n    def evaluate_population(self, func):\n        fitness = np.array([func(ind) for ind in self.population])\n        best_index = np.argmin(fitness)\n        if fitness[best_index] < self.best_fitness:\n            self.best_fitness = fitness[best_index]\n            self.best_solution = self.population[best_index]\n        return fitness\n\n    def select_parents(self, fitness):\n        total_fitness = np.sum(1 / (fitness + 1e-9))\n        selection_prob = (1 / (fitness + 1e-9)) / total_fitness\n        indices = np.random.choice(len(fitness), size=len(fitness), p=selection_prob)\n        parents = self.population[indices]\n        return parents\n\n    def crossover(self, parents):\n        offspring = np.empty_like(parents)\n        for i in range(0, len(parents), 2):\n            if i+1 >= len(parents):\n                break\n            alpha = np.random.rand(self.dim)\n            offspring[i] = alpha * parents[i] + (1 - alpha) * parents[i+1]\n            offspring[i+1] = alpha * parents[i+1] + (1 - alpha) * parents[i]\n        return offspring\n\n    def mutate(self, offspring, lb, ub):\n        mutation_matrix = np.random.rand(*offspring.shape) < self.mutation_rate\n        random_values = lb + (ub - lb) * np.random.rand(*offspring.shape)\n        offspring = np.where(mutation_matrix, random_values, offspring)\n        return np.clip(offspring, lb, ub)\n\n    def apply_quantum_entanglement(self, offspring):\n        for i in range(len(offspring)):\n            if np.random.rand() < self.entanglement_factor:\n                partner_idx = np.random.randint(len(offspring))\n                qubit_superposition = 0.5 * (offspring[i] + offspring[partner_idx])\n                offspring[i] = qubit_superposition + np.random.normal(0, 0.1, self.dim)\n        return offspring\n\n    def adapt_population_size(self):\n        progress_rate = min(0.5, 2 * (1 - self.best_fitness / (self.best_fitness + 1e-9)))\n        new_pop_size = int(self.initial_pop_size * (1 + progress_rate))\n        self.population = self.population[:new_pop_size]\n\n    def update_mutation_rate(self):\n        self.mutation_rate = max(0.01, 0.1 * (1 - self.generational_progress / self.budget))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            fitness = self.evaluate_population(func)\n            evaluations += len(fitness)\n\n            if evaluations >= self.budget:\n                break\n\n            parents = self.select_parents(fitness)\n            offspring = self.crossover(parents)\n            offspring = self.mutate(offspring, lb, ub)\n            offspring = self.apply_quantum_entanglement(offspring)\n\n            self.population = offspring\n            self.adapt_population_size()\n            self.update_mutation_rate()\n            self.generational_progress = evaluations\n\n        return self.best_solution, self.best_fitness", "name": "EnhancedQuantumInspiredGeneticAlgorithm", "description": "Enhanced Quantum-Inspired Genetic Algorithm with adaptive population resizing and dynamic mutation adjustment for improved exploration and convergence.", "configspace": "", "generation": 17, "fitness": 0.2767116004813258, "feedback": "The algorithm EnhancedQuantumInspiredGeneticAlgorithm got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28 with standard deviation 0.00.", "error": "", "parent_id": "133a2aca-0ab0-4675-96c6-22688827e14c", "metadata": {"aucs": [0.2767305277069959, 0.2765381348665188, 0.27686613887046274]}, "mutation_prompt": null}
{"id": "e74661ef-505e-46eb-91f7-24789bf147a6", "solution": "import numpy as np\n\nclass QuantumRotatedGeneticAlgorithm:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_pop_size = min(100, budget // 8)\n        self.population = None\n        self.best_solution = None\n        self.best_fitness = float('inf')\n        self.mutation_rate = 0.1\n        self.rotation_angle = 0.05\n        self.fitness_threshold = 0.1\n\n    def initialize_population(self, lb, ub):\n        self.population = lb + (ub - lb) * np.random.rand(self.initial_pop_size, self.dim)\n\n    def evaluate_population(self, func):\n        fitness = np.array([func(ind) for ind in self.population])\n        best_index = np.argmin(fitness)\n        if fitness[best_index] < self.best_fitness:\n            self.best_fitness = fitness[best_index]\n            self.best_solution = self.population[best_index]\n        return fitness\n\n    def select_parents(self, fitness):\n        total_fitness = np.sum(1 / (fitness + 1e-9))\n        selection_prob = (1 / (fitness + 1e-9)) / total_fitness\n        indices = np.random.choice(len(fitness), size=len(fitness), p=selection_prob)\n        parents = self.population[indices]\n        return parents\n\n    def crossover(self, parents):\n        offspring = np.empty_like(parents)\n        for i in range(0, len(parents), 2):\n            if i+1 >= len(parents):\n                break\n            alpha = np.random.rand(self.dim)\n            offspring[i] = alpha * parents[i] + (1 - alpha) * parents[i+1]\n            offspring[i+1] = alpha * parents[i+1] + (1 - alpha) * parents[i]\n        return offspring\n\n    def mutate(self, offspring, lb, ub):\n        mutation_matrix = np.random.rand(*offspring.shape) < self.mutation_rate\n        random_values = lb + (ub - lb) * np.random.rand(*offspring.shape)\n        offspring = np.where(mutation_matrix, random_values, offspring)\n        return np.clip(offspring, lb, ub)\n\n    def apply_quantum_rotation(self, offspring):\n        for i in range(len(offspring)):\n            rotation_matrix = np.eye(self.dim) + self.rotation_angle * np.random.randn(self.dim, self.dim)\n            offspring[i] = np.dot(rotation_matrix, offspring[i])\n        return offspring\n\n    def recycle_population(self, fitness):\n        threshold = np.median(fitness) + np.std(fitness)\n        survivors = self.population[fitness < threshold]\n        new_individuals = self.initialize_population(survivors.shape[0], survivors.shape[1])\n        self.population = np.vstack((survivors, new_individuals))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            fitness = self.evaluate_population(func)\n            evaluations += len(fitness)\n\n            if evaluations >= self.budget:\n                break\n\n            self.recycle_population(fitness)\n            parents = self.select_parents(fitness)\n            offspring = self.crossover(parents)\n            offspring = self.mutate(offspring, lb, ub)\n            offspring = self.apply_quantum_rotation(offspring)\n\n            self.population = offspring\n            self.mutation_rate = max(0.01, 0.1 * (1 - evaluations / self.budget))\n            self.rotation_angle = 0.05 * (1 - self.best_fitness / (self.best_fitness + 1e-9))\n\n        return self.best_solution, self.best_fitness", "name": "QuantumRotatedGeneticAlgorithm", "description": "Quantum-Inspired Genetic Algorithm with adaptive quantum rotation gates and fitness-based population recycling for enhanced convergence in complex search spaces.", "configspace": "", "generation": 18, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('all the input array dimensions except for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 10 and the array at index 1 has size 1').", "error": "ValueError('all the input array dimensions except for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 10 and the array at index 1 has size 1')", "parent_id": "8483163f-efa8-443e-837a-048bb70639f0", "metadata": {}, "mutation_prompt": null}
{"id": "7d2a01a8-2ad2-4e6d-b375-c9f78b65db8c", "solution": "import numpy as np\n\nclass QuantumDrivenGeneticAlgorithm:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_pop_size = min(50, budget // 10)\n        self.population = None\n        self.best_solution = None\n        self.best_fitness = float('inf')\n        self.mutation_rate = 0.1\n        self.entanglement_factor = 0.5\n        self.generational_progress = 0\n        self.learning_rate = 0.1  # New adaptive learning rate\n\n    def initialize_population(self, lb, ub):\n        self.population = lb + (ub - lb) * np.random.rand(self.initial_pop_size, self.dim)\n\n    def evaluate_population(self, func):\n        fitness = np.array([func(ind) for ind in self.population])\n        best_index = np.argmin(fitness)\n        if fitness[best_index] < self.best_fitness:\n            self.best_fitness = fitness[best_index]\n            self.best_solution = self.population[best_index]\n        return fitness\n\n    def select_parents(self, fitness):\n        total_fitness = np.sum(1 / (fitness + 1e-9))\n        selection_prob = (1 / (fitness + 1e-9)) / total_fitness\n        indices = np.random.choice(len(fitness), size=len(fitness), p=selection_prob)\n        parents = self.population[indices]\n        return parents\n\n    def crossover(self, parents):\n        offspring = np.empty_like(parents)\n        for i in range(0, len(parents), 2):\n            if i+1 >= len(parents):\n                break\n            alpha = np.random.rand(self.dim)\n            offspring[i] = alpha * parents[i] + (1 - alpha) * parents[i+1]\n            offspring[i+1] = alpha * parents[i+1] + (1 - alpha) * parents[i]\n        return offspring\n\n    def mutate(self, offspring, lb, ub):\n        mutation_matrix = np.random.rand(*offspring.shape) < self.mutation_rate\n        random_values = lb + (ub - lb) * np.random.rand(*offspring.shape)\n        offspring = np.where(mutation_matrix, random_values, offspring)\n        return np.clip(offspring, lb, ub)\n\n    def apply_quantum_entanglement(self, offspring):\n        for i in range(len(offspring)):\n            if np.random.rand() < self.entanglement_factor:\n                partner_idx = np.random.randint(len(offspring))\n                qubit_superposition = 0.5 * (offspring[i] + offspring[partner_idx])\n                offspring[i] = qubit_superposition + np.random.normal(0, self.learning_rate, self.dim)\n        return offspring\n\n    def quantum_tunneling(self, offspring, lb, ub):\n        # Introduce tunneling to escape local minima\n        if np.random.rand() < 0.2:  # 20% chance to apply tunneling\n            random_individuals = lb + (ub - lb) * np.random.rand(2, self.dim)\n            indices = np.random.choice(len(offspring), size=2, replace=False)\n            offspring[indices] = random_individuals\n        return np.clip(offspring, lb, ub)\n    \n    def adapt_population_size(self):\n        progress_rate = min(0.5, 2 * (1 - self.best_fitness / (self.best_fitness + 1e-9)))\n        new_pop_size = int(self.initial_pop_size * (1 + progress_rate))\n        self.population = self.population[:new_pop_size]\n\n    def update_mutation_rate_and_learning_rate(self):\n        self.mutation_rate = max(0.01, 0.1 * (1 - self.generational_progress / self.budget))\n        self.learning_rate = max(0.01, 0.1 * (1 - self.best_fitness / (self.best_fitness + 1e-9)))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            fitness = self.evaluate_population(func)\n            evaluations += len(fitness)\n\n            if evaluations >= self.budget:\n                break\n\n            parents = self.select_parents(fitness)\n            offspring = self.crossover(parents)\n            offspring = self.mutate(offspring, lb, ub)\n            offspring = self.apply_quantum_entanglement(offspring)\n            offspring = self.quantum_tunneling(offspring, lb, ub)\n\n            self.population = offspring\n            self.adapt_population_size()\n            self.update_mutation_rate_and_learning_rate()\n            self.generational_progress = evaluations\n\n        return self.best_solution, self.best_fitness", "name": "QuantumDrivenGeneticAlgorithm", "description": "Quantum-Driven Genetic Algorithm with adaptive learning rate and quantum tunneling for enhanced exploration and convergence in photonic structure optimization.", "configspace": "", "generation": 19, "fitness": 0.2768063494549852, "feedback": "The algorithm QuantumDrivenGeneticAlgorithm got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28 with standard deviation 0.00.", "error": "", "parent_id": "8483163f-efa8-443e-837a-048bb70639f0", "metadata": {"aucs": [0.27677114428230465, 0.2765851545604483, 0.2770627495222028]}, "mutation_prompt": null}
{"id": "dd3bd051-9bcc-45dc-826c-17f3d0551f3d", "solution": "import numpy as np\n\nclass AdaptiveQuantumSwarmOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = min(50, budget // 10)\n        self.particles = None\n        self.velocities = None\n        self.pbest_pos = None\n        self.pbest_fitness = None\n        self.gbest_pos = None\n        self.gbest_fitness = float('inf')\n        self.inertia_weight = 0.5\n        self.cognitive_coeff = 1.5\n        self.social_coeff = 1.5\n        self.quantum_jump_prob = 0.3\n\n    def initialize_swarm(self, lb, ub):\n        self.particles = lb + (ub - lb) * np.random.rand(self.swarm_size, self.dim)\n        self.velocities = np.zeros_like(self.particles)\n        self.pbest_pos = np.copy(self.particles)\n        self.pbest_fitness = np.full(self.swarm_size, float('inf'))\n\n    def evaluate_swarm(self, func):\n        fitness = np.array([func(p) for p in self.particles])\n        for i in range(self.swarm_size):\n            if fitness[i] < self.pbest_fitness[i]:\n                self.pbest_fitness[i] = fitness[i]\n                self.pbest_pos[i] = self.particles[i]\n            if fitness[i] < self.gbest_fitness:\n                self.gbest_fitness = fitness[i]\n                self.gbest_pos = self.particles[i]\n\n    def update_velocities_and_positions(self, lb, ub):\n        r1 = np.random.rand(self.swarm_size, self.dim)\n        r2 = np.random.rand(self.swarm_size, self.dim)\n        cognitive_component = self.cognitive_coeff * r1 * (self.pbest_pos - self.particles)\n        social_component = self.social_coeff * r2 * (self.gbest_pos - self.particles)\n        self.velocities = self.inertia_weight * self.velocities + cognitive_component + social_component\n        self.particles += self.velocities\n        self.particles = np.clip(self.particles, lb, ub)\n\n    def quantum_jump(self, lb, ub):\n        for i in range(self.swarm_size):\n            if np.random.rand() < self.quantum_jump_prob:\n                self.particles[i] = lb + (ub - lb) * np.random.rand(self.dim)\n\n    def adapt_parameters(self, evaluations):\n        self.inertia_weight = 0.9 - 0.7 * (evaluations / self.budget)\n        self.quantum_jump_prob = max(0.1, 0.3 * (1 - self.gbest_fitness / (self.gbest_fitness + 1e-9)))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_swarm(lb, ub)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            self.evaluate_swarm(func)\n            evaluations += self.swarm_size\n\n            if evaluations >= self.budget:\n                break\n\n            self.update_velocities_and_positions(lb, ub)\n            self.quantum_jump(lb, ub)\n            self.adapt_parameters(evaluations)\n\n        return self.gbest_pos, self.gbest_fitness", "name": "AdaptiveQuantumSwarmOptimization", "description": "Adaptive Quantum Swarm Optimization with Dynamic Particle Interactions and Quantum Leap Mechanism for efficient exploration and convergence in photonic structure optimization.", "configspace": "", "generation": 20, "fitness": 0.26601885232234873, "feedback": "The algorithm AdaptiveQuantumSwarmOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.27 with standard deviation 0.00.", "error": "", "parent_id": "7d2a01a8-2ad2-4e6d-b375-c9f78b65db8c", "metadata": {"aucs": [0.265758288946515, 0.26599799749175035, 0.2663002705287808]}, "mutation_prompt": null}
{"id": "c690ea89-b704-4b42-9584-d4aab66eb2f6", "solution": "import numpy as np\n\nclass QuantumSwarmHarmonySearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.harmony_memory_size = min(30, budget // 10)\n        self.population = None\n        self.best_solution = None\n        self.best_fitness = float('inf')\n        self.hmcr = 0.9  # Harmony Memory Consideration Rate\n        self.par = 0.3   # Pitch Adjustment Rate\n        self.mutation_rate = 0.05\n        self.generational_progress = 0\n\n    def initialize_population(self, lb, ub):\n        self.population = lb + (ub - lb) * np.random.rand(self.harmony_memory_size, self.dim)\n\n    def evaluate_population(self, func):\n        fitness = np.array([func(ind) for ind in self.population])\n        best_index = np.argmin(fitness)\n        if fitness[best_index] < self.best_fitness:\n            self.best_fitness = fitness[best_index]\n            self.best_solution = self.population[best_index]\n        return fitness\n\n    def improve_harmony(self, lb, ub):\n        new_harmony = np.empty(self.dim)\n        for i in range(self.dim):\n            if np.random.rand() < self.hmcr:\n                new_harmony[i] = self.population[np.random.randint(self.harmony_memory_size), i]\n                if np.random.rand() < self.par:\n                    pitch_adjustment = (ub[i] - lb[i]) * (np.random.rand() - 0.5)\n                    new_harmony[i] += pitch_adjustment\n            else:\n                new_harmony[i] = lb[i] + (ub[i] - lb[i]) * np.random.rand()\n        return np.clip(new_harmony, lb, ub)\n\n    def mutate_population(self, lb, ub):\n        mutation_matrix = np.random.rand(self.harmony_memory_size, self.dim) < self.mutation_rate\n        random_values = lb + (ub - lb) * np.random.rand(self.harmony_memory_size, self.dim)\n        self.population = np.where(mutation_matrix, random_values, self.population)\n        return np.clip(self.population, lb, ub)\n\n    def quantum_tunneling(self, lb, ub):\n        if np.random.rand() < 0.1:\n            random_individuals = lb + (ub - lb) * np.random.rand(2, self.dim)\n            indices = np.random.choice(self.harmony_memory_size, size=2, replace=False)\n            self.population[indices] = random_individuals\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            fitness = self.evaluate_population(func)\n            evaluations += len(fitness)\n\n            if evaluations >= self.budget:\n                break\n\n            new_harmony = self.improve_harmony(lb, ub)\n            new_fitness = func(new_harmony)\n            evaluations += 1\n\n            if new_fitness < self.best_fitness:\n                self.best_fitness = new_fitness\n                self.best_solution = new_harmony\n\n            if new_fitness < max(fitness):\n                worst_index = np.argmax(fitness)\n                self.population[worst_index] = new_harmony\n\n            self.mutate_population(lb, ub)\n            self.quantum_tunneling(lb, ub)\n\n            self.generational_progress = evaluations\n\n        return self.best_solution, self.best_fitness", "name": "QuantumSwarmHarmonySearch", "description": "Quantum Swarm Harmony Search combines quantum-inspired swarm intelligence with harmony search principles to enhance exploration and exploitation in photonic structure optimization.", "configspace": "", "generation": 21, "fitness": 0.2500670261837949, "feedback": "The algorithm QuantumSwarmHarmonySearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.25 with standard deviation 0.00.", "error": "", "parent_id": "7d2a01a8-2ad2-4e6d-b375-c9f78b65db8c", "metadata": {"aucs": [0.2459355452526858, 0.24855870153293114, 0.2557068317657677]}, "mutation_prompt": null}
{"id": "d0902f4d-1482-4b31-be9b-b3779da51ba3", "solution": "import numpy as np\n\nclass EnhancedQuantumDrivenGeneticAlgorithm:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_pop_size = min(50, budget // 10)\n        self.population = None\n        self.best_solution = None\n        self.best_fitness = float('inf')\n        self.mutation_rate = 0.1\n        self.entanglement_factor = 0.5\n        self.generational_progress = 0\n        self.learning_rate = 0.1\n        self.diversity_threshold = 0.1\n\n    def initialize_population(self, lb, ub):\n        self.population = lb + (ub - lb) * np.random.rand(self.initial_pop_size, self.dim)\n\n    def evaluate_population(self, func):\n        fitness = np.array([func(ind) for ind in self.population])\n        best_index = np.argmin(fitness)\n        if fitness[best_index] < self.best_fitness:\n            self.best_fitness = fitness[best_index]\n            self.best_solution = self.population[best_index]\n        return fitness\n\n    def select_parents(self, fitness):\n        total_fitness = np.sum(1 / (fitness + 1e-9))\n        selection_prob = (1 / (fitness + 1e-9)) / total_fitness\n        indices = np.random.choice(len(fitness), size=len(fitness), p=selection_prob)\n        parents = self.population[indices]\n        return parents\n\n    def crossover(self, parents):\n        offspring = np.empty_like(parents)\n        for i in range(0, len(parents), 2):\n            if i+1 >= len(parents):\n                break\n            alpha = np.random.rand(self.dim)\n            offspring[i] = alpha * parents[i] + (1 - alpha) * parents[i+1]\n            offspring[i+1] = alpha * parents[i+1] + (1 - alpha) * parents[i]\n        return offspring\n\n    def mutate(self, offspring, lb, ub):\n        mutation_matrix = np.random.rand(*offspring.shape) < self.mutation_rate\n        random_values = lb + (ub - lb) * np.random.rand(*offspring.shape)\n        offspring = np.where(mutation_matrix, random_values, offspring)\n        return np.clip(offspring, lb, ub)\n\n    def apply_quantum_entanglement(self, offspring):\n        for i in range(len(offspring)):\n            if np.random.rand() < self.entanglement_factor:\n                partner_idx = np.random.randint(len(offspring))\n                qubit_superposition = 0.5 * (offspring[i] + offspring[partner_idx])\n                offspring[i] = qubit_superposition + np.random.normal(0, self.learning_rate, self.dim)\n        return offspring\n\n    def quantum_tunneling(self, offspring, lb, ub):\n        if np.random.rand() < 0.2:\n            random_individuals = lb + (ub - lb) * np.random.rand(2, self.dim)\n            indices = np.random.choice(len(offspring), size=2, replace=False)\n            offspring[indices] = random_individuals\n        return np.clip(offspring, lb, ub)\n    \n    def adapt_population_size(self):\n        diversity = np.mean(np.std(self.population, axis=0))\n        if diversity < self.diversity_threshold:\n            self.population = np.append(self.population, self.initialize_population(self.population.min(axis=0), self.population.max(axis=0)), axis=0)\n        progress_rate = min(0.5, 2 * (1 - self.best_fitness / (self.best_fitness + 1e-9)))\n        new_pop_size = int(self.initial_pop_size * (1 + progress_rate))\n        self.population = self.population[:new_pop_size]\n\n    def update_mutation_rate_and_learning_rate(self):\n        self.mutation_rate = max(0.01, 0.1 * (1 - self.generational_progress / self.budget))\n        self.learning_rate = max(0.01, 0.1 * (1 - self.best_fitness / (self.best_fitness + 1e-9)))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            fitness = self.evaluate_population(func)\n            evaluations += len(fitness)\n\n            if evaluations >= self.budget:\n                break\n\n            parents = self.select_parents(fitness)\n            offspring = self.crossover(parents)\n            offspring = self.mutate(offspring, lb, ub)\n            offspring = self.apply_quantum_entanglement(offspring)\n            offspring = self.quantum_tunneling(offspring, lb, ub)\n\n            self.population = offspring\n            self.adapt_population_size()\n            self.update_mutation_rate_and_learning_rate()\n            self.generational_progress = evaluations\n\n        return self.best_solution, self.best_fitness", "name": "EnhancedQuantumDrivenGeneticAlgorithm", "description": "Enhanced Quantum-Driven Genetic Algorithm with dynamic population size, diversity preservation, and adaptive mutation for superior exploration and convergence in photonic structure optimization.", "configspace": "", "generation": 22, "fitness": 0.2768063494549852, "feedback": "The algorithm EnhancedQuantumDrivenGeneticAlgorithm got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28 with standard deviation 0.00.", "error": "", "parent_id": "7d2a01a8-2ad2-4e6d-b375-c9f78b65db8c", "metadata": {"aucs": [0.27677114428230465, 0.2765851545604483, 0.2770627495222028]}, "mutation_prompt": null}
{"id": "84c4f2e1-0b5c-42cf-b84b-d494cee80539", "solution": "import numpy as np\n\nclass EnhancedQuantumGeneticAlgorithm:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_pop_size = max(10, min(50, budget // 10))\n        self.population = None\n        self.best_solution = None\n        self.best_fitness = float('inf')\n        self.mutation_rate = 0.1\n        self.entanglement_factor = 0.5\n        self.generational_progress = 0\n        self.learning_rate = 0.1\n        self.exploration_phase = True\n\n    def initialize_population(self, lb, ub):\n        self.population = lb + (ub - lb) * np.random.rand(self.initial_pop_size, self.dim)\n\n    def evaluate_population(self, func):\n        fitness = np.array([func(ind) for ind in self.population])\n        best_index = np.argmin(fitness)\n        if fitness[best_index] < self.best_fitness:\n            self.best_fitness = fitness[best_index]\n            self.best_solution = self.population[best_index]\n        return fitness\n\n    def select_parents(self, fitness):\n        total_fitness = np.sum(1 / (fitness + 1e-9))\n        selection_prob = (1 / (fitness + 1e-9)) / total_fitness\n        indices = np.random.choice(len(fitness), size=len(fitness), p=selection_prob)\n        return self.population[indices]\n\n    def crossover(self, parents):\n        offspring = np.empty_like(parents)\n        for i in range(0, len(parents), 2):\n            if i+1 >= len(parents):\n                break\n            alpha = np.random.rand(self.dim)\n            offspring[i] = alpha * parents[i] + (1 - alpha) * parents[i+1]\n            offspring[i+1] = alpha * parents[i+1] + (1 - alpha) * parents[i]\n        return offspring\n\n    def mutate(self, offspring, lb, ub):\n        mutation_matrix = np.random.rand(*offspring.shape) < self.mutation_rate\n        random_values = lb + (ub - lb) * np.random.rand(*offspring.shape)\n        offspring = np.where(mutation_matrix, random_values, offspring)\n        return np.clip(offspring, lb, ub)\n\n    def apply_quantum_entanglement(self, offspring):\n        for i in range(len(offspring)):\n            if np.random.rand() < self.entanglement_factor:\n                partner_idx = np.random.randint(len(offspring))\n                qubit_superposition = 0.5 * (offspring[i] + offspring[partner_idx])\n                offspring[i] = qubit_superposition + np.random.normal(0, self.learning_rate, self.dim)\n        return offspring\n\n    def quantum_tunneling(self, offspring, lb, ub):\n        # More strategic tunneling: based on stagnation detection\n        if np.random.rand() < 0.3:  # 30% chance to apply tunneling\n            random_individuals = lb + (ub - lb) * np.random.rand(3, self.dim)\n            indices = np.random.choice(len(offspring), size=3, replace=False)\n            offspring[indices] = random_individuals\n        return np.clip(offspring, lb, ub)\n\n    def adapt_population_size(self):\n        progress_rate = min(0.5, 2 * (1 - self.best_fitness / (self.best_fitness + 1e-9)))\n        if self.generational_progress > self.budget // 2:\n            self.exploration_phase = False\n        if self.exploration_phase:\n            new_pop_size = int(self.initial_pop_size * (1 + progress_rate))\n        else:\n            new_pop_size = int(self.initial_pop_size * (1 - progress_rate / 2))\n        self.population = self.population[:new_pop_size]\n\n    def update_mutation_rate_and_learning_rate(self):\n        self.mutation_rate = max(0.01, 0.1 * (1 - self.generational_progress / self.budget))\n        self.learning_rate = max(0.01, 0.1 * (1 - self.best_fitness / (self.best_fitness + 1e-9)))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            fitness = self.evaluate_population(func)\n            evaluations += len(fitness)\n\n            if evaluations >= self.budget:\n                break\n\n            parents = self.select_parents(fitness)\n            offspring = self.crossover(parents)\n            offspring = self.mutate(offspring, lb, ub)\n            offspring = self.apply_quantum_entanglement(offspring)\n            offspring = self.quantum_tunneling(offspring, lb, ub)\n\n            self.population = offspring\n            self.adapt_population_size()\n            self.update_mutation_rate_and_learning_rate()\n            self.generational_progress = evaluations\n\n        return self.best_solution, self.best_fitness", "name": "EnhancedQuantumGeneticAlgorithm", "description": "Enhanced Quantum Genetic Algorithm with dynamic population control and strategic quantum tunneling for superior convergence in photonic structure optimization.", "configspace": "", "generation": 23, "fitness": 0.2770007466082467, "feedback": "The algorithm EnhancedQuantumGeneticAlgorithm got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28 with standard deviation 0.00.", "error": "", "parent_id": "7d2a01a8-2ad2-4e6d-b375-c9f78b65db8c", "metadata": {"aucs": [0.27686938633958236, 0.2769137808475497, 0.27721907263760803]}, "mutation_prompt": null}
{"id": "4c9f8366-3e6e-4bf0-8964-ae0622a06880", "solution": "import numpy as np\n\nclass AdaptiveSwarmOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = max(20, min(60, budget // 5))\n        self.position = None\n        self.velocity = None\n        self.best_swarm_position = None\n        self.best_swarm_fitness = float('inf')\n        self.individual_best_positions = None\n        self.individual_best_fitness = None\n        self.inertia_weight = 0.7\n        self.cognitive_coefficient = 1.5\n        self.social_coefficient = 1.5\n\n    def initialize_swarm(self, lb, ub):\n        self.position = lb + (ub - lb) * np.random.rand(self.swarm_size, self.dim)\n        self.velocity = np.random.uniform(-1, 1, (self.swarm_size, self.dim))\n        self.individual_best_positions = np.copy(self.position)\n        self.individual_best_fitness = np.full(self.swarm_size, float('inf'))\n\n    def evaluate_swarm(self, func):\n        fitness = np.array([func(ind) for ind in self.position])\n        for i in range(self.swarm_size):\n            if fitness[i] < self.individual_best_fitness[i]:\n                self.individual_best_fitness[i] = fitness[i]\n                self.individual_best_positions[i] = self.position[i]\n            if fitness[i] < self.best_swarm_fitness:\n                self.best_swarm_fitness = fitness[i]\n                self.best_swarm_position = self.position[i]\n        return fitness\n\n    def update_velocity_and_position(self, lb, ub):\n        r1, r2 = np.random.rand(self.swarm_size, self.dim), np.random.rand(self.swarm_size, self.dim)\n        cognitive_component = self.cognitive_coefficient * r1 * (self.individual_best_positions - self.position)\n        social_component = self.social_coefficient * r2 * (self.best_swarm_position - self.position)\n        self.velocity = (self.inertia_weight * self.velocity + cognitive_component + social_component)\n        self.position += self.velocity\n        self.position = np.clip(self.position, lb, ub)\n\n    def adapt_parameters(self, evaluations):\n        self.inertia_weight = 0.9 - 0.5 * (evaluations / self.budget)\n        self.cognitive_coefficient = 1.5 + (evaluations / self.budget)\n        self.social_coefficient = 1.5 + (1 - evaluations / self.budget)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_swarm(lb, ub)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            fitness = self.evaluate_swarm(func)\n            evaluations += self.swarm_size\n\n            if evaluations >= self.budget:\n                break\n\n            self.update_velocity_and_position(lb, ub)\n            self.adapt_parameters(evaluations)\n\n        return self.best_swarm_position, self.best_swarm_fitness", "name": "AdaptiveSwarmOptimization", "description": "Adaptive Swarm-Based Optimization utilizing dynamic leader election and stochastic velocity adjustments for efficient global search in photonic structure optimization.", "configspace": "", "generation": 24, "fitness": 0.25464255241259476, "feedback": "The algorithm AdaptiveSwarmOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.25 with standard deviation 0.00.", "error": "", "parent_id": "84c4f2e1-0b5c-42cf-b84b-d494cee80539", "metadata": {"aucs": [0.2504401288620497, 0.2519903186451079, 0.26149720973062673]}, "mutation_prompt": null}
{"id": "3dac7927-d428-40a9-b56d-27bbdc1b839f", "solution": "import numpy as np\n\nclass HybridParticleSwarmOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = max(10, min(50, budget // 10))\n        self.positions = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.global_best_position = None\n        self.personal_best_fitness = np.full(self.num_particles, float('inf'))\n        self.global_best_fitness = float('inf')\n        self.inertia_weight = 0.7\n        self.cognitive_weight = 1.5\n        self.social_weight = 1.5\n        self.adaptive_variance = 0.1\n\n    def initialize_particles(self, lb, ub):\n        self.positions = lb + (ub - lb) * np.random.rand(self.num_particles, self.dim)\n        self.velocities = np.random.rand(self.num_particles, self.dim) * (ub - lb) * 0.1\n        self.personal_best_positions = np.copy(self.positions)\n    \n    def evaluate_fitness(self, func):\n        fitness = np.array([func(pos) for pos in self.positions])\n        for i in range(self.num_particles):\n            if fitness[i] < self.personal_best_fitness[i]:\n                self.personal_best_fitness[i] = fitness[i]\n                self.personal_best_positions[i] = self.positions[i]\n            if fitness[i] < self.global_best_fitness:\n                self.global_best_fitness = fitness[i]\n                self.global_best_position = self.positions[i]\n        return fitness\n    \n    def update_velocities_and_positions(self, lb, ub):\n        r1 = np.random.rand(self.num_particles, self.dim)\n        r2 = np.random.rand(self.num_particles, self.dim)\n        cognitive_component = self.cognitive_weight * r1 * (self.personal_best_positions - self.positions)\n        social_component = self.social_weight * r2 * (self.global_best_position - self.positions)\n        self.velocities = (self.inertia_weight * self.velocities +\n                           cognitive_component +\n                           social_component)\n        self.positions = np.clip(self.positions + self.velocities, lb, ub)\n    \n    def apply_quantum_variance(self, lb, ub):\n        for i in range(self.num_particles):\n            if np.random.rand() < self.adaptive_variance:\n                self.positions[i] += np.random.normal(0, self.adaptive_variance, self.dim)\n                self.positions[i] = np.clip(self.positions[i], lb, ub)\n    \n    def adapt_parameters(self, evaluations):\n        self.inertia_weight = 0.9 - (0.5 * evaluations / self.budget)\n        self.adaptive_variance = 0.1 * (1 - evaluations / self.budget)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_particles(lb, ub)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            fitness = self.evaluate_fitness(func)\n            evaluations += len(fitness)\n\n            if evaluations >= self.budget:\n                break\n\n            self.update_velocities_and_positions(lb, ub)\n            self.apply_quantum_variance(lb, ub)\n            self.adapt_parameters(evaluations)\n        \n        return self.global_best_position, self.global_best_fitness", "name": "HybridParticleSwarmOptimization", "description": "Hybrid Particle Swarm Optimization with Adaptive Quantum-inspired Variance for efficient exploration and exploitation in optimizing photonic structures.", "configspace": "", "generation": 25, "fitness": 0.2760276652941576, "feedback": "The algorithm HybridParticleSwarmOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28 with standard deviation 0.00.", "error": "", "parent_id": "84c4f2e1-0b5c-42cf-b84b-d494cee80539", "metadata": {"aucs": [0.2764050888432593, 0.2753213125100614, 0.2763565945291522]}, "mutation_prompt": null}
{"id": "b2c0f103-84cf-4cdc-a9dd-d8e5c9d7160b", "solution": "import numpy as np\n\nclass AdaptiveQuantumInspiredEA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_pop_size = max(10, min(50, budget // 10))\n        self.population = None\n        self.best_solution = None\n        self.best_fitness = float('inf')\n        self.mutation_rate = 0.1\n        self.entanglement_factor = 0.5\n        self.generational_progress = 0\n        self.learning_rate = 0.1\n        self.exploration_phase = True\n\n    def initialize_population(self, lb, ub):\n        self.population = lb + (ub - lb) * np.random.rand(self.initial_pop_size, self.dim)\n\n    def evaluate_population(self, func):\n        fitness = np.array([func(ind) for ind in self.population])\n        best_index = np.argmin(fitness)\n        if fitness[best_index] < self.best_fitness:\n            self.best_fitness = fitness[best_index]\n            self.best_solution = self.population[best_index]\n        return fitness\n\n    def select_parents(self, fitness):\n        total_fitness = np.sum(1 / (fitness + 1e-9))\n        selection_prob = (1 / (fitness + 1e-9)) / total_fitness\n        indices = np.random.choice(len(fitness), size=len(fitness), p=selection_prob)\n        return self.population[indices]\n\n    def crossover(self, parents):\n        offspring = np.empty_like(parents)\n        for i in range(0, len(parents), 2):\n            if i+1 >= len(parents):\n                break\n            alpha = np.random.rand(self.dim)\n            offspring[i] = alpha * parents[i] + (1 - alpha) * parents[i+1]\n            offspring[i+1] = alpha * parents[i+1] + (1 - alpha) * parents[i]\n        return offspring\n\n    def mutate(self, offspring, lb, ub):\n        mutation_matrix = np.random.rand(*offspring.shape) < self.mutation_rate\n        random_values = lb + (ub - lb) * np.random.rand(*offspring.shape)\n        offspring = np.where(mutation_matrix, random_values, offspring)\n        return np.clip(offspring, lb, ub)\n\n    def apply_quantum_entanglement(self, offspring):\n        for i in range(len(offspring)):\n            if np.random.rand() < self.entanglement_factor:\n                partner_idx = np.random.randint(len(offspring))\n                qubit_superposition = 0.5 * (offspring[i] + offspring[partner_idx])\n                offspring[i] = qubit_superposition + np.random.normal(0, self.learning_rate, self.dim)\n        return offspring\n\n    def multiverse_immigration(self, offspring, lb, ub):\n        # Introduce immigration based on multiverse theory\n        for i in range(len(offspring)):\n            if np.random.rand() < self.entanglement_factor:\n                random_universe = lb + (ub - lb) * np.random.rand(self.dim)\n                offspring[i] = offspring[i] + np.random.normal(0, self.learning_rate, self.dim) + random_universe\n        return np.clip(offspring, lb, ub)\n\n    def adapt_population_size(self):\n        progress_rate = min(0.5, 2 * (1 - self.best_fitness / (self.best_fitness + 1e-9)))\n        if self.generational_progress > self.budget // 2:\n            self.exploration_phase = False\n        if self.exploration_phase:\n            new_pop_size = int(self.initial_pop_size * (1 + progress_rate))\n        else:\n            new_pop_size = int(self.initial_pop_size * (1 - progress_rate / 2))\n        self.population = self.population[:new_pop_size]\n\n    def update_mutation_rate_and_learning_rate(self):\n        self.mutation_rate = max(0.01, 0.1 * (1 - self.generational_progress / self.budget))\n        self.learning_rate = max(0.01, 0.1 * (1 - self.best_fitness / (self.best_fitness + 1e-9)))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            fitness = self.evaluate_population(func)\n            evaluations += len(fitness)\n\n            if evaluations >= self.budget:\n                break\n\n            parents = self.select_parents(fitness)\n            offspring = self.crossover(parents)\n            offspring = self.mutate(offspring, lb, ub)\n            offspring = self.apply_quantum_entanglement(offspring)\n            offspring = self.multiverse_immigration(offspring, lb, ub)\n\n            self.population = offspring\n            self.adapt_population_size()\n            self.update_mutation_rate_and_learning_rate()\n            self.generational_progress = evaluations\n\n        return self.best_solution, self.best_fitness", "name": "AdaptiveQuantumInspiredEA", "description": "Adaptive Quantum-Inspired Evolutionary Algorithm with dynamic multiverse-inspired immigration and mutation strategies for enhanced photonic structure optimization convergence.", "configspace": "", "generation": 26, "fitness": 0.23988904916785603, "feedback": "The algorithm AdaptiveQuantumInspiredEA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.24 with standard deviation 0.00.", "error": "", "parent_id": "84c4f2e1-0b5c-42cf-b84b-d494cee80539", "metadata": {"aucs": [0.2396297160207167, 0.23881841079495958, 0.2412190206878918]}, "mutation_prompt": null}
{"id": "44c6166a-aa98-430b-b158-6f347e247b3f", "solution": "import numpy as np\n\nclass QuantumGeneticAlgorithmAdaptive:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_pop_size = max(10, min(50, budget // 10))\n        self.population = None\n        self.best_solution = None\n        self.best_fitness = float('inf')\n        self.mutation_rate = 0.1\n        self.entanglement_factor = 0.5\n        self.generational_progress = 0\n        self.learning_rate = 0.1\n        self.exploration_phase = True\n\n    def initialize_population(self, lb, ub):\n        self.population = lb + (ub - lb) * np.random.rand(self.initial_pop_size, self.dim)\n\n    def evaluate_population(self, func):\n        fitness = np.array([func(ind) for ind in self.population])\n        best_index = np.argmin(fitness)\n        if fitness[best_index] < self.best_fitness:\n            self.best_fitness = fitness[best_index]\n            self.best_solution = self.population[best_index]\n        return fitness\n\n    def select_parents(self, fitness):\n        total_fitness = np.sum(1 / (fitness + 1e-9))\n        selection_prob = (1 / (fitness + 1e-9)) / total_fitness\n        indices = np.random.choice(len(fitness), size=len(fitness), p=selection_prob)\n        return self.population[indices]\n\n    def crossover(self, parents):\n        offspring = np.empty_like(parents)\n        for i in range(0, len(parents), 2):\n            if i+1 >= len(parents):\n                break\n            alpha = np.random.rand(self.dim)\n            offspring[i] = alpha * parents[i] + (1 - alpha) * parents[i+1]\n            offspring[i+1] = alpha * parents[i+1] + (1 - alpha) * parents[i]\n        return offspring\n\n    def mutate(self, offspring, lb, ub):\n        mutation_matrix = np.random.rand(*offspring.shape) < self.mutation_rate\n        random_values = lb + (ub - lb) * np.random.rand(*offspring.shape)\n        offspring = np.where(mutation_matrix, random_values, offspring)\n        return np.clip(offspring, lb, ub)\n\n    def apply_quantum_entanglement(self, offspring):\n        for i in range(len(offspring)):\n            if np.random.rand() < self.entanglement_factor:\n                partner_idx = np.random.randint(len(offspring))\n                qubit_superposition = 0.5 * (offspring[i] + offspring[partner_idx])\n                offspring[i] = qubit_superposition + np.random.normal(0, self.learning_rate, self.dim)\n        return offspring\n\n    def adaptive_quantum_tunneling(self, offspring, lb, ub, fitness, threshold=0.1):\n        # Adaptive tunneling based on fitness diversity\n        fitness_std = np.std(fitness)\n        if fitness_std < threshold:\n            tunneling_prob = 0.5  # Increase tunneling if population is stagnant\n        else:\n            tunneling_prob = 0.2  # Regular tunneling probability\n\n        if np.random.rand() < tunneling_prob:\n            random_individuals = lb + (ub - lb) * np.random.rand(3, self.dim)\n            indices = np.random.choice(len(offspring), size=3, replace=False)\n            offspring[indices] = random_individuals\n        return np.clip(offspring, lb, ub)\n\n    def adapt_population_size(self):\n        progress_rate = min(0.5, 2 * (1 - self.best_fitness / (self.best_fitness + 1e-9)))\n        if self.generational_progress > self.budget // 2:\n            self.exploration_phase = False\n        if self.exploration_phase:\n            new_pop_size = int(self.initial_pop_size * (1 + progress_rate))\n        else:\n            new_pop_size = int(self.initial_pop_size * (1 - progress_rate / 2))\n        self.population = self.population[:new_pop_size]\n\n    def update_mutation_rate_and_learning_rate(self):\n        self.mutation_rate = max(0.01, 0.1 * (1 - self.generational_progress / self.budget))\n        self.learning_rate = max(0.01, 0.1 * (1 - self.best_fitness / (self.best_fitness + 1e-9)))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            fitness = self.evaluate_population(func)\n            evaluations += len(fitness)\n\n            if evaluations >= self.budget:\n                break\n\n            parents = self.select_parents(fitness)\n            offspring = self.crossover(parents)\n            offspring = self.mutate(offspring, lb, ub)\n            offspring = self.apply_quantum_entanglement(offspring)\n            offspring = self.adaptive_quantum_tunneling(offspring, lb, ub, fitness)\n\n            self.population = offspring\n            self.adapt_population_size()\n            self.update_mutation_rate_and_learning_rate()\n            self.generational_progress = evaluations\n\n        return self.best_solution, self.best_fitness", "name": "QuantumGeneticAlgorithmAdaptive", "description": "Quantum Genetic Algorithm with adaptive tunneling and multi-modal retention for improved convergence in photonic structure optimization.", "configspace": "", "generation": 27, "fitness": 0.27675231458354493, "feedback": "The algorithm QuantumGeneticAlgorithmAdaptive got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28 with standard deviation 0.00.", "error": "", "parent_id": "84c4f2e1-0b5c-42cf-b84b-d494cee80539", "metadata": {"aucs": [0.2769721321381361, 0.27643440125798313, 0.27685041035451563]}, "mutation_prompt": null}
{"id": "9f3dc1ed-87c0-4eda-97a1-f36c3046c19e", "solution": "import numpy as np\n\nclass HybridSwarmEvolutionaryAlgorithm:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = max(20, min(60, budget // 8))\n        self.particles = None\n        self.best_solution = None\n        self.best_fitness = float('inf')\n        self.velocities = None\n        self.global_best_position = None\n        self.inertia = 0.7\n        self.cognitive_param = 1.5\n        self.social_param = 1.5\n        self.mutation_factor = 0.8\n        self.crossover_probability = 0.7\n\n    def initialize_swarm(self, lb, ub):\n        self.particles = lb + (ub - lb) * np.random.rand(self.swarm_size, self.dim)\n        self.velocities = np.zeros((self.swarm_size, self.dim))\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_fitnesses = np.full(self.swarm_size, float('inf'))\n\n    def evaluate_swarm(self, func):\n        fitness = np.array([func(p) for p in self.particles])\n        for i in range(self.swarm_size):\n            if fitness[i] < self.personal_best_fitnesses[i]:\n                self.personal_best_fitnesses[i] = fitness[i]\n                self.personal_best_positions[i] = self.particles[i]\n        global_best_index = np.argmin(self.personal_best_fitnesses)\n        if self.personal_best_fitnesses[global_best_index] < self.best_fitness:\n            self.best_fitness = self.personal_best_fitnesses[global_best_index]\n            self.global_best_position = self.personal_best_positions[global_best_index]\n\n    def update_velocities_and_positions(self, lb, ub):\n        r1, r2 = np.random.rand(self.swarm_size, self.dim), np.random.rand(self.swarm_size, self.dim)\n        cognitive_component = self.cognitive_param * r1 * (self.personal_best_positions - self.particles)\n        social_component = self.social_param * r2 * (self.global_best_position - self.particles)\n        self.velocities = self.inertia * self.velocities + cognitive_component + social_component\n        self.particles += self.velocities\n        self.particles = np.clip(self.particles, lb, ub)\n\n    def differential_evolution(self, lb, ub):\n        for i in range(self.swarm_size):\n            idxs = [idx for idx in range(self.swarm_size) if idx != i]\n            a, b, c = self.particles[np.random.choice(idxs, 3, replace=False)]\n            mutant = np.clip(a + self.mutation_factor * (b - c), lb, ub)\n            crossover = np.random.rand(self.dim) < self.crossover_probability\n            self.particles[i] = np.where(crossover, mutant, self.particles[i])\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_swarm(lb, ub)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            self.evaluate_swarm(func)\n            evaluations += self.swarm_size\n            if evaluations >= self.budget:\n                break\n            self.update_velocities_and_positions(lb, ub)\n            self.differential_evolution(lb, ub)\n\n        return self.global_best_position, self.best_fitness", "name": "HybridSwarmEvolutionaryAlgorithm", "description": "A Hybrid Swarm and Evolutionary Algorithm combining Particle Swarm Optimization with Differential Evolution to enhance global search capability and convergence speed for photonic structure optimization.", "configspace": "", "generation": 28, "fitness": 0.24786549157181503, "feedback": "The algorithm HybridSwarmEvolutionaryAlgorithm got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.25 with standard deviation 0.00.", "error": "", "parent_id": "84c4f2e1-0b5c-42cf-b84b-d494cee80539", "metadata": {"aucs": [0.246978896604788, 0.24499469620049652, 0.2516228819101606]}, "mutation_prompt": null}
{"id": "54a77999-4956-40d9-a3af-a33838aff665", "solution": "import numpy as np\n\nclass QuantumInspiredGeneticAlgorithm:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_pop_size = max(20, min(100, budget // 5))\n        self.population = None\n        self.best_solution = None\n        self.best_fitness = float('inf')\n        self.mutation_rate = 0.1\n        self.entanglement_factor = 0.6\n        self.generational_progress = 0\n        self.learning_rate = 0.1\n        self.exploration_phase = True\n        self.diversity_threshold = 0.1\n\n    def initialize_population(self, lb, ub):\n        self.population = lb + (ub - lb) * np.random.rand(self.initial_pop_size, self.dim)\n\n    def evaluate_population(self, func):\n        fitness = np.array([func(ind) for ind in self.population])\n        best_index = np.argmin(fitness)\n        if fitness[best_index] < self.best_fitness:\n            self.best_fitness = fitness[best_index]\n            self.best_solution = self.population[best_index]\n        return fitness\n\n    def select_parents(self, fitness):\n        selection_prob = 1 / (fitness + 1e-9)\n        selection_prob /= selection_prob.sum()\n        indices = np.random.choice(len(fitness), size=len(fitness), p=selection_prob)\n        return self.population[indices]\n\n    def crossover(self, parents):\n        offspring = np.empty_like(parents)\n        for i in range(0, len(parents), 2):\n            if i+1 >= len(parents):\n                break\n            alpha = np.random.rand(self.dim)\n            offspring[i] = alpha * parents[i] + (1 - alpha) * parents[i+1]\n            offspring[i+1] = alpha * parents[i+1] + (1 - alpha) * parents[i]\n        return offspring\n\n    def mutate(self, offspring, lb, ub):\n        mutation_matrix = np.random.rand(*offspring.shape) < self.mutation_rate\n        random_values = lb + (ub - lb) * np.random.rand(*offspring.shape)\n        offspring = np.where(mutation_matrix, random_values, offspring)\n        return np.clip(offspring, lb, ub)\n\n    def apply_quantum_entanglement(self, offspring):\n        for i in range(len(offspring)):\n            if np.random.rand() < self.entanglement_factor:\n                partner_idx = np.random.randint(len(offspring))\n                qubit_superposition = 0.5 * (offspring[i] + offspring[partner_idx])\n                offspring[i] = qubit_superposition + np.random.normal(0, self.learning_rate, self.dim)\n        return offspring\n\n    def quantum_tunneling(self, offspring, lb, ub):\n        diversity = np.std(offspring, axis=0).mean()\n        if diversity < self.diversity_threshold:\n            indices = np.random.choice(len(offspring), size=3, replace=False)\n            random_individuals = lb + (ub - lb) * np.random.rand(3, self.dim)\n            offspring[indices] = random_individuals\n        return np.clip(offspring, lb, ub)\n\n    def adapt_population_size(self):\n        if self.generational_progress > self.budget // 3:\n            self.exploration_phase = False\n        if self.exploration_phase:\n            new_pop_size = int(self.initial_pop_size * 1.2)\n        else:\n            new_pop_size = max(int(self.initial_pop_size * 0.8), 10)\n        self.population = self.population[:new_pop_size]\n\n    def update_mutation_rate_and_learning_rate(self):\n        self.mutation_rate = max(0.01, 0.1 * (1 - self.generational_progress / self.budget))\n        self.learning_rate = max(0.01, 0.1 * (1 - self.best_fitness / (self.best_fitness + 1e-9)))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            fitness = self.evaluate_population(func)\n            evaluations += len(fitness)\n\n            if evaluations >= self.budget:\n                break\n\n            parents = self.select_parents(fitness)\n            offspring = self.crossover(parents)\n            offspring = self.mutate(offspring, lb, ub)\n            offspring = self.apply_quantum_entanglement(offspring)\n            offspring = self.quantum_tunneling(offspring, lb, ub)\n\n            self.population = offspring\n            self.adapt_population_size()\n            self.update_mutation_rate_and_learning_rate()\n            self.generational_progress = evaluations\n\n        return self.best_solution, self.best_fitness", "name": "QuantumInspiredGeneticAlgorithm", "description": "Quantum-Inspired Genetic Algorithm with Adaptive Population Dynamics and Enhanced Quantum Tunneling for Efficient Photonic Structure Optimization.", "configspace": "", "generation": 29, "fitness": 0.27692516236273546, "feedback": "The algorithm QuantumInspiredGeneticAlgorithm got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28 with standard deviation 0.00.", "error": "", "parent_id": "84c4f2e1-0b5c-42cf-b84b-d494cee80539", "metadata": {"aucs": [0.2766612495525729, 0.27690759848601476, 0.27720663904961873]}, "mutation_prompt": null}
{"id": "eb7acd45-ab61-4357-aabc-58b5a81b6031", "solution": "import numpy as np\n\nclass AdvancedQuantumGeneticAlgorithm:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_pop_size = max(10, min(50, budget // 10))\n        self.population = None\n        self.best_solution = None\n        self.best_fitness = float('inf')\n        self.mutation_rate = 0.1\n        self.entanglement_factor = 0.5\n        self.generational_progress = 0\n        self.learning_rate = 0.1\n        self.elitism_rate = 0.05\n        self.exploration_phase = True\n\n    def initialize_population(self, lb, ub):\n        self.population = lb + (ub - lb) * np.random.rand(self.initial_pop_size, self.dim)\n\n    def evaluate_population(self, func):\n        fitness = np.array([func(ind) for ind in self.population])\n        best_index = np.argmin(fitness)\n        if fitness[best_index] < self.best_fitness:\n            self.best_fitness = fitness[best_index]\n            self.best_solution = self.population[best_index]\n        return fitness\n\n    def select_parents(self, fitness):\n        total_fitness = np.sum(1 / (fitness + 1e-9))\n        selection_prob = (1 / (fitness + 1e-9)) / total_fitness\n        indices = np.random.choice(len(fitness), size=len(fitness), p=selection_prob)\n        return self.population[indices]\n\n    def elitism(self, parents, fitness):\n        elite_count = max(1, int(self.elitism_rate * len(parents)))\n        elite_indices = np.argsort(fitness)[:elite_count]\n        return self.population[elite_indices]\n\n    def crossover(self, parents):\n        offspring = np.empty_like(parents)\n        for i in range(0, len(parents), 2):\n            if i + 1 >= len(parents):\n                break\n            alpha = np.random.rand(self.dim)\n            offspring[i] = alpha * parents[i] + (1 - alpha) * parents[i + 1]\n            offspring[i + 1] = alpha * parents[i + 1] + (1 - alpha) * parents[i]\n        return offspring\n\n    def mutate(self, offspring, lb, ub):\n        mutation_matrix = np.random.rand(*offspring.shape) < self.mutation_rate\n        random_values = lb + (ub - lb) * np.random.rand(*offspring.shape)\n        offspring = np.where(mutation_matrix, random_values, offspring)\n        return np.clip(offspring, lb, ub)\n\n    def apply_quantum_entanglement(self, offspring):\n        for i in range(len(offspring)):\n            if np.random.rand() < self.entanglement_factor:\n                partner_idx = np.random.randint(len(offspring))\n                qubit_superposition = 0.5 * (offspring[i] + offspring[partner_idx])\n                offspring[i] = qubit_superposition + np.random.normal(0, self.learning_rate, self.dim)\n        return offspring\n\n    def quantum_tunneling(self, offspring, lb, ub):\n        if np.random.rand() < 0.3:\n            stagnation_threshold = int(0.1 * self.budget)\n            if self.generational_progress > stagnation_threshold:\n                random_individual = lb + (ub - lb) * np.random.rand(1, self.dim)\n                worst_index = np.argmax([func(ind) for ind in offspring])\n                offspring[worst_index] = random_individual\n        return np.clip(offspring, lb, ub)\n\n    def adapt_population_size(self):\n        progress_rate = min(0.5, 2 * (1 - self.best_fitness / (self.best_fitness + 1e-9)))\n        if self.generational_progress > self.budget // 2:\n            self.exploration_phase = False\n        if self.exploration_phase:\n            new_pop_size = int(self.initial_pop_size * (1 + progress_rate))\n        else:\n            new_pop_size = int(self.initial_pop_size * (1 - progress_rate / 2))\n        self.population = self.population[:new_pop_size]\n\n    def update_mutation_rate_and_learning_rate(self):\n        self.mutation_rate = max(0.01, 0.1 * (1 - self.generational_progress / self.budget))\n        self.learning_rate = max(0.01, 0.1 * (1 - self.best_fitness / (self.best_fitness + 1e-9)))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            fitness = self.evaluate_population(func)\n            evaluations += len(fitness)\n\n            if evaluations >= self.budget:\n                break\n\n            elite_individuals = self.elitism(self.population, fitness)\n            parents = self.select_parents(fitness)\n            offspring = self.crossover(parents)\n            offspring = self.mutate(offspring, lb, ub)\n            offspring = self.apply_quantum_entanglement(offspring)\n            offspring = self.quantum_tunneling(offspring, lb, ub)\n\n            self.population = np.vstack((offspring, elite_individuals))\n            self.adapt_population_size()\n            self.update_mutation_rate_and_learning_rate()\n            self.generational_progress = evaluations\n\n        return self.best_solution, self.best_fitness", "name": "AdvancedQuantumGeneticAlgorithm", "description": "Advanced Quantum Genetic Algorithm with adaptive mutation, enhanced tunneling strategies, and elitism for improved global optimization of photonic structures.", "configspace": "", "generation": 30, "fitness": -Infinity, "feedback": "An exception occurred: NameError(\"name 'func' is not defined\").", "error": "NameError(\"name 'func' is not defined\")", "parent_id": "84c4f2e1-0b5c-42cf-b84b-d494cee80539", "metadata": {}, "mutation_prompt": null}
{"id": "6c99d009-4832-4552-aa16-f913ba6f45d6", "solution": "import numpy as np\n\nclass AdaptiveMemeticAlgorithm:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = max(20, min(100, budget // 5))\n        self.population = None\n        self.best_solution = None\n        self.best_fitness = float('inf')\n        self.crossover_rate = 0.8\n        self.mutation_factor = 0.9\n        self.temperature = 1.0\n        self.cooling_rate = 0.99\n\n    def initialize_population(self, lb, ub):\n        self.population = lb + (ub - lb) * np.random.rand(self.pop_size, self.dim)\n\n    def evaluate_population(self, func):\n        fitness = np.array([func(ind) for ind in self.population])\n        best_index = np.argmin(fitness)\n        if fitness[best_index] < self.best_fitness:\n            self.best_fitness = fitness[best_index]\n            self.best_solution = self.population[best_index]\n        return fitness\n\n    def select_parents(self, fitness):\n        indices = np.random.choice(self.pop_size, size=3, replace=False)\n        return self.population[indices]\n\n    def differential_evolution(self, parents, lb, ub):\n        target, a, b, c = parents\n        mutant = np.clip(a + self.mutation_factor * (b - c), lb, ub)\n        trial = np.where(np.random.rand(self.dim) < self.crossover_rate, mutant, target)\n        return trial\n\n    def simulated_annealing(self, solution, lb, ub):\n        candidate = solution + np.random.normal(0, self.temperature, self.dim)\n        candidate = np.clip(candidate, lb, ub)\n        return candidate\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            fitness = self.evaluate_population(func)\n            evaluations += self.pop_size\n\n            if evaluations >= self.budget:\n                break\n\n            new_population = []\n            for _ in range(self.pop_size):\n                parents = self.select_parents(fitness)\n                trial = self.differential_evolution(parents, lb, ub)\n                sa_trial = self.simulated_annealing(trial, lb, ub)\n                trial_fitness = func(trial)\n                sa_trial_fitness = func(sa_trial)\n                if sa_trial_fitness < trial_fitness:\n                    new_population.append(sa_trial)\n                else:\n                    new_population.append(trial)\n                evaluations += 2  # Two additional evaluations\n\n                if evaluations >= self.budget:\n                    break\n\n            self.population = np.array(new_population)\n            self.temperature *= self.cooling_rate\n\n        return self.best_solution, self.best_fitness", "name": "AdaptiveMemeticAlgorithm", "description": "Adaptive Memetic Algorithm with Differential Evolution and Simulated Annealing for efficient global optimization of photonic structures.", "configspace": "", "generation": 31, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('not enough values to unpack (expected 4, got 3)').", "error": "ValueError('not enough values to unpack (expected 4, got 3)')", "parent_id": "84c4f2e1-0b5c-42cf-b84b-d494cee80539", "metadata": {}, "mutation_prompt": null}
{"id": "de745f80-bde2-4bd5-985d-59440f40e402", "solution": "import numpy as np\n\nclass ImprovedQuantumGeneticAlgorithm:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_pop_size = max(10, min(50, budget // 10))\n        self.population = None\n        self.best_solution = None\n        self.best_fitness = float('inf')\n        self.mutation_rate = 0.1\n        self.entanglement_factor = 0.5\n        self.generational_progress = 0\n        self.learning_rate = 0.1\n        self.exploration_phase = True\n        self.entropy_threshold = 0.1\n\n    def initialize_population(self, lb, ub):\n        self.population = lb + (ub - lb) * np.random.rand(self.initial_pop_size, self.dim)\n\n    def evaluate_population(self, func):\n        fitness = np.array([func(ind) for ind in self.population])\n        best_index = np.argmin(fitness)\n        if fitness[best_index] < self.best_fitness:\n            self.best_fitness = fitness[best_index]\n            self.best_solution = self.population[best_index]\n        return fitness\n\n    def select_parents(self, fitness):\n        total_fitness = np.sum(1 / (fitness + 1e-9))\n        selection_prob = (1 / (fitness + 1e-9)) / total_fitness\n        indices = np.random.choice(len(fitness), size=len(fitness), p=selection_prob)\n        return self.population[indices]\n\n    def crossover(self, parents):\n        offspring = np.empty_like(parents)\n        for i in range(0, len(parents), 2):\n            if i+1 >= len(parents):\n                break\n            alpha = np.random.rand(self.dim)\n            offspring[i] = alpha * parents[i] + (1 - alpha) * parents[i+1]\n            offspring[i+1] = alpha * parents[i+1] + (1 - alpha) * parents[i]\n        return offspring\n\n    def mutate(self, offspring, lb, ub):\n        mutation_strength = np.random.normal(0, self.mutation_rate, offspring.shape)\n        offspring += mutation_strength\n        random_mutation = lb + (ub - lb) * np.random.rand(*offspring.shape)\n        mutation_matrix = np.random.rand(*offspring.shape) < self.mutation_rate\n        offspring = np.where(mutation_matrix, random_mutation, offspring)\n        return np.clip(offspring, lb, ub)\n\n    def apply_quantum_entanglement(self, offspring):\n        for i in range(len(offspring)):\n            if np.random.rand() < self.entanglement_factor:\n                partner_idx = np.random.randint(len(offspring))\n                qubit_superposition = 0.5 * (offspring[i] + offspring[partner_idx])\n                offspring[i] = qubit_superposition + np.random.normal(0, self.learning_rate, self.dim)\n        return offspring\n\n    def quantum_tunneling(self, offspring, lb, ub):\n        if np.random.rand() < 0.3:\n            random_individuals = lb + (ub - lb) * np.random.rand(3, self.dim)\n            indices = np.random.choice(len(offspring), size=3, replace=False)\n            offspring[indices] = random_individuals\n        return np.clip(offspring, lb, ub)\n\n    def adapt_population_size(self):\n        entropy = -np.sum((self.entanglement_factor * np.log(self.entanglement_factor + 1e-9)))\n        if entropy < self.entropy_threshold:\n            new_pop_size = max(10, int(self.initial_pop_size * 1.1))\n        else:\n            new_pop_size = max(10, int(self.initial_pop_size * 0.9))\n        self.population = self.population[:new_pop_size]\n\n    def update_mutation_rate_and_learning_rate(self):\n        self.mutation_rate = max(0.01, 0.1 * (1 - self.generational_progress / self.budget))\n        self.learning_rate = max(0.01, 0.1 * (1 - self.best_fitness / (self.best_fitness + 1e-9)))\n        self.entanglement_factor = min(0.9, self.entanglement_factor + 0.01)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            fitness = self.evaluate_population(func)\n            evaluations += len(fitness)\n\n            if evaluations >= self.budget:\n                break\n\n            parents = self.select_parents(fitness)\n            offspring = self.crossover(parents)\n            offspring = self.mutate(offspring, lb, ub)\n            offspring = self.apply_quantum_entanglement(offspring)\n            offspring = self.quantum_tunneling(offspring, lb, ub)\n\n            self.population = offspring\n            self.adapt_population_size()\n            self.update_mutation_rate_and_learning_rate()\n            self.generational_progress = evaluations\n\n        return self.best_solution, self.best_fitness", "name": "ImprovedQuantumGeneticAlgorithm", "description": "An improved Quantum Genetic Algorithm with adaptive entanglement scaling and hybrid mutation strategy for enhanced exploration-exploitation balance in photonic structure optimization.", "configspace": "", "generation": 32, "fitness": 0.27686866427701284, "feedback": "The algorithm ImprovedQuantumGeneticAlgorithm got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28 with standard deviation 0.00.", "error": "", "parent_id": "84c4f2e1-0b5c-42cf-b84b-d494cee80539", "metadata": {"aucs": [0.2768508687763388, 0.2769364636688224, 0.2768186603858773]}, "mutation_prompt": null}
{"id": "7ee4669f-d741-4d78-92b5-bad82a6ebcd7", "solution": "import numpy as np\n\nclass AdaptiveQuantumGeneticAlgorithm:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_pop_size = max(10, min(50, budget // 10))\n        self.population = None\n        self.best_solution = None\n        self.best_fitness = float('inf')\n        self.mutation_rate = 0.1\n        self.entanglement_factor = 0.5\n        self.generational_progress = 0\n        self.learning_rate = 0.1\n        self.exploration_phase = True\n\n    def initialize_population(self, lb, ub):\n        self.population = lb + (ub - lb) * np.random.rand(self.initial_pop_size, self.dim)\n\n    def evaluate_population(self, func):\n        fitness = np.array([func(ind) for ind in self.population])\n        best_index = np.argmin(fitness)\n        if fitness[best_index] < self.best_fitness:\n            self.best_fitness = fitness[best_index]\n            self.best_solution = self.population[best_index]\n        return fitness\n\n    def select_parents(self, fitness):\n        total_fitness = np.sum(1 / (fitness + 1e-9))\n        selection_prob = (1 / (fitness + 1e-9)) / total_fitness\n        indices = np.random.choice(len(fitness), size=len(fitness), p=selection_prob)\n        return self.population[indices]\n\n    def crossover(self, parents):\n        offspring = np.empty_like(parents)\n        entropy = np.random.rand(self.dim)\n        for i in range(0, len(parents), 2):\n            if i + 1 >= len(parents):\n                break\n            alpha = entropy\n            offspring[i] = alpha * parents[i] + (1 - alpha) * parents[i + 1]\n            offspring[i + 1] = alpha * parents[i + 1] + (1 - alpha) * parents[i]\n        return offspring\n\n    def mutate(self, offspring, lb, ub):\n        entropy = np.random.rand(*offspring.shape)\n        mutation_matrix = entropy < self.mutation_rate\n        random_values = lb + (ub - lb) * np.random.rand(*offspring.shape)\n        offspring = np.where(mutation_matrix, random_values, offspring)\n        return np.clip(offspring, lb, ub)\n\n    def apply_quantum_entanglement(self, offspring):\n        for i in range(len(offspring)):\n            if np.random.rand() < self.entanglement_factor:\n                partner_idx = np.random.randint(len(offspring))\n                qubit_superposition = 0.5 * (offspring[i] + offspring[partner_idx])\n                offspring[i] = qubit_superposition + np.random.normal(0, self.learning_rate, self.dim)\n        return offspring\n\n    def quantum_tunneling(self, offspring, lb, ub):\n        if np.random.rand() < 0.3:\n            random_individuals = lb + (ub - lb) * np.random.rand(3, self.dim)\n            indices = np.random.choice(len(offspring), size=3, replace=False)\n            offspring[indices] = random_individuals\n        return np.clip(offspring, lb, ub)\n\n    def adapt_population_size(self):\n        progress_rate = min(0.5, 2 * (1 - self.best_fitness / (self.best_fitness + 1e-9)))\n        if self.generational_progress > self.budget // 2:\n            self.exploration_phase = False\n        if self.exploration_phase:\n            new_pop_size = int(self.initial_pop_size * (1 + progress_rate))\n        else:\n            new_pop_size = int(self.initial_pop_size * (1 - progress_rate / 2))\n        self.population = self.population[:new_pop_size]\n\n    def update_mutation_rate_and_learning_rate(self):\n        self.mutation_rate = max(0.01, 0.1 * (1 - self.generational_progress / self.budget))\n        self.learning_rate = max(0.01, 0.1 * (1 - self.best_fitness / (self.best_fitness + 1e-9)))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            fitness = self.evaluate_population(func)\n            evaluations += len(fitness)\n\n            if evaluations >= self.budget:\n                break\n\n            parents = self.select_parents(fitness)\n            offspring = self.crossover(parents)\n            offspring = self.mutate(offspring, lb, ub)\n            offspring = self.apply_quantum_entanglement(offspring)\n            offspring = self.quantum_tunneling(offspring, lb, ub)\n\n            self.population = offspring\n            self.adapt_population_size()\n            self.update_mutation_rate_and_learning_rate()\n            self.generational_progress = evaluations\n\n        return self.best_solution, self.best_fitness", "name": "AdaptiveQuantumGeneticAlgorithm", "description": "Adaptive Quantum Genetic Algorithm with entropy-based dynamic mutation and crossover strategies to enhance exploration and exploitation balance in optimizing photonic structures.", "configspace": "", "generation": 33, "fitness": 0.27710177353838145, "feedback": "The algorithm AdaptiveQuantumGeneticAlgorithm got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28 with standard deviation 0.00.", "error": "", "parent_id": "84c4f2e1-0b5c-42cf-b84b-d494cee80539", "metadata": {"aucs": [0.27696945401035133, 0.27732936231618943, 0.2770065042886036]}, "mutation_prompt": null}
{"id": "2a6a2f88-e2ca-4eae-8611-d80f1821310f", "solution": "import numpy as np\n\nclass QuantumInspiredDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = max(10, min(50, budget // 10))\n        self.population = None\n        self.best_solution = None\n        self.best_fitness = float('inf')\n        self.F = 0.5  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.levy_alpha = 1.5  # Levy flight parameter\n        self.evaluations = 0\n\n    def initialize_population(self, lb, ub):\n        self.population = lb + (ub - lb) * np.random.rand(self.pop_size, self.dim)\n\n    def levy_flight(self):\n        u = np.random.normal(0, 1, self.dim) * (1 / np.abs(np.random.normal(0, 1)) ** (1 / self.levy_alpha))\n        return u\n\n    def evaluate_population(self, func):\n        fitness = np.array([func(ind) for ind in self.population])\n        best_index = np.argmin(fitness)\n        if fitness[best_index] < self.best_fitness:\n            self.best_fitness = fitness[best_index]\n            self.best_solution = self.population[best_index]\n        self.evaluations += len(fitness)\n        return fitness\n\n    def mutate_and_crossover(self, lb, ub):\n        new_population = np.empty_like(self.population)\n        for i in range(self.pop_size):\n            indices = np.random.choice(self.pop_size, 3, replace=False)\n            x_r1, x_r2, x_r3 = self.population[indices]\n            mutant = x_r1 + self.F * (x_r2 - x_r3) + self.levy_flight()\n            mutant = np.clip(mutant, lb, ub)\n            trial = np.where(np.random.rand(self.dim) < self.CR, mutant, self.population[i])\n            new_population[i] = trial\n        return new_population\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n\n        while self.evaluations < self.budget:\n            fitness = self.evaluate_population(func)\n            if self.evaluations >= self.budget:\n                break\n\n            offspring = self.mutate_and_crossover(lb, ub)\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.evaluations += len(offspring_fitness)\n\n            for i in range(self.pop_size):\n                if offspring_fitness[i] < fitness[i]:\n                    self.population[i] = offspring[i]\n                    fitness[i] = offspring_fitness[i]\n\n            best_index = np.argmin(fitness)\n            if fitness[best_index] < self.best_fitness:\n                self.best_fitness = fitness[best_index]\n                self.best_solution = self.population[best_index]\n\n        return self.best_solution, self.best_fitness", "name": "QuantumInspiredDifferentialEvolution", "description": "Quantum-Inspired Differential Evolution with Adaptive Levy Flight Mutations for Enhanced Exploration in Photonic Structure Optimization.", "configspace": "", "generation": 34, "fitness": 0.27003601658451687, "feedback": "The algorithm QuantumInspiredDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.27 with standard deviation 0.00.", "error": "", "parent_id": "7ee4669f-d741-4d78-92b5-bad82a6ebcd7", "metadata": {"aucs": [0.27045394710259174, 0.2711301153453123, 0.2685239873056465]}, "mutation_prompt": null}
{"id": "776cbbdc-781a-4693-bad6-9ee4c6705aff", "solution": "import numpy as np\n\nclass EnhancedQuantumGeneticAlgorithm:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_pop_size = max(10, min(50, budget // 10))\n        self.population = None\n        self.best_solution = None\n        self.best_fitness = float('inf')\n        self.mutation_rate = 0.1\n        self.entanglement_factor = 0.5\n        self.generational_progress = 0\n        self.learning_rate = 0.1\n        self.exploration_phase = True\n        self.agent_factor = 0.3\n\n    def initialize_population(self, lb, ub):\n        self.population = lb + (ub - lb) * np.random.rand(self.initial_pop_size, self.dim)\n\n    def evaluate_population(self, func):\n        fitness = np.array([func(ind) for ind in self.population])\n        best_index = np.argmin(fitness)\n        if fitness[best_index] < self.best_fitness:\n            self.best_fitness = fitness[best_index]\n            self.best_solution = self.population[best_index]\n        return fitness\n\n    def select_parents(self, fitness):\n        total_fitness = np.sum(1 / (fitness + 1e-9))\n        selection_prob = (1 / (fitness + 1e-9)) / total_fitness\n        indices = np.random.choice(len(fitness), size=len(fitness), p=selection_prob)\n        return self.population[indices]\n\n    def crossover(self, parents):\n        offspring = np.empty_like(parents)\n        entropy = np.random.rand(self.dim)\n        for i in range(0, len(parents), 2):\n            if i + 1 >= len(parents):\n                break\n            alpha = entropy\n            offspring[i] = alpha * parents[i] + (1 - alpha) * parents[i + 1]\n            offspring[i + 1] = alpha * parents[i + 1] + (1 - alpha) * parents[i]\n        return offspring\n\n    def mutate(self, offspring, lb, ub):\n        entropy = np.random.rand(*offspring.shape)\n        mutation_matrix = entropy < self.mutation_rate\n        random_values = lb + (ub - lb) * np.random.rand(*offspring.shape)\n        offspring = np.where(mutation_matrix, random_values, offspring)\n        return np.clip(offspring, lb, ub)\n\n    def apply_quantum_entanglement(self, offspring):\n        for i in range(len(offspring)):\n            if np.random.rand() < self.entanglement_factor:\n                partner_idx = np.random.randint(len(offspring))\n                qubit_superposition = 0.5 * (offspring[i] + offspring[partner_idx])\n                offspring[i] = qubit_superposition + np.random.normal(0, self.learning_rate, self.dim)\n        return offspring\n\n    def quantum_tunneling(self, offspring, lb, ub):\n        if np.random.rand() < 0.3:\n            random_individuals = lb + (ub - lb) * np.random.rand(3, self.dim)\n            indices = np.random.choice(len(offspring), size=3, replace=False)\n            offspring[indices] = random_individuals\n        return np.clip(offspring, lb, ub)\n\n    def adapt_population_size(self):\n        progress_rate = min(0.5, 2 * (1 - self.best_fitness / (self.best_fitness + 1e-9)))\n        if self.generational_progress > self.budget // 2:\n            self.exploration_phase = False\n        if self.exploration_phase:\n            new_pop_size = int(self.initial_pop_size * (1 + progress_rate))\n        else:\n            new_pop_size = int(self.initial_pop_size * (1 - progress_rate / 2))\n        self.population = self.population[:new_pop_size]\n\n    def update_mutation_rate_and_learning_rate(self):\n        self.mutation_rate = max(0.01, 0.1 * (1 - self.generational_progress / self.budget))\n        self.learning_rate = max(0.01, 0.1 * (1 - self.best_fitness / (self.best_fitness + 1e-9)))\n\n    def cooperative_learning(self):\n        num_agents = int(self.initial_pop_size * self.agent_factor)\n        for _ in range(num_agents):\n            i, j = np.random.choice(len(self.population), 2, replace=False)\n            if np.random.rand() < 0.5:\n                self.population[i] = self.population[i] + np.random.rand() * (self.best_solution - self.population[i])\n            else:\n                self.population[j] = self.population[j] + np.random.rand() * (self.best_solution - self.population[j])\n            self.population = np.clip(self.population, lb, ub)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            fitness = self.evaluate_population(func)\n            evaluations += len(fitness)\n\n            if evaluations >= self.budget:\n                break\n\n            parents = self.select_parents(fitness)\n            offspring = self.crossover(parents)\n            offspring = self.mutate(offspring, lb, ub)\n            offspring = self.apply_quantum_entanglement(offspring)\n            offspring = self.quantum_tunneling(offspring, lb, ub)\n            self.cooperative_learning()\n\n            self.population = offspring\n            self.adapt_population_size()\n            self.update_mutation_rate_and_learning_rate()\n            self.generational_progress = evaluations\n\n        return self.best_solution, self.best_fitness", "name": "EnhancedQuantumGeneticAlgorithm", "description": "Enhanced Quantum Genetic Algorithm with self-adaptive population dynamics and multi-agent cooperative learning to optimize photonic structures more efficiently.", "configspace": "", "generation": 35, "fitness": -Infinity, "feedback": "An exception occurred: NameError(\"name 'lb' is not defined\").", "error": "NameError(\"name 'lb' is not defined\")", "parent_id": "7ee4669f-d741-4d78-92b5-bad82a6ebcd7", "metadata": {}, "mutation_prompt": null}
{"id": "20352cb8-1339-4db0-9c5d-c40040edc31f", "solution": "import numpy as np\n\nclass QuantumInspiredParticleSwarmOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = max(20, min(60, budget // 20))\n        self.velocity = np.random.rand(self.swarm_size, self.dim)\n        self.position = None\n        self.personal_best_position = None\n        self.personal_best_fitness = np.full(self.swarm_size, np.inf)\n        self.global_best_position = None\n        self.global_best_fitness = np.inf\n        self.inertia_weight = 0.7\n        self.cognitive_coefficient = 1.5\n        self.social_coefficient = 1.5\n        self.neighborhood_topology = np.random.randint(0, self.swarm_size, size=(self.swarm_size, 3))\n\n    def initialize_swarm(self, lb, ub):\n        self.position = lb + (ub - lb) * np.random.rand(self.swarm_size, self.dim)\n        self.personal_best_position = self.position.copy()\n\n    def evaluate_swarm(self, func):\n        fitness = np.array([func(ind) for ind in self.position])\n        better_mask = fitness < self.personal_best_fitness\n        self.personal_best_fitness[better_mask] = fitness[better_mask]\n        self.personal_best_position[better_mask] = self.position[better_mask]\n        \n        global_best_idx = np.argmin(self.personal_best_fitness)\n        if self.personal_best_fitness[global_best_idx] < self.global_best_fitness:\n            self.global_best_fitness = self.personal_best_fitness[global_best_idx]\n            self.global_best_position = self.personal_best_position[global_best_idx]\n\n    def update_velocity_and_position(self, lb, ub):\n        for i in range(self.swarm_size):\n            local_best_position = self.position[self.neighborhood_topology[i]].min(axis=0)\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            cognitive_component = self.cognitive_coefficient * r1 * (self.personal_best_position[i] - self.position[i])\n            social_component = self.social_coefficient * r2 * (local_best_position - self.position[i])\n            quantized_update = self.inertia_weight * self.velocity[i] + cognitive_component + social_component\n            self.velocity[i] = np.random.normal(loc=quantized_update, scale=0.1)\n            self.position[i] += self.velocity[i]\n            self.position[i] = np.clip(self.position[i], lb, ub)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_swarm(lb, ub)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            self.evaluate_swarm(func)\n            evaluations += self.swarm_size\n\n            if evaluations >= self.budget:\n                break\n\n            self.update_velocity_and_position(lb, ub)\n\n            # Dynamic adjustment of neighborhood topology\n            if evaluations % (self.budget // 10) == 0:\n                self.neighborhood_topology = np.random.randint(0, self.swarm_size, size=(self.swarm_size, 3))\n\n        return self.global_best_position, self.global_best_fitness", "name": "QuantumInspiredParticleSwarmOptimization", "description": "Quantum-Inspired Particle Swarm Optimization (QIPSO) with hybridized positional updates and dynamic neighborhood topology for enhanced convergence in black box optimization.", "configspace": "", "generation": 36, "fitness": 0.2387731415857113, "feedback": "The algorithm QuantumInspiredParticleSwarmOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.24 with standard deviation 0.00.", "error": "", "parent_id": "7ee4669f-d741-4d78-92b5-bad82a6ebcd7", "metadata": {"aucs": [0.23477912594664607, 0.2434711341405843, 0.23806916466990358]}, "mutation_prompt": null}
{"id": "dbaae95e-d1ad-4fc7-8c01-e3e31d8fe82f", "solution": "import numpy as np\n\nclass QuantumInspiredParticleSwarmOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = max(10, min(50, budget // 10))\n        self.positions = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_scores = None\n        self.global_best_position = None\n        self.global_best_score = float('inf')\n        self.c1 = 2.0  # Cognitive coefficient\n        self.c2 = 2.0  # Social coefficient\n        self.w = 0.7   # Inertia weight\n        self.quantum_factor = 0.1\n\n    def initialize_swarm(self, lb, ub):\n        self.positions = lb + (ub - lb) * np.random.rand(self.swarm_size, self.dim)\n        self.velocities = np.random.rand(self.swarm_size, self.dim) - 0.5\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_scores = np.full(self.swarm_size, float('inf'))\n\n    def evaluate_swarm(self, func):\n        fitness = np.array([func(p) for p in self.positions])\n        for i in range(self.swarm_size):\n            if fitness[i] < self.personal_best_scores[i]:\n                self.personal_best_scores[i] = fitness[i]\n                self.personal_best_positions[i] = self.positions[i]\n            if fitness[i] < self.global_best_score:\n                self.global_best_score = fitness[i]\n                self.global_best_position = self.positions[i]\n\n    def update_velocities_and_positions(self, lb, ub):\n        for i in range(self.swarm_size):\n            r1, r2 = np.random.rand(), np.random.rand()\n            cognitive_component = self.c1 * r1 * (self.personal_best_positions[i] - self.positions[i])\n            social_component = self.c2 * r2 * (self.global_best_position - self.positions[i])\n            quantum_velocity = self.quantum_factor * (np.random.rand(self.dim) - 0.5)\n            self.velocities[i] = self.w * self.velocities[i] + cognitive_component + social_component + quantum_velocity\n\n            self.positions[i] += self.velocities[i]\n            # Ensure the particles stay within bounds\n            self.positions[i] = np.clip(self.positions[i], lb, ub)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_swarm(lb, ub)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            self.evaluate_swarm(func)\n            evaluations += self.swarm_size\n\n            if evaluations >= self.budget:\n                break\n\n            self.update_velocities_and_positions(lb, ub)\n\n        return self.global_best_position, self.global_best_score", "name": "QuantumInspiredParticleSwarmOptimization", "description": "Quantum Inspired Particle Swarm Optimization (QIPSO) with dynamic neighborhood topology and velocity quantization for efficient exploration and exploitation in photonic structure optimization.", "configspace": "", "generation": 37, "fitness": 0.27533532687856765, "feedback": "The algorithm QuantumInspiredParticleSwarmOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28 with standard deviation 0.00.", "error": "", "parent_id": "7ee4669f-d741-4d78-92b5-bad82a6ebcd7", "metadata": {"aucs": [0.27598704323513634, 0.2729902601025642, 0.2770286772980024]}, "mutation_prompt": null}
{"id": "480f0a6e-c16e-42cf-8a79-ea13e4e01583", "solution": "import numpy as np\n\nclass QuantumEntangledPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = max(10, min(30, budget // 15))\n        self.positions = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.global_best_position = None\n        self.personal_best_scores = None\n        self.global_best_score = float('inf')\n        self.inertia_weight = 0.7\n        self.cognitive_constant = 1.5\n        self.social_constant = 1.5\n        self.entanglement_factor = 0.3\n\n    def initialize_particles(self, lb, ub):\n        self.positions = lb + (ub - lb) * np.random.rand(self.num_particles, self.dim)\n        self.velocities = np.random.rand(self.num_particles, self.dim) * (ub - lb) * 0.1\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_scores = np.full(self.num_particles, float('inf'))\n\n    def update_velocities_and_positions(self, lb, ub):\n        r1 = np.random.rand(self.num_particles, self.dim)\n        r2 = np.random.rand(self.num_particles, self.dim)\n\n        cognitive_component = self.cognitive_constant * r1 * (self.personal_best_positions - self.positions)\n        social_component = self.social_constant * r2 * (self.global_best_position - self.positions)\n        \n        self.velocities = self.inertia_weight * self.velocities + cognitive_component + social_component\n        self.positions += self.velocities\n        \n        self.positions = np.clip(self.positions, lb, ub)\n\n    def apply_quantum_entanglement(self):\n        for i in range(self.num_particles):\n            if np.random.rand() < self.entanglement_factor:\n                partner_idx = np.random.randint(self.num_particles)\n                self.positions[i] = 0.5 * (self.positions[i] + self.positions[partner_idx]) + np.random.normal(0, 0.05, self.dim)\n\n    def evaluate_particles(self, func):\n        scores = np.array([func(pos) for pos in self.positions])\n        for i in range(self.num_particles):\n            if scores[i] < self.personal_best_scores[i]:\n                self.personal_best_scores[i] = scores[i]\n                self.personal_best_positions[i] = self.positions[i]\n            if scores[i] < self.global_best_score:\n                self.global_best_score = scores[i]\n                self.global_best_position = self.positions[i]\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_particles(lb, ub)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            self.evaluate_particles(func)\n            evaluations += self.num_particles\n            \n            if evaluations >= self.budget:\n                break\n\n            self.update_velocities_and_positions(lb, ub)\n            self.apply_quantum_entanglement()\n\n        return self.global_best_position, self.global_best_score", "name": "QuantumEntangledPSO", "description": "Particle Swarm Optimization with Quantum Entanglement for enhanced global search in photonic structures.", "configspace": "", "generation": 38, "fitness": 0.273666964745427, "feedback": "The algorithm QuantumEntangledPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.27 with standard deviation 0.00.", "error": "", "parent_id": "7ee4669f-d741-4d78-92b5-bad82a6ebcd7", "metadata": {"aucs": [0.2773868711277365, 0.2704144326573542, 0.27319959045119036]}, "mutation_prompt": null}
{"id": "a4da40fd-af0e-4d86-9a2b-d2f30f42cd77", "solution": "import numpy as np\n\nclass EnhancedAdaptiveQuantumGeneticAlgorithm:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_pop_size = max(10, min(50, budget // 10))\n        self.population = None\n        self.best_solution = None\n        self.best_fitness = float('inf')\n        self.mutation_rate = 0.1\n        self.entanglement_factor = 0.5\n        self.generational_progress = 0\n        self.learning_rate = 0.1\n        self.exploration_phase = True\n        self.diversity_threshold = 0.1\n\n    def initialize_population(self, lb, ub):\n        self.population = lb + (ub - lb) * np.random.rand(self.initial_pop_size, self.dim)\n\n    def evaluate_population(self, func):\n        fitness = np.array([func(ind) for ind in self.population])\n        best_index = np.argmin(fitness)\n        if fitness[best_index] < self.best_fitness:\n            self.best_fitness = fitness[best_index]\n            self.best_solution = self.population[best_index]\n        return fitness\n\n    def select_parents(self, fitness):\n        total_fitness = np.sum(1 / (fitness + 1e-9))\n        selection_prob = (1 / (fitness + 1e-9)) / total_fitness\n        indices = np.random.choice(len(fitness), size=len(fitness), p=selection_prob)\n        return self.population[indices]\n\n    def crossover(self, parents):\n        offspring = np.empty_like(parents)\n        entropy = np.random.rand(self.dim)\n        for i in range(0, len(parents), 2):\n            if i + 1 >= len(parents):\n                break\n            alpha = entropy\n            offspring[i] = alpha * parents[i] + (1 - alpha) * parents[i + 1]\n            offspring[i + 1] = alpha * parents[i + 1] + (1 - alpha) * parents[i]\n        return offspring\n\n    def mutate(self, offspring, lb, ub):\n        entropy = np.random.rand(*offspring.shape)\n        mutation_matrix = entropy < self.mutation_rate\n        random_values = lb + (ub - lb) * np.random.rand(*offspring.shape)\n        offspring = np.where(mutation_matrix, random_values, offspring)\n        return np.clip(offspring, lb, ub)\n\n    def apply_quantum_entanglement(self, offspring):\n        for i in range(len(offspring)):\n            if np.random.rand() < self.entanglement_factor:\n                partner_idx = np.random.randint(len(offspring))\n                qubit_superposition = 0.5 * (offspring[i] + offspring[partner_idx])\n                offspring[i] = qubit_superposition + np.random.normal(0, self.learning_rate, self.dim)\n        return offspring\n\n    def quantum_tunneling(self, offspring, lb, ub):\n        if np.random.rand() < 0.3:\n            random_individuals = lb + (ub - lb) * np.random.rand(3, self.dim)\n            indices = np.random.choice(len(offspring), size=3, replace=False)\n            offspring[indices] = random_individuals\n        return np.clip(offspring, lb, ub)\n\n    def adapt_population_size(self):\n        diversity = np.std(self.population, axis=0).mean()\n        if diversity < self.diversity_threshold:\n            self.exploration_phase = True\n        else:\n            self.exploration_phase = False\n\n        progress_rate = min(0.5, 2 * (1 - self.best_fitness / (self.best_fitness + 1e-9)))\n        if self.exploration_phase:\n            new_pop_size = int(self.initial_pop_size * (1 + progress_rate))\n        else:\n            new_pop_size = int(self.initial_pop_size * (1 - progress_rate / 2))\n        self.population = self.population[:new_pop_size]\n\n    def update_mutation_rate_and_learning_rate(self):\n        self.mutation_rate = max(0.01, 0.1 * (1 - self.generational_progress / self.budget))\n        self.learning_rate = max(0.01, 0.1 * (1 - self.best_fitness / (self.best_fitness + 1e-9)))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            fitness = self.evaluate_population(func)\n            evaluations += len(fitness)\n\n            if evaluations >= self.budget:\n                break\n\n            parents = self.select_parents(fitness)\n            offspring = self.crossover(parents)\n            offspring = self.mutate(offspring, lb, ub)\n            offspring = self.apply_quantum_entanglement(offspring)\n            offspring = self.quantum_tunneling(offspring, lb, ub)\n\n            self.population = offspring\n            self.adapt_population_size()\n            self.update_mutation_rate_and_learning_rate()\n            self.generational_progress = evaluations\n\n        return self.best_solution, self.best_fitness", "name": "EnhancedAdaptiveQuantumGeneticAlgorithm", "description": "Enhanced Adaptive Quantum Genetic Algorithm with multi-phase adaptation and diversity preservation for improved global optimization of photonic structures.", "configspace": "", "generation": 39, "fitness": 0.2768515835344416, "feedback": "The algorithm EnhancedAdaptiveQuantumGeneticAlgorithm got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28 with standard deviation 0.00.", "error": "", "parent_id": "7ee4669f-d741-4d78-92b5-bad82a6ebcd7", "metadata": {"aucs": [0.2768014212720191, 0.2768316121734512, 0.2769217171578545]}, "mutation_prompt": null}
{"id": "e5e5ae36-7c26-4b7f-98d7-137420c9c8be", "solution": "import numpy as np\n\nclass EnhancedAdaptiveParticleSwarmOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = max(10, min(50, budget // 10))\n        self.particles = None\n        self.velocities = None\n        self.best_global_position = None\n        self.best_global_fitness = float('inf')\n        self.best_personal_positions = None\n        self.best_personal_fitness = np.full(self.swarm_size, float('inf'))\n        self.inertia_weight = 0.9\n        self.cognitive_coeff = 2\n        self.social_coeff = 2\n        self.inertia_damping = 0.99\n\n    def initialize_swarm(self, lb, ub):\n        self.particles = lb + (ub - lb) * np.random.rand(self.swarm_size, self.dim)\n        self.velocities = np.zeros((self.swarm_size, self.dim))\n        self.best_personal_positions = self.particles.copy()\n\n    def evaluate_particles(self, func):\n        fitness = np.array([func(p) for p in self.particles])\n        for i in range(self.swarm_size):\n            if fitness[i] < self.best_personal_fitness[i]:\n                self.best_personal_fitness[i] = fitness[i]\n                self.best_personal_positions[i] = self.particles[i].copy()\n        min_fitness_index = np.argmin(fitness)\n        if fitness[min_fitness_index] < self.best_global_fitness:\n            self.best_global_fitness = fitness[min_fitness_index]\n            self.best_global_position = self.particles[min_fitness_index].copy()\n        return fitness\n\n    def update_velocities_and_positions(self, lb, ub):\n        r1, r2 = np.random.rand(self.swarm_size, self.dim), np.random.rand(self.swarm_size, self.dim)\n        cognitive_component = self.cognitive_coeff * r1 * (self.best_personal_positions - self.particles)\n        social_component = self.social_coeff * r2 * (self.best_global_position - self.particles)\n        self.velocities = self.inertia_weight * self.velocities + cognitive_component + social_component\n        self.particles += self.velocities\n        np.clip(self.particles, lb, ub, out=self.particles)\n        self.inertia_weight *= self.inertia_damping\n\n    def adapt_parameters(self, evaluations):\n        self.cognitive_coeff = 2 - 1.5 * (evaluations / self.budget)\n        self.social_coeff = 1.5 + 1.5 * (evaluations / self.budget)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_swarm(lb, ub)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            fitness = self.evaluate_particles(func)\n            evaluations += len(fitness)\n\n            if evaluations >= self.budget:\n                break\n\n            self.update_velocities_and_positions(lb, ub)\n            self.adapt_parameters(evaluations)\n\n        return self.best_global_position, self.best_global_fitness", "name": "EnhancedAdaptiveParticleSwarmOptimization", "description": "Enhanced Adaptive Particle Swarm Optimization with Dynamic Inertia Weight and Learning Factors for Robust Exploration and Exploitation in Photonic Structure Optimization.", "configspace": "", "generation": 40, "fitness": 0.2737140713681983, "feedback": "The algorithm EnhancedAdaptiveParticleSwarmOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.27 with standard deviation 0.00.", "error": "", "parent_id": "7ee4669f-d741-4d78-92b5-bad82a6ebcd7", "metadata": {"aucs": [0.2731147078287184, 0.27257016843463777, 0.2754573378412387]}, "mutation_prompt": null}
{"id": "6550abd3-a68e-4458-966e-e0d0ec15fd8a", "solution": "import numpy as np\n\nclass IntegrativeQuantumDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = max(10, min(50, budget // 10))\n        self.population = None\n        self.best_solution = None\n        self.best_fitness = float('inf')\n        self.f = 0.5  # Differential weight\n        self.cr = 0.9  # Crossover probability\n        self.quantum_weight = 0.6  # Quantum variability factor\n        self.generational_progress = 0\n\n    def initialize_population(self, lb, ub):\n        self.population = lb + (ub - lb) * np.random.rand(self.population_size, self.dim)\n\n    def evaluate_population(self, func):\n        fitness = np.array([func(ind) for ind in self.population])\n        best_index = np.argmin(fitness)\n        if fitness[best_index] < self.best_fitness:\n            self.best_fitness = fitness[best_index]\n            self.best_solution = self.population[best_index]\n        return fitness\n\n    def select_mutant(self, target_idx, lb, ub):\n        idxs = [idx for idx in range(self.population_size) if idx != target_idx]\n        a, b, c = np.random.choice(idxs, 3, replace=False)\n        mutant = np.clip(self.population[a] + self.f * (self.population[b] - self.population[c]), lb, ub)\n        return mutant\n\n    def crossover(self, target, mutant):\n        crossover_mask = np.random.rand(self.dim) < self.cr\n        offspring = np.where(crossover_mask, mutant, target)\n        return offspring\n\n    def apply_quantum_variation(self, individual, lb, ub):\n        if np.random.rand() < self.quantum_weight:\n            quantum_shift = np.random.normal(0, 0.1, self.dim)\n            individual += quantum_shift\n            individual = np.clip(individual, lb, ub)\n        return individual\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            fitness = self.evaluate_population(func)\n            evaluations += len(fitness)\n\n            if evaluations >= self.budget:\n                break\n\n            new_population = np.copy(self.population)\n            for i in range(self.population_size):\n                mutant = self.select_mutant(i, lb, ub)\n                offspring = self.crossover(self.population[i], mutant)\n                offspring = self.apply_quantum_variation(offspring, lb, ub)\n                if func(offspring) < fitness[i]:\n                    new_population[i] = offspring\n\n            self.population = new_population\n            self.generational_progress += 1\n\n        return self.best_solution, self.best_fitness", "name": "IntegrativeQuantumDifferentialEvolution", "description": "Integrative Quantum Differential Evolution (IQDE) combines quantum-inspired variability with differential evolution to optimize photonic structures efficiently by balancing exploration and exploitation.", "configspace": "", "generation": 41, "fitness": 0.26990667365964416, "feedback": "The algorithm IntegrativeQuantumDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.27 with standard deviation 0.00.", "error": "", "parent_id": "7ee4669f-d741-4d78-92b5-bad82a6ebcd7", "metadata": {"aucs": [0.27064789905217634, 0.26964879161667477, 0.26942333031008137]}, "mutation_prompt": null}
{"id": "306aeaa3-da07-4e29-91cf-397686087740", "solution": "import numpy as np\n\nclass EnhancedQuantumGeneticAlgorithm:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_pop_size = max(10, min(50, budget // 10))\n        self.population = None\n        self.best_solution = None\n        self.best_fitness = float('inf')\n        self.mutation_rate = 0.1\n        self.entanglement_factor = 0.5\n        self.generational_progress = 0\n        self.learning_rate = 0.1\n        self.exploration_phase = True\n        self.crossover_rate = 0.8\n\n    def initialize_population(self, lb, ub):\n        self.population = lb + (ub - lb) * np.random.rand(self.initial_pop_size, self.dim)\n\n    def evaluate_population(self, func):\n        fitness = np.array([func(ind) for ind in self.population])\n        best_index = np.argmin(fitness)\n        if fitness[best_index] < self.best_fitness:\n            self.best_fitness = fitness[best_index]\n            self.best_solution = self.population[best_index]\n        return fitness\n\n    def select_parents(self, fitness):\n        total_fitness = np.sum(1 / (fitness + 1e-9))\n        selection_prob = (1 / (fitness + 1e-9)) / total_fitness\n        indices = np.random.choice(len(fitness), size=len(fitness), p=selection_prob)\n        return self.population[indices]\n\n    def crossover(self, parents):\n        offspring = np.empty_like(parents)\n        for i in range(0, len(parents), 2):\n            if i + 1 >= len(parents):\n                break\n            if np.random.rand() < self.crossover_rate:\n                alpha = np.random.rand(self.dim)\n                offspring[i] = alpha * parents[i] + (1 - alpha) * parents[i + 1]\n                offspring[i + 1] = alpha * parents[i + 1] + (1 - alpha) * parents[i]\n            else:\n                offspring[i], offspring[i + 1] = parents[i], parents[i + 1]\n        return offspring\n\n    def mutate(self, offspring, lb, ub):\n        entropy = np.random.rand(*offspring.shape)\n        mutation_matrix = entropy < self.mutation_rate\n        random_values = lb + (ub - lb) * np.random.rand(*offspring.shape)\n        offspring = np.where(mutation_matrix, random_values, offspring)\n        return np.clip(offspring, lb, ub)\n\n    def apply_quantum_entanglement(self, offspring):\n        for i in range(len(offspring)):\n            if np.random.rand() < self.entanglement_factor:\n                partner_idx = np.random.randint(len(offspring))\n                qubit_superposition = 0.5 * (offspring[i] + offspring[partner_idx])\n                offspring[i] = qubit_superposition + np.random.normal(0, self.learning_rate, self.dim)\n        return offspring\n\n    def quantum_tunneling(self, offspring, lb, ub):\n        if np.random.rand() < 0.3:\n            random_individuals = lb + (ub - lb) * np.random.rand(3, self.dim)\n            indices = np.random.choice(len(offspring), size=3, replace=False)\n            offspring[indices] = random_individuals\n        return np.clip(offspring, lb, ub)\n\n    def adapt_population_size(self):\n        progress_rate = min(0.5, 2 * (1 - self.best_fitness / (self.best_fitness + 1e-9)))\n        if self.generational_progress > self.budget // 2:\n            self.exploration_phase = False\n        if self.exploration_phase:\n            new_pop_size = int(self.initial_pop_size * (1 + progress_rate))\n        else:\n            new_pop_size = int(self.initial_pop_size * (1 - progress_rate / 2))\n        self.population = self.population[:new_pop_size]\n\n    def update_mutation_rate_and_learning_rate(self):\n        self.mutation_rate = max(0.01, 0.1 * (1 - self.generational_progress / self.budget))\n        self.learning_rate = max(0.01, 0.1 * (1 - self.best_fitness / (self.best_fitness + 1e-9)))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            fitness = self.evaluate_population(func)\n            evaluations += len(fitness)\n\n            if evaluations >= self.budget:\n                break\n\n            parents = self.select_parents(fitness)\n            offspring = self.crossover(parents)\n            offspring = self.mutate(offspring, lb, ub)\n            offspring = self.apply_quantum_entanglement(offspring)\n            offspring = self.quantum_tunneling(offspring, lb, ub)\n\n            self.population = offspring\n            self.adapt_population_size()\n            self.update_mutation_rate_and_learning_rate()\n            self.generational_progress = evaluations\n\n        return self.best_solution, self.best_fitness", "name": "EnhancedQuantumGeneticAlgorithm", "description": "Enhanced Quantum Genetic Algorithm with hybrid dynamic entropy-controlled mutation and adaptive crossover to optimize photonic structures efficiently under varying conditions.", "configspace": "", "generation": 42, "fitness": 0.2766242635770974, "feedback": "The algorithm EnhancedQuantumGeneticAlgorithm got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28 with standard deviation 0.00.", "error": "", "parent_id": "7ee4669f-d741-4d78-92b5-bad82a6ebcd7", "metadata": {"aucs": [0.27669489018350346, 0.276743122116787, 0.27643477843100184]}, "mutation_prompt": null}
{"id": "b1229320-61ad-46bf-94e7-273f42bface4", "solution": "import numpy as np\n\nclass EnhancedQuantumGeneticAlgorithm:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_pop_size = max(10, min(50, budget // 10))\n        self.population = None\n        self.best_solution = None\n        self.best_fitness = float('inf')\n        self.mutation_rate = 0.1\n        self.entanglement_factor = 0.5\n        self.generational_progress = 0\n        self.learning_rate = 0.1\n        self.exploration_phase = True\n\n    def initialize_population(self, lb, ub):\n        self.population = lb + (ub - lb) * np.random.rand(self.initial_pop_size, self.dim)\n\n    def evaluate_population(self, func):\n        fitness = np.array([func(ind) for ind in self.population])\n        best_index = np.argmin(fitness)\n        if fitness[best_index] < self.best_fitness:\n            self.best_fitness = fitness[best_index]\n            self.best_solution = self.population[best_index]\n        return fitness\n\n    def select_parents(self, fitness):\n        scaled_fitness = np.max(fitness) - fitness + 1e-9\n        selection_prob = scaled_fitness / np.sum(scaled_fitness)\n        indices = np.random.choice(len(fitness), size=len(fitness), p=selection_prob)\n        return self.population[indices]\n\n    def crossover(self, parents):\n        offspring = np.empty_like(parents)\n        entropy = np.random.rand(self.dim)\n        for i in range(0, len(parents), 2):\n            if i + 1 >= len(parents):\n                break\n            alpha = entropy\n            offspring[i] = alpha * parents[i] + (1 - alpha) * parents[i + 1]\n            offspring[i + 1] = alpha * parents[i + 1] + (1 - alpha) * parents[i]\n        return offspring\n\n    def mutate(self, offspring, lb, ub):\n        entropy = np.random.rand(*offspring.shape)\n        mutation_matrix = entropy < self.mutation_rate\n        gaussian_noise = np.random.normal(0, 0.1, offspring.shape)\n        offspring = np.where(mutation_matrix, offspring + gaussian_noise, offspring)\n        return np.clip(offspring, lb, ub)\n\n    def apply_quantum_entanglement(self, offspring):\n        for i in range(len(offspring)):\n            if np.random.rand() < self.entanglement_factor:\n                partner_idx = np.random.randint(len(offspring))\n                qubit_superposition = 0.5 * (offspring[i] + offspring[partner_idx])\n                offspring[i] = qubit_superposition + np.random.normal(0, self.learning_rate, self.dim)\n        return offspring\n\n    def quantum_tunneling(self, offspring, lb, ub):\n        if np.random.rand() < 0.3:\n            random_individuals = lb + (ub - lb) * np.random.rand(3, self.dim)\n            indices = np.random.choice(len(offspring), size=3, replace=False)\n            offspring[indices] = random_individuals\n        return np.clip(offspring, lb, ub)\n\n    def adapt_population_size(self):\n        progress_rate = min(0.5, 2 * (1 - self.best_fitness / (self.best_fitness + 1e-9)))\n        if self.generational_progress > self.budget // 2:\n            self.exploration_phase = False\n        if self.exploration_phase:\n            new_pop_size = int(self.initial_pop_size * (1 + progress_rate))\n        else:\n            new_pop_size = int(self.initial_pop_size * (1 - progress_rate / 2))\n        self.population = self.population[:new_pop_size]\n\n    def update_mutation_rate_and_learning_rate(self):\n        self.mutation_rate = max(0.01, 0.1 * (1 - self.generational_progress / self.budget))\n        self.learning_rate = max(0.01, 0.1 * (1 - self.best_fitness / (self.best_fitness + 1e-9)))\n\n    def maintain_diversity(self):\n        if np.var(self.population) < 0.05:\n            new_individuals = np.random.uniform(func.bounds.lb, func.bounds.ub, (self.initial_pop_size, self.dim))\n            self.population = np.vstack((self.population, new_individuals))\n            self.population = self.population[:self.initial_pop_size]\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            fitness = self.evaluate_population(func)\n            evaluations += len(fitness)\n\n            if evaluations >= self.budget:\n                break\n\n            parents = self.select_parents(fitness)\n            offspring = self.crossover(parents)\n            offspring = self.mutate(offspring, lb, ub)\n            offspring = self.apply_quantum_entanglement(offspring)\n            offspring = self.quantum_tunneling(offspring, lb, ub)\n\n            self.population = offspring\n            self.adapt_population_size()\n            self.update_mutation_rate_and_learning_rate()\n            self.maintain_diversity()\n            self.generational_progress = evaluations\n\n        return self.best_solution, self.best_fitness", "name": "EnhancedQuantumGeneticAlgorithm", "description": "Enhanced Quantum Genetic Algorithm utilizing adaptive population dynamics and diversity maintenance techniques for robust global optimization of photonic structures.", "configspace": "", "generation": 43, "fitness": 0.27422039031976736, "feedback": "The algorithm EnhancedQuantumGeneticAlgorithm got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.27 with standard deviation 0.00.", "error": "", "parent_id": "7ee4669f-d741-4d78-92b5-bad82a6ebcd7", "metadata": {"aucs": [0.2731192866884685, 0.2767276883077763, 0.27281419596305734]}, "mutation_prompt": null}
{"id": "1f779814-a3d3-4af9-9b06-bb3ce1422ae6", "solution": "import numpy as np\n\nclass QuantumInspiredDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = max(10, min(50, budget // 10))\n        self.population = None\n        self.best_solution = None\n        self.best_fitness = float('inf')\n        self.mutation_factor = 0.8\n        self.crossover_rate = 0.7\n        self.generational_progress = 0\n        self.diversity_threshold = 0.1\n\n    def initialize_population(self, lb, ub):\n        self.population = lb + (ub - lb) * np.random.rand(self.population_size, self.dim)\n\n    def evaluate_population(self, func):\n        fitness = np.array([func(ind) for ind in self.population])\n        best_index = np.argmin(fitness)\n        if fitness[best_index] < self.best_fitness:\n            self.best_fitness = fitness[best_index]\n            self.best_solution = self.population[best_index]\n        return fitness\n\n    def mutate(self, indices, lb, ub):\n        idx_a, idx_b, idx_c = indices\n        a, b, c = self.population[idx_a], self.population[idx_b], self.population[idx_c]\n        mutant_vector = a + self.mutation_factor * (b - c)\n        return np.clip(mutant_vector, lb, ub)\n\n    def crossover(self, target, mutant):\n        crossover_mask = np.random.rand(self.dim) < self.crossover_rate\n        offspring = np.where(crossover_mask, mutant, target)\n        return offspring\n\n    def apply_gaussian_perturbation(self, individual, lb, ub):\n        if np.random.rand() < 0.3:\n            perturbation = np.random.normal(0, 0.1, size=self.dim)\n            individual += perturbation\n        return np.clip(individual, lb, ub)\n\n    def adapt_population_size(self, lb, ub):\n        diversity = np.mean(np.std(self.population, axis=0))\n        if diversity < self.diversity_threshold:\n            new_individuals = lb + (ub - lb) * np.random.rand(5, self.dim)\n            self.population = np.vstack((self.population, new_individuals))\n            self.population_size = len(self.population)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            fitness = self.evaluate_population(func)\n            evaluations += len(fitness)\n\n            if evaluations >= self.budget:\n                break\n\n            new_population = []\n            for idx in range(self.population_size):\n                indices = np.random.choice(self.population_size, 3, replace=False)\n                mutant_vector = self.mutate(indices, lb, ub)\n                target_vector = self.population[idx]\n                offspring = self.crossover(target_vector, mutant_vector)\n                offspring = self.apply_gaussian_perturbation(offspring, lb, ub)\n                new_population.append(offspring)\n\n            self.population = np.array(new_population)\n            self.adapt_population_size(lb, ub)\n            self.generational_progress = evaluations\n\n        return self.best_solution, self.best_fitness", "name": "QuantumInspiredDifferentialEvolution", "description": "Quantum-Inspired Differential Evolution incorporating adaptive population dynamics and Gaussian perturbation to enhance diversity and convergence in optimizing photonic structures.", "configspace": "", "generation": 44, "fitness": 0.24511003203370393, "feedback": "The algorithm QuantumInspiredDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.25 with standard deviation 0.00.", "error": "", "parent_id": "7ee4669f-d741-4d78-92b5-bad82a6ebcd7", "metadata": {"aucs": [0.2453996484969201, 0.2445042884045059, 0.2454261591996858]}, "mutation_prompt": null}
{"id": "7a9d939e-ae53-4f7a-b2ae-a440589d34ce", "solution": "import numpy as np\n\nclass EnhancedQuantumGeneticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = max(10, min(50, budget // 10))\n        self.population = None\n        self.best_solution = None\n        self.best_fitness = float('inf')\n        self.mutation_rate = 0.1\n        self.entanglement_factor = 0.5\n        self.generational_progress = 0\n        self.learning_rate = 0.1\n        self.exploration_phase = True\n\n    def initialize_population(self, lb, ub):\n        self.population = lb + (ub - lb) * np.random.rand(self.population_size, self.dim)\n\n    def evaluate_population(self, func):\n        fitness = np.array([func(ind) for ind in self.population])\n        best_index = np.argmin(fitness)\n        if fitness[best_index] < self.best_fitness:\n            self.best_fitness = fitness[best_index]\n            self.best_solution = self.population[best_index]\n        return fitness\n\n    def select_parents(self, fitness):\n        scaled_fitness = np.exp(-fitness)\n        total_fitness = np.sum(scaled_fitness)\n        selection_prob = scaled_fitness / total_fitness\n        indices = np.random.choice(len(fitness), size=len(fitness), p=selection_prob)\n        return self.population[indices]\n\n    def crossover(self, parents):\n        offspring = np.empty_like(parents)\n        entropy = np.random.rand(self.dim)\n        for i in range(0, len(parents), 2):\n            if i + 1 >= len(parents):\n                break\n            alpha = entropy\n            offspring[i] = alpha * parents[i] + (1 - alpha) * parents[i + 1]\n            offspring[i + 1] = alpha * parents[i + 1] + (1 - alpha) * parents[i]\n        return offspring\n\n    def mutate(self, offspring, lb, ub):\n        entropy = np.random.rand(*offspring.shape)\n        mutation_matrix = entropy < self.mutation_rate\n        random_values = lb + (ub - lb) * np.random.rand(*offspring.shape)\n        offspring = np.where(mutation_matrix, random_values, offspring)\n        return np.clip(offspring, lb, ub)\n\n    def apply_quantum_entanglement(self, offspring):\n        for i in range(len(offspring)):\n            if np.random.rand() < self.entanglement_factor:\n                partner_idx = np.random.randint(len(offspring))\n                qubit_superposition = 0.5 * (offspring[i] + offspring[partner_idx])\n                perturbation = np.random.normal(0, self.learning_rate, self.dim)\n                offspring[i] = np.clip(qubit_superposition + perturbation, func.bounds.lb, func.bounds.ub)\n        return offspring\n\n    def local_search(self, individual, lb, ub, func):\n        step_size = 0.05 * (ub - lb)\n        for _ in range(5):\n            candidate = individual + np.random.uniform(-step_size, step_size, self.dim)\n            candidate = np.clip(candidate, lb, ub)\n            if func(candidate) < func(individual):\n                individual = candidate\n        return individual\n\n    def adapt_population_size(self):\n        progress_rate = min(0.5, 2 * (1 - self.best_fitness / (self.best_fitness + 1e-9)))\n        if self.generational_progress > self.budget // 2:\n            self.exploration_phase = False\n        if self.exploration_phase:\n            new_pop_size = int(self.population_size * (1 + progress_rate))\n        else:\n            new_pop_size = int(self.population_size * (1 - progress_rate / 2))\n        self.population = self.population[:new_pop_size]\n\n    def update_mutation_rate_and_learning_rate(self):\n        self.mutation_rate = max(0.01, 0.1 * (1 - self.generational_progress / self.budget))\n        self.learning_rate = max(0.01, 0.1 * (1 - self.best_fitness / (self.best_fitness + 1e-9)))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            fitness = self.evaluate_population(func)\n            evaluations += len(fitness)\n\n            if evaluations >= self.budget:\n                break\n\n            parents = self.select_parents(fitness)\n            offspring = self.crossover(parents)\n            offspring = self.mutate(offspring, lb, ub)\n            offspring = self.apply_quantum_entanglement(offspring)\n\n            # Apply local search to the best offspring\n            best_offspring_idx = np.argmin([func(ind) for ind in offspring])\n            offspring[best_offspring_idx] = self.local_search(offspring[best_offspring_idx], lb, ub, func)\n\n            self.population = offspring\n            self.adapt_population_size()\n            self.update_mutation_rate_and_learning_rate()\n            self.generational_progress = evaluations\n\n        return self.best_solution, self.best_fitness", "name": "EnhancedQuantumGeneticOptimizer", "description": "This algorithm leverages adaptive quantum-inspired mechanisms with dynamic diversity preservation and local search enhancements to optimize photonic structures efficiently.", "configspace": "", "generation": 45, "fitness": -Infinity, "feedback": "An exception occurred: NameError(\"name 'func' is not defined\").", "error": "NameError(\"name 'func' is not defined\")", "parent_id": "7ee4669f-d741-4d78-92b5-bad82a6ebcd7", "metadata": {}, "mutation_prompt": null}
{"id": "8f855ef3-bb2e-48d4-969e-67f6a15bea7d", "solution": "import numpy as np\n\nclass QuantumInspiredParticleSwarmOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = max(10, min(50, budget // 10))\n        self.particles = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_fitness = np.full(self.num_particles, float('inf'))\n        self.global_best_position = None\n        self.global_best_fitness = float('inf')\n        self.cognitive_const = 2.0\n        self.social_const = 2.0\n        self.inertia_weight = 0.7\n        self.constriction_factor = 0.5\n        self.dynamic_neighborhood_size = 3\n        self.interference_prob = 0.1\n\n    def initialize_particles(self, lb, ub):\n        self.particles = lb + (ub - lb) * np.random.rand(self.num_particles, self.dim)\n        self.velocities = np.random.randn(self.num_particles, self.dim) * 0.1\n        self.personal_best_positions = np.copy(self.particles)\n\n    def evaluate_particles(self, func):\n        fitness = np.array([func(p) for p in self.particles])\n        for i, f in enumerate(fitness):\n            if f < self.personal_best_fitness[i]:\n                self.personal_best_fitness[i] = f\n                self.personal_best_positions[i] = self.particles[i]\n            if f < self.global_best_fitness:\n                self.global_best_fitness = f\n                self.global_best_position = self.particles[i]\n        return fitness\n\n    def update_velocities_and_positions(self, lb, ub):\n        for i in range(self.num_particles):\n            local_best = self.get_local_best(i)\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            cognitive_term = self.cognitive_const * r1 * (self.personal_best_positions[i] - self.particles[i])\n            social_term = self.social_const * r2 * (local_best - self.particles[i])\n            self.velocities[i] = (self.inertia_weight * self.velocities[i] +\n                                  cognitive_term + social_term)\n            self.particles[i] += self.constriction_factor * self.velocities[i]\n        self.particles = np.clip(self.particles, lb, ub)\n\n    def get_local_best(self, index):\n        neighborhood_indices = np.random.choice(self.num_particles, self.dynamic_neighborhood_size, replace=False)\n        local_best_index = min(neighborhood_indices, key=lambda i: self.personal_best_fitness[i])\n        return self.personal_best_positions[local_best_index]\n\n    def apply_quantum_interference(self, lb, ub):\n        for i in range(self.num_particles):\n            if np.random.rand() < self.interference_prob:\n                interference_vector = lb + (ub - lb) * np.random.rand(self.dim)\n                self.particles[i] = np.mean([self.particles[i], interference_vector], axis=0)\n                self.particles[i] = np.clip(self.particles[i], lb, ub)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_particles(lb, ub)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            self.evaluate_particles(func)\n            evaluations += self.num_particles\n\n            if evaluations >= self.budget:\n                break\n\n            self.update_velocities_and_positions(lb, ub)\n            self.apply_quantum_interference(lb, ub)\n\n        return self.global_best_position, self.global_best_fitness", "name": "QuantumInspiredParticleSwarmOptimization", "description": "Quantum-Inspired Particle Swarm Optimization with Adaptive Dynamic Neighborhoods and Multi-agent Interference for robust exploration and exploitation in optimizing photonic structures.", "configspace": "", "generation": 46, "fitness": 0.27711371732658513, "feedback": "The algorithm QuantumInspiredParticleSwarmOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28 with standard deviation 0.00.", "error": "", "parent_id": "7ee4669f-d741-4d78-92b5-bad82a6ebcd7", "metadata": {"aucs": [0.27698974883147054, 0.2771775393278715, 0.2771738638204134]}, "mutation_prompt": null}
{"id": "4d50d5e0-fd4c-4dad-89a8-bde2e4b2ca9a", "solution": "import numpy as np\n\nclass MultiPhaseEvolutionaryAlgorithm:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = max(20, min(100, budget // 5))\n        self.population = None\n        self.fitness = None\n        self.best_solution = None\n        self.best_fitness = float('inf')\n        self.mutation_rate = 0.1\n        self.dynamic_niche_radius = 0.1\n\n    def initialize_population(self, lb, ub):\n        self.population = lb + (ub - lb) * np.random.rand(self.population_size, self.dim)\n\n    def evaluate_population(self, func):\n        self.fitness = np.array([func(ind) for ind in self.population])\n        best_idx = np.argmin(self.fitness)\n        if self.fitness[best_idx] < self.best_fitness:\n            self.best_fitness = self.fitness[best_idx]\n            self.best_solution = self.population[best_idx]\n\n    def select_parents(self):\n        probabilities = 1 / (self.fitness + 1e-9)\n        probabilities /= probabilities.sum()\n        parents_indices = np.random.choice(self.population_size, size=self.population_size, p=probabilities)\n        return self.population[parents_indices]\n\n    def crossover(self, parents):\n        np.random.shuffle(parents)\n        for i in range(0, self.population_size, 2):\n            if i+1 < self.population_size:\n                crossover_point = np.random.randint(1, self.dim)\n                self.population[i, crossover_point:], self.population[i+1, crossover_point:] = \\\n                self.population[i+1, crossover_point:], self.population[i, crossover_point:]\n\n    def mutate(self, lb, ub):\n        mutation_mask = np.random.rand(self.population_size, self.dim) < self.mutation_rate\n        mutation_values = lb + (ub - lb) * np.random.rand(self.population_size, self.dim)\n        self.population = np.where(mutation_mask, mutation_values, self.population)\n        self.population = np.clip(self.population, lb, ub)\n\n    def adaptive_mutation(self, generation):\n        self.mutation_rate = 0.1 * (1 - (generation / (self.budget / self.population_size))) ** 2\n\n    def niche_preservation(self):\n        distances = np.linalg.norm(self.population[:, np.newaxis] - self.population, axis=2)\n        np.fill_diagonal(distances, np.inf)\n        for i in range(self.population_size):\n            neighbors = np.where(distances[i] < self.dynamic_niche_radius)[0]\n            if len(neighbors) > 1:\n                worst_neighbor = neighbors[np.argmax(self.fitness[neighbors])]\n                self.population[worst_neighbor] = self.best_solution\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        evaluations = 0\n        generation = 0\n\n        while evaluations < self.budget:\n            self.evaluate_population(func)\n            evaluations += self.population_size\n\n            if evaluations >= self.budget:\n                break\n\n            parents = self.select_parents()\n            self.crossover(parents)\n            self.adaptive_mutation(generation)\n            self.mutate(lb, ub)\n            self.niche_preservation()\n            generation += 1\n\n        return self.best_solution, self.best_fitness", "name": "MultiPhaseEvolutionaryAlgorithm", "description": "Multi-Phase Evolutionary Algorithm with Dynamic Niching and Adaptive Mutation for Enhanced Global Search of Photonic Structures.", "configspace": "", "generation": 47, "fitness": 0.2538789240299002, "feedback": "The algorithm MultiPhaseEvolutionaryAlgorithm got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.25 with standard deviation 0.00.", "error": "", "parent_id": "8f855ef3-bb2e-48d4-969e-67f6a15bea7d", "metadata": {"aucs": [0.2533868414575089, 0.2530300671582404, 0.2552198634739513]}, "mutation_prompt": null}
{"id": "624c80cb-39d3-4955-a368-2a7bd5b88716", "solution": "import numpy as np\n\nclass EnhancedQuantumInspiredPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = max(10, min(50, budget // 10))\n        self.particles = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_fitness = np.full(self.num_particles, float('inf'))\n        self.global_best_position = None\n        self.global_best_fitness = float('inf')\n        self.cognitive_const = 2.0\n        self.social_const = 2.0\n        self.inertia_weight = 0.7\n        self.constriction_factor = 0.5\n        self.dynamic_neighborhood_size = 3\n        self.interference_prob = 0.1\n        self.dynamic_exploration_exploitation_ratio = 0.5\n\n    def initialize_particles(self, lb, ub):\n        self.particles = lb + (ub - lb) * np.random.rand(self.num_particles, self.dim)\n        self.velocities = np.random.randn(self.num_particles, self.dim) * 0.1\n        self.personal_best_positions = np.copy(self.particles)\n\n    def evaluate_particles(self, func):\n        fitness = np.array([func(p) for p in self.particles])\n        for i, f in enumerate(fitness):\n            if f < self.personal_best_fitness[i]:\n                self.personal_best_fitness[i] = f\n                self.personal_best_positions[i] = self.particles[i]\n            if f < self.global_best_fitness:\n                self.global_best_fitness = f\n                self.global_best_position = self.particles[i]\n        return fitness\n\n    def update_velocities_and_positions(self, lb, ub):\n        for i in range(self.num_particles):\n            local_best = self.get_local_best(i)\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            cognitive_term = self.cognitive_const * r1 * (self.personal_best_positions[i] - self.particles[i])\n            social_term = self.social_const * r2 * (local_best - self.particles[i])\n            self.velocities[i] = (self.inertia_weight * self.velocities[i] +\n                                  cognitive_term + social_term)\n            self.particles[i] += self.constriction_factor * self.velocities[i]\n        self.particles = np.clip(self.particles, lb, ub)\n\n    def get_local_best(self, index):\n        adaptive_neighborhood_size = max(2, int(self.dynamic_neighborhood_size * (1 + 0.5 * np.sin(index / self.num_particles * np.pi))))\n        neighborhood_indices = np.random.choice(self.num_particles, adaptive_neighborhood_size, replace=False)\n        local_best_index = min(neighborhood_indices, key=lambda i: self.personal_best_fitness[i])\n        return self.personal_best_positions[local_best_index]\n\n    def apply_quantum_interference(self, lb, ub):\n        for i in range(self.num_particles):\n            if np.random.rand() < self.interference_prob:\n                interference_vector = lb + (ub - lb) * np.random.rand(self.dim)\n                self.particles[i] = np.mean([self.particles[i], interference_vector], axis=0)\n                self.particles[i] = np.clip(self.particles[i], lb, ub)\n\n    def apply_quantum_tunneling(self, lb, ub):\n        for i in range(self.num_particles):\n            tunneling_prob = 1 / (1 + np.exp(-0.1 * (self.global_best_fitness - self.personal_best_fitness[i])))\n            if np.random.rand() < tunneling_prob:\n                self.particles[i] = self.global_best_position + np.random.randn(self.dim) * 0.1\n                self.particles[i] = np.clip(self.particles[i], lb, ub)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_particles(lb, ub)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            self.evaluate_particles(func)\n            evaluations += self.num_particles\n\n            if evaluations >= self.budget:\n                break\n\n            self.update_velocities_and_positions(lb, ub)\n            self.apply_quantum_interference(lb, ub)\n            self.apply_quantum_tunneling(lb, ub)\n\n        return self.global_best_position, self.global_best_fitness", "name": "EnhancedQuantumInspiredPSO", "description": "Enhanced Quantum-Inspired Particle Swarm Optimization with Adaptive Exploration-Exploitation Balance using Dynamic Neighborhood Size and Quantum Tunneling for improved global search efficiency.", "configspace": "", "generation": 48, "fitness": 0.2749720848478476, "feedback": "The algorithm EnhancedQuantumInspiredPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.27 with standard deviation 0.00.", "error": "", "parent_id": "8f855ef3-bb2e-48d4-969e-67f6a15bea7d", "metadata": {"aucs": [0.2725130735897605, 0.2774228617593376, 0.27498031919444454]}, "mutation_prompt": null}
{"id": "3df1ba7c-7c71-49b9-afac-964906409459", "solution": "import numpy as np\n\nclass HybridQuantumDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = max(10, min(50, budget // 10))\n        self.population = None\n        self.fitness = None\n        self.best_position = None\n        self.best_fitness = float('inf')\n        self.mutation_factor = 0.5\n        self.crossover_rate = 0.7\n        self.interference_prob = 0.1\n\n    def initialize_population(self, lb, ub):\n        self.population = lb + (ub - lb) * np.random.rand(self.population_size, self.dim)\n        self.fitness = np.full(self.population_size, float('inf'))\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            self.fitness[i] = func(self.population[i])\n            if self.fitness[i] < self.best_fitness:\n                self.best_fitness = self.fitness[i]\n                self.best_position = self.population[i]\n        \n    def mutate(self, target_idx):\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        mutant_vector = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        return mutant_vector\n\n    def crossover(self, target, mutant):\n        crossover_vector = np.copy(target)\n        for i in range(self.dim):\n            if np.random.rand() < self.crossover_rate:\n                crossover_vector[i] = mutant[i]\n        return crossover_vector\n\n    def apply_quantum_interference(self, individual, lb, ub):\n        if np.random.rand() < self.interference_prob:\n            interference_vector = lb + (ub - lb) * np.random.rand(self.dim)\n            return np.mean([individual, interference_vector], axis=0)\n        return individual\n\n    def local_search(self, candidate, lb, ub):\n        candidate += np.random.uniform(-0.1, 0.1, self.dim)\n        return np.clip(candidate, lb, ub)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            self.evaluate_population(func)\n            evaluations += self.population_size\n\n            if evaluations >= self.budget:\n                break\n\n            for i in range(self.population_size):\n                mutant = self.mutate(i)\n                trial = self.crossover(self.population[i], mutant)\n                trial = self.apply_quantum_interference(trial, lb, ub)\n                trial = np.clip(trial, lb, ub)\n\n                trial_fitness = func(trial)\n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_fitness\n                    if trial_fitness < self.best_fitness:\n                        self.best_fitness = trial_fitness\n                        self.best_position = trial\n\n                self.population[i] = self.local_search(self.population[i], lb, ub)\n\n        return self.best_position, self.best_fitness", "name": "HybridQuantumDifferentialEvolution", "description": "Hybrid Quantum-Inspired Differential Evolution with Adaptive Mutation and Local Search for Enhanced Exploration and Exploitation in Photonic Structure Optimization.", "configspace": "", "generation": 49, "fitness": 0.2718777204333744, "feedback": "The algorithm HybridQuantumDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.27 with standard deviation 0.00.", "error": "", "parent_id": "8f855ef3-bb2e-48d4-969e-67f6a15bea7d", "metadata": {"aucs": [0.2718508979631026, 0.27081357702484776, 0.27296868631217275]}, "mutation_prompt": null}
{"id": "46bd0ebd-b346-4fb5-a428-39711ce413f7", "solution": "import numpy as np\n\nclass QuantumGeneticOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = max(10, min(50, budget // 10))\n        self.population = None\n        self.fitness = np.full(self.population_size, float('inf'))\n        self.global_best_position = None\n        self.global_best_fitness = float('inf')\n        self.crossover_prob = 0.8\n        self.mutation_prob = 0.2\n        self.interference_prob = 0.1\n\n    def initialize_population(self, lb, ub):\n        self.population = lb + (ub - lb) * np.random.rand(self.population_size, self.dim)\n\n    def evaluate_population(self, func):\n        fitness = np.array([func(individual) for individual in self.population])\n        for i, f in enumerate(fitness):\n            if f < self.fitness[i]:\n                self.fitness[i] = f\n            if f < self.global_best_fitness:\n                self.global_best_fitness = f\n                self.global_best_position = self.population[i]\n        return fitness\n\n    def crossover(self, parent1, parent2):\n        if np.random.rand() < self.crossover_prob:\n            point = np.random.randint(1, self.dim - 1)\n            child1 = np.concatenate([parent1[:point], parent2[point:]])\n            child2 = np.concatenate([parent2[:point], parent1[point:]])\n            return child1, child2\n        return parent1, parent2\n\n    def mutate(self, individual, lb, ub):\n        if np.random.rand() < self.mutation_prob:\n            mutation_vector = np.random.randn(self.dim) * 0.1\n            individual += mutation_vector\n            individual = np.clip(individual, lb, ub)\n        return individual\n\n    def apply_quantum_interference(self, lb, ub):\n        for i in range(self.population_size):\n            if np.random.rand() < self.interference_prob:\n                interference_vector = lb + (ub - lb) * np.random.rand(self.dim)\n                self.population[i] = np.mean([self.population[i], interference_vector], axis=0)\n                self.population[i] = np.clip(self.population[i], lb, ub)\n\n    def select_parents(self):\n        idx = np.random.choice(self.population_size, size=2, replace=False, p=self.fitness / np.sum(self.fitness))\n        return self.population[idx[0]], self.population[idx[1]]\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            self.evaluate_population(func)\n            evaluations += self.population_size\n\n            if evaluations >= self.budget:\n                break\n\n            new_population = []\n            for _ in range(self.population_size // 2):\n                parent1, parent2 = self.select_parents()\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.mutate(child1, lb, ub)\n                child2 = self.mutate(child2, lb, ub)\n                new_population.extend([child1, child2])\n            self.population = np.array(new_population)\n            self.apply_quantum_interference(lb, ub)\n\n        return self.global_best_position, self.global_best_fitness", "name": "QuantumGeneticOptimization", "description": "Quantum Genetic Optimization combines quantum-inspired evolutionary operations and genetic algorithms for enhanced exploration and exploitation in photonic structure optimization.", "configspace": "", "generation": 50, "fitness": 0.26967507723429757, "feedback": "The algorithm QuantumGeneticOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.27 with standard deviation 0.00.", "error": "", "parent_id": "8f855ef3-bb2e-48d4-969e-67f6a15bea7d", "metadata": {"aucs": [0.2703681365504391, 0.26896565849412646, 0.26969143665832707]}, "mutation_prompt": null}
{"id": "a0619024-15d7-47b9-bc2f-f2845ce92776", "solution": "import numpy as np\n\nclass HybridParticleSwarmOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = max(10, min(50, budget // 10))\n        self.particles = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_fitness = np.full(self.num_particles, float('inf'))\n        self.global_best_position = None\n        self.global_best_fitness = float('inf')\n        self.cognitive_const = 1.5\n        self.social_const = 1.5\n        self.inertia_weight = 0.7\n        self.crossover_prob = 0.8\n        self.mutation_prob = 0.1\n\n    def initialize_particles(self, lb, ub):\n        self.particles = lb + (ub - lb) * np.random.rand(self.num_particles, self.dim)\n        self.velocities = np.random.randn(self.num_particles, self.dim) * 0.1\n        self.personal_best_positions = np.copy(self.particles)\n\n    def evaluate_particles(self, func):\n        fitness = np.array([func(p) for p in self.particles])\n        for i, f in enumerate(fitness):\n            if f < self.personal_best_fitness[i]:\n                self.personal_best_fitness[i] = f\n                self.personal_best_positions[i] = self.particles[i]\n            if f < self.global_best_fitness:\n                self.global_best_fitness = f\n                self.global_best_position = self.particles[i]\n        return fitness\n\n    def update_velocities_and_positions(self, lb, ub):\n        for i in range(self.num_particles):\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            cognitive_term = self.cognitive_const * r1 * (self.personal_best_positions[i] - self.particles[i])\n            social_term = self.social_const * r2 * (self.global_best_position - self.particles[i])\n            self.velocities[i] = (self.inertia_weight * self.velocities[i] +\n                                  cognitive_term + social_term)\n            self.particles[i] += self.velocities[i]\n        self.particles = np.clip(self.particles, lb, ub)\n\n    def crossover(self, lb, ub):\n        for i in range(0, self.num_particles, 2):\n            if np.random.rand() < self.crossover_prob:\n                crossover_point = np.random.randint(0, self.dim)\n                parent1, parent2 = self.particles[i], self.particles[(i+1) % self.num_particles]\n                child1 = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n                child2 = np.concatenate([parent2[:crossover_point], parent1[crossover_point:]])\n                self.particles[i], self.particles[(i+1) % self.num_particles] = child1, child2\n        self.particles = np.clip(self.particles, lb, ub)\n\n    def mutate(self, lb, ub):\n        for i in range(self.num_particles):\n            if np.random.rand() < self.mutation_prob:\n                mutation_vector = lb + (ub - lb) * np.random.rand(self.dim)\n                mutation_index = np.random.randint(0, self.dim)\n                self.particles[i][mutation_index] = mutation_vector[mutation_index]\n        self.particles = np.clip(self.particles, lb, ub)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_particles(lb, ub)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            self.evaluate_particles(func)\n            evaluations += self.num_particles\n\n            if evaluations >= self.budget:\n                break\n\n            self.update_velocities_and_positions(lb, ub)\n            self.crossover(lb, ub)\n            self.mutate(lb, ub)\n        \n        return self.global_best_position, self.global_best_fitness", "name": "HybridParticleSwarmOptimization", "description": "Hybrid Particle Swarm Optimization with Genetic Crossover and Mutation for enhanced exploration and convergence in optimizing photonic structures.", "configspace": "", "generation": 51, "fitness": 0.251654050742246, "feedback": "The algorithm HybridParticleSwarmOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.25 with standard deviation 0.00.", "error": "", "parent_id": "8f855ef3-bb2e-48d4-969e-67f6a15bea7d", "metadata": {"aucs": [0.24852836345196416, 0.25697480093235714, 0.2494589878424167]}, "mutation_prompt": null}
{"id": "47dec932-e908-4bb2-9e17-ba05b0e2d070", "solution": "import numpy as np\n\nclass HybridGeneticAlgorithm:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = max(10, min(50, budget // 10))\n        self.population = None\n        self.fitness = None\n        self.mutation_rate = 0.1\n        self.crossover_rate = 0.8\n        self.quantum_mutation_prob = 0.05\n        self.niche_radius = 0.1\n\n    def initialize_population(self, lb, ub):\n        self.population = lb + (ub - lb) * np.random.rand(self.population_size, self.dim)\n        self.fitness = np.full(self.population_size, float('inf'))\n\n    def evaluate_population(self, func):\n        fitness = np.array([func(ind) for ind in self.population])\n        self.fitness = fitness\n        return fitness\n\n    def select_parents(self):\n        idx = np.random.choice(self.population_size, size=2, replace=False, p=self.fitness / self.fitness.sum())\n        return self.population[idx[0]], self.population[idx[1]]\n\n    def crossover(self, parent1, parent2):\n        if np.random.rand() < self.crossover_rate:\n            point = np.random.randint(1, self.dim-1)\n            child1 = np.concatenate((parent1[:point], parent2[point:]))\n            child2 = np.concatenate((parent2[:point], parent1[point:]))\n            return child1, child2\n        else:\n            return parent1, parent2\n\n    def mutate(self, individual, lb, ub):\n        for i in range(self.dim):\n            if np.random.rand() < self.mutation_rate:\n                individual[i] += np.random.randn() * 0.1\n                individual[i] = np.clip(individual[i], lb[i], ub[i])\n\n    def apply_quantum_mutation(self, individual, lb, ub):\n        if np.random.rand() < self.quantum_mutation_prob:\n            q_mutation = lb + (ub - lb) * np.random.rand(self.dim)\n            individual = np.mean([individual, q_mutation], axis=0)\n            individual = np.clip(individual, lb, ub)\n        return individual\n\n    def niche_sharing(self):\n        for i in range(self.population_size):\n            for j in range(i + 1, self.population_size):\n                distance = np.linalg.norm(self.population[i] - self.population[j])\n                if distance < self.niche_radius:\n                    if self.fitness[i] > self.fitness[j]:\n                        self.fitness[i] += self.niche_radius - distance\n                    else:\n                        self.fitness[j] += self.niche_radius - distance\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            self.evaluate_population(func)\n            self.niche_sharing()\n            evaluations += self.population_size\n\n            if evaluations >= self.budget:\n                break\n\n            new_population = []\n            for _ in range(self.population_size // 2):\n                parent1, parent2 = self.select_parents()\n                child1, child2 = self.crossover(parent1, parent2)\n                self.mutate(child1, lb, ub)\n                self.mutate(child2, lb, ub)\n                child1 = self.apply_quantum_mutation(child1, lb, ub)\n                child2 = self.apply_quantum_mutation(child2, lb, ub)\n                new_population.extend([child1, child2])\n\n            self.population = np.array(new_population)\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "HybridGeneticAlgorithm", "description": "Hybrid Genetic Algorithm with Quantum-Inspired Mutation and Dynamic Niche Sharing for enhanced exploration and exploitation in optimizing photonic structures.", "configspace": "", "generation": 52, "fitness": 0.26139308894227625, "feedback": "The algorithm HybridGeneticAlgorithm got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.26 with standard deviation 0.00.", "error": "", "parent_id": "8f855ef3-bb2e-48d4-969e-67f6a15bea7d", "metadata": {"aucs": [0.25927468303954093, 0.2591860992979217, 0.2657184844893661]}, "mutation_prompt": null}
{"id": "a1f89505-309e-4b7c-9d22-8ae071692a63", "solution": "import numpy as np\n\nclass AdaptiveHarmonySearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.harmony_memory_size = max(10, min(50, budget // 10))\n        self.harmonies = None\n        self.harmony_memory_consideration_rate = 0.9\n        self.pitch_adjustment_rate = 0.3\n        self.dynamic_adjustment_rate = 0.01\n        self.best_harmony = None\n        self.best_fitness = float('inf')\n        \n    def initialize_harmonies(self, lb, ub):\n        self.harmonies = lb + (ub - lb) * np.random.rand(self.harmony_memory_size, self.dim)\n        \n    def evaluate_harmonies(self, func):\n        fitness = np.array([func(h) for h in self.harmonies])\n        for i, f in enumerate(fitness):\n            if f < self.best_fitness:\n                self.best_fitness = f\n                self.best_harmony = self.harmonies[i]\n        return fitness\n    \n    def generate_new_harmony(self, lb, ub):\n        new_harmony = np.zeros(self.dim)\n        for i in range(self.dim):\n            if np.random.rand() < self.harmony_memory_consideration_rate:\n                new_harmony[i] = self.harmonies[np.random.randint(self.harmony_memory_size)][i]\n                if np.random.rand() < self.pitch_adjustment_rate:\n                    new_harmony[i] += self.dynamic_adjustment_rate * (ub[i] - lb[i]) * np.random.uniform(-1, 1)\n            else:\n                new_harmony[i] = lb[i] + (ub[i] - lb[i]) * np.random.rand()\n        new_harmony = np.clip(new_harmony, lb, ub)\n        return new_harmony\n    \n    def update_harmony_memory(self, new_harmony, new_fitness):\n        worst_index = np.argmax([func(h) for h in self.harmonies])\n        if new_fitness < self.best_fitness:\n            self.harmonies[worst_index] = new_harmony\n            self.best_fitness = new_fitness\n            self.best_harmony = new_harmony\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_harmonies(lb, ub)\n        evaluations = self.harmony_memory_size\n        \n        while evaluations < self.budget:\n            new_harmony = self.generate_new_harmony(lb, ub)\n            new_fitness = func(new_harmony)\n            self.update_harmony_memory(new_harmony, new_fitness)\n            evaluations += 1\n        \n        return self.best_harmony, self.best_fitness", "name": "AdaptiveHarmonySearch", "description": "Adaptive Harmony Search with Memory Consideration and Dynamic Adjustment for enhanced exploration and exploitation in optimizing photonic structures.", "configspace": "", "generation": 53, "fitness": -Infinity, "feedback": "An exception occurred: NameError(\"name 'func' is not defined\").", "error": "NameError(\"name 'func' is not defined\")", "parent_id": "8f855ef3-bb2e-48d4-969e-67f6a15bea7d", "metadata": {}, "mutation_prompt": null}
{"id": "76ebe72f-ed8d-4511-b2cf-1b862aac3642", "solution": "import numpy as np\n\nclass EnhancedQuantumInspiredPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = max(10, min(50, budget // 10))\n        self.particles = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_fitness = np.full(self.num_particles, float('inf'))\n        self.global_best_position = None\n        self.global_best_fitness = float('inf')\n        self.cognitive_const = 2.0\n        self.social_const = 2.0\n        self.inertia_weight = 0.7\n        self.constriction_factor = 0.5\n        self.dynamic_neighborhood_size = 3\n        self.interference_prob = 0.1\n        self.local_search_prob = 0.2  # New parameter for local search\n\n    def initialize_particles(self, lb, ub):\n        self.particles = lb + (ub - lb) * np.random.rand(self.num_particles, self.dim)\n        self.velocities = np.random.randn(self.num_particles, self.dim) * 0.1\n        self.personal_best_positions = np.copy(self.particles)\n\n    def evaluate_particles(self, func):\n        fitness = np.array([func(p) for p in self.particles])\n        for i, f in enumerate(fitness):\n            if f < self.personal_best_fitness[i]:\n                self.personal_best_fitness[i] = f\n                self.personal_best_positions[i] = self.particles[i]\n            if f < self.global_best_fitness:\n                self.global_best_fitness = f\n                self.global_best_position = self.particles[i]\n        return fitness\n\n    def update_velocities_and_positions(self, lb, ub):\n        for i in range(self.num_particles):\n            local_best = self.get_local_best(i)\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            cognitive_term = self.cognitive_const * r1 * (self.personal_best_positions[i] - self.particles[i])\n            social_term = self.social_const * r2 * (local_best - self.particles[i])\n            self.velocities[i] = (self.inertia_weight * self.velocities[i] +\n                                  cognitive_term + social_term)\n            self.particles[i] += self.constriction_factor * self.velocities[i]\n        self.particles = np.clip(self.particles, lb, ub)\n\n    def get_local_best(self, index):\n        neighborhood_indices = np.random.choice(self.num_particles, self.dynamic_neighborhood_size, replace=False)\n        local_best_index = min(neighborhood_indices, key=lambda i: self.personal_best_fitness[i])\n        return self.personal_best_positions[local_best_index]\n\n    def apply_quantum_interference(self, lb, ub):\n        for i in range(self.num_particles):\n            if np.random.rand() < self.interference_prob:\n                interference_vector = lb + (ub - lb) * np.random.rand(self.dim)\n                self.particles[i] = np.mean([self.particles[i], interference_vector], axis=0)\n                self.particles[i] = np.clip(self.particles[i], lb, ub)\n\n    def apply_local_search(self, func, lb, ub):\n        for i in range(self.num_particles):\n            if np.random.rand() < self.local_search_prob:\n                perturbation = np.random.randn(self.dim) * 0.01\n                candidate = self.particles[i] + perturbation\n                candidate = np.clip(candidate, lb, ub)\n                candidate_fitness = func(candidate)\n                if candidate_fitness < self.personal_best_fitness[i]:\n                    self.personal_best_fitness[i] = candidate_fitness\n                    self.personal_best_positions[i] = candidate\n                if candidate_fitness < self.global_best_fitness:\n                    self.global_best_fitness = candidate_fitness\n                    self.global_best_position = candidate\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_particles(lb, ub)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            self.evaluate_particles(func)\n            evaluations += self.num_particles\n\n            if evaluations >= self.budget:\n                break\n\n            self.update_velocities_and_positions(lb, ub)\n            self.apply_quantum_interference(lb, ub)\n            self.apply_local_search(func, lb, ub)\n\n        return self.global_best_position, self.global_best_fitness", "name": "EnhancedQuantumInspiredPSO", "description": "Enhanced Quantum-Inspired Particle Swarm Optimization with Adaptive Neighborhoods and Local Search Boost to improve global and local search balance for optimizing photonic structures.", "configspace": "", "generation": 54, "fitness": 0.2768625279783151, "feedback": "The algorithm EnhancedQuantumInspiredPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28 with standard deviation 0.00.", "error": "", "parent_id": "8f855ef3-bb2e-48d4-969e-67f6a15bea7d", "metadata": {"aucs": [0.27633835985154387, 0.2770704129921002, 0.27717881109130127]}, "mutation_prompt": null}
{"id": "1d653924-6b5f-454c-a0d4-386333bb9274", "solution": "import numpy as np\n\nclass AdaptiveGeneticAlgorithm:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = max(10, min(50, budget // 10))\n        self.population = None\n        self.fitness = None\n        self.mutation_rate = 0.1\n        self.crossover_rate = 0.8\n        self.elitism_count = max(1, self.population_size // 10)\n\n    def initialize_population(self, lb, ub):\n        self.population = lb + (ub - lb) * np.random.rand(self.population_size, self.dim)\n\n    def evaluate_population(self, func):\n        self.fitness = np.array([func(individual) for individual in self.population])\n\n    def select_parents(self):\n        selection_prob = 1.0 / (1.0 + self.fitness)\n        selection_prob /= selection_prob.sum()\n        parent_indices = np.random.choice(np.arange(self.population_size), size=self.population_size, p=selection_prob)\n        return self.population[parent_indices]\n\n    def crossover(self, parent1, parent2):\n        if np.random.rand() < self.crossover_rate:\n            crossover_point = np.random.randint(1, self.dim - 1)\n            return np.concatenate((parent1[:crossover_point], parent2[crossover_point:]))\n        return parent1 if np.random.rand() > 0.5 else parent2\n\n    def mutate(self, individual, lb, ub):\n        for i in range(self.dim):\n            if np.random.rand() < self.mutation_rate:\n                individual[i] = lb[i] + (ub[i] - lb[i]) * np.random.rand()\n        return np.clip(individual, lb, ub)\n\n    def generate_offspring(self, parents, lb, ub):\n        offspring = []\n        for i in range(0, self.population_size, 2):\n            parent1, parent2 = parents[i], parents[min(i + 1, self.population_size - 1)]\n            child1 = self.crossover(parent1, parent2)\n            child2 = self.crossover(parent2, parent1)\n            offspring.append(self.mutate(child1, lb, ub))\n            offspring.append(self.mutate(child2, lb, ub))\n        return np.array(offspring)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            self.evaluate_population(func)\n            evaluations += self.population_size\n\n            if evaluations >= self.budget:\n                break\n\n            best_indices = np.argsort(self.fitness)[:self.elitism_count]\n            elite_individuals = self.population[best_indices]\n            parents = self.select_parents()\n            offspring = self.generate_offspring(parents, lb, ub)\n            self.population = np.concatenate((elite_individuals, offspring[:self.population_size - self.elitism_count]))\n\n            self.mutation_rate = 0.1 - 0.09 * (evaluations / self.budget)\n\n        best_index = np.argmin(self.fitness)\n        return self.population[best_index], self.fitness[best_index]", "name": "AdaptiveGeneticAlgorithm", "description": "Adaptive Genetic Algorithm with Dynamic Mutation and Hybrid Crossover for Efficient Exploration and Exploitation in Photonic Structure Optimization.", "configspace": "", "generation": 55, "fitness": 0.27467900005290463, "feedback": "The algorithm AdaptiveGeneticAlgorithm got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.27 with standard deviation 0.00.", "error": "", "parent_id": "8f855ef3-bb2e-48d4-969e-67f6a15bea7d", "metadata": {"aucs": [0.27417198949133104, 0.2751900451928917, 0.2746749654744912]}, "mutation_prompt": null}
{"id": "4415ce76-d496-48b4-9206-6a61534e3924", "solution": "import numpy as np\n\nclass EnhancedQuantumInspiredPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = max(10, min(50, budget // 10))\n        self.particles = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_fitness = np.full(self.num_particles, float('inf'))\n        self.global_best_position = None\n        self.global_best_fitness = float('inf')\n        self.cognitive_const = 2.0\n        self.social_const = 2.0\n        self.inertia_weight = 0.9  # Starting inertia weight\n        self.inertia_weight_min = 0.4\n        self.inertia_weight_max = 0.9\n        self.constriction_factor = 0.5\n        self.dynamic_neighborhood_size = 3\n        self.interference_prob = 0.1\n\n    def initialize_particles(self, lb, ub):\n        self.particles = lb + (ub - lb) * np.random.rand(self.num_particles, self.dim)\n        self.velocities = np.random.randn(self.num_particles, self.dim) * 0.1\n        self.personal_best_positions = np.copy(self.particles)\n\n    def evaluate_particles(self, func):\n        fitness = np.array([func(p) for p in self.particles])\n        for i, f in enumerate(fitness):\n            if f < self.personal_best_fitness[i]:\n                self.personal_best_fitness[i] = f\n                self.personal_best_positions[i] = self.particles[i]\n            if f < self.global_best_fitness:\n                self.global_best_fitness = f\n                self.global_best_position = self.particles[i]\n        return fitness\n\n    def update_velocities_and_positions(self, lb, ub, fitness):\n        for i in range(self.num_particles):\n            local_best = self.get_local_best(i, fitness)\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            cognitive_term = self.cognitive_const * r1 * (self.personal_best_positions[i] - self.particles[i])\n            social_term = self.social_const * r2 * (local_best - self.particles[i])\n            self.velocities[i] = (self.inertia_weight * self.velocities[i] +\n                                  cognitive_term + social_term)\n            self.particles[i] += self.constriction_factor * self.velocities[i]\n        self.particles = np.clip(self.particles, lb, ub)\n\n    def get_local_best(self, index, fitness):\n        sorted_indices = np.argsort(fitness)\n        neighborhood_indices = sorted_indices[:self.dynamic_neighborhood_size]\n        local_best_index = min(neighborhood_indices, key=lambda i: self.personal_best_fitness[i])\n        return self.personal_best_positions[local_best_index]\n\n    def apply_quantum_interference(self, lb, ub):\n        for i in range(self.num_particles):\n            if np.random.rand() < self.interference_prob:\n                interference_vector = lb + (ub - lb) * np.random.rand(self.dim)\n                self.particles[i] = np.mean([self.particles[i], interference_vector], axis=0)\n                self.particles[i] = np.clip(self.particles[i], lb, ub)\n\n    def adapt_inertia_weight(self, evaluations):\n        self.inertia_weight = self.inertia_weight_max - (self.inertia_weight_max - self.inertia_weight_min) * (evaluations / self.budget)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_particles(lb, ub)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            fitness = self.evaluate_particles(func)\n            evaluations += self.num_particles\n\n            if evaluations >= self.budget:\n                break\n\n            self.adapt_inertia_weight(evaluations)\n            self.update_velocities_and_positions(lb, ub, fitness)\n            self.apply_quantum_interference(lb, ub)\n\n        return self.global_best_position, self.global_best_fitness", "name": "EnhancedQuantumInspiredPSO", "description": "Enhanced Quantum-Inspired Particle Swarm Optimization with Adaptive Inertia Weight and Fitness-based Dynamic Neighborhoods for improved convergence in photonic structure optimization.", "configspace": "", "generation": 56, "fitness": 0.27664817667044195, "feedback": "The algorithm EnhancedQuantumInspiredPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28 with standard deviation 0.00.", "error": "", "parent_id": "8f855ef3-bb2e-48d4-969e-67f6a15bea7d", "metadata": {"aucs": [0.2765108328462986, 0.27682063761030995, 0.27661305955471727]}, "mutation_prompt": null}
{"id": "0ee965fd-3a65-4a2c-8f54-faeea8fc631a", "solution": "import numpy as np\nfrom scipy.stats import levy\n\nclass AdaptiveLevyFlightPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = max(10, min(50, budget // 10))\n        self.particles = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_fitness = np.full(self.num_particles, float('inf'))\n        self.global_best_position = None\n        self.global_best_fitness = float('inf')\n        self.cognitive_const = 2.0\n        self.social_const = 2.0\n        self.inertia_weight = 0.7\n        self.dynamic_neighborhood_size = 3\n        self.levy_scale = 0.1\n\n    def initialize_particles(self, lb, ub):\n        self.particles = lb + (ub - lb) * np.random.rand(self.num_particles, self.dim)\n        self.velocities = np.random.randn(self.num_particles, self.dim) * 0.1\n        self.personal_best_positions = np.copy(self.particles)\n\n    def evaluate_particles(self, func):\n        fitness = np.array([func(p) for p in self.particles])\n        for i, f in enumerate(fitness):\n            if f < self.personal_best_fitness[i]:\n                self.personal_best_fitness[i] = f\n                self.personal_best_positions[i] = self.particles[i]\n            if f < self.global_best_fitness:\n                self.global_best_fitness = f\n                self.global_best_position = self.particles[i]\n        return fitness\n\n    def update_velocities_and_positions(self, lb, ub):\n        for i in range(self.num_particles):\n            local_best = self.get_local_best(i)\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            cognitive_term = self.cognitive_const * r1 * (self.personal_best_positions[i] - self.particles[i])\n            social_term = self.social_const * r2 * (local_best - self.particles[i])\n            self.velocities[i] = (self.inertia_weight * self.velocities[i] +\n                                  cognitive_term + social_term)\n            self.particles[i] += self.velocities[i]\n            # Levy flight adjustment for exploration\n            levy_jump = levy.rvs(size=self.dim, scale=self.levy_scale)\n            if np.random.rand() < 0.3:  # 30% chance to apply Levy flight\n                self.particles[i] += levy_jump\n            self.particles[i] = np.clip(self.particles[i], lb, ub)\n\n    def get_local_best(self, index):\n        neighborhood_indices = np.random.choice(self.num_particles, self.dynamic_neighborhood_size, replace=False)\n        local_best_index = min(neighborhood_indices, key=lambda i: self.personal_best_fitness[i])\n        return self.personal_best_positions[local_best_index]\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_particles(lb, ub)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            self.evaluate_particles(func)\n            evaluations += self.num_particles\n\n            if evaluations >= self.budget:\n                break\n\n            self.update_velocities_and_positions(lb, ub)\n\n        return self.global_best_position, self.global_best_fitness", "name": "AdaptiveLevyFlightPSO", "description": "Adaptive Levy Flight-Inspired Particle Swarm Optimization leveraging Levy flights for enhanced exploration and adaptive local convergence within constrained evaluations.", "configspace": "", "generation": 57, "fitness": 0.2526591401938059, "feedback": "The algorithm AdaptiveLevyFlightPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.25 with standard deviation 0.00.", "error": "", "parent_id": "8f855ef3-bb2e-48d4-969e-67f6a15bea7d", "metadata": {"aucs": [0.2522688463372148, 0.2539870922003761, 0.25172148204382694]}, "mutation_prompt": null}
{"id": "46c6337c-7879-4108-811f-d2668fa98a93", "solution": "import numpy as np\n\nclass EnhancedQuantumInspiredPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = max(10, min(50, budget // 10))\n        self.particles = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_fitness = np.full(self.num_particles, float('inf'))\n        self.global_best_position = None\n        self.global_best_fitness = float('inf')\n        self.cognitive_const = 2.0\n        self.social_const = 2.0\n        self.inertia_weight_max = 0.9\n        self.inertia_weight_min = 0.4\n        self.constriction_factor = 0.5\n        self.dynamic_neighborhood_size = 3\n        self.interference_prob = 0.1\n        self.mutation_prob = 0.05\n\n    def initialize_particles(self, lb, ub):\n        self.particles = lb + (ub - lb) * np.random.rand(self.num_particles, self.dim)\n        self.velocities = np.random.randn(self.num_particles, self.dim) * 0.1\n        self.personal_best_positions = np.copy(self.particles)\n\n    def evaluate_particles(self, func):\n        fitness = np.array([func(p) for p in self.particles])\n        for i, f in enumerate(fitness):\n            if f < self.personal_best_fitness[i]:\n                self.personal_best_fitness[i] = f\n                self.personal_best_positions[i] = self.particles[i]\n            if f < self.global_best_fitness:\n                self.global_best_fitness = f\n                self.global_best_position = self.particles[i]\n        return fitness\n\n    def update_velocities_and_positions(self, lb, ub, evaluations):\n        inertia_weight = self.inertia_weight_max - (self.inertia_weight_max - self.inertia_weight_min) * (evaluations / self.budget)\n        for i in range(self.num_particles):\n            local_best = self.get_local_best(i)\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            cognitive_term = self.cognitive_const * r1 * (self.personal_best_positions[i] - self.particles[i])\n            social_term = self.social_const * r2 * (local_best - self.particles[i])\n            self.velocities[i] = (inertia_weight * self.velocities[i] +\n                                  cognitive_term + social_term)\n            self.particles[i] += self.constriction_factor * self.velocities[i]\n        self.particles = np.clip(self.particles, lb, ub)\n\n    def get_local_best(self, index):\n        neighborhood_indices = np.random.choice(self.num_particles, self.dynamic_neighborhood_size, replace=False)\n        local_best_index = min(neighborhood_indices, key=lambda i: self.personal_best_fitness[i])\n        return self.personal_best_positions[local_best_index]\n\n    def apply_quantum_interference_and_mutation(self, lb, ub):\n        for i in range(self.num_particles):\n            if np.random.rand() < self.interference_prob:\n                interference_vector = lb + (ub - lb) * np.random.rand(self.dim)\n                self.particles[i] = np.mean([self.particles[i], interference_vector], axis=0)\n                self.particles[i] = np.clip(self.particles[i], lb, ub)\n            if np.random.rand() < self.mutation_prob:\n                mutation_vector = lb + (ub - lb) * np.random.rand(self.dim)\n                self.particles[i] = np.mean([self.particles[i], mutation_vector], axis=0)\n                self.particles[i] = np.clip(self.particles[i], lb, ub)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_particles(lb, ub)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            self.evaluate_particles(func)\n            evaluations += self.num_particles\n\n            if evaluations >= self.budget:\n                break\n\n            self.update_velocities_and_positions(lb, ub, evaluations)\n            self.apply_quantum_interference_and_mutation(lb, ub)\n\n        return self.global_best_position, self.global_best_fitness", "name": "EnhancedQuantumInspiredPSO", "description": "Enhanced Quantum-Inspired Particle Swarm Optimization with Dynamic Inertia Weight and Adaptive Mutation for more efficient exploration and exploitation in optimizing photonic structures.", "configspace": "", "generation": 58, "fitness": 0.27449498378581666, "feedback": "The algorithm EnhancedQuantumInspiredPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.27 with standard deviation 0.00.", "error": "", "parent_id": "8f855ef3-bb2e-48d4-969e-67f6a15bea7d", "metadata": {"aucs": [0.27470890512694346, 0.2743792678334648, 0.2743967783970417]}, "mutation_prompt": null}
{"id": "e9d00403-a54c-4cb0-80d5-bc18b88d906e", "solution": "import numpy as np\n\nclass EnhancedQuantumInspiredPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = max(10, min(50, budget // 10))\n        self.particles = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_fitness = np.full(self.num_particles, float('inf'))\n        self.global_best_position = None\n        self.global_best_fitness = float('inf')\n        self.cognitive_const = 2.0\n        self.social_const = 2.0\n        self.inertia_weight_initial = 0.9\n        self.inertia_weight_final = 0.4\n        self.dynamic_neighborhood_size = 3\n        self.interference_prob = 0.2\n\n    def initialize_particles(self, lb, ub):\n        self.particles = lb + (ub - lb) * np.random.rand(self.num_particles, self.dim)\n        self.velocities = np.random.randn(self.num_particles, self.dim) * 0.1\n        self.personal_best_positions = np.copy(self.particles)\n\n    def evaluate_particles(self, func):\n        fitness = np.array([func(p) for p in self.particles])\n        for i, f in enumerate(fitness):\n            if f < self.personal_best_fitness[i]:\n                self.personal_best_fitness[i] = f\n                self.personal_best_positions[i] = self.particles[i]\n            if f < self.global_best_fitness:\n                self.global_best_fitness = f\n                self.global_best_position = self.particles[i]\n        return fitness\n\n    def update_velocities_and_positions(self, lb, ub, iteration, max_iterations):\n        inertia_weight = self.inertia_weight_initial - \\\n                         (self.inertia_weight_initial - self.inertia_weight_final) * (iteration / max_iterations)\n        for i in range(self.num_particles):\n            local_best = self.get_local_best(i)\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            cognitive_term = self.cognitive_const * r1 * (self.personal_best_positions[i] - self.particles[i])\n            social_term = self.social_const * r2 * (local_best - self.particles[i])\n            self.velocities[i] = (inertia_weight * self.velocities[i] +\n                                  cognitive_term + social_term)\n            self.particles[i] += self.velocities[i]\n        self.particles = np.clip(self.particles, lb, ub)\n\n    def get_local_best(self, index):\n        neighborhood_indices = np.random.choice(self.num_particles, self.dynamic_neighborhood_size, replace=False)\n        local_best_index = min(neighborhood_indices, key=lambda i: self.personal_best_fitness[i])\n        return self.personal_best_positions[local_best_index]\n\n    def apply_quantum_interference(self, lb, ub):\n        for i in range(self.num_particles):\n            if np.random.rand() < self.interference_prob:\n                interference_vector = lb + (ub - lb) * np.random.rand(self.dim)\n                mix_factor = np.random.rand()\n                self.particles[i] = mix_factor * self.particles[i] + (1 - mix_factor) * interference_vector\n                self.particles[i] = np.clip(self.particles[i], lb, ub)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_particles(lb, ub)\n        evaluations = 0\n        iteration = 0\n        max_iterations = self.budget // self.num_particles\n\n        while evaluations < self.budget:\n            self.evaluate_particles(func)\n            evaluations += self.num_particles\n\n            if evaluations >= self.budget:\n                break\n\n            self.update_velocities_and_positions(lb, ub, iteration, max_iterations)\n            self.apply_quantum_interference(lb, ub)\n            iteration += 1\n\n        return self.global_best_position, self.global_best_fitness", "name": "EnhancedQuantumInspiredPSO", "description": "Enhanced Quantum-Inspired Particle Swarm Optimization with Dynamic Inertia and Multi-Strategy Interference for improved adaptability in photonic structure optimization.", "configspace": "", "generation": 59, "fitness": 0.26119385809241563, "feedback": "The algorithm EnhancedQuantumInspiredPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.26 with standard deviation 0.00.", "error": "", "parent_id": "8f855ef3-bb2e-48d4-969e-67f6a15bea7d", "metadata": {"aucs": [0.262055010836136, 0.25641345438133134, 0.2651131090597796]}, "mutation_prompt": null}
{"id": "57788a29-479c-476b-b042-d652a09099bd", "solution": "import numpy as np\n\nclass EnhancedQuantumInspiredPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = max(10, min(50, budget // 10))\n        self.particles = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_fitness = np.full(self.num_particles, float('inf'))\n        self.global_best_position = None\n        self.global_best_fitness = float('inf')\n        self.cognitive_const = 2.0\n        self.social_const = 2.0\n        self.inertia_weight_min = 0.4\n        self.inertia_weight_max = 0.9\n        self.constriction_factor = 0.5\n        self.dynamic_neighborhood_size = 3\n        self.interference_prob = 0.1\n        self.tunneling_prob = 0.05\n\n    def initialize_particles(self, lb, ub):\n        self.particles = lb + (ub - lb) * np.random.rand(self.num_particles, self.dim)\n        self.velocities = np.random.randn(self.num_particles, self.dim) * 0.1\n        self.personal_best_positions = np.copy(self.particles)\n\n    def evaluate_particles(self, func):\n        fitness = np.array([func(p) for p in self.particles])\n        for i, f in enumerate(fitness):\n            if f < self.personal_best_fitness[i]:\n                self.personal_best_fitness[i] = f\n                self.personal_best_positions[i] = self.particles[i]\n            if f < self.global_best_fitness:\n                self.global_best_fitness = f\n                self.global_best_position = self.particles[i]\n        return fitness\n\n    def update_velocities_and_positions(self, lb, ub, evaluations):\n        inertia_weight = (self.inertia_weight_max - self.inertia_weight_min) * (1 - evaluations / self.budget) + self.inertia_weight_min\n\n        for i in range(self.num_particles):\n            local_best = self.get_local_best(i)\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            cognitive_term = self.cognitive_const * r1 * (self.personal_best_positions[i] - self.particles[i])\n            social_term = self.social_const * r2 * (local_best - self.particles[i])\n            self.velocities[i] = (inertia_weight * self.velocities[i] +\n                                  cognitive_term + social_term)\n            self.particles[i] += self.constriction_factor * self.velocities[i]\n        self.particles = np.clip(self.particles, lb, ub)\n\n    def get_local_best(self, index):\n        neighborhood_indices = np.random.choice(self.num_particles, self.dynamic_neighborhood_size, replace=False)\n        local_best_index = min(neighborhood_indices, key=lambda i: self.personal_best_fitness[i])\n        return self.personal_best_positions[local_best_index]\n\n    def apply_quantum_interference(self, lb, ub):\n        for i in range(self.num_particles):\n            if np.random.rand() < self.interference_prob:\n                interference_vector = lb + (ub - lb) * np.random.rand(self.dim)\n                self.particles[i] = np.mean([self.particles[i], interference_vector], axis=0)\n                self.particles[i] = np.clip(self.particles[i], lb, ub)\n\n    def apply_quantum_tunneling(self, lb, ub):\n        for i in range(self.num_particles):\n            if np.random.rand() < self.tunneling_prob:\n                tunnel_direction = np.random.randn(self.dim)\n                tunnel_magnitude = np.linalg.norm(ub - lb) * np.random.rand()\n                self.particles[i] += tunnel_magnitude * tunnel_direction\n                self.particles[i] = np.clip(self.particles[i], lb, ub)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_particles(lb, ub)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            self.evaluate_particles(func)\n            evaluations += self.num_particles\n\n            if evaluations >= self.budget:\n                break\n\n            self.update_velocities_and_positions(lb, ub, evaluations)\n            self.apply_quantum_interference(lb, ub)\n            self.apply_quantum_tunneling(lb, ub)\n\n        return self.global_best_position, self.global_best_fitness", "name": "EnhancedQuantumInspiredPSO", "description": "Enhanced Quantum-Inspired Particle Swarm Optimization with Adaptive Inertia and Novel Quantum Tunneling for improved convergence in photonic structure optimization.", "configspace": "", "generation": 60, "fitness": 0.2729671918452096, "feedback": "The algorithm EnhancedQuantumInspiredPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.27 with standard deviation 0.00.", "error": "", "parent_id": "8f855ef3-bb2e-48d4-969e-67f6a15bea7d", "metadata": {"aucs": [0.2711219363271563, 0.27342961994637893, 0.27435001926209357]}, "mutation_prompt": null}
{"id": "6e34d1b4-879f-4028-ae75-b5bfb2b87bd5", "solution": "import numpy as np\n\nclass EnhancedQuantumInspiredParticleSwarmOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = max(10, min(50, budget // 10))\n        self.particles = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_fitness = np.full(self.num_particles, float('inf'))\n        self.global_best_position = None\n        self.global_best_fitness = float('inf')\n        self.base_cognitive_const = 2.0\n        self.base_social_const = 2.0\n        self.base_inertia_weight = 0.7\n        self.base_constriction_factor = 0.5\n        self.dynamic_neighborhood_size = 3\n        self.interference_prob = 0.1\n\n    def initialize_particles(self, lb, ub):\n        self.particles = lb + (ub - lb) * np.random.rand(self.num_particles, self.dim)\n        self.velocities = np.random.randn(self.num_particles, self.dim) * 0.1\n        self.personal_best_positions = np.copy(self.particles)\n\n    def evaluate_particles(self, func):\n        fitness = np.array([func(p) for p in self.particles])\n        for i, f in enumerate(fitness):\n            if f < self.personal_best_fitness[i]:\n                self.personal_best_fitness[i] = f\n                self.personal_best_positions[i] = self.particles[i]\n            if f < self.global_best_fitness:\n                self.global_best_fitness = f\n                self.global_best_position = self.particles[i]\n        return fitness\n\n    def update_velocities_and_positions(self, lb, ub):\n        for i in range(self.num_particles):\n            local_best = self.get_local_best(i)\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            inertia_weight = self.fuzzy_inertia_weight(i)\n            cognitive_const, social_const = self.fuzzy_learning_factors(i)\n            cognitive_term = cognitive_const * r1 * (self.personal_best_positions[i] - self.particles[i])\n            social_term = social_const * r2 * (local_best - self.particles[i])\n            self.velocities[i] = (inertia_weight * self.velocities[i] +\n                                  cognitive_term + social_term)\n            self.particles[i] += self.base_constriction_factor * self.velocities[i]\n        self.particles = np.clip(self.particles, lb, ub)\n\n    def get_local_best(self, index):\n        neighborhood_indices = np.random.choice(self.num_particles, self.dynamic_neighborhood_size, replace=False)\n        local_best_index = min(neighborhood_indices, key=lambda i: self.personal_best_fitness[i])\n        return self.personal_best_positions[local_best_index]\n\n    def apply_quantum_interference(self, lb, ub):\n        for i in range(self.num_particles):\n            if np.random.rand() < self.interference_prob:\n                interference_vector = lb + (ub - lb) * np.random.rand(self.dim)\n                self.particles[i] = np.mean([self.particles[i], interference_vector], axis=0)\n                self.particles[i] = np.clip(self.particles[i], lb, ub)\n\n    def fuzzy_inertia_weight(self, particle_index):\n        diversity = np.std(self.particles, axis=0).mean()\n        return self.base_inertia_weight + 0.1 * diversity\n\n    def fuzzy_learning_factors(self, particle_index):\n        improvement = (self.personal_best_fitness[particle_index] - self.global_best_fitness) / self.global_best_fitness\n        return (self.base_cognitive_const - 0.5 * improvement, self.base_social_const + 0.5 * improvement)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_particles(lb, ub)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            self.evaluate_particles(func)\n            evaluations += self.num_particles\n\n            if evaluations >= self.budget:\n                break\n\n            self.update_velocities_and_positions(lb, ub)\n            self.apply_quantum_interference(lb, ub)\n\n        return self.global_best_position, self.global_best_fitness", "name": "EnhancedQuantumInspiredParticleSwarmOptimization", "description": "Enhanced Quantum-Inspired Particle Swarm Optimization with Adaptive Neighborhoods and Multi-agent Interference using Fuzzy Logic for dynamic parameter adaptation in optimizing photonic structures.", "configspace": "", "generation": 61, "fitness": 0.24540540573188407, "feedback": "The algorithm EnhancedQuantumInspiredParticleSwarmOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.25 with standard deviation 0.01.", "error": "", "parent_id": "8f855ef3-bb2e-48d4-969e-67f6a15bea7d", "metadata": {"aucs": [0.23896686464257733, 0.24563184299624563, 0.25161750955682927]}, "mutation_prompt": null}
{"id": "1febbd73-2c8e-46aa-a20e-98a99af65e5f", "solution": "import numpy as np\n\nclass EnhancedQuantumInspiredPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = max(10, min(50, budget // 10))\n        self.particles = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_fitness = np.full(self.num_particles, float('inf'))\n        self.global_best_position = None\n        self.global_best_fitness = float('inf')\n        self.cognitive_const = 2.0\n        self.social_const = 2.0\n        self.inertia_weight = 0.7\n        self.constriction_factor = 0.5\n        self.dynamic_neighborhood_size = 3\n        self.interference_prob = 0.1\n        self.tunneling_prob = 0.05\n\n    def initialize_particles(self, lb, ub):\n        self.particles = lb + (ub - lb) * np.random.rand(self.num_particles, self.dim)\n        self.velocities = np.random.randn(self.num_particles, self.dim) * 0.1\n        self.personal_best_positions = np.copy(self.particles)\n\n    def evaluate_particles(self, func):\n        fitness = np.array([func(p) for p in self.particles])\n        for i, f in enumerate(fitness):\n            if f < self.personal_best_fitness[i]:\n                self.personal_best_fitness[i] = f\n                self.personal_best_positions[i] = self.particles[i]\n            if f < self.global_best_fitness:\n                self.global_best_fitness = f\n                self.global_best_position = self.particles[i]\n        return fitness\n\n    def update_velocities_and_positions(self, lb, ub):\n        for i in range(self.num_particles):\n            local_best = self.get_local_best(i)\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            cognitive_term = self.cognitive_const * r1 * (self.personal_best_positions[i] - self.particles[i])\n            social_term = self.social_const * r2 * (local_best - self.particles[i])\n            self.velocities[i] = (self.inertia_weight * self.velocities[i] +\n                                  cognitive_term + social_term)\n            self.particles[i] += self.constriction_factor * self.velocities[i]\n        self.particles = np.clip(self.particles, lb, ub)\n\n    def get_local_best(self, index):\n        neighborhood_indices = np.random.choice(self.num_particles, self.dynamic_neighborhood_size, replace=False)\n        local_best_index = min(neighborhood_indices, key=lambda i: self.personal_best_fitness[i])\n        return self.personal_best_positions[local_best_index]\n\n    def apply_quantum_interference(self, lb, ub):\n        for i in range(self.num_particles):\n            if np.random.rand() < self.interference_prob:\n                interference_vector = lb + (ub - lb) * np.random.rand(self.dim)\n                self.particles[i] = np.mean([self.particles[i], interference_vector], axis=0)\n                self.particles[i] = np.clip(self.particles[i], lb, ub)\n\n    def apply_quantum_tunneling(self, lb, ub):\n        for i in range(self.num_particles):\n            if np.random.rand() < self.tunneling_prob:\n                tunnel_vector = lb + (ub - lb) * np.random.rand(self.dim)\n                self.particles[i] = np.where(np.random.rand(self.dim) < 0.5, tunnel_vector, self.particles[i])\n                self.particles[i] = np.clip(self.particles[i], lb, ub)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_particles(lb, ub)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            self.evaluate_particles(func)\n            evaluations += self.num_particles\n\n            if evaluations >= self.budget:\n                break\n\n            self.update_velocities_and_positions(lb, ub)\n            self.apply_quantum_interference(lb, ub)\n            self.apply_quantum_tunneling(lb, ub)\n\n        return self.global_best_position, self.global_best_fitness", "name": "EnhancedQuantumInspiredPSO", "description": "Enhanced Quantum-Inspired Particle Swarm Optimization with Adaptive Quantum Tunneling and Neighborhood Restructuring for improved convergence in photonic structure optimization.", "configspace": "", "generation": 62, "fitness": 0.27664720594746534, "feedback": "The algorithm EnhancedQuantumInspiredPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28 with standard deviation 0.00.", "error": "", "parent_id": "8f855ef3-bb2e-48d4-969e-67f6a15bea7d", "metadata": {"aucs": [0.27676122566075645, 0.27658000429921714, 0.27660038788242236]}, "mutation_prompt": null}
{"id": "12bbc4b6-8610-482b-8b68-5fd8805a3bd2", "solution": "import numpy as np\n\nclass QuantumInspiredHarmonicSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.hmsize = max(10, min(50, budget // 10))\n        self.hmcr = 0.9  # Harmony Memory Considering Rate\n        self.par = 0.3   # Pitch Adjustment Rate\n        self.harmony_memory = None\n        self.best_harmony = None\n        self.best_fitness = float('inf')\n\n    def initialize_harmony_memory(self, lb, ub):\n        self.harmony_memory = lb + (ub - lb) * np.random.rand(self.hmsize, self.dim)\n        self.best_harmony = self.harmony_memory[0]\n\n    def evaluate_harmony_memory(self, func):\n        fitness = np.array([func(h) for h in self.harmony_memory])\n        for i, f in enumerate(fitness):\n            if f < self.best_fitness:\n                self.best_fitness = f\n                self.best_harmony = self.harmony_memory[i]\n        return fitness\n\n    def generate_new_harmony(self, lb, ub):\n        new_harmony = np.copy(self.best_harmony)\n        for i in range(self.dim):\n            if np.random.rand() < self.hmcr:\n                new_harmony[i] = self.harmony_memory[np.random.randint(self.hmsize)][i]\n            if np.random.rand() < self.par:\n                adjustment = (ub[i] - lb[i]) * (np.random.rand() - 0.5)\n                new_harmony[i] += adjustment\n                new_harmony[i] = np.clip(new_harmony[i], lb[i], ub[i])\n        return new_harmony\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_harmony_memory(lb, ub)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            fitness = self.evaluate_harmony_memory(func)\n            evaluations += self.hmsize\n\n            if evaluations >= self.budget:\n                break\n\n            new_harmony = self.generate_new_harmony(lb, ub)\n            new_fitness = func(new_harmony)\n            evaluations += 1\n\n            if new_fitness < max(fitness):\n                worst_index = np.argmax(fitness)\n                self.harmony_memory[worst_index] = new_harmony\n                if new_fitness < self.best_fitness:\n                    self.best_fitness = new_fitness\n                    self.best_harmony = new_harmony\n\n        return self.best_harmony, self.best_fitness", "name": "QuantumInspiredHarmonicSearch", "description": "Quantum-Inspired Harmonic Search Algorithm using stochastic harmonic balance and adaptive solution generation for efficient exploration and exploitation in optimizing photonic structures.", "configspace": "", "generation": 63, "fitness": 0.24421268378842775, "feedback": "The algorithm QuantumInspiredHarmonicSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.24 with standard deviation 0.00.", "error": "", "parent_id": "8f855ef3-bb2e-48d4-969e-67f6a15bea7d", "metadata": {"aucs": [0.24817194271232024, 0.2423242091280905, 0.24214189952487253]}, "mutation_prompt": null}
{"id": "ae60edab-898f-43f2-b6c9-0747564b4a96", "solution": "import numpy as np\n\nclass EnhancedQuantumInspiredPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = max(10, min(50, budget // 10))\n        self.particles = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_fitness = np.full(self.num_particles, float('inf'))\n        self.global_best_position = None\n        self.global_best_fitness = float('inf')\n        self.cognitive_const = 1.5\n        self.social_const = 2.5\n        self.inertia_weight = 0.6\n        self.constriction_factor = 0.7\n        self.dynamic_neighborhood_size = 5\n        self.interference_prob = 0.15\n        self.tunneling_prob = 0.05\n\n    def initialize_particles(self, lb, ub):\n        self.particles = lb + (ub - lb) * np.random.rand(self.num_particles, self.dim)\n        self.velocities = np.random.randn(self.num_particles, self.dim) * 0.1\n        self.personal_best_positions = np.copy(self.particles)\n\n    def evaluate_particles(self, func):\n        fitness = np.array([func(p) for p in self.particles])\n        for i, f in enumerate(fitness):\n            if f < self.personal_best_fitness[i]:\n                self.personal_best_fitness[i] = f\n                self.personal_best_positions[i] = self.particles[i]\n            if f < self.global_best_fitness:\n                self.global_best_fitness = f\n                self.global_best_position = self.particles[i]\n        return fitness\n\n    def update_velocities_and_positions(self, lb, ub):\n        for i in range(self.num_particles):\n            local_best = self.get_local_best(i)\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            cognitive_term = self.cognitive_const * r1 * (self.personal_best_positions[i] - self.particles[i])\n            social_term = self.social_const * r2 * (local_best - self.particles[i])\n            self.velocities[i] = (self.inertia_weight * self.velocities[i] +\n                                  cognitive_term + social_term)\n            self.particles[i] += self.constriction_factor * self.velocities[i]\n        self.particles = np.clip(self.particles, lb, ub)\n\n    def get_local_best(self, index):\n        neighborhood_indices = np.random.choice(self.num_particles, self.dynamic_neighborhood_size, replace=False)\n        local_best_index = min(neighborhood_indices, key=lambda i: self.personal_best_fitness[i])\n        return self.personal_best_positions[local_best_index]\n\n    def apply_quantum_interference(self, lb, ub):\n        for i in range(self.num_particles):\n            if np.random.rand() < self.interference_prob:\n                interference_vector = lb + (ub - lb) * np.random.rand(self.dim)\n                self.particles[i] = np.mean([self.particles[i], interference_vector], axis=0)\n                self.particles[i] = np.clip(self.particles[i], lb, ub)\n\n    def apply_quantum_tunneling(self, lb, ub):\n        for i in range(self.num_particles):\n            if np.random.rand() < self.tunneling_prob:\n                self.particles[i] = self.global_best_position + np.random.randn(self.dim) * 0.1\n                self.particles[i] = np.clip(self.particles[i], lb, ub)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_particles(lb, ub)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            self.evaluate_particles(func)\n            evaluations += self.num_particles\n\n            if evaluations >= self.budget:\n                break\n\n            self.update_velocities_and_positions(lb, ub)\n            self.apply_quantum_interference(lb, ub)\n            self.apply_quantum_tunneling(lb, ub)\n\n        return self.global_best_position, self.global_best_fitness", "name": "EnhancedQuantumInspiredPSO", "description": "Enhanced Quantum-Inspired Particle Swarm Optimization with Adaptive Neighborhoods and Quantum Tunneling for improved exploration and exploitation in optimizing photonic structures.", "configspace": "", "generation": 64, "fitness": 0.2722396173411422, "feedback": "The algorithm EnhancedQuantumInspiredPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.27 with standard deviation 0.00.", "error": "", "parent_id": "8f855ef3-bb2e-48d4-969e-67f6a15bea7d", "metadata": {"aucs": [0.27284635128730417, 0.2717111181704994, 0.27216138256562306]}, "mutation_prompt": null}
{"id": "588faf75-2617-4db1-b9ab-0cfbc6e7fcb9", "solution": "import numpy as np\n\nclass EnhancedQuantumInspiredParticleSwarmOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = max(10, min(50, budget // 10))\n        self.particles = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_fitness = np.full(self.num_particles, float('inf'))\n        self.global_best_position = None\n        self.global_best_fitness = float('inf')\n        self.cognitive_const = 2.0\n        self.social_const = 2.0\n        self.inertia_weight = 0.9  # Increase initial inertia for wider exploration\n        self.constriction_factor = 0.5\n        self.dynamic_neighborhood_size = 5  # Larger neighborhood to improve local exploration\n        self.interference_prob = 0.1\n        self.diversity_threshold = 0.1  # Diversity threshold for reinitialization\n        self.adaptive_rate = 0.995  # Adaptive decay rate for inertia\n\n    def initialize_particles(self, lb, ub):\n        self.particles = lb + (ub - lb) * np.random.rand(self.num_particles, self.dim)\n        self.velocities = np.random.randn(self.num_particles, self.dim) * 0.1\n        self.personal_best_positions = np.copy(self.particles)\n\n    def evaluate_particles(self, func):\n        fitness = np.array([func(p) for p in self.particles])\n        for i, f in enumerate(fitness):\n            if f < self.personal_best_fitness[i]:\n                self.personal_best_fitness[i] = f\n                self.personal_best_positions[i] = self.particles[i]\n            if f < self.global_best_fitness:\n                self.global_best_fitness = f\n                self.global_best_position = self.particles[i]\n        return fitness\n\n    def update_velocities_and_positions(self, lb, ub):\n        for i in range(self.num_particles):\n            local_best = self.get_local_best(i)\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            cognitive_term = self.cognitive_const * r1 * (self.personal_best_positions[i] - self.particles[i])\n            social_term = self.social_const * r2 * (local_best - self.particles[i])\n            self.velocities[i] = (self.inertia_weight * self.velocities[i] +\n                                  cognitive_term + social_term)\n            self.particles[i] += self.constriction_factor * self.velocities[i]\n        self.particles = np.clip(self.particles, lb, ub)\n        self.inertia_weight *= self.adaptive_rate  # Decay inertia for better convergence\n\n    def get_local_best(self, index):\n        neighborhood_indices = np.random.choice(self.num_particles, self.dynamic_neighborhood_size, replace=False)\n        local_best_index = min(neighborhood_indices, key=lambda i: self.personal_best_fitness[i])\n        return self.personal_best_positions[local_best_index]\n\n    def apply_quantum_interference(self, lb, ub):\n        for i in range(self.num_particles):\n            if np.random.rand() < self.interference_prob:\n                interference_vector = lb + (ub - lb) * np.random.rand(self.dim)\n                self.particles[i] = np.mean([self.particles[i], interference_vector], axis=0)\n                self.particles[i] = np.clip(self.particles[i], lb, ub)\n\n    def preserve_diversity(self, lb, ub):\n        diversity = np.std(self.particles, axis=0)\n        if np.any(diversity < self.diversity_threshold):\n            reinit_indices = np.random.choice(self.num_particles, size=self.num_particles // 2, replace=False)\n            self.particles[reinit_indices] = lb + (ub - lb) * np.random.rand(len(reinit_indices), self.dim)\n            self.velocities[reinit_indices] = np.random.randn(len(reinit_indices), self.dim) * 0.1\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_particles(lb, ub)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            self.evaluate_particles(func)\n            evaluations += self.num_particles\n\n            if evaluations >= self.budget:\n                break\n\n            self.update_velocities_and_positions(lb, ub)\n            self.apply_quantum_interference(lb, ub)\n            self.preserve_diversity(lb, ub)\n\n        return self.global_best_position, self.global_best_fitness", "name": "EnhancedQuantumInspiredParticleSwarmOptimization", "description": "Enhanced Quantum-Inspired Particle Swarm Optimization with Adaptive Dynamic Neighborhoods and Multi-agent Interference, incorporating a diversity-preserving mechanism and adaptive parameters for improved exploration and exploitation in complex photonic structure optimization.", "configspace": "", "generation": 65, "fitness": 0.27660418141483584, "feedback": "The algorithm EnhancedQuantumInspiredParticleSwarmOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28 with standard deviation 0.00.", "error": "", "parent_id": "8f855ef3-bb2e-48d4-969e-67f6a15bea7d", "metadata": {"aucs": [0.2767506138716138, 0.2761137653455453, 0.2769481650273484]}, "mutation_prompt": null}
{"id": "167488fc-85aa-4769-a3cc-db281324f92a", "solution": "import numpy as np\n\nclass AdaptiveDifferentialEvolutionWithChaos:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = max(10, min(50, budget // 10))\n        self.population = None\n        self.best_solution = None\n        self.best_fitness = float('inf')\n        self.scale_factor = 0.5\n        self.crossover_rate = 0.9\n        self.chaos_sequence = self.generate_chaos_sequence(self.population_size)\n\n    def generate_chaos_sequence(self, size):\n        x = np.zeros(size)\n        x[0] = np.random.rand()\n        for i in range(1, size):\n            x[i] = 4 * x[i-1] * (1 - x[i-1])  # Logistic map\n        return x\n\n    def initialize_population(self, lb, ub):\n        self.population = lb + (ub - lb) * np.random.rand(self.population_size, self.dim)\n\n    def evaluate_population(self, func):\n        fitness = np.array([func(individual) for individual in self.population])\n        for i, f in enumerate(fitness):\n            if f < self.best_fitness:\n                self.best_fitness = f\n                self.best_solution = self.population[i]\n        return fitness\n\n    def mutate(self, target_idx):\n        indices = [idx for idx in range(self.population_size) if idx != target_idx]\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        mutant_vector = (self.population[a] +\n                         self.scale_factor * (self.population[b] - self.population[c]))\n        return mutant_vector\n\n    def crossover(self, target_vector, mutant_vector):\n        crossover_mask = (np.random.rand(self.dim) < self.crossover_rate)\n        trial_vector = np.where(crossover_mask, mutant_vector, target_vector)\n        return trial_vector\n\n    def adaptive_population_size(self, evaluations):\n        return min(self.population_size, int(self.population_size * (1 - evaluations / self.budget)))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            current_pop_size = self.adaptive_population_size(evaluations)\n            fitness = self.evaluate_population(func)\n            evaluations += current_pop_size\n\n            if evaluations >= self.budget:\n                break\n\n            for i in range(current_pop_size):\n                mutant_vector = self.mutate(i)\n                trial_vector = self.crossover(self.population[i], mutant_vector)\n                trial_vector = np.clip(trial_vector, lb, ub)\n\n                trial_fitness = func(trial_vector)\n                evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    self.population[i] = trial_vector\n                    fitness[i] = trial_fitness\n\n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n                    self.best_solution = trial_vector\n\n        return self.best_solution, self.best_fitness", "name": "AdaptiveDifferentialEvolutionWithChaos", "description": "Adaptive Differential Evolution with Dynamic Population Sizing and Chaos Theory Integration for enhanced exploration and exploitation in photonic structure optimization.", "configspace": "", "generation": 66, "fitness": 0.27246591572265744, "feedback": "The algorithm AdaptiveDifferentialEvolutionWithChaos got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.27 with standard deviation 0.00.", "error": "", "parent_id": "8f855ef3-bb2e-48d4-969e-67f6a15bea7d", "metadata": {"aucs": [0.2733000199121094, 0.2725428809121573, 0.27155484634370564]}, "mutation_prompt": null}
{"id": "79e69625-ec81-4803-adc9-442f5b98b243", "solution": "import numpy as np\n\nclass EvolutionaryQuantumInspiredDE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = max(10, min(50, budget // 10))\n        self.population = None\n        self.fitness = np.full(self.population_size, float('inf'))\n        self.best_solution = None\n        self.best_fitness = float('inf')\n        self.mutation_factor = 0.8\n        self.crossover_rate = 0.7\n        self.interference_prob = 0.1\n\n    def initialize_population(self, lb, ub):\n        self.population = lb + (ub - lb) * np.random.rand(self.population_size, self.dim)\n\n    def evaluate_population(self, func):\n        self.fitness = np.array([func(ind) for ind in self.population])\n        for i, f in enumerate(self.fitness):\n            if f < self.best_fitness:\n                self.best_fitness = f\n                self.best_solution = self.population[i]\n        return self.fitness\n\n    def mutate_and_crossover(self, lb, ub):\n        new_population = np.copy(self.population)\n        for i in range(self.population_size):\n            indices = np.random.choice(self.population_size, 3, replace=False)\n            a, b, c = self.population[indices]\n            mutant = a + self.mutation_factor * (b - c)\n            mutant = np.clip(mutant, lb, ub)\n\n            crossover = np.random.rand(self.dim) < self.crossover_rate\n            if not np.any(crossover):\n                crossover[np.random.randint(0, self.dim)] = True\n\n            new_population[i] = np.where(crossover, mutant, self.population[i])\n\n        self.population = np.copy(new_population)\n\n    def apply_quantum_interference(self, lb, ub):\n        for i in range(self.population_size):\n            if np.random.rand() < self.interference_prob:\n                interference_vector = lb + (ub - lb) * np.random.rand(self.dim)\n                self.population[i] = np.mean([self.population[i], interference_vector], axis=0)\n                self.population[i] = np.clip(self.population[i], lb, ub)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            self.evaluate_population(func)\n            evaluations += self.population_size\n\n            if evaluations >= self.budget:\n                break\n\n            self.mutate_and_crossover(lb, ub)\n            self.apply_quantum_interference(lb, ub)\n\n        return self.best_solution, self.best_fitness", "name": "EvolutionaryQuantumInspiredDE", "description": "Evolutionary Quantum-Inspired Differential Evolution with Adaptive Neighborhood Interference for enhanced global exploration and local exploitation in photonic structure optimization.", "configspace": "", "generation": 67, "fitness": 0.25792330682283887, "feedback": "The algorithm EvolutionaryQuantumInspiredDE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.26 with standard deviation 0.00.", "error": "", "parent_id": "8f855ef3-bb2e-48d4-969e-67f6a15bea7d", "metadata": {"aucs": [0.2600631881554929, 0.25940571832963455, 0.2543010139833891]}, "mutation_prompt": null}
{"id": "02a8b391-7861-4e73-88dd-22c59c6043c5", "solution": "import numpy as np\n\nclass EnhancedQuantumInspiredParticleSwarmOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = max(10, min(50, budget // 10))\n        self.particles = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_fitness = np.full(self.num_particles, float('inf'))\n        self.global_best_position = None\n        self.global_best_fitness = float('inf')\n        self.cognitive_const = 2.0\n        self.social_const = 2.0\n        self.inertia_weight_min = 0.4\n        self.inertia_weight_max = 0.9\n        self.constriction_factor = 0.5\n        self.interference_prob = 0.1\n        self.dynamic_neighborhood_factor = 0.1\n\n    def initialize_particles(self, lb, ub):\n        self.particles = lb + (ub - lb) * np.random.rand(self.num_particles, self.dim)\n        self.velocities = np.random.randn(self.num_particles, self.dim) * 0.1\n        self.personal_best_positions = np.copy(self.particles)\n\n    def evaluate_particles(self, func):\n        fitness = np.array([func(p) for p in self.particles])\n        for i, f in enumerate(fitness):\n            if f < self.personal_best_fitness[i]:\n                self.personal_best_fitness[i] = f\n                self.personal_best_positions[i] = self.particles[i]\n            if f < self.global_best_fitness:\n                self.global_best_fitness = f\n                self.global_best_position = self.particles[i]\n        return fitness\n\n    def update_velocities_and_positions(self, lb, ub, evaluations):\n        for i in range(self.num_particles):\n            local_best = self.get_local_best(i)\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            inertia_weight = self.inertia_weight_min + (self.inertia_weight_max - self.inertia_weight_min) * (1 - evaluations / self.budget)\n            cognitive_term = self.cognitive_const * r1 * (self.personal_best_positions[i] - self.particles[i])\n            social_term = self.social_const * r2 * (local_best - self.particles[i])\n            self.velocities[i] = (inertia_weight * self.velocities[i] +\n                                  cognitive_term + social_term)\n            self.particles[i] += self.constriction_factor * self.velocities[i]\n        self.particles = np.clip(self.particles, lb, ub)\n\n    def get_local_best(self, index):\n        fitness_range = np.max(self.personal_best_fitness) - np.min(self.personal_best_fitness)\n        dynamic_neighborhood_size = max(2, int(self.dynamic_neighborhood_factor * self.num_particles * (1 - (self.personal_best_fitness[index] - np.min(self.personal_best_fitness)) / fitness_range)))\n        neighborhood_indices = np.random.choice(self.num_particles, dynamic_neighborhood_size, replace=False)\n        local_best_index = min(neighborhood_indices, key=lambda i: self.personal_best_fitness[i])\n        return self.personal_best_positions[local_best_index]\n\n    def apply_quantum_interference(self, lb, ub):\n        for i in range(self.num_particles):\n            if np.random.rand() < self.interference_prob:\n                interference_vector = lb + (ub - lb) * np.random.rand(self.dim)\n                self.particles[i] = np.mean([self.particles[i], interference_vector], axis=0)\n                self.particles[i] = np.clip(self.particles[i], lb, ub)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_particles(lb, ub)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            self.evaluate_particles(func)\n            evaluations += self.num_particles\n\n            if evaluations >= self.budget:\n                break\n\n            self.update_velocities_and_positions(lb, ub, evaluations)\n            self.apply_quantum_interference(lb, ub)\n\n        return self.global_best_position, self.global_best_fitness", "name": "EnhancedQuantumInspiredParticleSwarmOptimization", "description": "Enhanced Quantum-Inspired Particle Swarm Optimization with Stochastic Inertia Weight Adaptation and Fitness-Based Dynamic Neighborhood Adjustment for superior exploration and convergence in optimizing photonic structures.", "configspace": "", "generation": 68, "fitness": 0.27470219501217313, "feedback": "The algorithm EnhancedQuantumInspiredParticleSwarmOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.27 with standard deviation 0.00.", "error": "", "parent_id": "8f855ef3-bb2e-48d4-969e-67f6a15bea7d", "metadata": {"aucs": [0.274302017760208, 0.2742644974065803, 0.275540069869731]}, "mutation_prompt": null}
{"id": "228583ab-3c27-4f1d-9ca3-7785f378e6f6", "solution": "import numpy as np\n\nclass SelfAdaptiveQIDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_individuals = max(10, min(50, budget // 10))\n        self.population = None\n        self.fitness = None\n        self.best_position = None\n        self.best_fitness = float('inf')\n        self.mutation_factor = 0.5\n        self.crossover_rate = 0.9\n        self.quantum_prob = 0.1\n\n    def initialize_population(self, lb, ub):\n        self.population = lb + (ub - lb) * np.random.rand(self.num_individuals, self.dim)\n\n    def evaluate_population(self, func):\n        fitness = np.array([func(ind) for ind in self.population])\n        for i, f in enumerate(fitness):\n            if f < self.best_fitness:\n                self.best_fitness = f\n                self.best_position = self.population[i]\n        return fitness\n\n    def mutation(self, parent_idx):\n        indices = [idx for idx in range(self.num_individuals) if idx != parent_idx]\n        a, b, c = self.population[np.random.choice(indices, 3, replace=False)]\n        mutant_vector = a + self.mutation_factor * (b - c)\n        return np.clip(mutant_vector, 0, 1)\n\n    def crossover(self, target, mutant):\n        crossover_mask = np.random.rand(self.dim) < self.crossover_rate\n        offspring = np.where(crossover_mask, mutant, target)\n        return offspring\n\n    def quantum_inspired_mutation(self, lb, ub):\n        for i in range(self.num_individuals):\n            if np.random.rand() < self.quantum_prob:\n                quantum_vector = lb + (ub - lb) * np.random.rand(self.dim)\n                self.population[i] = np.mean([self.population[i], quantum_vector], axis=0)\n                self.population[i] = np.clip(self.population[i], lb, ub)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            self.fitness = self.evaluate_population(func)\n            evaluations += self.num_individuals\n\n            if evaluations >= self.budget:\n                break\n\n            new_population = np.copy(self.population)\n            for i in range(self.num_individuals):\n                mutant = self.mutation(i)\n                offspring = self.crossover(self.population[i], mutant)\n                offspring_fitness = func(offspring)\n                evaluations += 1\n\n                if offspring_fitness < self.fitness[i]:\n                    new_population[i] = offspring\n                    self.fitness[i] = offspring_fitness\n                    if offspring_fitness < self.best_fitness:\n                        self.best_fitness = offspring_fitness\n                        self.best_position = offspring\n\n            self.population = new_population\n            self.quantum_inspired_mutation(lb, ub)\n\n        return self.best_position, self.best_fitness", "name": "SelfAdaptiveQIDifferentialEvolution", "description": "Self-Adaptive Differential Evolution with Quantum-Inspired Mutation for enhanced balance between exploration and exploitation in photonic structure optimization.", "configspace": "", "generation": 69, "fitness": 0.2636027680023037, "feedback": "The algorithm SelfAdaptiveQIDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.26 with standard deviation 0.00.", "error": "", "parent_id": "8f855ef3-bb2e-48d4-969e-67f6a15bea7d", "metadata": {"aucs": [0.2628342084851272, 0.2662274327197206, 0.26174666280206327]}, "mutation_prompt": null}
{"id": "827957e3-341a-4acd-b7bb-5c589e3b7e7b", "solution": "import numpy as np\n\nclass QuantumHarmonicOscillatorOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = max(10, min(50, budget // 10))\n        self.particles = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_fitness = np.full(self.num_particles, float('inf'))\n        self.global_best_position = None\n        self.global_best_fitness = float('inf')\n        self.cognitive_const = 2.0\n        self.social_const = 2.0\n        self.inertia_weight = 0.9\n        self.wavefunction_collapse_prob = 0.1\n\n    def initialize_particles(self, lb, ub):\n        self.particles = lb + (ub - lb) * np.random.rand(self.num_particles, self.dim)\n        self.velocities = np.random.randn(self.num_particles, self.dim) * 0.1\n        self.personal_best_positions = np.copy(self.particles)\n\n    def evaluate_particles(self, func):\n        fitness = np.array([func(p) for p in self.particles])\n        for i, f in enumerate(fitness):\n            if f < self.personal_best_fitness[i]:\n                self.personal_best_fitness[i] = f\n                self.personal_best_positions[i] = self.particles[i]\n            if f < self.global_best_fitness:\n                self.global_best_fitness = f\n                self.global_best_position = self.particles[i]\n        return fitness\n\n    def update_velocities_and_positions(self, lb, ub):\n        for i in range(self.num_particles):\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            cognitive_term = self.cognitive_const * r1 * (self.personal_best_positions[i] - self.particles[i])\n            social_term = self.social_const * r2 * (self.global_best_position - self.particles[i])\n            self.velocities[i] = (self.inertia_weight * self.velocities[i] + cognitive_term + social_term)\n            self.particles[i] += self.velocities[i]\n        self.particles = np.clip(self.particles, lb, ub)\n\n    def wavefunction_collapse(self, lb, ub):\n        for i in range(self.num_particles):\n            if np.random.rand() < self.wavefunction_collapse_prob:\n                collapse_vector = lb + (ub - lb) * np.random.rand(self.dim)\n                self.particles[i] = np.mean([self.particles[i], collapse_vector], axis=0)\n                self.particles[i] = np.clip(self.particles[i], lb, ub)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_particles(lb, ub)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            self.evaluate_particles(func)\n            evaluations += self.num_particles\n\n            if evaluations >= self.budget:\n                break\n\n            self.update_velocities_and_positions(lb, ub)\n            self.wavefunction_collapse(lb, ub)\n\n        return self.global_best_position, self.global_best_fitness", "name": "QuantumHarmonicOscillatorOptimization", "description": "Quantum Harmonic Oscillator-Inspired Optimization with Dynamic Wavefunction Collapse for effective exploration-exploitation balance in photonic structure optimization.", "configspace": "", "generation": 70, "fitness": 0.2614148279308746, "feedback": "The algorithm QuantumHarmonicOscillatorOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.26 with standard deviation 0.00.", "error": "", "parent_id": "8f855ef3-bb2e-48d4-969e-67f6a15bea7d", "metadata": {"aucs": [0.265818495689908, 0.2635968593838972, 0.2548291287188186]}, "mutation_prompt": null}
{"id": "d3082945-a0da-45c0-9da4-8444188ec057", "solution": "import numpy as np\n\nclass QuantumInspiredDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_agents = max(10, min(50, budget // 10))\n        self.agents = None\n        self.personal_best_positions = None\n        self.personal_best_fitness = np.full(self.num_agents, float('inf'))\n        self.global_best_position = None\n        self.global_best_fitness = float('inf')\n        self.f_diff = 0.8\n        self.crossover_prob = 0.7\n        self.interference_prob = 0.1\n\n    def initialize_agents(self, lb, ub):\n        self.agents = lb + (ub - lb) * np.random.rand(self.num_agents, self.dim)\n        self.personal_best_positions = np.copy(self.agents)\n\n    def evaluate_agents(self, func):\n        fitness = np.array([func(agent) for agent in self.agents])\n        for i, f in enumerate(fitness):\n            if f < self.personal_best_fitness[i]:\n                self.personal_best_fitness[i] = f\n                self.personal_best_positions[i] = self.agents[i]\n            if f < self.global_best_fitness:\n                self.global_best_fitness = f\n                self.global_best_position = self.agents[i]\n        return fitness\n\n    def mutate_and_crossover(self, lb, ub):\n        new_agents = np.copy(self.agents)\n        for i in range(self.num_agents):\n            if np.random.rand() < self.interference_prob:\n                interference_vector = lb + (ub - lb) * np.random.rand(self.dim)\n                new_agents[i] = np.mean([self.agents[i], interference_vector], axis=0)\n            else:\n                indices = np.random.choice(list(range(i)) + list(range(i + 1, self.num_agents)), 3, replace=False)\n                a, b, c = self.agents[indices]\n                mutant_vector = a + self.f_diff * (b - c)\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.crossover_prob or j == j_rand:\n                        new_agents[i, j] = mutant_vector[j]\n            new_agents[i] = np.clip(new_agents[i], lb, ub)\n        self.agents = new_agents\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_agents(lb, ub)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            self.evaluate_agents(func)\n            evaluations += self.num_agents\n\n            if evaluations >= self.budget:\n                break\n\n            self.mutate_and_crossover(lb, ub)\n\n        return self.global_best_position, self.global_best_fitness", "name": "QuantumInspiredDifferentialEvolution", "description": "Quantum-Inspired Differential Evolution with Adaptive Interference and Localized Mutation for enhanced exploration and convergence in photonic structures optimization.", "configspace": "", "generation": 71, "fitness": 0.2611278330723892, "feedback": "The algorithm QuantumInspiredDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.26 with standard deviation 0.01.", "error": "", "parent_id": "8f855ef3-bb2e-48d4-969e-67f6a15bea7d", "metadata": {"aucs": [0.255169670300592, 0.2679421532770878, 0.2602716756394877]}, "mutation_prompt": null}
{"id": "c8d3762a-5bdc-449a-b4cb-5354f7906c0c", "solution": "import numpy as np\n\nclass EnhancedQuantumInspiredPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = max(10, min(50, budget // 10))\n        self.particles = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_fitness = np.full(self.num_particles, float('inf'))\n        self.global_best_position = None\n        self.global_best_fitness = float('inf')\n        self.cognitive_const = 2.0\n        self.social_const = 2.0\n        self.inertia_weight = 0.7\n        self.constriction_factor = 0.5\n        self.dynamic_neighborhood_size = 3\n        self.interference_prob = 0.1\n        self.adaptive_velocity_factor = 0.5\n\n    def initialize_particles(self, lb, ub):\n        self.particles = lb + (ub - lb) * np.random.rand(self.num_particles, self.dim)\n        self.velocities = np.random.randn(self.num_particles, self.dim) * 0.1\n        self.personal_best_positions = np.copy(self.particles)\n\n    def evaluate_particles(self, func):\n        fitness = np.array([func(p) for p in self.particles])\n        for i, f in enumerate(fitness):\n            if f < self.personal_best_fitness[i]:\n                self.personal_best_fitness[i] = f\n                self.personal_best_positions[i] = self.particles[i]\n            if f < self.global_best_fitness:\n                self.global_best_fitness = f\n                self.global_best_position = self.particles[i]\n        return fitness\n\n    def update_velocities_and_positions(self, lb, ub):\n        for i in range(self.num_particles):\n            local_best = self.get_local_best(i)\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            cognitive_term = self.cognitive_const * r1 * (self.personal_best_positions[i] - self.particles[i])\n            social_term = self.social_const * r2 * (local_best - self.particles[i])\n            self.velocities[i] = (self.inertia_weight * self.velocities[i] +\n                                  cognitive_term + social_term)\n            \n            # Adaptive velocity adjustment\n            velocity_norm = np.linalg.norm(self.velocities[i])\n            if velocity_norm > self.adaptive_velocity_factor:\n                self.velocities[i] *= self.adaptive_velocity_factor / velocity_norm\n            \n            self.particles[i] += self.constriction_factor * self.velocities[i]\n        self.particles = np.clip(self.particles, lb, ub)\n\n    def get_local_best(self, index):\n        neighborhood_indices = np.random.choice(self.num_particles, self.dynamic_neighborhood_size, replace=False)\n        local_best_index = min(neighborhood_indices, key=lambda i: self.personal_best_fitness[i])\n        return self.personal_best_positions[local_best_index]\n\n    def apply_quantum_interference(self, lb, ub):\n        for i in range(self.num_particles):\n            if np.random.rand() < self.interference_prob:\n                interference_vector = lb + (ub - lb) * np.random.rand(self.dim)\n                self.particles[i] = np.mean([self.particles[i], interference_vector], axis=0)\n                self.particles[i] = np.clip(self.particles[i], lb, ub)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_particles(lb, ub)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            self.evaluate_particles(func)\n            evaluations += self.num_particles\n\n            if evaluations >= self.budget:\n                break\n\n            self.update_velocities_and_positions(lb, ub)\n            self.apply_quantum_interference(lb, ub)\n\n        return self.global_best_position, self.global_best_fitness", "name": "EnhancedQuantumInspiredPSO", "description": "Enhanced Quantum-Inspired Particle Swarm Optimization with Adaptive Velocity Adjustment and Multi-modal Exploration for improved convergence in optimizing photonic structures.", "configspace": "", "generation": 72, "fitness": 0.2654560754011785, "feedback": "The algorithm EnhancedQuantumInspiredPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.27 with standard deviation 0.00.", "error": "", "parent_id": "8f855ef3-bb2e-48d4-969e-67f6a15bea7d", "metadata": {"aucs": [0.2669042035072623, 0.26325373082759085, 0.2662102918686823]}, "mutation_prompt": null}
{"id": "9197aabf-6710-4470-93de-9c449c56ad3f", "solution": "import numpy as np\n\nclass QuantumTunnelingAdaptiveCuckooSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_nests = max(10, min(50, budget // 10))\n        self.nests = None\n        self.best_nest = None\n        self.best_fitness = float('inf')\n        self.pa = 0.25  # discovery rate of alien eggs/solutions\n        self.sigma = 0.1\n        self.gamma = 0.9\n        self.alpha = 0.01\n\n    def levy_flight(self):\n        beta = 1.5\n        sigma_u = (np.math.gamma(1 + beta) * np.sin(np.pi * beta / 2) /\n                   (np.math.gamma((1 + beta) / 2) * beta * 2 ** ((beta - 1) / 2))) ** (1 / beta)\n        u = np.random.normal(0, sigma_u, size=self.dim)\n        v = np.random.normal(0, 1, size=self.dim)\n        step = u / abs(v) ** (1 / beta)\n        return step\n\n    def evaluate_nests(self, func):\n        fitness = np.array([func(n) for n in self.nests])\n        for i, f in enumerate(fitness):\n            if f < self.best_fitness:\n                self.best_fitness = f\n                self.best_nest = self.nests[i]\n        return fitness\n\n    def quantum_tunneling(self):\n        for i in range(self.num_nests):\n            if np.random.rand() < self.alpha:\n                step_size = np.random.randn(self.dim)\n                self.nests[i] += step_size * self.sigma * (self.best_nest - self.nests[i])\n                self.nests[i] = np.clip(self.nests[i], self.lb, self.ub)\n\n    def adaptive_cuckoo_search(self, func):\n        for _ in range(self.budget // self.num_nests):\n            for i in range(self.num_nests):\n                step = self.levy_flight()\n                new_cuckoo = self.nests[i] + self.gamma * step * (self.nests[i] - self.best_nest)\n                new_cuckoo = np.clip(new_cuckoo, self.lb, self.ub)\n                new_fitness = func(new_cuckoo)\n                if new_fitness < self.evaluate_nests(func)[i]:\n                    self.nests[i] = new_cuckoo\n            abandon_idxs = np.random.rand(self.num_nests) < self.pa\n            self.nests[abandon_idxs] = self.lb + (self.ub - self.lb) * np.random.rand(np.sum(abandon_idxs), self.dim)\n            self.quantum_tunneling()\n\n    def __call__(self, func):\n        self.lb, self.ub = func.bounds.lb, func.bounds.ub\n        self.nests = self.lb + (self.ub - self.lb) * np.random.rand(self.num_nests, self.dim)\n        self.evaluate_nests(func)\n\n        evaluations = 0\n        while evaluations < self.budget:\n            self.adaptive_cuckoo_search(func)\n            evaluations += self.num_nests\n\n        return self.best_nest, self.best_fitness", "name": "QuantumTunnelingAdaptiveCuckooSearch", "description": "Quantum-Tunneling Adaptive Cuckoo Search leveraging Lévy flights and environment-driven strategy adaptation for enhanced photonic structure optimization.", "configspace": "", "generation": 73, "fitness": 0.24083331760480356, "feedback": "The algorithm QuantumTunnelingAdaptiveCuckooSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.24 with standard deviation 0.00.", "error": "", "parent_id": "8f855ef3-bb2e-48d4-969e-67f6a15bea7d", "metadata": {"aucs": [0.23887542842958331, 0.23941487192992084, 0.24420965245490656]}, "mutation_prompt": null}
{"id": "31e98b47-09c9-4393-bcff-d0451a11d5ef", "solution": "import numpy as np\n\nclass EnhancedQuantumInspiredPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = max(10, min(50, budget // 10))\n        self.particles = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_fitness = np.full(self.num_particles, float('inf'))\n        self.global_best_position = None\n        self.global_best_fitness = float('inf')\n        self.cognitive_const = 2.0\n        self.social_const = 2.0\n        self.inertia_weight = 0.9\n        self.constriction_factor = 0.5\n        self.dynamic_neighborhood_size = 3\n        self.interference_prob = 0.1\n        self.max_velocity = None\n\n    def initialize_particles(self, lb, ub):\n        self.particles = lb + (ub - lb) * np.random.rand(self.num_particles, self.dim)\n        self.velocities = (ub - lb) * (np.random.rand(self.num_particles, self.dim) - 0.5)\n        self.personal_best_positions = np.copy(self.particles)\n        self.max_velocity = (ub - lb) * 0.1\n\n    def evaluate_particles(self, func):\n        fitness = np.array([func(p) for p in self.particles])\n        for i, f in enumerate(fitness):\n            if f < self.personal_best_fitness[i]:\n                self.personal_best_fitness[i] = f\n                self.personal_best_positions[i] = self.particles[i]\n            if f < self.global_best_fitness:\n                self.global_best_fitness = f\n                self.global_best_position = self.particles[i]\n        return fitness\n\n    def update_velocities_and_positions(self, lb, ub):\n        for i in range(self.num_particles):\n            local_best = self.get_local_best(i)\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            cognitive_term = self.cognitive_const * r1 * (self.personal_best_positions[i] - self.particles[i])\n            social_term = self.social_const * r2 * (local_best - self.particles[i])\n            self.velocities[i] = (self.inertia_weight * self.velocities[i] +\n                                  cognitive_term + social_term)\n            # Clip velocities\n            self.velocities[i] = np.clip(self.velocities[i], -self.max_velocity, self.max_velocity)\n            self.particles[i] += self.constriction_factor * self.velocities[i]\n        self.particles = np.clip(self.particles, lb, ub)\n\n    def get_local_best(self, index):\n        neighborhood_indices = np.random.choice(self.num_particles, self.dynamic_neighborhood_size, replace=False)\n        local_best_index = min(neighborhood_indices, key=lambda i: self.personal_best_fitness[i])\n        return self.personal_best_positions[local_best_index]\n\n    def apply_quantum_interference(self, lb, ub):\n        for i in range(self.num_particles):\n            if np.random.rand() < self.interference_prob:\n                interference_vector = lb + (ub - lb) * np.random.rand(self.dim)\n                self.particles[i] = np.mean([self.particles[i], interference_vector], axis=0)\n                self.particles[i] = np.clip(self.particles[i], lb, ub)\n\n    def quantum_tunneling(self, lb, ub):\n        for i in range(self.num_particles):\n            if self.personal_best_fitness[i] > self.global_best_fitness * 1.1:\n                tb = np.random.rand(self.dim) < 0.5  # Tunneling bits\n                self.particles[i][tb] = lb[tb] + (ub[tb] - lb[tb]) * np.random.rand(np.sum(tb))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_particles(lb, ub)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            self.evaluate_particles(func)\n            evaluations += self.num_particles\n\n            if evaluations >= self.budget:\n                break\n\n            self.update_velocities_and_positions(lb, ub)\n            self.apply_quantum_interference(lb, ub)\n            self.quantum_tunneling(lb, ub)\n\n        return self.global_best_position, self.global_best_fitness", "name": "EnhancedQuantumInspiredPSO", "description": "Enhanced Quantum-Inspired Particle Swarm Optimization with Dynamic Adaptive Velocity Adjustment and Quantum-Tunneling for improved global and local search balance in optimizing photonic structures.", "configspace": "", "generation": 74, "fitness": 0.2746427706960178, "feedback": "The algorithm EnhancedQuantumInspiredPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.27 with standard deviation 0.00.", "error": "", "parent_id": "8f855ef3-bb2e-48d4-969e-67f6a15bea7d", "metadata": {"aucs": [0.2754028331255046, 0.2741537958541671, 0.27437168310838156]}, "mutation_prompt": null}
{"id": "26116678-85b1-4843-ac69-0aca3c460264", "solution": "import numpy as np\n\nclass EnhancedQuantumInspiredPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = max(10, min(50, budget // 10))\n        self.particles = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_fitness = np.full(self.num_particles, float('inf'))\n        self.global_best_position = None\n        self.global_best_fitness = float('inf')\n        self.cognitive_const = 2.0\n        self.social_const = 2.0\n        self.inertia_weight = 0.7\n        self.constriction_factor = 0.5\n        self.mutation_rate = 0.1\n        self.adaptive_prob = 0.2\n\n    def initialize_particles(self, lb, ub):\n        self.particles = lb + (ub - lb) * np.random.rand(self.num_particles, self.dim)\n        self.velocities = np.random.randn(self.num_particles, self.dim) * 0.1\n        self.personal_best_positions = np.copy(self.particles)\n\n    def evaluate_particles(self, func):\n        fitness = np.array([func(p) for p in self.particles])\n        for i, f in enumerate(fitness):\n            if f < self.personal_best_fitness[i]:\n                self.personal_best_fitness[i] = f\n                self.personal_best_positions[i] = self.particles[i]\n            if f < self.global_best_fitness:\n                self.global_best_fitness = f\n                self.global_best_position = self.particles[i]\n        return fitness\n\n    def update_velocities_and_positions(self, lb, ub):\n        for i in range(self.num_particles):\n            local_best = self.get_fitness_based_local_best(i)\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            cognitive_term = self.cognitive_const * r1 * (self.personal_best_positions[i] - self.particles[i])\n            social_term = self.social_const * r2 * (local_best - self.particles[i])\n            self.velocities[i] = (self.inertia_weight * self.velocities[i] +\n                                  cognitive_term + social_term)\n            self.particles[i] += self.constriction_factor * self.velocities[i]\n        self.particles = np.clip(self.particles, lb, ub)\n\n    def get_fitness_based_local_best(self, index):\n        sorted_indices = np.argsort(self.personal_best_fitness)\n        neighborhood_size = max(2, min(self.dynamic_neighborhood_size(index), len(sorted_indices)))\n        neighborhood_indices = sorted_indices[:neighborhood_size]\n        local_best_index = min(neighborhood_indices, key=lambda i: self.personal_best_fitness[i])\n        return self.personal_best_positions[local_best_index]\n\n    def dynamic_neighborhood_size(self, index):\n        # Adjust neighborhood size based on fitness rank\n        rank = np.argsort(self.personal_best_fitness).tolist().index(index)\n        return max(3, self.num_particles // (rank + 1))\n\n    def apply_quantum_mutation(self, lb, ub):\n        for i in range(self.num_particles):\n            if np.random.rand() < self.adaptive_prob:\n                mutation_vector = lb + (ub - lb) * np.random.rand(self.dim)\n                self.particles[i] = np.mean([self.particles[i], mutation_vector], axis=0) * self.mutation_rate\n                self.particles[i] = np.clip(self.particles[i], lb, ub)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_particles(lb, ub)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            self.evaluate_particles(func)\n            evaluations += self.num_particles\n\n            if evaluations >= self.budget:\n                break\n\n            self.update_velocities_and_positions(lb, ub)\n            self.apply_quantum_mutation(lb, ub)\n\n        return self.global_best_position, self.global_best_fitness", "name": "EnhancedQuantumInspiredPSO", "description": "Enhanced Quantum-Inspired Particle Swarm Optimization with Adaptive Mutation and Fitness-Based Dynamic Neighborhoods for improved diversity and convergence in photonic structure optimization.", "configspace": "", "generation": 75, "fitness": 0.27591016195685675, "feedback": "The algorithm EnhancedQuantumInspiredPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28 with standard deviation 0.00.", "error": "", "parent_id": "8f855ef3-bb2e-48d4-969e-67f6a15bea7d", "metadata": {"aucs": [0.27615782139080136, 0.2751532735258585, 0.2764193909539103]}, "mutation_prompt": null}
{"id": "22450d2d-359e-42b5-9463-3c6ec7b811de", "solution": "import numpy as np\n\nclass QuantumDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = max(10, min(50, budget // 10))\n        self.population = None\n        self.fitness = None\n        self.global_best_position = None\n        self.global_best_fitness = float('inf')\n        self.scale_factor = 0.5\n        self.crossover_prob = 0.7\n        self.adaptive_rate = 0.1\n\n    def initialize_population(self, lb, ub):\n        self.population = lb + (ub - lb) * np.random.rand(self.population_size, self.dim)\n        self.fitness = np.full(self.population_size, float('inf'))\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            fitness_value = func(self.population[i])\n            if fitness_value < self.fitness[i]:\n                self.fitness[i] = fitness_value\n                if fitness_value < self.global_best_fitness:\n                    self.global_best_fitness = fitness_value\n                    self.global_best_position = np.copy(self.population[i])\n\n    def mutate_and_crossover(self, lb, ub):\n        new_population = np.copy(self.population)\n        for i in range(self.population_size):\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(indices, 3, replace=False)]\n            mutant_vector = a + self.scale_factor * (b - c)\n            trial_vector = np.copy(self.population[i])\n            \n            jrand = np.random.randint(self.dim)\n            for j in range(self.dim):\n                if np.random.rand() < self.crossover_prob or j == jrand:\n                    trial_vector[j] = mutant_vector[j]\n            \n            trial_fitness = func(trial_vector)\n            if trial_fitness < self.fitness[i]:\n                new_population[i] = trial_vector\n                self.fitness[i] = trial_fitness\n                if trial_fitness < self.global_best_fitness:\n                    self.global_best_fitness = trial_fitness\n                    self.global_best_position = np.copy(trial_vector)\n        \n        self.population = np.clip(new_population, lb, ub)\n\n    def apply_quantum_swarming(self, lb, ub):\n        for i in range(self.population_size):\n            if np.random.rand() < self.adaptive_rate:\n                random_vector = lb + (ub - lb) * np.random.rand(self.dim)\n                self.population[i] = np.mean([self.population[i], random_vector], axis=0)\n                self.population[i] = np.clip(self.population[i], lb, ub)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        evaluations = 0\n        \n        while evaluations < self.budget:\n            self.evaluate_population(func)\n            evaluations += self.population_size\n            \n            if evaluations >= self.budget:\n                break\n\n            self.mutate_and_crossover(lb, ub)\n            self.apply_quantum_swarming(lb, ub)\n\n        return self.global_best_position, self.global_best_fitness", "name": "QuantumDifferentialEvolution", "description": "Quantum Differential Evolution with Adaptive Perturbation and Swarming Mutation for efficient exploration and convergence in photonic structure optimization.", "configspace": "", "generation": 76, "fitness": -Infinity, "feedback": "An exception occurred: NameError(\"name 'func' is not defined\").", "error": "NameError(\"name 'func' is not defined\")", "parent_id": "8f855ef3-bb2e-48d4-969e-67f6a15bea7d", "metadata": {}, "mutation_prompt": null}
{"id": "88a612e4-e31a-4a98-a359-c04261e37e33", "solution": "import numpy as np\n\nclass EnhancedQuantumInspiredPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = max(10, min(50, budget // 10))\n        self.particles = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_fitness = np.full(self.num_particles, float('inf'))\n        self.global_best_position = None\n        self.global_best_fitness = float('inf')\n        self.cognitive_const = 2.0\n        self.social_const = 2.0\n        self.inertia_weight = 0.7\n        self.constriction_factor = 0.5\n        self.dynamic_neighborhood_size = 3\n        self.interference_prob = 0.1\n        self.mutation_prob = 0.2\n        self.local_search_intensity = 5\n\n    def initialize_particles(self, lb, ub):\n        self.particles = lb + (ub - lb) * np.random.rand(self.num_particles, self.dim)\n        self.velocities = np.random.randn(self.num_particles, self.dim) * 0.1\n        self.personal_best_positions = np.copy(self.particles)\n\n    def evaluate_particles(self, func):\n        fitness = np.array([func(p) for p in self.particles])\n        for i, f in enumerate(fitness):\n            if f < self.personal_best_fitness[i]:\n                self.personal_best_fitness[i] = f\n                self.personal_best_positions[i] = self.particles[i]\n            if f < self.global_best_fitness:\n                self.global_best_fitness = f\n                self.global_best_position = self.particles[i]\n        return fitness\n\n    def update_velocities_and_positions(self, lb, ub):\n        for i in range(self.num_particles):\n            local_best = self.get_local_best(i)\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            cognitive_term = self.cognitive_const * r1 * (self.personal_best_positions[i] - self.particles[i])\n            social_term = self.social_const * r2 * (local_best - self.particles[i])\n            self.velocities[i] = (self.inertia_weight * self.velocities[i] +\n                                  cognitive_term + social_term)\n            self.particles[i] += self.constriction_factor * self.velocities[i]\n        self.particles = np.clip(self.particles, lb, ub)\n\n    def get_local_best(self, index):\n        neighborhood_indices = np.random.choice(self.num_particles, self.dynamic_neighborhood_size, replace=False)\n        local_best_index = min(neighborhood_indices, key=lambda i: self.personal_best_fitness[i])\n        return self.personal_best_positions[local_best_index]\n\n    def apply_quantum_interference(self, lb, ub):\n        for i in range(self.num_particles):\n            if np.random.rand() < self.interference_prob:\n                interference_vector = lb + (ub - lb) * np.random.rand(self.dim)\n                self.particles[i] = np.mean([self.particles[i], interference_vector], axis=0)\n                self.particles[i] = np.clip(self.particles[i], lb, ub)\n\n    def apply_mutation(self, lb, ub):\n        for i in range(self.num_particles):\n            if np.random.rand() < self.mutation_prob:\n                mutation_vector = lb + (ub - lb) * np.random.rand(self.dim)\n                self.particles[i] = np.mean([self.particles[i], mutation_vector], axis=0)\n                self.particles[i] = np.clip(self.particles[i], lb, ub)\n\n    def enhanced_local_search(self, func, lb, ub):\n        for i in range(self.local_search_intensity):\n            trial_positions = self.particles + np.random.randn(self.num_particles, self.dim) * 0.1\n            trial_positions = np.clip(trial_positions, lb, ub)\n            trial_fitness = np.array([func(p) for p in trial_positions])\n            for j, f in enumerate(trial_fitness):\n                if f < self.personal_best_fitness[j]:\n                    self.personal_best_fitness[j] = f\n                    self.personal_best_positions[j] = trial_positions[j]\n                if f < self.global_best_fitness:\n                    self.global_best_fitness = f\n                    self.global_best_position = trial_positions[j]\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_particles(lb, ub)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            self.evaluate_particles(func)\n            evaluations += self.num_particles\n\n            if evaluations >= self.budget:\n                break\n\n            self.update_velocities_and_positions(lb, ub)\n            self.apply_quantum_interference(lb, ub)\n            self.apply_mutation(lb, ub)\n\n            if evaluations + self.local_search_intensity * self.num_particles <= self.budget:\n                self.enhanced_local_search(func, lb, ub)\n                evaluations += self.local_search_intensity * self.num_particles\n\n        return self.global_best_position, self.global_best_fitness", "name": "EnhancedQuantumInspiredPSO", "description": "Enhanced Quantum-Inspired Particle Swarm Optimization with Adaptive Mutation and Enhanced Local Search for improved convergence and robustness in optimizing photonic structures.", "configspace": "", "generation": 77, "fitness": 0.26964664791454057, "feedback": "The algorithm EnhancedQuantumInspiredPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.27 with standard deviation 0.00.", "error": "", "parent_id": "8f855ef3-bb2e-48d4-969e-67f6a15bea7d", "metadata": {"aucs": [0.27061979627234767, 0.2676437294553693, 0.2706764180159048]}, "mutation_prompt": null}
{"id": "f36faaf0-1cce-4b51-9fae-85ed08e3d907", "solution": "import numpy as np\n\nclass BioInspiredMemeticSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = max(10, min(50, budget // 15))\n        self.population = None\n        self.fitness = None\n        self.best_solution = None\n        self.best_fitness = float('inf')\n        self.mutation_rate = 0.1\n        self.crossover_rate = 0.6\n        self.local_search_prob = 0.2\n\n    def initialize_population(self, lb, ub):\n        self.population = lb + (ub - lb) * np.random.rand(self.population_size, self.dim)\n        self.fitness = np.full(self.population_size, float('inf'))\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            self.fitness[i] = func(self.population[i])\n            if self.fitness[i] < self.best_fitness:\n                self.best_fitness = self.fitness[i]\n                self.best_solution = self.population[i]\n\n    def select_parents(self):\n        probabilities = 1 / (1 + self.fitness)\n        probabilities /= probabilities.sum()\n        parents_indices = np.random.choice(self.population_size, size=2, p=probabilities)\n        return self.population[parents_indices[0]], self.population[parents_indices[1]]\n\n    def crossover(self, parent1, parent2):\n        if np.random.rand() < self.crossover_rate:\n            crossover_point = np.random.randint(1, self.dim)\n            child1 = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n            child2 = np.concatenate([parent2[:crossover_point], parent1[crossover_point:]])\n            return child1, child2\n        return parent1, parent2\n\n    def mutate(self, individual, lb, ub):\n        for i in range(self.dim):\n            if np.random.rand() < self.mutation_rate:\n                individual[i] = lb[i] + (ub[i] - lb[i]) * np.random.rand()\n        return np.clip(individual, lb, ub)\n\n    def local_search(self, individual, lb, ub, func):\n        local_budget = max(1, self.budget // (10 * self.population_size))\n        best_local = individual.copy()\n        best_local_fitness = func(individual)\n        for _ in range(local_budget):\n            perturbation = np.random.normal(0, 0.1, self.dim)\n            candidate = best_local + perturbation\n            candidate = np.clip(candidate, lb, ub)\n            candidate_fitness = func(candidate)\n            if candidate_fitness < best_local_fitness:\n                best_local = candidate\n                best_local_fitness = candidate_fitness\n        return best_local\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        evaluations = 0\n\n        while evaluations + self.population_size <= self.budget:\n            self.evaluate_population(func)\n            evaluations += self.population_size\n\n            new_population = []\n            for _ in range(self.population_size // 2):\n                parent1, parent2 = self.select_parents()\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.mutate(child1, lb, ub)\n                child2 = self.mutate(child2, lb, ub)\n                new_population.extend([child1, child2])\n\n            if len(new_population) < self.population_size:\n                new_population.append(self.mutate(self.select_parents()[0], lb, ub))\n\n            self.population = np.array(new_population[:self.population_size])\n\n            for i in range(self.population_size):\n                if np.random.rand() < self.local_search_prob:\n                    self.population[i] = self.local_search(self.population[i], lb, ub, func)\n\n        return self.best_solution, self.best_fitness", "name": "BioInspiredMemeticSearch", "description": "Bio-inspired Memetic Search using Adaptive Genetic Operators and Advanced Local Search for improved convergence in optimizing photonic structures.", "configspace": "", "generation": 78, "fitness": 0.24940512967348996, "feedback": "The algorithm BioInspiredMemeticSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.25 with standard deviation 0.00.", "error": "", "parent_id": "8f855ef3-bb2e-48d4-969e-67f6a15bea7d", "metadata": {"aucs": [0.24800618641761907, 0.2553729081037778, 0.24483629449907307]}, "mutation_prompt": null}
{"id": "04365400-83a1-4c50-bd79-23af4e38dfbb", "solution": "import numpy as np\n\nclass AdaptiveQuantumDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = max(20, min(100, budget // 5))\n        self.population = None\n        self.fitness = None\n        self.best_solution = None\n        self.best_fitness = float('inf')\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.jump_prob = 0.1  # Probability to apply quantum jump\n\n    def initialize_population(self, lb, ub):\n        self.population = lb + (ub - lb) * np.random.rand(self.population_size, self.dim)\n        self.fitness = np.full(self.population_size, float('inf'))\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            fit = func(self.population[i])\n            if fit < self.fitness[i]:\n                self.fitness[i] = fit\n                if fit < self.best_fitness:\n                    self.best_fitness = fit\n                    self.best_solution = self.population[i]\n\n    def mutate(self, idx):\n        indices = [i for i in range(self.population_size) if i != idx]\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        mutated = self.population[a] + self.F * (self.population[b] - self.population[c])\n        return mutated\n\n    def crossover(self, target, mutant):\n        crossover_mask = np.random.rand(self.dim) < self.CR\n        trial = np.where(crossover_mask, mutant, target)\n        return trial\n\n    def adapt_parameters(self):\n        self.F = np.random.uniform(0.5, 1.0)\n        self.CR = np.random.uniform(0.1, 0.9)\n\n    def quantum_jump(self, lb, ub):\n        for i in range(self.population_size):\n            if np.random.rand() < self.jump_prob:\n                jump_vector = lb + (ub - lb) * np.random.rand(self.dim)\n                self.population[i] = np.mean([self.population[i], jump_vector], axis=0)\n                self.population[i] = np.clip(self.population[i], lb, ub)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            self.evaluate_population(func)\n            evaluations += self.population_size\n\n            if evaluations >= self.budget:\n                break\n\n            for i in range(self.population_size):\n                mutant = self.mutate(i)\n                trial = self.crossover(self.population[i], mutant)\n                trial = np.clip(trial, lb, ub)\n                \n                trial_fitness = func(trial)\n                if trial_fitness < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = trial_fitness\n\n            self.adapt_parameters()\n            self.quantum_jump(lb, ub)\n\n        return self.best_solution, self.best_fitness", "name": "AdaptiveQuantumDifferentialEvolution", "description": "Adaptive Differential Evolution with Quantum-Inspired Jump Mechanism for enhanced exploration and convergence in photonic structure optimization.", "configspace": "", "generation": 79, "fitness": 0.2674763959016745, "feedback": "The algorithm AdaptiveQuantumDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.27 with standard deviation 0.00.", "error": "", "parent_id": "8f855ef3-bb2e-48d4-969e-67f6a15bea7d", "metadata": {"aucs": [0.2641073290909378, 0.268510508756277, 0.26981134985780864]}, "mutation_prompt": null}
{"id": "b41d233d-0963-4f4b-90e8-760db963eb45", "solution": "import numpy as np\n\nclass ChaoticQuantumInspiredDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_agents = max(10, min(50, budget // 10))\n        self.population = None\n        self.mutation_factor = 0.5\n        self.recombination_prob = 0.7\n        self.chaos_sequence = self._generate_chaos_sequence(self.budget)\n        self.interference_prob = 0.1\n\n    def _generate_chaos_sequence(self, length):\n        seq = np.zeros(length)\n        x = 0.7  # initial condition for the logistic map\n        r = 3.7  # chaotic parameter\n        for i in range(length):\n            x = r * x * (1 - x)\n            seq[i] = x\n        return seq\n\n    def initialize_population(self, lb, ub):\n        self.population = lb + (ub - lb) * np.random.rand(self.num_agents, self.dim)\n\n    def evaluate_population(self, func):\n        fitness = np.array([func(agent) for agent in self.population])\n        return fitness\n\n    def mutate_and_crossover(self, lb, ub, index, fitness):\n        indices = [i for i in range(self.num_agents) if i != index]\n        a, b, c = self.population[np.random.choice(indices, 3, replace=False)]\n        mutant_vector = a + self.mutation_factor * (b - c)\n        \n        chaos_factor = self.chaos_sequence[index % len(self.chaos_sequence)]\n        mutant_vector = chaos_factor * mutant_vector + (1 - chaos_factor) * self.population[index]\n        \n        trial_vector = np.copy(self.population[index])\n        for j in range(self.dim):\n            if np.random.rand() < self.recombination_prob:\n                trial_vector[j] = mutant_vector[j]\n        \n        trial_vector = np.clip(trial_vector, lb, ub)\n        trial_fitness = func(trial_vector)\n        if trial_fitness < fitness[index]:\n            self.population[index] = trial_vector\n            fitness[index] = trial_fitness\n\n    def apply_quantum_interference(self, lb, ub):\n        for i in range(self.num_agents):\n            if np.random.rand() < self.interference_prob:\n                interference_vector = lb + (ub - lb) * np.random.rand(self.dim)\n                self.population[i] = np.mean([self.population[i], interference_vector], axis=0)\n                self.population[i] = np.clip(self.population[i], lb, ub)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        fitness = self.evaluate_population(func)\n        evaluations = self.num_agents\n\n        best_idx = np.argmin(fitness)\n        global_best_position = self.population[best_idx]\n        global_best_fitness = fitness[best_idx]\n\n        while evaluations < self.budget:\n            for i in range(self.num_agents):\n                self.mutate_and_crossover(lb, ub, i, fitness)\n            self.apply_quantum_interference(lb, ub)\n\n            current_best_idx = np.argmin(fitness)\n            current_best_fitness = fitness[current_best_idx]\n\n            if current_best_fitness < global_best_fitness:\n                global_best_fitness = current_best_fitness\n                global_best_position = self.population[current_best_idx]\n\n            evaluations += self.num_agents\n\n        return global_best_position, global_best_fitness", "name": "ChaoticQuantumInspiredDifferentialEvolution", "description": "Chaotic Quantum-Inspired Differential Evolution combines chaotic maps with quantum-inspired perturbations to enhance exploration and convergence for optimizing complex photonic structures.", "configspace": "", "generation": 80, "fitness": -Infinity, "feedback": "An exception occurred: NameError(\"name 'func' is not defined\").", "error": "NameError(\"name 'func' is not defined\")", "parent_id": "8f855ef3-bb2e-48d4-969e-67f6a15bea7d", "metadata": {}, "mutation_prompt": null}
{"id": "f74ac618-eeac-4b34-819f-55a1c3d64575", "solution": "import numpy as np\n\nclass EnhancedQuantumInspiredPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = max(10, min(50, budget // 10))\n        self.particles = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_fitness = np.full(self.num_particles, float('inf'))\n        self.global_best_position = None\n        self.global_best_fitness = float('inf')\n        self.cognitive_const = 2.0\n        self.social_const = 2.0\n        self.base_inertia_weight = 0.9\n        self.min_inertia_weight = 0.4\n        self.constriction_factor = 0.5\n        self.dynamic_neighborhood_size = 3\n        self.interference_prob = 0.1\n        self.de_mutation_factor = 0.8\n        self.de_crossover_prob = 0.9\n\n    def initialize_particles(self, lb, ub):\n        self.particles = lb + (ub - lb) * np.random.rand(self.num_particles, self.dim)\n        self.velocities = np.random.randn(self.num_particles, self.dim) * 0.1\n        self.personal_best_positions = np.copy(self.particles)\n\n    def evaluate_particles(self, func):\n        fitness = np.array([func(p) for p in self.particles])\n        for i, f in enumerate(fitness):\n            if f < self.personal_best_fitness[i]:\n                self.personal_best_fitness[i] = f\n                self.personal_best_positions[i] = self.particles[i]\n            if f < self.global_best_fitness:\n                self.global_best_fitness = f\n                self.global_best_position = self.particles[i]\n        return fitness\n\n    def update_velocities_and_positions(self, lb, ub, evaluations):\n        inertia_weight = self.min_inertia_weight + (self.base_inertia_weight - self.min_inertia_weight) * \\\n                         (1 - (evaluations / self.budget))\n        for i in range(self.num_particles):\n            local_best = self.get_local_best(i)\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            cognitive_term = self.cognitive_const * r1 * (self.personal_best_positions[i] - self.particles[i])\n            social_term = self.social_const * r2 * (local_best - self.particles[i])\n            self.velocities[i] = (inertia_weight * self.velocities[i] +\n                                  cognitive_term + social_term)\n            self.particles[i] += self.constriction_factor * self.velocities[i]\n        self.particles = np.clip(self.particles, lb, ub)\n\n    def get_local_best(self, index):\n        neighborhood_indices = np.random.choice(self.num_particles, self.dynamic_neighborhood_size, replace=False)\n        local_best_index = min(neighborhood_indices, key=lambda i: self.personal_best_fitness[i])\n        return self.personal_best_positions[local_best_index]\n\n    def apply_quantum_interference(self, lb, ub):\n        for i in range(self.num_particles):\n            if np.random.rand() < self.interference_prob:\n                interference_vector = lb + (ub - lb) * np.random.rand(self.dim)\n                self.particles[i] = np.mean([self.particles[i], interference_vector], axis=0)\n                self.particles[i] = np.clip(self.particles[i], lb, ub)\n\n    def apply_de_mutation(self, lb, ub):\n        for i in range(self.num_particles):\n            if np.random.rand() < self.de_crossover_prob:\n                candidates = np.random.choice(self.num_particles, 3, replace=False)\n                a, b, c = candidates\n                mutant_vector = self.particles[a] + self.de_mutation_factor * (self.particles[b] - self.particles[c])\n                trial_vector = np.where(np.random.rand(self.dim) < self.de_crossover_prob, mutant_vector, self.particles[i])\n                trial_vector = np.clip(trial_vector, lb, ub)\n                trial_fitness = func(trial_vector)\n                if trial_fitness < self.personal_best_fitness[i]:\n                    self.personal_best_fitness[i] = trial_fitness\n                    self.personal_best_positions[i] = trial_vector\n                    if trial_fitness < self.global_best_fitness:\n                        self.global_best_fitness = trial_fitness\n                        self.global_best_position = trial_vector\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_particles(lb, ub)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            self.evaluate_particles(func)\n            evaluations += self.num_particles\n\n            if evaluations >= self.budget:\n                break\n\n            self.update_velocities_and_positions(lb, ub, evaluations)\n            self.apply_quantum_interference(lb, ub)\n            self.apply_de_mutation(lb, ub)\n\n        return self.global_best_position, self.global_best_fitness", "name": "EnhancedQuantumInspiredPSO", "description": "Enhanced Quantum-Inspired Particle Swarm Optimization integrating Dynamic Adaptive Inertia and Differential Evolution-based Mutation for improved diversity and convergence in photonic structure optimization.", "configspace": "", "generation": 81, "fitness": -Infinity, "feedback": "An exception occurred: NameError(\"name 'func' is not defined\").", "error": "NameError(\"name 'func' is not defined\")", "parent_id": "8f855ef3-bb2e-48d4-969e-67f6a15bea7d", "metadata": {}, "mutation_prompt": null}
{"id": "f411cc38-685a-47d6-8f32-e00da36a531d", "solution": "import numpy as np\n\nclass HybridQuantumInspiredPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = max(10, min(50, budget // 10))\n        self.particles = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_fitness = np.full(self.num_particles, float('inf'))\n        self.global_best_position = None\n        self.global_best_fitness = float('inf')\n        self.cognitive_const = 2.0\n        self.social_const = 2.0\n        self.inertia_weight = 0.7\n        self.constriction_factor = 0.5\n        self.dynamic_neighborhood_size = 3\n        self.interference_prob = 0.1\n        self.levy_prob = 0.2  # Probability of applying Lévy flight\n\n    def initialize_particles(self, lb, ub):\n        self.particles = lb + (ub - lb) * np.random.rand(self.num_particles, self.dim)\n        self.velocities = np.random.randn(self.num_particles, self.dim) * 0.1\n        self.personal_best_positions = np.copy(self.particles)\n\n    def evaluate_particles(self, func):\n        fitness = np.array([func(p) for p in self.particles])\n        for i, f in enumerate(fitness):\n            if f < self.personal_best_fitness[i]:\n                self.personal_best_fitness[i] = f\n                self.personal_best_positions[i] = self.particles[i]\n            if f < self.global_best_fitness:\n                self.global_best_fitness = f\n                self.global_best_position = self.particles[i]\n        return fitness\n\n    def update_velocities_and_positions(self, lb, ub):\n        for i in range(self.num_particles):\n            local_best = self.get_local_best(i)\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            cognitive_term = self.cognitive_const * r1 * (self.personal_best_positions[i] - self.particles[i])\n            social_term = self.social_const * r2 * (local_best - self.particles[i])\n            self.velocities[i] = (self.inertia_weight * self.velocities[i] +\n                                  cognitive_term + social_term)\n            self.particles[i] += self.constriction_factor * self.velocities[i]\n            if np.random.rand() < self.levy_prob:\n                self.levy_flight(i, lb, ub)\n        self.particles = np.clip(self.particles, lb, ub)\n\n    def get_local_best(self, index):\n        neighborhood_indices = np.random.choice(self.num_particles, self.dynamic_neighborhood_size, replace=False)\n        local_best_index = min(neighborhood_indices, key=lambda i: self.personal_best_fitness[i])\n        return self.personal_best_positions[local_best_index]\n\n    def levy_flight(self, i, lb, ub):\n        beta = 1.5\n        sigma = (np.math.gamma(1 + beta) * np.sin(np.pi * beta / 2) /\n                 (np.math.gamma((1 + beta) / 2) * beta * 2 ** ((beta - 1) / 2))) ** (1 / beta)\n        u = np.random.randn(self.dim) * sigma\n        v = np.random.randn(self.dim)\n        step = u / np.abs(v) ** (1 / beta)\n        self.particles[i] += 0.01 * step * (self.particles[i] - self.global_best_position)\n        self.particles[i] = np.clip(self.particles[i], lb, ub)\n\n    def apply_quantum_interference(self, lb, ub):\n        for i in range(self.num_particles):\n            if np.random.rand() < self.interference_prob:\n                interference_vector = lb + (ub - lb) * np.random.rand(self.dim)\n                self.particles[i] = np.mean([self.particles[i], interference_vector], axis=0)\n                self.particles[i] = np.clip(self.particles[i], lb, ub)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_particles(lb, ub)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            self.evaluate_particles(func)\n            evaluations += self.num_particles\n\n            if evaluations >= self.budget:\n                break\n\n            self.update_velocities_and_positions(lb, ub)\n            self.apply_quantum_interference(lb, ub)\n\n        return self.global_best_position, self.global_best_fitness", "name": "HybridQuantumInspiredPSO", "description": "Hybrid Quantum-Inspired Particle Swarm Optimization with Lévy Flight and Adaptive Dynamic Neighborhoods for enhanced exploration and exploitation in photonic structure optimization.", "configspace": "", "generation": 82, "fitness": 0.27722655647498756, "feedback": "The algorithm HybridQuantumInspiredPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28 with standard deviation 0.00.", "error": "", "parent_id": "8f855ef3-bb2e-48d4-969e-67f6a15bea7d", "metadata": {"aucs": [0.27729019427463386, 0.277251868987051, 0.27713760616327776]}, "mutation_prompt": null}
{"id": "384577c2-3974-4ff0-bffc-15e42ef8d452", "solution": "import numpy as np\n\nclass AdaptiveMemeticAlgorithm:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = max(10, min(50, budget // 10))\n        self.population = None\n        self.fitness = None\n        self.best_solution = None\n        self.best_fitness = float('inf')\n        self.local_search_prob = 0.3\n        self.global_perturbation_prob = 0.1\n        self.mutation_scale = 0.1\n\n    def initialize_population(self, lb, ub):\n        self.population = lb + (ub - lb) * np.random.rand(self.population_size, self.dim)\n        self.fitness = np.full(self.population_size, float('inf'))\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            f = func(self.population[i])\n            if f < self.fitness[i]:\n                self.fitness[i] = f\n            if f < self.best_fitness:\n                self.best_fitness = f\n                self.best_solution = self.population[i]\n\n    def local_search(self, candidate, lb, ub):\n        step_size = 0.1 * (ub - lb)\n        perturbation = np.random.normal(0, step_size, self.dim)\n        new_candidate = np.clip(candidate + perturbation, lb, ub)\n        return new_candidate\n\n    def global_perturbation(self, candidate, lb, ub):\n        perturbation = np.random.normal(0, self.mutation_scale, self.dim)\n        new_candidate = np.clip(candidate + perturbation, lb, ub)\n        return new_candidate\n\n    def reproduce(self, parents, lb, ub):\n        offspring = np.zeros_like(parents)\n        for i in range(self.population_size):\n            if np.random.rand() < self.local_search_prob:\n                offspring[i] = self.local_search(parents[i], lb, ub)\n            else:\n                offspring[i] = self.global_perturbation(parents[i], lb, ub)\n        return offspring\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            self.evaluate_population(func)\n            evaluations += self.population_size\n\n            if evaluations >= self.budget:\n                break\n\n            parents = self.population[np.argsort(self.fitness)[:self.population_size // 2]]\n            offspring = self.reproduce(parents, lb, ub)\n            self.population = np.vstack((parents, offspring))\n            self.fitness = np.hstack((self.fitness[np.argsort(self.fitness)[:self.population_size // 2]], np.full(self.population_size // 2, float('inf'))))\n\n        return self.best_solution, self.best_fitness", "name": "AdaptiveMemeticAlgorithm", "description": "Adaptive Memetic Algorithm with Dynamic Local Search and Quantum-Inspired Global Perturbations for optimizing photonic structures by leveraging hybrid local and global search strategies.", "configspace": "", "generation": 83, "fitness": -Infinity, "feedback": "An exception occurred: IndexError('index 25 is out of bounds for axis 0 with size 25').", "error": "IndexError('index 25 is out of bounds for axis 0 with size 25')", "parent_id": "f411cc38-685a-47d6-8f32-e00da36a531d", "metadata": {}, "mutation_prompt": null}
{"id": "98c32b76-7b09-4c78-bcff-43634dc7baf5", "solution": "import numpy as np\n\nclass AdaptiveQuantumGeneticAlgorithm:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = max(20, min(100, budget // 5))\n        self.mutation_rate = 0.1\n        self.crossover_rate = 0.8\n        self.mutation_step = 0.05\n        self.best_individual = None\n        self.best_fitness = float('inf')\n        self.interference_prob = 0.1\n\n    def initialize_population(self, lb, ub):\n        self.population = lb + (ub - lb) * np.random.rand(self.population_size, self.dim)\n\n    def evaluate_population(self, func):\n        fitness = np.array([func(individual) for individual in self.population])\n        for i, f in enumerate(fitness):\n            if f < self.best_fitness:\n                self.best_fitness = f\n                self.best_individual = self.population[i]\n        return fitness\n\n    def select_parents(self, fitness):\n        probabilities = fitness / fitness.sum()\n        indices = np.random.choice(self.population_size, size=2, p=probabilities)\n        return self.population[indices[0]], self.population[indices[1]]\n\n    def crossover(self, parent1, parent2):\n        if np.random.rand() < self.crossover_rate:\n            alpha = np.random.rand(self.dim)\n            child = alpha * parent1 + (1 - alpha) * parent2\n        else:\n            child = parent1 if np.random.rand() < 0.5 else parent2\n        return child\n\n    def mutate(self, individual, lb, ub):\n        if np.random.rand() < self.mutation_rate:\n            quantum_step = (np.random.rand(self.dim) - 0.5) * 2 * self.mutation_step\n            individual += quantum_step * (self.best_individual - individual)\n            individual = np.clip(individual, lb, ub)\n        return individual\n\n    def apply_social_learning(self, lb, ub):\n        for i in range(self.population_size):\n            if np.random.rand() < self.interference_prob:\n                random_vector = lb + (ub - lb) * np.random.rand(self.dim)\n                self.population[i] = np.mean([self.population[i], random_vector], axis=0)\n                self.population[i] = np.clip(self.population[i], lb, ub)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            fitness = self.evaluate_population(func)\n            evaluations += self.population_size\n\n            if evaluations >= self.budget:\n                break\n\n            new_population = []\n            for _ in range(self.population_size // 2):\n                parent1, parent2 = self.select_parents(fitness)\n                child1 = self.crossover(parent1, parent2)\n                child2 = self.crossover(parent2, parent1)\n                new_population.append(self.mutate(child1, lb, ub))\n                new_population.append(self.mutate(child2, lb, ub))\n\n            self.population = np.array(new_population)\n            self.apply_social_learning(lb, ub)\n\n        return self.best_individual, self.best_fitness", "name": "AdaptiveQuantumGeneticAlgorithm", "description": "Adaptive Genetic Algorithm with Quantum-Inspired Mutation and Social Learning for diverse exploration and efficient convergence in optimizing photonic structures.", "configspace": "", "generation": 84, "fitness": 0.2703841364735116, "feedback": "The algorithm AdaptiveQuantumGeneticAlgorithm got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.27 with standard deviation 0.00.", "error": "", "parent_id": "f411cc38-685a-47d6-8f32-e00da36a531d", "metadata": {"aucs": [0.2704545645296085, 0.27163431383798897, 0.26906353105293745]}, "mutation_prompt": null}
{"id": "4d234cb3-114e-4582-a384-ba5a1b6a4967", "solution": "import numpy as np\n\nclass EnhancedQuantumInspiredPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = max(10, min(50, budget // 10))\n        self.particles = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_fitness = np.full(self.num_particles, float('inf'))\n        self.global_best_position = None\n        self.global_best_fitness = float('inf')\n        self.cognitive_const = 1.5\n        self.social_const = 2.5\n        self.inertia_weight = 0.9\n        self.constriction_factor = 0.5\n        self.dynamic_neighborhood_size = 3\n        self.interference_prob = 0.1\n        self.levy_prob = 0.3  # Increased probability for more frequent exploration\n\n    def initialize_particles(self, lb, ub):\n        self.particles = lb + (ub - lb) * np.random.rand(self.num_particles, self.dim)\n        self.velocities = np.random.randn(self.num_particles, self.dim) * 0.1\n        self.personal_best_positions = np.copy(self.particles)\n\n    def evaluate_particles(self, func):\n        fitness = np.array([func(p) for p in self.particles])\n        for i, f in enumerate(fitness):\n            if f < self.personal_best_fitness[i]:\n                self.personal_best_fitness[i] = f\n                self.personal_best_positions[i] = self.particles[i]\n            if f < self.global_best_fitness:\n                self.global_best_fitness = f\n                self.global_best_position = self.particles[i]\n        return fitness\n\n    def update_velocities_and_positions(self, lb, ub):\n        for i in range(self.num_particles):\n            local_best = self.get_local_best(i)\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            cognitive_term = self.cognitive_const * r1 * (self.personal_best_positions[i] - self.particles[i])\n            social_term = self.social_const * r2 * (local_best - self.particles[i])\n            self.velocities[i] = (self.inertia_weight * self.velocities[i] +\n                                  cognitive_term + social_term)\n            self.particles[i] += self.constriction_factor * self.velocities[i]\n            if np.random.rand() < self.levy_prob:\n                self.dynamic_levy_flight(i, lb, ub)\n        self.particles = np.clip(self.particles, lb, ub)\n\n    def get_local_best(self, index):\n        neighborhood_indices = np.random.choice(self.num_particles, self.dynamic_neighborhood_size, replace=False)\n        local_best_index = min(neighborhood_indices, key=lambda i: self.personal_best_fitness[i])\n        return self.personal_best_positions[local_best_index]\n\n    def dynamic_levy_flight(self, i, lb, ub):\n        beta = 1.5\n        sigma = (np.math.gamma(1 + beta) * np.sin(np.pi * beta / 2) /\n                 (np.math.gamma((1 + beta) / 2) * beta * 2 ** ((beta - 1) / 2))) ** (1 / beta)\n        u = np.random.randn(self.dim) * sigma\n        v = np.random.randn(self.dim)\n        step = u / np.abs(v) ** (1 / beta)\n        step_size = 0.01 * (1 - (self.global_best_fitness / self.personal_best_fitness[i]))\n        self.particles[i] += step_size * step * (self.particles[i] - self.global_best_position)\n        self.particles[i] = np.clip(self.particles[i], lb, ub)\n\n    def apply_quantum_interference(self, lb, ub):\n        for i in range(self.num_particles):\n            if np.random.rand() < self.interference_prob:\n                interference_vector = lb + (ub - lb) * np.random.rand(self.dim)\n                self.particles[i] = (self.particles[i] + interference_vector) / 2\n                self.particles[i] = np.clip(self.particles[i], lb, ub)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_particles(lb, ub)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            self.evaluate_particles(func)\n            evaluations += self.num_particles\n\n            if evaluations >= self.budget:\n                break\n\n            self.update_velocities_and_positions(lb, ub)\n            self.apply_quantum_interference(lb, ub)\n\n        return self.global_best_position, self.global_best_fitness", "name": "EnhancedQuantumInspiredPSO", "description": "Enhanced Quantum-Inspired Particle Swarm Optimization with Dynamic Levy Flight and Adaptive Quantum Interference to improve convergence and robustness in black box photonic structure optimization.", "configspace": "", "generation": 85, "fitness": 0.2665606223899669, "feedback": "The algorithm EnhancedQuantumInspiredPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.27 with standard deviation 0.00.", "error": "", "parent_id": "f411cc38-685a-47d6-8f32-e00da36a531d", "metadata": {"aucs": [0.26629463709362566, 0.2651535250992413, 0.2682337049770338]}, "mutation_prompt": null}
{"id": "cc21e61b-5355-4884-9983-9102103e68a5", "solution": "import numpy as np\n\nclass AdaptiveQuantumInspiredDE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = max(10, min(60, budget // 5))\n        self.population = None\n        self.F = 0.5  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.best_solution = None\n        self.best_fitness = float('inf')\n        self.stochastic_local_search_prob = 0.25\n        self.levy_prob = 0.3\n\n    def initialize_population(self, lb, ub):\n        self.population = lb + (ub - lb) * np.random.rand(self.population_size, self.dim)\n\n    def evaluate_population(self, func):\n        fitness = np.array([func(ind) for ind in self.population])\n        for i, f in enumerate(fitness):\n            if f < self.best_fitness:\n                self.best_fitness = f\n                self.best_solution = self.population[i]\n        return fitness\n\n    def mutate(self, target_index, lb, ub):\n        indices = list(range(self.population_size))\n        indices.remove(target_index)\n        a, b, c = self.population[np.random.choice(indices, 3, replace=False)]\n        mutant = a + self.F * (b - c)\n        mutant = np.clip(mutant, lb, ub)\n        return mutant\n\n    def crossover(self, target, mutant):\n        crossover_mask = np.random.rand(self.dim) < self.CR\n        if not np.any(crossover_mask):\n            crossover_mask[np.random.randint(0, self.dim)] = True\n        trial = np.where(crossover_mask, mutant, target)\n        return trial\n\n    def levy_flight(self, i, lb, ub):\n        beta = 1.5\n        sigma = (np.math.gamma(1 + beta) * np.sin(np.pi * beta / 2) /\n                 (np.math.gamma((1 + beta) / 2) * beta * 2 ** ((beta - 1) / 2))) ** (1 / beta)\n        u = np.random.randn(self.dim) * sigma\n        v = np.random.randn(self.dim)\n        step = u / np.abs(v) ** (1 / beta)\n        return np.clip(self.population[i] + 0.01 * step * (self.population[i] - self.best_solution), lb, ub)\n\n    def stochastic_local_search(self, ind, lb, ub):\n        perturbation = 0.01 * (ub - lb) * np.random.randn(self.dim)\n        return np.clip(ind + perturbation, lb, ub)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            fitness = self.evaluate_population(func)\n            evaluations += self.population_size\n\n            if evaluations >= self.budget:\n                break\n\n            for i in range(self.population_size):\n                mutant = self.mutate(i, lb, ub)\n                trial = self.crossover(self.population[i], mutant)\n\n                if np.random.rand() < self.levy_prob:\n                    trial = self.levy_flight(i, lb, ub)\n\n                trial_fitness = func(trial)\n                evaluations += 1\n                if trial_fitness < fitness[i]:\n                    self.population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < self.best_fitness:\n                        self.best_fitness = trial_fitness\n                        self.best_solution = trial\n\n                if evaluations >= self.budget:\n                    break\n\n                if np.random.rand() < self.stochastic_local_search_prob:\n                    self.population[i] = self.stochastic_local_search(self.population[i], lb, ub)\n\n        return self.best_solution, self.best_fitness", "name": "AdaptiveQuantumInspiredDE", "description": "Adaptive Quantum-Inspired Differential Evolution with Lévy Flight and Stochastic Local Search for effective exploration and exploitation in photonic structure optimization.", "configspace": "", "generation": 86, "fitness": 0.2687186427298977, "feedback": "The algorithm AdaptiveQuantumInspiredDE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.27 with standard deviation 0.00.", "error": "", "parent_id": "f411cc38-685a-47d6-8f32-e00da36a531d", "metadata": {"aucs": [0.2694994020776801, 0.2661605701560479, 0.2704959559559652]}, "mutation_prompt": null}
{"id": "2c8586e8-a39f-41a9-8ce2-395f2ccdbb6b", "solution": "import numpy as np\n\nclass EnhancedHybridQuantumInspiredPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = max(10, min(50, budget // 10))\n        self.particles = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_fitness = np.full(self.num_particles, float('inf'))\n        self.global_best_position = None\n        self.global_best_fitness = float('inf')\n        self.cognitive_const = 2.0\n        self.social_const = 2.0\n        self.inertia_weight = 0.7\n        self.constriction_factor = 0.5\n        self.dynamic_neighborhood_size = 3\n        self.interference_prob = 0.1\n        self.levy_prob = 0.2\n        self.memory_factor = 0.1  # Factor for adaptive memory\n        self.reinit_threshold = 5  # Iterations without improvement before reinitialization\n\n    def initialize_particles(self, lb, ub):\n        self.particles = lb + (ub - lb) * np.random.rand(self.num_particles, self.dim)\n        self.velocities = np.random.randn(self.num_particles, self.dim) * 0.1\n        self.personal_best_positions = np.copy(self.particles)\n        self.last_improvement = 0\n\n    def evaluate_particles(self, func):\n        fitness = np.array([func(p) for p in self.particles])\n        for i, f in enumerate(fitness):\n            if f < self.personal_best_fitness[i]:\n                self.personal_best_fitness[i] = f\n                self.personal_best_positions[i] = self.particles[i]\n            if f < self.global_best_fitness:\n                self.global_best_fitness = f\n                self.global_best_position = self.particles[i]\n                self.last_improvement = 0\n        return fitness\n\n    def update_velocities_and_positions(self, lb, ub):\n        for i in range(self.num_particles):\n            local_best = self.get_local_best(i)\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            cognitive_term = self.cognitive_const * r1 * (self.personal_best_positions[i] - self.particles[i])\n            social_term = self.social_const * r2 * (local_best - self.particles[i])\n            adaptive_memory = self.memory_factor * (self.global_best_position - self.particles[i])\n            self.velocities[i] = (self.inertia_weight * self.velocities[i] +\n                                  cognitive_term + social_term + adaptive_memory)\n            self.particles[i] += self.constriction_factor * self.velocities[i]\n            if np.random.rand() < self.levy_prob:\n                self.levy_flight(i, lb, ub)\n        self.particles = np.clip(self.particles, lb, ub)\n\n    def get_local_best(self, index):\n        neighborhood_indices = np.random.choice(self.num_particles, self.dynamic_neighborhood_size, replace=False)\n        local_best_index = min(neighborhood_indices, key=lambda i: self.personal_best_fitness[i])\n        return self.personal_best_positions[local_best_index]\n\n    def levy_flight(self, i, lb, ub):\n        beta = 1.5\n        sigma = (np.math.gamma(1 + beta) * np.sin(np.pi * beta / 2) /\n                 (np.math.gamma((1 + beta) / 2) * beta * 2 ** ((beta - 1) / 2))) ** (1 / beta)\n        u = np.random.randn(self.dim) * sigma\n        v = np.random.randn(self.dim)\n        step = u / np.abs(v) ** (1 / beta)\n        self.particles[i] += 0.01 * step * (self.particles[i] - self.global_best_position)\n        self.particles[i] = np.clip(self.particles[i], lb, ub)\n\n    def apply_quantum_interference(self, lb, ub):\n        for i in range(self.num_particles):\n            if np.random.rand() < self.interference_prob:\n                interference_vector = lb + (ub - lb) * np.random.rand(self.dim)\n                self.particles[i] = np.mean([self.particles[i], interference_vector], axis=0)\n                self.particles[i] = np.clip(self.particles[i], lb, ub)\n\n    def stochastic_velocity_reinit(self):\n        if self.last_improvement >= self.reinit_threshold:\n            indices = np.random.choice(self.num_particles, self.num_particles // 3, replace=False)\n            for i in indices:\n                self.velocities[i] = np.random.randn(self.dim) * 0.1\n            self.last_improvement = 0\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_particles(lb, ub)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            self.evaluate_particles(func)\n            evaluations += self.num_particles\n\n            if evaluations >= self.budget:\n                break\n\n            self.update_velocities_and_positions(lb, ub)\n            self.apply_quantum_interference(lb, ub)\n            self.stochastic_velocity_reinit()\n            self.last_improvement += 1\n\n        return self.global_best_position, self.global_best_fitness", "name": "EnhancedHybridQuantumInspiredPSO", "description": "Enhanced Hybrid Quantum-Inspired PSO with Stochastic Velocity Reinitialization and Adaptive Memory to boost convergence in photonic structure optimization.", "configspace": "", "generation": 87, "fitness": 0.27733380933853585, "feedback": "The algorithm EnhancedHybridQuantumInspiredPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28 with standard deviation 0.00.", "error": "", "parent_id": "f411cc38-685a-47d6-8f32-e00da36a531d", "metadata": {"aucs": [0.2770373019540827, 0.27748193404698707, 0.2774821920145377]}, "mutation_prompt": null}
{"id": "09a59d8f-5bf7-4d0c-9580-8ec2e2cb93ba", "solution": "import numpy as np\n\nclass AdvancedQuantumInspiredPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = max(10, min(50, budget // 10))\n        self.particles = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_fitness = np.full(self.num_particles, float('inf'))\n        self.global_best_position = None\n        self.global_best_fitness = float('inf')\n        self.cognitive_const = 2.0\n        self.social_const = 2.0\n        self.inertia_weight = 0.7\n        self.constriction_factor = 0.5\n        self.dynamic_neighborhood_size = 3\n        self.interference_prob = 0.1\n        self.levy_prob = 0.2\n        self.memory_factor = 0.1\n        self.reinit_threshold = 5\n        self.learning_rate = 0.1  # Initial learning rate for adaptive control\n\n    def initialize_particles(self, lb, ub):\n        self.particles = lb + (ub - lb) * np.random.rand(self.num_particles, self.dim)\n        self.velocities = np.random.randn(self.num_particles, self.dim) * 0.1\n        self.personal_best_positions = np.copy(self.particles)\n        self.last_improvement = 0\n\n    def evaluate_particles(self, func):\n        fitness = np.array([func(p) for p in self.particles])\n        for i, f in enumerate(fitness):\n            if f < self.personal_best_fitness[i]:\n                self.personal_best_fitness[i] = f\n                self.personal_best_positions[i] = self.particles[i]\n            if f < self.global_best_fitness:\n                self.global_best_fitness = f\n                self.global_best_position = self.particles[i]\n                self.last_improvement = 0\n        return fitness\n\n    def update_velocities_and_positions(self, lb, ub):\n        for i in range(self.num_particles):\n            local_best = self.get_local_best(i)\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            cognitive_term = self.learning_rate * self.cognitive_const * r1 * (self.personal_best_positions[i] - self.particles[i])\n            social_term = self.learning_rate * self.social_const * r2 * (local_best - self.particles[i])\n            adaptive_memory = self.memory_factor * (self.global_best_position - self.particles[i])\n            self.velocities[i] = (self.inertia_weight * self.velocities[i] +\n                                  cognitive_term + social_term + adaptive_memory)\n            self.particles[i] += self.constriction_factor * self.velocities[i]\n            if np.random.rand() < self.levy_prob:\n                self.levy_flight(i, lb, ub)\n        self.particles = np.clip(self.particles, lb, ub)\n\n    def get_local_best(self, index):\n        neighborhood_indices = np.random.choice(self.num_particles, self.dynamic_neighborhood_size, replace=False)\n        local_best_index = min(neighborhood_indices, key=lambda i: self.personal_best_fitness[i])\n        return self.personal_best_positions[local_best_index]\n\n    def levy_flight(self, i, lb, ub):\n        beta = 1.5\n        sigma = (np.math.gamma(1 + beta) * np.sin(np.pi * beta / 2) /\n                 (np.math.gamma((1 + beta) / 2) * beta * 2 ** ((beta - 1) / 2))) ** (1 / beta)\n        u = np.random.randn(self.dim) * sigma\n        v = np.random.randn(self.dim)\n        step = u / np.abs(v) ** (1 / beta)\n        self.particles[i] += 0.01 * step * (self.particles[i] - self.global_best_position)\n        self.particles[i] = np.clip(self.particles[i], lb, ub)\n\n    def apply_quantum_interference(self, lb, ub):\n        for i in range(self.num_particles):\n            if np.random.rand() < self.interference_prob:\n                interference_vector = lb + (ub - lb) * np.random.rand(self.dim)\n                self.particles[i] = np.mean([self.particles[i], interference_vector], axis=0)\n                self.particles[i] = np.clip(self.particles[i], lb, ub)\n\n    def stochastic_velocity_reinit(self):\n        if self.last_improvement >= self.reinit_threshold:\n            indices = np.random.choice(self.num_particles, self.num_particles // 3, replace=False)\n            for i in indices:\n                self.velocities[i] = np.random.randn(self.dim) * 0.1\n            self.last_improvement = 0\n\n    def adapt_learning_rate(self, evaluations):\n        max_eval = self.budget\n        self.learning_rate = 0.1 + (0.9 * (1 - evaluations / max_eval))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_particles(lb, ub)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            self.evaluate_particles(func)\n            evaluations += self.num_particles\n\n            if evaluations >= self.budget:\n                break\n\n            self.adapt_learning_rate(evaluations)\n            self.update_velocities_and_positions(lb, ub)\n            self.apply_quantum_interference(lb, ub)\n            self.stochastic_velocity_reinit()\n            self.last_improvement += 1\n\n        return self.global_best_position, self.global_best_fitness", "name": "AdvancedQuantumInspiredPSO", "description": "Integrated Dynamic Adaptive Learning Rate and Cooperative Behavior in Quantum-Inspired PSO to enhance exploration-exploitation balance for photonic structure optimization.", "configspace": "", "generation": 88, "fitness": 0.2774241307649619, "feedback": "The algorithm AdvancedQuantumInspiredPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28 with standard deviation 0.00.", "error": "", "parent_id": "2c8586e8-a39f-41a9-8ce2-395f2ccdbb6b", "metadata": {"aucs": [0.2770741044067314, 0.2776581033646154, 0.2775401845235389]}, "mutation_prompt": null}
{"id": "0f138c76-0ebf-46e3-8244-64ff7d1f3cfe", "solution": "import numpy as np\n\nclass EnhancedCooperativeQuantumSwarm:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_swarms = 3\n        self.num_particles = max(10, min(30, budget // 10))\n        self.swarms = [self.initialize_swarm() for _ in range(self.num_swarms)]\n        self.global_best_position = None\n        self.global_best_fitness = float('inf')\n    \n    def initialize_swarm(self):\n        swarm = {\n            'particles': None,\n            'velocities': None,\n            'personal_best_positions': None,\n            'personal_best_fitness': np.full(self.num_particles, float('inf')),\n            'local_best_position': None,\n            'local_best_fitness': float('inf')\n        }\n        return swarm\n    \n    def initialize_particles(self, swarm, lb, ub):\n        swarm['particles'] = lb + (ub - lb) * np.random.rand(self.num_particles, self.dim)\n        swarm['velocities'] = np.random.randn(self.num_particles, self.dim) * 0.1\n        swarm['personal_best_positions'] = np.copy(swarm['particles'])\n    \n    def evaluate_particles(self, swarm, func):\n        fitness = np.array([func(p) for p in swarm['particles']])\n        for i, f in enumerate(fitness):\n            if f < swarm['personal_best_fitness'][i]:\n                swarm['personal_best_fitness'][i] = f\n                swarm['personal_best_positions'][i] = swarm['particles'][i]\n            if f < swarm['local_best_fitness']:\n                swarm['local_best_fitness'] = f\n                swarm['local_best_position'] = swarm['particles'][i]\n            if f < self.global_best_fitness:\n                self.global_best_fitness = f\n                self.global_best_position = swarm['particles'][i]\n        return fitness\n    \n    def update_velocities_and_positions(self, swarm, lb, ub):\n        inertia_weight = 0.7\n        cognitive_const = 2.0\n        social_const = 2.0\n        constriction_factor = 0.5\n        \n        for i in range(self.num_particles):\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            cognitive_term = cognitive_const * r1 * (swarm['personal_best_positions'][i] - swarm['particles'][i])\n            social_term = social_const * r2 * (swarm['local_best_position'] - swarm['particles'][i])\n            swarm['velocities'][i] = (inertia_weight * swarm['velocities'][i] +\n                                      cognitive_term + social_term)\n            swarm['particles'][i] += constriction_factor * swarm['velocities'][i]\n        \n        swarm['particles'] = np.clip(swarm['particles'], lb, ub)\n    \n    def adaptive_quantum_behaviors(self, swarm, lb, ub):\n        for i in range(self.num_particles):\n            if np.random.rand() < 0.1:  # Quantum teleportation probability\n                swarm['particles'][i] = lb + (ub - lb) * np.random.rand(self.dim)\n            elif np.random.rand() < 0.1:  # Quantum coherence\n                coherence_vector = lb + (ub - lb) * np.random.rand(self.dim)\n                swarm['particles'][i] = 0.5 * (swarm['particles'][i] + coherence_vector)\n            swarm['particles'][i] = np.clip(swarm['particles'][i], lb, ub)\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        for swarm in self.swarms:\n            self.initialize_particles(swarm, lb, ub)\n        \n        evaluations = 0\n        while evaluations < self.budget:\n            for swarm in self.swarms:\n                self.evaluate_particles(swarm, func)\n                evaluations += self.num_particles\n                if evaluations >= self.budget:\n                    break\n                self.update_velocities_and_positions(swarm, lb, ub)\n                self.adaptive_quantum_behaviors(swarm, lb, ub)\n        \n        return self.global_best_position, self.global_best_fitness", "name": "EnhancedCooperativeQuantumSwarm", "description": "Enhanced Cooperative Coevolution with Multi-Swarm Strategy using Adaptive Quantum Behaviors for efficient global optimization in photonic structures.", "configspace": "", "generation": 89, "fitness": 0.2732119158124931, "feedback": "The algorithm EnhancedCooperativeQuantumSwarm got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.27 with standard deviation 0.00.", "error": "", "parent_id": "09a59d8f-5bf7-4d0c-9580-8ec2e2cb93ba", "metadata": {"aucs": [0.27232251717441047, 0.274732848314698, 0.27258038194837075]}, "mutation_prompt": null}
{"id": "cf880f7c-0d77-4083-b8fb-0268d9972a31", "solution": "import numpy as np\n\nclass EnhancedQuantumInspiredPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = max(10, min(50, budget // 10))\n        self.particles = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_fitness = np.full(self.num_particles, float('inf'))\n        self.global_best_position = None\n        self.global_best_fitness = float('inf')\n        self.cognitive_const = 2.0\n        self.social_const = 2.0\n        self.initial_inertia_weight = 0.9\n        self.final_inertia_weight = 0.4\n        self.constriction_factor = 0.5\n        self.initial_neighborhood_size = 3\n        self.interference_prob = 0.1\n        self.levy_prob = 0.2\n        self.memory_factor = 0.1\n        self.reinit_threshold = 5\n        self.learning_rate = 0.1\n\n    def initialize_particles(self, lb, ub):\n        self.particles = lb + (ub - lb) * np.random.rand(self.num_particles, self.dim)\n        self.velocities = np.random.randn(self.num_particles, self.dim) * 0.1\n        self.personal_best_positions = np.copy(self.particles)\n        self.last_improvement = 0\n\n    def evaluate_particles(self, func):\n        fitness = np.array([func(p) for p in self.particles])\n        for i, f in enumerate(fitness):\n            if f < self.personal_best_fitness[i]:\n                self.personal_best_fitness[i] = f\n                self.personal_best_positions[i] = self.particles[i]\n            if f < self.global_best_fitness:\n                self.global_best_fitness = f\n                self.global_best_position = self.particles[i]\n                self.last_improvement = 0\n        return fitness\n\n    def update_velocities_and_positions(self, lb, ub, evaluations):\n        inertia_weight = self.get_inertia_weight(evaluations)\n        neighborhood_size = self.get_adaptive_neighborhood_size(evaluations)\n        for i in range(self.num_particles):\n            local_best = self.get_local_best(i, neighborhood_size)\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            cognitive_term = self.learning_rate * self.cognitive_const * r1 * (self.personal_best_positions[i] - self.particles[i])\n            social_term = self.learning_rate * self.social_const * r2 * (local_best - self.particles[i])\n            adaptive_memory = self.memory_factor * (self.global_best_position - self.particles[i])\n            self.velocities[i] = (inertia_weight * self.velocities[i] +\n                                  cognitive_term + social_term + adaptive_memory)\n            self.particles[i] += self.constriction_factor * self.velocities[i]\n            if np.random.rand() < self.levy_prob:\n                self.levy_flight(i, lb, ub)\n        self.particles = np.clip(self.particles, lb, ub)\n\n    def get_local_best(self, index, neighborhood_size):\n        neighborhood_indices = np.random.choice(self.num_particles, neighborhood_size, replace=False)\n        local_best_index = min(neighborhood_indices, key=lambda i: self.personal_best_fitness[i])\n        return self.personal_best_positions[local_best_index]\n\n    def levy_flight(self, i, lb, ub):\n        beta = 1.5\n        sigma = (np.math.gamma(1 + beta) * np.sin(np.pi * beta / 2) /\n                 (np.math.gamma((1 + beta) / 2) * beta * 2 ** ((beta - 1) / 2))) ** (1 / beta)\n        u = np.random.randn(self.dim) * sigma\n        v = np.random.randn(self.dim)\n        step = u / np.abs(v) ** (1 / beta)\n        self.particles[i] += 0.01 * step * (self.particles[i] - self.global_best_position)\n        self.particles[i] = np.clip(self.particles[i], lb, ub)\n\n    def apply_quantum_interference(self, lb, ub):\n        for i in range(self.num_particles):\n            if np.random.rand() < self.interference_prob:\n                interference_vector = lb + (ub - lb) * np.random.rand(self.dim)\n                self.particles[i] = np.mean([self.particles[i], interference_vector], axis=0)\n                self.particles[i] = np.clip(self.particles[i], lb, ub)\n\n    def stochastic_velocity_reinit(self):\n        if self.last_improvement >= self.reinit_threshold:\n            indices = np.random.choice(self.num_particles, self.num_particles // 3, replace=False)\n            for i in indices:\n                self.velocities[i] = np.random.randn(self.dim) * 0.1\n            self.last_improvement = 0\n\n    def adapt_learning_rate(self, evaluations):\n        max_eval = self.budget\n        self.learning_rate = 0.1 + (0.9 * (1 - evaluations / max_eval))\n\n    def get_inertia_weight(self, evaluations):\n        return self.initial_inertia_weight - (self.initial_inertia_weight - self.final_inertia_weight) * (evaluations / self.budget)\n\n    def get_adaptive_neighborhood_size(self, evaluations):\n        return self.initial_neighborhood_size + int((self.num_particles - self.initial_neighborhood_size) * (evaluations / self.budget))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_particles(lb, ub)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            self.evaluate_particles(func)\n            evaluations += self.num_particles\n\n            if evaluations >= self.budget:\n                break\n\n            self.adapt_learning_rate(evaluations)\n            self.update_velocities_and_positions(lb, ub, evaluations)\n            self.apply_quantum_interference(lb, ub)\n            self.stochastic_velocity_reinit()\n            self.last_improvement += 1\n\n        return self.global_best_position, self.global_best_fitness", "name": "EnhancedQuantumInspiredPSO", "description": "Introduce adaptive neighborhood size and chaotic map-based inertia weight adjustment in Quantum-Inspired PSO for enhanced convergence and diversity in photonic structure optimization.", "configspace": "", "generation": 90, "fitness": 0.2768825424126268, "feedback": "The algorithm EnhancedQuantumInspiredPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28 with standard deviation 0.00.", "error": "", "parent_id": "09a59d8f-5bf7-4d0c-9580-8ec2e2cb93ba", "metadata": {"aucs": [0.2765560356200929, 0.27713095790348585, 0.2769606337143016]}, "mutation_prompt": null}
{"id": "5b978a70-ecd7-4be2-aa2a-dd2221b2e499", "solution": "import numpy as np\n\nclass EnhancedQuantumInspiredPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = max(10, min(50, budget // 10))\n        self.particles = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_fitness = np.full(self.num_particles, float('inf'))\n        self.global_best_position = None\n        self.global_best_fitness = float('inf')\n        self.cognitive_const = 2.0\n        self.social_const = 2.0\n        self.inertia_weight = 0.7\n        self.constriction_factor = 0.5\n        self.dynamic_neighborhood_size = 3\n        self.interference_prob = 0.1\n        self.levy_prob = 0.2\n        self.memory_factor = 0.1\n        self.reinit_threshold = 5\n        self.learning_rate = 0.1\n        self.opp_learning_prob = 0.3\n\n    def initialize_particles(self, lb, ub):\n        self.particles = lb + (ub - lb) * np.random.rand(self.num_particles, self.dim)\n        self.velocities = np.random.randn(self.num_particles, self.dim) * 0.1\n        self.personal_best_positions = np.copy(self.particles)\n        self.last_improvement = 0\n\n    def evaluate_particles(self, func):\n        fitness = np.array([func(p) for p in self.particles])\n        for i, f in enumerate(fitness):\n            if f < self.personal_best_fitness[i]:\n                self.personal_best_fitness[i] = f\n                self.personal_best_positions[i] = self.particles[i]\n            if f < self.global_best_fitness:\n                self.global_best_fitness = f\n                self.global_best_position = self.particles[i]\n                self.last_improvement = 0\n        return fitness\n\n    def update_velocities_and_positions(self, lb, ub):\n        for i in range(self.num_particles):\n            local_best = self.get_local_best(i)\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            cognitive_term = self.learning_rate * self.cognitive_const * r1 * (self.personal_best_positions[i] - self.particles[i])\n            social_term = self.learning_rate * self.social_const * r2 * (local_best - self.particles[i])\n            adaptive_memory = self.memory_factor * (self.global_best_position - self.particles[i])\n            self.velocities[i] = (self.inertia_weight * self.velocities[i] +\n                                  cognitive_term + social_term + adaptive_memory)\n            self.particles[i] += self.constriction_factor * self.velocities[i]\n            if np.random.rand() < self.levy_prob:\n                self.levy_flight(i, lb, ub)\n        self.particles = np.clip(self.particles, lb, ub)\n\n    def get_local_best(self, index):\n        neighborhood_indices = np.random.choice(self.num_particles, self.dynamic_neighborhood_size, replace=False)\n        local_best_index = min(neighborhood_indices, key=lambda i: self.personal_best_fitness[i])\n        return self.personal_best_positions[local_best_index]\n\n    def levy_flight(self, i, lb, ub):\n        beta = 1.5\n        sigma = (np.math.gamma(1 + beta) * np.sin(np.pi * beta / 2) /\n                 (np.math.gamma((1 + beta) / 2) * beta * 2 ** ((beta - 1) / 2))) ** (1 / beta)\n        u = np.random.randn(self.dim) * sigma\n        v = np.random.randn(self.dim)\n        step = u / np.abs(v) ** (1 / beta)\n        self.particles[i] += 0.01 * step * (self.particles[i] - self.global_best_position)\n        self.particles[i] = np.clip(self.particles[i], lb, ub)\n\n    def apply_quantum_interference(self, lb, ub):\n        for i in range(self.num_particles):\n            if np.random.rand() < self.interference_prob:\n                interference_vector = lb + (ub - lb) * np.random.rand(self.dim)\n                self.particles[i] = np.mean([self.particles[i], interference_vector], axis=0)\n                self.particles[i] = np.clip(self.particles[i], lb, ub)\n\n    def stochastic_velocity_reinit(self):\n        if self.last_improvement >= self.reinit_threshold:\n            indices = np.random.choice(self.num_particles, self.num_particles // 3, replace=False)\n            for i in indices:\n                self.velocities[i] = np.random.randn(self.dim) * 0.1\n            self.last_improvement = 0\n\n    def adapt_learning_rate(self, evaluations):\n        max_eval = self.budget\n        self.learning_rate = 0.1 + (0.9 * (1 - evaluations / max_eval))\n\n    def assist_with_opp_learning(self, lb, ub):\n        if np.random.rand() < self.opp_learning_prob:\n            opposite_particles = lb + ub - self.particles\n            opposite_particles = np.clip(opposite_particles, lb, ub)\n            opposite_fitness = np.array([func(p) for p in opposite_particles])\n            improved_indices = opposite_fitness < self.personal_best_fitness\n            self.particles[improved_indices] = opposite_particles[improved_indices]\n            self.personal_best_fitness[improved_indices] = opposite_fitness[improved_indices]\n            self.personal_best_positions[improved_indices] = opposite_particles[improved_indices]\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_particles(lb, ub)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            self.evaluate_particles(func)\n            evaluations += self.num_particles\n\n            if evaluations >= self.budget:\n                break\n\n            self.adapt_learning_rate(evaluations)\n            self.update_velocities_and_positions(lb, ub)\n            self.apply_quantum_interference(lb, ub)\n            self.stochastic_velocity_reinit()\n            self.assist_with_opp_learning(lb, ub)\n            self.last_improvement += 1\n\n        return self.global_best_position, self.global_best_fitness", "name": "EnhancedQuantumInspiredPSO", "description": "Enhanced Quantum-Inspired Particle Swarm Optimization with Adaptive Opposition-Based Learning to improve convergence speed and solution diversity for photonic structure optimization.", "configspace": "", "generation": 91, "fitness": -Infinity, "feedback": "An exception occurred: NameError(\"name 'func' is not defined\").", "error": "NameError(\"name 'func' is not defined\")", "parent_id": "09a59d8f-5bf7-4d0c-9580-8ec2e2cb93ba", "metadata": {}, "mutation_prompt": null}
{"id": "41570dc1-ec09-470c-9347-f6fd0d5d2861", "solution": "import numpy as np\n\nclass AdaptiveQuantumSwarm:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = max(10, min(50, budget // 10))\n        self.particles = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_fitness = np.full(self.num_particles, float('inf'))\n        self.global_best_position = None\n        self.global_best_fitness = float('inf')\n        self.cognitive_const = 2.0\n        self.social_const = 2.0\n        self.inertia_weight = 0.7\n        self.constriction_factor = 0.5\n        self.interference_prob = 0.1\n        self.levy_prob = 0.2\n        self.memory_factor = 0.1\n        self.reinit_threshold = 5\n        self.learning_rate = 0.1\n        self.local_search_prob = 0.2  # Probability for enhanced local search\n\n    def initialize_particles(self, lb, ub):\n        self.particles = lb + (ub - lb) * np.random.rand(self.num_particles, self.dim)\n        self.velocities = np.random.randn(self.num_particles, self.dim) * 0.1\n        self.personal_best_positions = np.copy(self.particles)\n        self.last_improvement = 0\n\n    def evaluate_particles(self, func):\n        fitness = np.array([func(p) for p in self.particles])\n        for i, f in enumerate(fitness):\n            if f < self.personal_best_fitness[i]:\n                self.personal_best_fitness[i] = f\n                self.personal_best_positions[i] = self.particles[i]\n            if f < self.global_best_fitness:\n                self.global_best_fitness = f\n                self.global_best_position = self.particles[i]\n                self.last_improvement = 0\n        return fitness\n\n    def update_velocities_and_positions(self, lb, ub):\n        for i in range(self.num_particles):\n            local_best = self.get_local_best(i)\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            cognitive_term = self.learning_rate * self.cognitive_const * r1 * (self.personal_best_positions[i] - self.particles[i])\n            social_term = self.learning_rate * self.social_const * r2 * (local_best - self.particles[i])\n            adaptive_memory = self.memory_factor * (self.global_best_position - self.particles[i])\n            self.velocities[i] = (self.inertia_weight * self.velocities[i] +\n                                  cognitive_term + social_term + adaptive_memory)\n            if np.random.rand() < self.local_search_prob:\n                self.enhanced_local_search(i, lb, ub)\n            else:\n                self.particles[i] += self.constriction_factor * self.velocities[i]\n            if np.random.rand() < self.levy_prob:\n                self.levy_flight(i, lb, ub)\n        self.particles = np.clip(self.particles, lb, ub)\n\n    def get_local_best(self, index):\n        neighborhood_indices = np.random.choice(self.num_particles, 3, replace=False)\n        local_best_index = min(neighborhood_indices, key=lambda i: self.personal_best_fitness[i])\n        return self.personal_best_positions[local_best_index]\n\n    def levy_flight(self, i, lb, ub):\n        beta = 1.5\n        sigma = (np.math.gamma(1 + beta) * np.sin(np.pi * beta / 2) /\n                 (np.math.gamma((1 + beta) / 2) * beta * 2 ** ((beta - 1) / 2))) ** (1 / beta)\n        u = np.random.randn(self.dim) * sigma\n        v = np.random.randn(self.dim)\n        step = u / np.abs(v) ** (1 / beta)\n        self.particles[i] += 0.01 * step * (self.particles[i] - self.global_best_position)\n        self.particles[i] = np.clip(self.particles[i], lb, ub)\n\n    def enhanced_local_search(self, i, lb, ub):\n        perturbation = np.random.randn(self.dim) * 0.05\n        candidate = self.particles[i] + perturbation\n        candidate = np.clip(candidate, lb, ub)\n        if self.evaluate_single(func, candidate) < self.personal_best_fitness[i]:\n            self.particles[i] = candidate\n\n    def evaluate_single(self, func, particle):\n        return func(particle)\n\n    def apply_quantum_interference(self, lb, ub):\n        for i in range(self.num_particles):\n            if np.random.rand() < self.interference_prob:\n                interference_vector = lb + (ub - lb) * np.random.rand(self.dim)\n                self.particles[i] = np.mean([self.particles[i], interference_vector], axis=0)\n                self.particles[i] = np.clip(self.particles[i], lb, ub)\n\n    def stochastic_velocity_reinit(self):\n        if self.last_improvement >= self.reinit_threshold:\n            indices = np.random.choice(self.num_particles, self.num_particles // 3, replace=False)\n            for i in indices:\n                self.velocities[i] = np.random.randn(self.dim) * 0.1\n            self.last_improvement = 0\n\n    def adapt_learning_rate(self, evaluations):\n        max_eval = self.budget\n        self.learning_rate = 0.1 + (0.9 * (1 - evaluations / max_eval))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_particles(lb, ub)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            self.evaluate_particles(func)\n            evaluations += self.num_particles\n\n            if evaluations >= self.budget:\n                break\n\n            self.adapt_learning_rate(evaluations)\n            self.update_velocities_and_positions(lb, ub)\n            self.apply_quantum_interference(lb, ub)\n            self.stochastic_velocity_reinit()\n            self.last_improvement += 1\n\n        return self.global_best_position, self.global_best_fitness", "name": "AdaptiveQuantumSwarm", "description": "Introduce Adaptive Quantum Swarm with Enhanced Local Search and Adaptive Memory for improved photonic structure optimization.", "configspace": "", "generation": 92, "fitness": -Infinity, "feedback": "An exception occurred: NameError(\"name 'func' is not defined\").", "error": "NameError(\"name 'func' is not defined\")", "parent_id": "09a59d8f-5bf7-4d0c-9580-8ec2e2cb93ba", "metadata": {}, "mutation_prompt": null}
{"id": "b93f4e99-7481-472d-a6a8-c1a2dfafc34f", "solution": "import numpy as np\n\nclass EnhancedQuantumInspiredPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = max(10, min(50, budget // 10))\n        self.particles = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_fitness = np.full(self.num_particles, float('inf'))\n        self.global_best_position = None\n        self.global_best_fitness = float('inf')\n        self.cognitive_const = 2.0\n        self.social_const = 2.0\n        self.inertia_weight = 0.7\n        self.constriction_factor = 0.5\n        self.dynamic_neighborhood_size = 3\n        self.interference_prob = 0.1\n        self.levy_prob = 0.2\n        self.memory_factor = 0.1\n        self.reinit_threshold = 5\n        self.learning_rate = 0.1\n\n    def initialize_particles(self, lb, ub):\n        self.particles = lb + (ub - lb) * np.random.rand(self.num_particles, self.dim)\n        self.velocities = np.random.randn(self.num_particles, self.dim) * 0.1\n        self.personal_best_positions = np.copy(self.particles)\n        self.last_improvement = 0\n\n    def evaluate_particles(self, func):\n        fitness = np.array([func(p) for p in self.particles])\n        for i, f in enumerate(fitness):\n            if f < self.personal_best_fitness[i]:\n                self.personal_best_fitness[i] = f\n                self.personal_best_positions[i] = self.particles[i]\n            if f < self.global_best_fitness:\n                self.global_best_fitness = f\n                self.global_best_position = self.particles[i]\n                self.last_improvement = 0\n        return fitness\n\n    def update_velocities_and_positions(self, lb, ub):\n        for i in range(self.num_particles):\n            local_best = self.get_dynamic_local_best(i)\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            cognitive_term = self.learning_rate * self.cognitive_const * r1 * (self.personal_best_positions[i] - self.particles[i])\n            social_term = self.learning_rate * self.social_const * r2 * (local_best - self.particles[i])\n            adaptive_memory = self.memory_factor * (self.global_best_position - self.particles[i])\n            self.velocities[i] = (self.inertia_weight * self.velocities[i] +\n                                  cognitive_term + social_term + adaptive_memory)\n            self.particles[i] += self.constriction_factor * self.velocities[i]\n            if np.random.rand() < self.levy_prob:\n                self.levy_flight(i, lb, ub)\n        self.particles = np.clip(self.particles, lb, ub)\n\n    def get_dynamic_local_best(self, index):\n        neighborhood_size = int(self.dynamic_neighborhood_size * (1 + 0.5 * np.sin(index / self.num_particles * np.pi)))\n        neighborhood_indices = np.random.choice(self.num_particles, neighborhood_size, replace=False)\n        local_best_index = min(neighborhood_indices, key=lambda i: self.personal_best_fitness[i])\n        return self.personal_best_positions[local_best_index]\n\n    def levy_flight(self, i, lb, ub):\n        beta = 1.5\n        sigma = (np.math.gamma(1 + beta) * np.sin(np.pi * beta / 2) /\n                 (np.math.gamma((1 + beta) / 2) * beta * 2 ** ((beta - 1) / 2))) ** (1 / beta)\n        u = np.random.randn(self.dim) * sigma\n        v = np.random.randn(self.dim)\n        step = u / np.abs(v) ** (1 / beta)\n        self.particles[i] += 0.01 * step * (self.particles[i] - self.global_best_position)\n        self.particles[i] = np.clip(self.particles[i], lb, ub)\n\n    def apply_quantum_interference(self, lb, ub):\n        for i in range(self.num_particles):\n            if np.random.rand() < self.interference_prob:\n                interference_vector = lb + (ub - lb) * np.random.rand(self.dim)\n                self.particles[i] = np.mean([self.particles[i], interference_vector], axis=0)\n                self.particles[i] = np.clip(self.particles[i], lb, ub)\n\n    def stochastic_velocity_reinit(self):\n        if self.last_improvement >= self.reinit_threshold:\n            indices = np.random.choice(self.num_particles, self.num_particles // 3, replace=False)\n            for i in indices:\n                self.velocities[i] = np.random.randn(self.dim) * 0.1\n            self.last_improvement = 0\n\n    def adapt_learning_rate(self, evaluations):\n        max_eval = self.budget\n        self.learning_rate = 0.1 + (0.9 * (1 - evaluations / max_eval))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_particles(lb, ub)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            self.evaluate_particles(func)\n            evaluations += self.num_particles\n\n            if evaluations >= self.budget:\n                break\n\n            self.adapt_learning_rate(evaluations)\n            self.update_velocities_and_positions(lb, ub)\n            self.apply_quantum_interference(lb, ub)\n            self.stochastic_velocity_reinit()\n            self.last_improvement += 1\n\n        return self.global_best_position, self.global_best_fitness", "name": "EnhancedQuantumInspiredPSO", "description": "Enhanced Quantum-Inspired PSO with Dynamic Dimensional Learning and Adaptive Neighborhoods for improved convergence in photonic structure optimization.", "configspace": "", "generation": 93, "fitness": 0.27756274879489923, "feedback": "The algorithm EnhancedQuantumInspiredPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28 with standard deviation 0.00.", "error": "", "parent_id": "09a59d8f-5bf7-4d0c-9580-8ec2e2cb93ba", "metadata": {"aucs": [0.27754001363420944, 0.27763239722777766, 0.27751583552271064]}, "mutation_prompt": null}
{"id": "e686b102-7318-4ed3-81fb-0f14ade90191", "solution": "import numpy as np\n\nclass DASH_DEPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_individuals = max(15, min(60, budget // 12))\n        self.population = None\n        self.velocities = None\n        self.best_positions = None\n        self.best_fitness = np.full(self.num_individuals, float('inf'))\n        self.global_best_position = None\n        self.global_best_fitness = float('inf')\n        self.F = 0.5\n        self.CR = 0.9\n        self.w = 0.7\n        self.c1 = 1.5\n        self.c2 = 1.5\n        self.chaos_factor = 0.05\n        self.mutation_rate = 0.2\n\n    def initialize_population(self, lb, ub):\n        self.population = lb + (ub - lb) * np.random.rand(self.num_individuals, self.dim)\n        self.velocities = np.random.randn(self.num_individuals, self.dim) * 0.1\n        self.best_positions = np.copy(self.population)\n\n    def evaluate_population(self, func):\n        fitness = np.array([func(ind) for ind in self.population])\n        for i, f in enumerate(fitness):\n            if f < self.best_fitness[i]:\n                self.best_fitness[i] = f\n                self.best_positions[i] = self.population[i]\n            if f < self.global_best_fitness:\n                self.global_best_fitness = f\n                self.global_best_position = self.population[i]\n        return fitness\n\n    def update_population(self, lb, ub):\n        for i in range(self.num_individuals):\n            indices = np.random.choice(self.num_individuals, 3, replace=False)\n            x1, x2, x3 = self.population[indices]\n            mutant = x1 + self.F * (x2 - x3)\n            mutant = np.clip(mutant, lb, ub)\n            crossover = (np.random.rand(self.dim) < self.CR)\n            offspring = np.where(crossover, mutant, self.population[i])\n            if np.random.rand() < self.mutation_rate:\n                offspring = lb + (ub - lb) * np.random.rand(self.dim)\n            new_velocity = (self.w * self.velocities[i] +\n                            self.c1 * np.random.rand(self.dim) * (self.best_positions[i] - self.population[i]) +\n                            self.c2 * np.random.rand(self.dim) * (self.global_best_position - self.population[i]))\n            self.velocities[i] = new_velocity\n            self.population[i] = offspring + new_velocity\n            self.population[i] = np.clip(self.population[i], lb, ub)\n\n    def apply_chaos_search(self, lb, ub):\n        chaotically_perturbed = lb + (ub - lb) * np.random.rand(self.num_individuals, self.dim) * self.chaos_factor\n        self.population = np.clip(self.population + chaotically_perturbed, lb, ub)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            self.evaluate_population(func)\n            evaluations += self.num_individuals\n\n            if evaluations >= self.budget:\n                break\n\n            self.update_population(lb, ub)\n            if evaluations % (self.budget // 10) == 0:\n                self.apply_chaos_search(lb, ub)\n\n        return self.global_best_position, self.global_best_fitness", "name": "DASH_DEPSO", "description": "Dynamic Adaptive Strategy with Hybrid Differential Evolution and Particle Swarm Optimization (DASH-DEPSO) leveraging adaptive population diversity and chaos-enhanced search for robust optimization of photonic structures.", "configspace": "", "generation": 94, "fitness": 0.24984566121971452, "feedback": "The algorithm DASH_DEPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.25 with standard deviation 0.00.", "error": "", "parent_id": "b93f4e99-7481-472d-a6a8-c1a2dfafc34f", "metadata": {"aucs": [0.24587764525497902, 0.24894106414487938, 0.25471827425928517]}, "mutation_prompt": null}
{"id": "b960e45c-3e10-4eac-a749-a296eb4d503d", "solution": "import numpy as np\n\nclass AdaptiveQuantumInspiredPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = max(10, min(50, budget // 10))\n        self.particles = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_fitness = np.full(self.num_particles, float('inf'))\n        self.global_best_position = None\n        self.global_best_fitness = float('inf')\n        self.cognitive_const = 2.0\n        self.social_const = 2.0\n        self.inertia_weight = 0.9\n        self.constriction_factor = 0.6\n        self.dynamic_neighborhood_size = 3\n        self.interference_prob = 0.15\n        self.levy_prob = 0.25\n        self.memory_factor = 0.15\n        self.reinit_threshold = 5\n        self.learning_rate = 0.1\n        self.dynamic_reinforcement = 0.05\n\n    def initialize_particles(self, lb, ub):\n        self.particles = lb + (ub - lb) * np.random.rand(self.num_particles, self.dim)\n        self.velocities = np.random.randn(self.num_particles, self.dim) * 0.1\n        self.personal_best_positions = np.copy(self.particles)\n        self.last_improvement = 0\n\n    def evaluate_particles(self, func):\n        fitness = np.array([func(p) for p in self.particles])\n        for i, f in enumerate(fitness):\n            if f < self.personal_best_fitness[i]:\n                self.personal_best_fitness[i] = f\n                self.personal_best_positions[i] = self.particles[i]\n            if f < self.global_best_fitness:\n                self.global_best_fitness = f\n                self.global_best_position = self.particles[i]\n                self.last_improvement = 0\n        return fitness\n\n    def update_velocities_and_positions(self, lb, ub):\n        for i in range(self.num_particles):\n            local_best = self.get_dynamic_local_best(i)\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            cognitive_term = self.learning_rate * self.cognitive_const * r1 * (self.personal_best_positions[i] - self.particles[i])\n            social_term = self.learning_rate * self.social_const * r2 * (local_best - self.particles[i])\n            dynamic_term = self.dynamic_reinforcement * np.random.randn(self.dim)\n            adaptive_memory = self.memory_factor * (self.global_best_position - self.particles[i])\n            self.velocities[i] = (self.inertia_weight * self.velocities[i] +\n                                  cognitive_term + social_term + adaptive_memory + dynamic_term)\n            self.particles[i] += self.constriction_factor * self.velocities[i]\n            if np.random.rand() < self.levy_prob:\n                self.levy_flight(i, lb, ub)\n        self.particles = np.clip(self.particles, lb, ub)\n\n    def get_dynamic_local_best(self, index):\n        neighborhood_size = int(self.dynamic_neighborhood_size * (1 + 0.5 * np.sin(index / self.num_particles * np.pi)))\n        neighborhood_indices = np.random.choice(self.num_particles, neighborhood_size, replace=False)\n        local_best_index = min(neighborhood_indices, key=lambda i: self.personal_best_fitness[i])\n        return self.personal_best_positions[local_best_index]\n\n    def levy_flight(self, i, lb, ub):\n        beta = 1.5\n        sigma = (np.math.gamma(1 + beta) * np.sin(np.pi * beta / 2) /\n                 (np.math.gamma((1 + beta) / 2) * beta * 2 ** ((beta - 1) / 2))) ** (1 / beta)\n        u = np.random.randn(self.dim) * sigma\n        v = np.random.randn(self.dim)\n        step = u / np.abs(v) ** (1 / beta)\n        self.particles[i] += 0.01 * step * (self.particles[i] - self.global_best_position)\n        self.particles[i] = np.clip(self.particles[i], lb, ub)\n\n    def apply_quantum_interference(self, lb, ub):\n        for i in range(self.num_particles):\n            if np.random.rand() < self.interference_prob:\n                interference_vector = lb + (ub - lb) * np.random.rand(self.dim)\n                self.particles[i] = np.mean([self.particles[i], interference_vector], axis=0)\n                self.particles[i] = np.clip(self.particles[i], lb, ub)\n\n    def stochastic_velocity_reinit(self):\n        if self.last_improvement >= self.reinit_threshold:\n            indices = np.random.choice(self.num_particles, self.num_particles // 3, replace=False)\n            for i in indices:\n                self.velocities[i] = np.random.randn(self.dim) * 0.1\n            self.last_improvement = 0\n\n    def adapt_learning_rate(self, evaluations):\n        max_eval = self.budget\n        self.learning_rate = 0.1 + (0.9 * (1 - evaluations / max_eval))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_particles(lb, ub)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            self.evaluate_particles(func)\n            evaluations += self.num_particles\n\n            if evaluations >= self.budget:\n                break\n\n            self.adapt_learning_rate(evaluations)\n            self.update_velocities_and_positions(lb, ub)\n            self.apply_quantum_interference(lb, ub)\n            self.stochastic_velocity_reinit()\n            self.last_improvement += 1\n\n        return self.global_best_position, self.global_best_fitness", "name": "AdaptiveQuantumInspiredPSO", "description": "Adaptive Quantum-Inspired PSO with Dynamic Particle Reinforcement and Levy Flight for enhanced exploration-exploitation balance in photonic structure optimization.", "configspace": "", "generation": 95, "fitness": 0.2735373800914635, "feedback": "The algorithm AdaptiveQuantumInspiredPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.27 with standard deviation 0.00.", "error": "", "parent_id": "b93f4e99-7481-472d-a6a8-c1a2dfafc34f", "metadata": {"aucs": [0.273978049422716, 0.2733069913260714, 0.2733270995256032]}, "mutation_prompt": null}
{"id": "ab6f2cc8-398a-4f73-baec-e077f35047f0", "solution": "import numpy as np\n\nclass QuantumEnhancedAdaptivePSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = max(10, min(50, budget // 7))\n        self.particles = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_fitness = np.full(self.num_particles, float('inf'))\n        self.global_best_position = None\n        self.global_best_fitness = float('inf')\n        self.cognitive_const = 2.0\n        self.social_const = 2.0\n        self.inertia_weight = 0.9\n        self.min_inertia = 0.4\n        self.max_inertia = 0.9\n        self.dynamic_neighborhood_size = 5\n        self.interference_prob = 0.15\n        self.levy_prob = 0.3\n        self.memory_factor = 0.05\n        self.reinit_threshold = 10\n        self.learning_rate = 0.05\n\n    def initialize_particles(self, lb, ub):\n        self.particles = lb + (ub - lb) * np.random.rand(self.num_particles, self.dim)\n        self.velocities = np.random.randn(self.num_particles, self.dim) * 0.1\n        self.personal_best_positions = np.copy(self.particles)\n        self.last_improvement = 0\n\n    def evaluate_particles(self, func):\n        fitness = np.array([func(p) for p in self.particles])\n        for i, f in enumerate(fitness):\n            if f < self.personal_best_fitness[i]:\n                self.personal_best_fitness[i] = f\n                self.personal_best_positions[i] = self.particles[i]\n            if f < self.global_best_fitness:\n                self.global_best_fitness = f\n                self.global_best_position = self.particles[i]\n                self.last_improvement = 0\n        return fitness\n\n    def update_velocities_and_positions(self, lb, ub):\n        for i in range(self.num_particles):\n            local_best = self.get_dynamic_local_best(i)\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            cognitive_term = self.learning_rate * self.cognitive_const * r1 * (self.personal_best_positions[i] - self.particles[i])\n            social_term = self.learning_rate * self.social_const * r2 * (local_best - self.particles[i])\n            adaptive_memory = self.memory_factor * (self.global_best_position - self.particles[i])\n            self.velocities[i] = (self.inertia_weight * self.velocities[i] +\n                                  cognitive_term + social_term + adaptive_memory)\n            self.particles[i] += self.velocities[i]\n            if np.random.rand() < self.levy_prob:\n                self.levy_flight(i, lb, ub)\n        self.particles = np.clip(self.particles, lb, ub)\n\n    def get_dynamic_local_best(self, index):\n        neighborhood_size = int(self.dynamic_neighborhood_size * (1 + 0.5 * np.cos(index / self.num_particles * np.pi)))\n        neighborhood_indices = np.random.choice(self.num_particles, neighborhood_size, replace=False)\n        local_best_index = min(neighborhood_indices, key=lambda i: self.personal_best_fitness[i])\n        return self.personal_best_positions[local_best_index]\n\n    def levy_flight(self, i, lb, ub):\n        beta = 1.5\n        sigma = (np.math.gamma(1 + beta) * np.sin(np.pi * beta / 2) /\n                 (np.math.gamma((1 + beta) / 2) * beta * 2 ** ((beta - 1) / 2))) ** (1 / beta)\n        u = np.random.randn(self.dim) * sigma\n        v = np.random.randn(self.dim)\n        step = u / np.abs(v) ** (1 / beta)\n        self.particles[i] += 0.01 * step * (self.particles[i] - self.global_best_position)\n        self.particles[i] = np.clip(self.particles[i], lb, ub)\n\n    def apply_quantum_interference(self, lb, ub):\n        for i in range(self.num_particles):\n            if np.random.rand() < self.interference_prob:\n                interference_vector = lb + (ub - lb) * np.random.rand(self.dim)\n                self.particles[i] = np.mean([self.particles[i], interference_vector], axis=0)\n                self.particles[i] = np.clip(self.particles[i], lb, ub)\n\n    def stochastic_velocity_reinit(self):\n        if self.last_improvement >= self.reinit_threshold:\n            indices = np.random.choice(self.num_particles, self.num_particles // 3, replace=False)\n            for i in indices:\n                self.velocities[i] = np.random.randn(self.dim) * 0.1\n            self.last_improvement = 0\n\n    def adapt_learning_rate(self, evaluations):\n        max_eval = self.budget\n        self.inertia_weight = self.max_inertia - (evaluations / max_eval) * (self.max_inertia - self.min_inertia)\n        self.learning_rate = 0.05 + 0.95 * (1 - evaluations / max_eval)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_particles(lb, ub)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            self.evaluate_particles(func)\n            evaluations += self.num_particles\n\n            if evaluations >= self.budget:\n                break\n\n            self.adapt_learning_rate(evaluations)\n            self.update_velocities_and_positions(lb, ub)\n            self.apply_quantum_interference(lb, ub)\n            self.stochastic_velocity_reinit()\n            self.last_improvement += 1\n\n        return self.global_best_position, self.global_best_fitness", "name": "QuantumEnhancedAdaptivePSO", "description": "Quantum Enhanced Adaptive Particle Swarm Optimization with Dynamic Neighborhoods and Constriction Control for efficient photonic structure optimization.  ", "configspace": "", "generation": 96, "fitness": 0.2718228977255205, "feedback": "The algorithm QuantumEnhancedAdaptivePSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.27 with standard deviation 0.00.", "error": "", "parent_id": "b93f4e99-7481-472d-a6a8-c1a2dfafc34f", "metadata": {"aucs": [0.26900841462280534, 0.27460855888997915, 0.27185171966377697]}, "mutation_prompt": null}
{"id": "fd7ca020-2add-44b1-958b-77918814e048", "solution": "import numpy as np\n\nclass AdaptiveMemeticQPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = max(10, min(50, budget // 10))\n        self.particles = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_fitness = np.full(self.num_particles, float('inf'))\n        self.global_best_position = None\n        self.global_best_fitness = float('inf')\n        self.cognitive_const = 1.5\n        self.social_const = 1.5\n        self.inertia_weight = 0.6\n        self.levy_prob = 0.3\n        self.mutation_prob = 0.1\n        self.diversification_factor = 0.05\n\n    def initialize_particles(self, lb, ub):\n        self.particles = lb + (ub - lb) * np.random.rand(self.num_particles, self.dim)\n        self.velocities = np.random.randn(self.num_particles, self.dim) * 0.1\n        self.personal_best_positions = np.copy(self.particles)\n\n    def evaluate_particles(self, func):\n        fitness = np.array([func(p) for p in self.particles])\n        for i, f in enumerate(fitness):\n            if f < self.personal_best_fitness[i]:\n                self.personal_best_fitness[i] = f\n                self.personal_best_positions[i] = self.particles[i]\n            if f < self.global_best_fitness:\n                self.global_best_fitness = f\n                self.global_best_position = self.particles[i]\n        return fitness\n\n    def update_velocities_and_positions(self, lb, ub):\n        for i in range(self.num_particles):\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            cognitive_term = self.cognitive_const * r1 * (self.personal_best_positions[i] - self.particles[i])\n            social_term = self.social_const * r2 * (self.global_best_position - self.particles[i])\n            self.velocities[i] = (self.inertia_weight * self.velocities[i] + cognitive_term + social_term)\n            self.particles[i] += self.velocities[i]\n            if np.random.rand() < self.levy_prob:\n                self.levy_flight(i)\n            if np.random.rand() < self.mutation_prob:\n                self.mutate_particle(i, lb, ub)\n        self.particles = np.clip(self.particles, lb, ub)\n\n    def levy_flight(self, i):\n        beta = 1.5\n        sigma = (np.math.gamma(1 + beta) * np.sin(np.pi * beta / 2) /\n                 (np.math.gamma((1 + beta) / 2) * beta * 2 ** ((beta - 1) / 2))) ** (1 / beta)\n        u = np.random.randn(self.dim) * sigma\n        v = np.random.randn(self.dim)\n        step = u / np.abs(v) ** (1 / beta)\n        self.particles[i] += 0.01 * step\n\n    def mutate_particle(self, i, lb, ub):\n        perturbation = self.diversification_factor * (ub - lb) * (np.random.rand(self.dim) - 0.5)\n        self.particles[i] += perturbation\n        self.particles[i] = np.clip(self.particles[i], lb, ub)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_particles(lb, ub)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            self.evaluate_particles(func)\n            evaluations += self.num_particles\n\n            if evaluations >= self.budget:\n                break\n\n            self.update_velocities_and_positions(lb, ub)\n\n        return self.global_best_position, self.global_best_fitness", "name": "AdaptiveMemeticQPSO", "description": "Adaptive Memetic Quantum-inspired Particle Swarm Optimization (AMQPSO) with Hybrid Learning Mechanism and Diversification Strategy for enhanced exploration and exploitation in optimizing photonic structures.", "configspace": "", "generation": 97, "fitness": 0.27481003972790397, "feedback": "The algorithm AdaptiveMemeticQPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.27 with standard deviation 0.00.", "error": "", "parent_id": "b93f4e99-7481-472d-a6a8-c1a2dfafc34f", "metadata": {"aucs": [0.2768940365337318, 0.273275602820811, 0.2742604798291691]}, "mutation_prompt": null}
{"id": "1535d2f7-1931-440d-ac31-1dd479d64dfa", "solution": "import numpy as np\n\nclass HybridQuantumInspiredPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = max(10, min(50, budget // 10))\n        self.particles = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_fitness = np.full(self.num_particles, float('inf'))\n        self.global_best_position = None\n        self.global_best_fitness = float('inf')\n        self.cognitive_const = 2.0\n        self.social_const = 2.0\n        self.inertia_weight = 0.7\n        self.constriction_factor = 0.5\n        self.dynamic_neighborhood_size = 3\n        self.interference_prob = 0.1\n        self.levy_prob = 0.2\n        self.mutation_prob = 0.2\n        self.reinit_threshold = 5\n        self.learning_rate = 0.1\n\n    def initialize_particles(self, lb, ub):\n        self.particles = lb + (ub - lb) * np.random.rand(self.num_particles, self.dim)\n        self.velocities = np.random.randn(self.num_particles, self.dim) * 0.1\n        self.personal_best_positions = np.copy(self.particles)\n        self.last_improvement = 0\n\n    def evaluate_particles(self, func):\n        fitness = np.array([func(p) for p in self.particles])\n        for i, f in enumerate(fitness):\n            if f < self.personal_best_fitness[i]:\n                self.personal_best_fitness[i] = f\n                self.personal_best_positions[i] = self.particles[i]\n            if f < self.global_best_fitness:\n                self.global_best_fitness = f\n                self.global_best_position = self.particles[i]\n                self.last_improvement = 0\n        return fitness\n\n    def update_velocities_and_positions(self, lb, ub):\n        for i in range(self.num_particles):\n            local_best = self.get_dynamic_local_best(i)\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            cognitive_term = self.learning_rate * self.cognitive_const * r1 * (self.personal_best_positions[i] - self.particles[i])\n            social_term = self.learning_rate * self.social_const * r2 * (local_best - self.particles[i])\n            adaptive_memory = self.adaptive_memory(i)\n            self.velocities[i] = (self.inertia_weight * self.velocities[i] +\n                                  cognitive_term + social_term + adaptive_memory)\n            self.particles[i] += self.constriction_factor * self.velocities[i]\n            if np.random.rand() < self.levy_prob:\n                self.levy_flight(i, lb, ub)\n            if np.random.rand() < self.mutation_prob:\n                self.adaptive_mutation(i, lb, ub)\n        self.particles = np.clip(self.particles, lb, ub)\n\n    def adaptive_memory(self, i):\n        if self.last_improvement < self.reinit_threshold:\n            return np.zeros(self.dim)\n        return np.random.rand(self.dim) * (self.global_best_position - self.particles[i])\n\n    def get_dynamic_local_best(self, index):\n        neighborhood_size = int(self.dynamic_neighborhood_size * (1 + 0.5 * np.sin(index / self.num_particles * np.pi)))\n        neighborhood_indices = np.random.choice(self.num_particles, neighborhood_size, replace=False)\n        local_best_index = min(neighborhood_indices, key=lambda i: self.personal_best_fitness[i])\n        return self.personal_best_positions[local_best_index]\n\n    def levy_flight(self, i, lb, ub):\n        beta = 1.5\n        sigma = (np.math.gamma(1 + beta) * np.sin(np.pi * beta / 2) /\n                 (np.math.gamma((1 + beta) / 2) * beta * 2 ** ((beta - 1) / 2))) ** (1 / beta)\n        u = np.random.randn(self.dim) * sigma\n        v = np.random.randn(self.dim)\n        step = u / np.abs(v) ** (1 / beta)\n        self.particles[i] += 0.01 * step * (self.particles[i] - self.global_best_position)\n        self.particles[i] = np.clip(self.particles[i], lb, ub)\n\n    def adaptive_mutation(self, i, lb, ub):\n        mutation_vector = np.random.rand(self.dim) * (ub - lb) + lb\n        self.particles[i] = np.mean([self.particles[i], mutation_vector], axis=0)\n        self.particles[i] = np.clip(self.particles[i], lb, ub)\n\n    def stochastic_velocity_reinit(self):\n        if self.last_improvement >= self.reinit_threshold:\n            indices = np.random.choice(self.num_particles, self.num_particles // 3, replace=False)\n            for i in indices:\n                self.velocities[i] = np.random.randn(self.dim) * 0.1\n            self.last_improvement = 0\n\n    def adapt_learning_rate(self, evaluations):\n        max_eval = self.budget\n        self.learning_rate = 0.1 + (0.9 * (1 - evaluations / max_eval))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_particles(lb, ub)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            self.evaluate_particles(func)\n            evaluations += self.num_particles\n\n            if evaluations >= self.budget:\n                break\n\n            self.adapt_learning_rate(evaluations)\n            self.update_velocities_and_positions(lb, ub)\n            self.stochastic_velocity_reinit()\n            self.last_improvement += 1\n\n        return self.global_best_position, self.global_best_fitness", "name": "HybridQuantumInspiredPSO", "description": "Hybrid Quantum-Inspired PSO with Adaptive Mutation and Adaptive Memory for Enhanced Exploration and Exploitation in Photonic Structure Optimization.", "configspace": "", "generation": 98, "fitness": 0.2771419565603812, "feedback": "The algorithm HybridQuantumInspiredPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28 with standard deviation 0.00.", "error": "", "parent_id": "b93f4e99-7481-472d-a6a8-c1a2dfafc34f", "metadata": {"aucs": [0.27703613021870743, 0.27733665130584373, 0.27705308815659246]}, "mutation_prompt": null}
{"id": "a772f57e-83f1-4d60-a9d7-5d360f4eb73e", "solution": "import numpy as np\n\nclass HybridQuantumGeneticAlgorithm:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = max(20, min(100, budget // 5))\n        self.population = None\n        self.fitness = None\n        self.global_best_position = None\n        self.global_best_fitness = float('inf')\n        self.mutation_rate = 0.05\n        self.crossover_rate = 0.8\n        self.adaptive_factor = 0.1\n        self.quantum_delta = 0.01\n        self.tournament_size = 5\n\n    def initialize_population(self, lb, ub):\n        self.population = lb + (ub - lb) * np.random.rand(self.population_size, self.dim)\n        self.fitness = np.full(self.population_size, float('inf'))\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            f = func(self.population[i])\n            if f < self.fitness[i]:\n                self.fitness[i] = f\n                if f < self.global_best_fitness:\n                    self.global_best_fitness = f\n                    self.global_best_position = np.copy(self.population[i])\n\n    def tournament_selection(self):\n        selected_indices = np.zeros(self.population_size, dtype=int)\n        for i in range(self.population_size):\n            competitors = np.random.choice(self.population_size, self.tournament_size, replace=False)\n            selected_indices[i] = min(competitors, key=lambda idx: self.fitness[idx])\n        return selected_indices\n\n    def crossover(self, parent1, parent2):\n        if np.random.rand() < self.crossover_rate:\n            point = np.random.randint(1, self.dim - 1)\n            child1 = np.concatenate((parent1[:point], parent2[point:]))\n            child2 = np.concatenate((parent2[:point], parent1[point:]))\n            return child1, child2\n        else:\n            return np.copy(parent1), np.copy(parent2)\n\n    def mutate(self, individual, lb, ub):\n        for i in range(self.dim):\n            if np.random.rand() < self.mutation_rate:\n                individual[i] += self.adaptive_factor * np.random.randn() * (ub[i] - lb[i])\n                individual[i] = np.clip(individual[i], lb[i], ub[i])\n\n    def quantum_mutation(self, individual, lb, ub):\n        if np.random.rand() < self.quantum_delta:\n            interference_vector = lb + (ub - lb) * np.random.rand(self.dim)\n            individual[:] = np.mean([individual, interference_vector], axis=0)\n            individual[:] = np.clip(individual, lb, ub)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            self.evaluate_population(func)\n            evaluations += self.population_size\n\n            if evaluations >= self.budget:\n                break\n\n            selected_indices = self.tournament_selection()\n            new_population = np.zeros_like(self.population)\n\n            for i in range(0, self.population_size, 2):\n                parent1 = self.population[selected_indices[i]]\n                parent2 = self.population[selected_indices[i + 1]]\n                child1, child2 = self.crossover(parent1, parent2)\n                self.mutate(child1, lb, ub)\n                self.mutate(child2, lb, ub)\n                self.quantum_mutation(child1, lb, ub)\n                self.quantum_mutation(child2, lb, ub)\n                new_population[i] = child1\n                new_population[i + 1] = child2\n\n            self.population = new_population\n            self.mutation_rate *= 0.99  # Decaying mutation rate to enhance exploitation\n\n        return self.global_best_position, self.global_best_fitness", "name": "HybridQuantumGeneticAlgorithm", "description": "Hybrid Quantum Genetic Algorithm with Adaptive Mutation Strategies for enhanced exploration and exploitation in photonic structure optimization.", "configspace": "", "generation": 99, "fitness": 0.2773931506626001, "feedback": "The algorithm HybridQuantumGeneticAlgorithm got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28 with standard deviation 0.00.", "error": "", "parent_id": "b93f4e99-7481-472d-a6a8-c1a2dfafc34f", "metadata": {"aucs": [0.27733442411055975, 0.27755427406872213, 0.2772907538085184]}, "mutation_prompt": null}
