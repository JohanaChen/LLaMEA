{"id": "3acdc6cd-ddc7-43f8-8c2d-464d0fe2b5e1", "solution": "import numpy as np\n\nclass PSO_SA_Optimizer:\n    def __init__(self, budget, dim, pop_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.w = 0.5  # Inertia weight\n        self.c1 = 1.5  # Cognitive parameter\n        self.c2 = 1.5  # Social parameter\n        self.temp = 1.0  # Initial temperature for SA\n\n    def __call__(self, func):\n        np.random.seed(42)  # For reproducibility\n        \n        # Initialize particles\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.array([func(pos) for pos in positions])\n        global_best_position = personal_best_positions[np.argmin(personal_best_scores)]\n        global_best_score = np.min(personal_best_scores)\n        \n        eval_count = self.pop_size\n        \n        while eval_count < self.budget:\n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                velocities[i] = (self.w * velocities[i]\n                                 + self.c1 * r1 * (personal_best_positions[i] - positions[i])\n                                 + self.c2 * r2 * (global_best_position - positions[i]))\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], self.lower_bound, self.upper_bound)\n\n                # Evaluate the new position\n                score = func(positions[i])\n                eval_count += 1\n                if eval_count >= self.budget:\n                    break\n\n                # Update personal best\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n\n                # Update global best\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = positions[i]\n\n                # Simulated Annealing Acceptance\n                if np.random.rand() < np.exp(-(score - global_best_score) / self.temp):\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n                    if score < global_best_score:\n                        global_best_score = score\n                        global_best_position = positions[i]\n\n            # Temperature cooling schedule\n            self.temp *= 0.95\n\n        return global_best_position, global_best_score", "name": "PSO_SA_Optimizer", "description": "A novel hybrid Particle Swarm Optimization with Simulated Annealing (PSO-SA) that leverages swarm intelligence for exploration and a cooling schedule for exploitation to balance local and global search effectively.", "configspace": "", "generation": 0, "fitness": 0.21059790697116187, "feedback": "The algorithm PSO_SA_Optimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.21 with standard deviation 0.19.", "error": "", "parent_id": null, "metadata": {"aucs": [0.33180480772342813, 0.33180480772342813, 0.33180480772342813, 0.36288488077686154, 0.36288488077686154, 0.36288488077686154, 0.3524736208739604, 0.3524736208739604, 0.3524736208739604, 0.019804201973858238, 0.019804201973858238, 0.019804201973858238, 0.12705097111581132, 0.12705097111581132, 0.12705097111581132, 0.005070909016537395, 0.005070909016537395, 0.005070909016537395, 0.14781258849985157, 0.14781258849985157, 0.14781258849985157, 0.0739480251375022, 0.0739480251375022, 0.0739480251375022, 0.07073332447140823, 0.07073332447140823, 0.07073332447140823, 0.06879204837801989, 0.06879204837801989, 0.06879204837801989, 0.09331936714507605, 0.09331936714507605, 0.09331936714507605, 0.06733613886654477, 0.06733613886654477, 0.06733613886654477, 0.9903031515539998, 0.9903031515539998, 0.9903031515539998, 0.9947192478610297, 0.9947192478610297, 0.9947192478610297, 0.9890387250111802, 0.9890387250111802, 0.9890387250111802, 0.30709987270664374, 0.30709987270664374, 0.30709987270664374, 0.15530680417395537, 0.15530680417395537, 0.15530680417395537, 0.3394415208220134, 0.3394415208220134, 0.3394415208220134, 0.24298728017591742, 0.24298728017591742, 0.24298728017591742, 0.16161800043553431, 0.16161800043553431, 0.16161800043553431, 0.1173217364685677, 0.1173217364685677, 0.1173217364685677, 0.19322942770854046, 0.19322942770854046, 0.19322942770854046, 0.2983965720491797, 0.2983965720491797, 0.2983965720491797, 0.1317150895951441, 0.1317150895951441, 0.1317150895951441, 0.17800730831532308, 0.17800730831532308, 0.17800730831532308, 0.13089013401344585, 0.13089013401344585, 0.13089013401344585, 0.13783319048218146, 0.13783319048218146, 0.13783319048218146, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.06275889367695586, 0.06275889367695586, 0.06275889367695586, 0.0921172203415126, 0.0921172203415126, 0.0921172203415126, 0.07986567406052014, 0.07986567406052014, 0.07986567406052014, 0.09738886712669792, 0.09738886712669792, 0.09738886712669792, 0.16643999934033182, 0.16643999934033182, 0.16643999934033182, 0.07342646552950849, 0.07342646552950849, 0.07342646552950849, 0.12758519974444626, 0.12758519974444626, 0.12758519974444626, 0.08000095052500733, 0.08000095052500733, 0.08000095052500733, 0.14453183125769864, 0.14453183125769864, 0.14453183125769864, 0.03980922322614411, 0.03980922322614411, 0.03980922322614411, 0.08462753076390905, 0.08462753076390905, 0.08462753076390905, 0.4671244194660692, 0.4671244194660692, 0.4671244194660692, 0.5045918281190267, 0.5045918281190267, 0.5045918281190267, 0.3509994412720403, 0.3509994412720403, 0.3509994412720403, 0.07257488906637022, 0.07257488906637022, 0.07257488906637022, 0.12356996615542681, 0.12356996615542681, 0.12356996615542681, 0.1335001189901539, 0.1335001189901539, 0.1335001189901539, 0.19426965622165127, 0.19426965622165127, 0.19426965622165127, 0.20234185013691963, 0.20234185013691963, 0.20234185013691963, 0.2516400816034029, 0.2516400816034029, 0.2516400816034029, 0.22961330330537189, 0.22961330330537189, 0.22961330330537189, 0.24167076151018063, 0.24167076151018063, 0.24167076151018063, 0.1536442870904673, 0.1536442870904673, 0.1536442870904673, 0.22373106428887368, 0.22373106428887368, 0.22373106428887368, 0.14764796790191415, 0.14764796790191415, 0.14764796790191415, 0.1303138991730377, 0.1303138991730377, 0.1303138991730377, 0.23052497567089292, 0.23052497567089292, 0.23052497567089292, 0.20104779533348627, 0.20104779533348627, 0.20104779533348627, 0.17029465856323533, 0.17029465856323533, 0.17029465856323533, 0.1820730591843247, 0.1820730591843247, 0.1820730591843247, 0.2053444456554102, 0.2053444456554102, 0.2053444456554102, 0.2130250730216161, 0.2130250730216161, 0.2130250730216161, 0.5094102849266552, 0.5094102849266552, 0.5094102849266552, 0.15877051354414273, 0.15877051354414273, 0.15877051354414273, 0.17640780579590787, 0.17640780579590787, 0.17640780579590787, 0.16913806230392836, 0.16913806230392836, 0.16913806230392836, 0.21300540126855982, 0.21300540126855982, 0.21300540126855982, 0.15678109906489357, 0.15678109906489357, 0.15678109906489357, 0.23658620893611437, 0.23658620893611437, 0.23658620893611437, 0.24289835742278865, 0.24289835742278865, 0.24289835742278865, 0.24137359473521847, 0.24137359473521847, 0.24137359473521847, 0.08766623665503481, 0.08766623665503481, 0.08766623665503481, 0.10105819935769578, 0.10105819935769578, 0.10105819935769578, 0.10281919523859473, 0.10281919523859473, 0.10281919523859473]}, "mutation_prompt": null}
{"id": "8f632cf8-5117-419c-980c-8ba120d752eb", "solution": "import numpy as np\n\nclass PSO_SA_Optimizer:\n    def __init__(self, budget, dim, pop_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.w = 0.5  # Inertia weight\n        self.c1 = 1.5  # Cognitive parameter\n        self.c2 = 1.5  # Social parameter\n        self.temp = 1.0  # Initial temperature for SA\n\n    def __call__(self, func):\n        np.random.seed(42)  # For reproducibility\n        \n        # Initialize particles\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.array([func(pos) for pos in positions])\n        global_best_position = personal_best_positions[np.argmin(personal_best_scores)]\n        global_best_score = np.min(personal_best_scores)\n        \n        eval_count = self.pop_size\n        \n        while eval_count < self.budget:\n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                velocities[i] = (self.w * velocities[i]\n                                 + self.c1 * r1 * (personal_best_positions[i] - positions[i])\n                                 + self.c2 * r2 * (global_best_position - positions[i]))\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], self.lower_bound, self.upper_bound)\n\n                # Evaluate the new position\n                score = func(positions[i])\n                eval_count += 1\n                if eval_count >= self.budget:\n                    break\n\n                # Update personal best\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n\n                # Update global best\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = positions[i]\n\n                # Simulated Annealing Acceptance\n                if np.random.rand() < np.exp(-(score - global_best_score) / self.temp):\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n                    if score < global_best_score:\n                        global_best_score = score\n                        global_best_position = positions[i]\n\n            # Temperature cooling schedule\n            self.temp *= 0.95\n\n        return global_best_position, global_best_score", "name": "PSO_SA_Optimizer", "description": "A novel hybrid Particle Swarm Optimization with Simulated Annealing (PSO-SA) that leverages swarm intelligence for exploration and a cooling schedule for exploitation to balance local and global search effectively.", "configspace": "", "generation": 1, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "3acdc6cd-ddc7-43f8-8c2d-464d0fe2b5e1", "metadata": {"aucs": [0.33180480772342813, 0.33180480772342813, 0.33180480772342813, 0.36288488077686154, 0.36288488077686154, 0.36288488077686154, 0.3524736208739604, 0.3524736208739604, 0.3524736208739604, 0.019804201973858238, 0.019804201973858238, 0.019804201973858238, 0.12705097111581132, 0.12705097111581132, 0.12705097111581132, 0.005070909016537395, 0.005070909016537395, 0.005070909016537395, 0.14781258849985157, 0.14781258849985157, 0.14781258849985157, 0.0739480251375022, 0.0739480251375022, 0.0739480251375022, 0.07073332447140823, 0.07073332447140823, 0.07073332447140823, 0.06879204837801989, 0.06879204837801989, 0.06879204837801989, 0.09331936714507605, 0.09331936714507605, 0.09331936714507605, 0.06733613886654477, 0.06733613886654477, 0.06733613886654477, 0.9903031515539998, 0.9903031515539998, 0.9903031515539998, 0.9947192478610297, 0.9947192478610297, 0.9947192478610297, 0.9890387250111802, 0.9890387250111802, 0.9890387250111802, 0.30709987270664374, 0.30709987270664374, 0.30709987270664374, 0.15530680417395537, 0.15530680417395537, 0.15530680417395537, 0.3394415208220134, 0.3394415208220134, 0.3394415208220134, 0.24298728017591742, 0.24298728017591742, 0.24298728017591742, 0.16161800043553431, 0.16161800043553431, 0.16161800043553431, 0.1173217364685677, 0.1173217364685677, 0.1173217364685677, 0.19322942770854046, 0.19322942770854046, 0.19322942770854046, 0.2983965720491797, 0.2983965720491797, 0.2983965720491797, 0.1317150895951441, 0.1317150895951441, 0.1317150895951441, 0.17800730831532308, 0.17800730831532308, 0.17800730831532308, 0.13089013401344585, 0.13089013401344585, 0.13089013401344585, 0.13783319048218146, 0.13783319048218146, 0.13783319048218146, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.06275889367695586, 0.06275889367695586, 0.06275889367695586, 0.0921172203415126, 0.0921172203415126, 0.0921172203415126, 0.07986567406052014, 0.07986567406052014, 0.07986567406052014, 0.09738886712669792, 0.09738886712669792, 0.09738886712669792, 0.16643999934033182, 0.16643999934033182, 0.16643999934033182, 0.07342646552950849, 0.07342646552950849, 0.07342646552950849, 0.12758519974444626, 0.12758519974444626, 0.12758519974444626, 0.08000095052500733, 0.08000095052500733, 0.08000095052500733, 0.14453183125769864, 0.14453183125769864, 0.14453183125769864, 0.03980922322614411, 0.03980922322614411, 0.03980922322614411, 0.08462753076390905, 0.08462753076390905, 0.08462753076390905, 0.4671244194660692, 0.4671244194660692, 0.4671244194660692, 0.5045918281190267, 0.5045918281190267, 0.5045918281190267, 0.3509994412720403, 0.3509994412720403, 0.3509994412720403, 0.07257488906637022, 0.07257488906637022, 0.07257488906637022, 0.12356996615542681, 0.12356996615542681, 0.12356996615542681, 0.1335001189901539, 0.1335001189901539, 0.1335001189901539, 0.19426965622165127, 0.19426965622165127, 0.19426965622165127, 0.20234185013691963, 0.20234185013691963, 0.20234185013691963, 0.2516400816034029, 0.2516400816034029, 0.2516400816034029, 0.22961330330537189, 0.22961330330537189, 0.22961330330537189, 0.24167076151018063, 0.24167076151018063, 0.24167076151018063, 0.1536442870904673, 0.1536442870904673, 0.1536442870904673, 0.22373106428887368, 0.22373106428887368, 0.22373106428887368, 0.14764796790191415, 0.14764796790191415, 0.14764796790191415, 0.1303138991730377, 0.1303138991730377, 0.1303138991730377, 0.23052497567089292, 0.23052497567089292, 0.23052497567089292, 0.20104779533348627, 0.20104779533348627, 0.20104779533348627, 0.17029465856323533, 0.17029465856323533, 0.17029465856323533, 0.1820730591843247, 0.1820730591843247, 0.1820730591843247, 0.2053444456554102, 0.2053444456554102, 0.2053444456554102, 0.2130250730216161, 0.2130250730216161, 0.2130250730216161, 0.5094102849266552, 0.5094102849266552, 0.5094102849266552, 0.15877051354414273, 0.15877051354414273, 0.15877051354414273, 0.17640780579590787, 0.17640780579590787, 0.17640780579590787, 0.16913806230392836, 0.16913806230392836, 0.16913806230392836, 0.21300540126855982, 0.21300540126855982, 0.21300540126855982, 0.15678109906489357, 0.15678109906489357, 0.15678109906489357, 0.23658620893611437, 0.23658620893611437, 0.23658620893611437, 0.24289835742278865, 0.24289835742278865, 0.24289835742278865, 0.24137359473521847, 0.24137359473521847, 0.24137359473521847, 0.08766623665503481, 0.08766623665503481, 0.08766623665503481, 0.10105819935769578, 0.10105819935769578, 0.10105819935769578, 0.10281919523859473, 0.10281919523859473, 0.10281919523859473]}, "mutation_prompt": null}
{"id": "ba24c378-b9aa-4c53-823f-1f503269d78e", "solution": "import numpy as np\n\nclass PSO_SA_Optimizer:\n    def __init__(self, budget, dim, pop_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.w = 0.5  # Inertia weight\n        self.c1 = 1.5  # Cognitive parameter\n        self.c2 = 1.5  # Social parameter\n        self.temp = 1.0  # Initial temperature for SA\n\n    def __call__(self, func):\n        np.random.seed(42)  # For reproducibility\n        \n        # Initialize particles\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.array([func(pos) for pos in positions])\n        global_best_position = personal_best_positions[np.argmin(personal_best_scores)]\n        global_best_score = np.min(personal_best_scores)\n        \n        eval_count = self.pop_size\n        \n        while eval_count < self.budget:\n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                velocities[i] = (self.w * velocities[i]\n                                 + self.c1 * r1 * (personal_best_positions[i] - positions[i])\n                                 + self.c2 * r2 * (global_best_position - positions[i]))\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], self.lower_bound, self.upper_bound)\n\n                # Evaluate the new position\n                score = func(positions[i])\n                eval_count += 1\n                if eval_count >= self.budget:\n                    break\n\n                # Update personal best\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n\n                # Update global best\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = positions[i]\n\n                # Simulated Annealing Acceptance\n                if np.random.rand() < np.exp(-(score - global_best_score) / self.temp):\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n                    if score < global_best_score:\n                        global_best_score = score\n                        global_best_position = positions[i]\n\n            # Temperature cooling schedule\n            self.temp *= 0.95\n\n        return global_best_position, global_best_score", "name": "PSO_SA_Optimizer", "description": "A novel hybrid Particle Swarm Optimization with Simulated Annealing (PSO-SA) that leverages swarm intelligence for exploration and a cooling schedule for exploitation to balance local and global search effectively.", "configspace": "", "generation": 1, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "3acdc6cd-ddc7-43f8-8c2d-464d0fe2b5e1", "metadata": {"aucs": [0.33180480772342813, 0.33180480772342813, 0.33180480772342813, 0.36288488077686154, 0.36288488077686154, 0.36288488077686154, 0.3524736208739604, 0.3524736208739604, 0.3524736208739604, 0.019804201973858238, 0.019804201973858238, 0.019804201973858238, 0.12705097111581132, 0.12705097111581132, 0.12705097111581132, 0.005070909016537395, 0.005070909016537395, 0.005070909016537395, 0.14781258849985157, 0.14781258849985157, 0.14781258849985157, 0.0739480251375022, 0.0739480251375022, 0.0739480251375022, 0.07073332447140823, 0.07073332447140823, 0.07073332447140823, 0.06879204837801989, 0.06879204837801989, 0.06879204837801989, 0.09331936714507605, 0.09331936714507605, 0.09331936714507605, 0.06733613886654477, 0.06733613886654477, 0.06733613886654477, 0.9903031515539998, 0.9903031515539998, 0.9903031515539998, 0.9947192478610297, 0.9947192478610297, 0.9947192478610297, 0.9890387250111802, 0.9890387250111802, 0.9890387250111802, 0.30709987270664374, 0.30709987270664374, 0.30709987270664374, 0.15530680417395537, 0.15530680417395537, 0.15530680417395537, 0.3394415208220134, 0.3394415208220134, 0.3394415208220134, 0.24298728017591742, 0.24298728017591742, 0.24298728017591742, 0.16161800043553431, 0.16161800043553431, 0.16161800043553431, 0.1173217364685677, 0.1173217364685677, 0.1173217364685677, 0.19322942770854046, 0.19322942770854046, 0.19322942770854046, 0.2983965720491797, 0.2983965720491797, 0.2983965720491797, 0.1317150895951441, 0.1317150895951441, 0.1317150895951441, 0.17800730831532308, 0.17800730831532308, 0.17800730831532308, 0.13089013401344585, 0.13089013401344585, 0.13089013401344585, 0.13783319048218146, 0.13783319048218146, 0.13783319048218146, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.06275889367695586, 0.06275889367695586, 0.06275889367695586, 0.0921172203415126, 0.0921172203415126, 0.0921172203415126, 0.07986567406052014, 0.07986567406052014, 0.07986567406052014, 0.09738886712669792, 0.09738886712669792, 0.09738886712669792, 0.16643999934033182, 0.16643999934033182, 0.16643999934033182, 0.07342646552950849, 0.07342646552950849, 0.07342646552950849, 0.12758519974444626, 0.12758519974444626, 0.12758519974444626, 0.08000095052500733, 0.08000095052500733, 0.08000095052500733, 0.14453183125769864, 0.14453183125769864, 0.14453183125769864, 0.03980922322614411, 0.03980922322614411, 0.03980922322614411, 0.08462753076390905, 0.08462753076390905, 0.08462753076390905, 0.4671244194660692, 0.4671244194660692, 0.4671244194660692, 0.5045918281190267, 0.5045918281190267, 0.5045918281190267, 0.3509994412720403, 0.3509994412720403, 0.3509994412720403, 0.07257488906637022, 0.07257488906637022, 0.07257488906637022, 0.12356996615542681, 0.12356996615542681, 0.12356996615542681, 0.1335001189901539, 0.1335001189901539, 0.1335001189901539, 0.19426965622165127, 0.19426965622165127, 0.19426965622165127, 0.20234185013691963, 0.20234185013691963, 0.20234185013691963, 0.2516400816034029, 0.2516400816034029, 0.2516400816034029, 0.22961330330537189, 0.22961330330537189, 0.22961330330537189, 0.24167076151018063, 0.24167076151018063, 0.24167076151018063, 0.1536442870904673, 0.1536442870904673, 0.1536442870904673, 0.22373106428887368, 0.22373106428887368, 0.22373106428887368, 0.14764796790191415, 0.14764796790191415, 0.14764796790191415, 0.1303138991730377, 0.1303138991730377, 0.1303138991730377, 0.23052497567089292, 0.23052497567089292, 0.23052497567089292, 0.20104779533348627, 0.20104779533348627, 0.20104779533348627, 0.17029465856323533, 0.17029465856323533, 0.17029465856323533, 0.1820730591843247, 0.1820730591843247, 0.1820730591843247, 0.2053444456554102, 0.2053444456554102, 0.2053444456554102, 0.2130250730216161, 0.2130250730216161, 0.2130250730216161, 0.5094102849266552, 0.5094102849266552, 0.5094102849266552, 0.15877051354414273, 0.15877051354414273, 0.15877051354414273, 0.17640780579590787, 0.17640780579590787, 0.17640780579590787, 0.16913806230392836, 0.16913806230392836, 0.16913806230392836, 0.21300540126855982, 0.21300540126855982, 0.21300540126855982, 0.15678109906489357, 0.15678109906489357, 0.15678109906489357, 0.23658620893611437, 0.23658620893611437, 0.23658620893611437, 0.24289835742278865, 0.24289835742278865, 0.24289835742278865, 0.24137359473521847, 0.24137359473521847, 0.24137359473521847, 0.08766623665503481, 0.08766623665503481, 0.08766623665503481, 0.10105819935769578, 0.10105819935769578, 0.10105819935769578, 0.10281919523859473, 0.10281919523859473, 0.10281919523859473]}, "mutation_prompt": null}
{"id": "8674610b-db06-483a-8132-28e8a863b3a7", "solution": "import numpy as np\n\nclass PSO_SA_Optimizer:\n    def __init__(self, budget, dim, pop_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.w = 0.5  # Inertia weight\n        self.c1 = 1.5  # Cognitive parameter\n        self.c2 = 1.5  # Social parameter\n        self.temp = 1.0  # Initial temperature for SA\n\n    def __call__(self, func):\n        np.random.seed(42)  # For reproducibility\n        \n        # Initialize particles\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.array([func(pos) for pos in positions])\n        global_best_position = personal_best_positions[np.argmin(personal_best_scores)]\n        global_best_score = np.min(personal_best_scores)\n        \n        eval_count = self.pop_size\n        \n        while eval_count < self.budget:\n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                velocities[i] = (self.w * velocities[i]\n                                 + self.c1 * r1 * (personal_best_positions[i] - positions[i])\n                                 + self.c2 * r2 * (global_best_position - positions[i]))\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], self.lower_bound, self.upper_bound)\n\n                # Evaluate the new position\n                score = func(positions[i])\n                eval_count += 1\n                if eval_count >= self.budget:\n                    break\n\n                # Update personal best\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n\n                # Update global best\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = positions[i]\n\n                # Simulated Annealing Acceptance\n                if np.random.rand() < np.exp(-(score - global_best_score) / self.temp):\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n                    if score < global_best_score:\n                        global_best_score = score\n                        global_best_position = positions[i]\n\n            # Temperature cooling schedule\n            self.temp *= 0.95\n\n        return global_best_position, global_best_score", "name": "PSO_SA_Optimizer", "description": "A novel hybrid Particle Swarm Optimization with Simulated Annealing (PSO-SA) that leverages swarm intelligence for exploration and a cooling schedule for exploitation to balance local and global search effectively.", "configspace": "", "generation": 1, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "3acdc6cd-ddc7-43f8-8c2d-464d0fe2b5e1", "metadata": {"aucs": [0.33180480772342813, 0.33180480772342813, 0.33180480772342813, 0.36288488077686154, 0.36288488077686154, 0.36288488077686154, 0.3524736208739604, 0.3524736208739604, 0.3524736208739604, 0.019804201973858238, 0.019804201973858238, 0.019804201973858238, 0.12705097111581132, 0.12705097111581132, 0.12705097111581132, 0.005070909016537395, 0.005070909016537395, 0.005070909016537395, 0.14781258849985157, 0.14781258849985157, 0.14781258849985157, 0.0739480251375022, 0.0739480251375022, 0.0739480251375022, 0.07073332447140823, 0.07073332447140823, 0.07073332447140823, 0.06879204837801989, 0.06879204837801989, 0.06879204837801989, 0.09331936714507605, 0.09331936714507605, 0.09331936714507605, 0.06733613886654477, 0.06733613886654477, 0.06733613886654477, 0.9903031515539998, 0.9903031515539998, 0.9903031515539998, 0.9947192478610297, 0.9947192478610297, 0.9947192478610297, 0.9890387250111802, 0.9890387250111802, 0.9890387250111802, 0.30709987270664374, 0.30709987270664374, 0.30709987270664374, 0.15530680417395537, 0.15530680417395537, 0.15530680417395537, 0.3394415208220134, 0.3394415208220134, 0.3394415208220134, 0.24298728017591742, 0.24298728017591742, 0.24298728017591742, 0.16161800043553431, 0.16161800043553431, 0.16161800043553431, 0.1173217364685677, 0.1173217364685677, 0.1173217364685677, 0.19322942770854046, 0.19322942770854046, 0.19322942770854046, 0.2983965720491797, 0.2983965720491797, 0.2983965720491797, 0.1317150895951441, 0.1317150895951441, 0.1317150895951441, 0.17800730831532308, 0.17800730831532308, 0.17800730831532308, 0.13089013401344585, 0.13089013401344585, 0.13089013401344585, 0.13783319048218146, 0.13783319048218146, 0.13783319048218146, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.06275889367695586, 0.06275889367695586, 0.06275889367695586, 0.0921172203415126, 0.0921172203415126, 0.0921172203415126, 0.07986567406052014, 0.07986567406052014, 0.07986567406052014, 0.09738886712669792, 0.09738886712669792, 0.09738886712669792, 0.16643999934033182, 0.16643999934033182, 0.16643999934033182, 0.07342646552950849, 0.07342646552950849, 0.07342646552950849, 0.12758519974444626, 0.12758519974444626, 0.12758519974444626, 0.08000095052500733, 0.08000095052500733, 0.08000095052500733, 0.14453183125769864, 0.14453183125769864, 0.14453183125769864, 0.03980922322614411, 0.03980922322614411, 0.03980922322614411, 0.08462753076390905, 0.08462753076390905, 0.08462753076390905, 0.4671244194660692, 0.4671244194660692, 0.4671244194660692, 0.5045918281190267, 0.5045918281190267, 0.5045918281190267, 0.3509994412720403, 0.3509994412720403, 0.3509994412720403, 0.07257488906637022, 0.07257488906637022, 0.07257488906637022, 0.12356996615542681, 0.12356996615542681, 0.12356996615542681, 0.1335001189901539, 0.1335001189901539, 0.1335001189901539, 0.19426965622165127, 0.19426965622165127, 0.19426965622165127, 0.20234185013691963, 0.20234185013691963, 0.20234185013691963, 0.2516400816034029, 0.2516400816034029, 0.2516400816034029, 0.22961330330537189, 0.22961330330537189, 0.22961330330537189, 0.24167076151018063, 0.24167076151018063, 0.24167076151018063, 0.1536442870904673, 0.1536442870904673, 0.1536442870904673, 0.22373106428887368, 0.22373106428887368, 0.22373106428887368, 0.14764796790191415, 0.14764796790191415, 0.14764796790191415, 0.1303138991730377, 0.1303138991730377, 0.1303138991730377, 0.23052497567089292, 0.23052497567089292, 0.23052497567089292, 0.20104779533348627, 0.20104779533348627, 0.20104779533348627, 0.17029465856323533, 0.17029465856323533, 0.17029465856323533, 0.1820730591843247, 0.1820730591843247, 0.1820730591843247, 0.2053444456554102, 0.2053444456554102, 0.2053444456554102, 0.2130250730216161, 0.2130250730216161, 0.2130250730216161, 0.5094102849266552, 0.5094102849266552, 0.5094102849266552, 0.15877051354414273, 0.15877051354414273, 0.15877051354414273, 0.17640780579590787, 0.17640780579590787, 0.17640780579590787, 0.16913806230392836, 0.16913806230392836, 0.16913806230392836, 0.21300540126855982, 0.21300540126855982, 0.21300540126855982, 0.15678109906489357, 0.15678109906489357, 0.15678109906489357, 0.23658620893611437, 0.23658620893611437, 0.23658620893611437, 0.24289835742278865, 0.24289835742278865, 0.24289835742278865, 0.24137359473521847, 0.24137359473521847, 0.24137359473521847, 0.08766623665503481, 0.08766623665503481, 0.08766623665503481, 0.10105819935769578, 0.10105819935769578, 0.10105819935769578, 0.10281919523859473, 0.10281919523859473, 0.10281919523859473]}, "mutation_prompt": null}
{"id": "5ebada43-9f9e-481c-8877-23b2180c560d", "solution": "import numpy as np\n\nclass PSO_SA_Optimizer:\n    def __init__(self, budget, dim, pop_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.w = 0.5  # Inertia weight\n        self.c1 = 1.5  # Cognitive parameter\n        self.c2 = 1.5  # Social parameter\n        self.temp = 1.0  # Initial temperature for SA\n\n    def __call__(self, func):\n        np.random.seed(42)  # For reproducibility\n        \n        # Initialize particles\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.array([func(pos) for pos in positions])\n        global_best_position = personal_best_positions[np.argmin(personal_best_scores)]\n        global_best_score = np.min(personal_best_scores)\n        \n        eval_count = self.pop_size\n        \n        while eval_count < self.budget:\n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                velocities[i] = (self.w * velocities[i]\n                                 + self.c1 * r1 * (personal_best_positions[i] - positions[i])\n                                 + self.c2 * r2 * (global_best_position - positions[i]))\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], self.lower_bound, self.upper_bound)\n\n                # Evaluate the new position\n                score = func(positions[i])\n                eval_count += 1\n                if eval_count >= self.budget:\n                    break\n\n                # Update personal best\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n\n                # Update global best\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = positions[i]\n\n                # Simulated Annealing Acceptance\n                if np.random.rand() < np.exp(-(score - global_best_score) / self.temp):\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n                    if score < global_best_score:\n                        global_best_score = score\n                        global_best_position = positions[i]\n\n            # Temperature cooling schedule\n            self.temp *= 0.95\n\n        return global_best_position, global_best_score", "name": "PSO_SA_Optimizer", "description": "A novel hybrid Particle Swarm Optimization with Simulated Annealing (PSO-SA) that leverages swarm intelligence for exploration and a cooling schedule for exploitation to balance local and global search effectively.", "configspace": "", "generation": 1, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "3acdc6cd-ddc7-43f8-8c2d-464d0fe2b5e1", "metadata": {"aucs": [0.33180480772342813, 0.33180480772342813, 0.33180480772342813, 0.36288488077686154, 0.36288488077686154, 0.36288488077686154, 0.3524736208739604, 0.3524736208739604, 0.3524736208739604, 0.019804201973858238, 0.019804201973858238, 0.019804201973858238, 0.12705097111581132, 0.12705097111581132, 0.12705097111581132, 0.005070909016537395, 0.005070909016537395, 0.005070909016537395, 0.14781258849985157, 0.14781258849985157, 0.14781258849985157, 0.0739480251375022, 0.0739480251375022, 0.0739480251375022, 0.07073332447140823, 0.07073332447140823, 0.07073332447140823, 0.06879204837801989, 0.06879204837801989, 0.06879204837801989, 0.09331936714507605, 0.09331936714507605, 0.09331936714507605, 0.06733613886654477, 0.06733613886654477, 0.06733613886654477, 0.9903031515539998, 0.9903031515539998, 0.9903031515539998, 0.9947192478610297, 0.9947192478610297, 0.9947192478610297, 0.9890387250111802, 0.9890387250111802, 0.9890387250111802, 0.30709987270664374, 0.30709987270664374, 0.30709987270664374, 0.15530680417395537, 0.15530680417395537, 0.15530680417395537, 0.3394415208220134, 0.3394415208220134, 0.3394415208220134, 0.24298728017591742, 0.24298728017591742, 0.24298728017591742, 0.16161800043553431, 0.16161800043553431, 0.16161800043553431, 0.1173217364685677, 0.1173217364685677, 0.1173217364685677, 0.19322942770854046, 0.19322942770854046, 0.19322942770854046, 0.2983965720491797, 0.2983965720491797, 0.2983965720491797, 0.1317150895951441, 0.1317150895951441, 0.1317150895951441, 0.17800730831532308, 0.17800730831532308, 0.17800730831532308, 0.13089013401344585, 0.13089013401344585, 0.13089013401344585, 0.13783319048218146, 0.13783319048218146, 0.13783319048218146, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.06275889367695586, 0.06275889367695586, 0.06275889367695586, 0.0921172203415126, 0.0921172203415126, 0.0921172203415126, 0.07986567406052014, 0.07986567406052014, 0.07986567406052014, 0.09738886712669792, 0.09738886712669792, 0.09738886712669792, 0.16643999934033182, 0.16643999934033182, 0.16643999934033182, 0.07342646552950849, 0.07342646552950849, 0.07342646552950849, 0.12758519974444626, 0.12758519974444626, 0.12758519974444626, 0.08000095052500733, 0.08000095052500733, 0.08000095052500733, 0.14453183125769864, 0.14453183125769864, 0.14453183125769864, 0.03980922322614411, 0.03980922322614411, 0.03980922322614411, 0.08462753076390905, 0.08462753076390905, 0.08462753076390905, 0.4671244194660692, 0.4671244194660692, 0.4671244194660692, 0.5045918281190267, 0.5045918281190267, 0.5045918281190267, 0.3509994412720403, 0.3509994412720403, 0.3509994412720403, 0.07257488906637022, 0.07257488906637022, 0.07257488906637022, 0.12356996615542681, 0.12356996615542681, 0.12356996615542681, 0.1335001189901539, 0.1335001189901539, 0.1335001189901539, 0.19426965622165127, 0.19426965622165127, 0.19426965622165127, 0.20234185013691963, 0.20234185013691963, 0.20234185013691963, 0.2516400816034029, 0.2516400816034029, 0.2516400816034029, 0.22961330330537189, 0.22961330330537189, 0.22961330330537189, 0.24167076151018063, 0.24167076151018063, 0.24167076151018063, 0.1536442870904673, 0.1536442870904673, 0.1536442870904673, 0.22373106428887368, 0.22373106428887368, 0.22373106428887368, 0.14764796790191415, 0.14764796790191415, 0.14764796790191415, 0.1303138991730377, 0.1303138991730377, 0.1303138991730377, 0.23052497567089292, 0.23052497567089292, 0.23052497567089292, 0.20104779533348627, 0.20104779533348627, 0.20104779533348627, 0.17029465856323533, 0.17029465856323533, 0.17029465856323533, 0.1820730591843247, 0.1820730591843247, 0.1820730591843247, 0.2053444456554102, 0.2053444456554102, 0.2053444456554102, 0.2130250730216161, 0.2130250730216161, 0.2130250730216161, 0.5094102849266552, 0.5094102849266552, 0.5094102849266552, 0.15877051354414273, 0.15877051354414273, 0.15877051354414273, 0.17640780579590787, 0.17640780579590787, 0.17640780579590787, 0.16913806230392836, 0.16913806230392836, 0.16913806230392836, 0.21300540126855982, 0.21300540126855982, 0.21300540126855982, 0.15678109906489357, 0.15678109906489357, 0.15678109906489357, 0.23658620893611437, 0.23658620893611437, 0.23658620893611437, 0.24289835742278865, 0.24289835742278865, 0.24289835742278865, 0.24137359473521847, 0.24137359473521847, 0.24137359473521847, 0.08766623665503481, 0.08766623665503481, 0.08766623665503481, 0.10105819935769578, 0.10105819935769578, 0.10105819935769578, 0.10281919523859473, 0.10281919523859473, 0.10281919523859473]}, "mutation_prompt": null}
{"id": "0411155c-9b0a-4e86-94b2-317eaab11b15", "solution": "import numpy as np\n\nclass PSO_SA_Optimizer_Enhanced:\n    def __init__(self, budget, dim, pop_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.w_max = 0.9  # Maximum inertia weight\n        self.w_min = 0.4  # Minimum inertia weight\n        self.c1 = 1.5  # Cognitive parameter\n        self.c2 = 1.5  # Social parameter\n        self.temp = 1.0  # Initial temperature for SA\n\n    def __call__(self, func):\n        np.random.seed(42)  # For reproducibility\n        \n        # Initialize particles\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.array([func(pos) for pos in positions])\n        global_best_position = personal_best_positions[np.argmin(personal_best_scores)]\n        global_best_score = np.min(personal_best_scores)\n        \n        eval_count = self.pop_size\n        \n        while eval_count < self.budget:\n            w = self.w_max - ((self.w_max - self.w_min) * (eval_count / self.budget))\n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                velocities[i] = (w * velocities[i]\n                                 + self.c1 * r1 * (personal_best_positions[i] - positions[i])\n                                 + self.c2 * r2 * (global_best_position - positions[i]))\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], self.lower_bound, self.upper_bound)\n\n                # Evaluate the new position\n                score = func(positions[i])\n                eval_count += 1\n                if eval_count >= self.budget:\n                    break\n\n                # Update personal best\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n\n                # Update global best\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = positions[i]\n\n                # Simulated Annealing Acceptance\n                if np.random.rand() < np.exp(-(score - global_best_score) / self.temp):\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n                    if score < global_best_score:\n                        global_best_score = score\n                        global_best_position = positions[i]\n\n            # Temperature cooling schedule\n            self.temp *= 0.95\n\n        return global_best_position, global_best_score", "name": "PSO_SA_Optimizer_Enhanced", "description": "Enhanced hybrid Particle Swarm Optimization with Simulated Annealing (PSO-SA) and adaptive inertia weight for improved convergence.", "configspace": "", "generation": 5, "fitness": 0.23407423169568736, "feedback": "The algorithm PSO_SA_Optimizer_Enhanced got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.23 with standard deviation 0.21.", "error": "", "parent_id": "3acdc6cd-ddc7-43f8-8c2d-464d0fe2b5e1", "metadata": {"aucs": [0.5541935495271029, 0.5541935495271029, 0.5541935495271029, 0.5472676162748873, 0.5472676162748873, 0.5472676162748873, 0.4995050001955086, 0.4995050001955086, 0.4995050001955086, 0.37677698123758485, 0.37677698123758485, 0.37677698123758485, 0.2602571254910885, 0.2602571254910885, 0.2602571254910885, 0.003330909247681313, 0.003330909247681313, 0.003330909247681313, 0.16044625244716004, 0.16044625244716004, 0.16044625244716004, 0.06165347877030414, 0.06165347877030414, 0.06165347877030414, 0.11895346010869268, 0.11895346010869268, 0.11895346010869268, 0.07493708294659229, 0.07493708294659229, 0.07493708294659229, 0.1223881702479801, 0.1223881702479801, 0.1223881702479801, 0.11364990543254094, 0.11364990543254094, 0.11364990543254094, 0.9917969237266142, 0.9917969237266142, 0.9917969237266142, 0.9947464319524106, 0.9947464319524106, 0.9947464319524106, 0.9923522331758592, 0.9923522331758592, 0.9923522331758592, 0.23292936537637376, 0.23292936537637376, 0.23292936537637376, 0.3241275789159769, 0.3241275789159769, 0.3241275789159769, 0.368066327161232, 0.368066327161232, 0.368066327161232, 0.3151000101329813, 0.3151000101329813, 0.3151000101329813, 0.15819099217998733, 0.15819099217998733, 0.15819099217998733, 0.1161189521585051, 0.1161189521585051, 0.1161189521585051, 0.39416284155128345, 0.39416284155128345, 0.39416284155128345, 0.12212977487851262, 0.12212977487851262, 0.12212977487851262, 0.2644190003152572, 0.2644190003152572, 0.2644190003152572, 0.35801657538571496, 0.35801657538571496, 0.35801657538571496, 0.12297914518591568, 0.12297914518591568, 0.12297914518591568, 0.1960278080415332, 0.1960278080415332, 0.1960278080415332, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.084775675890568, 0.084775675890568, 0.084775675890568, 0.3202396802670431, 0.3202396802670431, 0.3202396802670431, 0.07097896700228357, 0.07097896700228357, 0.07097896700228357, 0.08885677525536484, 0.08885677525536484, 0.08885677525536484, 0.03314189301901549, 0.03314189301901549, 0.03314189301901549, 0.04537410703272038, 0.04537410703272038, 0.04537410703272038, 0.03922412471399417, 0.03922412471399417, 0.03922412471399417, 0.1155219345450258, 0.1155219345450258, 0.1155219345450258, 0.034755019981584145, 0.034755019981584145, 0.034755019981584145, 0.07102106026707755, 0.07102106026707755, 0.07102106026707755, 0.5006794078134473, 0.5006794078134473, 0.5006794078134473, 0.4561303012753095, 0.4561303012753095, 0.4561303012753095, 0.5158501654337682, 0.5158501654337682, 0.5158501654337682, 0.11653379618486104, 0.11653379618486104, 0.11653379618486104, 0.07532313407863034, 0.07532313407863034, 0.07532313407863034, 0.12449952434684819, 0.12449952434684819, 0.12449952434684819, 0.35694259995702704, 0.35694259995702704, 0.35694259995702704, 0.17193702509720898, 0.17193702509720898, 0.17193702509720898, 0.13374219188996994, 0.13374219188996994, 0.13374219188996994, 0.2475515055418741, 0.2475515055418741, 0.2475515055418741, 0.2277427774917169, 0.2277427774917169, 0.2277427774917169, 0.16379641556242075, 0.16379641556242075, 0.16379641556242075, 0.20698761037109803, 0.20698761037109803, 0.20698761037109803, 0.2166739607932202, 0.2166739607932202, 0.2166739607932202, 0.12743080958398967, 0.12743080958398967, 0.12743080958398967, 0.20836779232376135, 0.20836779232376135, 0.20836779232376135, 0.2218463570025747, 0.2218463570025747, 0.2218463570025747, 0.2266211048527721, 0.2266211048527721, 0.2266211048527721, 0.19974449838695618, 0.19974449838695618, 0.19974449838695618, 0.19798302527326395, 0.19798302527326395, 0.19798302527326395, 0.20921222160855857, 0.20921222160855857, 0.20921222160855857, 0.5247151717610008, 0.5247151717610008, 0.5247151717610008, 0.1654403704961498, 0.1654403704961498, 0.1654403704961498, 0.16601278941076159, 0.16601278941076159, 0.16601278941076159, 0.16848472192743125, 0.16848472192743125, 0.16848472192743125, 0.2061724960517588, 0.2061724960517588, 0.2061724960517588, 0.14991930742146542, 0.14991930742146542, 0.14991930742146542, 0.17303961150351843, 0.17303961150351843, 0.17303961150351843, 0.19293796404668373, 0.19293796404668373, 0.19293796404668373, 0.21318655457744973, 0.21318655457744973, 0.21318655457744973, 0.09952291400030866, 0.09952291400030866, 0.09952291400030866, 0.06908111797165462, 0.06908111797165462, 0.06908111797165462, 0.07062270801207382, 0.07062270801207382, 0.07062270801207382]}, "mutation_prompt": null}
{"id": "655ab4e0-7e36-4c6c-8c7e-fc24777d5bda", "solution": "import numpy as np\n\nclass PSO_SA_Optimizer_Enhanced:\n    def __init__(self, budget, dim, pop_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.w_max = 0.9  # Maximum inertia weight\n        self.w_min = 0.4  # Minimum inertia weight\n        self.c1 = 1.5  # Cognitive parameter\n        self.c2 = 1.5  # Social parameter\n        self.temp = 1.0  # Initial temperature for SA\n\n    def __call__(self, func):\n        np.random.seed(42)  # For reproducibility\n        \n        # Initialize particles\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.array([func(pos) for pos in positions])\n        global_best_position = personal_best_positions[np.argmin(personal_best_scores)]\n        global_best_score = np.min(personal_best_scores)\n        \n        eval_count = self.pop_size\n        \n        while eval_count < self.budget:\n            w = self.w_max - ((self.w_max - self.w_min) * (eval_count / self.budget))\n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                velocities[i] = (w * velocities[i]\n                                 + self.c1 * r1 * (personal_best_positions[i] - positions[i])\n                                 + self.c2 * r2 * (global_best_position - positions[i]))\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], self.lower_bound, self.upper_bound)\n\n                # Evaluate the new position\n                score = func(positions[i])\n                eval_count += 1\n                if eval_count >= self.budget:\n                    break\n\n                # Update personal best\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n\n                # Update global best\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = positions[i]\n\n                # Simulated Annealing Acceptance\n                if np.random.rand() < np.exp(-(score - global_best_score) / self.temp):\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n                    if score < global_best_score:\n                        global_best_score = score\n                        global_best_position = positions[i]\n\n            # Temperature cooling schedule\n            self.temp *= 0.95\n\n        return global_best_position, global_best_score", "name": "PSO_SA_Optimizer_Enhanced", "description": "Enhanced hybrid Particle Swarm Optimization with Simulated Annealing (PSO-SA) and adaptive inertia weight for improved convergence.", "configspace": "", "generation": 6, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "0411155c-9b0a-4e86-94b2-317eaab11b15", "metadata": {"aucs": [0.5541935495271029, 0.5541935495271029, 0.5541935495271029, 0.5472676162748873, 0.5472676162748873, 0.5472676162748873, 0.4995050001955086, 0.4995050001955086, 0.4995050001955086, 0.37677698123758485, 0.37677698123758485, 0.37677698123758485, 0.2602571254910885, 0.2602571254910885, 0.2602571254910885, 0.003330909247681313, 0.003330909247681313, 0.003330909247681313, 0.16044625244716004, 0.16044625244716004, 0.16044625244716004, 0.06165347877030414, 0.06165347877030414, 0.06165347877030414, 0.11895346010869268, 0.11895346010869268, 0.11895346010869268, 0.07493708294659229, 0.07493708294659229, 0.07493708294659229, 0.1223881702479801, 0.1223881702479801, 0.1223881702479801, 0.11364990543254094, 0.11364990543254094, 0.11364990543254094, 0.9917969237266142, 0.9917969237266142, 0.9917969237266142, 0.9947464319524106, 0.9947464319524106, 0.9947464319524106, 0.9923522331758592, 0.9923522331758592, 0.9923522331758592, 0.23292936537637376, 0.23292936537637376, 0.23292936537637376, 0.3241275789159769, 0.3241275789159769, 0.3241275789159769, 0.368066327161232, 0.368066327161232, 0.368066327161232, 0.3151000101329813, 0.3151000101329813, 0.3151000101329813, 0.15819099217998733, 0.15819099217998733, 0.15819099217998733, 0.1161189521585051, 0.1161189521585051, 0.1161189521585051, 0.39416284155128345, 0.39416284155128345, 0.39416284155128345, 0.12212977487851262, 0.12212977487851262, 0.12212977487851262, 0.2644190003152572, 0.2644190003152572, 0.2644190003152572, 0.35801657538571496, 0.35801657538571496, 0.35801657538571496, 0.12297914518591568, 0.12297914518591568, 0.12297914518591568, 0.1960278080415332, 0.1960278080415332, 0.1960278080415332, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.084775675890568, 0.084775675890568, 0.084775675890568, 0.3202396802670431, 0.3202396802670431, 0.3202396802670431, 0.07097896700228357, 0.07097896700228357, 0.07097896700228357, 0.08885677525536484, 0.08885677525536484, 0.08885677525536484, 0.03314189301901549, 0.03314189301901549, 0.03314189301901549, 0.04537410703272038, 0.04537410703272038, 0.04537410703272038, 0.03922412471399417, 0.03922412471399417, 0.03922412471399417, 0.1155219345450258, 0.1155219345450258, 0.1155219345450258, 0.034755019981584145, 0.034755019981584145, 0.034755019981584145, 0.07102106026707755, 0.07102106026707755, 0.07102106026707755, 0.5006794078134473, 0.5006794078134473, 0.5006794078134473, 0.4561303012753095, 0.4561303012753095, 0.4561303012753095, 0.5158501654337682, 0.5158501654337682, 0.5158501654337682, 0.11653379618486104, 0.11653379618486104, 0.11653379618486104, 0.07532313407863034, 0.07532313407863034, 0.07532313407863034, 0.12449952434684819, 0.12449952434684819, 0.12449952434684819, 0.35694259995702704, 0.35694259995702704, 0.35694259995702704, 0.17193702509720898, 0.17193702509720898, 0.17193702509720898, 0.13374219188996994, 0.13374219188996994, 0.13374219188996994, 0.2475515055418741, 0.2475515055418741, 0.2475515055418741, 0.2277427774917169, 0.2277427774917169, 0.2277427774917169, 0.16379641556242075, 0.16379641556242075, 0.16379641556242075, 0.20698761037109803, 0.20698761037109803, 0.20698761037109803, 0.2166739607932202, 0.2166739607932202, 0.2166739607932202, 0.12743080958398967, 0.12743080958398967, 0.12743080958398967, 0.20836779232376135, 0.20836779232376135, 0.20836779232376135, 0.2218463570025747, 0.2218463570025747, 0.2218463570025747, 0.2266211048527721, 0.2266211048527721, 0.2266211048527721, 0.19974449838695618, 0.19974449838695618, 0.19974449838695618, 0.19798302527326395, 0.19798302527326395, 0.19798302527326395, 0.20921222160855857, 0.20921222160855857, 0.20921222160855857, 0.5247151717610008, 0.5247151717610008, 0.5247151717610008, 0.1654403704961498, 0.1654403704961498, 0.1654403704961498, 0.16601278941076159, 0.16601278941076159, 0.16601278941076159, 0.16848472192743125, 0.16848472192743125, 0.16848472192743125, 0.2061724960517588, 0.2061724960517588, 0.2061724960517588, 0.14991930742146542, 0.14991930742146542, 0.14991930742146542, 0.17303961150351843, 0.17303961150351843, 0.17303961150351843, 0.19293796404668373, 0.19293796404668373, 0.19293796404668373, 0.21318655457744973, 0.21318655457744973, 0.21318655457744973, 0.09952291400030866, 0.09952291400030866, 0.09952291400030866, 0.06908111797165462, 0.06908111797165462, 0.06908111797165462, 0.07062270801207382, 0.07062270801207382, 0.07062270801207382]}, "mutation_prompt": null}
{"id": "0a53cf25-3f7e-43d9-8441-c5543fcf9926", "solution": "import numpy as np\n\nclass PSO_SA_Optimizer_Enhanced:\n    def __init__(self, budget, dim, pop_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.w_max = 0.9  # Maximum inertia weight\n        self.w_min = 0.4  # Minimum inertia weight\n        self.c1 = 1.5  # Cognitive parameter\n        self.c2 = 1.5  # Social parameter\n        self.temp = 1.0  # Initial temperature for SA\n\n    def __call__(self, func):\n        np.random.seed(42)  # For reproducibility\n        \n        # Initialize particles\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.array([func(pos) for pos in positions])\n        global_best_position = personal_best_positions[np.argmin(personal_best_scores)]\n        global_best_score = np.min(personal_best_scores)\n        \n        eval_count = self.pop_size\n        \n        while eval_count < self.budget:\n            w = self.w_max - ((self.w_max - self.w_min) * (eval_count / self.budget))\n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                velocities[i] = (w * velocities[i]\n                                 + self.c1 * r1 * (personal_best_positions[i] - positions[i])\n                                 + self.c2 * r2 * (global_best_position - positions[i]))\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], self.lower_bound, self.upper_bound)\n\n                # Evaluate the new position\n                score = func(positions[i])\n                eval_count += 1\n                if eval_count >= self.budget:\n                    break\n\n                # Update personal best\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n\n                # Update global best\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = positions[i]\n\n                # Simulated Annealing Acceptance\n                if np.random.rand() < np.exp(-(score - global_best_score) / self.temp):\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n                    if score < global_best_score:\n                        global_best_score = score\n                        global_best_position = positions[i]\n\n            # Temperature cooling schedule\n            self.temp *= 0.95\n\n        return global_best_position, global_best_score", "name": "PSO_SA_Optimizer_Enhanced", "description": "Enhanced hybrid Particle Swarm Optimization with Simulated Annealing (PSO-SA) and adaptive inertia weight for improved convergence.", "configspace": "", "generation": 6, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "0411155c-9b0a-4e86-94b2-317eaab11b15", "metadata": {"aucs": [0.5541935495271029, 0.5541935495271029, 0.5541935495271029, 0.5472676162748873, 0.5472676162748873, 0.5472676162748873, 0.4995050001955086, 0.4995050001955086, 0.4995050001955086, 0.37677698123758485, 0.37677698123758485, 0.37677698123758485, 0.2602571254910885, 0.2602571254910885, 0.2602571254910885, 0.003330909247681313, 0.003330909247681313, 0.003330909247681313, 0.16044625244716004, 0.16044625244716004, 0.16044625244716004, 0.06165347877030414, 0.06165347877030414, 0.06165347877030414, 0.11895346010869268, 0.11895346010869268, 0.11895346010869268, 0.07493708294659229, 0.07493708294659229, 0.07493708294659229, 0.1223881702479801, 0.1223881702479801, 0.1223881702479801, 0.11364990543254094, 0.11364990543254094, 0.11364990543254094, 0.9917969237266142, 0.9917969237266142, 0.9917969237266142, 0.9947464319524106, 0.9947464319524106, 0.9947464319524106, 0.9923522331758592, 0.9923522331758592, 0.9923522331758592, 0.23292936537637376, 0.23292936537637376, 0.23292936537637376, 0.3241275789159769, 0.3241275789159769, 0.3241275789159769, 0.368066327161232, 0.368066327161232, 0.368066327161232, 0.3151000101329813, 0.3151000101329813, 0.3151000101329813, 0.15819099217998733, 0.15819099217998733, 0.15819099217998733, 0.1161189521585051, 0.1161189521585051, 0.1161189521585051, 0.39416284155128345, 0.39416284155128345, 0.39416284155128345, 0.12212977487851262, 0.12212977487851262, 0.12212977487851262, 0.2644190003152572, 0.2644190003152572, 0.2644190003152572, 0.35801657538571496, 0.35801657538571496, 0.35801657538571496, 0.12297914518591568, 0.12297914518591568, 0.12297914518591568, 0.1960278080415332, 0.1960278080415332, 0.1960278080415332, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.084775675890568, 0.084775675890568, 0.084775675890568, 0.3202396802670431, 0.3202396802670431, 0.3202396802670431, 0.07097896700228357, 0.07097896700228357, 0.07097896700228357, 0.08885677525536484, 0.08885677525536484, 0.08885677525536484, 0.03314189301901549, 0.03314189301901549, 0.03314189301901549, 0.04537410703272038, 0.04537410703272038, 0.04537410703272038, 0.03922412471399417, 0.03922412471399417, 0.03922412471399417, 0.1155219345450258, 0.1155219345450258, 0.1155219345450258, 0.034755019981584145, 0.034755019981584145, 0.034755019981584145, 0.07102106026707755, 0.07102106026707755, 0.07102106026707755, 0.5006794078134473, 0.5006794078134473, 0.5006794078134473, 0.4561303012753095, 0.4561303012753095, 0.4561303012753095, 0.5158501654337682, 0.5158501654337682, 0.5158501654337682, 0.11653379618486104, 0.11653379618486104, 0.11653379618486104, 0.07532313407863034, 0.07532313407863034, 0.07532313407863034, 0.12449952434684819, 0.12449952434684819, 0.12449952434684819, 0.35694259995702704, 0.35694259995702704, 0.35694259995702704, 0.17193702509720898, 0.17193702509720898, 0.17193702509720898, 0.13374219188996994, 0.13374219188996994, 0.13374219188996994, 0.2475515055418741, 0.2475515055418741, 0.2475515055418741, 0.2277427774917169, 0.2277427774917169, 0.2277427774917169, 0.16379641556242075, 0.16379641556242075, 0.16379641556242075, 0.20698761037109803, 0.20698761037109803, 0.20698761037109803, 0.2166739607932202, 0.2166739607932202, 0.2166739607932202, 0.12743080958398967, 0.12743080958398967, 0.12743080958398967, 0.20836779232376135, 0.20836779232376135, 0.20836779232376135, 0.2218463570025747, 0.2218463570025747, 0.2218463570025747, 0.2266211048527721, 0.2266211048527721, 0.2266211048527721, 0.19974449838695618, 0.19974449838695618, 0.19974449838695618, 0.19798302527326395, 0.19798302527326395, 0.19798302527326395, 0.20921222160855857, 0.20921222160855857, 0.20921222160855857, 0.5247151717610008, 0.5247151717610008, 0.5247151717610008, 0.1654403704961498, 0.1654403704961498, 0.1654403704961498, 0.16601278941076159, 0.16601278941076159, 0.16601278941076159, 0.16848472192743125, 0.16848472192743125, 0.16848472192743125, 0.2061724960517588, 0.2061724960517588, 0.2061724960517588, 0.14991930742146542, 0.14991930742146542, 0.14991930742146542, 0.17303961150351843, 0.17303961150351843, 0.17303961150351843, 0.19293796404668373, 0.19293796404668373, 0.19293796404668373, 0.21318655457744973, 0.21318655457744973, 0.21318655457744973, 0.09952291400030866, 0.09952291400030866, 0.09952291400030866, 0.06908111797165462, 0.06908111797165462, 0.06908111797165462, 0.07062270801207382, 0.07062270801207382, 0.07062270801207382]}, "mutation_prompt": null}
{"id": "e473bafb-f047-4aea-bd8c-4ac2b0707ac5", "solution": "import numpy as np\n\nclass PSO_SA_Optimizer_Enhanced:\n    def __init__(self, budget, dim, pop_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.w_max = 0.9  # Maximum inertia weight\n        self.w_min = 0.4  # Minimum inertia weight\n        self.c1 = 1.5  # Cognitive parameter\n        self.c2 = 1.5  # Social parameter\n        self.temp = 1.0  # Initial temperature for SA\n\n    def __call__(self, func):\n        np.random.seed(42)  # For reproducibility\n        \n        # Initialize particles\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.array([func(pos) for pos in positions])\n        global_best_position = personal_best_positions[np.argmin(personal_best_scores)]\n        global_best_score = np.min(personal_best_scores)\n        \n        eval_count = self.pop_size\n        \n        while eval_count < self.budget:\n            w = self.w_max - ((self.w_max - self.w_min) * (eval_count / self.budget))\n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                velocities[i] = (w * velocities[i]\n                                 + self.c1 * r1 * (personal_best_positions[i] - positions[i])\n                                 + self.c2 * r2 * (global_best_position - positions[i]))\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], self.lower_bound, self.upper_bound)\n\n                # Evaluate the new position\n                score = func(positions[i])\n                eval_count += 1\n                if eval_count >= self.budget:\n                    break\n\n                # Update personal best\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n\n                # Update global best\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = positions[i]\n\n                # Simulated Annealing Acceptance\n                if np.random.rand() < np.exp(-(score - global_best_score) / self.temp):\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n                    if score < global_best_score:\n                        global_best_score = score\n                        global_best_position = positions[i]\n\n            # Temperature cooling schedule\n            self.temp *= 0.95\n\n        return global_best_position, global_best_score", "name": "PSO_SA_Optimizer_Enhanced", "description": "Enhanced hybrid Particle Swarm Optimization with Simulated Annealing (PSO-SA) and adaptive inertia weight for improved convergence.", "configspace": "", "generation": 6, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "0411155c-9b0a-4e86-94b2-317eaab11b15", "metadata": {"aucs": [0.5541935495271029, 0.5541935495271029, 0.5541935495271029, 0.5472676162748873, 0.5472676162748873, 0.5472676162748873, 0.4995050001955086, 0.4995050001955086, 0.4995050001955086, 0.37677698123758485, 0.37677698123758485, 0.37677698123758485, 0.2602571254910885, 0.2602571254910885, 0.2602571254910885, 0.003330909247681313, 0.003330909247681313, 0.003330909247681313, 0.16044625244716004, 0.16044625244716004, 0.16044625244716004, 0.06165347877030414, 0.06165347877030414, 0.06165347877030414, 0.11895346010869268, 0.11895346010869268, 0.11895346010869268, 0.07493708294659229, 0.07493708294659229, 0.07493708294659229, 0.1223881702479801, 0.1223881702479801, 0.1223881702479801, 0.11364990543254094, 0.11364990543254094, 0.11364990543254094, 0.9917969237266142, 0.9917969237266142, 0.9917969237266142, 0.9947464319524106, 0.9947464319524106, 0.9947464319524106, 0.9923522331758592, 0.9923522331758592, 0.9923522331758592, 0.23292936537637376, 0.23292936537637376, 0.23292936537637376, 0.3241275789159769, 0.3241275789159769, 0.3241275789159769, 0.368066327161232, 0.368066327161232, 0.368066327161232, 0.3151000101329813, 0.3151000101329813, 0.3151000101329813, 0.15819099217998733, 0.15819099217998733, 0.15819099217998733, 0.1161189521585051, 0.1161189521585051, 0.1161189521585051, 0.39416284155128345, 0.39416284155128345, 0.39416284155128345, 0.12212977487851262, 0.12212977487851262, 0.12212977487851262, 0.2644190003152572, 0.2644190003152572, 0.2644190003152572, 0.35801657538571496, 0.35801657538571496, 0.35801657538571496, 0.12297914518591568, 0.12297914518591568, 0.12297914518591568, 0.1960278080415332, 0.1960278080415332, 0.1960278080415332, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.084775675890568, 0.084775675890568, 0.084775675890568, 0.3202396802670431, 0.3202396802670431, 0.3202396802670431, 0.07097896700228357, 0.07097896700228357, 0.07097896700228357, 0.08885677525536484, 0.08885677525536484, 0.08885677525536484, 0.03314189301901549, 0.03314189301901549, 0.03314189301901549, 0.04537410703272038, 0.04537410703272038, 0.04537410703272038, 0.03922412471399417, 0.03922412471399417, 0.03922412471399417, 0.1155219345450258, 0.1155219345450258, 0.1155219345450258, 0.034755019981584145, 0.034755019981584145, 0.034755019981584145, 0.07102106026707755, 0.07102106026707755, 0.07102106026707755, 0.5006794078134473, 0.5006794078134473, 0.5006794078134473, 0.4561303012753095, 0.4561303012753095, 0.4561303012753095, 0.5158501654337682, 0.5158501654337682, 0.5158501654337682, 0.11653379618486104, 0.11653379618486104, 0.11653379618486104, 0.07532313407863034, 0.07532313407863034, 0.07532313407863034, 0.12449952434684819, 0.12449952434684819, 0.12449952434684819, 0.35694259995702704, 0.35694259995702704, 0.35694259995702704, 0.17193702509720898, 0.17193702509720898, 0.17193702509720898, 0.13374219188996994, 0.13374219188996994, 0.13374219188996994, 0.2475515055418741, 0.2475515055418741, 0.2475515055418741, 0.2277427774917169, 0.2277427774917169, 0.2277427774917169, 0.16379641556242075, 0.16379641556242075, 0.16379641556242075, 0.20698761037109803, 0.20698761037109803, 0.20698761037109803, 0.2166739607932202, 0.2166739607932202, 0.2166739607932202, 0.12743080958398967, 0.12743080958398967, 0.12743080958398967, 0.20836779232376135, 0.20836779232376135, 0.20836779232376135, 0.2218463570025747, 0.2218463570025747, 0.2218463570025747, 0.2266211048527721, 0.2266211048527721, 0.2266211048527721, 0.19974449838695618, 0.19974449838695618, 0.19974449838695618, 0.19798302527326395, 0.19798302527326395, 0.19798302527326395, 0.20921222160855857, 0.20921222160855857, 0.20921222160855857, 0.5247151717610008, 0.5247151717610008, 0.5247151717610008, 0.1654403704961498, 0.1654403704961498, 0.1654403704961498, 0.16601278941076159, 0.16601278941076159, 0.16601278941076159, 0.16848472192743125, 0.16848472192743125, 0.16848472192743125, 0.2061724960517588, 0.2061724960517588, 0.2061724960517588, 0.14991930742146542, 0.14991930742146542, 0.14991930742146542, 0.17303961150351843, 0.17303961150351843, 0.17303961150351843, 0.19293796404668373, 0.19293796404668373, 0.19293796404668373, 0.21318655457744973, 0.21318655457744973, 0.21318655457744973, 0.09952291400030866, 0.09952291400030866, 0.09952291400030866, 0.06908111797165462, 0.06908111797165462, 0.06908111797165462, 0.07062270801207382, 0.07062270801207382, 0.07062270801207382]}, "mutation_prompt": null}
{"id": "de2f5eae-68c6-43e1-8da4-60fe82ee5618", "solution": "import numpy as np\n\nclass PSO_SA_Optimizer_Enhanced:\n    def __init__(self, budget, dim, pop_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.w_max = 0.9  # Maximum inertia weight\n        self.w_min = 0.4  # Minimum inertia weight\n        self.c1 = 1.5  # Cognitive parameter\n        self.c2 = 1.5  # Social parameter\n        self.temp = 1.0  # Initial temperature for SA\n\n    def __call__(self, func):\n        np.random.seed(42)  # For reproducibility\n        \n        # Initialize particles\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.array([func(pos) for pos in positions])\n        global_best_position = personal_best_positions[np.argmin(personal_best_scores)]\n        global_best_score = np.min(personal_best_scores)\n        \n        eval_count = self.pop_size\n        \n        while eval_count < self.budget:\n            w = self.w_max - ((self.w_max - self.w_min) * (eval_count / self.budget))\n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                velocities[i] = (w * velocities[i]\n                                 + self.c1 * r1 * (personal_best_positions[i] - positions[i])\n                                 + self.c2 * r2 * (global_best_position - positions[i]))\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], self.lower_bound, self.upper_bound)\n\n                # Evaluate the new position\n                score = func(positions[i])\n                eval_count += 1\n                if eval_count >= self.budget:\n                    break\n\n                # Update personal best\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n\n                # Update global best\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = positions[i]\n\n                # Simulated Annealing Acceptance\n                if np.random.rand() < np.exp(-(score - global_best_score) / self.temp):\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n                    if score < global_best_score:\n                        global_best_score = score\n                        global_best_position = positions[i]\n\n            # Temperature cooling schedule\n            self.temp *= 0.95\n\n        return global_best_position, global_best_score", "name": "PSO_SA_Optimizer_Enhanced", "description": "Enhanced hybrid Particle Swarm Optimization with Simulated Annealing (PSO-SA) and adaptive inertia weight for improved convergence.", "configspace": "", "generation": 6, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "0411155c-9b0a-4e86-94b2-317eaab11b15", "metadata": {"aucs": [0.5541935495271029, 0.5541935495271029, 0.5541935495271029, 0.5472676162748873, 0.5472676162748873, 0.5472676162748873, 0.4995050001955086, 0.4995050001955086, 0.4995050001955086, 0.37677698123758485, 0.37677698123758485, 0.37677698123758485, 0.2602571254910885, 0.2602571254910885, 0.2602571254910885, 0.003330909247681313, 0.003330909247681313, 0.003330909247681313, 0.16044625244716004, 0.16044625244716004, 0.16044625244716004, 0.06165347877030414, 0.06165347877030414, 0.06165347877030414, 0.11895346010869268, 0.11895346010869268, 0.11895346010869268, 0.07493708294659229, 0.07493708294659229, 0.07493708294659229, 0.1223881702479801, 0.1223881702479801, 0.1223881702479801, 0.11364990543254094, 0.11364990543254094, 0.11364990543254094, 0.9917969237266142, 0.9917969237266142, 0.9917969237266142, 0.9947464319524106, 0.9947464319524106, 0.9947464319524106, 0.9923522331758592, 0.9923522331758592, 0.9923522331758592, 0.23292936537637376, 0.23292936537637376, 0.23292936537637376, 0.3241275789159769, 0.3241275789159769, 0.3241275789159769, 0.368066327161232, 0.368066327161232, 0.368066327161232, 0.3151000101329813, 0.3151000101329813, 0.3151000101329813, 0.15819099217998733, 0.15819099217998733, 0.15819099217998733, 0.1161189521585051, 0.1161189521585051, 0.1161189521585051, 0.39416284155128345, 0.39416284155128345, 0.39416284155128345, 0.12212977487851262, 0.12212977487851262, 0.12212977487851262, 0.2644190003152572, 0.2644190003152572, 0.2644190003152572, 0.35801657538571496, 0.35801657538571496, 0.35801657538571496, 0.12297914518591568, 0.12297914518591568, 0.12297914518591568, 0.1960278080415332, 0.1960278080415332, 0.1960278080415332, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.084775675890568, 0.084775675890568, 0.084775675890568, 0.3202396802670431, 0.3202396802670431, 0.3202396802670431, 0.07097896700228357, 0.07097896700228357, 0.07097896700228357, 0.08885677525536484, 0.08885677525536484, 0.08885677525536484, 0.03314189301901549, 0.03314189301901549, 0.03314189301901549, 0.04537410703272038, 0.04537410703272038, 0.04537410703272038, 0.03922412471399417, 0.03922412471399417, 0.03922412471399417, 0.1155219345450258, 0.1155219345450258, 0.1155219345450258, 0.034755019981584145, 0.034755019981584145, 0.034755019981584145, 0.07102106026707755, 0.07102106026707755, 0.07102106026707755, 0.5006794078134473, 0.5006794078134473, 0.5006794078134473, 0.4561303012753095, 0.4561303012753095, 0.4561303012753095, 0.5158501654337682, 0.5158501654337682, 0.5158501654337682, 0.11653379618486104, 0.11653379618486104, 0.11653379618486104, 0.07532313407863034, 0.07532313407863034, 0.07532313407863034, 0.12449952434684819, 0.12449952434684819, 0.12449952434684819, 0.35694259995702704, 0.35694259995702704, 0.35694259995702704, 0.17193702509720898, 0.17193702509720898, 0.17193702509720898, 0.13374219188996994, 0.13374219188996994, 0.13374219188996994, 0.2475515055418741, 0.2475515055418741, 0.2475515055418741, 0.2277427774917169, 0.2277427774917169, 0.2277427774917169, 0.16379641556242075, 0.16379641556242075, 0.16379641556242075, 0.20698761037109803, 0.20698761037109803, 0.20698761037109803, 0.2166739607932202, 0.2166739607932202, 0.2166739607932202, 0.12743080958398967, 0.12743080958398967, 0.12743080958398967, 0.20836779232376135, 0.20836779232376135, 0.20836779232376135, 0.2218463570025747, 0.2218463570025747, 0.2218463570025747, 0.2266211048527721, 0.2266211048527721, 0.2266211048527721, 0.19974449838695618, 0.19974449838695618, 0.19974449838695618, 0.19798302527326395, 0.19798302527326395, 0.19798302527326395, 0.20921222160855857, 0.20921222160855857, 0.20921222160855857, 0.5247151717610008, 0.5247151717610008, 0.5247151717610008, 0.1654403704961498, 0.1654403704961498, 0.1654403704961498, 0.16601278941076159, 0.16601278941076159, 0.16601278941076159, 0.16848472192743125, 0.16848472192743125, 0.16848472192743125, 0.2061724960517588, 0.2061724960517588, 0.2061724960517588, 0.14991930742146542, 0.14991930742146542, 0.14991930742146542, 0.17303961150351843, 0.17303961150351843, 0.17303961150351843, 0.19293796404668373, 0.19293796404668373, 0.19293796404668373, 0.21318655457744973, 0.21318655457744973, 0.21318655457744973, 0.09952291400030866, 0.09952291400030866, 0.09952291400030866, 0.06908111797165462, 0.06908111797165462, 0.06908111797165462, 0.07062270801207382, 0.07062270801207382, 0.07062270801207382]}, "mutation_prompt": null}
{"id": "fdbc727e-5106-44d0-a7d1-6d059e3792b4", "solution": "import numpy as np\n\nclass PSO_SA_Optimizer_Enhanced:\n    def __init__(self, budget, dim, pop_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.w_max = 0.9  # Maximum inertia weight\n        self.w_min = 0.4  # Minimum inertia weight\n        self.c1 = 1.5  # Cognitive parameter\n        self.c2 = 1.5  # Social parameter\n        self.temp = 1.0  # Initial temperature for SA\n\n    def __call__(self, func):\n        np.random.seed(42)  # For reproducibility\n        \n        # Initialize particles\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.array([func(pos) for pos in positions])\n        global_best_position = personal_best_positions[np.argmin(personal_best_scores)]\n        global_best_score = np.min(personal_best_scores)\n        \n        eval_count = self.pop_size\n        \n        while eval_count < self.budget:\n            w = self.w_max - ((self.w_max - self.w_min) * (eval_count / self.budget))\n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                velocities[i] = (w * velocities[i]\n                                 + self.c1 * r1 * (personal_best_positions[i] - positions[i])\n                                 + self.c2 * r2 * (global_best_position - positions[i]))\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], self.lower_bound, self.upper_bound)\n\n                # Evaluate the new position\n                score = func(positions[i])\n                eval_count += 1\n                if eval_count >= self.budget:\n                    break\n\n                # Update personal best\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n\n                # Update global best\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = positions[i]\n\n                # Simulated Annealing Acceptance\n                if np.random.rand() < np.exp(-(score - global_best_score) / self.temp):\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n                    if score < global_best_score:\n                        global_best_score = score\n                        global_best_position = positions[i]\n\n            # Temperature cooling schedule\n            self.temp *= 0.95\n\n        return global_best_position, global_best_score", "name": "PSO_SA_Optimizer_Enhanced", "description": "Enhanced hybrid Particle Swarm Optimization with Simulated Annealing (PSO-SA) and adaptive inertia weight for improved convergence.", "configspace": "", "generation": 6, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "0411155c-9b0a-4e86-94b2-317eaab11b15", "metadata": {"aucs": [0.5541935495271029, 0.5541935495271029, 0.5541935495271029, 0.5472676162748873, 0.5472676162748873, 0.5472676162748873, 0.4995050001955086, 0.4995050001955086, 0.4995050001955086, 0.37677698123758485, 0.37677698123758485, 0.37677698123758485, 0.2602571254910885, 0.2602571254910885, 0.2602571254910885, 0.003330909247681313, 0.003330909247681313, 0.003330909247681313, 0.16044625244716004, 0.16044625244716004, 0.16044625244716004, 0.06165347877030414, 0.06165347877030414, 0.06165347877030414, 0.11895346010869268, 0.11895346010869268, 0.11895346010869268, 0.07493708294659229, 0.07493708294659229, 0.07493708294659229, 0.1223881702479801, 0.1223881702479801, 0.1223881702479801, 0.11364990543254094, 0.11364990543254094, 0.11364990543254094, 0.9917969237266142, 0.9917969237266142, 0.9917969237266142, 0.9947464319524106, 0.9947464319524106, 0.9947464319524106, 0.9923522331758592, 0.9923522331758592, 0.9923522331758592, 0.23292936537637376, 0.23292936537637376, 0.23292936537637376, 0.3241275789159769, 0.3241275789159769, 0.3241275789159769, 0.368066327161232, 0.368066327161232, 0.368066327161232, 0.3151000101329813, 0.3151000101329813, 0.3151000101329813, 0.15819099217998733, 0.15819099217998733, 0.15819099217998733, 0.1161189521585051, 0.1161189521585051, 0.1161189521585051, 0.39416284155128345, 0.39416284155128345, 0.39416284155128345, 0.12212977487851262, 0.12212977487851262, 0.12212977487851262, 0.2644190003152572, 0.2644190003152572, 0.2644190003152572, 0.35801657538571496, 0.35801657538571496, 0.35801657538571496, 0.12297914518591568, 0.12297914518591568, 0.12297914518591568, 0.1960278080415332, 0.1960278080415332, 0.1960278080415332, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.084775675890568, 0.084775675890568, 0.084775675890568, 0.3202396802670431, 0.3202396802670431, 0.3202396802670431, 0.07097896700228357, 0.07097896700228357, 0.07097896700228357, 0.08885677525536484, 0.08885677525536484, 0.08885677525536484, 0.03314189301901549, 0.03314189301901549, 0.03314189301901549, 0.04537410703272038, 0.04537410703272038, 0.04537410703272038, 0.03922412471399417, 0.03922412471399417, 0.03922412471399417, 0.1155219345450258, 0.1155219345450258, 0.1155219345450258, 0.034755019981584145, 0.034755019981584145, 0.034755019981584145, 0.07102106026707755, 0.07102106026707755, 0.07102106026707755, 0.5006794078134473, 0.5006794078134473, 0.5006794078134473, 0.4561303012753095, 0.4561303012753095, 0.4561303012753095, 0.5158501654337682, 0.5158501654337682, 0.5158501654337682, 0.11653379618486104, 0.11653379618486104, 0.11653379618486104, 0.07532313407863034, 0.07532313407863034, 0.07532313407863034, 0.12449952434684819, 0.12449952434684819, 0.12449952434684819, 0.35694259995702704, 0.35694259995702704, 0.35694259995702704, 0.17193702509720898, 0.17193702509720898, 0.17193702509720898, 0.13374219188996994, 0.13374219188996994, 0.13374219188996994, 0.2475515055418741, 0.2475515055418741, 0.2475515055418741, 0.2277427774917169, 0.2277427774917169, 0.2277427774917169, 0.16379641556242075, 0.16379641556242075, 0.16379641556242075, 0.20698761037109803, 0.20698761037109803, 0.20698761037109803, 0.2166739607932202, 0.2166739607932202, 0.2166739607932202, 0.12743080958398967, 0.12743080958398967, 0.12743080958398967, 0.20836779232376135, 0.20836779232376135, 0.20836779232376135, 0.2218463570025747, 0.2218463570025747, 0.2218463570025747, 0.2266211048527721, 0.2266211048527721, 0.2266211048527721, 0.19974449838695618, 0.19974449838695618, 0.19974449838695618, 0.19798302527326395, 0.19798302527326395, 0.19798302527326395, 0.20921222160855857, 0.20921222160855857, 0.20921222160855857, 0.5247151717610008, 0.5247151717610008, 0.5247151717610008, 0.1654403704961498, 0.1654403704961498, 0.1654403704961498, 0.16601278941076159, 0.16601278941076159, 0.16601278941076159, 0.16848472192743125, 0.16848472192743125, 0.16848472192743125, 0.2061724960517588, 0.2061724960517588, 0.2061724960517588, 0.14991930742146542, 0.14991930742146542, 0.14991930742146542, 0.17303961150351843, 0.17303961150351843, 0.17303961150351843, 0.19293796404668373, 0.19293796404668373, 0.19293796404668373, 0.21318655457744973, 0.21318655457744973, 0.21318655457744973, 0.09952291400030866, 0.09952291400030866, 0.09952291400030866, 0.06908111797165462, 0.06908111797165462, 0.06908111797165462, 0.07062270801207382, 0.07062270801207382, 0.07062270801207382]}, "mutation_prompt": null}
{"id": "c065e7d3-f892-4480-b63b-f78a531184fc", "solution": "import numpy as np\n\nclass PSO_SA_Optimizer_Enhanced:\n    def __init__(self, budget, dim, pop_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.w_max = 0.9  # Maximum inertia weight\n        self.w_min = 0.4  # Minimum inertia weight\n        self.c1 = 1.5  # Cognitive parameter\n        self.c2 = 1.5  # Social parameter\n        self.temp = 1.0  # Initial temperature for SA\n\n    def __call__(self, func):\n        np.random.seed(42)  # For reproducibility\n        \n        # Initialize particles\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.array([func(pos) for pos in positions])\n        global_best_position = personal_best_positions[np.argmin(personal_best_scores)]\n        global_best_score = np.min(personal_best_scores)\n        \n        eval_count = self.pop_size\n        \n        while eval_count < self.budget:\n            w = self.w_max - ((self.w_max - self.w_min) * (eval_count / self.budget))\n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                velocities[i] = (w * velocities[i]\n                                 + self.c1 * r1 * (personal_best_positions[i] - positions[i])\n                                 + self.c2 * r2 * (global_best_position - positions[i]))\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], self.lower_bound, self.upper_bound)\n\n                # Evaluate the new position\n                score = func(positions[i])\n                eval_count += 1\n                if eval_count >= self.budget:\n                    break\n\n                # Update personal best\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n\n                # Update global best\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = positions[i]\n\n                # Simulated Annealing Acceptance\n                if np.random.rand() < np.exp(-(score - global_best_score) / self.temp):\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n                    if score < global_best_score:\n                        global_best_score = score\n                        global_best_position = positions[i]\n\n            # Temperature cooling schedule\n            self.temp *= 0.95\n\n        return global_best_position, global_best_score", "name": "PSO_SA_Optimizer_Enhanced", "description": "Enhanced hybrid Particle Swarm Optimization with Simulated Annealing (PSO-SA) and adaptive inertia weight for improved convergence.", "configspace": "", "generation": 6, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "0411155c-9b0a-4e86-94b2-317eaab11b15", "metadata": {"aucs": [0.5541935495271029, 0.5541935495271029, 0.5541935495271029, 0.5472676162748873, 0.5472676162748873, 0.5472676162748873, 0.4995050001955086, 0.4995050001955086, 0.4995050001955086, 0.37677698123758485, 0.37677698123758485, 0.37677698123758485, 0.2602571254910885, 0.2602571254910885, 0.2602571254910885, 0.003330909247681313, 0.003330909247681313, 0.003330909247681313, 0.16044625244716004, 0.16044625244716004, 0.16044625244716004, 0.06165347877030414, 0.06165347877030414, 0.06165347877030414, 0.11895346010869268, 0.11895346010869268, 0.11895346010869268, 0.07493708294659229, 0.07493708294659229, 0.07493708294659229, 0.1223881702479801, 0.1223881702479801, 0.1223881702479801, 0.11364990543254094, 0.11364990543254094, 0.11364990543254094, 0.9917969237266142, 0.9917969237266142, 0.9917969237266142, 0.9947464319524106, 0.9947464319524106, 0.9947464319524106, 0.9923522331758592, 0.9923522331758592, 0.9923522331758592, 0.23292936537637376, 0.23292936537637376, 0.23292936537637376, 0.3241275789159769, 0.3241275789159769, 0.3241275789159769, 0.368066327161232, 0.368066327161232, 0.368066327161232, 0.3151000101329813, 0.3151000101329813, 0.3151000101329813, 0.15819099217998733, 0.15819099217998733, 0.15819099217998733, 0.1161189521585051, 0.1161189521585051, 0.1161189521585051, 0.39416284155128345, 0.39416284155128345, 0.39416284155128345, 0.12212977487851262, 0.12212977487851262, 0.12212977487851262, 0.2644190003152572, 0.2644190003152572, 0.2644190003152572, 0.35801657538571496, 0.35801657538571496, 0.35801657538571496, 0.12297914518591568, 0.12297914518591568, 0.12297914518591568, 0.1960278080415332, 0.1960278080415332, 0.1960278080415332, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.084775675890568, 0.084775675890568, 0.084775675890568, 0.3202396802670431, 0.3202396802670431, 0.3202396802670431, 0.07097896700228357, 0.07097896700228357, 0.07097896700228357, 0.08885677525536484, 0.08885677525536484, 0.08885677525536484, 0.03314189301901549, 0.03314189301901549, 0.03314189301901549, 0.04537410703272038, 0.04537410703272038, 0.04537410703272038, 0.03922412471399417, 0.03922412471399417, 0.03922412471399417, 0.1155219345450258, 0.1155219345450258, 0.1155219345450258, 0.034755019981584145, 0.034755019981584145, 0.034755019981584145, 0.07102106026707755, 0.07102106026707755, 0.07102106026707755, 0.5006794078134473, 0.5006794078134473, 0.5006794078134473, 0.4561303012753095, 0.4561303012753095, 0.4561303012753095, 0.5158501654337682, 0.5158501654337682, 0.5158501654337682, 0.11653379618486104, 0.11653379618486104, 0.11653379618486104, 0.07532313407863034, 0.07532313407863034, 0.07532313407863034, 0.12449952434684819, 0.12449952434684819, 0.12449952434684819, 0.35694259995702704, 0.35694259995702704, 0.35694259995702704, 0.17193702509720898, 0.17193702509720898, 0.17193702509720898, 0.13374219188996994, 0.13374219188996994, 0.13374219188996994, 0.2475515055418741, 0.2475515055418741, 0.2475515055418741, 0.2277427774917169, 0.2277427774917169, 0.2277427774917169, 0.16379641556242075, 0.16379641556242075, 0.16379641556242075, 0.20698761037109803, 0.20698761037109803, 0.20698761037109803, 0.2166739607932202, 0.2166739607932202, 0.2166739607932202, 0.12743080958398967, 0.12743080958398967, 0.12743080958398967, 0.20836779232376135, 0.20836779232376135, 0.20836779232376135, 0.2218463570025747, 0.2218463570025747, 0.2218463570025747, 0.2266211048527721, 0.2266211048527721, 0.2266211048527721, 0.19974449838695618, 0.19974449838695618, 0.19974449838695618, 0.19798302527326395, 0.19798302527326395, 0.19798302527326395, 0.20921222160855857, 0.20921222160855857, 0.20921222160855857, 0.5247151717610008, 0.5247151717610008, 0.5247151717610008, 0.1654403704961498, 0.1654403704961498, 0.1654403704961498, 0.16601278941076159, 0.16601278941076159, 0.16601278941076159, 0.16848472192743125, 0.16848472192743125, 0.16848472192743125, 0.2061724960517588, 0.2061724960517588, 0.2061724960517588, 0.14991930742146542, 0.14991930742146542, 0.14991930742146542, 0.17303961150351843, 0.17303961150351843, 0.17303961150351843, 0.19293796404668373, 0.19293796404668373, 0.19293796404668373, 0.21318655457744973, 0.21318655457744973, 0.21318655457744973, 0.09952291400030866, 0.09952291400030866, 0.09952291400030866, 0.06908111797165462, 0.06908111797165462, 0.06908111797165462, 0.07062270801207382, 0.07062270801207382, 0.07062270801207382]}, "mutation_prompt": null}
{"id": "ad94ca4f-0de6-48b4-b9b8-7eac69d6c288", "solution": "import numpy as np\n\nclass PSO_SA_Optimizer_Enhanced:\n    def __init__(self, budget, dim, pop_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.w_max = 0.9  # Maximum inertia weight\n        self.w_min = 0.4  # Minimum inertia weight\n        self.c1 = 1.5  # Cognitive parameter\n        self.c2 = 1.5  # Social parameter\n        self.temp = 1.0  # Initial temperature for SA\n\n    def __call__(self, func):\n        np.random.seed(42)  # For reproducibility\n        \n        # Initialize particles\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.array([func(pos) for pos in positions])\n        global_best_position = personal_best_positions[np.argmin(personal_best_scores)]\n        global_best_score = np.min(personal_best_scores)\n        \n        eval_count = self.pop_size\n        \n        while eval_count < self.budget:\n            w = self.w_max - ((self.w_max - self.w_min) * (eval_count / self.budget))\n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                velocities[i] = (w * velocities[i]\n                                 + self.c1 * r1 * (personal_best_positions[i] - positions[i])\n                                 + self.c2 * r2 * (global_best_position - positions[i]))\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], self.lower_bound, self.upper_bound)\n\n                # Evaluate the new position\n                score = func(positions[i])\n                eval_count += 1\n                if eval_count >= self.budget:\n                    break\n\n                # Update personal best\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n\n                # Update global best\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = positions[i]\n\n                # Simulated Annealing Acceptance\n                if np.random.rand() < np.exp(-(score - global_best_score) / self.temp):\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n                    if score < global_best_score:\n                        global_best_score = score\n                        global_best_position = positions[i]\n\n            # Temperature cooling schedule\n            self.temp *= 0.95\n\n        return global_best_position, global_best_score", "name": "PSO_SA_Optimizer_Enhanced", "description": "Enhanced hybrid Particle Swarm Optimization with Simulated Annealing (PSO-SA) and adaptive inertia weight for improved convergence.", "configspace": "", "generation": 6, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "0411155c-9b0a-4e86-94b2-317eaab11b15", "metadata": {"aucs": [0.5541935495271029, 0.5541935495271029, 0.5541935495271029, 0.5472676162748873, 0.5472676162748873, 0.5472676162748873, 0.4995050001955086, 0.4995050001955086, 0.4995050001955086, 0.37677698123758485, 0.37677698123758485, 0.37677698123758485, 0.2602571254910885, 0.2602571254910885, 0.2602571254910885, 0.003330909247681313, 0.003330909247681313, 0.003330909247681313, 0.16044625244716004, 0.16044625244716004, 0.16044625244716004, 0.06165347877030414, 0.06165347877030414, 0.06165347877030414, 0.11895346010869268, 0.11895346010869268, 0.11895346010869268, 0.07493708294659229, 0.07493708294659229, 0.07493708294659229, 0.1223881702479801, 0.1223881702479801, 0.1223881702479801, 0.11364990543254094, 0.11364990543254094, 0.11364990543254094, 0.9917969237266142, 0.9917969237266142, 0.9917969237266142, 0.9947464319524106, 0.9947464319524106, 0.9947464319524106, 0.9923522331758592, 0.9923522331758592, 0.9923522331758592, 0.23292936537637376, 0.23292936537637376, 0.23292936537637376, 0.3241275789159769, 0.3241275789159769, 0.3241275789159769, 0.368066327161232, 0.368066327161232, 0.368066327161232, 0.3151000101329813, 0.3151000101329813, 0.3151000101329813, 0.15819099217998733, 0.15819099217998733, 0.15819099217998733, 0.1161189521585051, 0.1161189521585051, 0.1161189521585051, 0.39416284155128345, 0.39416284155128345, 0.39416284155128345, 0.12212977487851262, 0.12212977487851262, 0.12212977487851262, 0.2644190003152572, 0.2644190003152572, 0.2644190003152572, 0.35801657538571496, 0.35801657538571496, 0.35801657538571496, 0.12297914518591568, 0.12297914518591568, 0.12297914518591568, 0.1960278080415332, 0.1960278080415332, 0.1960278080415332, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.084775675890568, 0.084775675890568, 0.084775675890568, 0.3202396802670431, 0.3202396802670431, 0.3202396802670431, 0.07097896700228357, 0.07097896700228357, 0.07097896700228357, 0.08885677525536484, 0.08885677525536484, 0.08885677525536484, 0.03314189301901549, 0.03314189301901549, 0.03314189301901549, 0.04537410703272038, 0.04537410703272038, 0.04537410703272038, 0.03922412471399417, 0.03922412471399417, 0.03922412471399417, 0.1155219345450258, 0.1155219345450258, 0.1155219345450258, 0.034755019981584145, 0.034755019981584145, 0.034755019981584145, 0.07102106026707755, 0.07102106026707755, 0.07102106026707755, 0.5006794078134473, 0.5006794078134473, 0.5006794078134473, 0.4561303012753095, 0.4561303012753095, 0.4561303012753095, 0.5158501654337682, 0.5158501654337682, 0.5158501654337682, 0.11653379618486104, 0.11653379618486104, 0.11653379618486104, 0.07532313407863034, 0.07532313407863034, 0.07532313407863034, 0.12449952434684819, 0.12449952434684819, 0.12449952434684819, 0.35694259995702704, 0.35694259995702704, 0.35694259995702704, 0.17193702509720898, 0.17193702509720898, 0.17193702509720898, 0.13374219188996994, 0.13374219188996994, 0.13374219188996994, 0.2475515055418741, 0.2475515055418741, 0.2475515055418741, 0.2277427774917169, 0.2277427774917169, 0.2277427774917169, 0.16379641556242075, 0.16379641556242075, 0.16379641556242075, 0.20698761037109803, 0.20698761037109803, 0.20698761037109803, 0.2166739607932202, 0.2166739607932202, 0.2166739607932202, 0.12743080958398967, 0.12743080958398967, 0.12743080958398967, 0.20836779232376135, 0.20836779232376135, 0.20836779232376135, 0.2218463570025747, 0.2218463570025747, 0.2218463570025747, 0.2266211048527721, 0.2266211048527721, 0.2266211048527721, 0.19974449838695618, 0.19974449838695618, 0.19974449838695618, 0.19798302527326395, 0.19798302527326395, 0.19798302527326395, 0.20921222160855857, 0.20921222160855857, 0.20921222160855857, 0.5247151717610008, 0.5247151717610008, 0.5247151717610008, 0.1654403704961498, 0.1654403704961498, 0.1654403704961498, 0.16601278941076159, 0.16601278941076159, 0.16601278941076159, 0.16848472192743125, 0.16848472192743125, 0.16848472192743125, 0.2061724960517588, 0.2061724960517588, 0.2061724960517588, 0.14991930742146542, 0.14991930742146542, 0.14991930742146542, 0.17303961150351843, 0.17303961150351843, 0.17303961150351843, 0.19293796404668373, 0.19293796404668373, 0.19293796404668373, 0.21318655457744973, 0.21318655457744973, 0.21318655457744973, 0.09952291400030866, 0.09952291400030866, 0.09952291400030866, 0.06908111797165462, 0.06908111797165462, 0.06908111797165462, 0.07062270801207382, 0.07062270801207382, 0.07062270801207382]}, "mutation_prompt": null}
{"id": "b7d2194c-307b-4220-ba9a-aeb6e5daeaa3", "solution": "import numpy as np\n\nclass PSO_SA_Optimizer_Enhanced:\n    def __init__(self, budget, dim, pop_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.w_max = 0.9  # Maximum inertia weight\n        self.w_min = 0.4  # Minimum inertia weight\n        self.c1 = 1.5  # Cognitive parameter\n        self.c2 = 1.5  # Social parameter\n        self.temp = 1.0  # Initial temperature for SA\n\n    def __call__(self, func):\n        np.random.seed(42)  # For reproducibility\n        \n        # Initialize particles\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.array([func(pos) for pos in positions])\n        global_best_position = personal_best_positions[np.argmin(personal_best_scores)]\n        global_best_score = np.min(personal_best_scores)\n        \n        eval_count = self.pop_size\n        \n        while eval_count < self.budget:\n            w = self.w_max - ((self.w_max - self.w_min) * (eval_count / self.budget))\n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                velocities[i] = (w * velocities[i]\n                                 + self.c1 * r1 * (personal_best_positions[i] - positions[i])\n                                 + self.c2 * r2 * (global_best_position - positions[i]))\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], self.lower_bound, self.upper_bound)\n\n                # Evaluate the new position\n                score = func(positions[i])\n                eval_count += 1\n                if eval_count >= self.budget:\n                    break\n\n                # Update personal best\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n\n                # Update global best\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = positions[i]\n\n                # Simulated Annealing Acceptance\n                if np.random.rand() < np.exp(-(score - global_best_score) / self.temp):\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n                    if score < global_best_score:\n                        global_best_score = score\n                        global_best_position = positions[i]\n\n            # Temperature cooling schedule\n            self.temp *= 0.95\n\n        return global_best_position, global_best_score", "name": "PSO_SA_Optimizer_Enhanced", "description": "Enhanced hybrid Particle Swarm Optimization with Simulated Annealing (PSO-SA) and adaptive inertia weight for improved convergence.", "configspace": "", "generation": 6, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "0411155c-9b0a-4e86-94b2-317eaab11b15", "metadata": {"aucs": [0.5541935495271029, 0.5541935495271029, 0.5541935495271029, 0.5472676162748873, 0.5472676162748873, 0.5472676162748873, 0.4995050001955086, 0.4995050001955086, 0.4995050001955086, 0.37677698123758485, 0.37677698123758485, 0.37677698123758485, 0.2602571254910885, 0.2602571254910885, 0.2602571254910885, 0.003330909247681313, 0.003330909247681313, 0.003330909247681313, 0.16044625244716004, 0.16044625244716004, 0.16044625244716004, 0.06165347877030414, 0.06165347877030414, 0.06165347877030414, 0.11895346010869268, 0.11895346010869268, 0.11895346010869268, 0.07493708294659229, 0.07493708294659229, 0.07493708294659229, 0.1223881702479801, 0.1223881702479801, 0.1223881702479801, 0.11364990543254094, 0.11364990543254094, 0.11364990543254094, 0.9917969237266142, 0.9917969237266142, 0.9917969237266142, 0.9947464319524106, 0.9947464319524106, 0.9947464319524106, 0.9923522331758592, 0.9923522331758592, 0.9923522331758592, 0.23292936537637376, 0.23292936537637376, 0.23292936537637376, 0.3241275789159769, 0.3241275789159769, 0.3241275789159769, 0.368066327161232, 0.368066327161232, 0.368066327161232, 0.3151000101329813, 0.3151000101329813, 0.3151000101329813, 0.15819099217998733, 0.15819099217998733, 0.15819099217998733, 0.1161189521585051, 0.1161189521585051, 0.1161189521585051, 0.39416284155128345, 0.39416284155128345, 0.39416284155128345, 0.12212977487851262, 0.12212977487851262, 0.12212977487851262, 0.2644190003152572, 0.2644190003152572, 0.2644190003152572, 0.35801657538571496, 0.35801657538571496, 0.35801657538571496, 0.12297914518591568, 0.12297914518591568, 0.12297914518591568, 0.1960278080415332, 0.1960278080415332, 0.1960278080415332, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.084775675890568, 0.084775675890568, 0.084775675890568, 0.3202396802670431, 0.3202396802670431, 0.3202396802670431, 0.07097896700228357, 0.07097896700228357, 0.07097896700228357, 0.08885677525536484, 0.08885677525536484, 0.08885677525536484, 0.03314189301901549, 0.03314189301901549, 0.03314189301901549, 0.04537410703272038, 0.04537410703272038, 0.04537410703272038, 0.03922412471399417, 0.03922412471399417, 0.03922412471399417, 0.1155219345450258, 0.1155219345450258, 0.1155219345450258, 0.034755019981584145, 0.034755019981584145, 0.034755019981584145, 0.07102106026707755, 0.07102106026707755, 0.07102106026707755, 0.5006794078134473, 0.5006794078134473, 0.5006794078134473, 0.4561303012753095, 0.4561303012753095, 0.4561303012753095, 0.5158501654337682, 0.5158501654337682, 0.5158501654337682, 0.11653379618486104, 0.11653379618486104, 0.11653379618486104, 0.07532313407863034, 0.07532313407863034, 0.07532313407863034, 0.12449952434684819, 0.12449952434684819, 0.12449952434684819, 0.35694259995702704, 0.35694259995702704, 0.35694259995702704, 0.17193702509720898, 0.17193702509720898, 0.17193702509720898, 0.13374219188996994, 0.13374219188996994, 0.13374219188996994, 0.2475515055418741, 0.2475515055418741, 0.2475515055418741, 0.2277427774917169, 0.2277427774917169, 0.2277427774917169, 0.16379641556242075, 0.16379641556242075, 0.16379641556242075, 0.20698761037109803, 0.20698761037109803, 0.20698761037109803, 0.2166739607932202, 0.2166739607932202, 0.2166739607932202, 0.12743080958398967, 0.12743080958398967, 0.12743080958398967, 0.20836779232376135, 0.20836779232376135, 0.20836779232376135, 0.2218463570025747, 0.2218463570025747, 0.2218463570025747, 0.2266211048527721, 0.2266211048527721, 0.2266211048527721, 0.19974449838695618, 0.19974449838695618, 0.19974449838695618, 0.19798302527326395, 0.19798302527326395, 0.19798302527326395, 0.20921222160855857, 0.20921222160855857, 0.20921222160855857, 0.5247151717610008, 0.5247151717610008, 0.5247151717610008, 0.1654403704961498, 0.1654403704961498, 0.1654403704961498, 0.16601278941076159, 0.16601278941076159, 0.16601278941076159, 0.16848472192743125, 0.16848472192743125, 0.16848472192743125, 0.2061724960517588, 0.2061724960517588, 0.2061724960517588, 0.14991930742146542, 0.14991930742146542, 0.14991930742146542, 0.17303961150351843, 0.17303961150351843, 0.17303961150351843, 0.19293796404668373, 0.19293796404668373, 0.19293796404668373, 0.21318655457744973, 0.21318655457744973, 0.21318655457744973, 0.09952291400030866, 0.09952291400030866, 0.09952291400030866, 0.06908111797165462, 0.06908111797165462, 0.06908111797165462, 0.07062270801207382, 0.07062270801207382, 0.07062270801207382]}, "mutation_prompt": null}
{"id": "ebcae65b-c3f2-4455-8186-81b71d4513dd", "solution": "import numpy as np\n\nclass PSO_SA_Optimizer_Enhanced:\n    def __init__(self, budget, dim, pop_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.w_max = 0.9  # Maximum inertia weight\n        self.w_min = 0.4  # Minimum inertia weight\n        self.c1 = 1.5  # Cognitive parameter\n        self.c2 = 1.5  # Social parameter\n        self.temp = 1.0  # Initial temperature for SA\n\n    def __call__(self, func):\n        np.random.seed(42)  # For reproducibility\n        \n        # Initialize particles\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.array([func(pos) for pos in positions])\n        global_best_position = personal_best_positions[np.argmin(personal_best_scores)]\n        global_best_score = np.min(personal_best_scores)\n        \n        eval_count = self.pop_size\n        \n        while eval_count < self.budget:\n            w = self.w_max - ((self.w_max - self.w_min) * (eval_count / self.budget))\n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                velocities[i] = (w * velocities[i]\n                                 + self.c1 * r1 * (personal_best_positions[i] - positions[i])\n                                 + self.c2 * r2 * (global_best_position - positions[i]))\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], self.lower_bound, self.upper_bound)\n\n                # Evaluate the new position\n                score = func(positions[i])\n                eval_count += 1\n                if eval_count >= self.budget:\n                    break\n\n                # Update personal best\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n\n                # Update global best\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = positions[i]\n\n                # Simulated Annealing Acceptance\n                if np.random.rand() < np.exp(-(score - global_best_score) / self.temp):\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n                    if score < global_best_score:\n                        global_best_score = score\n                        global_best_position = positions[i]\n\n            # Temperature cooling schedule\n            self.temp *= 0.95\n\n        return global_best_position, global_best_score", "name": "PSO_SA_Optimizer_Enhanced", "description": "Enhanced hybrid Particle Swarm Optimization with Simulated Annealing (PSO-SA) and adaptive inertia weight for improved convergence.", "configspace": "", "generation": 6, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "0411155c-9b0a-4e86-94b2-317eaab11b15", "metadata": {"aucs": [0.5541935495271029, 0.5541935495271029, 0.5541935495271029, 0.5472676162748873, 0.5472676162748873, 0.5472676162748873, 0.4995050001955086, 0.4995050001955086, 0.4995050001955086, 0.37677698123758485, 0.37677698123758485, 0.37677698123758485, 0.2602571254910885, 0.2602571254910885, 0.2602571254910885, 0.003330909247681313, 0.003330909247681313, 0.003330909247681313, 0.16044625244716004, 0.16044625244716004, 0.16044625244716004, 0.06165347877030414, 0.06165347877030414, 0.06165347877030414, 0.11895346010869268, 0.11895346010869268, 0.11895346010869268, 0.07493708294659229, 0.07493708294659229, 0.07493708294659229, 0.1223881702479801, 0.1223881702479801, 0.1223881702479801, 0.11364990543254094, 0.11364990543254094, 0.11364990543254094, 0.9917969237266142, 0.9917969237266142, 0.9917969237266142, 0.9947464319524106, 0.9947464319524106, 0.9947464319524106, 0.9923522331758592, 0.9923522331758592, 0.9923522331758592, 0.23292936537637376, 0.23292936537637376, 0.23292936537637376, 0.3241275789159769, 0.3241275789159769, 0.3241275789159769, 0.368066327161232, 0.368066327161232, 0.368066327161232, 0.3151000101329813, 0.3151000101329813, 0.3151000101329813, 0.15819099217998733, 0.15819099217998733, 0.15819099217998733, 0.1161189521585051, 0.1161189521585051, 0.1161189521585051, 0.39416284155128345, 0.39416284155128345, 0.39416284155128345, 0.12212977487851262, 0.12212977487851262, 0.12212977487851262, 0.2644190003152572, 0.2644190003152572, 0.2644190003152572, 0.35801657538571496, 0.35801657538571496, 0.35801657538571496, 0.12297914518591568, 0.12297914518591568, 0.12297914518591568, 0.1960278080415332, 0.1960278080415332, 0.1960278080415332, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.084775675890568, 0.084775675890568, 0.084775675890568, 0.3202396802670431, 0.3202396802670431, 0.3202396802670431, 0.07097896700228357, 0.07097896700228357, 0.07097896700228357, 0.08885677525536484, 0.08885677525536484, 0.08885677525536484, 0.03314189301901549, 0.03314189301901549, 0.03314189301901549, 0.04537410703272038, 0.04537410703272038, 0.04537410703272038, 0.03922412471399417, 0.03922412471399417, 0.03922412471399417, 0.1155219345450258, 0.1155219345450258, 0.1155219345450258, 0.034755019981584145, 0.034755019981584145, 0.034755019981584145, 0.07102106026707755, 0.07102106026707755, 0.07102106026707755, 0.5006794078134473, 0.5006794078134473, 0.5006794078134473, 0.4561303012753095, 0.4561303012753095, 0.4561303012753095, 0.5158501654337682, 0.5158501654337682, 0.5158501654337682, 0.11653379618486104, 0.11653379618486104, 0.11653379618486104, 0.07532313407863034, 0.07532313407863034, 0.07532313407863034, 0.12449952434684819, 0.12449952434684819, 0.12449952434684819, 0.35694259995702704, 0.35694259995702704, 0.35694259995702704, 0.17193702509720898, 0.17193702509720898, 0.17193702509720898, 0.13374219188996994, 0.13374219188996994, 0.13374219188996994, 0.2475515055418741, 0.2475515055418741, 0.2475515055418741, 0.2277427774917169, 0.2277427774917169, 0.2277427774917169, 0.16379641556242075, 0.16379641556242075, 0.16379641556242075, 0.20698761037109803, 0.20698761037109803, 0.20698761037109803, 0.2166739607932202, 0.2166739607932202, 0.2166739607932202, 0.12743080958398967, 0.12743080958398967, 0.12743080958398967, 0.20836779232376135, 0.20836779232376135, 0.20836779232376135, 0.2218463570025747, 0.2218463570025747, 0.2218463570025747, 0.2266211048527721, 0.2266211048527721, 0.2266211048527721, 0.19974449838695618, 0.19974449838695618, 0.19974449838695618, 0.19798302527326395, 0.19798302527326395, 0.19798302527326395, 0.20921222160855857, 0.20921222160855857, 0.20921222160855857, 0.5247151717610008, 0.5247151717610008, 0.5247151717610008, 0.1654403704961498, 0.1654403704961498, 0.1654403704961498, 0.16601278941076159, 0.16601278941076159, 0.16601278941076159, 0.16848472192743125, 0.16848472192743125, 0.16848472192743125, 0.2061724960517588, 0.2061724960517588, 0.2061724960517588, 0.14991930742146542, 0.14991930742146542, 0.14991930742146542, 0.17303961150351843, 0.17303961150351843, 0.17303961150351843, 0.19293796404668373, 0.19293796404668373, 0.19293796404668373, 0.21318655457744973, 0.21318655457744973, 0.21318655457744973, 0.09952291400030866, 0.09952291400030866, 0.09952291400030866, 0.06908111797165462, 0.06908111797165462, 0.06908111797165462, 0.07062270801207382, 0.07062270801207382, 0.07062270801207382]}, "mutation_prompt": null}
{"id": "1764ef4b-495b-4bd4-87af-17701dfcd4bd", "solution": "import numpy as np\n\nclass PSO_SA_Optimizer_Enhanced:\n    def __init__(self, budget, dim, pop_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.w_max = 0.9  # Maximum inertia weight\n        self.w_min = 0.4  # Minimum inertia weight\n        self.c1 = 1.5  # Cognitive parameter\n        self.c2 = 1.5  # Social parameter\n        self.temp = 1.0  # Initial temperature for SA\n\n    def __call__(self, func):\n        np.random.seed(42)  # For reproducibility\n        \n        # Initialize particles\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.array([func(pos) for pos in positions])\n        global_best_position = personal_best_positions[np.argmin(personal_best_scores)]\n        global_best_score = np.min(personal_best_scores)\n        \n        eval_count = self.pop_size\n        \n        while eval_count < self.budget:\n            w = self.w_max - ((self.w_max - self.w_min) * (eval_count / self.budget))\n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                velocities[i] = (w * velocities[i]\n                                 + self.c1 * r1 * (personal_best_positions[i] - positions[i])\n                                 + self.c2 * r2 * (global_best_position - positions[i]))\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], self.lower_bound, self.upper_bound)\n\n                # Evaluate the new position\n                score = func(positions[i])\n                eval_count += 1\n                if eval_count >= self.budget:\n                    break\n\n                # Update personal best\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n\n                # Update global best\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = positions[i]\n\n                # Simulated Annealing Acceptance\n                if np.random.rand() < np.exp(-(score - global_best_score) / self.temp):\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n                    if score < global_best_score:\n                        global_best_score = score\n                        global_best_position = positions[i]\n\n            # Temperature cooling schedule\n            self.temp *= 0.95\n\n        return global_best_position, global_best_score", "name": "PSO_SA_Optimizer_Enhanced", "description": "Enhanced hybrid Particle Swarm Optimization with Simulated Annealing (PSO-SA) and adaptive inertia weight for improved convergence.", "configspace": "", "generation": 6, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "0411155c-9b0a-4e86-94b2-317eaab11b15", "metadata": {"aucs": [0.5541935495271029, 0.5541935495271029, 0.5541935495271029, 0.5472676162748873, 0.5472676162748873, 0.5472676162748873, 0.4995050001955086, 0.4995050001955086, 0.4995050001955086, 0.37677698123758485, 0.37677698123758485, 0.37677698123758485, 0.2602571254910885, 0.2602571254910885, 0.2602571254910885, 0.003330909247681313, 0.003330909247681313, 0.003330909247681313, 0.16044625244716004, 0.16044625244716004, 0.16044625244716004, 0.06165347877030414, 0.06165347877030414, 0.06165347877030414, 0.11895346010869268, 0.11895346010869268, 0.11895346010869268, 0.07493708294659229, 0.07493708294659229, 0.07493708294659229, 0.1223881702479801, 0.1223881702479801, 0.1223881702479801, 0.11364990543254094, 0.11364990543254094, 0.11364990543254094, 0.9917969237266142, 0.9917969237266142, 0.9917969237266142, 0.9947464319524106, 0.9947464319524106, 0.9947464319524106, 0.9923522331758592, 0.9923522331758592, 0.9923522331758592, 0.23292936537637376, 0.23292936537637376, 0.23292936537637376, 0.3241275789159769, 0.3241275789159769, 0.3241275789159769, 0.368066327161232, 0.368066327161232, 0.368066327161232, 0.3151000101329813, 0.3151000101329813, 0.3151000101329813, 0.15819099217998733, 0.15819099217998733, 0.15819099217998733, 0.1161189521585051, 0.1161189521585051, 0.1161189521585051, 0.39416284155128345, 0.39416284155128345, 0.39416284155128345, 0.12212977487851262, 0.12212977487851262, 0.12212977487851262, 0.2644190003152572, 0.2644190003152572, 0.2644190003152572, 0.35801657538571496, 0.35801657538571496, 0.35801657538571496, 0.12297914518591568, 0.12297914518591568, 0.12297914518591568, 0.1960278080415332, 0.1960278080415332, 0.1960278080415332, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.084775675890568, 0.084775675890568, 0.084775675890568, 0.3202396802670431, 0.3202396802670431, 0.3202396802670431, 0.07097896700228357, 0.07097896700228357, 0.07097896700228357, 0.08885677525536484, 0.08885677525536484, 0.08885677525536484, 0.03314189301901549, 0.03314189301901549, 0.03314189301901549, 0.04537410703272038, 0.04537410703272038, 0.04537410703272038, 0.03922412471399417, 0.03922412471399417, 0.03922412471399417, 0.1155219345450258, 0.1155219345450258, 0.1155219345450258, 0.034755019981584145, 0.034755019981584145, 0.034755019981584145, 0.07102106026707755, 0.07102106026707755, 0.07102106026707755, 0.5006794078134473, 0.5006794078134473, 0.5006794078134473, 0.4561303012753095, 0.4561303012753095, 0.4561303012753095, 0.5158501654337682, 0.5158501654337682, 0.5158501654337682, 0.11653379618486104, 0.11653379618486104, 0.11653379618486104, 0.07532313407863034, 0.07532313407863034, 0.07532313407863034, 0.12449952434684819, 0.12449952434684819, 0.12449952434684819, 0.35694259995702704, 0.35694259995702704, 0.35694259995702704, 0.17193702509720898, 0.17193702509720898, 0.17193702509720898, 0.13374219188996994, 0.13374219188996994, 0.13374219188996994, 0.2475515055418741, 0.2475515055418741, 0.2475515055418741, 0.2277427774917169, 0.2277427774917169, 0.2277427774917169, 0.16379641556242075, 0.16379641556242075, 0.16379641556242075, 0.20698761037109803, 0.20698761037109803, 0.20698761037109803, 0.2166739607932202, 0.2166739607932202, 0.2166739607932202, 0.12743080958398967, 0.12743080958398967, 0.12743080958398967, 0.20836779232376135, 0.20836779232376135, 0.20836779232376135, 0.2218463570025747, 0.2218463570025747, 0.2218463570025747, 0.2266211048527721, 0.2266211048527721, 0.2266211048527721, 0.19974449838695618, 0.19974449838695618, 0.19974449838695618, 0.19798302527326395, 0.19798302527326395, 0.19798302527326395, 0.20921222160855857, 0.20921222160855857, 0.20921222160855857, 0.5247151717610008, 0.5247151717610008, 0.5247151717610008, 0.1654403704961498, 0.1654403704961498, 0.1654403704961498, 0.16601278941076159, 0.16601278941076159, 0.16601278941076159, 0.16848472192743125, 0.16848472192743125, 0.16848472192743125, 0.2061724960517588, 0.2061724960517588, 0.2061724960517588, 0.14991930742146542, 0.14991930742146542, 0.14991930742146542, 0.17303961150351843, 0.17303961150351843, 0.17303961150351843, 0.19293796404668373, 0.19293796404668373, 0.19293796404668373, 0.21318655457744973, 0.21318655457744973, 0.21318655457744973, 0.09952291400030866, 0.09952291400030866, 0.09952291400030866, 0.06908111797165462, 0.06908111797165462, 0.06908111797165462, 0.07062270801207382, 0.07062270801207382, 0.07062270801207382]}, "mutation_prompt": null}
{"id": "620c9f99-ef9a-4a35-9ea9-1e620059e291", "solution": "import numpy as np\n\nclass PSO_SA_Optimizer_Enhanced:\n    def __init__(self, budget, dim, pop_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.w_max = 0.9  # Maximum inertia weight\n        self.w_min = 0.4  # Minimum inertia weight\n        self.c1 = 1.5  # Cognitive parameter\n        self.c2 = 1.5  # Social parameter\n        self.temp = 1.0  # Initial temperature for SA\n\n    def __call__(self, func):\n        np.random.seed(42)  # For reproducibility\n        \n        # Initialize particles\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.array([func(pos) for pos in positions])\n        global_best_position = personal_best_positions[np.argmin(personal_best_scores)]\n        global_best_score = np.min(personal_best_scores)\n        \n        eval_count = self.pop_size\n        \n        while eval_count < self.budget:\n            w = self.w_max - ((self.w_max - self.w_min) * (eval_count / self.budget))\n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                velocities[i] = (w * velocities[i]\n                                 + self.c1 * r1 * (personal_best_positions[i] - positions[i])\n                                 + self.c2 * r2 * (global_best_position - positions[i]))\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], self.lower_bound, self.upper_bound)\n\n                # Evaluate the new position\n                score = func(positions[i])\n                eval_count += 1\n                if eval_count >= self.budget:\n                    break\n\n                # Update personal best\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n\n                # Update global best\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = positions[i]\n\n                # Simulated Annealing Acceptance\n                if np.random.rand() < np.exp(-(score - global_best_score) / self.temp):\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n                    if score < global_best_score:\n                        global_best_score = score\n                        global_best_position = positions[i]\n\n            # Temperature cooling schedule\n            self.temp *= 0.95\n\n        return global_best_position, global_best_score", "name": "PSO_SA_Optimizer_Enhanced", "description": "Enhanced hybrid Particle Swarm Optimization with Simulated Annealing (PSO-SA) and adaptive inertia weight for improved convergence.", "configspace": "", "generation": 6, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "0411155c-9b0a-4e86-94b2-317eaab11b15", "metadata": {"aucs": [0.5541935495271029, 0.5541935495271029, 0.5541935495271029, 0.5472676162748873, 0.5472676162748873, 0.5472676162748873, 0.4995050001955086, 0.4995050001955086, 0.4995050001955086, 0.37677698123758485, 0.37677698123758485, 0.37677698123758485, 0.2602571254910885, 0.2602571254910885, 0.2602571254910885, 0.003330909247681313, 0.003330909247681313, 0.003330909247681313, 0.16044625244716004, 0.16044625244716004, 0.16044625244716004, 0.06165347877030414, 0.06165347877030414, 0.06165347877030414, 0.11895346010869268, 0.11895346010869268, 0.11895346010869268, 0.07493708294659229, 0.07493708294659229, 0.07493708294659229, 0.1223881702479801, 0.1223881702479801, 0.1223881702479801, 0.11364990543254094, 0.11364990543254094, 0.11364990543254094, 0.9917969237266142, 0.9917969237266142, 0.9917969237266142, 0.9947464319524106, 0.9947464319524106, 0.9947464319524106, 0.9923522331758592, 0.9923522331758592, 0.9923522331758592, 0.23292936537637376, 0.23292936537637376, 0.23292936537637376, 0.3241275789159769, 0.3241275789159769, 0.3241275789159769, 0.368066327161232, 0.368066327161232, 0.368066327161232, 0.3151000101329813, 0.3151000101329813, 0.3151000101329813, 0.15819099217998733, 0.15819099217998733, 0.15819099217998733, 0.1161189521585051, 0.1161189521585051, 0.1161189521585051, 0.39416284155128345, 0.39416284155128345, 0.39416284155128345, 0.12212977487851262, 0.12212977487851262, 0.12212977487851262, 0.2644190003152572, 0.2644190003152572, 0.2644190003152572, 0.35801657538571496, 0.35801657538571496, 0.35801657538571496, 0.12297914518591568, 0.12297914518591568, 0.12297914518591568, 0.1960278080415332, 0.1960278080415332, 0.1960278080415332, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.084775675890568, 0.084775675890568, 0.084775675890568, 0.3202396802670431, 0.3202396802670431, 0.3202396802670431, 0.07097896700228357, 0.07097896700228357, 0.07097896700228357, 0.08885677525536484, 0.08885677525536484, 0.08885677525536484, 0.03314189301901549, 0.03314189301901549, 0.03314189301901549, 0.04537410703272038, 0.04537410703272038, 0.04537410703272038, 0.03922412471399417, 0.03922412471399417, 0.03922412471399417, 0.1155219345450258, 0.1155219345450258, 0.1155219345450258, 0.034755019981584145, 0.034755019981584145, 0.034755019981584145, 0.07102106026707755, 0.07102106026707755, 0.07102106026707755, 0.5006794078134473, 0.5006794078134473, 0.5006794078134473, 0.4561303012753095, 0.4561303012753095, 0.4561303012753095, 0.5158501654337682, 0.5158501654337682, 0.5158501654337682, 0.11653379618486104, 0.11653379618486104, 0.11653379618486104, 0.07532313407863034, 0.07532313407863034, 0.07532313407863034, 0.12449952434684819, 0.12449952434684819, 0.12449952434684819, 0.35694259995702704, 0.35694259995702704, 0.35694259995702704, 0.17193702509720898, 0.17193702509720898, 0.17193702509720898, 0.13374219188996994, 0.13374219188996994, 0.13374219188996994, 0.2475515055418741, 0.2475515055418741, 0.2475515055418741, 0.2277427774917169, 0.2277427774917169, 0.2277427774917169, 0.16379641556242075, 0.16379641556242075, 0.16379641556242075, 0.20698761037109803, 0.20698761037109803, 0.20698761037109803, 0.2166739607932202, 0.2166739607932202, 0.2166739607932202, 0.12743080958398967, 0.12743080958398967, 0.12743080958398967, 0.20836779232376135, 0.20836779232376135, 0.20836779232376135, 0.2218463570025747, 0.2218463570025747, 0.2218463570025747, 0.2266211048527721, 0.2266211048527721, 0.2266211048527721, 0.19974449838695618, 0.19974449838695618, 0.19974449838695618, 0.19798302527326395, 0.19798302527326395, 0.19798302527326395, 0.20921222160855857, 0.20921222160855857, 0.20921222160855857, 0.5247151717610008, 0.5247151717610008, 0.5247151717610008, 0.1654403704961498, 0.1654403704961498, 0.1654403704961498, 0.16601278941076159, 0.16601278941076159, 0.16601278941076159, 0.16848472192743125, 0.16848472192743125, 0.16848472192743125, 0.2061724960517588, 0.2061724960517588, 0.2061724960517588, 0.14991930742146542, 0.14991930742146542, 0.14991930742146542, 0.17303961150351843, 0.17303961150351843, 0.17303961150351843, 0.19293796404668373, 0.19293796404668373, 0.19293796404668373, 0.21318655457744973, 0.21318655457744973, 0.21318655457744973, 0.09952291400030866, 0.09952291400030866, 0.09952291400030866, 0.06908111797165462, 0.06908111797165462, 0.06908111797165462, 0.07062270801207382, 0.07062270801207382, 0.07062270801207382]}, "mutation_prompt": null}
{"id": "dc77e202-b070-4bb5-9c1b-479b9b92e122", "solution": "import numpy as np\n\nclass PSO_SA_Optimizer_Enhanced:\n    def __init__(self, budget, dim, pop_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.w_max = 0.9  # Maximum inertia weight\n        self.w_min = 0.4  # Minimum inertia weight\n        self.c1 = 1.5  # Cognitive parameter\n        self.c2 = 1.5  # Social parameter\n        self.temp = 1.0  # Initial temperature for SA\n\n    def __call__(self, func):\n        np.random.seed(42)  # For reproducibility\n        \n        # Initialize particles\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.array([func(pos) for pos in positions])\n        global_best_position = personal_best_positions[np.argmin(personal_best_scores)]\n        global_best_score = np.min(personal_best_scores)\n        \n        eval_count = self.pop_size\n        \n        while eval_count < self.budget:\n            w = self.w_max - ((self.w_max - self.w_min) * (eval_count / self.budget))\n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                velocities[i] = (w * velocities[i]\n                                 + self.c1 * r1 * (personal_best_positions[i] - positions[i])\n                                 + self.c2 * r2 * (global_best_position - positions[i]))\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], self.lower_bound, self.upper_bound)\n\n                # Evaluate the new position\n                score = func(positions[i])\n                eval_count += 1\n                if eval_count >= self.budget:\n                    break\n\n                # Update personal best\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n\n                # Update global best\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = positions[i]\n\n                # Simulated Annealing Acceptance\n                if np.random.rand() < np.exp(-(score - global_best_score) / self.temp):\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n                    if score < global_best_score:\n                        global_best_score = score\n                        global_best_position = positions[i]\n\n            # Temperature cooling schedule\n            self.temp *= 0.95\n\n        return global_best_position, global_best_score", "name": "PSO_SA_Optimizer_Enhanced", "description": "Enhanced hybrid Particle Swarm Optimization with Simulated Annealing (PSO-SA) and adaptive inertia weight for improved convergence.", "configspace": "", "generation": 6, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "0411155c-9b0a-4e86-94b2-317eaab11b15", "metadata": {"aucs": [0.5541935495271029, 0.5541935495271029, 0.5541935495271029, 0.5472676162748873, 0.5472676162748873, 0.5472676162748873, 0.4995050001955086, 0.4995050001955086, 0.4995050001955086, 0.37677698123758485, 0.37677698123758485, 0.37677698123758485, 0.2602571254910885, 0.2602571254910885, 0.2602571254910885, 0.003330909247681313, 0.003330909247681313, 0.003330909247681313, 0.16044625244716004, 0.16044625244716004, 0.16044625244716004, 0.06165347877030414, 0.06165347877030414, 0.06165347877030414, 0.11895346010869268, 0.11895346010869268, 0.11895346010869268, 0.07493708294659229, 0.07493708294659229, 0.07493708294659229, 0.1223881702479801, 0.1223881702479801, 0.1223881702479801, 0.11364990543254094, 0.11364990543254094, 0.11364990543254094, 0.9917969237266142, 0.9917969237266142, 0.9917969237266142, 0.9947464319524106, 0.9947464319524106, 0.9947464319524106, 0.9923522331758592, 0.9923522331758592, 0.9923522331758592, 0.23292936537637376, 0.23292936537637376, 0.23292936537637376, 0.3241275789159769, 0.3241275789159769, 0.3241275789159769, 0.368066327161232, 0.368066327161232, 0.368066327161232, 0.3151000101329813, 0.3151000101329813, 0.3151000101329813, 0.15819099217998733, 0.15819099217998733, 0.15819099217998733, 0.1161189521585051, 0.1161189521585051, 0.1161189521585051, 0.39416284155128345, 0.39416284155128345, 0.39416284155128345, 0.12212977487851262, 0.12212977487851262, 0.12212977487851262, 0.2644190003152572, 0.2644190003152572, 0.2644190003152572, 0.35801657538571496, 0.35801657538571496, 0.35801657538571496, 0.12297914518591568, 0.12297914518591568, 0.12297914518591568, 0.1960278080415332, 0.1960278080415332, 0.1960278080415332, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.084775675890568, 0.084775675890568, 0.084775675890568, 0.3202396802670431, 0.3202396802670431, 0.3202396802670431, 0.07097896700228357, 0.07097896700228357, 0.07097896700228357, 0.08885677525536484, 0.08885677525536484, 0.08885677525536484, 0.03314189301901549, 0.03314189301901549, 0.03314189301901549, 0.04537410703272038, 0.04537410703272038, 0.04537410703272038, 0.03922412471399417, 0.03922412471399417, 0.03922412471399417, 0.1155219345450258, 0.1155219345450258, 0.1155219345450258, 0.034755019981584145, 0.034755019981584145, 0.034755019981584145, 0.07102106026707755, 0.07102106026707755, 0.07102106026707755, 0.5006794078134473, 0.5006794078134473, 0.5006794078134473, 0.4561303012753095, 0.4561303012753095, 0.4561303012753095, 0.5158501654337682, 0.5158501654337682, 0.5158501654337682, 0.11653379618486104, 0.11653379618486104, 0.11653379618486104, 0.07532313407863034, 0.07532313407863034, 0.07532313407863034, 0.12449952434684819, 0.12449952434684819, 0.12449952434684819, 0.35694259995702704, 0.35694259995702704, 0.35694259995702704, 0.17193702509720898, 0.17193702509720898, 0.17193702509720898, 0.13374219188996994, 0.13374219188996994, 0.13374219188996994, 0.2475515055418741, 0.2475515055418741, 0.2475515055418741, 0.2277427774917169, 0.2277427774917169, 0.2277427774917169, 0.16379641556242075, 0.16379641556242075, 0.16379641556242075, 0.20698761037109803, 0.20698761037109803, 0.20698761037109803, 0.2166739607932202, 0.2166739607932202, 0.2166739607932202, 0.12743080958398967, 0.12743080958398967, 0.12743080958398967, 0.20836779232376135, 0.20836779232376135, 0.20836779232376135, 0.2218463570025747, 0.2218463570025747, 0.2218463570025747, 0.2266211048527721, 0.2266211048527721, 0.2266211048527721, 0.19974449838695618, 0.19974449838695618, 0.19974449838695618, 0.19798302527326395, 0.19798302527326395, 0.19798302527326395, 0.20921222160855857, 0.20921222160855857, 0.20921222160855857, 0.5247151717610008, 0.5247151717610008, 0.5247151717610008, 0.1654403704961498, 0.1654403704961498, 0.1654403704961498, 0.16601278941076159, 0.16601278941076159, 0.16601278941076159, 0.16848472192743125, 0.16848472192743125, 0.16848472192743125, 0.2061724960517588, 0.2061724960517588, 0.2061724960517588, 0.14991930742146542, 0.14991930742146542, 0.14991930742146542, 0.17303961150351843, 0.17303961150351843, 0.17303961150351843, 0.19293796404668373, 0.19293796404668373, 0.19293796404668373, 0.21318655457744973, 0.21318655457744973, 0.21318655457744973, 0.09952291400030866, 0.09952291400030866, 0.09952291400030866, 0.06908111797165462, 0.06908111797165462, 0.06908111797165462, 0.07062270801207382, 0.07062270801207382, 0.07062270801207382]}, "mutation_prompt": null}
{"id": "4d324173-bc05-4355-be71-4fb655bea8c6", "solution": "import numpy as np\n\nclass PSO_SA_Optimizer_Enhanced:\n    def __init__(self, budget, dim, pop_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.w_max = 0.9  # Maximum inertia weight\n        self.w_min = 0.4  # Minimum inertia weight\n        self.c1 = 1.5  # Cognitive parameter\n        self.c2 = 1.5  # Social parameter\n        self.temp = 1.0  # Initial temperature for SA\n\n    def __call__(self, func):\n        np.random.seed(42)  # For reproducibility\n        \n        # Initialize particles\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.array([func(pos) for pos in positions])\n        global_best_position = personal_best_positions[np.argmin(personal_best_scores)]\n        global_best_score = np.min(personal_best_scores)\n        \n        eval_count = self.pop_size\n        \n        while eval_count < self.budget:\n            w = self.w_max - ((self.w_max - self.w_min) * (eval_count / self.budget))\n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                velocities[i] = (w * velocities[i]\n                                 + self.c1 * r1 * (personal_best_positions[i] - positions[i])\n                                 + self.c2 * r2 * (global_best_position - positions[i]))\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], self.lower_bound, self.upper_bound)\n\n                # Evaluate the new position\n                score = func(positions[i])\n                eval_count += 1\n                if eval_count >= self.budget:\n                    break\n\n                # Update personal best\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n\n                # Update global best\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = positions[i]\n\n                # Simulated Annealing Acceptance\n                if np.random.rand() < np.exp(-(score - global_best_score) / self.temp):\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n                    if score < global_best_score:\n                        global_best_score = score\n                        global_best_position = positions[i]\n\n            # Temperature cooling schedule\n            self.temp *= 0.95\n\n        return global_best_position, global_best_score", "name": "PSO_SA_Optimizer_Enhanced", "description": "Enhanced hybrid Particle Swarm Optimization with Simulated Annealing (PSO-SA) and adaptive inertia weight for improved convergence.", "configspace": "", "generation": 6, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "0411155c-9b0a-4e86-94b2-317eaab11b15", "metadata": {"aucs": [0.5541935495271029, 0.5541935495271029, 0.5541935495271029, 0.5472676162748873, 0.5472676162748873, 0.5472676162748873, 0.4995050001955086, 0.4995050001955086, 0.4995050001955086, 0.37677698123758485, 0.37677698123758485, 0.37677698123758485, 0.2602571254910885, 0.2602571254910885, 0.2602571254910885, 0.003330909247681313, 0.003330909247681313, 0.003330909247681313, 0.16044625244716004, 0.16044625244716004, 0.16044625244716004, 0.06165347877030414, 0.06165347877030414, 0.06165347877030414, 0.11895346010869268, 0.11895346010869268, 0.11895346010869268, 0.07493708294659229, 0.07493708294659229, 0.07493708294659229, 0.1223881702479801, 0.1223881702479801, 0.1223881702479801, 0.11364990543254094, 0.11364990543254094, 0.11364990543254094, 0.9917969237266142, 0.9917969237266142, 0.9917969237266142, 0.9947464319524106, 0.9947464319524106, 0.9947464319524106, 0.9923522331758592, 0.9923522331758592, 0.9923522331758592, 0.23292936537637376, 0.23292936537637376, 0.23292936537637376, 0.3241275789159769, 0.3241275789159769, 0.3241275789159769, 0.368066327161232, 0.368066327161232, 0.368066327161232, 0.3151000101329813, 0.3151000101329813, 0.3151000101329813, 0.15819099217998733, 0.15819099217998733, 0.15819099217998733, 0.1161189521585051, 0.1161189521585051, 0.1161189521585051, 0.39416284155128345, 0.39416284155128345, 0.39416284155128345, 0.12212977487851262, 0.12212977487851262, 0.12212977487851262, 0.2644190003152572, 0.2644190003152572, 0.2644190003152572, 0.35801657538571496, 0.35801657538571496, 0.35801657538571496, 0.12297914518591568, 0.12297914518591568, 0.12297914518591568, 0.1960278080415332, 0.1960278080415332, 0.1960278080415332, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.084775675890568, 0.084775675890568, 0.084775675890568, 0.3202396802670431, 0.3202396802670431, 0.3202396802670431, 0.07097896700228357, 0.07097896700228357, 0.07097896700228357, 0.08885677525536484, 0.08885677525536484, 0.08885677525536484, 0.03314189301901549, 0.03314189301901549, 0.03314189301901549, 0.04537410703272038, 0.04537410703272038, 0.04537410703272038, 0.03922412471399417, 0.03922412471399417, 0.03922412471399417, 0.1155219345450258, 0.1155219345450258, 0.1155219345450258, 0.034755019981584145, 0.034755019981584145, 0.034755019981584145, 0.07102106026707755, 0.07102106026707755, 0.07102106026707755, 0.5006794078134473, 0.5006794078134473, 0.5006794078134473, 0.4561303012753095, 0.4561303012753095, 0.4561303012753095, 0.5158501654337682, 0.5158501654337682, 0.5158501654337682, 0.11653379618486104, 0.11653379618486104, 0.11653379618486104, 0.07532313407863034, 0.07532313407863034, 0.07532313407863034, 0.12449952434684819, 0.12449952434684819, 0.12449952434684819, 0.35694259995702704, 0.35694259995702704, 0.35694259995702704, 0.17193702509720898, 0.17193702509720898, 0.17193702509720898, 0.13374219188996994, 0.13374219188996994, 0.13374219188996994, 0.2475515055418741, 0.2475515055418741, 0.2475515055418741, 0.2277427774917169, 0.2277427774917169, 0.2277427774917169, 0.16379641556242075, 0.16379641556242075, 0.16379641556242075, 0.20698761037109803, 0.20698761037109803, 0.20698761037109803, 0.2166739607932202, 0.2166739607932202, 0.2166739607932202, 0.12743080958398967, 0.12743080958398967, 0.12743080958398967, 0.20836779232376135, 0.20836779232376135, 0.20836779232376135, 0.2218463570025747, 0.2218463570025747, 0.2218463570025747, 0.2266211048527721, 0.2266211048527721, 0.2266211048527721, 0.19974449838695618, 0.19974449838695618, 0.19974449838695618, 0.19798302527326395, 0.19798302527326395, 0.19798302527326395, 0.20921222160855857, 0.20921222160855857, 0.20921222160855857, 0.5247151717610008, 0.5247151717610008, 0.5247151717610008, 0.1654403704961498, 0.1654403704961498, 0.1654403704961498, 0.16601278941076159, 0.16601278941076159, 0.16601278941076159, 0.16848472192743125, 0.16848472192743125, 0.16848472192743125, 0.2061724960517588, 0.2061724960517588, 0.2061724960517588, 0.14991930742146542, 0.14991930742146542, 0.14991930742146542, 0.17303961150351843, 0.17303961150351843, 0.17303961150351843, 0.19293796404668373, 0.19293796404668373, 0.19293796404668373, 0.21318655457744973, 0.21318655457744973, 0.21318655457744973, 0.09952291400030866, 0.09952291400030866, 0.09952291400030866, 0.06908111797165462, 0.06908111797165462, 0.06908111797165462, 0.07062270801207382, 0.07062270801207382, 0.07062270801207382]}, "mutation_prompt": null}
{"id": "3388e761-13b6-4fea-b831-4eb3cbc0ecc2", "solution": "import numpy as np\n\nclass PSO_SA_Optimizer_Enhanced:\n    def __init__(self, budget, dim, pop_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.w_max = 0.9  # Maximum inertia weight\n        self.w_min = 0.4  # Minimum inertia weight\n        self.c1 = 1.5  # Cognitive parameter\n        self.c2 = 1.5  # Social parameter\n        self.temp = 1.0  # Initial temperature for SA\n\n    def __call__(self, func):\n        np.random.seed(42)  # For reproducibility\n        \n        # Initialize particles\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.array([func(pos) for pos in positions])\n        global_best_position = personal_best_positions[np.argmin(personal_best_scores)]\n        global_best_score = np.min(personal_best_scores)\n        \n        eval_count = self.pop_size\n        \n        while eval_count < self.budget:\n            w = self.w_max - ((self.w_max - self.w_min) * (eval_count / self.budget))\n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                velocities[i] = (w * velocities[i]\n                                 + self.c1 * r1 * (personal_best_positions[i] - positions[i])\n                                 + self.c2 * r2 * (global_best_position - positions[i]))\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], self.lower_bound, self.upper_bound)\n\n                # Evaluate the new position\n                score = func(positions[i])\n                eval_count += 1\n                if eval_count >= self.budget:\n                    break\n\n                # Update personal best\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n\n                # Update global best\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = positions[i]\n\n                # Simulated Annealing Acceptance\n                if np.random.rand() < np.exp(-(score - global_best_score) / self.temp):\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n                    if score < global_best_score:\n                        global_best_score = score\n                        global_best_position = positions[i]\n\n            # Temperature cooling schedule\n            self.temp *= 0.95\n\n        return global_best_position, global_best_score", "name": "PSO_SA_Optimizer_Enhanced", "description": "Enhanced hybrid Particle Swarm Optimization with Simulated Annealing (PSO-SA) and adaptive inertia weight for improved convergence.", "configspace": "", "generation": 6, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "0411155c-9b0a-4e86-94b2-317eaab11b15", "metadata": {"aucs": [0.5541935495271029, 0.5541935495271029, 0.5541935495271029, 0.5472676162748873, 0.5472676162748873, 0.5472676162748873, 0.4995050001955086, 0.4995050001955086, 0.4995050001955086, 0.37677698123758485, 0.37677698123758485, 0.37677698123758485, 0.2602571254910885, 0.2602571254910885, 0.2602571254910885, 0.003330909247681313, 0.003330909247681313, 0.003330909247681313, 0.16044625244716004, 0.16044625244716004, 0.16044625244716004, 0.06165347877030414, 0.06165347877030414, 0.06165347877030414, 0.11895346010869268, 0.11895346010869268, 0.11895346010869268, 0.07493708294659229, 0.07493708294659229, 0.07493708294659229, 0.1223881702479801, 0.1223881702479801, 0.1223881702479801, 0.11364990543254094, 0.11364990543254094, 0.11364990543254094, 0.9917969237266142, 0.9917969237266142, 0.9917969237266142, 0.9947464319524106, 0.9947464319524106, 0.9947464319524106, 0.9923522331758592, 0.9923522331758592, 0.9923522331758592, 0.23292936537637376, 0.23292936537637376, 0.23292936537637376, 0.3241275789159769, 0.3241275789159769, 0.3241275789159769, 0.368066327161232, 0.368066327161232, 0.368066327161232, 0.3151000101329813, 0.3151000101329813, 0.3151000101329813, 0.15819099217998733, 0.15819099217998733, 0.15819099217998733, 0.1161189521585051, 0.1161189521585051, 0.1161189521585051, 0.39416284155128345, 0.39416284155128345, 0.39416284155128345, 0.12212977487851262, 0.12212977487851262, 0.12212977487851262, 0.2644190003152572, 0.2644190003152572, 0.2644190003152572, 0.35801657538571496, 0.35801657538571496, 0.35801657538571496, 0.12297914518591568, 0.12297914518591568, 0.12297914518591568, 0.1960278080415332, 0.1960278080415332, 0.1960278080415332, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.084775675890568, 0.084775675890568, 0.084775675890568, 0.3202396802670431, 0.3202396802670431, 0.3202396802670431, 0.07097896700228357, 0.07097896700228357, 0.07097896700228357, 0.08885677525536484, 0.08885677525536484, 0.08885677525536484, 0.03314189301901549, 0.03314189301901549, 0.03314189301901549, 0.04537410703272038, 0.04537410703272038, 0.04537410703272038, 0.03922412471399417, 0.03922412471399417, 0.03922412471399417, 0.1155219345450258, 0.1155219345450258, 0.1155219345450258, 0.034755019981584145, 0.034755019981584145, 0.034755019981584145, 0.07102106026707755, 0.07102106026707755, 0.07102106026707755, 0.5006794078134473, 0.5006794078134473, 0.5006794078134473, 0.4561303012753095, 0.4561303012753095, 0.4561303012753095, 0.5158501654337682, 0.5158501654337682, 0.5158501654337682, 0.11653379618486104, 0.11653379618486104, 0.11653379618486104, 0.07532313407863034, 0.07532313407863034, 0.07532313407863034, 0.12449952434684819, 0.12449952434684819, 0.12449952434684819, 0.35694259995702704, 0.35694259995702704, 0.35694259995702704, 0.17193702509720898, 0.17193702509720898, 0.17193702509720898, 0.13374219188996994, 0.13374219188996994, 0.13374219188996994, 0.2475515055418741, 0.2475515055418741, 0.2475515055418741, 0.2277427774917169, 0.2277427774917169, 0.2277427774917169, 0.16379641556242075, 0.16379641556242075, 0.16379641556242075, 0.20698761037109803, 0.20698761037109803, 0.20698761037109803, 0.2166739607932202, 0.2166739607932202, 0.2166739607932202, 0.12743080958398967, 0.12743080958398967, 0.12743080958398967, 0.20836779232376135, 0.20836779232376135, 0.20836779232376135, 0.2218463570025747, 0.2218463570025747, 0.2218463570025747, 0.2266211048527721, 0.2266211048527721, 0.2266211048527721, 0.19974449838695618, 0.19974449838695618, 0.19974449838695618, 0.19798302527326395, 0.19798302527326395, 0.19798302527326395, 0.20921222160855857, 0.20921222160855857, 0.20921222160855857, 0.5247151717610008, 0.5247151717610008, 0.5247151717610008, 0.1654403704961498, 0.1654403704961498, 0.1654403704961498, 0.16601278941076159, 0.16601278941076159, 0.16601278941076159, 0.16848472192743125, 0.16848472192743125, 0.16848472192743125, 0.2061724960517588, 0.2061724960517588, 0.2061724960517588, 0.14991930742146542, 0.14991930742146542, 0.14991930742146542, 0.17303961150351843, 0.17303961150351843, 0.17303961150351843, 0.19293796404668373, 0.19293796404668373, 0.19293796404668373, 0.21318655457744973, 0.21318655457744973, 0.21318655457744973, 0.09952291400030866, 0.09952291400030866, 0.09952291400030866, 0.06908111797165462, 0.06908111797165462, 0.06908111797165462, 0.07062270801207382, 0.07062270801207382, 0.07062270801207382]}, "mutation_prompt": null}
{"id": "1263a803-f673-4b31-a9d2-edc5b7967e4c", "solution": "import numpy as np\n\nclass PSO_SA_Optimizer_Enhanced:\n    def __init__(self, budget, dim, pop_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.w_max = 0.9  # Maximum inertia weight\n        self.w_min = 0.4  # Minimum inertia weight\n        self.c1 = 1.5  # Cognitive parameter\n        self.c2 = 1.5  # Social parameter\n        self.temp = 1.0  # Initial temperature for SA\n\n    def __call__(self, func):\n        np.random.seed(42)  # For reproducibility\n        \n        # Initialize particles\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.array([func(pos) for pos in positions])\n        global_best_position = personal_best_positions[np.argmin(personal_best_scores)]\n        global_best_score = np.min(personal_best_scores)\n        \n        eval_count = self.pop_size\n        \n        while eval_count < self.budget:\n            w = self.w_max - ((self.w_max - self.w_min) * (eval_count / self.budget))\n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                velocities[i] = (w * velocities[i]\n                                 + self.c1 * r1 * (personal_best_positions[i] - positions[i])\n                                 + self.c2 * r2 * (global_best_position - positions[i]))\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], self.lower_bound, self.upper_bound)\n\n                # Evaluate the new position\n                score = func(positions[i])\n                eval_count += 1\n                if eval_count >= self.budget:\n                    break\n\n                # Update personal best\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n\n                # Update global best\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = positions[i]\n\n                # Simulated Annealing Acceptance\n                if np.random.rand() < np.exp(-(score - global_best_score) / self.temp):\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n                    if score < global_best_score:\n                        global_best_score = score\n                        global_best_position = positions[i]\n\n            # Temperature cooling schedule\n            self.temp *= 0.95\n\n        return global_best_position, global_best_score", "name": "PSO_SA_Optimizer_Enhanced", "description": "Enhanced hybrid Particle Swarm Optimization with Simulated Annealing (PSO-SA) and adaptive inertia weight for improved convergence.", "configspace": "", "generation": 6, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "0411155c-9b0a-4e86-94b2-317eaab11b15", "metadata": {"aucs": [0.5541935495271029, 0.5541935495271029, 0.5541935495271029, 0.5472676162748873, 0.5472676162748873, 0.5472676162748873, 0.4995050001955086, 0.4995050001955086, 0.4995050001955086, 0.37677698123758485, 0.37677698123758485, 0.37677698123758485, 0.2602571254910885, 0.2602571254910885, 0.2602571254910885, 0.003330909247681313, 0.003330909247681313, 0.003330909247681313, 0.16044625244716004, 0.16044625244716004, 0.16044625244716004, 0.06165347877030414, 0.06165347877030414, 0.06165347877030414, 0.11895346010869268, 0.11895346010869268, 0.11895346010869268, 0.07493708294659229, 0.07493708294659229, 0.07493708294659229, 0.1223881702479801, 0.1223881702479801, 0.1223881702479801, 0.11364990543254094, 0.11364990543254094, 0.11364990543254094, 0.9917969237266142, 0.9917969237266142, 0.9917969237266142, 0.9947464319524106, 0.9947464319524106, 0.9947464319524106, 0.9923522331758592, 0.9923522331758592, 0.9923522331758592, 0.23292936537637376, 0.23292936537637376, 0.23292936537637376, 0.3241275789159769, 0.3241275789159769, 0.3241275789159769, 0.368066327161232, 0.368066327161232, 0.368066327161232, 0.3151000101329813, 0.3151000101329813, 0.3151000101329813, 0.15819099217998733, 0.15819099217998733, 0.15819099217998733, 0.1161189521585051, 0.1161189521585051, 0.1161189521585051, 0.39416284155128345, 0.39416284155128345, 0.39416284155128345, 0.12212977487851262, 0.12212977487851262, 0.12212977487851262, 0.2644190003152572, 0.2644190003152572, 0.2644190003152572, 0.35801657538571496, 0.35801657538571496, 0.35801657538571496, 0.12297914518591568, 0.12297914518591568, 0.12297914518591568, 0.1960278080415332, 0.1960278080415332, 0.1960278080415332, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.084775675890568, 0.084775675890568, 0.084775675890568, 0.3202396802670431, 0.3202396802670431, 0.3202396802670431, 0.07097896700228357, 0.07097896700228357, 0.07097896700228357, 0.08885677525536484, 0.08885677525536484, 0.08885677525536484, 0.03314189301901549, 0.03314189301901549, 0.03314189301901549, 0.04537410703272038, 0.04537410703272038, 0.04537410703272038, 0.03922412471399417, 0.03922412471399417, 0.03922412471399417, 0.1155219345450258, 0.1155219345450258, 0.1155219345450258, 0.034755019981584145, 0.034755019981584145, 0.034755019981584145, 0.07102106026707755, 0.07102106026707755, 0.07102106026707755, 0.5006794078134473, 0.5006794078134473, 0.5006794078134473, 0.4561303012753095, 0.4561303012753095, 0.4561303012753095, 0.5158501654337682, 0.5158501654337682, 0.5158501654337682, 0.11653379618486104, 0.11653379618486104, 0.11653379618486104, 0.07532313407863034, 0.07532313407863034, 0.07532313407863034, 0.12449952434684819, 0.12449952434684819, 0.12449952434684819, 0.35694259995702704, 0.35694259995702704, 0.35694259995702704, 0.17193702509720898, 0.17193702509720898, 0.17193702509720898, 0.13374219188996994, 0.13374219188996994, 0.13374219188996994, 0.2475515055418741, 0.2475515055418741, 0.2475515055418741, 0.2277427774917169, 0.2277427774917169, 0.2277427774917169, 0.16379641556242075, 0.16379641556242075, 0.16379641556242075, 0.20698761037109803, 0.20698761037109803, 0.20698761037109803, 0.2166739607932202, 0.2166739607932202, 0.2166739607932202, 0.12743080958398967, 0.12743080958398967, 0.12743080958398967, 0.20836779232376135, 0.20836779232376135, 0.20836779232376135, 0.2218463570025747, 0.2218463570025747, 0.2218463570025747, 0.2266211048527721, 0.2266211048527721, 0.2266211048527721, 0.19974449838695618, 0.19974449838695618, 0.19974449838695618, 0.19798302527326395, 0.19798302527326395, 0.19798302527326395, 0.20921222160855857, 0.20921222160855857, 0.20921222160855857, 0.5247151717610008, 0.5247151717610008, 0.5247151717610008, 0.1654403704961498, 0.1654403704961498, 0.1654403704961498, 0.16601278941076159, 0.16601278941076159, 0.16601278941076159, 0.16848472192743125, 0.16848472192743125, 0.16848472192743125, 0.2061724960517588, 0.2061724960517588, 0.2061724960517588, 0.14991930742146542, 0.14991930742146542, 0.14991930742146542, 0.17303961150351843, 0.17303961150351843, 0.17303961150351843, 0.19293796404668373, 0.19293796404668373, 0.19293796404668373, 0.21318655457744973, 0.21318655457744973, 0.21318655457744973, 0.09952291400030866, 0.09952291400030866, 0.09952291400030866, 0.06908111797165462, 0.06908111797165462, 0.06908111797165462, 0.07062270801207382, 0.07062270801207382, 0.07062270801207382]}, "mutation_prompt": null}
{"id": "d887b417-7454-4785-bca0-f9d249a9c56d", "solution": "import numpy as np\n\nclass PSO_SA_Optimizer_Enhanced:\n    def __init__(self, budget, dim, pop_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.w_max = 0.9  # Maximum inertia weight\n        self.w_min = 0.4  # Minimum inertia weight\n        self.c1 = 1.5  # Cognitive parameter\n        self.c2 = 1.5  # Social parameter\n        self.temp = 1.0  # Initial temperature for SA\n\n    def __call__(self, func):\n        np.random.seed(42)  # For reproducibility\n        \n        # Initialize particles\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.array([func(pos) for pos in positions])\n        global_best_position = personal_best_positions[np.argmin(personal_best_scores)]\n        global_best_score = np.min(personal_best_scores)\n        \n        eval_count = self.pop_size\n        \n        while eval_count < self.budget:\n            w = self.w_max - ((self.w_max - self.w_min) * (eval_count / self.budget))\n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                velocities[i] = (w * velocities[i]\n                                 + self.c1 * r1 * (personal_best_positions[i] - positions[i])\n                                 + self.c2 * r2 * (global_best_position - positions[i]))\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], self.lower_bound, self.upper_bound)\n\n                # Evaluate the new position\n                score = func(positions[i])\n                eval_count += 1\n                if eval_count >= self.budget:\n                    break\n\n                # Update personal best\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n\n                # Update global best\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = positions[i]\n\n                # Simulated Annealing Acceptance\n                if np.random.rand() < np.exp(-(score - global_best_score) / self.temp):\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n                    if score < global_best_score:\n                        global_best_score = score\n                        global_best_position = positions[i]\n\n            # Temperature cooling schedule\n            self.temp *= 0.95\n\n        return global_best_position, global_best_score", "name": "PSO_SA_Optimizer_Enhanced", "description": "Enhanced hybrid Particle Swarm Optimization with Simulated Annealing (PSO-SA) and adaptive inertia weight for improved convergence.", "configspace": "", "generation": 6, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "0411155c-9b0a-4e86-94b2-317eaab11b15", "metadata": {"aucs": [0.5541935495271029, 0.5541935495271029, 0.5541935495271029, 0.5472676162748873, 0.5472676162748873, 0.5472676162748873, 0.4995050001955086, 0.4995050001955086, 0.4995050001955086, 0.37677698123758485, 0.37677698123758485, 0.37677698123758485, 0.2602571254910885, 0.2602571254910885, 0.2602571254910885, 0.003330909247681313, 0.003330909247681313, 0.003330909247681313, 0.16044625244716004, 0.16044625244716004, 0.16044625244716004, 0.06165347877030414, 0.06165347877030414, 0.06165347877030414, 0.11895346010869268, 0.11895346010869268, 0.11895346010869268, 0.07493708294659229, 0.07493708294659229, 0.07493708294659229, 0.1223881702479801, 0.1223881702479801, 0.1223881702479801, 0.11364990543254094, 0.11364990543254094, 0.11364990543254094, 0.9917969237266142, 0.9917969237266142, 0.9917969237266142, 0.9947464319524106, 0.9947464319524106, 0.9947464319524106, 0.9923522331758592, 0.9923522331758592, 0.9923522331758592, 0.23292936537637376, 0.23292936537637376, 0.23292936537637376, 0.3241275789159769, 0.3241275789159769, 0.3241275789159769, 0.368066327161232, 0.368066327161232, 0.368066327161232, 0.3151000101329813, 0.3151000101329813, 0.3151000101329813, 0.15819099217998733, 0.15819099217998733, 0.15819099217998733, 0.1161189521585051, 0.1161189521585051, 0.1161189521585051, 0.39416284155128345, 0.39416284155128345, 0.39416284155128345, 0.12212977487851262, 0.12212977487851262, 0.12212977487851262, 0.2644190003152572, 0.2644190003152572, 0.2644190003152572, 0.35801657538571496, 0.35801657538571496, 0.35801657538571496, 0.12297914518591568, 0.12297914518591568, 0.12297914518591568, 0.1960278080415332, 0.1960278080415332, 0.1960278080415332, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.084775675890568, 0.084775675890568, 0.084775675890568, 0.3202396802670431, 0.3202396802670431, 0.3202396802670431, 0.07097896700228357, 0.07097896700228357, 0.07097896700228357, 0.08885677525536484, 0.08885677525536484, 0.08885677525536484, 0.03314189301901549, 0.03314189301901549, 0.03314189301901549, 0.04537410703272038, 0.04537410703272038, 0.04537410703272038, 0.03922412471399417, 0.03922412471399417, 0.03922412471399417, 0.1155219345450258, 0.1155219345450258, 0.1155219345450258, 0.034755019981584145, 0.034755019981584145, 0.034755019981584145, 0.07102106026707755, 0.07102106026707755, 0.07102106026707755, 0.5006794078134473, 0.5006794078134473, 0.5006794078134473, 0.4561303012753095, 0.4561303012753095, 0.4561303012753095, 0.5158501654337682, 0.5158501654337682, 0.5158501654337682, 0.11653379618486104, 0.11653379618486104, 0.11653379618486104, 0.07532313407863034, 0.07532313407863034, 0.07532313407863034, 0.12449952434684819, 0.12449952434684819, 0.12449952434684819, 0.35694259995702704, 0.35694259995702704, 0.35694259995702704, 0.17193702509720898, 0.17193702509720898, 0.17193702509720898, 0.13374219188996994, 0.13374219188996994, 0.13374219188996994, 0.2475515055418741, 0.2475515055418741, 0.2475515055418741, 0.2277427774917169, 0.2277427774917169, 0.2277427774917169, 0.16379641556242075, 0.16379641556242075, 0.16379641556242075, 0.20698761037109803, 0.20698761037109803, 0.20698761037109803, 0.2166739607932202, 0.2166739607932202, 0.2166739607932202, 0.12743080958398967, 0.12743080958398967, 0.12743080958398967, 0.20836779232376135, 0.20836779232376135, 0.20836779232376135, 0.2218463570025747, 0.2218463570025747, 0.2218463570025747, 0.2266211048527721, 0.2266211048527721, 0.2266211048527721, 0.19974449838695618, 0.19974449838695618, 0.19974449838695618, 0.19798302527326395, 0.19798302527326395, 0.19798302527326395, 0.20921222160855857, 0.20921222160855857, 0.20921222160855857, 0.5247151717610008, 0.5247151717610008, 0.5247151717610008, 0.1654403704961498, 0.1654403704961498, 0.1654403704961498, 0.16601278941076159, 0.16601278941076159, 0.16601278941076159, 0.16848472192743125, 0.16848472192743125, 0.16848472192743125, 0.2061724960517588, 0.2061724960517588, 0.2061724960517588, 0.14991930742146542, 0.14991930742146542, 0.14991930742146542, 0.17303961150351843, 0.17303961150351843, 0.17303961150351843, 0.19293796404668373, 0.19293796404668373, 0.19293796404668373, 0.21318655457744973, 0.21318655457744973, 0.21318655457744973, 0.09952291400030866, 0.09952291400030866, 0.09952291400030866, 0.06908111797165462, 0.06908111797165462, 0.06908111797165462, 0.07062270801207382, 0.07062270801207382, 0.07062270801207382]}, "mutation_prompt": null}
{"id": "6a5418a4-632d-4e40-98dc-2c01c1303c9b", "solution": "import numpy as np\n\nclass PSO_SA_Optimizer_Enhanced:\n    def __init__(self, budget, dim, pop_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.w_max = 0.9  # Maximum inertia weight\n        self.w_min = 0.4  # Minimum inertia weight\n        self.c1 = 1.5  # Cognitive parameter\n        self.c2 = 1.5  # Social parameter\n        self.temp = 1.0  # Initial temperature for SA\n\n    def __call__(self, func):\n        np.random.seed(42)  # For reproducibility\n        \n        # Initialize particles\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.array([func(pos) for pos in positions])\n        global_best_position = personal_best_positions[np.argmin(personal_best_scores)]\n        global_best_score = np.min(personal_best_scores)\n        \n        eval_count = self.pop_size\n        \n        while eval_count < self.budget:\n            w = self.w_max - ((self.w_max - self.w_min) * (eval_count / self.budget))\n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                velocities[i] = (w * velocities[i]\n                                 + self.c1 * r1 * (personal_best_positions[i] - positions[i])\n                                 + self.c2 * r2 * (global_best_position - positions[i]))\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], self.lower_bound, self.upper_bound)\n\n                # Evaluate the new position\n                score = func(positions[i])\n                eval_count += 1\n                if eval_count >= self.budget:\n                    break\n\n                # Update personal best\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n\n                # Update global best\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = positions[i]\n\n                # Simulated Annealing Acceptance\n                if np.random.rand() < np.exp(-(score - global_best_score) / self.temp):\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n                    if score < global_best_score:\n                        global_best_score = score\n                        global_best_position = positions[i]\n\n            # Temperature cooling schedule\n            self.temp *= 0.95\n\n        return global_best_position, global_best_score", "name": "PSO_SA_Optimizer_Enhanced", "description": "Enhanced hybrid Particle Swarm Optimization with Simulated Annealing (PSO-SA) and adaptive inertia weight for improved convergence.", "configspace": "", "generation": 6, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "0411155c-9b0a-4e86-94b2-317eaab11b15", "metadata": {"aucs": [0.5541935495271029, 0.5541935495271029, 0.5541935495271029, 0.5472676162748873, 0.5472676162748873, 0.5472676162748873, 0.4995050001955086, 0.4995050001955086, 0.4995050001955086, 0.37677698123758485, 0.37677698123758485, 0.37677698123758485, 0.2602571254910885, 0.2602571254910885, 0.2602571254910885, 0.003330909247681313, 0.003330909247681313, 0.003330909247681313, 0.16044625244716004, 0.16044625244716004, 0.16044625244716004, 0.06165347877030414, 0.06165347877030414, 0.06165347877030414, 0.11895346010869268, 0.11895346010869268, 0.11895346010869268, 0.07493708294659229, 0.07493708294659229, 0.07493708294659229, 0.1223881702479801, 0.1223881702479801, 0.1223881702479801, 0.11364990543254094, 0.11364990543254094, 0.11364990543254094, 0.9917969237266142, 0.9917969237266142, 0.9917969237266142, 0.9947464319524106, 0.9947464319524106, 0.9947464319524106, 0.9923522331758592, 0.9923522331758592, 0.9923522331758592, 0.23292936537637376, 0.23292936537637376, 0.23292936537637376, 0.3241275789159769, 0.3241275789159769, 0.3241275789159769, 0.368066327161232, 0.368066327161232, 0.368066327161232, 0.3151000101329813, 0.3151000101329813, 0.3151000101329813, 0.15819099217998733, 0.15819099217998733, 0.15819099217998733, 0.1161189521585051, 0.1161189521585051, 0.1161189521585051, 0.39416284155128345, 0.39416284155128345, 0.39416284155128345, 0.12212977487851262, 0.12212977487851262, 0.12212977487851262, 0.2644190003152572, 0.2644190003152572, 0.2644190003152572, 0.35801657538571496, 0.35801657538571496, 0.35801657538571496, 0.12297914518591568, 0.12297914518591568, 0.12297914518591568, 0.1960278080415332, 0.1960278080415332, 0.1960278080415332, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.084775675890568, 0.084775675890568, 0.084775675890568, 0.3202396802670431, 0.3202396802670431, 0.3202396802670431, 0.07097896700228357, 0.07097896700228357, 0.07097896700228357, 0.08885677525536484, 0.08885677525536484, 0.08885677525536484, 0.03314189301901549, 0.03314189301901549, 0.03314189301901549, 0.04537410703272038, 0.04537410703272038, 0.04537410703272038, 0.03922412471399417, 0.03922412471399417, 0.03922412471399417, 0.1155219345450258, 0.1155219345450258, 0.1155219345450258, 0.034755019981584145, 0.034755019981584145, 0.034755019981584145, 0.07102106026707755, 0.07102106026707755, 0.07102106026707755, 0.5006794078134473, 0.5006794078134473, 0.5006794078134473, 0.4561303012753095, 0.4561303012753095, 0.4561303012753095, 0.5158501654337682, 0.5158501654337682, 0.5158501654337682, 0.11653379618486104, 0.11653379618486104, 0.11653379618486104, 0.07532313407863034, 0.07532313407863034, 0.07532313407863034, 0.12449952434684819, 0.12449952434684819, 0.12449952434684819, 0.35694259995702704, 0.35694259995702704, 0.35694259995702704, 0.17193702509720898, 0.17193702509720898, 0.17193702509720898, 0.13374219188996994, 0.13374219188996994, 0.13374219188996994, 0.2475515055418741, 0.2475515055418741, 0.2475515055418741, 0.2277427774917169, 0.2277427774917169, 0.2277427774917169, 0.16379641556242075, 0.16379641556242075, 0.16379641556242075, 0.20698761037109803, 0.20698761037109803, 0.20698761037109803, 0.2166739607932202, 0.2166739607932202, 0.2166739607932202, 0.12743080958398967, 0.12743080958398967, 0.12743080958398967, 0.20836779232376135, 0.20836779232376135, 0.20836779232376135, 0.2218463570025747, 0.2218463570025747, 0.2218463570025747, 0.2266211048527721, 0.2266211048527721, 0.2266211048527721, 0.19974449838695618, 0.19974449838695618, 0.19974449838695618, 0.19798302527326395, 0.19798302527326395, 0.19798302527326395, 0.20921222160855857, 0.20921222160855857, 0.20921222160855857, 0.5247151717610008, 0.5247151717610008, 0.5247151717610008, 0.1654403704961498, 0.1654403704961498, 0.1654403704961498, 0.16601278941076159, 0.16601278941076159, 0.16601278941076159, 0.16848472192743125, 0.16848472192743125, 0.16848472192743125, 0.2061724960517588, 0.2061724960517588, 0.2061724960517588, 0.14991930742146542, 0.14991930742146542, 0.14991930742146542, 0.17303961150351843, 0.17303961150351843, 0.17303961150351843, 0.19293796404668373, 0.19293796404668373, 0.19293796404668373, 0.21318655457744973, 0.21318655457744973, 0.21318655457744973, 0.09952291400030866, 0.09952291400030866, 0.09952291400030866, 0.06908111797165462, 0.06908111797165462, 0.06908111797165462, 0.07062270801207382, 0.07062270801207382, 0.07062270801207382]}, "mutation_prompt": null}
{"id": "26a26ec0-bf83-4b9a-9f23-9fcbe16619ff", "solution": "import numpy as np\n\nclass PSO_SA_Optimizer_Enhanced:\n    def __init__(self, budget, dim, pop_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.w_max = 0.9  # Maximum inertia weight\n        self.w_min = 0.4  # Minimum inertia weight\n        self.c1 = 1.5  # Cognitive parameter\n        self.c2 = 1.5  # Social parameter\n        self.temp = 1.0  # Initial temperature for SA\n\n    def __call__(self, func):\n        np.random.seed(42)  # For reproducibility\n        \n        # Initialize particles\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.array([func(pos) for pos in positions])\n        global_best_position = personal_best_positions[np.argmin(personal_best_scores)]\n        global_best_score = np.min(personal_best_scores)\n        \n        eval_count = self.pop_size\n        \n        while eval_count < self.budget:\n            w = self.w_max - ((self.w_max - self.w_min) * (eval_count / self.budget))\n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                velocities[i] = (w * velocities[i]\n                                 + self.c1 * r1 * (personal_best_positions[i] - positions[i])\n                                 + self.c2 * r2 * (global_best_position - positions[i]))\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], self.lower_bound, self.upper_bound)\n\n                # Evaluate the new position\n                score = func(positions[i])\n                eval_count += 1\n                if eval_count >= self.budget:\n                    break\n\n                # Update personal best\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n\n                # Update global best\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = positions[i]\n\n                # Simulated Annealing Acceptance\n                if np.random.rand() < np.exp(-(score - global_best_score) / self.temp):\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n                    if score < global_best_score:\n                        global_best_score = score\n                        global_best_position = positions[i]\n\n            # Temperature cooling schedule\n            self.temp *= 0.95\n\n        return global_best_position, global_best_score", "name": "PSO_SA_Optimizer_Enhanced", "description": "Enhanced hybrid Particle Swarm Optimization with Simulated Annealing (PSO-SA) and adaptive inertia weight for improved convergence.", "configspace": "", "generation": 6, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "0411155c-9b0a-4e86-94b2-317eaab11b15", "metadata": {"aucs": [0.5541935495271029, 0.5541935495271029, 0.5541935495271029, 0.5472676162748873, 0.5472676162748873, 0.5472676162748873, 0.4995050001955086, 0.4995050001955086, 0.4995050001955086, 0.37677698123758485, 0.37677698123758485, 0.37677698123758485, 0.2602571254910885, 0.2602571254910885, 0.2602571254910885, 0.003330909247681313, 0.003330909247681313, 0.003330909247681313, 0.16044625244716004, 0.16044625244716004, 0.16044625244716004, 0.06165347877030414, 0.06165347877030414, 0.06165347877030414, 0.11895346010869268, 0.11895346010869268, 0.11895346010869268, 0.07493708294659229, 0.07493708294659229, 0.07493708294659229, 0.1223881702479801, 0.1223881702479801, 0.1223881702479801, 0.11364990543254094, 0.11364990543254094, 0.11364990543254094, 0.9917969237266142, 0.9917969237266142, 0.9917969237266142, 0.9947464319524106, 0.9947464319524106, 0.9947464319524106, 0.9923522331758592, 0.9923522331758592, 0.9923522331758592, 0.23292936537637376, 0.23292936537637376, 0.23292936537637376, 0.3241275789159769, 0.3241275789159769, 0.3241275789159769, 0.368066327161232, 0.368066327161232, 0.368066327161232, 0.3151000101329813, 0.3151000101329813, 0.3151000101329813, 0.15819099217998733, 0.15819099217998733, 0.15819099217998733, 0.1161189521585051, 0.1161189521585051, 0.1161189521585051, 0.39416284155128345, 0.39416284155128345, 0.39416284155128345, 0.12212977487851262, 0.12212977487851262, 0.12212977487851262, 0.2644190003152572, 0.2644190003152572, 0.2644190003152572, 0.35801657538571496, 0.35801657538571496, 0.35801657538571496, 0.12297914518591568, 0.12297914518591568, 0.12297914518591568, 0.1960278080415332, 0.1960278080415332, 0.1960278080415332, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.084775675890568, 0.084775675890568, 0.084775675890568, 0.3202396802670431, 0.3202396802670431, 0.3202396802670431, 0.07097896700228357, 0.07097896700228357, 0.07097896700228357, 0.08885677525536484, 0.08885677525536484, 0.08885677525536484, 0.03314189301901549, 0.03314189301901549, 0.03314189301901549, 0.04537410703272038, 0.04537410703272038, 0.04537410703272038, 0.03922412471399417, 0.03922412471399417, 0.03922412471399417, 0.1155219345450258, 0.1155219345450258, 0.1155219345450258, 0.034755019981584145, 0.034755019981584145, 0.034755019981584145, 0.07102106026707755, 0.07102106026707755, 0.07102106026707755, 0.5006794078134473, 0.5006794078134473, 0.5006794078134473, 0.4561303012753095, 0.4561303012753095, 0.4561303012753095, 0.5158501654337682, 0.5158501654337682, 0.5158501654337682, 0.11653379618486104, 0.11653379618486104, 0.11653379618486104, 0.07532313407863034, 0.07532313407863034, 0.07532313407863034, 0.12449952434684819, 0.12449952434684819, 0.12449952434684819, 0.35694259995702704, 0.35694259995702704, 0.35694259995702704, 0.17193702509720898, 0.17193702509720898, 0.17193702509720898, 0.13374219188996994, 0.13374219188996994, 0.13374219188996994, 0.2475515055418741, 0.2475515055418741, 0.2475515055418741, 0.2277427774917169, 0.2277427774917169, 0.2277427774917169, 0.16379641556242075, 0.16379641556242075, 0.16379641556242075, 0.20698761037109803, 0.20698761037109803, 0.20698761037109803, 0.2166739607932202, 0.2166739607932202, 0.2166739607932202, 0.12743080958398967, 0.12743080958398967, 0.12743080958398967, 0.20836779232376135, 0.20836779232376135, 0.20836779232376135, 0.2218463570025747, 0.2218463570025747, 0.2218463570025747, 0.2266211048527721, 0.2266211048527721, 0.2266211048527721, 0.19974449838695618, 0.19974449838695618, 0.19974449838695618, 0.19798302527326395, 0.19798302527326395, 0.19798302527326395, 0.20921222160855857, 0.20921222160855857, 0.20921222160855857, 0.5247151717610008, 0.5247151717610008, 0.5247151717610008, 0.1654403704961498, 0.1654403704961498, 0.1654403704961498, 0.16601278941076159, 0.16601278941076159, 0.16601278941076159, 0.16848472192743125, 0.16848472192743125, 0.16848472192743125, 0.2061724960517588, 0.2061724960517588, 0.2061724960517588, 0.14991930742146542, 0.14991930742146542, 0.14991930742146542, 0.17303961150351843, 0.17303961150351843, 0.17303961150351843, 0.19293796404668373, 0.19293796404668373, 0.19293796404668373, 0.21318655457744973, 0.21318655457744973, 0.21318655457744973, 0.09952291400030866, 0.09952291400030866, 0.09952291400030866, 0.06908111797165462, 0.06908111797165462, 0.06908111797165462, 0.07062270801207382, 0.07062270801207382, 0.07062270801207382]}, "mutation_prompt": null}
{"id": "a6ced734-3ce2-4438-a3b1-cd500dc0bfe6", "solution": "import numpy as np\n\nclass PSO_SA_Optimizer_Enhanced:\n    def __init__(self, budget, dim, pop_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.w_max = 0.9\n        self.w_min = 0.4\n        self.c1_init = 2.0  # Initial cognitive parameter\n        self.c2_init = 1.0  # Initial social parameter\n        self.temp = 1.0\n\n    def __call__(self, func):\n        np.random.seed(42)\n        \n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.array([func(pos) for pos in positions])\n        global_best_position = personal_best_positions[np.argmin(personal_best_scores)]\n        global_best_score = np.min(personal_best_scores)\n        \n        eval_count = self.pop_size\n        \n        while eval_count < self.budget:\n            w = self.w_max - ((self.w_max - self.w_min) * (eval_count / self.budget))\n            c1 = self.c1_init * (1 - eval_count / self.budget)  # Dynamic cognitive\n            c2 = self.c2_init * (eval_count / self.budget)  # Dynamic social\n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                velocities[i] = (w * velocities[i]\n                                 + c1 * r1 * (personal_best_positions[i] - positions[i])\n                                 + c2 * r2 * (global_best_position - positions[i]))\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], self.lower_bound, self.upper_bound)\n\n                score = func(positions[i])\n                eval_count += 1\n                if eval_count >= self.budget:\n                    break\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = positions[i]\n\n                if np.random.rand() < np.exp(-(score - global_best_score) / self.temp):\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n                    if score < global_best_score:\n                        global_best_score = score\n                        global_best_position = positions[i]\n\n            self.temp *= 0.95\n\n        return global_best_position, global_best_score", "name": "PSO_SA_Optimizer_Enhanced", "description": "Modified PSO-SA incorporating dynamic cognitive and social parameters for improved convergence.", "configspace": "", "generation": 24, "fitness": 0.23594832018982562, "feedback": "The algorithm PSO_SA_Optimizer_Enhanced got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.24 with standard deviation 0.19.", "error": "", "parent_id": "0411155c-9b0a-4e86-94b2-317eaab11b15", "metadata": {"aucs": [0.5552032341291239, 0.5552032341291239, 0.5552032341291239, 0.45079232168892436, 0.45079232168892436, 0.45079232168892436, 0.49261961612188865, 0.49261961612188865, 0.49261961612188865, 0.1534035740593066, 0.1534035740593066, 0.1534035740593066, 0.03196945674084872, 0.03196945674084872, 0.03196945674084872, 0.028423438180651517, 0.028423438180651517, 0.028423438180651517, 0.09722680421452112, 0.09722680421452112, 0.09722680421452112, 0.08441730956529958, 0.08441730956529958, 0.08441730956529958, 0.11456156249563842, 0.11456156249563842, 0.11456156249563842, 0.09107142533975343, 0.09107142533975343, 0.09107142533975343, 0.08211323659876146, 0.08211323659876146, 0.08211323659876146, 0.10206430761582197, 0.10206430761582197, 0.10206430761582197, 0.5924351363112623, 0.5924351363112623, 0.5924351363112623, 0.9047980793814148, 0.9047980793814148, 0.9047980793814148, 0.8495027452395066, 0.8495027452395066, 0.8495027452395066, 0.26652443155696826, 0.26652443155696826, 0.26652443155696826, 0.3101120552834319, 0.3101120552834319, 0.3101120552834319, 0.23692796700254026, 0.23692796700254026, 0.23692796700254026, 0.1438816659635016, 0.1438816659635016, 0.1438816659635016, 0.19167430865111124, 0.19167430865111124, 0.19167430865111124, 0.2253607926875516, 0.2253607926875516, 0.2253607926875516, 0.13534101194193848, 0.13534101194193848, 0.13534101194193848, 0.08784400890223076, 0.08784400890223076, 0.08784400890223076, 0.12085401156217468, 0.12085401156217468, 0.12085401156217468, 0.15354298018844237, 0.15354298018844237, 0.15354298018844237, 0.1335247507026852, 0.1335247507026852, 0.1335247507026852, 0.12652675422387816, 0.12652675422387816, 0.12652675422387816, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.015638664098327704, 0.015638664098327704, 0.015638664098327704, 0.06348601714929225, 0.06348601714929225, 0.06348601714929225, 0.5128201195876666, 0.5128201195876666, 0.5128201195876666, 0.3244201399797112, 0.3244201399797112, 0.3244201399797112, 0.45247627263357393, 0.45247627263357393, 0.45247627263357393, 0.04451653784519172, 0.04451653784519172, 0.04451653784519172, 0.10244570601375602, 0.10244570601375602, 0.10244570601375602, 0.18293966820632035, 0.18293966820632035, 0.18293966820632035, 0.17744947312927328, 0.17744947312927328, 0.17744947312927328, 0.21510233156622172, 0.21510233156622172, 0.21510233156622172, 0.2355370232572408, 0.2355370232572408, 0.2355370232572408, 0.4148020855085609, 0.4148020855085609, 0.4148020855085609, 0.47609313173401036, 0.47609313173401036, 0.47609313173401036, 0.5343860690848905, 0.5343860690848905, 0.5343860690848905, 0.08606766261267618, 0.08606766261267618, 0.08606766261267618, 0.07384008998724745, 0.07384008998724745, 0.07384008998724745, 0.05731658403093409, 0.05731658403093409, 0.05731658403093409, 0.1864398127353536, 0.1864398127353536, 0.1864398127353536, 0.2522622823718256, 0.2522622823718256, 0.2522622823718256, 0.2219281605217851, 0.2219281605217851, 0.2219281605217851, 0.24675734876587718, 0.24675734876587718, 0.24675734876587718, 0.28422697657660434, 0.28422697657660434, 0.28422697657660434, 0.19285446987115784, 0.19285446987115784, 0.19285446987115784, 0.2535563484588895, 0.2535563484588895, 0.2535563484588895, 0.16542937537019864, 0.16542937537019864, 0.16542937537019864, 0.13177240973192006, 0.13177240973192006, 0.13177240973192006, 0.2186666325572999, 0.2186666325572999, 0.2186666325572999, 0.2280332499933979, 0.2280332499933979, 0.2280332499933979, 0.22633589337301674, 0.22633589337301674, 0.22633589337301674, 0.18181320157776804, 0.18181320157776804, 0.18181320157776804, 0.17564371084747676, 0.17564371084747676, 0.17564371084747676, 0.1965169021585963, 0.1965169021585963, 0.1965169021585963, 0.7218961390601533, 0.7218961390601533, 0.7218961390601533, 0.1906535662842197, 0.1906535662842197, 0.1906535662842197, 0.1528929509784832, 0.1528929509784832, 0.1528929509784832, 0.45106342858558135, 0.45106342858558135, 0.45106342858558135, 0.1996635041682644, 0.1996635041682644, 0.1996635041682644, 0.46981473137622387, 0.46981473137622387, 0.46981473137622387, 0.19155638439528389, 0.19155638439528389, 0.19155638439528389, 0.23186334416635013, 0.23186334416635013, 0.23186334416635013, 0.19238491741213615, 0.19238491741213615, 0.19238491741213615, 0.08784421046769497, 0.08784421046769497, 0.08784421046769497, 0.09506561691979443, 0.09506561691979443, 0.09506561691979443, 0.10918892216801945, 0.10918892216801945, 0.10918892216801945]}, "mutation_prompt": null}
{"id": "9f7d85e5-6c57-4393-94da-cf740bf78999", "solution": "import numpy as np\n\nclass PSO_SA_Optimizer_Enhanced:\n    def __init__(self, budget, dim, pop_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.w_max = 0.9\n        self.w_min = 0.4\n        self.c1_init = 2.0  # Initial cognitive parameter\n        self.c2_init = 1.0  # Initial social parameter\n        self.temp = 1.0\n\n    def __call__(self, func):\n        np.random.seed(42)\n        \n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.array([func(pos) for pos in positions])\n        global_best_position = personal_best_positions[np.argmin(personal_best_scores)]\n        global_best_score = np.min(personal_best_scores)\n        \n        eval_count = self.pop_size\n        \n        while eval_count < self.budget:\n            w = self.w_max - ((self.w_max - self.w_min) * (eval_count / self.budget))\n            c1 = self.c1_init * (1 - eval_count / self.budget)  # Dynamic cognitive\n            c2 = self.c2_init * (eval_count / self.budget)  # Dynamic social\n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                velocities[i] = (w * velocities[i]\n                                 + c1 * r1 * (personal_best_positions[i] - positions[i])\n                                 + c2 * r2 * (global_best_position - positions[i]))\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], self.lower_bound, self.upper_bound)\n\n                score = func(positions[i])\n                eval_count += 1\n                if eval_count >= self.budget:\n                    break\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = positions[i]\n\n                if np.random.rand() < np.exp(-(score - global_best_score) / self.temp):\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n                    if score < global_best_score:\n                        global_best_score = score\n                        global_best_position = positions[i]\n\n            self.temp *= 0.95\n\n        return global_best_position, global_best_score", "name": "PSO_SA_Optimizer_Enhanced", "description": "Modified PSO-SA incorporating dynamic cognitive and social parameters for improved convergence.", "configspace": "", "generation": 25, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "a6ced734-3ce2-4438-a3b1-cd500dc0bfe6", "metadata": {"aucs": [0.5552032341291239, 0.5552032341291239, 0.5552032341291239, 0.45079232168892436, 0.45079232168892436, 0.45079232168892436, 0.49261961612188865, 0.49261961612188865, 0.49261961612188865, 0.1534035740593066, 0.1534035740593066, 0.1534035740593066, 0.03196945674084872, 0.03196945674084872, 0.03196945674084872, 0.028423438180651517, 0.028423438180651517, 0.028423438180651517, 0.09722680421452112, 0.09722680421452112, 0.09722680421452112, 0.08441730956529958, 0.08441730956529958, 0.08441730956529958, 0.11456156249563842, 0.11456156249563842, 0.11456156249563842, 0.09107142533975343, 0.09107142533975343, 0.09107142533975343, 0.08211323659876146, 0.08211323659876146, 0.08211323659876146, 0.10206430761582197, 0.10206430761582197, 0.10206430761582197, 0.5924351363112623, 0.5924351363112623, 0.5924351363112623, 0.9047980793814148, 0.9047980793814148, 0.9047980793814148, 0.8495027452395066, 0.8495027452395066, 0.8495027452395066, 0.26652443155696826, 0.26652443155696826, 0.26652443155696826, 0.3101120552834319, 0.3101120552834319, 0.3101120552834319, 0.23692796700254026, 0.23692796700254026, 0.23692796700254026, 0.1438816659635016, 0.1438816659635016, 0.1438816659635016, 0.19167430865111124, 0.19167430865111124, 0.19167430865111124, 0.2253607926875516, 0.2253607926875516, 0.2253607926875516, 0.13534101194193848, 0.13534101194193848, 0.13534101194193848, 0.08784400890223076, 0.08784400890223076, 0.08784400890223076, 0.12085401156217468, 0.12085401156217468, 0.12085401156217468, 0.15354298018844237, 0.15354298018844237, 0.15354298018844237, 0.1335247507026852, 0.1335247507026852, 0.1335247507026852, 0.12652675422387816, 0.12652675422387816, 0.12652675422387816, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.015638664098327704, 0.015638664098327704, 0.015638664098327704, 0.06348601714929225, 0.06348601714929225, 0.06348601714929225, 0.5128201195876666, 0.5128201195876666, 0.5128201195876666, 0.3244201399797112, 0.3244201399797112, 0.3244201399797112, 0.45247627263357393, 0.45247627263357393, 0.45247627263357393, 0.04451653784519172, 0.04451653784519172, 0.04451653784519172, 0.10244570601375602, 0.10244570601375602, 0.10244570601375602, 0.18293966820632035, 0.18293966820632035, 0.18293966820632035, 0.17744947312927328, 0.17744947312927328, 0.17744947312927328, 0.21510233156622172, 0.21510233156622172, 0.21510233156622172, 0.2355370232572408, 0.2355370232572408, 0.2355370232572408, 0.4148020855085609, 0.4148020855085609, 0.4148020855085609, 0.47609313173401036, 0.47609313173401036, 0.47609313173401036, 0.5343860690848905, 0.5343860690848905, 0.5343860690848905, 0.08606766261267618, 0.08606766261267618, 0.08606766261267618, 0.07384008998724745, 0.07384008998724745, 0.07384008998724745, 0.05731658403093409, 0.05731658403093409, 0.05731658403093409, 0.1864398127353536, 0.1864398127353536, 0.1864398127353536, 0.2522622823718256, 0.2522622823718256, 0.2522622823718256, 0.2219281605217851, 0.2219281605217851, 0.2219281605217851, 0.24675734876587718, 0.24675734876587718, 0.24675734876587718, 0.28422697657660434, 0.28422697657660434, 0.28422697657660434, 0.19285446987115784, 0.19285446987115784, 0.19285446987115784, 0.2535563484588895, 0.2535563484588895, 0.2535563484588895, 0.16542937537019864, 0.16542937537019864, 0.16542937537019864, 0.13177240973192006, 0.13177240973192006, 0.13177240973192006, 0.2186666325572999, 0.2186666325572999, 0.2186666325572999, 0.2280332499933979, 0.2280332499933979, 0.2280332499933979, 0.22633589337301674, 0.22633589337301674, 0.22633589337301674, 0.18181320157776804, 0.18181320157776804, 0.18181320157776804, 0.17564371084747676, 0.17564371084747676, 0.17564371084747676, 0.1965169021585963, 0.1965169021585963, 0.1965169021585963, 0.7218961390601533, 0.7218961390601533, 0.7218961390601533, 0.1906535662842197, 0.1906535662842197, 0.1906535662842197, 0.1528929509784832, 0.1528929509784832, 0.1528929509784832, 0.45106342858558135, 0.45106342858558135, 0.45106342858558135, 0.1996635041682644, 0.1996635041682644, 0.1996635041682644, 0.46981473137622387, 0.46981473137622387, 0.46981473137622387, 0.19155638439528389, 0.19155638439528389, 0.19155638439528389, 0.23186334416635013, 0.23186334416635013, 0.23186334416635013, 0.19238491741213615, 0.19238491741213615, 0.19238491741213615, 0.08784421046769497, 0.08784421046769497, 0.08784421046769497, 0.09506561691979443, 0.09506561691979443, 0.09506561691979443, 0.10918892216801945, 0.10918892216801945, 0.10918892216801945]}, "mutation_prompt": null}
{"id": "9a855f90-e408-4025-a81e-dcc320277ddb", "solution": "import numpy as np\n\nclass PSO_SA_Optimizer_Enhanced:\n    def __init__(self, budget, dim, pop_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.w_max = 0.9\n        self.w_min = 0.4\n        self.c1_init = 2.0  # Initial cognitive parameter\n        self.c2_init = 1.0  # Initial social parameter\n        self.temp = 1.0\n\n    def __call__(self, func):\n        np.random.seed(42)\n        \n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.array([func(pos) for pos in positions])\n        global_best_position = personal_best_positions[np.argmin(personal_best_scores)]\n        global_best_score = np.min(personal_best_scores)\n        \n        eval_count = self.pop_size\n        \n        while eval_count < self.budget:\n            w = self.w_max - ((self.w_max - self.w_min) * (eval_count / self.budget))\n            c1 = self.c1_init * (1 - eval_count / self.budget)  # Dynamic cognitive\n            c2 = self.c2_init * (eval_count / self.budget)  # Dynamic social\n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                velocities[i] = (w * velocities[i]\n                                 + c1 * r1 * (personal_best_positions[i] - positions[i])\n                                 + c2 * r2 * (global_best_position - positions[i]))\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], self.lower_bound, self.upper_bound)\n\n                score = func(positions[i])\n                eval_count += 1\n                if eval_count >= self.budget:\n                    break\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = positions[i]\n\n                if np.random.rand() < np.exp(-(score - global_best_score) / self.temp):\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n                    if score < global_best_score:\n                        global_best_score = score\n                        global_best_position = positions[i]\n\n            self.temp *= 0.95\n\n        return global_best_position, global_best_score", "name": "PSO_SA_Optimizer_Enhanced", "description": "Modified PSO-SA incorporating dynamic cognitive and social parameters for improved convergence.", "configspace": "", "generation": 25, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "a6ced734-3ce2-4438-a3b1-cd500dc0bfe6", "metadata": {"aucs": [0.5552032341291239, 0.5552032341291239, 0.5552032341291239, 0.45079232168892436, 0.45079232168892436, 0.45079232168892436, 0.49261961612188865, 0.49261961612188865, 0.49261961612188865, 0.1534035740593066, 0.1534035740593066, 0.1534035740593066, 0.03196945674084872, 0.03196945674084872, 0.03196945674084872, 0.028423438180651517, 0.028423438180651517, 0.028423438180651517, 0.09722680421452112, 0.09722680421452112, 0.09722680421452112, 0.08441730956529958, 0.08441730956529958, 0.08441730956529958, 0.11456156249563842, 0.11456156249563842, 0.11456156249563842, 0.09107142533975343, 0.09107142533975343, 0.09107142533975343, 0.08211323659876146, 0.08211323659876146, 0.08211323659876146, 0.10206430761582197, 0.10206430761582197, 0.10206430761582197, 0.5924351363112623, 0.5924351363112623, 0.5924351363112623, 0.9047980793814148, 0.9047980793814148, 0.9047980793814148, 0.8495027452395066, 0.8495027452395066, 0.8495027452395066, 0.26652443155696826, 0.26652443155696826, 0.26652443155696826, 0.3101120552834319, 0.3101120552834319, 0.3101120552834319, 0.23692796700254026, 0.23692796700254026, 0.23692796700254026, 0.1438816659635016, 0.1438816659635016, 0.1438816659635016, 0.19167430865111124, 0.19167430865111124, 0.19167430865111124, 0.2253607926875516, 0.2253607926875516, 0.2253607926875516, 0.13534101194193848, 0.13534101194193848, 0.13534101194193848, 0.08784400890223076, 0.08784400890223076, 0.08784400890223076, 0.12085401156217468, 0.12085401156217468, 0.12085401156217468, 0.15354298018844237, 0.15354298018844237, 0.15354298018844237, 0.1335247507026852, 0.1335247507026852, 0.1335247507026852, 0.12652675422387816, 0.12652675422387816, 0.12652675422387816, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.015638664098327704, 0.015638664098327704, 0.015638664098327704, 0.06348601714929225, 0.06348601714929225, 0.06348601714929225, 0.5128201195876666, 0.5128201195876666, 0.5128201195876666, 0.3244201399797112, 0.3244201399797112, 0.3244201399797112, 0.45247627263357393, 0.45247627263357393, 0.45247627263357393, 0.04451653784519172, 0.04451653784519172, 0.04451653784519172, 0.10244570601375602, 0.10244570601375602, 0.10244570601375602, 0.18293966820632035, 0.18293966820632035, 0.18293966820632035, 0.17744947312927328, 0.17744947312927328, 0.17744947312927328, 0.21510233156622172, 0.21510233156622172, 0.21510233156622172, 0.2355370232572408, 0.2355370232572408, 0.2355370232572408, 0.4148020855085609, 0.4148020855085609, 0.4148020855085609, 0.47609313173401036, 0.47609313173401036, 0.47609313173401036, 0.5343860690848905, 0.5343860690848905, 0.5343860690848905, 0.08606766261267618, 0.08606766261267618, 0.08606766261267618, 0.07384008998724745, 0.07384008998724745, 0.07384008998724745, 0.05731658403093409, 0.05731658403093409, 0.05731658403093409, 0.1864398127353536, 0.1864398127353536, 0.1864398127353536, 0.2522622823718256, 0.2522622823718256, 0.2522622823718256, 0.2219281605217851, 0.2219281605217851, 0.2219281605217851, 0.24675734876587718, 0.24675734876587718, 0.24675734876587718, 0.28422697657660434, 0.28422697657660434, 0.28422697657660434, 0.19285446987115784, 0.19285446987115784, 0.19285446987115784, 0.2535563484588895, 0.2535563484588895, 0.2535563484588895, 0.16542937537019864, 0.16542937537019864, 0.16542937537019864, 0.13177240973192006, 0.13177240973192006, 0.13177240973192006, 0.2186666325572999, 0.2186666325572999, 0.2186666325572999, 0.2280332499933979, 0.2280332499933979, 0.2280332499933979, 0.22633589337301674, 0.22633589337301674, 0.22633589337301674, 0.18181320157776804, 0.18181320157776804, 0.18181320157776804, 0.17564371084747676, 0.17564371084747676, 0.17564371084747676, 0.1965169021585963, 0.1965169021585963, 0.1965169021585963, 0.7218961390601533, 0.7218961390601533, 0.7218961390601533, 0.1906535662842197, 0.1906535662842197, 0.1906535662842197, 0.1528929509784832, 0.1528929509784832, 0.1528929509784832, 0.45106342858558135, 0.45106342858558135, 0.45106342858558135, 0.1996635041682644, 0.1996635041682644, 0.1996635041682644, 0.46981473137622387, 0.46981473137622387, 0.46981473137622387, 0.19155638439528389, 0.19155638439528389, 0.19155638439528389, 0.23186334416635013, 0.23186334416635013, 0.23186334416635013, 0.19238491741213615, 0.19238491741213615, 0.19238491741213615, 0.08784421046769497, 0.08784421046769497, 0.08784421046769497, 0.09506561691979443, 0.09506561691979443, 0.09506561691979443, 0.10918892216801945, 0.10918892216801945, 0.10918892216801945]}, "mutation_prompt": null}
{"id": "4b790817-79bf-4c0e-b640-024909f20bf3", "solution": "import numpy as np\n\nclass PSO_SA_Optimizer_Enhanced:\n    def __init__(self, budget, dim, pop_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.w_max = 0.9\n        self.w_min = 0.4\n        self.c1_init = 2.0  # Initial cognitive parameter\n        self.c2_init = 1.0  # Initial social parameter\n        self.temp = 1.0\n\n    def __call__(self, func):\n        np.random.seed(42)\n        \n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.array([func(pos) for pos in positions])\n        global_best_position = personal_best_positions[np.argmin(personal_best_scores)]\n        global_best_score = np.min(personal_best_scores)\n        \n        eval_count = self.pop_size\n        \n        while eval_count < self.budget:\n            w = self.w_max - ((self.w_max - self.w_min) * (eval_count / self.budget))\n            c1 = self.c1_init * (1 - eval_count / self.budget)  # Dynamic cognitive\n            c2 = self.c2_init * (eval_count / self.budget)  # Dynamic social\n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                velocities[i] = (w * velocities[i]\n                                 + c1 * r1 * (personal_best_positions[i] - positions[i])\n                                 + c2 * r2 * (global_best_position - positions[i]))\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], self.lower_bound, self.upper_bound)\n\n                score = func(positions[i])\n                eval_count += 1\n                if eval_count >= self.budget:\n                    break\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = positions[i]\n\n                if np.random.rand() < np.exp(-(score - global_best_score) / self.temp):\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n                    if score < global_best_score:\n                        global_best_score = score\n                        global_best_position = positions[i]\n\n            self.temp *= 0.95\n\n        return global_best_position, global_best_score", "name": "PSO_SA_Optimizer_Enhanced", "description": "Modified PSO-SA incorporating dynamic cognitive and social parameters for improved convergence.", "configspace": "", "generation": 25, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "a6ced734-3ce2-4438-a3b1-cd500dc0bfe6", "metadata": {"aucs": [0.5552032341291239, 0.5552032341291239, 0.5552032341291239, 0.45079232168892436, 0.45079232168892436, 0.45079232168892436, 0.49261961612188865, 0.49261961612188865, 0.49261961612188865, 0.1534035740593066, 0.1534035740593066, 0.1534035740593066, 0.03196945674084872, 0.03196945674084872, 0.03196945674084872, 0.028423438180651517, 0.028423438180651517, 0.028423438180651517, 0.09722680421452112, 0.09722680421452112, 0.09722680421452112, 0.08441730956529958, 0.08441730956529958, 0.08441730956529958, 0.11456156249563842, 0.11456156249563842, 0.11456156249563842, 0.09107142533975343, 0.09107142533975343, 0.09107142533975343, 0.08211323659876146, 0.08211323659876146, 0.08211323659876146, 0.10206430761582197, 0.10206430761582197, 0.10206430761582197, 0.5924351363112623, 0.5924351363112623, 0.5924351363112623, 0.9047980793814148, 0.9047980793814148, 0.9047980793814148, 0.8495027452395066, 0.8495027452395066, 0.8495027452395066, 0.26652443155696826, 0.26652443155696826, 0.26652443155696826, 0.3101120552834319, 0.3101120552834319, 0.3101120552834319, 0.23692796700254026, 0.23692796700254026, 0.23692796700254026, 0.1438816659635016, 0.1438816659635016, 0.1438816659635016, 0.19167430865111124, 0.19167430865111124, 0.19167430865111124, 0.2253607926875516, 0.2253607926875516, 0.2253607926875516, 0.13534101194193848, 0.13534101194193848, 0.13534101194193848, 0.08784400890223076, 0.08784400890223076, 0.08784400890223076, 0.12085401156217468, 0.12085401156217468, 0.12085401156217468, 0.15354298018844237, 0.15354298018844237, 0.15354298018844237, 0.1335247507026852, 0.1335247507026852, 0.1335247507026852, 0.12652675422387816, 0.12652675422387816, 0.12652675422387816, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.015638664098327704, 0.015638664098327704, 0.015638664098327704, 0.06348601714929225, 0.06348601714929225, 0.06348601714929225, 0.5128201195876666, 0.5128201195876666, 0.5128201195876666, 0.3244201399797112, 0.3244201399797112, 0.3244201399797112, 0.45247627263357393, 0.45247627263357393, 0.45247627263357393, 0.04451653784519172, 0.04451653784519172, 0.04451653784519172, 0.10244570601375602, 0.10244570601375602, 0.10244570601375602, 0.18293966820632035, 0.18293966820632035, 0.18293966820632035, 0.17744947312927328, 0.17744947312927328, 0.17744947312927328, 0.21510233156622172, 0.21510233156622172, 0.21510233156622172, 0.2355370232572408, 0.2355370232572408, 0.2355370232572408, 0.4148020855085609, 0.4148020855085609, 0.4148020855085609, 0.47609313173401036, 0.47609313173401036, 0.47609313173401036, 0.5343860690848905, 0.5343860690848905, 0.5343860690848905, 0.08606766261267618, 0.08606766261267618, 0.08606766261267618, 0.07384008998724745, 0.07384008998724745, 0.07384008998724745, 0.05731658403093409, 0.05731658403093409, 0.05731658403093409, 0.1864398127353536, 0.1864398127353536, 0.1864398127353536, 0.2522622823718256, 0.2522622823718256, 0.2522622823718256, 0.2219281605217851, 0.2219281605217851, 0.2219281605217851, 0.24675734876587718, 0.24675734876587718, 0.24675734876587718, 0.28422697657660434, 0.28422697657660434, 0.28422697657660434, 0.19285446987115784, 0.19285446987115784, 0.19285446987115784, 0.2535563484588895, 0.2535563484588895, 0.2535563484588895, 0.16542937537019864, 0.16542937537019864, 0.16542937537019864, 0.13177240973192006, 0.13177240973192006, 0.13177240973192006, 0.2186666325572999, 0.2186666325572999, 0.2186666325572999, 0.2280332499933979, 0.2280332499933979, 0.2280332499933979, 0.22633589337301674, 0.22633589337301674, 0.22633589337301674, 0.18181320157776804, 0.18181320157776804, 0.18181320157776804, 0.17564371084747676, 0.17564371084747676, 0.17564371084747676, 0.1965169021585963, 0.1965169021585963, 0.1965169021585963, 0.7218961390601533, 0.7218961390601533, 0.7218961390601533, 0.1906535662842197, 0.1906535662842197, 0.1906535662842197, 0.1528929509784832, 0.1528929509784832, 0.1528929509784832, 0.45106342858558135, 0.45106342858558135, 0.45106342858558135, 0.1996635041682644, 0.1996635041682644, 0.1996635041682644, 0.46981473137622387, 0.46981473137622387, 0.46981473137622387, 0.19155638439528389, 0.19155638439528389, 0.19155638439528389, 0.23186334416635013, 0.23186334416635013, 0.23186334416635013, 0.19238491741213615, 0.19238491741213615, 0.19238491741213615, 0.08784421046769497, 0.08784421046769497, 0.08784421046769497, 0.09506561691979443, 0.09506561691979443, 0.09506561691979443, 0.10918892216801945, 0.10918892216801945, 0.10918892216801945]}, "mutation_prompt": null}
{"id": "f50c2c8b-d2f5-46a4-85cf-49a433fbd824", "solution": "import numpy as np\n\nclass PSO_SA_Optimizer_Enhanced:\n    def __init__(self, budget, dim, pop_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.w_max = 0.9\n        self.w_min = 0.4\n        self.c1_init = 2.0  # Initial cognitive parameter\n        self.c2_init = 1.0  # Initial social parameter\n        self.temp = 1.0\n\n    def __call__(self, func):\n        np.random.seed(42)\n        \n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.array([func(pos) for pos in positions])\n        global_best_position = personal_best_positions[np.argmin(personal_best_scores)]\n        global_best_score = np.min(personal_best_scores)\n        \n        eval_count = self.pop_size\n        \n        while eval_count < self.budget:\n            w = self.w_max - ((self.w_max - self.w_min) * (eval_count / self.budget))\n            c1 = self.c1_init * (1 - eval_count / self.budget)  # Dynamic cognitive\n            c2 = self.c2_init * (eval_count / self.budget)  # Dynamic social\n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                velocities[i] = (w * velocities[i]\n                                 + c1 * r1 * (personal_best_positions[i] - positions[i])\n                                 + c2 * r2 * (global_best_position - positions[i]))\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], self.lower_bound, self.upper_bound)\n\n                score = func(positions[i])\n                eval_count += 1\n                if eval_count >= self.budget:\n                    break\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = positions[i]\n\n                if np.random.rand() < np.exp(-(score - global_best_score) / self.temp):\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n                    if score < global_best_score:\n                        global_best_score = score\n                        global_best_position = positions[i]\n\n            self.temp *= 0.95\n\n        return global_best_position, global_best_score", "name": "PSO_SA_Optimizer_Enhanced", "description": "Modified PSO-SA incorporating dynamic cognitive and social parameters for improved convergence.", "configspace": "", "generation": 25, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "a6ced734-3ce2-4438-a3b1-cd500dc0bfe6", "metadata": {"aucs": [0.5552032341291239, 0.5552032341291239, 0.5552032341291239, 0.45079232168892436, 0.45079232168892436, 0.45079232168892436, 0.49261961612188865, 0.49261961612188865, 0.49261961612188865, 0.1534035740593066, 0.1534035740593066, 0.1534035740593066, 0.03196945674084872, 0.03196945674084872, 0.03196945674084872, 0.028423438180651517, 0.028423438180651517, 0.028423438180651517, 0.09722680421452112, 0.09722680421452112, 0.09722680421452112, 0.08441730956529958, 0.08441730956529958, 0.08441730956529958, 0.11456156249563842, 0.11456156249563842, 0.11456156249563842, 0.09107142533975343, 0.09107142533975343, 0.09107142533975343, 0.08211323659876146, 0.08211323659876146, 0.08211323659876146, 0.10206430761582197, 0.10206430761582197, 0.10206430761582197, 0.5924351363112623, 0.5924351363112623, 0.5924351363112623, 0.9047980793814148, 0.9047980793814148, 0.9047980793814148, 0.8495027452395066, 0.8495027452395066, 0.8495027452395066, 0.26652443155696826, 0.26652443155696826, 0.26652443155696826, 0.3101120552834319, 0.3101120552834319, 0.3101120552834319, 0.23692796700254026, 0.23692796700254026, 0.23692796700254026, 0.1438816659635016, 0.1438816659635016, 0.1438816659635016, 0.19167430865111124, 0.19167430865111124, 0.19167430865111124, 0.2253607926875516, 0.2253607926875516, 0.2253607926875516, 0.13534101194193848, 0.13534101194193848, 0.13534101194193848, 0.08784400890223076, 0.08784400890223076, 0.08784400890223076, 0.12085401156217468, 0.12085401156217468, 0.12085401156217468, 0.15354298018844237, 0.15354298018844237, 0.15354298018844237, 0.1335247507026852, 0.1335247507026852, 0.1335247507026852, 0.12652675422387816, 0.12652675422387816, 0.12652675422387816, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.015638664098327704, 0.015638664098327704, 0.015638664098327704, 0.06348601714929225, 0.06348601714929225, 0.06348601714929225, 0.5128201195876666, 0.5128201195876666, 0.5128201195876666, 0.3244201399797112, 0.3244201399797112, 0.3244201399797112, 0.45247627263357393, 0.45247627263357393, 0.45247627263357393, 0.04451653784519172, 0.04451653784519172, 0.04451653784519172, 0.10244570601375602, 0.10244570601375602, 0.10244570601375602, 0.18293966820632035, 0.18293966820632035, 0.18293966820632035, 0.17744947312927328, 0.17744947312927328, 0.17744947312927328, 0.21510233156622172, 0.21510233156622172, 0.21510233156622172, 0.2355370232572408, 0.2355370232572408, 0.2355370232572408, 0.4148020855085609, 0.4148020855085609, 0.4148020855085609, 0.47609313173401036, 0.47609313173401036, 0.47609313173401036, 0.5343860690848905, 0.5343860690848905, 0.5343860690848905, 0.08606766261267618, 0.08606766261267618, 0.08606766261267618, 0.07384008998724745, 0.07384008998724745, 0.07384008998724745, 0.05731658403093409, 0.05731658403093409, 0.05731658403093409, 0.1864398127353536, 0.1864398127353536, 0.1864398127353536, 0.2522622823718256, 0.2522622823718256, 0.2522622823718256, 0.2219281605217851, 0.2219281605217851, 0.2219281605217851, 0.24675734876587718, 0.24675734876587718, 0.24675734876587718, 0.28422697657660434, 0.28422697657660434, 0.28422697657660434, 0.19285446987115784, 0.19285446987115784, 0.19285446987115784, 0.2535563484588895, 0.2535563484588895, 0.2535563484588895, 0.16542937537019864, 0.16542937537019864, 0.16542937537019864, 0.13177240973192006, 0.13177240973192006, 0.13177240973192006, 0.2186666325572999, 0.2186666325572999, 0.2186666325572999, 0.2280332499933979, 0.2280332499933979, 0.2280332499933979, 0.22633589337301674, 0.22633589337301674, 0.22633589337301674, 0.18181320157776804, 0.18181320157776804, 0.18181320157776804, 0.17564371084747676, 0.17564371084747676, 0.17564371084747676, 0.1965169021585963, 0.1965169021585963, 0.1965169021585963, 0.7218961390601533, 0.7218961390601533, 0.7218961390601533, 0.1906535662842197, 0.1906535662842197, 0.1906535662842197, 0.1528929509784832, 0.1528929509784832, 0.1528929509784832, 0.45106342858558135, 0.45106342858558135, 0.45106342858558135, 0.1996635041682644, 0.1996635041682644, 0.1996635041682644, 0.46981473137622387, 0.46981473137622387, 0.46981473137622387, 0.19155638439528389, 0.19155638439528389, 0.19155638439528389, 0.23186334416635013, 0.23186334416635013, 0.23186334416635013, 0.19238491741213615, 0.19238491741213615, 0.19238491741213615, 0.08784421046769497, 0.08784421046769497, 0.08784421046769497, 0.09506561691979443, 0.09506561691979443, 0.09506561691979443, 0.10918892216801945, 0.10918892216801945, 0.10918892216801945]}, "mutation_prompt": null}
{"id": "4d39faf4-467a-4d58-a9c9-6c998c97db6c", "solution": "import numpy as np\n\nclass PSO_SA_Optimizer_Enhanced:\n    def __init__(self, budget, dim, pop_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.w_max = 0.9\n        self.w_min = 0.4\n        self.c1_init = 1.5  # Reduced cognitive parameter\n        self.c2_init = 1.5  # Increased social parameter\n        self.temp = 1.0\n        self.v_max = (self.upper_bound - self.lower_bound) / 10.0  # Max velocity\n\n    def __call__(self, func):\n        np.random.seed(42)\n\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        velocities = np.random.uniform(-self.v_max, self.v_max, (self.pop_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.array([func(pos) for pos in positions])\n        global_best_position = personal_best_positions[np.argmin(personal_best_scores)]\n        global_best_score = np.min(personal_best_scores)\n\n        eval_count = self.pop_size\n\n        while eval_count < self.budget:\n            w = self.w_max - ((self.w_max - self.w_min) * (eval_count / self.budget))\n            c1 = self.c1_init * (1 - eval_count / self.budget)\n            c2 = self.c2_init * (eval_count / self.budget)\n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                velocities[i] = np.clip(\n                    w * velocities[i]\n                    + c1 * r1 * (personal_best_positions[i] - positions[i])\n                    + c2 * r2 * (global_best_position - positions[i]),\n                    -self.v_max, self.v_max\n                )\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], self.lower_bound, self.upper_bound)\n\n                score = func(positions[i])\n                eval_count += 1\n                if eval_count >= self.budget:\n                    break\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = positions[i]\n\n                if np.random.rand() < np.exp(-(score - global_best_score) / self.temp):\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n                    if score < global_best_score:\n                        global_best_score = score\n                        global_best_position = positions[i]\n\n            self.temp *= 0.95\n\n        return global_best_position, global_best_score", "name": "PSO_SA_Optimizer_Enhanced", "description": "Enhanced PSO-SA with adaptive velocity clamping and stochastic annealing for improved exploration-exploitation balance.", "configspace": "", "generation": 29, "fitness": 0.23478278053829638, "feedback": "The algorithm PSO_SA_Optimizer_Enhanced got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.23 with standard deviation 0.18.", "error": "", "parent_id": "a6ced734-3ce2-4438-a3b1-cd500dc0bfe6", "metadata": {"aucs": [0.4060414065271941, 0.4060414065271941, 0.4060414065271941, 0.4176261554035873, 0.4176261554035873, 0.4176261554035873, 0.45170067743440057, 0.45170067743440057, 0.45170067743440057, 0.10936336878076314, 0.10936336878076314, 0.10936336878076314, 0.010704720204144369, 0.010704720204144369, 0.010704720204144369, 0.061752248421685585, 0.061752248421685585, 0.061752248421685585, 0.15553660667672897, 0.15553660667672897, 0.15553660667672897, 0.09883506602513603, 0.09883506602513603, 0.09883506602513603, 0.14536029810417295, 0.14536029810417295, 0.14536029810417295, 0.08542242422005819, 0.08542242422005819, 0.08542242422005819, 0.08188695513825572, 0.08188695513825572, 0.08188695513825572, 0.10276499638030379, 0.10276499638030379, 0.10276499638030379, 0.7482279534850058, 0.7482279534850058, 0.7482279534850058, 0.9240298114984098, 0.9240298114984098, 0.9240298114984098, 0.8985017357682366, 0.8985017357682366, 0.8985017357682366, 0.4378460493954238, 0.4378460493954238, 0.4378460493954238, 0.3670622286893849, 0.3670622286893849, 0.3670622286893849, 0.3674088771284294, 0.3674088771284294, 0.3674088771284294, 0.13223307791289807, 0.13223307791289807, 0.13223307791289807, 0.21508006743330477, 0.21508006743330477, 0.21508006743330477, 0.17372690318685835, 0.17372690318685835, 0.17372690318685835, 0.19113558066337055, 0.19113558066337055, 0.19113558066337055, 0.11408905537981595, 0.11408905537981595, 0.11408905537981595, 0.21817416202911133, 0.21817416202911133, 0.21817416202911133, 0.1369527683439954, 0.1369527683439954, 0.1369527683439954, 0.14991865507333657, 0.14991865507333657, 0.14991865507333657, 0.16274828201148905, 0.16274828201148905, 0.16274828201148905, 0.18854964444217004, 0.18854964444217004, 0.18854964444217004, 0.04512945171902916, 0.04512945171902916, 0.04512945171902916, 0.08454300252014069, 0.08454300252014069, 0.08454300252014069, 0.22981938033446303, 0.22981938033446303, 0.22981938033446303, 0.12932436269708103, 0.12932436269708103, 0.12932436269708103, 0.21064806034343797, 0.21064806034343797, 0.21064806034343797, 0.04711463861644172, 0.04711463861644172, 0.04711463861644172, 0.2861181710918803, 0.2861181710918803, 0.2861181710918803, 0.15997924995725898, 0.15997924995725898, 0.15997924995725898, 0.14203559624474238, 0.14203559624474238, 0.14203559624474238, 0.1505034451152213, 0.1505034451152213, 0.1505034451152213, 0.10959651500325085, 0.10959651500325085, 0.10959651500325085, 0.4211058465796912, 0.4211058465796912, 0.4211058465796912, 0.4206165153659157, 0.4206165153659157, 0.4206165153659157, 0.4112985203905095, 0.4112985203905095, 0.4112985203905095, 0.11265100316370846, 0.11265100316370846, 0.11265100316370846, 0.07725897659444181, 0.07725897659444181, 0.07725897659444181, 0.08368687093556348, 0.08368687093556348, 0.08368687093556348, 0.20149818597974023, 0.20149818597974023, 0.20149818597974023, 0.1311824016296801, 0.1311824016296801, 0.1311824016296801, 0.14905337175275235, 0.14905337175275235, 0.14905337175275235, 0.2339638561141515, 0.2339638561141515, 0.2339638561141515, 0.19347282957879897, 0.19347282957879897, 0.19347282957879897, 0.1699830396628189, 0.1699830396628189, 0.1699830396628189, 0.1964684089875527, 0.1964684089875527, 0.1964684089875527, 0.17097155652588703, 0.17097155652588703, 0.17097155652588703, 0.12463410775656014, 0.12463410775656014, 0.12463410775656014, 0.2265674499762288, 0.2265674499762288, 0.2265674499762288, 0.2185549342230212, 0.2185549342230212, 0.2185549342230212, 0.22764203872035693, 0.22764203872035693, 0.22764203872035693, 0.21723947972989954, 0.21723947972989954, 0.21723947972989954, 0.2159961094241598, 0.2159961094241598, 0.2159961094241598, 0.18093546199168575, 0.18093546199168575, 0.18093546199168575, 0.5667212854844212, 0.5667212854844212, 0.5667212854844212, 0.19144089042677825, 0.19144089042677825, 0.19144089042677825, 0.16490874455927385, 0.16490874455927385, 0.16490874455927385, 0.5398494370369455, 0.5398494370369455, 0.5398494370369455, 0.4956032762672202, 0.4956032762672202, 0.4956032762672202, 0.4840211503391133, 0.4840211503391133, 0.4840211503391133, 0.1985887872079941, 0.1985887872079941, 0.1985887872079941, 0.2060375676538002, 0.2060375676538002, 0.2060375676538002, 0.22864950584825172, 0.22864950584825172, 0.22864950584825172, 0.09364579079275603, 0.09364579079275603, 0.09364579079275603, 0.12454027035778603, 0.12454027035778603, 0.12454027035778603, 0.07808087829925547, 0.07808087829925547, 0.07808087829925547]}, "mutation_prompt": null}
{"id": "3bf50b58-7885-4d3d-8fd9-4e47d1dcfd0d", "solution": "import numpy as np\n\nclass PSO_SA_Optimizer_Enhanced_V2:\n    def __init__(self, budget, dim, pop_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.w_max = 0.9\n        self.w_min = 0.4\n        self.c1_init = 2.0\n        self.c2_init = 1.0\n        self.temp = 1.0\n\n    def __call__(self, func):\n        np.random.seed(42)\n        \n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.array([func(pos) for pos in positions])\n        global_best_position = personal_best_positions[np.argmin(personal_best_scores)]\n        global_best_score = np.min(personal_best_scores)\n\n        eval_count = self.pop_size\n        inertia_decay = (self.w_max - self.w_min) / (self.budget // self.pop_size)\n\n        while eval_count < self.budget:\n            w = self.w_max - inertia_decay * (eval_count // self.pop_size)\n            c1 = self.c1_init * (1 - eval_count / self.budget) ** 2\n            c2 = self.c2_init * (eval_count / self.budget) ** 2\n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                velocities[i] = (w * velocities[i]\n                                 + c1 * r1 * (personal_best_positions[i] - positions[i])\n                                 + c2 * r2 * (global_best_position - positions[i]))\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], self.lower_bound, self.upper_bound)\n\n                score = func(positions[i])\n                eval_count += 1\n                if eval_count >= self.budget:\n                    break\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = positions[i]\n\n                if np.random.rand() < np.exp(-(score - global_best_score) / self.temp):\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n                    if score < global_best_score:\n                        global_best_score = score\n                        global_best_position = positions[i]\n\n            self.temp *= 0.95\n\n        return global_best_position, global_best_score", "name": "PSO_SA_Optimizer_Enhanced_V2", "description": "Enhanced PSO-SA with adaptive inertia and parameter tuning for efficient convergence.", "configspace": "", "generation": 30, "fitness": 0.1289559873872613, "feedback": "The algorithm PSO_SA_Optimizer_Enhanced_V2 got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.13 with standard deviation 0.10.", "error": "", "parent_id": "a6ced734-3ce2-4438-a3b1-cd500dc0bfe6", "metadata": {"aucs": [0.32104256399400377, 0.32104256399400377, 0.32104256399400377, 0.18761538344375617, 0.18761538344375617, 0.18761538344375617, 0.30648998107935355, 0.30648998107935355, 0.30648998107935355, 0.002360816729758297, 0.002360816729758297, 0.002360816729758297, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.0813944484829704, 0.0813944484829704, 0.0813944484829704, 0.08935065019517696, 0.08935065019517696, 0.08935065019517696, 0.06778138969151881, 0.06778138969151881, 0.06778138969151881, 0.05645393853306746, 0.05645393853306746, 0.05645393853306746, 0.06003912338477435, 0.06003912338477435, 0.06003912338477435, 0.05299260716407084, 0.05299260716407084, 0.05299260716407084, 0.06919048499519942, 0.06919048499519942, 0.06919048499519942, 0.09983989325864984, 0.09983989325864984, 0.09983989325864984, 0.08647302330715434, 0.08647302330715434, 0.08647302330715434, 0.05736117683609954, 0.05736117683609954, 0.05736117683609954, 0.0936691324555492, 0.0936691324555492, 0.0936691324555492, 0.11432106021527921, 0.11432106021527921, 0.11432106021527921, 0.17321433354112448, 0.17321433354112448, 0.17321433354112448, 0.16797744480055188, 0.16797744480055188, 0.16797744480055188, 0.09891965649263779, 0.09891965649263779, 0.09891965649263779, 0.05659263805054471, 0.05659263805054471, 0.05659263805054471, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.04044174021252134, 0.04044174021252134, 0.04044174021252134, 0.11786537071632941, 0.11786537071632941, 0.11786537071632941, 0.07288481959418747, 0.07288481959418747, 0.07288481959418747, 0.07073685276808883, 0.07073685276808883, 0.07073685276808883, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.015096823689008665, 0.015096823689008665, 0.015096823689008665, 0.06142702422842139, 0.06142702422842139, 0.06142702422842139, 0.18361241326913347, 0.18361241326913347, 0.18361241326913347, 0.13881688203350384, 0.13881688203350384, 0.13881688203350384, 0.12700200287478436, 0.12700200287478436, 0.12700200287478436, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.07479942809872464, 0.07479942809872464, 0.07479942809872464, 0.001620922192316132, 0.001620922192316132, 0.001620922192316132, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.24625538024305382, 0.24625538024305382, 0.24625538024305382, 0.40576543253398156, 0.40576543253398156, 0.40576543253398156, 0.27014831696142905, 0.27014831696142905, 0.27014831696142905, 0.06835923325701188, 0.06835923325701188, 0.06835923325701188, 0.07150924788046409, 0.07150924788046409, 0.07150924788046409, 0.05612951416151413, 0.05612951416151413, 0.05612951416151413, 0.15754352127581484, 0.15754352127581484, 0.15754352127581484, 0.19035132889867823, 0.19035132889867823, 0.19035132889867823, 0.14049823119869775, 0.14049823119869775, 0.14049823119869775, 0.23743485797320252, 0.23743485797320252, 0.23743485797320252, 0.1819662938490958, 0.1819662938490958, 0.1819662938490958, 0.147017622974412, 0.147017622974412, 0.147017622974412, 0.18095415454527453, 0.18095415454527453, 0.18095415454527453, 0.12379941097792346, 0.12379941097792346, 0.12379941097792346, 0.12029602295464359, 0.12029602295464359, 0.12029602295464359, 0.1962845951533836, 0.1962845951533836, 0.1962845951533836, 0.1983247215570193, 0.1983247215570193, 0.1983247215570193, 0.1801949019581549, 0.1801949019581549, 0.1801949019581549, 0.15521748927025825, 0.15521748927025825, 0.15521748927025825, 0.15141883263656286, 0.15141883263656286, 0.15141883263656286, 0.15956077479837927, 0.15956077479837927, 0.15956077479837927, 0.4228031242333179, 0.4228031242333179, 0.4228031242333179, 0.16613481636705874, 0.16613481636705874, 0.16613481636705874, 0.4276641160112272, 0.4276641160112272, 0.4276641160112272, 0.2665393237632102, 0.2665393237632102, 0.2665393237632102, 0.18484242669570228, 0.18484242669570228, 0.18484242669570228, 0.20912219352292039, 0.20912219352292039, 0.20912219352292039, 0.18617936011376324, 0.18617936011376324, 0.18617936011376324, 0.19137225423445947, 0.19137225423445947, 0.19137225423445947, 0.19702335371514523, 0.19702335371514523, 0.19702335371514523, 0.07095952931687133, 0.07095952931687133, 0.07095952931687133, 0.1050926526777739, 0.1050926526777739, 0.1050926526777739, 0.06988402984414765, 0.06988402984414765, 0.06988402984414765]}, "mutation_prompt": null}
{"id": "4c84639b-8a34-4bfb-8186-d2611acf4538", "solution": "import numpy as np\n\nclass PSO_SA_Optimizer_Enhanced:\n    def __init__(self, budget, dim, pop_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.w_max = 0.9\n        self.w_min = 0.4\n        self.c1_init = 2.0  # Initial cognitive parameter\n        self.c2_init = 1.0  # Initial social parameter\n        self.temp = 1.0\n\n    def __call__(self, func):\n        np.random.seed(42)\n        \n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.array([func(pos) for pos in positions])\n        global_best_position = personal_best_positions[np.argmin(personal_best_scores)]\n        global_best_score = np.min(personal_best_scores)\n        \n        eval_count = self.pop_size\n        \n        while eval_count < self.budget:\n            w = self.w_max - ((self.w_max - self.w_min) * (eval_count / self.budget))\n            c1 = self.c1_init * (1 - eval_count / self.budget)  # Dynamic cognitive\n            c2 = self.c2_init * (eval_count / self.budget)  # Dynamic social\n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                velocities[i] = (w * velocities[i]\n                                 + c1 * r1 * (personal_best_positions[i] - positions[i])\n                                 + c2 * r2 * (global_best_position - positions[i]))\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], self.lower_bound, self.upper_bound)\n\n                score = func(positions[i])\n                eval_count += 1\n                if eval_count >= self.budget:\n                    break\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = positions[i]\n\n                if np.random.rand() < np.exp(-(score - global_best_score) / self.temp):\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n                    if score < global_best_score:\n                        global_best_score = score\n                        global_best_position = positions[i]\n\n            self.temp *= 0.95\n\n        return global_best_position, global_best_score", "name": "PSO_SA_Optimizer_Enhanced", "description": "Modified PSO-SA incorporating dynamic cognitive and social parameters for improved convergence.", "configspace": "", "generation": 25, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "a6ced734-3ce2-4438-a3b1-cd500dc0bfe6", "metadata": {"aucs": [0.5552032341291239, 0.5552032341291239, 0.5552032341291239, 0.45079232168892436, 0.45079232168892436, 0.45079232168892436, 0.49261961612188865, 0.49261961612188865, 0.49261961612188865, 0.1534035740593066, 0.1534035740593066, 0.1534035740593066, 0.03196945674084872, 0.03196945674084872, 0.03196945674084872, 0.028423438180651517, 0.028423438180651517, 0.028423438180651517, 0.09722680421452112, 0.09722680421452112, 0.09722680421452112, 0.08441730956529958, 0.08441730956529958, 0.08441730956529958, 0.11456156249563842, 0.11456156249563842, 0.11456156249563842, 0.09107142533975343, 0.09107142533975343, 0.09107142533975343, 0.08211323659876146, 0.08211323659876146, 0.08211323659876146, 0.10206430761582197, 0.10206430761582197, 0.10206430761582197, 0.5924351363112623, 0.5924351363112623, 0.5924351363112623, 0.9047980793814148, 0.9047980793814148, 0.9047980793814148, 0.8495027452395066, 0.8495027452395066, 0.8495027452395066, 0.26652443155696826, 0.26652443155696826, 0.26652443155696826, 0.3101120552834319, 0.3101120552834319, 0.3101120552834319, 0.23692796700254026, 0.23692796700254026, 0.23692796700254026, 0.1438816659635016, 0.1438816659635016, 0.1438816659635016, 0.19167430865111124, 0.19167430865111124, 0.19167430865111124, 0.2253607926875516, 0.2253607926875516, 0.2253607926875516, 0.13534101194193848, 0.13534101194193848, 0.13534101194193848, 0.08784400890223076, 0.08784400890223076, 0.08784400890223076, 0.12085401156217468, 0.12085401156217468, 0.12085401156217468, 0.15354298018844237, 0.15354298018844237, 0.15354298018844237, 0.1335247507026852, 0.1335247507026852, 0.1335247507026852, 0.12652675422387816, 0.12652675422387816, 0.12652675422387816, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.015638664098327704, 0.015638664098327704, 0.015638664098327704, 0.06348601714929225, 0.06348601714929225, 0.06348601714929225, 0.5128201195876666, 0.5128201195876666, 0.5128201195876666, 0.3244201399797112, 0.3244201399797112, 0.3244201399797112, 0.45247627263357393, 0.45247627263357393, 0.45247627263357393, 0.04451653784519172, 0.04451653784519172, 0.04451653784519172, 0.10244570601375602, 0.10244570601375602, 0.10244570601375602, 0.18293966820632035, 0.18293966820632035, 0.18293966820632035, 0.17744947312927328, 0.17744947312927328, 0.17744947312927328, 0.21510233156622172, 0.21510233156622172, 0.21510233156622172, 0.2355370232572408, 0.2355370232572408, 0.2355370232572408, 0.4148020855085609, 0.4148020855085609, 0.4148020855085609, 0.47609313173401036, 0.47609313173401036, 0.47609313173401036, 0.5343860690848905, 0.5343860690848905, 0.5343860690848905, 0.08606766261267618, 0.08606766261267618, 0.08606766261267618, 0.07384008998724745, 0.07384008998724745, 0.07384008998724745, 0.05731658403093409, 0.05731658403093409, 0.05731658403093409, 0.1864398127353536, 0.1864398127353536, 0.1864398127353536, 0.2522622823718256, 0.2522622823718256, 0.2522622823718256, 0.2219281605217851, 0.2219281605217851, 0.2219281605217851, 0.24675734876587718, 0.24675734876587718, 0.24675734876587718, 0.28422697657660434, 0.28422697657660434, 0.28422697657660434, 0.19285446987115784, 0.19285446987115784, 0.19285446987115784, 0.2535563484588895, 0.2535563484588895, 0.2535563484588895, 0.16542937537019864, 0.16542937537019864, 0.16542937537019864, 0.13177240973192006, 0.13177240973192006, 0.13177240973192006, 0.2186666325572999, 0.2186666325572999, 0.2186666325572999, 0.2280332499933979, 0.2280332499933979, 0.2280332499933979, 0.22633589337301674, 0.22633589337301674, 0.22633589337301674, 0.18181320157776804, 0.18181320157776804, 0.18181320157776804, 0.17564371084747676, 0.17564371084747676, 0.17564371084747676, 0.1965169021585963, 0.1965169021585963, 0.1965169021585963, 0.7218961390601533, 0.7218961390601533, 0.7218961390601533, 0.1906535662842197, 0.1906535662842197, 0.1906535662842197, 0.1528929509784832, 0.1528929509784832, 0.1528929509784832, 0.45106342858558135, 0.45106342858558135, 0.45106342858558135, 0.1996635041682644, 0.1996635041682644, 0.1996635041682644, 0.46981473137622387, 0.46981473137622387, 0.46981473137622387, 0.19155638439528389, 0.19155638439528389, 0.19155638439528389, 0.23186334416635013, 0.23186334416635013, 0.23186334416635013, 0.19238491741213615, 0.19238491741213615, 0.19238491741213615, 0.08784421046769497, 0.08784421046769497, 0.08784421046769497, 0.09506561691979443, 0.09506561691979443, 0.09506561691979443, 0.10918892216801945, 0.10918892216801945, 0.10918892216801945]}, "mutation_prompt": null}
{"id": "5d1d7112-0ca4-4350-b048-1ea22b7d65be", "solution": "import numpy as np\n\nclass PSO_SA_Optimizer_Enhanced:\n    def __init__(self, budget, dim, pop_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.w_max = 0.9\n        self.w_min = 0.4\n        self.c1_init = 2.0  # Initial cognitive parameter\n        self.c2_init = 1.0  # Initial social parameter\n        self.temp = 1.0\n\n    def __call__(self, func):\n        np.random.seed(42)\n        \n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.array([func(pos) for pos in positions])\n        global_best_position = personal_best_positions[np.argmin(personal_best_scores)]\n        global_best_score = np.min(personal_best_scores)\n        \n        eval_count = self.pop_size\n        \n        while eval_count < self.budget:\n            w = self.w_max - ((self.w_max - self.w_min) * (eval_count / self.budget))\n            c1 = self.c1_init * (1 - eval_count / self.budget)  # Dynamic cognitive\n            c2 = self.c2_init * (eval_count / self.budget)  # Dynamic social\n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                velocities[i] = (w * velocities[i]\n                                 + c1 * r1 * (personal_best_positions[i] - positions[i])\n                                 + c2 * r2 * (global_best_position - positions[i]))\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], self.lower_bound, self.upper_bound)\n\n                score = func(positions[i])\n                eval_count += 1\n                if eval_count >= self.budget:\n                    break\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = positions[i]\n\n                if np.random.rand() < np.exp(-(score - global_best_score) / self.temp):\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n                    if score < global_best_score:\n                        global_best_score = score\n                        global_best_position = positions[i]\n\n            self.temp *= 0.95\n\n        return global_best_position, global_best_score", "name": "PSO_SA_Optimizer_Enhanced", "description": "Modified PSO-SA incorporating dynamic cognitive and social parameters for improved convergence.", "configspace": "", "generation": 25, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "a6ced734-3ce2-4438-a3b1-cd500dc0bfe6", "metadata": {"aucs": [0.5552032341291239, 0.5552032341291239, 0.5552032341291239, 0.45079232168892436, 0.45079232168892436, 0.45079232168892436, 0.49261961612188865, 0.49261961612188865, 0.49261961612188865, 0.1534035740593066, 0.1534035740593066, 0.1534035740593066, 0.03196945674084872, 0.03196945674084872, 0.03196945674084872, 0.028423438180651517, 0.028423438180651517, 0.028423438180651517, 0.09722680421452112, 0.09722680421452112, 0.09722680421452112, 0.08441730956529958, 0.08441730956529958, 0.08441730956529958, 0.11456156249563842, 0.11456156249563842, 0.11456156249563842, 0.09107142533975343, 0.09107142533975343, 0.09107142533975343, 0.08211323659876146, 0.08211323659876146, 0.08211323659876146, 0.10206430761582197, 0.10206430761582197, 0.10206430761582197, 0.5924351363112623, 0.5924351363112623, 0.5924351363112623, 0.9047980793814148, 0.9047980793814148, 0.9047980793814148, 0.8495027452395066, 0.8495027452395066, 0.8495027452395066, 0.26652443155696826, 0.26652443155696826, 0.26652443155696826, 0.3101120552834319, 0.3101120552834319, 0.3101120552834319, 0.23692796700254026, 0.23692796700254026, 0.23692796700254026, 0.1438816659635016, 0.1438816659635016, 0.1438816659635016, 0.19167430865111124, 0.19167430865111124, 0.19167430865111124, 0.2253607926875516, 0.2253607926875516, 0.2253607926875516, 0.13534101194193848, 0.13534101194193848, 0.13534101194193848, 0.08784400890223076, 0.08784400890223076, 0.08784400890223076, 0.12085401156217468, 0.12085401156217468, 0.12085401156217468, 0.15354298018844237, 0.15354298018844237, 0.15354298018844237, 0.1335247507026852, 0.1335247507026852, 0.1335247507026852, 0.12652675422387816, 0.12652675422387816, 0.12652675422387816, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.015638664098327704, 0.015638664098327704, 0.015638664098327704, 0.06348601714929225, 0.06348601714929225, 0.06348601714929225, 0.5128201195876666, 0.5128201195876666, 0.5128201195876666, 0.3244201399797112, 0.3244201399797112, 0.3244201399797112, 0.45247627263357393, 0.45247627263357393, 0.45247627263357393, 0.04451653784519172, 0.04451653784519172, 0.04451653784519172, 0.10244570601375602, 0.10244570601375602, 0.10244570601375602, 0.18293966820632035, 0.18293966820632035, 0.18293966820632035, 0.17744947312927328, 0.17744947312927328, 0.17744947312927328, 0.21510233156622172, 0.21510233156622172, 0.21510233156622172, 0.2355370232572408, 0.2355370232572408, 0.2355370232572408, 0.4148020855085609, 0.4148020855085609, 0.4148020855085609, 0.47609313173401036, 0.47609313173401036, 0.47609313173401036, 0.5343860690848905, 0.5343860690848905, 0.5343860690848905, 0.08606766261267618, 0.08606766261267618, 0.08606766261267618, 0.07384008998724745, 0.07384008998724745, 0.07384008998724745, 0.05731658403093409, 0.05731658403093409, 0.05731658403093409, 0.1864398127353536, 0.1864398127353536, 0.1864398127353536, 0.2522622823718256, 0.2522622823718256, 0.2522622823718256, 0.2219281605217851, 0.2219281605217851, 0.2219281605217851, 0.24675734876587718, 0.24675734876587718, 0.24675734876587718, 0.28422697657660434, 0.28422697657660434, 0.28422697657660434, 0.19285446987115784, 0.19285446987115784, 0.19285446987115784, 0.2535563484588895, 0.2535563484588895, 0.2535563484588895, 0.16542937537019864, 0.16542937537019864, 0.16542937537019864, 0.13177240973192006, 0.13177240973192006, 0.13177240973192006, 0.2186666325572999, 0.2186666325572999, 0.2186666325572999, 0.2280332499933979, 0.2280332499933979, 0.2280332499933979, 0.22633589337301674, 0.22633589337301674, 0.22633589337301674, 0.18181320157776804, 0.18181320157776804, 0.18181320157776804, 0.17564371084747676, 0.17564371084747676, 0.17564371084747676, 0.1965169021585963, 0.1965169021585963, 0.1965169021585963, 0.7218961390601533, 0.7218961390601533, 0.7218961390601533, 0.1906535662842197, 0.1906535662842197, 0.1906535662842197, 0.1528929509784832, 0.1528929509784832, 0.1528929509784832, 0.45106342858558135, 0.45106342858558135, 0.45106342858558135, 0.1996635041682644, 0.1996635041682644, 0.1996635041682644, 0.46981473137622387, 0.46981473137622387, 0.46981473137622387, 0.19155638439528389, 0.19155638439528389, 0.19155638439528389, 0.23186334416635013, 0.23186334416635013, 0.23186334416635013, 0.19238491741213615, 0.19238491741213615, 0.19238491741213615, 0.08784421046769497, 0.08784421046769497, 0.08784421046769497, 0.09506561691979443, 0.09506561691979443, 0.09506561691979443, 0.10918892216801945, 0.10918892216801945, 0.10918892216801945]}, "mutation_prompt": null}
{"id": "21438f39-9a7e-43a1-9059-d9b32c05402c", "solution": "import numpy as np\n\nclass PSO_SA_Optimizer_Enhanced:\n    def __init__(self, budget, dim, pop_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.w_max = 0.9\n        self.w_min = 0.4\n        self.c1_init = 2.0  # Initial cognitive parameter\n        self.c2_init = 1.0  # Initial social parameter\n        self.temp = 1.0\n\n    def __call__(self, func):\n        np.random.seed(42)\n        \n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.array([func(pos) for pos in positions])\n        global_best_position = personal_best_positions[np.argmin(personal_best_scores)]\n        global_best_score = np.min(personal_best_scores)\n        \n        eval_count = self.pop_size\n        \n        while eval_count < self.budget:\n            w = self.w_max - ((self.w_max - self.w_min) * (eval_count / self.budget))\n            c1 = self.c1_init * (1 - eval_count / self.budget)  # Dynamic cognitive\n            c2 = self.c2_init * (eval_count / self.budget)  # Dynamic social\n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                velocities[i] = (w * velocities[i]\n                                 + c1 * r1 * (personal_best_positions[i] - positions[i])\n                                 + c2 * r2 * (global_best_position - positions[i]))\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], self.lower_bound, self.upper_bound)\n\n                score = func(positions[i])\n                eval_count += 1\n                if eval_count >= self.budget:\n                    break\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = positions[i]\n\n                if np.random.rand() < np.exp(-(score - global_best_score) / self.temp):\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n                    if score < global_best_score:\n                        global_best_score = score\n                        global_best_position = positions[i]\n\n            self.temp *= 0.95\n\n        return global_best_position, global_best_score", "name": "PSO_SA_Optimizer_Enhanced", "description": "Modified PSO-SA incorporating dynamic cognitive and social parameters for improved convergence.", "configspace": "", "generation": 25, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "a6ced734-3ce2-4438-a3b1-cd500dc0bfe6", "metadata": {"aucs": [0.5552032341291239, 0.5552032341291239, 0.5552032341291239, 0.45079232168892436, 0.45079232168892436, 0.45079232168892436, 0.49261961612188865, 0.49261961612188865, 0.49261961612188865, 0.1534035740593066, 0.1534035740593066, 0.1534035740593066, 0.03196945674084872, 0.03196945674084872, 0.03196945674084872, 0.028423438180651517, 0.028423438180651517, 0.028423438180651517, 0.09722680421452112, 0.09722680421452112, 0.09722680421452112, 0.08441730956529958, 0.08441730956529958, 0.08441730956529958, 0.11456156249563842, 0.11456156249563842, 0.11456156249563842, 0.09107142533975343, 0.09107142533975343, 0.09107142533975343, 0.08211323659876146, 0.08211323659876146, 0.08211323659876146, 0.10206430761582197, 0.10206430761582197, 0.10206430761582197, 0.5924351363112623, 0.5924351363112623, 0.5924351363112623, 0.9047980793814148, 0.9047980793814148, 0.9047980793814148, 0.8495027452395066, 0.8495027452395066, 0.8495027452395066, 0.26652443155696826, 0.26652443155696826, 0.26652443155696826, 0.3101120552834319, 0.3101120552834319, 0.3101120552834319, 0.23692796700254026, 0.23692796700254026, 0.23692796700254026, 0.1438816659635016, 0.1438816659635016, 0.1438816659635016, 0.19167430865111124, 0.19167430865111124, 0.19167430865111124, 0.2253607926875516, 0.2253607926875516, 0.2253607926875516, 0.13534101194193848, 0.13534101194193848, 0.13534101194193848, 0.08784400890223076, 0.08784400890223076, 0.08784400890223076, 0.12085401156217468, 0.12085401156217468, 0.12085401156217468, 0.15354298018844237, 0.15354298018844237, 0.15354298018844237, 0.1335247507026852, 0.1335247507026852, 0.1335247507026852, 0.12652675422387816, 0.12652675422387816, 0.12652675422387816, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.015638664098327704, 0.015638664098327704, 0.015638664098327704, 0.06348601714929225, 0.06348601714929225, 0.06348601714929225, 0.5128201195876666, 0.5128201195876666, 0.5128201195876666, 0.3244201399797112, 0.3244201399797112, 0.3244201399797112, 0.45247627263357393, 0.45247627263357393, 0.45247627263357393, 0.04451653784519172, 0.04451653784519172, 0.04451653784519172, 0.10244570601375602, 0.10244570601375602, 0.10244570601375602, 0.18293966820632035, 0.18293966820632035, 0.18293966820632035, 0.17744947312927328, 0.17744947312927328, 0.17744947312927328, 0.21510233156622172, 0.21510233156622172, 0.21510233156622172, 0.2355370232572408, 0.2355370232572408, 0.2355370232572408, 0.4148020855085609, 0.4148020855085609, 0.4148020855085609, 0.47609313173401036, 0.47609313173401036, 0.47609313173401036, 0.5343860690848905, 0.5343860690848905, 0.5343860690848905, 0.08606766261267618, 0.08606766261267618, 0.08606766261267618, 0.07384008998724745, 0.07384008998724745, 0.07384008998724745, 0.05731658403093409, 0.05731658403093409, 0.05731658403093409, 0.1864398127353536, 0.1864398127353536, 0.1864398127353536, 0.2522622823718256, 0.2522622823718256, 0.2522622823718256, 0.2219281605217851, 0.2219281605217851, 0.2219281605217851, 0.24675734876587718, 0.24675734876587718, 0.24675734876587718, 0.28422697657660434, 0.28422697657660434, 0.28422697657660434, 0.19285446987115784, 0.19285446987115784, 0.19285446987115784, 0.2535563484588895, 0.2535563484588895, 0.2535563484588895, 0.16542937537019864, 0.16542937537019864, 0.16542937537019864, 0.13177240973192006, 0.13177240973192006, 0.13177240973192006, 0.2186666325572999, 0.2186666325572999, 0.2186666325572999, 0.2280332499933979, 0.2280332499933979, 0.2280332499933979, 0.22633589337301674, 0.22633589337301674, 0.22633589337301674, 0.18181320157776804, 0.18181320157776804, 0.18181320157776804, 0.17564371084747676, 0.17564371084747676, 0.17564371084747676, 0.1965169021585963, 0.1965169021585963, 0.1965169021585963, 0.7218961390601533, 0.7218961390601533, 0.7218961390601533, 0.1906535662842197, 0.1906535662842197, 0.1906535662842197, 0.1528929509784832, 0.1528929509784832, 0.1528929509784832, 0.45106342858558135, 0.45106342858558135, 0.45106342858558135, 0.1996635041682644, 0.1996635041682644, 0.1996635041682644, 0.46981473137622387, 0.46981473137622387, 0.46981473137622387, 0.19155638439528389, 0.19155638439528389, 0.19155638439528389, 0.23186334416635013, 0.23186334416635013, 0.23186334416635013, 0.19238491741213615, 0.19238491741213615, 0.19238491741213615, 0.08784421046769497, 0.08784421046769497, 0.08784421046769497, 0.09506561691979443, 0.09506561691979443, 0.09506561691979443, 0.10918892216801945, 0.10918892216801945, 0.10918892216801945]}, "mutation_prompt": null}
{"id": "7f7578d2-f529-4c36-bf38-ad7c0d7762a0", "solution": "import numpy as np\n\nclass PSO_SA_Optimizer_Enhanced:\n    def __init__(self, budget, dim, pop_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.w_max = 0.9\n        self.w_min = 0.4\n        self.c1_init = 2.0  # Initial cognitive parameter\n        self.c2_init = 1.0  # Initial social parameter\n        self.temp = 1.0\n\n    def __call__(self, func):\n        np.random.seed(42)\n        \n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.array([func(pos) for pos in positions])\n        global_best_position = personal_best_positions[np.argmin(personal_best_scores)]\n        global_best_score = np.min(personal_best_scores)\n        \n        eval_count = self.pop_size\n        \n        while eval_count < self.budget:\n            w = self.w_max - ((self.w_max - self.w_min) * (eval_count / self.budget))\n            c1 = self.c1_init * (1 - eval_count / self.budget)  # Dynamic cognitive\n            c2 = self.c2_init * (eval_count / self.budget)  # Dynamic social\n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                velocities[i] = (w * velocities[i]\n                                 + c1 * r1 * (personal_best_positions[i] - positions[i])\n                                 + c2 * r2 * (global_best_position - positions[i]))\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], self.lower_bound, self.upper_bound)\n\n                score = func(positions[i])\n                eval_count += 1\n                if eval_count >= self.budget:\n                    break\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = positions[i]\n\n                if np.random.rand() < np.exp(-(score - global_best_score) / self.temp):\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n                    if score < global_best_score:\n                        global_best_score = score\n                        global_best_position = positions[i]\n\n            self.temp *= 0.95\n\n        return global_best_position, global_best_score", "name": "PSO_SA_Optimizer_Enhanced", "description": "Modified PSO-SA incorporating dynamic cognitive and social parameters for improved convergence.", "configspace": "", "generation": 25, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "a6ced734-3ce2-4438-a3b1-cd500dc0bfe6", "metadata": {"aucs": [0.5552032341291239, 0.5552032341291239, 0.5552032341291239, 0.45079232168892436, 0.45079232168892436, 0.45079232168892436, 0.49261961612188865, 0.49261961612188865, 0.49261961612188865, 0.1534035740593066, 0.1534035740593066, 0.1534035740593066, 0.03196945674084872, 0.03196945674084872, 0.03196945674084872, 0.028423438180651517, 0.028423438180651517, 0.028423438180651517, 0.09722680421452112, 0.09722680421452112, 0.09722680421452112, 0.08441730956529958, 0.08441730956529958, 0.08441730956529958, 0.11456156249563842, 0.11456156249563842, 0.11456156249563842, 0.09107142533975343, 0.09107142533975343, 0.09107142533975343, 0.08211323659876146, 0.08211323659876146, 0.08211323659876146, 0.10206430761582197, 0.10206430761582197, 0.10206430761582197, 0.5924351363112623, 0.5924351363112623, 0.5924351363112623, 0.9047980793814148, 0.9047980793814148, 0.9047980793814148, 0.8495027452395066, 0.8495027452395066, 0.8495027452395066, 0.26652443155696826, 0.26652443155696826, 0.26652443155696826, 0.3101120552834319, 0.3101120552834319, 0.3101120552834319, 0.23692796700254026, 0.23692796700254026, 0.23692796700254026, 0.1438816659635016, 0.1438816659635016, 0.1438816659635016, 0.19167430865111124, 0.19167430865111124, 0.19167430865111124, 0.2253607926875516, 0.2253607926875516, 0.2253607926875516, 0.13534101194193848, 0.13534101194193848, 0.13534101194193848, 0.08784400890223076, 0.08784400890223076, 0.08784400890223076, 0.12085401156217468, 0.12085401156217468, 0.12085401156217468, 0.15354298018844237, 0.15354298018844237, 0.15354298018844237, 0.1335247507026852, 0.1335247507026852, 0.1335247507026852, 0.12652675422387816, 0.12652675422387816, 0.12652675422387816, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.015638664098327704, 0.015638664098327704, 0.015638664098327704, 0.06348601714929225, 0.06348601714929225, 0.06348601714929225, 0.5128201195876666, 0.5128201195876666, 0.5128201195876666, 0.3244201399797112, 0.3244201399797112, 0.3244201399797112, 0.45247627263357393, 0.45247627263357393, 0.45247627263357393, 0.04451653784519172, 0.04451653784519172, 0.04451653784519172, 0.10244570601375602, 0.10244570601375602, 0.10244570601375602, 0.18293966820632035, 0.18293966820632035, 0.18293966820632035, 0.17744947312927328, 0.17744947312927328, 0.17744947312927328, 0.21510233156622172, 0.21510233156622172, 0.21510233156622172, 0.2355370232572408, 0.2355370232572408, 0.2355370232572408, 0.4148020855085609, 0.4148020855085609, 0.4148020855085609, 0.47609313173401036, 0.47609313173401036, 0.47609313173401036, 0.5343860690848905, 0.5343860690848905, 0.5343860690848905, 0.08606766261267618, 0.08606766261267618, 0.08606766261267618, 0.07384008998724745, 0.07384008998724745, 0.07384008998724745, 0.05731658403093409, 0.05731658403093409, 0.05731658403093409, 0.1864398127353536, 0.1864398127353536, 0.1864398127353536, 0.2522622823718256, 0.2522622823718256, 0.2522622823718256, 0.2219281605217851, 0.2219281605217851, 0.2219281605217851, 0.24675734876587718, 0.24675734876587718, 0.24675734876587718, 0.28422697657660434, 0.28422697657660434, 0.28422697657660434, 0.19285446987115784, 0.19285446987115784, 0.19285446987115784, 0.2535563484588895, 0.2535563484588895, 0.2535563484588895, 0.16542937537019864, 0.16542937537019864, 0.16542937537019864, 0.13177240973192006, 0.13177240973192006, 0.13177240973192006, 0.2186666325572999, 0.2186666325572999, 0.2186666325572999, 0.2280332499933979, 0.2280332499933979, 0.2280332499933979, 0.22633589337301674, 0.22633589337301674, 0.22633589337301674, 0.18181320157776804, 0.18181320157776804, 0.18181320157776804, 0.17564371084747676, 0.17564371084747676, 0.17564371084747676, 0.1965169021585963, 0.1965169021585963, 0.1965169021585963, 0.7218961390601533, 0.7218961390601533, 0.7218961390601533, 0.1906535662842197, 0.1906535662842197, 0.1906535662842197, 0.1528929509784832, 0.1528929509784832, 0.1528929509784832, 0.45106342858558135, 0.45106342858558135, 0.45106342858558135, 0.1996635041682644, 0.1996635041682644, 0.1996635041682644, 0.46981473137622387, 0.46981473137622387, 0.46981473137622387, 0.19155638439528389, 0.19155638439528389, 0.19155638439528389, 0.23186334416635013, 0.23186334416635013, 0.23186334416635013, 0.19238491741213615, 0.19238491741213615, 0.19238491741213615, 0.08784421046769497, 0.08784421046769497, 0.08784421046769497, 0.09506561691979443, 0.09506561691979443, 0.09506561691979443, 0.10918892216801945, 0.10918892216801945, 0.10918892216801945]}, "mutation_prompt": null}
{"id": "2958d84e-66ae-4d39-8d0d-fff0c55ecec5", "solution": "import numpy as np\n\nclass PSO_SA_Optimizer_Enhanced:\n    def __init__(self, budget, dim, pop_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.w_max = 0.9\n        self.w_min = 0.4\n        self.c1_init = 2.0  # Initial cognitive parameter\n        self.c2_init = 1.0  # Initial social parameter\n        self.temp = 1.0\n\n    def __call__(self, func):\n        np.random.seed(42)\n        \n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.array([func(pos) for pos in positions])\n        global_best_position = personal_best_positions[np.argmin(personal_best_scores)]\n        global_best_score = np.min(personal_best_scores)\n        \n        eval_count = self.pop_size\n        \n        while eval_count < self.budget:\n            w = self.w_max - ((self.w_max - self.w_min) * (eval_count / self.budget))\n            c1 = self.c1_init * (1 - eval_count / self.budget)  # Dynamic cognitive\n            c2 = self.c2_init * (eval_count / self.budget)  # Dynamic social\n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                velocities[i] = (w * velocities[i]\n                                 + c1 * r1 * (personal_best_positions[i] - positions[i])\n                                 + c2 * r2 * (global_best_position - positions[i]))\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], self.lower_bound, self.upper_bound)\n\n                score = func(positions[i])\n                eval_count += 1\n                if eval_count >= self.budget:\n                    break\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = positions[i]\n\n                if np.random.rand() < np.exp(-(score - global_best_score) / self.temp):\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n                    if score < global_best_score:\n                        global_best_score = score\n                        global_best_position = positions[i]\n\n            self.temp *= 0.95\n\n        return global_best_position, global_best_score", "name": "PSO_SA_Optimizer_Enhanced", "description": "Modified PSO-SA incorporating dynamic cognitive and social parameters for improved convergence.", "configspace": "", "generation": 25, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "a6ced734-3ce2-4438-a3b1-cd500dc0bfe6", "metadata": {"aucs": [0.5552032341291239, 0.5552032341291239, 0.5552032341291239, 0.45079232168892436, 0.45079232168892436, 0.45079232168892436, 0.49261961612188865, 0.49261961612188865, 0.49261961612188865, 0.1534035740593066, 0.1534035740593066, 0.1534035740593066, 0.03196945674084872, 0.03196945674084872, 0.03196945674084872, 0.028423438180651517, 0.028423438180651517, 0.028423438180651517, 0.09722680421452112, 0.09722680421452112, 0.09722680421452112, 0.08441730956529958, 0.08441730956529958, 0.08441730956529958, 0.11456156249563842, 0.11456156249563842, 0.11456156249563842, 0.09107142533975343, 0.09107142533975343, 0.09107142533975343, 0.08211323659876146, 0.08211323659876146, 0.08211323659876146, 0.10206430761582197, 0.10206430761582197, 0.10206430761582197, 0.5924351363112623, 0.5924351363112623, 0.5924351363112623, 0.9047980793814148, 0.9047980793814148, 0.9047980793814148, 0.8495027452395066, 0.8495027452395066, 0.8495027452395066, 0.26652443155696826, 0.26652443155696826, 0.26652443155696826, 0.3101120552834319, 0.3101120552834319, 0.3101120552834319, 0.23692796700254026, 0.23692796700254026, 0.23692796700254026, 0.1438816659635016, 0.1438816659635016, 0.1438816659635016, 0.19167430865111124, 0.19167430865111124, 0.19167430865111124, 0.2253607926875516, 0.2253607926875516, 0.2253607926875516, 0.13534101194193848, 0.13534101194193848, 0.13534101194193848, 0.08784400890223076, 0.08784400890223076, 0.08784400890223076, 0.12085401156217468, 0.12085401156217468, 0.12085401156217468, 0.15354298018844237, 0.15354298018844237, 0.15354298018844237, 0.1335247507026852, 0.1335247507026852, 0.1335247507026852, 0.12652675422387816, 0.12652675422387816, 0.12652675422387816, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.015638664098327704, 0.015638664098327704, 0.015638664098327704, 0.06348601714929225, 0.06348601714929225, 0.06348601714929225, 0.5128201195876666, 0.5128201195876666, 0.5128201195876666, 0.3244201399797112, 0.3244201399797112, 0.3244201399797112, 0.45247627263357393, 0.45247627263357393, 0.45247627263357393, 0.04451653784519172, 0.04451653784519172, 0.04451653784519172, 0.10244570601375602, 0.10244570601375602, 0.10244570601375602, 0.18293966820632035, 0.18293966820632035, 0.18293966820632035, 0.17744947312927328, 0.17744947312927328, 0.17744947312927328, 0.21510233156622172, 0.21510233156622172, 0.21510233156622172, 0.2355370232572408, 0.2355370232572408, 0.2355370232572408, 0.4148020855085609, 0.4148020855085609, 0.4148020855085609, 0.47609313173401036, 0.47609313173401036, 0.47609313173401036, 0.5343860690848905, 0.5343860690848905, 0.5343860690848905, 0.08606766261267618, 0.08606766261267618, 0.08606766261267618, 0.07384008998724745, 0.07384008998724745, 0.07384008998724745, 0.05731658403093409, 0.05731658403093409, 0.05731658403093409, 0.1864398127353536, 0.1864398127353536, 0.1864398127353536, 0.2522622823718256, 0.2522622823718256, 0.2522622823718256, 0.2219281605217851, 0.2219281605217851, 0.2219281605217851, 0.24675734876587718, 0.24675734876587718, 0.24675734876587718, 0.28422697657660434, 0.28422697657660434, 0.28422697657660434, 0.19285446987115784, 0.19285446987115784, 0.19285446987115784, 0.2535563484588895, 0.2535563484588895, 0.2535563484588895, 0.16542937537019864, 0.16542937537019864, 0.16542937537019864, 0.13177240973192006, 0.13177240973192006, 0.13177240973192006, 0.2186666325572999, 0.2186666325572999, 0.2186666325572999, 0.2280332499933979, 0.2280332499933979, 0.2280332499933979, 0.22633589337301674, 0.22633589337301674, 0.22633589337301674, 0.18181320157776804, 0.18181320157776804, 0.18181320157776804, 0.17564371084747676, 0.17564371084747676, 0.17564371084747676, 0.1965169021585963, 0.1965169021585963, 0.1965169021585963, 0.7218961390601533, 0.7218961390601533, 0.7218961390601533, 0.1906535662842197, 0.1906535662842197, 0.1906535662842197, 0.1528929509784832, 0.1528929509784832, 0.1528929509784832, 0.45106342858558135, 0.45106342858558135, 0.45106342858558135, 0.1996635041682644, 0.1996635041682644, 0.1996635041682644, 0.46981473137622387, 0.46981473137622387, 0.46981473137622387, 0.19155638439528389, 0.19155638439528389, 0.19155638439528389, 0.23186334416635013, 0.23186334416635013, 0.23186334416635013, 0.19238491741213615, 0.19238491741213615, 0.19238491741213615, 0.08784421046769497, 0.08784421046769497, 0.08784421046769497, 0.09506561691979443, 0.09506561691979443, 0.09506561691979443, 0.10918892216801945, 0.10918892216801945, 0.10918892216801945]}, "mutation_prompt": null}
{"id": "98998b62-fa24-499a-af1f-debb0ade6b7b", "solution": "import numpy as np\n\nclass PSO_SA_Optimizer_Enhanced:\n    def __init__(self, budget, dim, pop_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.w_max = 0.9\n        self.w_min = 0.4\n        self.c1_init = 2.0  # Initial cognitive parameter\n        self.c2_init = 1.0  # Initial social parameter\n        self.temp = 1.0\n\n    def __call__(self, func):\n        np.random.seed(42)\n        \n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.array([func(pos) for pos in positions])\n        global_best_position = personal_best_positions[np.argmin(personal_best_scores)]\n        global_best_score = np.min(personal_best_scores)\n        \n        eval_count = self.pop_size\n        \n        while eval_count < self.budget:\n            w = self.w_max - ((self.w_max - self.w_min) * (eval_count / self.budget))\n            c1 = self.c1_init * (1 - eval_count / self.budget)  # Dynamic cognitive\n            c2 = self.c2_init * (eval_count / self.budget)  # Dynamic social\n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                velocities[i] = (w * velocities[i]\n                                 + c1 * r1 * (personal_best_positions[i] - positions[i])\n                                 + c2 * r2 * (global_best_position - positions[i]))\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], self.lower_bound, self.upper_bound)\n\n                score = func(positions[i])\n                eval_count += 1\n                if eval_count >= self.budget:\n                    break\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = positions[i]\n\n                if np.random.rand() < np.exp(-(score - global_best_score) / self.temp):\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n                    if score < global_best_score:\n                        global_best_score = score\n                        global_best_position = positions[i]\n\n            self.temp *= 0.95\n\n        return global_best_position, global_best_score", "name": "PSO_SA_Optimizer_Enhanced", "description": "Modified PSO-SA incorporating dynamic cognitive and social parameters for improved convergence.", "configspace": "", "generation": 25, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "a6ced734-3ce2-4438-a3b1-cd500dc0bfe6", "metadata": {"aucs": [0.5552032341291239, 0.5552032341291239, 0.5552032341291239, 0.45079232168892436, 0.45079232168892436, 0.45079232168892436, 0.49261961612188865, 0.49261961612188865, 0.49261961612188865, 0.1534035740593066, 0.1534035740593066, 0.1534035740593066, 0.03196945674084872, 0.03196945674084872, 0.03196945674084872, 0.028423438180651517, 0.028423438180651517, 0.028423438180651517, 0.09722680421452112, 0.09722680421452112, 0.09722680421452112, 0.08441730956529958, 0.08441730956529958, 0.08441730956529958, 0.11456156249563842, 0.11456156249563842, 0.11456156249563842, 0.09107142533975343, 0.09107142533975343, 0.09107142533975343, 0.08211323659876146, 0.08211323659876146, 0.08211323659876146, 0.10206430761582197, 0.10206430761582197, 0.10206430761582197, 0.5924351363112623, 0.5924351363112623, 0.5924351363112623, 0.9047980793814148, 0.9047980793814148, 0.9047980793814148, 0.8495027452395066, 0.8495027452395066, 0.8495027452395066, 0.26652443155696826, 0.26652443155696826, 0.26652443155696826, 0.3101120552834319, 0.3101120552834319, 0.3101120552834319, 0.23692796700254026, 0.23692796700254026, 0.23692796700254026, 0.1438816659635016, 0.1438816659635016, 0.1438816659635016, 0.19167430865111124, 0.19167430865111124, 0.19167430865111124, 0.2253607926875516, 0.2253607926875516, 0.2253607926875516, 0.13534101194193848, 0.13534101194193848, 0.13534101194193848, 0.08784400890223076, 0.08784400890223076, 0.08784400890223076, 0.12085401156217468, 0.12085401156217468, 0.12085401156217468, 0.15354298018844237, 0.15354298018844237, 0.15354298018844237, 0.1335247507026852, 0.1335247507026852, 0.1335247507026852, 0.12652675422387816, 0.12652675422387816, 0.12652675422387816, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.015638664098327704, 0.015638664098327704, 0.015638664098327704, 0.06348601714929225, 0.06348601714929225, 0.06348601714929225, 0.5128201195876666, 0.5128201195876666, 0.5128201195876666, 0.3244201399797112, 0.3244201399797112, 0.3244201399797112, 0.45247627263357393, 0.45247627263357393, 0.45247627263357393, 0.04451653784519172, 0.04451653784519172, 0.04451653784519172, 0.10244570601375602, 0.10244570601375602, 0.10244570601375602, 0.18293966820632035, 0.18293966820632035, 0.18293966820632035, 0.17744947312927328, 0.17744947312927328, 0.17744947312927328, 0.21510233156622172, 0.21510233156622172, 0.21510233156622172, 0.2355370232572408, 0.2355370232572408, 0.2355370232572408, 0.4148020855085609, 0.4148020855085609, 0.4148020855085609, 0.47609313173401036, 0.47609313173401036, 0.47609313173401036, 0.5343860690848905, 0.5343860690848905, 0.5343860690848905, 0.08606766261267618, 0.08606766261267618, 0.08606766261267618, 0.07384008998724745, 0.07384008998724745, 0.07384008998724745, 0.05731658403093409, 0.05731658403093409, 0.05731658403093409, 0.1864398127353536, 0.1864398127353536, 0.1864398127353536, 0.2522622823718256, 0.2522622823718256, 0.2522622823718256, 0.2219281605217851, 0.2219281605217851, 0.2219281605217851, 0.24675734876587718, 0.24675734876587718, 0.24675734876587718, 0.28422697657660434, 0.28422697657660434, 0.28422697657660434, 0.19285446987115784, 0.19285446987115784, 0.19285446987115784, 0.2535563484588895, 0.2535563484588895, 0.2535563484588895, 0.16542937537019864, 0.16542937537019864, 0.16542937537019864, 0.13177240973192006, 0.13177240973192006, 0.13177240973192006, 0.2186666325572999, 0.2186666325572999, 0.2186666325572999, 0.2280332499933979, 0.2280332499933979, 0.2280332499933979, 0.22633589337301674, 0.22633589337301674, 0.22633589337301674, 0.18181320157776804, 0.18181320157776804, 0.18181320157776804, 0.17564371084747676, 0.17564371084747676, 0.17564371084747676, 0.1965169021585963, 0.1965169021585963, 0.1965169021585963, 0.7218961390601533, 0.7218961390601533, 0.7218961390601533, 0.1906535662842197, 0.1906535662842197, 0.1906535662842197, 0.1528929509784832, 0.1528929509784832, 0.1528929509784832, 0.45106342858558135, 0.45106342858558135, 0.45106342858558135, 0.1996635041682644, 0.1996635041682644, 0.1996635041682644, 0.46981473137622387, 0.46981473137622387, 0.46981473137622387, 0.19155638439528389, 0.19155638439528389, 0.19155638439528389, 0.23186334416635013, 0.23186334416635013, 0.23186334416635013, 0.19238491741213615, 0.19238491741213615, 0.19238491741213615, 0.08784421046769497, 0.08784421046769497, 0.08784421046769497, 0.09506561691979443, 0.09506561691979443, 0.09506561691979443, 0.10918892216801945, 0.10918892216801945, 0.10918892216801945]}, "mutation_prompt": null}
{"id": "e4c3dea5-3803-4116-a581-a1a481714f28", "solution": "import numpy as np\n\nclass PSO_SA_Optimizer_Enhanced:\n    def __init__(self, budget, dim, pop_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.w_max = 0.9\n        self.w_min = 0.4\n        self.c1_init = 2.0  # Initial cognitive parameter\n        self.c2_init = 1.0  # Initial social parameter\n        self.temp = 1.0\n\n    def __call__(self, func):\n        np.random.seed(42)\n        \n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.array([func(pos) for pos in positions])\n        global_best_position = personal_best_positions[np.argmin(personal_best_scores)]\n        global_best_score = np.min(personal_best_scores)\n        \n        eval_count = self.pop_size\n        \n        while eval_count < self.budget:\n            w = self.w_max - ((self.w_max - self.w_min) * (eval_count / self.budget))\n            c1 = self.c1_init * (1 - eval_count / self.budget)  # Dynamic cognitive\n            c2 = self.c2_init * (eval_count / self.budget)  # Dynamic social\n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                velocities[i] = (w * velocities[i]\n                                 + c1 * r1 * (personal_best_positions[i] - positions[i])\n                                 + c2 * r2 * (global_best_position - positions[i]))\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], self.lower_bound, self.upper_bound)\n\n                score = func(positions[i])\n                eval_count += 1\n                if eval_count >= self.budget:\n                    break\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = positions[i]\n\n                if np.random.rand() < np.exp(-(score - global_best_score) / self.temp):\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n                    if score < global_best_score:\n                        global_best_score = score\n                        global_best_position = positions[i]\n\n            self.temp *= 0.95\n\n        return global_best_position, global_best_score", "name": "PSO_SA_Optimizer_Enhanced", "description": "Modified PSO-SA incorporating dynamic cognitive and social parameters for improved convergence.", "configspace": "", "generation": 25, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "a6ced734-3ce2-4438-a3b1-cd500dc0bfe6", "metadata": {"aucs": [0.5552032341291239, 0.5552032341291239, 0.5552032341291239, 0.45079232168892436, 0.45079232168892436, 0.45079232168892436, 0.49261961612188865, 0.49261961612188865, 0.49261961612188865, 0.1534035740593066, 0.1534035740593066, 0.1534035740593066, 0.03196945674084872, 0.03196945674084872, 0.03196945674084872, 0.028423438180651517, 0.028423438180651517, 0.028423438180651517, 0.09722680421452112, 0.09722680421452112, 0.09722680421452112, 0.08441730956529958, 0.08441730956529958, 0.08441730956529958, 0.11456156249563842, 0.11456156249563842, 0.11456156249563842, 0.09107142533975343, 0.09107142533975343, 0.09107142533975343, 0.08211323659876146, 0.08211323659876146, 0.08211323659876146, 0.10206430761582197, 0.10206430761582197, 0.10206430761582197, 0.5924351363112623, 0.5924351363112623, 0.5924351363112623, 0.9047980793814148, 0.9047980793814148, 0.9047980793814148, 0.8495027452395066, 0.8495027452395066, 0.8495027452395066, 0.26652443155696826, 0.26652443155696826, 0.26652443155696826, 0.3101120552834319, 0.3101120552834319, 0.3101120552834319, 0.23692796700254026, 0.23692796700254026, 0.23692796700254026, 0.1438816659635016, 0.1438816659635016, 0.1438816659635016, 0.19167430865111124, 0.19167430865111124, 0.19167430865111124, 0.2253607926875516, 0.2253607926875516, 0.2253607926875516, 0.13534101194193848, 0.13534101194193848, 0.13534101194193848, 0.08784400890223076, 0.08784400890223076, 0.08784400890223076, 0.12085401156217468, 0.12085401156217468, 0.12085401156217468, 0.15354298018844237, 0.15354298018844237, 0.15354298018844237, 0.1335247507026852, 0.1335247507026852, 0.1335247507026852, 0.12652675422387816, 0.12652675422387816, 0.12652675422387816, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.015638664098327704, 0.015638664098327704, 0.015638664098327704, 0.06348601714929225, 0.06348601714929225, 0.06348601714929225, 0.5128201195876666, 0.5128201195876666, 0.5128201195876666, 0.3244201399797112, 0.3244201399797112, 0.3244201399797112, 0.45247627263357393, 0.45247627263357393, 0.45247627263357393, 0.04451653784519172, 0.04451653784519172, 0.04451653784519172, 0.10244570601375602, 0.10244570601375602, 0.10244570601375602, 0.18293966820632035, 0.18293966820632035, 0.18293966820632035, 0.17744947312927328, 0.17744947312927328, 0.17744947312927328, 0.21510233156622172, 0.21510233156622172, 0.21510233156622172, 0.2355370232572408, 0.2355370232572408, 0.2355370232572408, 0.4148020855085609, 0.4148020855085609, 0.4148020855085609, 0.47609313173401036, 0.47609313173401036, 0.47609313173401036, 0.5343860690848905, 0.5343860690848905, 0.5343860690848905, 0.08606766261267618, 0.08606766261267618, 0.08606766261267618, 0.07384008998724745, 0.07384008998724745, 0.07384008998724745, 0.05731658403093409, 0.05731658403093409, 0.05731658403093409, 0.1864398127353536, 0.1864398127353536, 0.1864398127353536, 0.2522622823718256, 0.2522622823718256, 0.2522622823718256, 0.2219281605217851, 0.2219281605217851, 0.2219281605217851, 0.24675734876587718, 0.24675734876587718, 0.24675734876587718, 0.28422697657660434, 0.28422697657660434, 0.28422697657660434, 0.19285446987115784, 0.19285446987115784, 0.19285446987115784, 0.2535563484588895, 0.2535563484588895, 0.2535563484588895, 0.16542937537019864, 0.16542937537019864, 0.16542937537019864, 0.13177240973192006, 0.13177240973192006, 0.13177240973192006, 0.2186666325572999, 0.2186666325572999, 0.2186666325572999, 0.2280332499933979, 0.2280332499933979, 0.2280332499933979, 0.22633589337301674, 0.22633589337301674, 0.22633589337301674, 0.18181320157776804, 0.18181320157776804, 0.18181320157776804, 0.17564371084747676, 0.17564371084747676, 0.17564371084747676, 0.1965169021585963, 0.1965169021585963, 0.1965169021585963, 0.7218961390601533, 0.7218961390601533, 0.7218961390601533, 0.1906535662842197, 0.1906535662842197, 0.1906535662842197, 0.1528929509784832, 0.1528929509784832, 0.1528929509784832, 0.45106342858558135, 0.45106342858558135, 0.45106342858558135, 0.1996635041682644, 0.1996635041682644, 0.1996635041682644, 0.46981473137622387, 0.46981473137622387, 0.46981473137622387, 0.19155638439528389, 0.19155638439528389, 0.19155638439528389, 0.23186334416635013, 0.23186334416635013, 0.23186334416635013, 0.19238491741213615, 0.19238491741213615, 0.19238491741213615, 0.08784421046769497, 0.08784421046769497, 0.08784421046769497, 0.09506561691979443, 0.09506561691979443, 0.09506561691979443, 0.10918892216801945, 0.10918892216801945, 0.10918892216801945]}, "mutation_prompt": null}
{"id": "14172b94-b71c-428b-be93-a7a9992f5dda", "solution": "import numpy as np\n\nclass PSO_SA_Optimizer_Enhanced:\n    def __init__(self, budget, dim, pop_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.w_max = 0.9\n        self.w_min = 0.4\n        self.c1_init = 2.0  # Initial cognitive parameter\n        self.c2_init = 1.0  # Initial social parameter\n        self.temp = 1.0\n\n    def __call__(self, func):\n        np.random.seed(42)\n        \n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.array([func(pos) for pos in positions])\n        global_best_position = personal_best_positions[np.argmin(personal_best_scores)]\n        global_best_score = np.min(personal_best_scores)\n        \n        eval_count = self.pop_size\n        \n        while eval_count < self.budget:\n            w = self.w_max - ((self.w_max - self.w_min) * (eval_count / self.budget))\n            c1 = self.c1_init * (1 - eval_count / self.budget)  # Dynamic cognitive\n            c2 = self.c2_init * (eval_count / self.budget)  # Dynamic social\n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                velocities[i] = (w * velocities[i]\n                                 + c1 * r1 * (personal_best_positions[i] - positions[i])\n                                 + c2 * r2 * (global_best_position - positions[i]))\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], self.lower_bound, self.upper_bound)\n\n                score = func(positions[i])\n                eval_count += 1\n                if eval_count >= self.budget:\n                    break\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = positions[i]\n\n                if np.random.rand() < np.exp(-(score - global_best_score) / self.temp):\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n                    if score < global_best_score:\n                        global_best_score = score\n                        global_best_position = positions[i]\n\n            self.temp *= 0.95\n\n        return global_best_position, global_best_score", "name": "PSO_SA_Optimizer_Enhanced", "description": "Modified PSO-SA incorporating dynamic cognitive and social parameters for improved convergence.", "configspace": "", "generation": 25, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "a6ced734-3ce2-4438-a3b1-cd500dc0bfe6", "metadata": {"aucs": [0.5552032341291239, 0.5552032341291239, 0.5552032341291239, 0.45079232168892436, 0.45079232168892436, 0.45079232168892436, 0.49261961612188865, 0.49261961612188865, 0.49261961612188865, 0.1534035740593066, 0.1534035740593066, 0.1534035740593066, 0.03196945674084872, 0.03196945674084872, 0.03196945674084872, 0.028423438180651517, 0.028423438180651517, 0.028423438180651517, 0.09722680421452112, 0.09722680421452112, 0.09722680421452112, 0.08441730956529958, 0.08441730956529958, 0.08441730956529958, 0.11456156249563842, 0.11456156249563842, 0.11456156249563842, 0.09107142533975343, 0.09107142533975343, 0.09107142533975343, 0.08211323659876146, 0.08211323659876146, 0.08211323659876146, 0.10206430761582197, 0.10206430761582197, 0.10206430761582197, 0.5924351363112623, 0.5924351363112623, 0.5924351363112623, 0.9047980793814148, 0.9047980793814148, 0.9047980793814148, 0.8495027452395066, 0.8495027452395066, 0.8495027452395066, 0.26652443155696826, 0.26652443155696826, 0.26652443155696826, 0.3101120552834319, 0.3101120552834319, 0.3101120552834319, 0.23692796700254026, 0.23692796700254026, 0.23692796700254026, 0.1438816659635016, 0.1438816659635016, 0.1438816659635016, 0.19167430865111124, 0.19167430865111124, 0.19167430865111124, 0.2253607926875516, 0.2253607926875516, 0.2253607926875516, 0.13534101194193848, 0.13534101194193848, 0.13534101194193848, 0.08784400890223076, 0.08784400890223076, 0.08784400890223076, 0.12085401156217468, 0.12085401156217468, 0.12085401156217468, 0.15354298018844237, 0.15354298018844237, 0.15354298018844237, 0.1335247507026852, 0.1335247507026852, 0.1335247507026852, 0.12652675422387816, 0.12652675422387816, 0.12652675422387816, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.015638664098327704, 0.015638664098327704, 0.015638664098327704, 0.06348601714929225, 0.06348601714929225, 0.06348601714929225, 0.5128201195876666, 0.5128201195876666, 0.5128201195876666, 0.3244201399797112, 0.3244201399797112, 0.3244201399797112, 0.45247627263357393, 0.45247627263357393, 0.45247627263357393, 0.04451653784519172, 0.04451653784519172, 0.04451653784519172, 0.10244570601375602, 0.10244570601375602, 0.10244570601375602, 0.18293966820632035, 0.18293966820632035, 0.18293966820632035, 0.17744947312927328, 0.17744947312927328, 0.17744947312927328, 0.21510233156622172, 0.21510233156622172, 0.21510233156622172, 0.2355370232572408, 0.2355370232572408, 0.2355370232572408, 0.4148020855085609, 0.4148020855085609, 0.4148020855085609, 0.47609313173401036, 0.47609313173401036, 0.47609313173401036, 0.5343860690848905, 0.5343860690848905, 0.5343860690848905, 0.08606766261267618, 0.08606766261267618, 0.08606766261267618, 0.07384008998724745, 0.07384008998724745, 0.07384008998724745, 0.05731658403093409, 0.05731658403093409, 0.05731658403093409, 0.1864398127353536, 0.1864398127353536, 0.1864398127353536, 0.2522622823718256, 0.2522622823718256, 0.2522622823718256, 0.2219281605217851, 0.2219281605217851, 0.2219281605217851, 0.24675734876587718, 0.24675734876587718, 0.24675734876587718, 0.28422697657660434, 0.28422697657660434, 0.28422697657660434, 0.19285446987115784, 0.19285446987115784, 0.19285446987115784, 0.2535563484588895, 0.2535563484588895, 0.2535563484588895, 0.16542937537019864, 0.16542937537019864, 0.16542937537019864, 0.13177240973192006, 0.13177240973192006, 0.13177240973192006, 0.2186666325572999, 0.2186666325572999, 0.2186666325572999, 0.2280332499933979, 0.2280332499933979, 0.2280332499933979, 0.22633589337301674, 0.22633589337301674, 0.22633589337301674, 0.18181320157776804, 0.18181320157776804, 0.18181320157776804, 0.17564371084747676, 0.17564371084747676, 0.17564371084747676, 0.1965169021585963, 0.1965169021585963, 0.1965169021585963, 0.7218961390601533, 0.7218961390601533, 0.7218961390601533, 0.1906535662842197, 0.1906535662842197, 0.1906535662842197, 0.1528929509784832, 0.1528929509784832, 0.1528929509784832, 0.45106342858558135, 0.45106342858558135, 0.45106342858558135, 0.1996635041682644, 0.1996635041682644, 0.1996635041682644, 0.46981473137622387, 0.46981473137622387, 0.46981473137622387, 0.19155638439528389, 0.19155638439528389, 0.19155638439528389, 0.23186334416635013, 0.23186334416635013, 0.23186334416635013, 0.19238491741213615, 0.19238491741213615, 0.19238491741213615, 0.08784421046769497, 0.08784421046769497, 0.08784421046769497, 0.09506561691979443, 0.09506561691979443, 0.09506561691979443, 0.10918892216801945, 0.10918892216801945, 0.10918892216801945]}, "mutation_prompt": null}
{"id": "e68a0fee-e3d5-4504-a5c9-7b5046210e42", "solution": "import numpy as np\n\nclass PSO_SA_Optimizer_Enhanced:\n    def __init__(self, budget, dim, pop_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.w_max = 0.9\n        self.w_min = 0.4\n        self.c1_init = 2.0  # Initial cognitive parameter\n        self.c2_init = 1.0  # Initial social parameter\n        self.temp = 1.0\n\n    def __call__(self, func):\n        np.random.seed(42)\n        \n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.array([func(pos) for pos in positions])\n        global_best_position = personal_best_positions[np.argmin(personal_best_scores)]\n        global_best_score = np.min(personal_best_scores)\n        \n        eval_count = self.pop_size\n        \n        while eval_count < self.budget:\n            w = self.w_max - ((self.w_max - self.w_min) * (eval_count / self.budget))\n            c1 = self.c1_init * (1 - eval_count / self.budget)  # Dynamic cognitive\n            c2 = self.c2_init * (eval_count / self.budget)  # Dynamic social\n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                velocities[i] = (w * velocities[i]\n                                 + c1 * r1 * (personal_best_positions[i] - positions[i])\n                                 + c2 * r2 * (global_best_position - positions[i]))\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], self.lower_bound, self.upper_bound)\n\n                score = func(positions[i])\n                eval_count += 1\n                if eval_count >= self.budget:\n                    break\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = positions[i]\n\n                if np.random.rand() < np.exp(-(score - global_best_score) / self.temp):\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n                    if score < global_best_score:\n                        global_best_score = score\n                        global_best_position = positions[i]\n\n            self.temp *= 0.95\n\n        return global_best_position, global_best_score", "name": "PSO_SA_Optimizer_Enhanced", "description": "Modified PSO-SA incorporating dynamic cognitive and social parameters for improved convergence.", "configspace": "", "generation": 25, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "a6ced734-3ce2-4438-a3b1-cd500dc0bfe6", "metadata": {"aucs": [0.5552032341291239, 0.5552032341291239, 0.5552032341291239, 0.45079232168892436, 0.45079232168892436, 0.45079232168892436, 0.49261961612188865, 0.49261961612188865, 0.49261961612188865, 0.1534035740593066, 0.1534035740593066, 0.1534035740593066, 0.03196945674084872, 0.03196945674084872, 0.03196945674084872, 0.028423438180651517, 0.028423438180651517, 0.028423438180651517, 0.09722680421452112, 0.09722680421452112, 0.09722680421452112, 0.08441730956529958, 0.08441730956529958, 0.08441730956529958, 0.11456156249563842, 0.11456156249563842, 0.11456156249563842, 0.09107142533975343, 0.09107142533975343, 0.09107142533975343, 0.08211323659876146, 0.08211323659876146, 0.08211323659876146, 0.10206430761582197, 0.10206430761582197, 0.10206430761582197, 0.5924351363112623, 0.5924351363112623, 0.5924351363112623, 0.9047980793814148, 0.9047980793814148, 0.9047980793814148, 0.8495027452395066, 0.8495027452395066, 0.8495027452395066, 0.26652443155696826, 0.26652443155696826, 0.26652443155696826, 0.3101120552834319, 0.3101120552834319, 0.3101120552834319, 0.23692796700254026, 0.23692796700254026, 0.23692796700254026, 0.1438816659635016, 0.1438816659635016, 0.1438816659635016, 0.19167430865111124, 0.19167430865111124, 0.19167430865111124, 0.2253607926875516, 0.2253607926875516, 0.2253607926875516, 0.13534101194193848, 0.13534101194193848, 0.13534101194193848, 0.08784400890223076, 0.08784400890223076, 0.08784400890223076, 0.12085401156217468, 0.12085401156217468, 0.12085401156217468, 0.15354298018844237, 0.15354298018844237, 0.15354298018844237, 0.1335247507026852, 0.1335247507026852, 0.1335247507026852, 0.12652675422387816, 0.12652675422387816, 0.12652675422387816, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.015638664098327704, 0.015638664098327704, 0.015638664098327704, 0.06348601714929225, 0.06348601714929225, 0.06348601714929225, 0.5128201195876666, 0.5128201195876666, 0.5128201195876666, 0.3244201399797112, 0.3244201399797112, 0.3244201399797112, 0.45247627263357393, 0.45247627263357393, 0.45247627263357393, 0.04451653784519172, 0.04451653784519172, 0.04451653784519172, 0.10244570601375602, 0.10244570601375602, 0.10244570601375602, 0.18293966820632035, 0.18293966820632035, 0.18293966820632035, 0.17744947312927328, 0.17744947312927328, 0.17744947312927328, 0.21510233156622172, 0.21510233156622172, 0.21510233156622172, 0.2355370232572408, 0.2355370232572408, 0.2355370232572408, 0.4148020855085609, 0.4148020855085609, 0.4148020855085609, 0.47609313173401036, 0.47609313173401036, 0.47609313173401036, 0.5343860690848905, 0.5343860690848905, 0.5343860690848905, 0.08606766261267618, 0.08606766261267618, 0.08606766261267618, 0.07384008998724745, 0.07384008998724745, 0.07384008998724745, 0.05731658403093409, 0.05731658403093409, 0.05731658403093409, 0.1864398127353536, 0.1864398127353536, 0.1864398127353536, 0.2522622823718256, 0.2522622823718256, 0.2522622823718256, 0.2219281605217851, 0.2219281605217851, 0.2219281605217851, 0.24675734876587718, 0.24675734876587718, 0.24675734876587718, 0.28422697657660434, 0.28422697657660434, 0.28422697657660434, 0.19285446987115784, 0.19285446987115784, 0.19285446987115784, 0.2535563484588895, 0.2535563484588895, 0.2535563484588895, 0.16542937537019864, 0.16542937537019864, 0.16542937537019864, 0.13177240973192006, 0.13177240973192006, 0.13177240973192006, 0.2186666325572999, 0.2186666325572999, 0.2186666325572999, 0.2280332499933979, 0.2280332499933979, 0.2280332499933979, 0.22633589337301674, 0.22633589337301674, 0.22633589337301674, 0.18181320157776804, 0.18181320157776804, 0.18181320157776804, 0.17564371084747676, 0.17564371084747676, 0.17564371084747676, 0.1965169021585963, 0.1965169021585963, 0.1965169021585963, 0.7218961390601533, 0.7218961390601533, 0.7218961390601533, 0.1906535662842197, 0.1906535662842197, 0.1906535662842197, 0.1528929509784832, 0.1528929509784832, 0.1528929509784832, 0.45106342858558135, 0.45106342858558135, 0.45106342858558135, 0.1996635041682644, 0.1996635041682644, 0.1996635041682644, 0.46981473137622387, 0.46981473137622387, 0.46981473137622387, 0.19155638439528389, 0.19155638439528389, 0.19155638439528389, 0.23186334416635013, 0.23186334416635013, 0.23186334416635013, 0.19238491741213615, 0.19238491741213615, 0.19238491741213615, 0.08784421046769497, 0.08784421046769497, 0.08784421046769497, 0.09506561691979443, 0.09506561691979443, 0.09506561691979443, 0.10918892216801945, 0.10918892216801945, 0.10918892216801945]}, "mutation_prompt": null}
{"id": "5fc12c54-3b6f-43db-98ef-934db14b6256", "solution": "import numpy as np\n\nclass PSO_SA_Optimizer_Enhanced:\n    def __init__(self, budget, dim, pop_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.w_max = 0.9\n        self.w_min = 0.4\n        self.c1_init = 2.0  # Initial cognitive parameter\n        self.c2_init = 1.0  # Initial social parameter\n        self.temp = 1.0\n\n    def __call__(self, func):\n        np.random.seed(42)\n        \n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.array([func(pos) for pos in positions])\n        global_best_position = personal_best_positions[np.argmin(personal_best_scores)]\n        global_best_score = np.min(personal_best_scores)\n        \n        eval_count = self.pop_size\n        \n        while eval_count < self.budget:\n            w = self.w_max - ((self.w_max - self.w_min) * (eval_count / self.budget))\n            c1 = self.c1_init * (1 - eval_count / self.budget)  # Dynamic cognitive\n            c2 = self.c2_init * (eval_count / self.budget)  # Dynamic social\n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                velocities[i] = (w * velocities[i]\n                                 + c1 * r1 * (personal_best_positions[i] - positions[i])\n                                 + c2 * r2 * (global_best_position - positions[i]))\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], self.lower_bound, self.upper_bound)\n\n                score = func(positions[i])\n                eval_count += 1\n                if eval_count >= self.budget:\n                    break\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = positions[i]\n\n                if np.random.rand() < np.exp(-(score - global_best_score) / self.temp):\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n                    if score < global_best_score:\n                        global_best_score = score\n                        global_best_position = positions[i]\n\n            self.temp *= 0.95\n\n        return global_best_position, global_best_score", "name": "PSO_SA_Optimizer_Enhanced", "description": "Modified PSO-SA incorporating dynamic cognitive and social parameters for improved convergence.", "configspace": "", "generation": 25, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "a6ced734-3ce2-4438-a3b1-cd500dc0bfe6", "metadata": {"aucs": [0.5552032341291239, 0.5552032341291239, 0.5552032341291239, 0.45079232168892436, 0.45079232168892436, 0.45079232168892436, 0.49261961612188865, 0.49261961612188865, 0.49261961612188865, 0.1534035740593066, 0.1534035740593066, 0.1534035740593066, 0.03196945674084872, 0.03196945674084872, 0.03196945674084872, 0.028423438180651517, 0.028423438180651517, 0.028423438180651517, 0.09722680421452112, 0.09722680421452112, 0.09722680421452112, 0.08441730956529958, 0.08441730956529958, 0.08441730956529958, 0.11456156249563842, 0.11456156249563842, 0.11456156249563842, 0.09107142533975343, 0.09107142533975343, 0.09107142533975343, 0.08211323659876146, 0.08211323659876146, 0.08211323659876146, 0.10206430761582197, 0.10206430761582197, 0.10206430761582197, 0.5924351363112623, 0.5924351363112623, 0.5924351363112623, 0.9047980793814148, 0.9047980793814148, 0.9047980793814148, 0.8495027452395066, 0.8495027452395066, 0.8495027452395066, 0.26652443155696826, 0.26652443155696826, 0.26652443155696826, 0.3101120552834319, 0.3101120552834319, 0.3101120552834319, 0.23692796700254026, 0.23692796700254026, 0.23692796700254026, 0.1438816659635016, 0.1438816659635016, 0.1438816659635016, 0.19167430865111124, 0.19167430865111124, 0.19167430865111124, 0.2253607926875516, 0.2253607926875516, 0.2253607926875516, 0.13534101194193848, 0.13534101194193848, 0.13534101194193848, 0.08784400890223076, 0.08784400890223076, 0.08784400890223076, 0.12085401156217468, 0.12085401156217468, 0.12085401156217468, 0.15354298018844237, 0.15354298018844237, 0.15354298018844237, 0.1335247507026852, 0.1335247507026852, 0.1335247507026852, 0.12652675422387816, 0.12652675422387816, 0.12652675422387816, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.015638664098327704, 0.015638664098327704, 0.015638664098327704, 0.06348601714929225, 0.06348601714929225, 0.06348601714929225, 0.5128201195876666, 0.5128201195876666, 0.5128201195876666, 0.3244201399797112, 0.3244201399797112, 0.3244201399797112, 0.45247627263357393, 0.45247627263357393, 0.45247627263357393, 0.04451653784519172, 0.04451653784519172, 0.04451653784519172, 0.10244570601375602, 0.10244570601375602, 0.10244570601375602, 0.18293966820632035, 0.18293966820632035, 0.18293966820632035, 0.17744947312927328, 0.17744947312927328, 0.17744947312927328, 0.21510233156622172, 0.21510233156622172, 0.21510233156622172, 0.2355370232572408, 0.2355370232572408, 0.2355370232572408, 0.4148020855085609, 0.4148020855085609, 0.4148020855085609, 0.47609313173401036, 0.47609313173401036, 0.47609313173401036, 0.5343860690848905, 0.5343860690848905, 0.5343860690848905, 0.08606766261267618, 0.08606766261267618, 0.08606766261267618, 0.07384008998724745, 0.07384008998724745, 0.07384008998724745, 0.05731658403093409, 0.05731658403093409, 0.05731658403093409, 0.1864398127353536, 0.1864398127353536, 0.1864398127353536, 0.2522622823718256, 0.2522622823718256, 0.2522622823718256, 0.2219281605217851, 0.2219281605217851, 0.2219281605217851, 0.24675734876587718, 0.24675734876587718, 0.24675734876587718, 0.28422697657660434, 0.28422697657660434, 0.28422697657660434, 0.19285446987115784, 0.19285446987115784, 0.19285446987115784, 0.2535563484588895, 0.2535563484588895, 0.2535563484588895, 0.16542937537019864, 0.16542937537019864, 0.16542937537019864, 0.13177240973192006, 0.13177240973192006, 0.13177240973192006, 0.2186666325572999, 0.2186666325572999, 0.2186666325572999, 0.2280332499933979, 0.2280332499933979, 0.2280332499933979, 0.22633589337301674, 0.22633589337301674, 0.22633589337301674, 0.18181320157776804, 0.18181320157776804, 0.18181320157776804, 0.17564371084747676, 0.17564371084747676, 0.17564371084747676, 0.1965169021585963, 0.1965169021585963, 0.1965169021585963, 0.7218961390601533, 0.7218961390601533, 0.7218961390601533, 0.1906535662842197, 0.1906535662842197, 0.1906535662842197, 0.1528929509784832, 0.1528929509784832, 0.1528929509784832, 0.45106342858558135, 0.45106342858558135, 0.45106342858558135, 0.1996635041682644, 0.1996635041682644, 0.1996635041682644, 0.46981473137622387, 0.46981473137622387, 0.46981473137622387, 0.19155638439528389, 0.19155638439528389, 0.19155638439528389, 0.23186334416635013, 0.23186334416635013, 0.23186334416635013, 0.19238491741213615, 0.19238491741213615, 0.19238491741213615, 0.08784421046769497, 0.08784421046769497, 0.08784421046769497, 0.09506561691979443, 0.09506561691979443, 0.09506561691979443, 0.10918892216801945, 0.10918892216801945, 0.10918892216801945]}, "mutation_prompt": null}
{"id": "8275fdc9-dd69-4f35-8938-1476f28cfb9b", "solution": "import numpy as np\n\nclass PSO_SA_Optimizer_Enhanced_V2:\n    def __init__(self, budget, dim, pop_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.w_max = 0.9\n        self.w_min = 0.4\n        self.c1_init = 2.0\n        self.c2_init = 1.0\n        self.temp_init = 1.0\n\n    def __call__(self, func):\n        np.random.seed(42)\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.array([func(pos) for pos in positions])\n        global_best_position = personal_best_positions[np.argmin(personal_best_scores)]\n        global_best_score = np.min(personal_best_scores)\n        \n        eval_count = self.pop_size\n        temperature = self.temp_init\n        \n        while eval_count < self.budget:\n            w = self.w_max - ((self.w_max - self.w_min) * (eval_count / self.budget))\n            c1 = self.c1_init * (1 - eval_count / self.budget)\n            c2 = self.c2_init * (eval_count / self.budget)\n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                velocities[i] = (w * velocities[i]\n                                 + c1 * r1 * (personal_best_positions[i] - positions[i])\n                                 + c2 * r2 * (global_best_position - positions[i]))\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], self.lower_bound, self.upper_bound)\n\n                score = func(positions[i])\n                eval_count += 1\n                if eval_count >= self.budget:\n                    break\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = positions[i]\n\n                if np.random.rand() < np.exp(-(score - global_best_score) / temperature):\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n                    if score < global_best_score:\n                        global_best_score = score\n                        global_best_position = positions[i]\n\n            temperature *= 0.9  # Enhanced annealing factor for faster convergence\n\n        return global_best_position, global_best_score", "name": "PSO_SA_Optimizer_Enhanced_V2", "description": "Enhanced PSO-SA with adaptive temperature and improved velocity update for better exploration-exploitation balance.", "configspace": "", "generation": 41, "fitness": 0.2547742446077234, "feedback": "The algorithm PSO_SA_Optimizer_Enhanced_V2 got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.25 with standard deviation 0.21.", "error": "", "parent_id": "a6ced734-3ce2-4438-a3b1-cd500dc0bfe6", "metadata": {"aucs": [0.7648824746567023, 0.7648824746567023, 0.7648824746567023, 0.5989209008757856, 0.5989209008757856, 0.5989209008757856, 0.6742174754797561, 0.6742174754797561, 0.6742174754797561, 0.15340181758075755, 0.15340181758075755, 0.15340181758075755, 0.03196945575794474, 0.03196945575794474, 0.03196945575794474, 0.028430204481377896, 0.028430204481377896, 0.028430204481377896, 0.09721927887848547, 0.09721927887848547, 0.09721927887848547, 0.10055554133996336, 0.10055554133996336, 0.10055554133996336, 0.11442808809856964, 0.11442808809856964, 0.11442808809856964, 0.09106273135940757, 0.09106273135940757, 0.09106273135940757, 0.08133700413676104, 0.08133700413676104, 0.08133700413676104, 0.10219557668092238, 0.10219557668092238, 0.10219557668092238, 0.5924351363112623, 0.5924351363112623, 0.5924351363112623, 0.9047980793814148, 0.9047980793814148, 0.9047980793814148, 0.8502442090048321, 0.8502442090048321, 0.8502442090048321, 0.2454031538157342, 0.2454031538157342, 0.2454031538157342, 0.47754369891190485, 0.47754369891190485, 0.47754369891190485, 0.22604147488547977, 0.22604147488547977, 0.22604147488547977, 0.22058670269023306, 0.22058670269023306, 0.22058670269023306, 0.1872551853639175, 0.1872551853639175, 0.1872551853639175, 0.22519173578017293, 0.22519173578017293, 0.22519173578017293, 0.13233694616142366, 0.13233694616142366, 0.13233694616142366, 0.08781796531803687, 0.08781796531803687, 0.08781796531803687, 0.12131359600021774, 0.12131359600021774, 0.12131359600021774, 0.15118783046577833, 0.15118783046577833, 0.15118783046577833, 0.1315876710508912, 0.1315876710508912, 0.1315876710508912, 0.1256842034853637, 0.1256842034853637, 0.1256842034853637, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.015613376015341962, 0.015613376015341962, 0.015613376015341962, 0.06348628008509782, 0.06348628008509782, 0.06348628008509782, 0.2167579326396849, 0.2167579326396849, 0.2167579326396849, 0.2349078877534202, 0.2349078877534202, 0.2349078877534202, 0.5253652982472471, 0.5253652982472471, 0.5253652982472471, 0.04451652761113345, 0.04451652761113345, 0.04451652761113345, 0.1024304225623921, 0.1024304225623921, 0.1024304225623921, 0.18289120643451884, 0.18289120643451884, 0.18289120643451884, 0.17757247285435862, 0.17757247285435862, 0.17757247285435862, 0.18569096270455365, 0.18569096270455365, 0.18569096270455365, 0.270899573595341, 0.270899573595341, 0.270899573595341, 0.5796476966545081, 0.5796476966545081, 0.5796476966545081, 0.5087363546109619, 0.5087363546109619, 0.5087363546109619, 0.5459100039062216, 0.5459100039062216, 0.5459100039062216, 0.08535240446130943, 0.08535240446130943, 0.08535240446130943, 0.07356035276132311, 0.07356035276132311, 0.07356035276132311, 0.05727254799191683, 0.05727254799191683, 0.05727254799191683, 0.18616749733741433, 0.18616749733741433, 0.18616749733741433, 0.2522625076609373, 0.2522625076609373, 0.2522625076609373, 0.2333528420209755, 0.2333528420209755, 0.2333528420209755, 0.2711349095509601, 0.2711349095509601, 0.2711349095509601, 0.4119320601537232, 0.4119320601537232, 0.4119320601537232, 0.2011107881828319, 0.2011107881828319, 0.2011107881828319, 0.25373640507951833, 0.25373640507951833, 0.25373640507951833, 0.14102066452975304, 0.14102066452975304, 0.14102066452975304, 0.13177254753958223, 0.13177254753958223, 0.13177254753958223, 0.21871522686147926, 0.21871522686147926, 0.21871522686147926, 0.22800241336751603, 0.22800241336751603, 0.22800241336751603, 0.22642111368136242, 0.22642111368136242, 0.22642111368136242, 0.18939421987477667, 0.18939421987477667, 0.18939421987477667, 0.17532536458645043, 0.17532536458645043, 0.17532536458645043, 0.19729198941379522, 0.19729198941379522, 0.19729198941379522, 0.7144551409791454, 0.7144551409791454, 0.7144551409791454, 0.18883939777303727, 0.18883939777303727, 0.18883939777303727, 0.7419333238254875, 0.7419333238254875, 0.7419333238254875, 0.382646355783097, 0.382646355783097, 0.382646355783097, 0.19933351641918018, 0.19933351641918018, 0.19933351641918018, 0.4826961049052948, 0.4826961049052948, 0.4826961049052948, 0.1915559962106098, 0.1915559962106098, 0.1915559962106098, 0.23517183855747403, 0.23517183855747403, 0.23517183855747403, 0.19238856119414083, 0.19238856119414083, 0.19238856119414083, 0.0877198066400795, 0.0877198066400795, 0.0877198066400795, 0.1116867896885797, 0.1116867896885797, 0.1116867896885797, 0.10891879109645541, 0.10891879109645541, 0.10891879109645541]}, "mutation_prompt": null}
{"id": "0ed3b0b0-26d6-4f77-89d5-b6f5347f6375", "solution": "import numpy as np\n\nclass PSO_SA_Optimizer_Enhanced_V3:\n    def __init__(self, budget, dim, pop_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.w_max = 0.8\n        self.w_min = 0.3\n        self.c1_init = 1.5\n        self.c2_init = 1.5\n        self.temp_init = 1.0\n\n    def __call__(self, func):\n        np.random.seed(42)\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        velocities = np.random.uniform(-0.5, 0.5, (self.pop_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.array([func(pos) for pos in positions])\n        global_best_position = personal_best_positions[np.argmin(personal_best_scores)]\n        global_best_score = np.min(personal_best_scores)\n        \n        eval_count = self.pop_size\n        temperature = self.temp_init\n\n        while eval_count < self.budget:\n            w = self.w_max - ((self.w_max - self.w_min) * (eval_count / self.budget))\n            c1 = self.c1_init * (1 - (eval_count / self.budget)**1.5)\n            c2 = self.c2_init * ((eval_count / self.budget)**1.5)\n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                velocities[i] = (w * velocities[i]\n                                 + c1 * r1 * (personal_best_positions[i] - positions[i])\n                                 + c2 * r2 * (global_best_position - positions[i]))\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], self.lower_bound, self.upper_bound)\n\n                score = func(positions[i])\n                eval_count += 1\n                if eval_count >= self.budget:\n                    break\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = positions[i]\n\n                if np.random.rand() < np.exp(-(score - global_best_score) / max(temperature, 1e-8)):\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n                    if score < global_best_score:\n                        global_best_score = score\n                        global_best_position = positions[i]\n\n            temperature *= 0.95  # Refined annealing for improved convergence\n\n        return global_best_position, global_best_score", "name": "PSO_SA_Optimizer_Enhanced_V3", "description": "Adaptive PSO-SA with dynamic parameter scaling and annealing schedule refinement for enhanced convergence.", "configspace": "", "generation": 42, "fitness": 0.13176181923073263, "feedback": "The algorithm PSO_SA_Optimizer_Enhanced_V3 got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.13 with standard deviation 0.11.", "error": "", "parent_id": "8275fdc9-dd69-4f35-8938-1476f28cfb9b", "metadata": {"aucs": [0.36505672294353053, 0.36505672294353053, 0.36505672294353053, 0.14593920776057423, 0.14593920776057423, 0.14593920776057423, 0.29800878163121114, 0.29800878163121114, 0.29800878163121114, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.13526037541167424, 0.13526037541167424, 0.13526037541167424, 0.05050638264953888, 0.05050638264953888, 0.05050638264953888, 0.04896807860579788, 0.04896807860579788, 0.04896807860579788, 0.04176025467342592, 0.04176025467342592, 0.04176025467342592, 0.05181521957467228, 0.05181521957467228, 0.05181521957467228, 0.05832110589758599, 0.05832110589758599, 0.05832110589758599, 0.05800831604952761, 0.05800831604952761, 0.05800831604952761, 0.2546035920083396, 0.2546035920083396, 0.2546035920083396, 0.05663907540552371, 0.05663907540552371, 0.05663907540552371, 0.07028333999818315, 0.07028333999818315, 0.07028333999818315, 0.045837489882244564, 0.045837489882244564, 0.045837489882244564, 0.12182559144993499, 0.12182559144993499, 0.12182559144993499, 0.12511830639552757, 0.12511830639552757, 0.12511830639552757, 0.1743318586427095, 0.1743318586427095, 0.1743318586427095, 0.09897664281792573, 0.09897664281792573, 0.09897664281792573, 0.003034417887592622, 0.003034417887592622, 0.003034417887592622, 0.0884963938150084, 0.0884963938150084, 0.0884963938150084, 0.10931428839028012, 0.10931428839028012, 0.10931428839028012, 0.13470155009150442, 0.13470155009150442, 0.13470155009150442, 0.12326738866267495, 0.12326738866267495, 0.12326738866267495, 0.12087924729761601, 0.12087924729761601, 0.12087924729761601, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.047824453318694005, 0.047824453318694005, 0.047824453318694005, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.17025270779076163, 0.17025270779076163, 0.17025270779076163, 0.07074988504120827, 0.07074988504120827, 0.07074988504120827, 0.19914967471885825, 0.19914967471885825, 0.19914967471885825, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.15489705787120944, 0.15489705787120944, 0.15489705787120944, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.07141225058657341, 0.07141225058657341, 0.07141225058657341, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.3418054765680033, 0.3418054765680033, 0.3418054765680033, 0.25101479223394285, 0.25101479223394285, 0.25101479223394285, 0.22445303062101252, 0.22445303062101252, 0.22445303062101252, 0.02760570099674764, 0.02760570099674764, 0.02760570099674764, 0.04931670131249044, 0.04931670131249044, 0.04931670131249044, 0.06405241662102512, 0.06405241662102512, 0.06405241662102512, 0.10338946320854392, 0.10338946320854392, 0.10338946320854392, 0.1813195626081472, 0.1813195626081472, 0.1813195626081472, 0.11936570829043303, 0.11936570829043303, 0.11936570829043303, 0.19872486716956628, 0.19872486716956628, 0.19872486716956628, 0.15999012093944798, 0.15999012093944798, 0.15999012093944798, 0.14209218601878582, 0.14209218601878582, 0.14209218601878582, 0.1491155017414003, 0.1491155017414003, 0.1491155017414003, 0.11482584689896269, 0.11482584689896269, 0.11482584689896269, 0.12385908491643183, 0.12385908491643183, 0.12385908491643183, 0.23023513536572904, 0.23023513536572904, 0.23023513536572904, 0.23263437528414765, 0.23263437528414765, 0.23263437528414765, 0.2160913634326851, 0.2160913634326851, 0.2160913634326851, 0.18918156656767615, 0.18918156656767615, 0.18918156656767615, 0.15146562955204956, 0.15146562955204956, 0.15146562955204956, 0.16076012022475605, 0.16076012022475605, 0.16076012022475605, 0.18389172592074532, 0.18389172592074532, 0.18389172592074532, 0.6524015142193436, 0.6524015142193436, 0.6524015142193436, 0.1452887349911629, 0.1452887349911629, 0.1452887349911629, 0.2839459553355217, 0.2839459553355217, 0.2839459553355217, 0.1793470031206471, 0.1793470031206471, 0.1793470031206471, 0.2825953233690096, 0.2825953233690096, 0.2825953233690096, 0.18191306584509925, 0.18191306584509925, 0.18191306584509925, 0.1958273894177215, 0.1958273894177215, 0.1958273894177215, 0.19606284972161359, 0.19606284972161359, 0.19606284972161359, 0.10525905516883849, 0.10525905516883849, 0.10525905516883849, 0.07684863115080243, 0.07684863115080243, 0.07684863115080243, 0.07603142851034894, 0.07603142851034894, 0.07603142851034894]}, "mutation_prompt": null}
{"id": "28759ade-810e-453b-8b17-aab2343b81aa", "solution": "import numpy as np\n\nclass PSO_SA_Optimizer_Enhanced_V2:\n    def __init__(self, budget, dim, pop_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.w_max = 0.9\n        self.w_min = 0.4\n        self.c1_init = 2.0\n        self.c2_init = 1.0\n        self.temp_init = 1.0\n\n    def __call__(self, func):\n        np.random.seed(42)\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.array([func(pos) for pos in positions])\n        global_best_position = personal_best_positions[np.argmin(personal_best_scores)]\n        global_best_score = np.min(personal_best_scores)\n        \n        eval_count = self.pop_size\n        temperature = self.temp_init\n        \n        while eval_count < self.budget:\n            w = self.w_max - ((self.w_max - self.w_min) * (eval_count / self.budget))\n            c1 = self.c1_init * (1 - eval_count / self.budget)\n            c2 = self.c2_init * (eval_count / self.budget)\n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                velocities[i] = (w * velocities[i]\n                                 + c1 * r1 * (personal_best_positions[i] - positions[i])\n                                 + c2 * r2 * (global_best_position - positions[i]))\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], self.lower_bound, self.upper_bound)\n\n                score = func(positions[i])\n                eval_count += 1\n                if eval_count >= self.budget:\n                    break\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = positions[i]\n\n                if np.random.rand() < np.exp(-(score - global_best_score) / temperature):\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n                    if score < global_best_score:\n                        global_best_score = score\n                        global_best_position = positions[i]\n\n            temperature *= 0.9  # Enhanced annealing factor for faster convergence\n\n        return global_best_position, global_best_score", "name": "PSO_SA_Optimizer_Enhanced_V2", "description": "Enhanced PSO-SA with adaptive temperature and improved velocity update for better exploration-exploitation balance.", "configspace": "", "generation": 42, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "8275fdc9-dd69-4f35-8938-1476f28cfb9b", "metadata": {"aucs": [0.7648824746567023, 0.7648824746567023, 0.7648824746567023, 0.5989209008757856, 0.5989209008757856, 0.5989209008757856, 0.6742174754797561, 0.6742174754797561, 0.6742174754797561, 0.15340181758075755, 0.15340181758075755, 0.15340181758075755, 0.03196945575794474, 0.03196945575794474, 0.03196945575794474, 0.028430204481377896, 0.028430204481377896, 0.028430204481377896, 0.09721927887848547, 0.09721927887848547, 0.09721927887848547, 0.10055554133996336, 0.10055554133996336, 0.10055554133996336, 0.11442808809856964, 0.11442808809856964, 0.11442808809856964, 0.09106273135940757, 0.09106273135940757, 0.09106273135940757, 0.08133700413676104, 0.08133700413676104, 0.08133700413676104, 0.10219557668092238, 0.10219557668092238, 0.10219557668092238, 0.5924351363112623, 0.5924351363112623, 0.5924351363112623, 0.9047980793814148, 0.9047980793814148, 0.9047980793814148, 0.8502442090048321, 0.8502442090048321, 0.8502442090048321, 0.2454031538157342, 0.2454031538157342, 0.2454031538157342, 0.47754369891190485, 0.47754369891190485, 0.47754369891190485, 0.22604147488547977, 0.22604147488547977, 0.22604147488547977, 0.22058670269023306, 0.22058670269023306, 0.22058670269023306, 0.1872551853639175, 0.1872551853639175, 0.1872551853639175, 0.22519173578017293, 0.22519173578017293, 0.22519173578017293, 0.13233694616142366, 0.13233694616142366, 0.13233694616142366, 0.08781796531803687, 0.08781796531803687, 0.08781796531803687, 0.12131359600021774, 0.12131359600021774, 0.12131359600021774, 0.15118783046577833, 0.15118783046577833, 0.15118783046577833, 0.1315876710508912, 0.1315876710508912, 0.1315876710508912, 0.1256842034853637, 0.1256842034853637, 0.1256842034853637, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.015613376015341962, 0.015613376015341962, 0.015613376015341962, 0.06348628008509782, 0.06348628008509782, 0.06348628008509782, 0.2167579326396849, 0.2167579326396849, 0.2167579326396849, 0.2349078877534202, 0.2349078877534202, 0.2349078877534202, 0.5253652982472471, 0.5253652982472471, 0.5253652982472471, 0.04451652761113345, 0.04451652761113345, 0.04451652761113345, 0.1024304225623921, 0.1024304225623921, 0.1024304225623921, 0.18289120643451884, 0.18289120643451884, 0.18289120643451884, 0.17757247285435862, 0.17757247285435862, 0.17757247285435862, 0.18569096270455365, 0.18569096270455365, 0.18569096270455365, 0.270899573595341, 0.270899573595341, 0.270899573595341, 0.5796476966545081, 0.5796476966545081, 0.5796476966545081, 0.5087363546109619, 0.5087363546109619, 0.5087363546109619, 0.5459100039062216, 0.5459100039062216, 0.5459100039062216, 0.08535240446130943, 0.08535240446130943, 0.08535240446130943, 0.07356035276132311, 0.07356035276132311, 0.07356035276132311, 0.05727254799191683, 0.05727254799191683, 0.05727254799191683, 0.18616749733741433, 0.18616749733741433, 0.18616749733741433, 0.2522625076609373, 0.2522625076609373, 0.2522625076609373, 0.2333528420209755, 0.2333528420209755, 0.2333528420209755, 0.2711349095509601, 0.2711349095509601, 0.2711349095509601, 0.4119320601537232, 0.4119320601537232, 0.4119320601537232, 0.2011107881828319, 0.2011107881828319, 0.2011107881828319, 0.25373640507951833, 0.25373640507951833, 0.25373640507951833, 0.14102066452975304, 0.14102066452975304, 0.14102066452975304, 0.13177254753958223, 0.13177254753958223, 0.13177254753958223, 0.21871522686147926, 0.21871522686147926, 0.21871522686147926, 0.22800241336751603, 0.22800241336751603, 0.22800241336751603, 0.22642111368136242, 0.22642111368136242, 0.22642111368136242, 0.18939421987477667, 0.18939421987477667, 0.18939421987477667, 0.17532536458645043, 0.17532536458645043, 0.17532536458645043, 0.19729198941379522, 0.19729198941379522, 0.19729198941379522, 0.7144551409791454, 0.7144551409791454, 0.7144551409791454, 0.18883939777303727, 0.18883939777303727, 0.18883939777303727, 0.7419333238254875, 0.7419333238254875, 0.7419333238254875, 0.382646355783097, 0.382646355783097, 0.382646355783097, 0.19933351641918018, 0.19933351641918018, 0.19933351641918018, 0.4826961049052948, 0.4826961049052948, 0.4826961049052948, 0.1915559962106098, 0.1915559962106098, 0.1915559962106098, 0.23517183855747403, 0.23517183855747403, 0.23517183855747403, 0.19238856119414083, 0.19238856119414083, 0.19238856119414083, 0.0877198066400795, 0.0877198066400795, 0.0877198066400795, 0.1116867896885797, 0.1116867896885797, 0.1116867896885797, 0.10891879109645541, 0.10891879109645541, 0.10891879109645541]}, "mutation_prompt": null}
{"id": "a92490d6-8ee9-4d08-a739-08334c4f4f0e", "solution": "import numpy as np\n\nclass PSO_SA_Optimizer_Enhanced_V3:\n    def __init__(self, budget, dim, pop_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.w_max = 0.9\n        self.w_min = 0.4\n        self.c1_init = 2.0\n        self.c2_init = 1.0\n        self.temp_init = 1.0\n\n    def __call__(self, func):\n        np.random.seed(42)\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.array([func(pos) for pos in positions])\n        global_best_position = personal_best_positions[np.argmin(personal_best_scores)]\n        global_best_score = np.min(personal_best_scores)\n        \n        eval_count = self.pop_size\n        temperature = self.temp_init\n\n        inertia_weight_schedule = np.linspace(self.w_max, self.w_min, self.budget)\n\n        while eval_count < self.budget:\n            w = inertia_weight_schedule[eval_count]  # Dynamic inertia weight\n            c1 = self.c1_init * (1 - eval_count / self.budget)\n            c2 = self.c2_init * (eval_count / self.budget)\n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                velocities[i] = (w * velocities[i]\n                                 + c1 * r1 * (personal_best_positions[i] - positions[i])\n                                 + c2 * r2 * (global_best_position - positions[i]))\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], self.lower_bound, self.upper_bound)\n\n                score = func(positions[i])\n                eval_count += 1\n                if eval_count >= self.budget:\n                    break\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = positions[i]\n\n                if np.random.rand() < np.exp(-(score - global_best_score) / temperature):\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n                    if score < global_best_score:\n                        global_best_score = score\n                        global_best_position = positions[i]\n\n            temperature *= 0.95  # Adjusted annealing factor for balanced convergence\n\n        return global_best_position, global_best_score", "name": "PSO_SA_Optimizer_Enhanced_V3", "description": "Enhanced PSO-SA with dynamic inertia and social-cognitive balance for improved convergence.", "configspace": "", "generation": 44, "fitness": 0.23537502103519226, "feedback": "The algorithm PSO_SA_Optimizer_Enhanced_V3 got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.24 with standard deviation 0.19.", "error": "", "parent_id": "8275fdc9-dd69-4f35-8938-1476f28cfb9b", "metadata": {"aucs": [0.5548899662935698, 0.5548899662935698, 0.5548899662935698, 0.4508468484470638, 0.4508468484470638, 0.4508468484470638, 0.4926027246034216, 0.4926027246034216, 0.4926027246034216, 0.1624149116172664, 0.1624149116172664, 0.1624149116172664, 0.007590471105647922, 0.007590471105647922, 0.007590471105647922, 0.028433648629087815, 0.028433648629087815, 0.028433648629087815, 0.09722844342498282, 0.09722844342498282, 0.09722844342498282, 0.09774883779292376, 0.09774883779292376, 0.09774883779292376, 0.11432822025110323, 0.11432822025110323, 0.11432822025110323, 0.08806855027736837, 0.08806855027736837, 0.08806855027736837, 0.08071599086840342, 0.08071599086840342, 0.08071599086840342, 0.10209049134075743, 0.10209049134075743, 0.10209049134075743, 0.5924146396018692, 0.5924146396018692, 0.5924146396018692, 0.9047977004352709, 0.9047977004352709, 0.9047977004352709, 0.8494698996377609, 0.8494698996377609, 0.8494698996377609, 0.26642878536116577, 0.26642878536116577, 0.26642878536116577, 0.32680069948639545, 0.32680069948639545, 0.32680069948639545, 0.23491964906200402, 0.23491964906200402, 0.23491964906200402, 0.1438816659635016, 0.1438816659635016, 0.1438816659635016, 0.20435546536808946, 0.20435546536808946, 0.20435546536808946, 0.2253607926875516, 0.2253607926875516, 0.2253607926875516, 0.13746903211362038, 0.13746903211362038, 0.13746903211362038, 0.08779963957835257, 0.08779963957835257, 0.08779963957835257, 0.12137018962582302, 0.12137018962582302, 0.12137018962582302, 0.1532217853422344, 0.1532217853422344, 0.1532217853422344, 0.13327851638627763, 0.13327851638627763, 0.13327851638627763, 0.12645701780878393, 0.12645701780878393, 0.12645701780878393, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.01563060091932189, 0.01563060091932189, 0.01563060091932189, 0.06373658291808026, 0.06373658291808026, 0.06373658291808026, 0.5805378430892643, 0.5805378430892643, 0.5805378430892643, 0.2271846979569303, 0.2271846979569303, 0.2271846979569303, 0.6304477644693806, 0.6304477644693806, 0.6304477644693806, 0.0445643162267112, 0.0445643162267112, 0.0445643162267112, 0.10246300440766387, 0.10246300440766387, 0.10246300440766387, 0.1828718490219139, 0.1828718490219139, 0.1828718490219139, 0.17744398675240425, 0.17744398675240425, 0.17744398675240425, 0.21194214510574072, 0.21194214510574072, 0.21194214510574072, 0.23546959390321576, 0.23546959390321576, 0.23546959390321576, 0.3997046167535194, 0.3997046167535194, 0.3997046167535194, 0.47564121761251765, 0.47564121761251765, 0.47564121761251765, 0.5126127790827385, 0.5126127790827385, 0.5126127790827385, 0.08607144884749307, 0.08607144884749307, 0.08607144884749307, 0.07384045249992888, 0.07384045249992888, 0.07384045249992888, 0.05729475033204867, 0.05729475033204867, 0.05729475033204867, 0.19160210943038614, 0.19160210943038614, 0.19160210943038614, 0.16666348746790005, 0.16666348746790005, 0.16666348746790005, 0.23797900699345864, 0.23797900699345864, 0.23797900699345864, 0.2879932535044595, 0.2879932535044595, 0.2879932535044595, 0.19878285535227425, 0.19878285535227425, 0.19878285535227425, 0.19279458271076522, 0.19279458271076522, 0.19279458271076522, 0.24209187607620664, 0.24209187607620664, 0.24209187607620664, 0.16553072992562046, 0.16553072992562046, 0.16553072992562046, 0.13149814039354912, 0.13149814039354912, 0.13149814039354912, 0.21153797924385065, 0.21153797924385065, 0.21153797924385065, 0.20253670386865696, 0.20253670386865696, 0.20253670386865696, 0.2096548900495191, 0.2096548900495191, 0.2096548900495191, 0.18181444292792914, 0.18181444292792914, 0.18181444292792914, 0.17546969033665027, 0.17546969033665027, 0.17546969033665027, 0.19652339490505366, 0.19652339490505366, 0.19652339490505366, 0.721676502137342, 0.721676502137342, 0.721676502137342, 0.19063647543153106, 0.19063647543153106, 0.19063647543153106, 0.15122481169425206, 0.15122481169425206, 0.15122481169425206, 0.4512007394408939, 0.4512007394408939, 0.4512007394408939, 0.19966390606949724, 0.19966390606949724, 0.19966390606949724, 0.47080338411165934, 0.47080338411165934, 0.47080338411165934, 0.19007631563556115, 0.19007631563556115, 0.19007631563556115, 0.2328446282558435, 0.2328446282558435, 0.2328446282558435, 0.19236801075467502, 0.19236801075467502, 0.19236801075467502, 0.08786185961645965, 0.08786185961645965, 0.08786185961645965, 0.0944637845971873, 0.0944637845971873, 0.0944637845971873, 0.10916572059349094, 0.10916572059349094, 0.10916572059349094]}, "mutation_prompt": null}
{"id": "c0a10fab-51c5-4aef-a8be-8f243102c87f", "solution": "import numpy as np\n\nclass PSO_SA_Optimizer_Enhanced_V2:\n    def __init__(self, budget, dim, pop_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.w_max = 0.9\n        self.w_min = 0.4\n        self.c1_init = 2.0\n        self.c2_init = 1.0\n        self.temp_init = 1.0\n\n    def __call__(self, func):\n        np.random.seed(42)\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.array([func(pos) for pos in positions])\n        global_best_position = personal_best_positions[np.argmin(personal_best_scores)]\n        global_best_score = np.min(personal_best_scores)\n        \n        eval_count = self.pop_size\n        temperature = self.temp_init\n        \n        while eval_count < self.budget:\n            w = self.w_max - ((self.w_max - self.w_min) * (eval_count / self.budget))\n            c1 = self.c1_init * (1 - eval_count / self.budget)\n            c2 = self.c2_init * (eval_count / self.budget)\n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                velocities[i] = (w * velocities[i]\n                                 + c1 * r1 * (personal_best_positions[i] - positions[i])\n                                 + c2 * r2 * (global_best_position - positions[i]))\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], self.lower_bound, self.upper_bound)\n\n                score = func(positions[i])\n                eval_count += 1\n                if eval_count >= self.budget:\n                    break\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = positions[i]\n\n                if np.random.rand() < np.exp(-(score - global_best_score) / temperature):\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n                    if score < global_best_score:\n                        global_best_score = score\n                        global_best_position = positions[i]\n\n            temperature *= 0.9  # Enhanced annealing factor for faster convergence\n\n        return global_best_position, global_best_score", "name": "PSO_SA_Optimizer_Enhanced_V2", "description": "Enhanced PSO-SA with adaptive temperature and improved velocity update for better exploration-exploitation balance.", "configspace": "", "generation": 42, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "8275fdc9-dd69-4f35-8938-1476f28cfb9b", "metadata": {"aucs": [0.7648824746567023, 0.7648824746567023, 0.7648824746567023, 0.5989209008757856, 0.5989209008757856, 0.5989209008757856, 0.6742174754797561, 0.6742174754797561, 0.6742174754797561, 0.15340181758075755, 0.15340181758075755, 0.15340181758075755, 0.03196945575794474, 0.03196945575794474, 0.03196945575794474, 0.028430204481377896, 0.028430204481377896, 0.028430204481377896, 0.09721927887848547, 0.09721927887848547, 0.09721927887848547, 0.10055554133996336, 0.10055554133996336, 0.10055554133996336, 0.11442808809856964, 0.11442808809856964, 0.11442808809856964, 0.09106273135940757, 0.09106273135940757, 0.09106273135940757, 0.08133700413676104, 0.08133700413676104, 0.08133700413676104, 0.10219557668092238, 0.10219557668092238, 0.10219557668092238, 0.5924351363112623, 0.5924351363112623, 0.5924351363112623, 0.9047980793814148, 0.9047980793814148, 0.9047980793814148, 0.8502442090048321, 0.8502442090048321, 0.8502442090048321, 0.2454031538157342, 0.2454031538157342, 0.2454031538157342, 0.47754369891190485, 0.47754369891190485, 0.47754369891190485, 0.22604147488547977, 0.22604147488547977, 0.22604147488547977, 0.22058670269023306, 0.22058670269023306, 0.22058670269023306, 0.1872551853639175, 0.1872551853639175, 0.1872551853639175, 0.22519173578017293, 0.22519173578017293, 0.22519173578017293, 0.13233694616142366, 0.13233694616142366, 0.13233694616142366, 0.08781796531803687, 0.08781796531803687, 0.08781796531803687, 0.12131359600021774, 0.12131359600021774, 0.12131359600021774, 0.15118783046577833, 0.15118783046577833, 0.15118783046577833, 0.1315876710508912, 0.1315876710508912, 0.1315876710508912, 0.1256842034853637, 0.1256842034853637, 0.1256842034853637, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.015613376015341962, 0.015613376015341962, 0.015613376015341962, 0.06348628008509782, 0.06348628008509782, 0.06348628008509782, 0.2167579326396849, 0.2167579326396849, 0.2167579326396849, 0.2349078877534202, 0.2349078877534202, 0.2349078877534202, 0.5253652982472471, 0.5253652982472471, 0.5253652982472471, 0.04451652761113345, 0.04451652761113345, 0.04451652761113345, 0.1024304225623921, 0.1024304225623921, 0.1024304225623921, 0.18289120643451884, 0.18289120643451884, 0.18289120643451884, 0.17757247285435862, 0.17757247285435862, 0.17757247285435862, 0.18569096270455365, 0.18569096270455365, 0.18569096270455365, 0.270899573595341, 0.270899573595341, 0.270899573595341, 0.5796476966545081, 0.5796476966545081, 0.5796476966545081, 0.5087363546109619, 0.5087363546109619, 0.5087363546109619, 0.5459100039062216, 0.5459100039062216, 0.5459100039062216, 0.08535240446130943, 0.08535240446130943, 0.08535240446130943, 0.07356035276132311, 0.07356035276132311, 0.07356035276132311, 0.05727254799191683, 0.05727254799191683, 0.05727254799191683, 0.18616749733741433, 0.18616749733741433, 0.18616749733741433, 0.2522625076609373, 0.2522625076609373, 0.2522625076609373, 0.2333528420209755, 0.2333528420209755, 0.2333528420209755, 0.2711349095509601, 0.2711349095509601, 0.2711349095509601, 0.4119320601537232, 0.4119320601537232, 0.4119320601537232, 0.2011107881828319, 0.2011107881828319, 0.2011107881828319, 0.25373640507951833, 0.25373640507951833, 0.25373640507951833, 0.14102066452975304, 0.14102066452975304, 0.14102066452975304, 0.13177254753958223, 0.13177254753958223, 0.13177254753958223, 0.21871522686147926, 0.21871522686147926, 0.21871522686147926, 0.22800241336751603, 0.22800241336751603, 0.22800241336751603, 0.22642111368136242, 0.22642111368136242, 0.22642111368136242, 0.18939421987477667, 0.18939421987477667, 0.18939421987477667, 0.17532536458645043, 0.17532536458645043, 0.17532536458645043, 0.19729198941379522, 0.19729198941379522, 0.19729198941379522, 0.7144551409791454, 0.7144551409791454, 0.7144551409791454, 0.18883939777303727, 0.18883939777303727, 0.18883939777303727, 0.7419333238254875, 0.7419333238254875, 0.7419333238254875, 0.382646355783097, 0.382646355783097, 0.382646355783097, 0.19933351641918018, 0.19933351641918018, 0.19933351641918018, 0.4826961049052948, 0.4826961049052948, 0.4826961049052948, 0.1915559962106098, 0.1915559962106098, 0.1915559962106098, 0.23517183855747403, 0.23517183855747403, 0.23517183855747403, 0.19238856119414083, 0.19238856119414083, 0.19238856119414083, 0.0877198066400795, 0.0877198066400795, 0.0877198066400795, 0.1116867896885797, 0.1116867896885797, 0.1116867896885797, 0.10891879109645541, 0.10891879109645541, 0.10891879109645541]}, "mutation_prompt": null}
{"id": "4a9c12a4-898e-4c8e-b0e8-dc3df721c647", "solution": "import numpy as np\n\nclass PSO_SA_Optimizer_Enhanced_V2:\n    def __init__(self, budget, dim, pop_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.w_max = 0.9\n        self.w_min = 0.4\n        self.c1_init = 2.0\n        self.c2_init = 1.0\n        self.temp_init = 1.0\n\n    def __call__(self, func):\n        np.random.seed(42)\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.array([func(pos) for pos in positions])\n        global_best_position = personal_best_positions[np.argmin(personal_best_scores)]\n        global_best_score = np.min(personal_best_scores)\n        \n        eval_count = self.pop_size\n        temperature = self.temp_init\n        \n        while eval_count < self.budget:\n            w = self.w_max - ((self.w_max - self.w_min) * (eval_count / self.budget))\n            c1 = self.c1_init * (1 - eval_count / self.budget)\n            c2 = self.c2_init * (eval_count / self.budget)\n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                velocities[i] = (w * velocities[i]\n                                 + c1 * r1 * (personal_best_positions[i] - positions[i])\n                                 + c2 * r2 * (global_best_position - positions[i]))\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], self.lower_bound, self.upper_bound)\n\n                score = func(positions[i])\n                eval_count += 1\n                if eval_count >= self.budget:\n                    break\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = positions[i]\n\n                if np.random.rand() < np.exp(-(score - global_best_score) / temperature):\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n                    if score < global_best_score:\n                        global_best_score = score\n                        global_best_position = positions[i]\n\n            temperature *= 0.9  # Enhanced annealing factor for faster convergence\n\n        return global_best_position, global_best_score", "name": "PSO_SA_Optimizer_Enhanced_V2", "description": "Enhanced PSO-SA with adaptive temperature and improved velocity update for better exploration-exploitation balance.", "configspace": "", "generation": 42, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "8275fdc9-dd69-4f35-8938-1476f28cfb9b", "metadata": {"aucs": [0.7648824746567023, 0.7648824746567023, 0.7648824746567023, 0.5989209008757856, 0.5989209008757856, 0.5989209008757856, 0.6742174754797561, 0.6742174754797561, 0.6742174754797561, 0.15340181758075755, 0.15340181758075755, 0.15340181758075755, 0.03196945575794474, 0.03196945575794474, 0.03196945575794474, 0.028430204481377896, 0.028430204481377896, 0.028430204481377896, 0.09721927887848547, 0.09721927887848547, 0.09721927887848547, 0.10055554133996336, 0.10055554133996336, 0.10055554133996336, 0.11442808809856964, 0.11442808809856964, 0.11442808809856964, 0.09106273135940757, 0.09106273135940757, 0.09106273135940757, 0.08133700413676104, 0.08133700413676104, 0.08133700413676104, 0.10219557668092238, 0.10219557668092238, 0.10219557668092238, 0.5924351363112623, 0.5924351363112623, 0.5924351363112623, 0.9047980793814148, 0.9047980793814148, 0.9047980793814148, 0.8502442090048321, 0.8502442090048321, 0.8502442090048321, 0.2454031538157342, 0.2454031538157342, 0.2454031538157342, 0.47754369891190485, 0.47754369891190485, 0.47754369891190485, 0.22604147488547977, 0.22604147488547977, 0.22604147488547977, 0.22058670269023306, 0.22058670269023306, 0.22058670269023306, 0.1872551853639175, 0.1872551853639175, 0.1872551853639175, 0.22519173578017293, 0.22519173578017293, 0.22519173578017293, 0.13233694616142366, 0.13233694616142366, 0.13233694616142366, 0.08781796531803687, 0.08781796531803687, 0.08781796531803687, 0.12131359600021774, 0.12131359600021774, 0.12131359600021774, 0.15118783046577833, 0.15118783046577833, 0.15118783046577833, 0.1315876710508912, 0.1315876710508912, 0.1315876710508912, 0.1256842034853637, 0.1256842034853637, 0.1256842034853637, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.015613376015341962, 0.015613376015341962, 0.015613376015341962, 0.06348628008509782, 0.06348628008509782, 0.06348628008509782, 0.2167579326396849, 0.2167579326396849, 0.2167579326396849, 0.2349078877534202, 0.2349078877534202, 0.2349078877534202, 0.5253652982472471, 0.5253652982472471, 0.5253652982472471, 0.04451652761113345, 0.04451652761113345, 0.04451652761113345, 0.1024304225623921, 0.1024304225623921, 0.1024304225623921, 0.18289120643451884, 0.18289120643451884, 0.18289120643451884, 0.17757247285435862, 0.17757247285435862, 0.17757247285435862, 0.18569096270455365, 0.18569096270455365, 0.18569096270455365, 0.270899573595341, 0.270899573595341, 0.270899573595341, 0.5796476966545081, 0.5796476966545081, 0.5796476966545081, 0.5087363546109619, 0.5087363546109619, 0.5087363546109619, 0.5459100039062216, 0.5459100039062216, 0.5459100039062216, 0.08535240446130943, 0.08535240446130943, 0.08535240446130943, 0.07356035276132311, 0.07356035276132311, 0.07356035276132311, 0.05727254799191683, 0.05727254799191683, 0.05727254799191683, 0.18616749733741433, 0.18616749733741433, 0.18616749733741433, 0.2522625076609373, 0.2522625076609373, 0.2522625076609373, 0.2333528420209755, 0.2333528420209755, 0.2333528420209755, 0.2711349095509601, 0.2711349095509601, 0.2711349095509601, 0.4119320601537232, 0.4119320601537232, 0.4119320601537232, 0.2011107881828319, 0.2011107881828319, 0.2011107881828319, 0.25373640507951833, 0.25373640507951833, 0.25373640507951833, 0.14102066452975304, 0.14102066452975304, 0.14102066452975304, 0.13177254753958223, 0.13177254753958223, 0.13177254753958223, 0.21871522686147926, 0.21871522686147926, 0.21871522686147926, 0.22800241336751603, 0.22800241336751603, 0.22800241336751603, 0.22642111368136242, 0.22642111368136242, 0.22642111368136242, 0.18939421987477667, 0.18939421987477667, 0.18939421987477667, 0.17532536458645043, 0.17532536458645043, 0.17532536458645043, 0.19729198941379522, 0.19729198941379522, 0.19729198941379522, 0.7144551409791454, 0.7144551409791454, 0.7144551409791454, 0.18883939777303727, 0.18883939777303727, 0.18883939777303727, 0.7419333238254875, 0.7419333238254875, 0.7419333238254875, 0.382646355783097, 0.382646355783097, 0.382646355783097, 0.19933351641918018, 0.19933351641918018, 0.19933351641918018, 0.4826961049052948, 0.4826961049052948, 0.4826961049052948, 0.1915559962106098, 0.1915559962106098, 0.1915559962106098, 0.23517183855747403, 0.23517183855747403, 0.23517183855747403, 0.19238856119414083, 0.19238856119414083, 0.19238856119414083, 0.0877198066400795, 0.0877198066400795, 0.0877198066400795, 0.1116867896885797, 0.1116867896885797, 0.1116867896885797, 0.10891879109645541, 0.10891879109645541, 0.10891879109645541]}, "mutation_prompt": null}
{"id": "cfd1a40a-30bb-4328-85d2-3eb46d291fec", "solution": "import numpy as np\n\nclass PSO_SA_Optimizer_Enhanced_V2:\n    def __init__(self, budget, dim, pop_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.w_max = 0.9\n        self.w_min = 0.4\n        self.c1_init = 2.0\n        self.c2_init = 1.0\n        self.temp_init = 1.0\n\n    def __call__(self, func):\n        np.random.seed(42)\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.array([func(pos) for pos in positions])\n        global_best_position = personal_best_positions[np.argmin(personal_best_scores)]\n        global_best_score = np.min(personal_best_scores)\n        \n        eval_count = self.pop_size\n        temperature = self.temp_init\n        \n        while eval_count < self.budget:\n            w = self.w_max - ((self.w_max - self.w_min) * (eval_count / self.budget))\n            c1 = self.c1_init * (1 - eval_count / self.budget)\n            c2 = self.c2_init * (eval_count / self.budget)\n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                velocities[i] = (w * velocities[i]\n                                 + c1 * r1 * (personal_best_positions[i] - positions[i])\n                                 + c2 * r2 * (global_best_position - positions[i]))\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], self.lower_bound, self.upper_bound)\n\n                score = func(positions[i])\n                eval_count += 1\n                if eval_count >= self.budget:\n                    break\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = positions[i]\n\n                if np.random.rand() < np.exp(-(score - global_best_score) / temperature):\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n                    if score < global_best_score:\n                        global_best_score = score\n                        global_best_position = positions[i]\n\n            temperature *= 0.9  # Enhanced annealing factor for faster convergence\n\n        return global_best_position, global_best_score", "name": "PSO_SA_Optimizer_Enhanced_V2", "description": "Enhanced PSO-SA with adaptive temperature and improved velocity update for better exploration-exploitation balance.", "configspace": "", "generation": 42, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "8275fdc9-dd69-4f35-8938-1476f28cfb9b", "metadata": {"aucs": [0.7648824746567023, 0.7648824746567023, 0.7648824746567023, 0.5989209008757856, 0.5989209008757856, 0.5989209008757856, 0.6742174754797561, 0.6742174754797561, 0.6742174754797561, 0.15340181758075755, 0.15340181758075755, 0.15340181758075755, 0.03196945575794474, 0.03196945575794474, 0.03196945575794474, 0.028430204481377896, 0.028430204481377896, 0.028430204481377896, 0.09721927887848547, 0.09721927887848547, 0.09721927887848547, 0.10055554133996336, 0.10055554133996336, 0.10055554133996336, 0.11442808809856964, 0.11442808809856964, 0.11442808809856964, 0.09106273135940757, 0.09106273135940757, 0.09106273135940757, 0.08133700413676104, 0.08133700413676104, 0.08133700413676104, 0.10219557668092238, 0.10219557668092238, 0.10219557668092238, 0.5924351363112623, 0.5924351363112623, 0.5924351363112623, 0.9047980793814148, 0.9047980793814148, 0.9047980793814148, 0.8502442090048321, 0.8502442090048321, 0.8502442090048321, 0.2454031538157342, 0.2454031538157342, 0.2454031538157342, 0.47754369891190485, 0.47754369891190485, 0.47754369891190485, 0.22604147488547977, 0.22604147488547977, 0.22604147488547977, 0.22058670269023306, 0.22058670269023306, 0.22058670269023306, 0.1872551853639175, 0.1872551853639175, 0.1872551853639175, 0.22519173578017293, 0.22519173578017293, 0.22519173578017293, 0.13233694616142366, 0.13233694616142366, 0.13233694616142366, 0.08781796531803687, 0.08781796531803687, 0.08781796531803687, 0.12131359600021774, 0.12131359600021774, 0.12131359600021774, 0.15118783046577833, 0.15118783046577833, 0.15118783046577833, 0.1315876710508912, 0.1315876710508912, 0.1315876710508912, 0.1256842034853637, 0.1256842034853637, 0.1256842034853637, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.015613376015341962, 0.015613376015341962, 0.015613376015341962, 0.06348628008509782, 0.06348628008509782, 0.06348628008509782, 0.2167579326396849, 0.2167579326396849, 0.2167579326396849, 0.2349078877534202, 0.2349078877534202, 0.2349078877534202, 0.5253652982472471, 0.5253652982472471, 0.5253652982472471, 0.04451652761113345, 0.04451652761113345, 0.04451652761113345, 0.1024304225623921, 0.1024304225623921, 0.1024304225623921, 0.18289120643451884, 0.18289120643451884, 0.18289120643451884, 0.17757247285435862, 0.17757247285435862, 0.17757247285435862, 0.18569096270455365, 0.18569096270455365, 0.18569096270455365, 0.270899573595341, 0.270899573595341, 0.270899573595341, 0.5796476966545081, 0.5796476966545081, 0.5796476966545081, 0.5087363546109619, 0.5087363546109619, 0.5087363546109619, 0.5459100039062216, 0.5459100039062216, 0.5459100039062216, 0.08535240446130943, 0.08535240446130943, 0.08535240446130943, 0.07356035276132311, 0.07356035276132311, 0.07356035276132311, 0.05727254799191683, 0.05727254799191683, 0.05727254799191683, 0.18616749733741433, 0.18616749733741433, 0.18616749733741433, 0.2522625076609373, 0.2522625076609373, 0.2522625076609373, 0.2333528420209755, 0.2333528420209755, 0.2333528420209755, 0.2711349095509601, 0.2711349095509601, 0.2711349095509601, 0.4119320601537232, 0.4119320601537232, 0.4119320601537232, 0.2011107881828319, 0.2011107881828319, 0.2011107881828319, 0.25373640507951833, 0.25373640507951833, 0.25373640507951833, 0.14102066452975304, 0.14102066452975304, 0.14102066452975304, 0.13177254753958223, 0.13177254753958223, 0.13177254753958223, 0.21871522686147926, 0.21871522686147926, 0.21871522686147926, 0.22800241336751603, 0.22800241336751603, 0.22800241336751603, 0.22642111368136242, 0.22642111368136242, 0.22642111368136242, 0.18939421987477667, 0.18939421987477667, 0.18939421987477667, 0.17532536458645043, 0.17532536458645043, 0.17532536458645043, 0.19729198941379522, 0.19729198941379522, 0.19729198941379522, 0.7144551409791454, 0.7144551409791454, 0.7144551409791454, 0.18883939777303727, 0.18883939777303727, 0.18883939777303727, 0.7419333238254875, 0.7419333238254875, 0.7419333238254875, 0.382646355783097, 0.382646355783097, 0.382646355783097, 0.19933351641918018, 0.19933351641918018, 0.19933351641918018, 0.4826961049052948, 0.4826961049052948, 0.4826961049052948, 0.1915559962106098, 0.1915559962106098, 0.1915559962106098, 0.23517183855747403, 0.23517183855747403, 0.23517183855747403, 0.19238856119414083, 0.19238856119414083, 0.19238856119414083, 0.0877198066400795, 0.0877198066400795, 0.0877198066400795, 0.1116867896885797, 0.1116867896885797, 0.1116867896885797, 0.10891879109645541, 0.10891879109645541, 0.10891879109645541]}, "mutation_prompt": null}
{"id": "f4efa277-cae1-4b33-8402-87b2b9cb4f38", "solution": "import numpy as np\n\nclass PSO_SA_Optimizer_Enhanced_V2:\n    def __init__(self, budget, dim, pop_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.w_max = 0.9\n        self.w_min = 0.4\n        self.c1_init = 2.0\n        self.c2_init = 1.0\n        self.temp_init = 1.0\n\n    def __call__(self, func):\n        np.random.seed(42)\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.array([func(pos) for pos in positions])\n        global_best_position = personal_best_positions[np.argmin(personal_best_scores)]\n        global_best_score = np.min(personal_best_scores)\n        \n        eval_count = self.pop_size\n        temperature = self.temp_init\n        \n        while eval_count < self.budget:\n            w = self.w_max - ((self.w_max - self.w_min) * (eval_count / self.budget))\n            c1 = self.c1_init * (1 - eval_count / self.budget)\n            c2 = self.c2_init * (eval_count / self.budget)\n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                velocities[i] = (w * velocities[i]\n                                 + c1 * r1 * (personal_best_positions[i] - positions[i])\n                                 + c2 * r2 * (global_best_position - positions[i]))\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], self.lower_bound, self.upper_bound)\n\n                score = func(positions[i])\n                eval_count += 1\n                if eval_count >= self.budget:\n                    break\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = positions[i]\n\n                if np.random.rand() < np.exp(-(score - global_best_score) / temperature):\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n                    if score < global_best_score:\n                        global_best_score = score\n                        global_best_position = positions[i]\n\n            temperature *= 0.9  # Enhanced annealing factor for faster convergence\n\n        return global_best_position, global_best_score", "name": "PSO_SA_Optimizer_Enhanced_V2", "description": "Enhanced PSO-SA with adaptive temperature and improved velocity update for better exploration-exploitation balance.", "configspace": "", "generation": 42, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "8275fdc9-dd69-4f35-8938-1476f28cfb9b", "metadata": {"aucs": [0.7648824746567023, 0.7648824746567023, 0.7648824746567023, 0.5989209008757856, 0.5989209008757856, 0.5989209008757856, 0.6742174754797561, 0.6742174754797561, 0.6742174754797561, 0.15340181758075755, 0.15340181758075755, 0.15340181758075755, 0.03196945575794474, 0.03196945575794474, 0.03196945575794474, 0.028430204481377896, 0.028430204481377896, 0.028430204481377896, 0.09721927887848547, 0.09721927887848547, 0.09721927887848547, 0.10055554133996336, 0.10055554133996336, 0.10055554133996336, 0.11442808809856964, 0.11442808809856964, 0.11442808809856964, 0.09106273135940757, 0.09106273135940757, 0.09106273135940757, 0.08133700413676104, 0.08133700413676104, 0.08133700413676104, 0.10219557668092238, 0.10219557668092238, 0.10219557668092238, 0.5924351363112623, 0.5924351363112623, 0.5924351363112623, 0.9047980793814148, 0.9047980793814148, 0.9047980793814148, 0.8502442090048321, 0.8502442090048321, 0.8502442090048321, 0.2454031538157342, 0.2454031538157342, 0.2454031538157342, 0.47754369891190485, 0.47754369891190485, 0.47754369891190485, 0.22604147488547977, 0.22604147488547977, 0.22604147488547977, 0.22058670269023306, 0.22058670269023306, 0.22058670269023306, 0.1872551853639175, 0.1872551853639175, 0.1872551853639175, 0.22519173578017293, 0.22519173578017293, 0.22519173578017293, 0.13233694616142366, 0.13233694616142366, 0.13233694616142366, 0.08781796531803687, 0.08781796531803687, 0.08781796531803687, 0.12131359600021774, 0.12131359600021774, 0.12131359600021774, 0.15118783046577833, 0.15118783046577833, 0.15118783046577833, 0.1315876710508912, 0.1315876710508912, 0.1315876710508912, 0.1256842034853637, 0.1256842034853637, 0.1256842034853637, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.015613376015341962, 0.015613376015341962, 0.015613376015341962, 0.06348628008509782, 0.06348628008509782, 0.06348628008509782, 0.2167579326396849, 0.2167579326396849, 0.2167579326396849, 0.2349078877534202, 0.2349078877534202, 0.2349078877534202, 0.5253652982472471, 0.5253652982472471, 0.5253652982472471, 0.04451652761113345, 0.04451652761113345, 0.04451652761113345, 0.1024304225623921, 0.1024304225623921, 0.1024304225623921, 0.18289120643451884, 0.18289120643451884, 0.18289120643451884, 0.17757247285435862, 0.17757247285435862, 0.17757247285435862, 0.18569096270455365, 0.18569096270455365, 0.18569096270455365, 0.270899573595341, 0.270899573595341, 0.270899573595341, 0.5796476966545081, 0.5796476966545081, 0.5796476966545081, 0.5087363546109619, 0.5087363546109619, 0.5087363546109619, 0.5459100039062216, 0.5459100039062216, 0.5459100039062216, 0.08535240446130943, 0.08535240446130943, 0.08535240446130943, 0.07356035276132311, 0.07356035276132311, 0.07356035276132311, 0.05727254799191683, 0.05727254799191683, 0.05727254799191683, 0.18616749733741433, 0.18616749733741433, 0.18616749733741433, 0.2522625076609373, 0.2522625076609373, 0.2522625076609373, 0.2333528420209755, 0.2333528420209755, 0.2333528420209755, 0.2711349095509601, 0.2711349095509601, 0.2711349095509601, 0.4119320601537232, 0.4119320601537232, 0.4119320601537232, 0.2011107881828319, 0.2011107881828319, 0.2011107881828319, 0.25373640507951833, 0.25373640507951833, 0.25373640507951833, 0.14102066452975304, 0.14102066452975304, 0.14102066452975304, 0.13177254753958223, 0.13177254753958223, 0.13177254753958223, 0.21871522686147926, 0.21871522686147926, 0.21871522686147926, 0.22800241336751603, 0.22800241336751603, 0.22800241336751603, 0.22642111368136242, 0.22642111368136242, 0.22642111368136242, 0.18939421987477667, 0.18939421987477667, 0.18939421987477667, 0.17532536458645043, 0.17532536458645043, 0.17532536458645043, 0.19729198941379522, 0.19729198941379522, 0.19729198941379522, 0.7144551409791454, 0.7144551409791454, 0.7144551409791454, 0.18883939777303727, 0.18883939777303727, 0.18883939777303727, 0.7419333238254875, 0.7419333238254875, 0.7419333238254875, 0.382646355783097, 0.382646355783097, 0.382646355783097, 0.19933351641918018, 0.19933351641918018, 0.19933351641918018, 0.4826961049052948, 0.4826961049052948, 0.4826961049052948, 0.1915559962106098, 0.1915559962106098, 0.1915559962106098, 0.23517183855747403, 0.23517183855747403, 0.23517183855747403, 0.19238856119414083, 0.19238856119414083, 0.19238856119414083, 0.0877198066400795, 0.0877198066400795, 0.0877198066400795, 0.1116867896885797, 0.1116867896885797, 0.1116867896885797, 0.10891879109645541, 0.10891879109645541, 0.10891879109645541]}, "mutation_prompt": null}
{"id": "6f8fd21b-730e-4bec-8310-c62456c328d8", "solution": "import numpy as np\n\nclass PSO_SA_Optimizer_Enhanced_V2:\n    def __init__(self, budget, dim, pop_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.w_max = 0.9\n        self.w_min = 0.4\n        self.c1_init = 2.0\n        self.c2_init = 1.0\n        self.temp_init = 1.0\n\n    def __call__(self, func):\n        np.random.seed(42)\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.array([func(pos) for pos in positions])\n        global_best_position = personal_best_positions[np.argmin(personal_best_scores)]\n        global_best_score = np.min(personal_best_scores)\n        \n        eval_count = self.pop_size\n        temperature = self.temp_init\n        \n        while eval_count < self.budget:\n            w = self.w_max - ((self.w_max - self.w_min) * (eval_count / self.budget))\n            c1 = self.c1_init * (1 - eval_count / self.budget)\n            c2 = self.c2_init * (eval_count / self.budget)\n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                velocities[i] = (w * velocities[i]\n                                 + c1 * r1 * (personal_best_positions[i] - positions[i])\n                                 + c2 * r2 * (global_best_position - positions[i]))\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], self.lower_bound, self.upper_bound)\n\n                score = func(positions[i])\n                eval_count += 1\n                if eval_count >= self.budget:\n                    break\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = positions[i]\n\n                if np.random.rand() < np.exp(-(score - global_best_score) / temperature):\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n                    if score < global_best_score:\n                        global_best_score = score\n                        global_best_position = positions[i]\n\n            temperature *= 0.9  # Enhanced annealing factor for faster convergence\n\n        return global_best_position, global_best_score", "name": "PSO_SA_Optimizer_Enhanced_V2", "description": "Enhanced PSO-SA with adaptive temperature and improved velocity update for better exploration-exploitation balance.", "configspace": "", "generation": 42, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "8275fdc9-dd69-4f35-8938-1476f28cfb9b", "metadata": {"aucs": [0.7648824746567023, 0.7648824746567023, 0.7648824746567023, 0.5989209008757856, 0.5989209008757856, 0.5989209008757856, 0.6742174754797561, 0.6742174754797561, 0.6742174754797561, 0.15340181758075755, 0.15340181758075755, 0.15340181758075755, 0.03196945575794474, 0.03196945575794474, 0.03196945575794474, 0.028430204481377896, 0.028430204481377896, 0.028430204481377896, 0.09721927887848547, 0.09721927887848547, 0.09721927887848547, 0.10055554133996336, 0.10055554133996336, 0.10055554133996336, 0.11442808809856964, 0.11442808809856964, 0.11442808809856964, 0.09106273135940757, 0.09106273135940757, 0.09106273135940757, 0.08133700413676104, 0.08133700413676104, 0.08133700413676104, 0.10219557668092238, 0.10219557668092238, 0.10219557668092238, 0.5924351363112623, 0.5924351363112623, 0.5924351363112623, 0.9047980793814148, 0.9047980793814148, 0.9047980793814148, 0.8502442090048321, 0.8502442090048321, 0.8502442090048321, 0.2454031538157342, 0.2454031538157342, 0.2454031538157342, 0.47754369891190485, 0.47754369891190485, 0.47754369891190485, 0.22604147488547977, 0.22604147488547977, 0.22604147488547977, 0.22058670269023306, 0.22058670269023306, 0.22058670269023306, 0.1872551853639175, 0.1872551853639175, 0.1872551853639175, 0.22519173578017293, 0.22519173578017293, 0.22519173578017293, 0.13233694616142366, 0.13233694616142366, 0.13233694616142366, 0.08781796531803687, 0.08781796531803687, 0.08781796531803687, 0.12131359600021774, 0.12131359600021774, 0.12131359600021774, 0.15118783046577833, 0.15118783046577833, 0.15118783046577833, 0.1315876710508912, 0.1315876710508912, 0.1315876710508912, 0.1256842034853637, 0.1256842034853637, 0.1256842034853637, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.015613376015341962, 0.015613376015341962, 0.015613376015341962, 0.06348628008509782, 0.06348628008509782, 0.06348628008509782, 0.2167579326396849, 0.2167579326396849, 0.2167579326396849, 0.2349078877534202, 0.2349078877534202, 0.2349078877534202, 0.5253652982472471, 0.5253652982472471, 0.5253652982472471, 0.04451652761113345, 0.04451652761113345, 0.04451652761113345, 0.1024304225623921, 0.1024304225623921, 0.1024304225623921, 0.18289120643451884, 0.18289120643451884, 0.18289120643451884, 0.17757247285435862, 0.17757247285435862, 0.17757247285435862, 0.18569096270455365, 0.18569096270455365, 0.18569096270455365, 0.270899573595341, 0.270899573595341, 0.270899573595341, 0.5796476966545081, 0.5796476966545081, 0.5796476966545081, 0.5087363546109619, 0.5087363546109619, 0.5087363546109619, 0.5459100039062216, 0.5459100039062216, 0.5459100039062216, 0.08535240446130943, 0.08535240446130943, 0.08535240446130943, 0.07356035276132311, 0.07356035276132311, 0.07356035276132311, 0.05727254799191683, 0.05727254799191683, 0.05727254799191683, 0.18616749733741433, 0.18616749733741433, 0.18616749733741433, 0.2522625076609373, 0.2522625076609373, 0.2522625076609373, 0.2333528420209755, 0.2333528420209755, 0.2333528420209755, 0.2711349095509601, 0.2711349095509601, 0.2711349095509601, 0.4119320601537232, 0.4119320601537232, 0.4119320601537232, 0.2011107881828319, 0.2011107881828319, 0.2011107881828319, 0.25373640507951833, 0.25373640507951833, 0.25373640507951833, 0.14102066452975304, 0.14102066452975304, 0.14102066452975304, 0.13177254753958223, 0.13177254753958223, 0.13177254753958223, 0.21871522686147926, 0.21871522686147926, 0.21871522686147926, 0.22800241336751603, 0.22800241336751603, 0.22800241336751603, 0.22642111368136242, 0.22642111368136242, 0.22642111368136242, 0.18939421987477667, 0.18939421987477667, 0.18939421987477667, 0.17532536458645043, 0.17532536458645043, 0.17532536458645043, 0.19729198941379522, 0.19729198941379522, 0.19729198941379522, 0.7144551409791454, 0.7144551409791454, 0.7144551409791454, 0.18883939777303727, 0.18883939777303727, 0.18883939777303727, 0.7419333238254875, 0.7419333238254875, 0.7419333238254875, 0.382646355783097, 0.382646355783097, 0.382646355783097, 0.19933351641918018, 0.19933351641918018, 0.19933351641918018, 0.4826961049052948, 0.4826961049052948, 0.4826961049052948, 0.1915559962106098, 0.1915559962106098, 0.1915559962106098, 0.23517183855747403, 0.23517183855747403, 0.23517183855747403, 0.19238856119414083, 0.19238856119414083, 0.19238856119414083, 0.0877198066400795, 0.0877198066400795, 0.0877198066400795, 0.1116867896885797, 0.1116867896885797, 0.1116867896885797, 0.10891879109645541, 0.10891879109645541, 0.10891879109645541]}, "mutation_prompt": null}
{"id": "af41c1ae-6c56-48b3-8cb3-b7e84f20adb9", "solution": "import numpy as np\n\nclass PSO_SA_Optimizer_Enhanced_V2:\n    def __init__(self, budget, dim, pop_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.w_max = 0.9\n        self.w_min = 0.4\n        self.c1_init = 2.0\n        self.c2_init = 1.0\n        self.temp_init = 1.0\n\n    def __call__(self, func):\n        np.random.seed(42)\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.array([func(pos) for pos in positions])\n        global_best_position = personal_best_positions[np.argmin(personal_best_scores)]\n        global_best_score = np.min(personal_best_scores)\n        \n        eval_count = self.pop_size\n        temperature = self.temp_init\n        \n        while eval_count < self.budget:\n            w = self.w_max - ((self.w_max - self.w_min) * (eval_count / self.budget))\n            c1 = self.c1_init * (1 - eval_count / self.budget)\n            c2 = self.c2_init * (eval_count / self.budget)\n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                velocities[i] = (w * velocities[i]\n                                 + c1 * r1 * (personal_best_positions[i] - positions[i])\n                                 + c2 * r2 * (global_best_position - positions[i]))\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], self.lower_bound, self.upper_bound)\n\n                score = func(positions[i])\n                eval_count += 1\n                if eval_count >= self.budget:\n                    break\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = positions[i]\n\n                if np.random.rand() < np.exp(-(score - global_best_score) / temperature):\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n                    if score < global_best_score:\n                        global_best_score = score\n                        global_best_position = positions[i]\n\n            temperature *= 0.9  # Enhanced annealing factor for faster convergence\n\n        return global_best_position, global_best_score", "name": "PSO_SA_Optimizer_Enhanced_V2", "description": "Enhanced PSO-SA with adaptive temperature and improved velocity update for better exploration-exploitation balance.", "configspace": "", "generation": 42, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "8275fdc9-dd69-4f35-8938-1476f28cfb9b", "metadata": {"aucs": [0.7648824746567023, 0.7648824746567023, 0.7648824746567023, 0.5989209008757856, 0.5989209008757856, 0.5989209008757856, 0.6742174754797561, 0.6742174754797561, 0.6742174754797561, 0.15340181758075755, 0.15340181758075755, 0.15340181758075755, 0.03196945575794474, 0.03196945575794474, 0.03196945575794474, 0.028430204481377896, 0.028430204481377896, 0.028430204481377896, 0.09721927887848547, 0.09721927887848547, 0.09721927887848547, 0.10055554133996336, 0.10055554133996336, 0.10055554133996336, 0.11442808809856964, 0.11442808809856964, 0.11442808809856964, 0.09106273135940757, 0.09106273135940757, 0.09106273135940757, 0.08133700413676104, 0.08133700413676104, 0.08133700413676104, 0.10219557668092238, 0.10219557668092238, 0.10219557668092238, 0.5924351363112623, 0.5924351363112623, 0.5924351363112623, 0.9047980793814148, 0.9047980793814148, 0.9047980793814148, 0.8502442090048321, 0.8502442090048321, 0.8502442090048321, 0.2454031538157342, 0.2454031538157342, 0.2454031538157342, 0.47754369891190485, 0.47754369891190485, 0.47754369891190485, 0.22604147488547977, 0.22604147488547977, 0.22604147488547977, 0.22058670269023306, 0.22058670269023306, 0.22058670269023306, 0.1872551853639175, 0.1872551853639175, 0.1872551853639175, 0.22519173578017293, 0.22519173578017293, 0.22519173578017293, 0.13233694616142366, 0.13233694616142366, 0.13233694616142366, 0.08781796531803687, 0.08781796531803687, 0.08781796531803687, 0.12131359600021774, 0.12131359600021774, 0.12131359600021774, 0.15118783046577833, 0.15118783046577833, 0.15118783046577833, 0.1315876710508912, 0.1315876710508912, 0.1315876710508912, 0.1256842034853637, 0.1256842034853637, 0.1256842034853637, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.015613376015341962, 0.015613376015341962, 0.015613376015341962, 0.06348628008509782, 0.06348628008509782, 0.06348628008509782, 0.2167579326396849, 0.2167579326396849, 0.2167579326396849, 0.2349078877534202, 0.2349078877534202, 0.2349078877534202, 0.5253652982472471, 0.5253652982472471, 0.5253652982472471, 0.04451652761113345, 0.04451652761113345, 0.04451652761113345, 0.1024304225623921, 0.1024304225623921, 0.1024304225623921, 0.18289120643451884, 0.18289120643451884, 0.18289120643451884, 0.17757247285435862, 0.17757247285435862, 0.17757247285435862, 0.18569096270455365, 0.18569096270455365, 0.18569096270455365, 0.270899573595341, 0.270899573595341, 0.270899573595341, 0.5796476966545081, 0.5796476966545081, 0.5796476966545081, 0.5087363546109619, 0.5087363546109619, 0.5087363546109619, 0.5459100039062216, 0.5459100039062216, 0.5459100039062216, 0.08535240446130943, 0.08535240446130943, 0.08535240446130943, 0.07356035276132311, 0.07356035276132311, 0.07356035276132311, 0.05727254799191683, 0.05727254799191683, 0.05727254799191683, 0.18616749733741433, 0.18616749733741433, 0.18616749733741433, 0.2522625076609373, 0.2522625076609373, 0.2522625076609373, 0.2333528420209755, 0.2333528420209755, 0.2333528420209755, 0.2711349095509601, 0.2711349095509601, 0.2711349095509601, 0.4119320601537232, 0.4119320601537232, 0.4119320601537232, 0.2011107881828319, 0.2011107881828319, 0.2011107881828319, 0.25373640507951833, 0.25373640507951833, 0.25373640507951833, 0.14102066452975304, 0.14102066452975304, 0.14102066452975304, 0.13177254753958223, 0.13177254753958223, 0.13177254753958223, 0.21871522686147926, 0.21871522686147926, 0.21871522686147926, 0.22800241336751603, 0.22800241336751603, 0.22800241336751603, 0.22642111368136242, 0.22642111368136242, 0.22642111368136242, 0.18939421987477667, 0.18939421987477667, 0.18939421987477667, 0.17532536458645043, 0.17532536458645043, 0.17532536458645043, 0.19729198941379522, 0.19729198941379522, 0.19729198941379522, 0.7144551409791454, 0.7144551409791454, 0.7144551409791454, 0.18883939777303727, 0.18883939777303727, 0.18883939777303727, 0.7419333238254875, 0.7419333238254875, 0.7419333238254875, 0.382646355783097, 0.382646355783097, 0.382646355783097, 0.19933351641918018, 0.19933351641918018, 0.19933351641918018, 0.4826961049052948, 0.4826961049052948, 0.4826961049052948, 0.1915559962106098, 0.1915559962106098, 0.1915559962106098, 0.23517183855747403, 0.23517183855747403, 0.23517183855747403, 0.19238856119414083, 0.19238856119414083, 0.19238856119414083, 0.0877198066400795, 0.0877198066400795, 0.0877198066400795, 0.1116867896885797, 0.1116867896885797, 0.1116867896885797, 0.10891879109645541, 0.10891879109645541, 0.10891879109645541]}, "mutation_prompt": null}
{"id": "0cb645b3-2057-46c7-951b-ca1e5c61583b", "solution": "import numpy as np\n\nclass PSO_SA_Optimizer_Enhanced_V2:\n    def __init__(self, budget, dim, pop_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.w_max = 0.9\n        self.w_min = 0.4\n        self.c1_init = 2.0\n        self.c2_init = 1.0\n        self.temp_init = 1.0\n\n    def __call__(self, func):\n        np.random.seed(42)\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.array([func(pos) for pos in positions])\n        global_best_position = personal_best_positions[np.argmin(personal_best_scores)]\n        global_best_score = np.min(personal_best_scores)\n        \n        eval_count = self.pop_size\n        temperature = self.temp_init\n        \n        while eval_count < self.budget:\n            w = self.w_max - ((self.w_max - self.w_min) * (eval_count / self.budget))\n            c1 = self.c1_init * (1 - eval_count / self.budget)\n            c2 = self.c2_init * (eval_count / self.budget)\n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                velocities[i] = (w * velocities[i]\n                                 + c1 * r1 * (personal_best_positions[i] - positions[i])\n                                 + c2 * r2 * (global_best_position - positions[i]))\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], self.lower_bound, self.upper_bound)\n\n                score = func(positions[i])\n                eval_count += 1\n                if eval_count >= self.budget:\n                    break\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = positions[i]\n\n                if np.random.rand() < np.exp(-(score - global_best_score) / temperature):\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n                    if score < global_best_score:\n                        global_best_score = score\n                        global_best_position = positions[i]\n\n            temperature *= 0.9  # Enhanced annealing factor for faster convergence\n\n        return global_best_position, global_best_score", "name": "PSO_SA_Optimizer_Enhanced_V2", "description": "Enhanced PSO-SA with adaptive temperature and improved velocity update for better exploration-exploitation balance.", "configspace": "", "generation": 42, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "8275fdc9-dd69-4f35-8938-1476f28cfb9b", "metadata": {"aucs": [0.7648824746567023, 0.7648824746567023, 0.7648824746567023, 0.5989209008757856, 0.5989209008757856, 0.5989209008757856, 0.6742174754797561, 0.6742174754797561, 0.6742174754797561, 0.15340181758075755, 0.15340181758075755, 0.15340181758075755, 0.03196945575794474, 0.03196945575794474, 0.03196945575794474, 0.028430204481377896, 0.028430204481377896, 0.028430204481377896, 0.09721927887848547, 0.09721927887848547, 0.09721927887848547, 0.10055554133996336, 0.10055554133996336, 0.10055554133996336, 0.11442808809856964, 0.11442808809856964, 0.11442808809856964, 0.09106273135940757, 0.09106273135940757, 0.09106273135940757, 0.08133700413676104, 0.08133700413676104, 0.08133700413676104, 0.10219557668092238, 0.10219557668092238, 0.10219557668092238, 0.5924351363112623, 0.5924351363112623, 0.5924351363112623, 0.9047980793814148, 0.9047980793814148, 0.9047980793814148, 0.8502442090048321, 0.8502442090048321, 0.8502442090048321, 0.2454031538157342, 0.2454031538157342, 0.2454031538157342, 0.47754369891190485, 0.47754369891190485, 0.47754369891190485, 0.22604147488547977, 0.22604147488547977, 0.22604147488547977, 0.22058670269023306, 0.22058670269023306, 0.22058670269023306, 0.1872551853639175, 0.1872551853639175, 0.1872551853639175, 0.22519173578017293, 0.22519173578017293, 0.22519173578017293, 0.13233694616142366, 0.13233694616142366, 0.13233694616142366, 0.08781796531803687, 0.08781796531803687, 0.08781796531803687, 0.12131359600021774, 0.12131359600021774, 0.12131359600021774, 0.15118783046577833, 0.15118783046577833, 0.15118783046577833, 0.1315876710508912, 0.1315876710508912, 0.1315876710508912, 0.1256842034853637, 0.1256842034853637, 0.1256842034853637, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.015613376015341962, 0.015613376015341962, 0.015613376015341962, 0.06348628008509782, 0.06348628008509782, 0.06348628008509782, 0.2167579326396849, 0.2167579326396849, 0.2167579326396849, 0.2349078877534202, 0.2349078877534202, 0.2349078877534202, 0.5253652982472471, 0.5253652982472471, 0.5253652982472471, 0.04451652761113345, 0.04451652761113345, 0.04451652761113345, 0.1024304225623921, 0.1024304225623921, 0.1024304225623921, 0.18289120643451884, 0.18289120643451884, 0.18289120643451884, 0.17757247285435862, 0.17757247285435862, 0.17757247285435862, 0.18569096270455365, 0.18569096270455365, 0.18569096270455365, 0.270899573595341, 0.270899573595341, 0.270899573595341, 0.5796476966545081, 0.5796476966545081, 0.5796476966545081, 0.5087363546109619, 0.5087363546109619, 0.5087363546109619, 0.5459100039062216, 0.5459100039062216, 0.5459100039062216, 0.08535240446130943, 0.08535240446130943, 0.08535240446130943, 0.07356035276132311, 0.07356035276132311, 0.07356035276132311, 0.05727254799191683, 0.05727254799191683, 0.05727254799191683, 0.18616749733741433, 0.18616749733741433, 0.18616749733741433, 0.2522625076609373, 0.2522625076609373, 0.2522625076609373, 0.2333528420209755, 0.2333528420209755, 0.2333528420209755, 0.2711349095509601, 0.2711349095509601, 0.2711349095509601, 0.4119320601537232, 0.4119320601537232, 0.4119320601537232, 0.2011107881828319, 0.2011107881828319, 0.2011107881828319, 0.25373640507951833, 0.25373640507951833, 0.25373640507951833, 0.14102066452975304, 0.14102066452975304, 0.14102066452975304, 0.13177254753958223, 0.13177254753958223, 0.13177254753958223, 0.21871522686147926, 0.21871522686147926, 0.21871522686147926, 0.22800241336751603, 0.22800241336751603, 0.22800241336751603, 0.22642111368136242, 0.22642111368136242, 0.22642111368136242, 0.18939421987477667, 0.18939421987477667, 0.18939421987477667, 0.17532536458645043, 0.17532536458645043, 0.17532536458645043, 0.19729198941379522, 0.19729198941379522, 0.19729198941379522, 0.7144551409791454, 0.7144551409791454, 0.7144551409791454, 0.18883939777303727, 0.18883939777303727, 0.18883939777303727, 0.7419333238254875, 0.7419333238254875, 0.7419333238254875, 0.382646355783097, 0.382646355783097, 0.382646355783097, 0.19933351641918018, 0.19933351641918018, 0.19933351641918018, 0.4826961049052948, 0.4826961049052948, 0.4826961049052948, 0.1915559962106098, 0.1915559962106098, 0.1915559962106098, 0.23517183855747403, 0.23517183855747403, 0.23517183855747403, 0.19238856119414083, 0.19238856119414083, 0.19238856119414083, 0.0877198066400795, 0.0877198066400795, 0.0877198066400795, 0.1116867896885797, 0.1116867896885797, 0.1116867896885797, 0.10891879109645541, 0.10891879109645541, 0.10891879109645541]}, "mutation_prompt": null}
{"id": "f4a97037-78f1-4db2-bfb6-041853ecd9cb", "solution": "import numpy as np\n\nclass PSO_SA_Optimizer_Enhanced_V2:\n    def __init__(self, budget, dim, pop_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.w_max = 0.9\n        self.w_min = 0.4\n        self.c1_init = 2.0\n        self.c2_init = 1.0\n        self.temp_init = 1.0\n\n    def __call__(self, func):\n        np.random.seed(42)\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.array([func(pos) for pos in positions])\n        global_best_position = personal_best_positions[np.argmin(personal_best_scores)]\n        global_best_score = np.min(personal_best_scores)\n        \n        eval_count = self.pop_size\n        temperature = self.temp_init\n        \n        while eval_count < self.budget:\n            w = self.w_max - ((self.w_max - self.w_min) * (eval_count / self.budget))\n            c1 = self.c1_init * (1 - eval_count / self.budget)\n            c2 = self.c2_init * (eval_count / self.budget)\n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                velocities[i] = (w * velocities[i]\n                                 + c1 * r1 * (personal_best_positions[i] - positions[i])\n                                 + c2 * r2 * (global_best_position - positions[i]))\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], self.lower_bound, self.upper_bound)\n\n                score = func(positions[i])\n                eval_count += 1\n                if eval_count >= self.budget:\n                    break\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = positions[i]\n\n                if np.random.rand() < np.exp(-(score - global_best_score) / temperature):\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n                    if score < global_best_score:\n                        global_best_score = score\n                        global_best_position = positions[i]\n\n            temperature *= 0.9  # Enhanced annealing factor for faster convergence\n\n        return global_best_position, global_best_score", "name": "PSO_SA_Optimizer_Enhanced_V2", "description": "Enhanced PSO-SA with adaptive temperature and improved velocity update for better exploration-exploitation balance.", "configspace": "", "generation": 42, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "8275fdc9-dd69-4f35-8938-1476f28cfb9b", "metadata": {"aucs": [0.7648824746567023, 0.7648824746567023, 0.7648824746567023, 0.5989209008757856, 0.5989209008757856, 0.5989209008757856, 0.6742174754797561, 0.6742174754797561, 0.6742174754797561, 0.15340181758075755, 0.15340181758075755, 0.15340181758075755, 0.03196945575794474, 0.03196945575794474, 0.03196945575794474, 0.028430204481377896, 0.028430204481377896, 0.028430204481377896, 0.09721927887848547, 0.09721927887848547, 0.09721927887848547, 0.10055554133996336, 0.10055554133996336, 0.10055554133996336, 0.11442808809856964, 0.11442808809856964, 0.11442808809856964, 0.09106273135940757, 0.09106273135940757, 0.09106273135940757, 0.08133700413676104, 0.08133700413676104, 0.08133700413676104, 0.10219557668092238, 0.10219557668092238, 0.10219557668092238, 0.5924351363112623, 0.5924351363112623, 0.5924351363112623, 0.9047980793814148, 0.9047980793814148, 0.9047980793814148, 0.8502442090048321, 0.8502442090048321, 0.8502442090048321, 0.2454031538157342, 0.2454031538157342, 0.2454031538157342, 0.47754369891190485, 0.47754369891190485, 0.47754369891190485, 0.22604147488547977, 0.22604147488547977, 0.22604147488547977, 0.22058670269023306, 0.22058670269023306, 0.22058670269023306, 0.1872551853639175, 0.1872551853639175, 0.1872551853639175, 0.22519173578017293, 0.22519173578017293, 0.22519173578017293, 0.13233694616142366, 0.13233694616142366, 0.13233694616142366, 0.08781796531803687, 0.08781796531803687, 0.08781796531803687, 0.12131359600021774, 0.12131359600021774, 0.12131359600021774, 0.15118783046577833, 0.15118783046577833, 0.15118783046577833, 0.1315876710508912, 0.1315876710508912, 0.1315876710508912, 0.1256842034853637, 0.1256842034853637, 0.1256842034853637, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.015613376015341962, 0.015613376015341962, 0.015613376015341962, 0.06348628008509782, 0.06348628008509782, 0.06348628008509782, 0.2167579326396849, 0.2167579326396849, 0.2167579326396849, 0.2349078877534202, 0.2349078877534202, 0.2349078877534202, 0.5253652982472471, 0.5253652982472471, 0.5253652982472471, 0.04451652761113345, 0.04451652761113345, 0.04451652761113345, 0.1024304225623921, 0.1024304225623921, 0.1024304225623921, 0.18289120643451884, 0.18289120643451884, 0.18289120643451884, 0.17757247285435862, 0.17757247285435862, 0.17757247285435862, 0.18569096270455365, 0.18569096270455365, 0.18569096270455365, 0.270899573595341, 0.270899573595341, 0.270899573595341, 0.5796476966545081, 0.5796476966545081, 0.5796476966545081, 0.5087363546109619, 0.5087363546109619, 0.5087363546109619, 0.5459100039062216, 0.5459100039062216, 0.5459100039062216, 0.08535240446130943, 0.08535240446130943, 0.08535240446130943, 0.07356035276132311, 0.07356035276132311, 0.07356035276132311, 0.05727254799191683, 0.05727254799191683, 0.05727254799191683, 0.18616749733741433, 0.18616749733741433, 0.18616749733741433, 0.2522625076609373, 0.2522625076609373, 0.2522625076609373, 0.2333528420209755, 0.2333528420209755, 0.2333528420209755, 0.2711349095509601, 0.2711349095509601, 0.2711349095509601, 0.4119320601537232, 0.4119320601537232, 0.4119320601537232, 0.2011107881828319, 0.2011107881828319, 0.2011107881828319, 0.25373640507951833, 0.25373640507951833, 0.25373640507951833, 0.14102066452975304, 0.14102066452975304, 0.14102066452975304, 0.13177254753958223, 0.13177254753958223, 0.13177254753958223, 0.21871522686147926, 0.21871522686147926, 0.21871522686147926, 0.22800241336751603, 0.22800241336751603, 0.22800241336751603, 0.22642111368136242, 0.22642111368136242, 0.22642111368136242, 0.18939421987477667, 0.18939421987477667, 0.18939421987477667, 0.17532536458645043, 0.17532536458645043, 0.17532536458645043, 0.19729198941379522, 0.19729198941379522, 0.19729198941379522, 0.7144551409791454, 0.7144551409791454, 0.7144551409791454, 0.18883939777303727, 0.18883939777303727, 0.18883939777303727, 0.7419333238254875, 0.7419333238254875, 0.7419333238254875, 0.382646355783097, 0.382646355783097, 0.382646355783097, 0.19933351641918018, 0.19933351641918018, 0.19933351641918018, 0.4826961049052948, 0.4826961049052948, 0.4826961049052948, 0.1915559962106098, 0.1915559962106098, 0.1915559962106098, 0.23517183855747403, 0.23517183855747403, 0.23517183855747403, 0.19238856119414083, 0.19238856119414083, 0.19238856119414083, 0.0877198066400795, 0.0877198066400795, 0.0877198066400795, 0.1116867896885797, 0.1116867896885797, 0.1116867896885797, 0.10891879109645541, 0.10891879109645541, 0.10891879109645541]}, "mutation_prompt": null}
{"id": "53c6f20f-5a07-4dab-a87b-1df90caefb12", "solution": "import numpy as np\n\nclass PSO_SA_Optimizer_Enhanced_V2:\n    def __init__(self, budget, dim, pop_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.w_max = 0.9\n        self.w_min = 0.4\n        self.c1_init = 2.0\n        self.c2_init = 1.0\n        self.temp_init = 1.0\n\n    def __call__(self, func):\n        np.random.seed(42)\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.array([func(pos) for pos in positions])\n        global_best_position = personal_best_positions[np.argmin(personal_best_scores)]\n        global_best_score = np.min(personal_best_scores)\n        \n        eval_count = self.pop_size\n        temperature = self.temp_init\n        \n        while eval_count < self.budget:\n            w = self.w_max - ((self.w_max - self.w_min) * (eval_count / self.budget))\n            c1 = self.c1_init * (1 - eval_count / self.budget)\n            c2 = self.c2_init * (eval_count / self.budget)\n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                velocities[i] = (w * velocities[i]\n                                 + c1 * r1 * (personal_best_positions[i] - positions[i])\n                                 + c2 * r2 * (global_best_position - positions[i]))\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], self.lower_bound, self.upper_bound)\n\n                score = func(positions[i])\n                eval_count += 1\n                if eval_count >= self.budget:\n                    break\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = positions[i]\n\n                if np.random.rand() < np.exp(-(score - global_best_score) / temperature):\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n                    if score < global_best_score:\n                        global_best_score = score\n                        global_best_position = positions[i]\n\n            temperature *= 0.9  # Enhanced annealing factor for faster convergence\n\n        return global_best_position, global_best_score", "name": "PSO_SA_Optimizer_Enhanced_V2", "description": "Enhanced PSO-SA with adaptive temperature and improved velocity update for better exploration-exploitation balance.", "configspace": "", "generation": 42, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "8275fdc9-dd69-4f35-8938-1476f28cfb9b", "metadata": {"aucs": [0.7648824746567023, 0.7648824746567023, 0.7648824746567023, 0.5989209008757856, 0.5989209008757856, 0.5989209008757856, 0.6742174754797561, 0.6742174754797561, 0.6742174754797561, 0.15340181758075755, 0.15340181758075755, 0.15340181758075755, 0.03196945575794474, 0.03196945575794474, 0.03196945575794474, 0.028430204481377896, 0.028430204481377896, 0.028430204481377896, 0.09721927887848547, 0.09721927887848547, 0.09721927887848547, 0.10055554133996336, 0.10055554133996336, 0.10055554133996336, 0.11442808809856964, 0.11442808809856964, 0.11442808809856964, 0.09106273135940757, 0.09106273135940757, 0.09106273135940757, 0.08133700413676104, 0.08133700413676104, 0.08133700413676104, 0.10219557668092238, 0.10219557668092238, 0.10219557668092238, 0.5924351363112623, 0.5924351363112623, 0.5924351363112623, 0.9047980793814148, 0.9047980793814148, 0.9047980793814148, 0.8502442090048321, 0.8502442090048321, 0.8502442090048321, 0.2454031538157342, 0.2454031538157342, 0.2454031538157342, 0.47754369891190485, 0.47754369891190485, 0.47754369891190485, 0.22604147488547977, 0.22604147488547977, 0.22604147488547977, 0.22058670269023306, 0.22058670269023306, 0.22058670269023306, 0.1872551853639175, 0.1872551853639175, 0.1872551853639175, 0.22519173578017293, 0.22519173578017293, 0.22519173578017293, 0.13233694616142366, 0.13233694616142366, 0.13233694616142366, 0.08781796531803687, 0.08781796531803687, 0.08781796531803687, 0.12131359600021774, 0.12131359600021774, 0.12131359600021774, 0.15118783046577833, 0.15118783046577833, 0.15118783046577833, 0.1315876710508912, 0.1315876710508912, 0.1315876710508912, 0.1256842034853637, 0.1256842034853637, 0.1256842034853637, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.015613376015341962, 0.015613376015341962, 0.015613376015341962, 0.06348628008509782, 0.06348628008509782, 0.06348628008509782, 0.2167579326396849, 0.2167579326396849, 0.2167579326396849, 0.2349078877534202, 0.2349078877534202, 0.2349078877534202, 0.5253652982472471, 0.5253652982472471, 0.5253652982472471, 0.04451652761113345, 0.04451652761113345, 0.04451652761113345, 0.1024304225623921, 0.1024304225623921, 0.1024304225623921, 0.18289120643451884, 0.18289120643451884, 0.18289120643451884, 0.17757247285435862, 0.17757247285435862, 0.17757247285435862, 0.18569096270455365, 0.18569096270455365, 0.18569096270455365, 0.270899573595341, 0.270899573595341, 0.270899573595341, 0.5796476966545081, 0.5796476966545081, 0.5796476966545081, 0.5087363546109619, 0.5087363546109619, 0.5087363546109619, 0.5459100039062216, 0.5459100039062216, 0.5459100039062216, 0.08535240446130943, 0.08535240446130943, 0.08535240446130943, 0.07356035276132311, 0.07356035276132311, 0.07356035276132311, 0.05727254799191683, 0.05727254799191683, 0.05727254799191683, 0.18616749733741433, 0.18616749733741433, 0.18616749733741433, 0.2522625076609373, 0.2522625076609373, 0.2522625076609373, 0.2333528420209755, 0.2333528420209755, 0.2333528420209755, 0.2711349095509601, 0.2711349095509601, 0.2711349095509601, 0.4119320601537232, 0.4119320601537232, 0.4119320601537232, 0.2011107881828319, 0.2011107881828319, 0.2011107881828319, 0.25373640507951833, 0.25373640507951833, 0.25373640507951833, 0.14102066452975304, 0.14102066452975304, 0.14102066452975304, 0.13177254753958223, 0.13177254753958223, 0.13177254753958223, 0.21871522686147926, 0.21871522686147926, 0.21871522686147926, 0.22800241336751603, 0.22800241336751603, 0.22800241336751603, 0.22642111368136242, 0.22642111368136242, 0.22642111368136242, 0.18939421987477667, 0.18939421987477667, 0.18939421987477667, 0.17532536458645043, 0.17532536458645043, 0.17532536458645043, 0.19729198941379522, 0.19729198941379522, 0.19729198941379522, 0.7144551409791454, 0.7144551409791454, 0.7144551409791454, 0.18883939777303727, 0.18883939777303727, 0.18883939777303727, 0.7419333238254875, 0.7419333238254875, 0.7419333238254875, 0.382646355783097, 0.382646355783097, 0.382646355783097, 0.19933351641918018, 0.19933351641918018, 0.19933351641918018, 0.4826961049052948, 0.4826961049052948, 0.4826961049052948, 0.1915559962106098, 0.1915559962106098, 0.1915559962106098, 0.23517183855747403, 0.23517183855747403, 0.23517183855747403, 0.19238856119414083, 0.19238856119414083, 0.19238856119414083, 0.0877198066400795, 0.0877198066400795, 0.0877198066400795, 0.1116867896885797, 0.1116867896885797, 0.1116867896885797, 0.10891879109645541, 0.10891879109645541, 0.10891879109645541]}, "mutation_prompt": null}
{"id": "104cf98e-8138-4906-b55a-546d6de8214b", "solution": "import numpy as np\n\nclass PSO_SA_Optimizer_Enhanced_V3:\n    def __init__(self, budget, dim, pop_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.w_max = 0.9\n        self.w_min = 0.4\n        self.c1_init = 1.5\n        self.c2_init = 1.5\n        self.temp_init = 1.0\n\n    def __call__(self, func):\n        np.random.seed(42)\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.array([func(pos) for pos in positions])\n        global_best_position = personal_best_positions[np.argmin(personal_best_scores)]\n        global_best_score = np.min(personal_best_scores)\n        \n        eval_count = self.pop_size\n        temperature = self.temp_init\n        \n        while eval_count < self.budget:\n            w = self.w_max - ((self.w_max - self.w_min) * (eval_count / self.budget))\n            c1 = self.c1_init * (1 - eval_count / self.budget) + 0.5\n            c2 = self.c2_init * (eval_count / self.budget) + 0.5\n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                velocities[i] = (w * velocities[i]\n                                 + c1 * r1 * (personal_best_positions[i] - positions[i])\n                                 + c2 * r2 * (global_best_position - positions[i]))\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], self.lower_bound, self.upper_bound)\n\n                score = func(positions[i])\n                eval_count += 1\n                if eval_count >= self.budget:\n                    break\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = positions[i]\n\n                if np.random.rand() < np.exp(-(score - global_best_score) / temperature):\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n                    if score < global_best_score:\n                        global_best_score = score\n                        global_best_position = positions[i]\n\n            temperature *= 0.95  # Improved annealing factor for convergence\n\n        return global_best_position, global_best_score", "name": "PSO_SA_Optimizer_Enhanced_V3", "description": "Improved adaptive search strategy in PSO-SA with dynamically updating parameters and better temperature decay.", "configspace": "", "generation": 54, "fitness": 0.26024368715690044, "feedback": "The algorithm PSO_SA_Optimizer_Enhanced_V3 got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.26 with standard deviation 0.21.", "error": "", "parent_id": "8275fdc9-dd69-4f35-8938-1476f28cfb9b", "metadata": {"aucs": [0.5051733381719483, 0.5051733381719483, 0.5051733381719483, 0.4864875265041222, 0.4864875265041222, 0.4864875265041222, 0.5980878545878154, 0.5980878545878154, 0.5980878545878154, 0.06730631129871123, 0.06730631129871123, 0.06730631129871123, 0.09301964064620494, 0.09301964064620494, 0.09301964064620494, 0.36676061687109374, 0.36676061687109374, 0.36676061687109374, 0.1093498751795694, 0.1093498751795694, 0.1093498751795694, 0.11800704322385025, 0.11800704322385025, 0.11800704322385025, 0.11014226708156238, 0.11014226708156238, 0.11014226708156238, 0.15004683892585147, 0.15004683892585147, 0.15004683892585147, 0.13033125670061696, 0.13033125670061696, 0.13033125670061696, 0.1176642388246808, 0.1176642388246808, 0.1176642388246808, 0.9817783524568978, 0.9817783524568978, 0.9817783524568978, 0.98749484089464, 0.98749484089464, 0.98749484089464, 0.9873373389794495, 0.9873373389794495, 0.9873373389794495, 0.3193023035569802, 0.3193023035569802, 0.3193023035569802, 0.4749253967860462, 0.4749253967860462, 0.4749253967860462, 0.33952636113240786, 0.33952636113240786, 0.33952636113240786, 0.23404906009051418, 0.23404906009051418, 0.23404906009051418, 0.1867241095540818, 0.1867241095540818, 0.1867241095540818, 0.22360410437078448, 0.22360410437078448, 0.22360410437078448, 0.38917833239145483, 0.38917833239145483, 0.38917833239145483, 0.17677793426833743, 0.17677793426833743, 0.17677793426833743, 0.1866522982738108, 0.1866522982738108, 0.1866522982738108, 0.1794288919186895, 0.1794288919186895, 0.1794288919186895, 0.1713064460088295, 0.1713064460088295, 0.1713064460088295, 0.1503593130339168, 0.1503593130339168, 0.1503593130339168, 0.16896150548229494, 0.16896150548229494, 0.16896150548229494, 0.1517238337156679, 0.1517238337156679, 0.1517238337156679, 0.07801075606316066, 0.07801075606316066, 0.07801075606316066, 0.1306258192704034, 0.1306258192704034, 0.1306258192704034, 0.043036206734873295, 0.043036206734873295, 0.043036206734873295, 0.1713736095711984, 0.1713736095711984, 0.1713736095711984, 0.03478423785365725, 0.03478423785365725, 0.03478423785365725, 0.19180632709776668, 0.19180632709776668, 0.19180632709776668, 0.0590246198988168, 0.0590246198988168, 0.0590246198988168, 0.20081792517374752, 0.20081792517374752, 0.20081792517374752, 0.033491506494330636, 0.033491506494330636, 0.033491506494330636, 0.05021043193870478, 0.05021043193870478, 0.05021043193870478, 0.5003727892349492, 0.5003727892349492, 0.5003727892349492, 0.4657333813986042, 0.4657333813986042, 0.4657333813986042, 0.46917186793370924, 0.46917186793370924, 0.46917186793370924, 0.14342207768301163, 0.14342207768301163, 0.14342207768301163, 0.12145871132800168, 0.12145871132800168, 0.12145871132800168, 0.1057475206242896, 0.1057475206242896, 0.1057475206242896, 0.35642057844886454, 0.35642057844886454, 0.35642057844886454, 0.21465072589842016, 0.21465072589842016, 0.21465072589842016, 0.2444189364073357, 0.2444189364073357, 0.2444189364073357, 0.286894944324837, 0.286894944324837, 0.286894944324837, 0.28791410732075406, 0.28791410732075406, 0.28791410732075406, 0.288665474020558, 0.288665474020558, 0.288665474020558, 0.22945855198095, 0.22945855198095, 0.22945855198095, 0.19982102082297581, 0.19982102082297581, 0.19982102082297581, 0.12103828021249086, 0.12103828021249086, 0.12103828021249086, 0.1874220403361898, 0.1874220403361898, 0.1874220403361898, 0.24617784072219195, 0.24617784072219195, 0.24617784072219195, 0.2752280251020164, 0.2752280251020164, 0.2752280251020164, 0.1880647070348399, 0.1880647070348399, 0.1880647070348399, 0.1738231871514354, 0.1738231871514354, 0.1738231871514354, 0.48468137841406633, 0.48468137841406633, 0.48468137841406633, 0.6910667162082013, 0.6910667162082013, 0.6910667162082013, 0.15879721238597644, 0.15879721238597644, 0.15879721238597644, 0.746649682486173, 0.746649682486173, 0.746649682486173, 0.1689669226231103, 0.1689669226231103, 0.1689669226231103, 0.21053313703713594, 0.21053313703713594, 0.21053313703713594, 0.16560864026155453, 0.16560864026155453, 0.16560864026155453, 0.19735138827661058, 0.19735138827661058, 0.19735138827661058, 0.17825965747894357, 0.17825965747894357, 0.17825965747894357, 0.20460318845723502, 0.20460318845723502, 0.20460318845723502, 0.08741913219627273, 0.08741913219627273, 0.08741913219627273, 0.10170633761050629, 0.10170633761050629, 0.10170633761050629, 0.08130864284712969, 0.08130864284712969, 0.08130864284712969]}, "mutation_prompt": null}
{"id": "a2d01d4e-a400-467b-b0d1-3ea6da7bf7ac", "solution": "import numpy as np\n\nclass PSO_SA_Optimizer_Enhanced_V3:\n    def __init__(self, budget, dim, pop_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.w_max = 0.9\n        self.w_min = 0.4\n        self.c1_init = 1.5\n        self.c2_init = 1.5\n        self.temp_init = 1.0\n\n    def __call__(self, func):\n        np.random.seed(42)\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.array([func(pos) for pos in positions])\n        global_best_position = personal_best_positions[np.argmin(personal_best_scores)]\n        global_best_score = np.min(personal_best_scores)\n        \n        eval_count = self.pop_size\n        temperature = self.temp_init\n        \n        while eval_count < self.budget:\n            w = self.w_max - ((self.w_max - self.w_min) * (eval_count / self.budget))\n            c1 = self.c1_init * (1 - eval_count / self.budget) + 0.5\n            c2 = self.c2_init * (eval_count / self.budget) + 0.5\n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                velocities[i] = (w * velocities[i]\n                                 + c1 * r1 * (personal_best_positions[i] - positions[i])\n                                 + c2 * r2 * (global_best_position - positions[i]))\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], self.lower_bound, self.upper_bound)\n\n                score = func(positions[i])\n                eval_count += 1\n                if eval_count >= self.budget:\n                    break\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = positions[i]\n\n                if np.random.rand() < np.exp(-(score - global_best_score) / temperature):\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n                    if score < global_best_score:\n                        global_best_score = score\n                        global_best_position = positions[i]\n\n            temperature *= 0.95  # Improved annealing factor for convergence\n\n        return global_best_position, global_best_score", "name": "PSO_SA_Optimizer_Enhanced_V3", "description": "Improved adaptive search strategy in PSO-SA with dynamically updating parameters and better temperature decay.", "configspace": "", "generation": 55, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "104cf98e-8138-4906-b55a-546d6de8214b", "metadata": {"aucs": [0.5051733381719483, 0.5051733381719483, 0.5051733381719483, 0.4864875265041222, 0.4864875265041222, 0.4864875265041222, 0.5980878545878154, 0.5980878545878154, 0.5980878545878154, 0.06730631129871123, 0.06730631129871123, 0.06730631129871123, 0.09301964064620494, 0.09301964064620494, 0.09301964064620494, 0.36676061687109374, 0.36676061687109374, 0.36676061687109374, 0.1093498751795694, 0.1093498751795694, 0.1093498751795694, 0.11800704322385025, 0.11800704322385025, 0.11800704322385025, 0.11014226708156238, 0.11014226708156238, 0.11014226708156238, 0.15004683892585147, 0.15004683892585147, 0.15004683892585147, 0.13033125670061696, 0.13033125670061696, 0.13033125670061696, 0.1176642388246808, 0.1176642388246808, 0.1176642388246808, 0.9817783524568978, 0.9817783524568978, 0.9817783524568978, 0.98749484089464, 0.98749484089464, 0.98749484089464, 0.9873373389794495, 0.9873373389794495, 0.9873373389794495, 0.3193023035569802, 0.3193023035569802, 0.3193023035569802, 0.4749253967860462, 0.4749253967860462, 0.4749253967860462, 0.33952636113240786, 0.33952636113240786, 0.33952636113240786, 0.23404906009051418, 0.23404906009051418, 0.23404906009051418, 0.1867241095540818, 0.1867241095540818, 0.1867241095540818, 0.22360410437078448, 0.22360410437078448, 0.22360410437078448, 0.38917833239145483, 0.38917833239145483, 0.38917833239145483, 0.17677793426833743, 0.17677793426833743, 0.17677793426833743, 0.1866522982738108, 0.1866522982738108, 0.1866522982738108, 0.1794288919186895, 0.1794288919186895, 0.1794288919186895, 0.1713064460088295, 0.1713064460088295, 0.1713064460088295, 0.1503593130339168, 0.1503593130339168, 0.1503593130339168, 0.16896150548229494, 0.16896150548229494, 0.16896150548229494, 0.1517238337156679, 0.1517238337156679, 0.1517238337156679, 0.07801075606316066, 0.07801075606316066, 0.07801075606316066, 0.1306258192704034, 0.1306258192704034, 0.1306258192704034, 0.043036206734873295, 0.043036206734873295, 0.043036206734873295, 0.1713736095711984, 0.1713736095711984, 0.1713736095711984, 0.03478423785365725, 0.03478423785365725, 0.03478423785365725, 0.19180632709776668, 0.19180632709776668, 0.19180632709776668, 0.0590246198988168, 0.0590246198988168, 0.0590246198988168, 0.20081792517374752, 0.20081792517374752, 0.20081792517374752, 0.033491506494330636, 0.033491506494330636, 0.033491506494330636, 0.05021043193870478, 0.05021043193870478, 0.05021043193870478, 0.5003727892349492, 0.5003727892349492, 0.5003727892349492, 0.4657333813986042, 0.4657333813986042, 0.4657333813986042, 0.46917186793370924, 0.46917186793370924, 0.46917186793370924, 0.14342207768301163, 0.14342207768301163, 0.14342207768301163, 0.12145871132800168, 0.12145871132800168, 0.12145871132800168, 0.1057475206242896, 0.1057475206242896, 0.1057475206242896, 0.35642057844886454, 0.35642057844886454, 0.35642057844886454, 0.21465072589842016, 0.21465072589842016, 0.21465072589842016, 0.2444189364073357, 0.2444189364073357, 0.2444189364073357, 0.286894944324837, 0.286894944324837, 0.286894944324837, 0.28791410732075406, 0.28791410732075406, 0.28791410732075406, 0.288665474020558, 0.288665474020558, 0.288665474020558, 0.22945855198095, 0.22945855198095, 0.22945855198095, 0.19982102082297581, 0.19982102082297581, 0.19982102082297581, 0.12103828021249086, 0.12103828021249086, 0.12103828021249086, 0.1874220403361898, 0.1874220403361898, 0.1874220403361898, 0.24617784072219195, 0.24617784072219195, 0.24617784072219195, 0.2752280251020164, 0.2752280251020164, 0.2752280251020164, 0.1880647070348399, 0.1880647070348399, 0.1880647070348399, 0.1738231871514354, 0.1738231871514354, 0.1738231871514354, 0.48468137841406633, 0.48468137841406633, 0.48468137841406633, 0.6910667162082013, 0.6910667162082013, 0.6910667162082013, 0.15879721238597644, 0.15879721238597644, 0.15879721238597644, 0.746649682486173, 0.746649682486173, 0.746649682486173, 0.1689669226231103, 0.1689669226231103, 0.1689669226231103, 0.21053313703713594, 0.21053313703713594, 0.21053313703713594, 0.16560864026155453, 0.16560864026155453, 0.16560864026155453, 0.19735138827661058, 0.19735138827661058, 0.19735138827661058, 0.17825965747894357, 0.17825965747894357, 0.17825965747894357, 0.20460318845723502, 0.20460318845723502, 0.20460318845723502, 0.08741913219627273, 0.08741913219627273, 0.08741913219627273, 0.10170633761050629, 0.10170633761050629, 0.10170633761050629, 0.08130864284712969, 0.08130864284712969, 0.08130864284712969]}, "mutation_prompt": null}
{"id": "e9e9a5d8-d881-476c-9886-c197e6f54c45", "solution": "import numpy as np\n\nclass PSO_SA_Optimizer_Enhanced_V3:\n    def __init__(self, budget, dim, pop_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.w_max = 0.9\n        self.w_min = 0.4\n        self.c1_init = 1.5\n        self.c2_init = 1.5\n        self.temp_init = 1.0\n\n    def __call__(self, func):\n        np.random.seed(42)\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.array([func(pos) for pos in positions])\n        global_best_position = personal_best_positions[np.argmin(personal_best_scores)]\n        global_best_score = np.min(personal_best_scores)\n        \n        eval_count = self.pop_size\n        temperature = self.temp_init\n        \n        while eval_count < self.budget:\n            w = self.w_max - ((self.w_max - self.w_min) * (eval_count / self.budget))\n            c1 = self.c1_init * (1 - eval_count / self.budget) + 0.5\n            c2 = self.c2_init * (eval_count / self.budget) + 0.5\n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                velocities[i] = (w * velocities[i]\n                                 + c1 * r1 * (personal_best_positions[i] - positions[i])\n                                 + c2 * r2 * (global_best_position - positions[i]))\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], self.lower_bound, self.upper_bound)\n\n                score = func(positions[i])\n                eval_count += 1\n                if eval_count >= self.budget:\n                    break\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = positions[i]\n\n                if np.random.rand() < np.exp(-(score - global_best_score) / temperature):\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n                    if score < global_best_score:\n                        global_best_score = score\n                        global_best_position = positions[i]\n\n            temperature *= 0.95  # Improved annealing factor for convergence\n\n        return global_best_position, global_best_score", "name": "PSO_SA_Optimizer_Enhanced_V3", "description": "Improved adaptive search strategy in PSO-SA with dynamically updating parameters and better temperature decay.", "configspace": "", "generation": 55, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "104cf98e-8138-4906-b55a-546d6de8214b", "metadata": {"aucs": [0.5051733381719483, 0.5051733381719483, 0.5051733381719483, 0.4864875265041222, 0.4864875265041222, 0.4864875265041222, 0.5980878545878154, 0.5980878545878154, 0.5980878545878154, 0.06730631129871123, 0.06730631129871123, 0.06730631129871123, 0.09301964064620494, 0.09301964064620494, 0.09301964064620494, 0.36676061687109374, 0.36676061687109374, 0.36676061687109374, 0.1093498751795694, 0.1093498751795694, 0.1093498751795694, 0.11800704322385025, 0.11800704322385025, 0.11800704322385025, 0.11014226708156238, 0.11014226708156238, 0.11014226708156238, 0.15004683892585147, 0.15004683892585147, 0.15004683892585147, 0.13033125670061696, 0.13033125670061696, 0.13033125670061696, 0.1176642388246808, 0.1176642388246808, 0.1176642388246808, 0.9817783524568978, 0.9817783524568978, 0.9817783524568978, 0.98749484089464, 0.98749484089464, 0.98749484089464, 0.9873373389794495, 0.9873373389794495, 0.9873373389794495, 0.3193023035569802, 0.3193023035569802, 0.3193023035569802, 0.4749253967860462, 0.4749253967860462, 0.4749253967860462, 0.33952636113240786, 0.33952636113240786, 0.33952636113240786, 0.23404906009051418, 0.23404906009051418, 0.23404906009051418, 0.1867241095540818, 0.1867241095540818, 0.1867241095540818, 0.22360410437078448, 0.22360410437078448, 0.22360410437078448, 0.38917833239145483, 0.38917833239145483, 0.38917833239145483, 0.17677793426833743, 0.17677793426833743, 0.17677793426833743, 0.1866522982738108, 0.1866522982738108, 0.1866522982738108, 0.1794288919186895, 0.1794288919186895, 0.1794288919186895, 0.1713064460088295, 0.1713064460088295, 0.1713064460088295, 0.1503593130339168, 0.1503593130339168, 0.1503593130339168, 0.16896150548229494, 0.16896150548229494, 0.16896150548229494, 0.1517238337156679, 0.1517238337156679, 0.1517238337156679, 0.07801075606316066, 0.07801075606316066, 0.07801075606316066, 0.1306258192704034, 0.1306258192704034, 0.1306258192704034, 0.043036206734873295, 0.043036206734873295, 0.043036206734873295, 0.1713736095711984, 0.1713736095711984, 0.1713736095711984, 0.03478423785365725, 0.03478423785365725, 0.03478423785365725, 0.19180632709776668, 0.19180632709776668, 0.19180632709776668, 0.0590246198988168, 0.0590246198988168, 0.0590246198988168, 0.20081792517374752, 0.20081792517374752, 0.20081792517374752, 0.033491506494330636, 0.033491506494330636, 0.033491506494330636, 0.05021043193870478, 0.05021043193870478, 0.05021043193870478, 0.5003727892349492, 0.5003727892349492, 0.5003727892349492, 0.4657333813986042, 0.4657333813986042, 0.4657333813986042, 0.46917186793370924, 0.46917186793370924, 0.46917186793370924, 0.14342207768301163, 0.14342207768301163, 0.14342207768301163, 0.12145871132800168, 0.12145871132800168, 0.12145871132800168, 0.1057475206242896, 0.1057475206242896, 0.1057475206242896, 0.35642057844886454, 0.35642057844886454, 0.35642057844886454, 0.21465072589842016, 0.21465072589842016, 0.21465072589842016, 0.2444189364073357, 0.2444189364073357, 0.2444189364073357, 0.286894944324837, 0.286894944324837, 0.286894944324837, 0.28791410732075406, 0.28791410732075406, 0.28791410732075406, 0.288665474020558, 0.288665474020558, 0.288665474020558, 0.22945855198095, 0.22945855198095, 0.22945855198095, 0.19982102082297581, 0.19982102082297581, 0.19982102082297581, 0.12103828021249086, 0.12103828021249086, 0.12103828021249086, 0.1874220403361898, 0.1874220403361898, 0.1874220403361898, 0.24617784072219195, 0.24617784072219195, 0.24617784072219195, 0.2752280251020164, 0.2752280251020164, 0.2752280251020164, 0.1880647070348399, 0.1880647070348399, 0.1880647070348399, 0.1738231871514354, 0.1738231871514354, 0.1738231871514354, 0.48468137841406633, 0.48468137841406633, 0.48468137841406633, 0.6910667162082013, 0.6910667162082013, 0.6910667162082013, 0.15879721238597644, 0.15879721238597644, 0.15879721238597644, 0.746649682486173, 0.746649682486173, 0.746649682486173, 0.1689669226231103, 0.1689669226231103, 0.1689669226231103, 0.21053313703713594, 0.21053313703713594, 0.21053313703713594, 0.16560864026155453, 0.16560864026155453, 0.16560864026155453, 0.19735138827661058, 0.19735138827661058, 0.19735138827661058, 0.17825965747894357, 0.17825965747894357, 0.17825965747894357, 0.20460318845723502, 0.20460318845723502, 0.20460318845723502, 0.08741913219627273, 0.08741913219627273, 0.08741913219627273, 0.10170633761050629, 0.10170633761050629, 0.10170633761050629, 0.08130864284712969, 0.08130864284712969, 0.08130864284712969]}, "mutation_prompt": null}
{"id": "ea103922-b7c4-4ad1-82aa-8b6fb3cb0c0a", "solution": "import numpy as np\n\nclass PSO_SA_Optimizer_Enhanced_V3:\n    def __init__(self, budget, dim, pop_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.w_max = 0.9\n        self.w_min = 0.4\n        self.c1_init = 1.5\n        self.c2_init = 1.5\n        self.temp_init = 1.0\n\n    def __call__(self, func):\n        np.random.seed(42)\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.array([func(pos) for pos in positions])\n        global_best_position = personal_best_positions[np.argmin(personal_best_scores)]\n        global_best_score = np.min(personal_best_scores)\n        \n        eval_count = self.pop_size\n        temperature = self.temp_init\n        \n        while eval_count < self.budget:\n            w = self.w_max - ((self.w_max - self.w_min) * (eval_count / self.budget))\n            c1 = self.c1_init * (1 - eval_count / self.budget) + 0.5\n            c2 = self.c2_init * (eval_count / self.budget) + 0.5\n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                velocities[i] = (w * velocities[i]\n                                 + c1 * r1 * (personal_best_positions[i] - positions[i])\n                                 + c2 * r2 * (global_best_position - positions[i]))\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], self.lower_bound, self.upper_bound)\n\n                score = func(positions[i])\n                eval_count += 1\n                if eval_count >= self.budget:\n                    break\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = positions[i]\n\n                if np.random.rand() < np.exp(-(score - global_best_score) / temperature):\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n                    if score < global_best_score:\n                        global_best_score = score\n                        global_best_position = positions[i]\n\n            temperature *= 0.95  # Improved annealing factor for convergence\n\n        return global_best_position, global_best_score", "name": "PSO_SA_Optimizer_Enhanced_V3", "description": "Improved adaptive search strategy in PSO-SA with dynamically updating parameters and better temperature decay.", "configspace": "", "generation": 55, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "104cf98e-8138-4906-b55a-546d6de8214b", "metadata": {"aucs": [0.5051733381719483, 0.5051733381719483, 0.5051733381719483, 0.4864875265041222, 0.4864875265041222, 0.4864875265041222, 0.5980878545878154, 0.5980878545878154, 0.5980878545878154, 0.06730631129871123, 0.06730631129871123, 0.06730631129871123, 0.09301964064620494, 0.09301964064620494, 0.09301964064620494, 0.36676061687109374, 0.36676061687109374, 0.36676061687109374, 0.1093498751795694, 0.1093498751795694, 0.1093498751795694, 0.11800704322385025, 0.11800704322385025, 0.11800704322385025, 0.11014226708156238, 0.11014226708156238, 0.11014226708156238, 0.15004683892585147, 0.15004683892585147, 0.15004683892585147, 0.13033125670061696, 0.13033125670061696, 0.13033125670061696, 0.1176642388246808, 0.1176642388246808, 0.1176642388246808, 0.9817783524568978, 0.9817783524568978, 0.9817783524568978, 0.98749484089464, 0.98749484089464, 0.98749484089464, 0.9873373389794495, 0.9873373389794495, 0.9873373389794495, 0.3193023035569802, 0.3193023035569802, 0.3193023035569802, 0.4749253967860462, 0.4749253967860462, 0.4749253967860462, 0.33952636113240786, 0.33952636113240786, 0.33952636113240786, 0.23404906009051418, 0.23404906009051418, 0.23404906009051418, 0.1867241095540818, 0.1867241095540818, 0.1867241095540818, 0.22360410437078448, 0.22360410437078448, 0.22360410437078448, 0.38917833239145483, 0.38917833239145483, 0.38917833239145483, 0.17677793426833743, 0.17677793426833743, 0.17677793426833743, 0.1866522982738108, 0.1866522982738108, 0.1866522982738108, 0.1794288919186895, 0.1794288919186895, 0.1794288919186895, 0.1713064460088295, 0.1713064460088295, 0.1713064460088295, 0.1503593130339168, 0.1503593130339168, 0.1503593130339168, 0.16896150548229494, 0.16896150548229494, 0.16896150548229494, 0.1517238337156679, 0.1517238337156679, 0.1517238337156679, 0.07801075606316066, 0.07801075606316066, 0.07801075606316066, 0.1306258192704034, 0.1306258192704034, 0.1306258192704034, 0.043036206734873295, 0.043036206734873295, 0.043036206734873295, 0.1713736095711984, 0.1713736095711984, 0.1713736095711984, 0.03478423785365725, 0.03478423785365725, 0.03478423785365725, 0.19180632709776668, 0.19180632709776668, 0.19180632709776668, 0.0590246198988168, 0.0590246198988168, 0.0590246198988168, 0.20081792517374752, 0.20081792517374752, 0.20081792517374752, 0.033491506494330636, 0.033491506494330636, 0.033491506494330636, 0.05021043193870478, 0.05021043193870478, 0.05021043193870478, 0.5003727892349492, 0.5003727892349492, 0.5003727892349492, 0.4657333813986042, 0.4657333813986042, 0.4657333813986042, 0.46917186793370924, 0.46917186793370924, 0.46917186793370924, 0.14342207768301163, 0.14342207768301163, 0.14342207768301163, 0.12145871132800168, 0.12145871132800168, 0.12145871132800168, 0.1057475206242896, 0.1057475206242896, 0.1057475206242896, 0.35642057844886454, 0.35642057844886454, 0.35642057844886454, 0.21465072589842016, 0.21465072589842016, 0.21465072589842016, 0.2444189364073357, 0.2444189364073357, 0.2444189364073357, 0.286894944324837, 0.286894944324837, 0.286894944324837, 0.28791410732075406, 0.28791410732075406, 0.28791410732075406, 0.288665474020558, 0.288665474020558, 0.288665474020558, 0.22945855198095, 0.22945855198095, 0.22945855198095, 0.19982102082297581, 0.19982102082297581, 0.19982102082297581, 0.12103828021249086, 0.12103828021249086, 0.12103828021249086, 0.1874220403361898, 0.1874220403361898, 0.1874220403361898, 0.24617784072219195, 0.24617784072219195, 0.24617784072219195, 0.2752280251020164, 0.2752280251020164, 0.2752280251020164, 0.1880647070348399, 0.1880647070348399, 0.1880647070348399, 0.1738231871514354, 0.1738231871514354, 0.1738231871514354, 0.48468137841406633, 0.48468137841406633, 0.48468137841406633, 0.6910667162082013, 0.6910667162082013, 0.6910667162082013, 0.15879721238597644, 0.15879721238597644, 0.15879721238597644, 0.746649682486173, 0.746649682486173, 0.746649682486173, 0.1689669226231103, 0.1689669226231103, 0.1689669226231103, 0.21053313703713594, 0.21053313703713594, 0.21053313703713594, 0.16560864026155453, 0.16560864026155453, 0.16560864026155453, 0.19735138827661058, 0.19735138827661058, 0.19735138827661058, 0.17825965747894357, 0.17825965747894357, 0.17825965747894357, 0.20460318845723502, 0.20460318845723502, 0.20460318845723502, 0.08741913219627273, 0.08741913219627273, 0.08741913219627273, 0.10170633761050629, 0.10170633761050629, 0.10170633761050629, 0.08130864284712969, 0.08130864284712969, 0.08130864284712969]}, "mutation_prompt": null}
{"id": "b0ef1d54-29e3-471a-9fe8-6e665a8c93cf", "solution": "import numpy as np\n\nclass PSO_SA_Optimizer_Enhanced_V3:\n    def __init__(self, budget, dim, pop_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.w_max = 0.9\n        self.w_min = 0.4\n        self.c1_init = 1.5\n        self.c2_init = 1.5\n        self.temp_init = 1.0\n\n    def __call__(self, func):\n        np.random.seed(42)\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.array([func(pos) for pos in positions])\n        global_best_position = personal_best_positions[np.argmin(personal_best_scores)]\n        global_best_score = np.min(personal_best_scores)\n        \n        eval_count = self.pop_size\n        temperature = self.temp_init\n        \n        while eval_count < self.budget:\n            w = self.w_max - ((self.w_max - self.w_min) * (eval_count / self.budget))\n            c1 = self.c1_init * (1 - eval_count / self.budget) + 0.5\n            c2 = self.c2_init * (eval_count / self.budget) + 0.5\n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                velocities[i] = (w * velocities[i]\n                                 + c1 * r1 * (personal_best_positions[i] - positions[i])\n                                 + c2 * r2 * (global_best_position - positions[i]))\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], self.lower_bound, self.upper_bound)\n\n                score = func(positions[i])\n                eval_count += 1\n                if eval_count >= self.budget:\n                    break\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = positions[i]\n\n                if np.random.rand() < np.exp(-(score - global_best_score) / temperature):\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n                    if score < global_best_score:\n                        global_best_score = score\n                        global_best_position = positions[i]\n\n            temperature *= 0.95  # Improved annealing factor for convergence\n\n        return global_best_position, global_best_score", "name": "PSO_SA_Optimizer_Enhanced_V3", "description": "Improved adaptive search strategy in PSO-SA with dynamically updating parameters and better temperature decay.", "configspace": "", "generation": 55, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "104cf98e-8138-4906-b55a-546d6de8214b", "metadata": {"aucs": [0.5051733381719483, 0.5051733381719483, 0.5051733381719483, 0.4864875265041222, 0.4864875265041222, 0.4864875265041222, 0.5980878545878154, 0.5980878545878154, 0.5980878545878154, 0.06730631129871123, 0.06730631129871123, 0.06730631129871123, 0.09301964064620494, 0.09301964064620494, 0.09301964064620494, 0.36676061687109374, 0.36676061687109374, 0.36676061687109374, 0.1093498751795694, 0.1093498751795694, 0.1093498751795694, 0.11800704322385025, 0.11800704322385025, 0.11800704322385025, 0.11014226708156238, 0.11014226708156238, 0.11014226708156238, 0.15004683892585147, 0.15004683892585147, 0.15004683892585147, 0.13033125670061696, 0.13033125670061696, 0.13033125670061696, 0.1176642388246808, 0.1176642388246808, 0.1176642388246808, 0.9817783524568978, 0.9817783524568978, 0.9817783524568978, 0.98749484089464, 0.98749484089464, 0.98749484089464, 0.9873373389794495, 0.9873373389794495, 0.9873373389794495, 0.3193023035569802, 0.3193023035569802, 0.3193023035569802, 0.4749253967860462, 0.4749253967860462, 0.4749253967860462, 0.33952636113240786, 0.33952636113240786, 0.33952636113240786, 0.23404906009051418, 0.23404906009051418, 0.23404906009051418, 0.1867241095540818, 0.1867241095540818, 0.1867241095540818, 0.22360410437078448, 0.22360410437078448, 0.22360410437078448, 0.38917833239145483, 0.38917833239145483, 0.38917833239145483, 0.17677793426833743, 0.17677793426833743, 0.17677793426833743, 0.1866522982738108, 0.1866522982738108, 0.1866522982738108, 0.1794288919186895, 0.1794288919186895, 0.1794288919186895, 0.1713064460088295, 0.1713064460088295, 0.1713064460088295, 0.1503593130339168, 0.1503593130339168, 0.1503593130339168, 0.16896150548229494, 0.16896150548229494, 0.16896150548229494, 0.1517238337156679, 0.1517238337156679, 0.1517238337156679, 0.07801075606316066, 0.07801075606316066, 0.07801075606316066, 0.1306258192704034, 0.1306258192704034, 0.1306258192704034, 0.043036206734873295, 0.043036206734873295, 0.043036206734873295, 0.1713736095711984, 0.1713736095711984, 0.1713736095711984, 0.03478423785365725, 0.03478423785365725, 0.03478423785365725, 0.19180632709776668, 0.19180632709776668, 0.19180632709776668, 0.0590246198988168, 0.0590246198988168, 0.0590246198988168, 0.20081792517374752, 0.20081792517374752, 0.20081792517374752, 0.033491506494330636, 0.033491506494330636, 0.033491506494330636, 0.05021043193870478, 0.05021043193870478, 0.05021043193870478, 0.5003727892349492, 0.5003727892349492, 0.5003727892349492, 0.4657333813986042, 0.4657333813986042, 0.4657333813986042, 0.46917186793370924, 0.46917186793370924, 0.46917186793370924, 0.14342207768301163, 0.14342207768301163, 0.14342207768301163, 0.12145871132800168, 0.12145871132800168, 0.12145871132800168, 0.1057475206242896, 0.1057475206242896, 0.1057475206242896, 0.35642057844886454, 0.35642057844886454, 0.35642057844886454, 0.21465072589842016, 0.21465072589842016, 0.21465072589842016, 0.2444189364073357, 0.2444189364073357, 0.2444189364073357, 0.286894944324837, 0.286894944324837, 0.286894944324837, 0.28791410732075406, 0.28791410732075406, 0.28791410732075406, 0.288665474020558, 0.288665474020558, 0.288665474020558, 0.22945855198095, 0.22945855198095, 0.22945855198095, 0.19982102082297581, 0.19982102082297581, 0.19982102082297581, 0.12103828021249086, 0.12103828021249086, 0.12103828021249086, 0.1874220403361898, 0.1874220403361898, 0.1874220403361898, 0.24617784072219195, 0.24617784072219195, 0.24617784072219195, 0.2752280251020164, 0.2752280251020164, 0.2752280251020164, 0.1880647070348399, 0.1880647070348399, 0.1880647070348399, 0.1738231871514354, 0.1738231871514354, 0.1738231871514354, 0.48468137841406633, 0.48468137841406633, 0.48468137841406633, 0.6910667162082013, 0.6910667162082013, 0.6910667162082013, 0.15879721238597644, 0.15879721238597644, 0.15879721238597644, 0.746649682486173, 0.746649682486173, 0.746649682486173, 0.1689669226231103, 0.1689669226231103, 0.1689669226231103, 0.21053313703713594, 0.21053313703713594, 0.21053313703713594, 0.16560864026155453, 0.16560864026155453, 0.16560864026155453, 0.19735138827661058, 0.19735138827661058, 0.19735138827661058, 0.17825965747894357, 0.17825965747894357, 0.17825965747894357, 0.20460318845723502, 0.20460318845723502, 0.20460318845723502, 0.08741913219627273, 0.08741913219627273, 0.08741913219627273, 0.10170633761050629, 0.10170633761050629, 0.10170633761050629, 0.08130864284712969, 0.08130864284712969, 0.08130864284712969]}, "mutation_prompt": null}
{"id": "f4a0fc32-5232-4c47-969c-8b0f666e0f91", "solution": "import numpy as np\n\nclass PSO_SA_Optimizer_Enhanced_V3:\n    def __init__(self, budget, dim, pop_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.w_max = 0.9\n        self.w_min = 0.4\n        self.c1_init = 1.5\n        self.c2_init = 1.5\n        self.temp_init = 1.0\n\n    def __call__(self, func):\n        np.random.seed(42)\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.array([func(pos) for pos in positions])\n        global_best_position = personal_best_positions[np.argmin(personal_best_scores)]\n        global_best_score = np.min(personal_best_scores)\n        \n        eval_count = self.pop_size\n        temperature = self.temp_init\n        \n        while eval_count < self.budget:\n            w = self.w_max - ((self.w_max - self.w_min) * (eval_count / self.budget))\n            c1 = self.c1_init * (1 - eval_count / self.budget) + 0.5\n            c2 = self.c2_init * (eval_count / self.budget) + 0.5\n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                velocities[i] = (w * velocities[i]\n                                 + c1 * r1 * (personal_best_positions[i] - positions[i])\n                                 + c2 * r2 * (global_best_position - positions[i]))\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], self.lower_bound, self.upper_bound)\n\n                score = func(positions[i])\n                eval_count += 1\n                if eval_count >= self.budget:\n                    break\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = positions[i]\n\n                if np.random.rand() < np.exp(-(score - global_best_score) / temperature):\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n                    if score < global_best_score:\n                        global_best_score = score\n                        global_best_position = positions[i]\n\n            temperature *= 0.95  # Improved annealing factor for convergence\n\n        return global_best_position, global_best_score", "name": "PSO_SA_Optimizer_Enhanced_V3", "description": "Improved adaptive search strategy in PSO-SA with dynamically updating parameters and better temperature decay.", "configspace": "", "generation": 55, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "104cf98e-8138-4906-b55a-546d6de8214b", "metadata": {"aucs": [0.5051733381719483, 0.5051733381719483, 0.5051733381719483, 0.4864875265041222, 0.4864875265041222, 0.4864875265041222, 0.5980878545878154, 0.5980878545878154, 0.5980878545878154, 0.06730631129871123, 0.06730631129871123, 0.06730631129871123, 0.09301964064620494, 0.09301964064620494, 0.09301964064620494, 0.36676061687109374, 0.36676061687109374, 0.36676061687109374, 0.1093498751795694, 0.1093498751795694, 0.1093498751795694, 0.11800704322385025, 0.11800704322385025, 0.11800704322385025, 0.11014226708156238, 0.11014226708156238, 0.11014226708156238, 0.15004683892585147, 0.15004683892585147, 0.15004683892585147, 0.13033125670061696, 0.13033125670061696, 0.13033125670061696, 0.1176642388246808, 0.1176642388246808, 0.1176642388246808, 0.9817783524568978, 0.9817783524568978, 0.9817783524568978, 0.98749484089464, 0.98749484089464, 0.98749484089464, 0.9873373389794495, 0.9873373389794495, 0.9873373389794495, 0.3193023035569802, 0.3193023035569802, 0.3193023035569802, 0.4749253967860462, 0.4749253967860462, 0.4749253967860462, 0.33952636113240786, 0.33952636113240786, 0.33952636113240786, 0.23404906009051418, 0.23404906009051418, 0.23404906009051418, 0.1867241095540818, 0.1867241095540818, 0.1867241095540818, 0.22360410437078448, 0.22360410437078448, 0.22360410437078448, 0.38917833239145483, 0.38917833239145483, 0.38917833239145483, 0.17677793426833743, 0.17677793426833743, 0.17677793426833743, 0.1866522982738108, 0.1866522982738108, 0.1866522982738108, 0.1794288919186895, 0.1794288919186895, 0.1794288919186895, 0.1713064460088295, 0.1713064460088295, 0.1713064460088295, 0.1503593130339168, 0.1503593130339168, 0.1503593130339168, 0.16896150548229494, 0.16896150548229494, 0.16896150548229494, 0.1517238337156679, 0.1517238337156679, 0.1517238337156679, 0.07801075606316066, 0.07801075606316066, 0.07801075606316066, 0.1306258192704034, 0.1306258192704034, 0.1306258192704034, 0.043036206734873295, 0.043036206734873295, 0.043036206734873295, 0.1713736095711984, 0.1713736095711984, 0.1713736095711984, 0.03478423785365725, 0.03478423785365725, 0.03478423785365725, 0.19180632709776668, 0.19180632709776668, 0.19180632709776668, 0.0590246198988168, 0.0590246198988168, 0.0590246198988168, 0.20081792517374752, 0.20081792517374752, 0.20081792517374752, 0.033491506494330636, 0.033491506494330636, 0.033491506494330636, 0.05021043193870478, 0.05021043193870478, 0.05021043193870478, 0.5003727892349492, 0.5003727892349492, 0.5003727892349492, 0.4657333813986042, 0.4657333813986042, 0.4657333813986042, 0.46917186793370924, 0.46917186793370924, 0.46917186793370924, 0.14342207768301163, 0.14342207768301163, 0.14342207768301163, 0.12145871132800168, 0.12145871132800168, 0.12145871132800168, 0.1057475206242896, 0.1057475206242896, 0.1057475206242896, 0.35642057844886454, 0.35642057844886454, 0.35642057844886454, 0.21465072589842016, 0.21465072589842016, 0.21465072589842016, 0.2444189364073357, 0.2444189364073357, 0.2444189364073357, 0.286894944324837, 0.286894944324837, 0.286894944324837, 0.28791410732075406, 0.28791410732075406, 0.28791410732075406, 0.288665474020558, 0.288665474020558, 0.288665474020558, 0.22945855198095, 0.22945855198095, 0.22945855198095, 0.19982102082297581, 0.19982102082297581, 0.19982102082297581, 0.12103828021249086, 0.12103828021249086, 0.12103828021249086, 0.1874220403361898, 0.1874220403361898, 0.1874220403361898, 0.24617784072219195, 0.24617784072219195, 0.24617784072219195, 0.2752280251020164, 0.2752280251020164, 0.2752280251020164, 0.1880647070348399, 0.1880647070348399, 0.1880647070348399, 0.1738231871514354, 0.1738231871514354, 0.1738231871514354, 0.48468137841406633, 0.48468137841406633, 0.48468137841406633, 0.6910667162082013, 0.6910667162082013, 0.6910667162082013, 0.15879721238597644, 0.15879721238597644, 0.15879721238597644, 0.746649682486173, 0.746649682486173, 0.746649682486173, 0.1689669226231103, 0.1689669226231103, 0.1689669226231103, 0.21053313703713594, 0.21053313703713594, 0.21053313703713594, 0.16560864026155453, 0.16560864026155453, 0.16560864026155453, 0.19735138827661058, 0.19735138827661058, 0.19735138827661058, 0.17825965747894357, 0.17825965747894357, 0.17825965747894357, 0.20460318845723502, 0.20460318845723502, 0.20460318845723502, 0.08741913219627273, 0.08741913219627273, 0.08741913219627273, 0.10170633761050629, 0.10170633761050629, 0.10170633761050629, 0.08130864284712969, 0.08130864284712969, 0.08130864284712969]}, "mutation_prompt": null}
{"id": "f8a0927a-a0e9-4494-af3b-e9c5d4726f9e", "solution": "import numpy as np\n\nclass PSO_SA_Optimizer_Enhanced_V3:\n    def __init__(self, budget, dim, pop_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.w_max = 0.9\n        self.w_min = 0.4\n        self.c1_init = 1.5\n        self.c2_init = 1.5\n        self.temp_init = 1.0\n\n    def __call__(self, func):\n        np.random.seed(42)\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.array([func(pos) for pos in positions])\n        global_best_position = personal_best_positions[np.argmin(personal_best_scores)]\n        global_best_score = np.min(personal_best_scores)\n        \n        eval_count = self.pop_size\n        temperature = self.temp_init\n        \n        while eval_count < self.budget:\n            w = self.w_max - ((self.w_max - self.w_min) * (eval_count / self.budget))\n            c1 = self.c1_init * (1 - eval_count / self.budget) + 0.5\n            c2 = self.c2_init * (eval_count / self.budget) + 0.5\n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                velocities[i] = (w * velocities[i]\n                                 + c1 * r1 * (personal_best_positions[i] - positions[i])\n                                 + c2 * r2 * (global_best_position - positions[i]))\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], self.lower_bound, self.upper_bound)\n\n                score = func(positions[i])\n                eval_count += 1\n                if eval_count >= self.budget:\n                    break\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = positions[i]\n\n                if np.random.rand() < np.exp(-(score - global_best_score) / temperature):\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n                    if score < global_best_score:\n                        global_best_score = score\n                        global_best_position = positions[i]\n\n            temperature *= 0.95  # Improved annealing factor for convergence\n\n        return global_best_position, global_best_score", "name": "PSO_SA_Optimizer_Enhanced_V3", "description": "Improved adaptive search strategy in PSO-SA with dynamically updating parameters and better temperature decay.", "configspace": "", "generation": 55, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "104cf98e-8138-4906-b55a-546d6de8214b", "metadata": {"aucs": [0.5051733381719483, 0.5051733381719483, 0.5051733381719483, 0.4864875265041222, 0.4864875265041222, 0.4864875265041222, 0.5980878545878154, 0.5980878545878154, 0.5980878545878154, 0.06730631129871123, 0.06730631129871123, 0.06730631129871123, 0.09301964064620494, 0.09301964064620494, 0.09301964064620494, 0.36676061687109374, 0.36676061687109374, 0.36676061687109374, 0.1093498751795694, 0.1093498751795694, 0.1093498751795694, 0.11800704322385025, 0.11800704322385025, 0.11800704322385025, 0.11014226708156238, 0.11014226708156238, 0.11014226708156238, 0.15004683892585147, 0.15004683892585147, 0.15004683892585147, 0.13033125670061696, 0.13033125670061696, 0.13033125670061696, 0.1176642388246808, 0.1176642388246808, 0.1176642388246808, 0.9817783524568978, 0.9817783524568978, 0.9817783524568978, 0.98749484089464, 0.98749484089464, 0.98749484089464, 0.9873373389794495, 0.9873373389794495, 0.9873373389794495, 0.3193023035569802, 0.3193023035569802, 0.3193023035569802, 0.4749253967860462, 0.4749253967860462, 0.4749253967860462, 0.33952636113240786, 0.33952636113240786, 0.33952636113240786, 0.23404906009051418, 0.23404906009051418, 0.23404906009051418, 0.1867241095540818, 0.1867241095540818, 0.1867241095540818, 0.22360410437078448, 0.22360410437078448, 0.22360410437078448, 0.38917833239145483, 0.38917833239145483, 0.38917833239145483, 0.17677793426833743, 0.17677793426833743, 0.17677793426833743, 0.1866522982738108, 0.1866522982738108, 0.1866522982738108, 0.1794288919186895, 0.1794288919186895, 0.1794288919186895, 0.1713064460088295, 0.1713064460088295, 0.1713064460088295, 0.1503593130339168, 0.1503593130339168, 0.1503593130339168, 0.16896150548229494, 0.16896150548229494, 0.16896150548229494, 0.1517238337156679, 0.1517238337156679, 0.1517238337156679, 0.07801075606316066, 0.07801075606316066, 0.07801075606316066, 0.1306258192704034, 0.1306258192704034, 0.1306258192704034, 0.043036206734873295, 0.043036206734873295, 0.043036206734873295, 0.1713736095711984, 0.1713736095711984, 0.1713736095711984, 0.03478423785365725, 0.03478423785365725, 0.03478423785365725, 0.19180632709776668, 0.19180632709776668, 0.19180632709776668, 0.0590246198988168, 0.0590246198988168, 0.0590246198988168, 0.20081792517374752, 0.20081792517374752, 0.20081792517374752, 0.033491506494330636, 0.033491506494330636, 0.033491506494330636, 0.05021043193870478, 0.05021043193870478, 0.05021043193870478, 0.5003727892349492, 0.5003727892349492, 0.5003727892349492, 0.4657333813986042, 0.4657333813986042, 0.4657333813986042, 0.46917186793370924, 0.46917186793370924, 0.46917186793370924, 0.14342207768301163, 0.14342207768301163, 0.14342207768301163, 0.12145871132800168, 0.12145871132800168, 0.12145871132800168, 0.1057475206242896, 0.1057475206242896, 0.1057475206242896, 0.35642057844886454, 0.35642057844886454, 0.35642057844886454, 0.21465072589842016, 0.21465072589842016, 0.21465072589842016, 0.2444189364073357, 0.2444189364073357, 0.2444189364073357, 0.286894944324837, 0.286894944324837, 0.286894944324837, 0.28791410732075406, 0.28791410732075406, 0.28791410732075406, 0.288665474020558, 0.288665474020558, 0.288665474020558, 0.22945855198095, 0.22945855198095, 0.22945855198095, 0.19982102082297581, 0.19982102082297581, 0.19982102082297581, 0.12103828021249086, 0.12103828021249086, 0.12103828021249086, 0.1874220403361898, 0.1874220403361898, 0.1874220403361898, 0.24617784072219195, 0.24617784072219195, 0.24617784072219195, 0.2752280251020164, 0.2752280251020164, 0.2752280251020164, 0.1880647070348399, 0.1880647070348399, 0.1880647070348399, 0.1738231871514354, 0.1738231871514354, 0.1738231871514354, 0.48468137841406633, 0.48468137841406633, 0.48468137841406633, 0.6910667162082013, 0.6910667162082013, 0.6910667162082013, 0.15879721238597644, 0.15879721238597644, 0.15879721238597644, 0.746649682486173, 0.746649682486173, 0.746649682486173, 0.1689669226231103, 0.1689669226231103, 0.1689669226231103, 0.21053313703713594, 0.21053313703713594, 0.21053313703713594, 0.16560864026155453, 0.16560864026155453, 0.16560864026155453, 0.19735138827661058, 0.19735138827661058, 0.19735138827661058, 0.17825965747894357, 0.17825965747894357, 0.17825965747894357, 0.20460318845723502, 0.20460318845723502, 0.20460318845723502, 0.08741913219627273, 0.08741913219627273, 0.08741913219627273, 0.10170633761050629, 0.10170633761050629, 0.10170633761050629, 0.08130864284712969, 0.08130864284712969, 0.08130864284712969]}, "mutation_prompt": null}
{"id": "3cc37f58-8c8f-45f1-955f-0869a2099036", "solution": "import numpy as np\n\nclass PSO_SA_Optimizer_Enhanced_V3:\n    def __init__(self, budget, dim, pop_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.w_max = 0.9\n        self.w_min = 0.4\n        self.c1_init = 1.5\n        self.c2_init = 1.5\n        self.temp_init = 1.0\n\n    def __call__(self, func):\n        np.random.seed(42)\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.array([func(pos) for pos in positions])\n        global_best_position = personal_best_positions[np.argmin(personal_best_scores)]\n        global_best_score = np.min(personal_best_scores)\n        \n        eval_count = self.pop_size\n        temperature = self.temp_init\n        \n        while eval_count < self.budget:\n            w = self.w_max - ((self.w_max - self.w_min) * (eval_count / self.budget))\n            c1 = self.c1_init * (1 - eval_count / self.budget) + 0.5\n            c2 = self.c2_init * (eval_count / self.budget) + 0.5\n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                velocities[i] = (w * velocities[i]\n                                 + c1 * r1 * (personal_best_positions[i] - positions[i])\n                                 + c2 * r2 * (global_best_position - positions[i]))\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], self.lower_bound, self.upper_bound)\n\n                score = func(positions[i])\n                eval_count += 1\n                if eval_count >= self.budget:\n                    break\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = positions[i]\n\n                if np.random.rand() < np.exp(-(score - global_best_score) / temperature):\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n                    if score < global_best_score:\n                        global_best_score = score\n                        global_best_position = positions[i]\n\n            temperature *= 0.95  # Improved annealing factor for convergence\n\n        return global_best_position, global_best_score", "name": "PSO_SA_Optimizer_Enhanced_V3", "description": "Improved adaptive search strategy in PSO-SA with dynamically updating parameters and better temperature decay.", "configspace": "", "generation": 55, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "104cf98e-8138-4906-b55a-546d6de8214b", "metadata": {"aucs": [0.5051733381719483, 0.5051733381719483, 0.5051733381719483, 0.4864875265041222, 0.4864875265041222, 0.4864875265041222, 0.5980878545878154, 0.5980878545878154, 0.5980878545878154, 0.06730631129871123, 0.06730631129871123, 0.06730631129871123, 0.09301964064620494, 0.09301964064620494, 0.09301964064620494, 0.36676061687109374, 0.36676061687109374, 0.36676061687109374, 0.1093498751795694, 0.1093498751795694, 0.1093498751795694, 0.11800704322385025, 0.11800704322385025, 0.11800704322385025, 0.11014226708156238, 0.11014226708156238, 0.11014226708156238, 0.15004683892585147, 0.15004683892585147, 0.15004683892585147, 0.13033125670061696, 0.13033125670061696, 0.13033125670061696, 0.1176642388246808, 0.1176642388246808, 0.1176642388246808, 0.9817783524568978, 0.9817783524568978, 0.9817783524568978, 0.98749484089464, 0.98749484089464, 0.98749484089464, 0.9873373389794495, 0.9873373389794495, 0.9873373389794495, 0.3193023035569802, 0.3193023035569802, 0.3193023035569802, 0.4749253967860462, 0.4749253967860462, 0.4749253967860462, 0.33952636113240786, 0.33952636113240786, 0.33952636113240786, 0.23404906009051418, 0.23404906009051418, 0.23404906009051418, 0.1867241095540818, 0.1867241095540818, 0.1867241095540818, 0.22360410437078448, 0.22360410437078448, 0.22360410437078448, 0.38917833239145483, 0.38917833239145483, 0.38917833239145483, 0.17677793426833743, 0.17677793426833743, 0.17677793426833743, 0.1866522982738108, 0.1866522982738108, 0.1866522982738108, 0.1794288919186895, 0.1794288919186895, 0.1794288919186895, 0.1713064460088295, 0.1713064460088295, 0.1713064460088295, 0.1503593130339168, 0.1503593130339168, 0.1503593130339168, 0.16896150548229494, 0.16896150548229494, 0.16896150548229494, 0.1517238337156679, 0.1517238337156679, 0.1517238337156679, 0.07801075606316066, 0.07801075606316066, 0.07801075606316066, 0.1306258192704034, 0.1306258192704034, 0.1306258192704034, 0.043036206734873295, 0.043036206734873295, 0.043036206734873295, 0.1713736095711984, 0.1713736095711984, 0.1713736095711984, 0.03478423785365725, 0.03478423785365725, 0.03478423785365725, 0.19180632709776668, 0.19180632709776668, 0.19180632709776668, 0.0590246198988168, 0.0590246198988168, 0.0590246198988168, 0.20081792517374752, 0.20081792517374752, 0.20081792517374752, 0.033491506494330636, 0.033491506494330636, 0.033491506494330636, 0.05021043193870478, 0.05021043193870478, 0.05021043193870478, 0.5003727892349492, 0.5003727892349492, 0.5003727892349492, 0.4657333813986042, 0.4657333813986042, 0.4657333813986042, 0.46917186793370924, 0.46917186793370924, 0.46917186793370924, 0.14342207768301163, 0.14342207768301163, 0.14342207768301163, 0.12145871132800168, 0.12145871132800168, 0.12145871132800168, 0.1057475206242896, 0.1057475206242896, 0.1057475206242896, 0.35642057844886454, 0.35642057844886454, 0.35642057844886454, 0.21465072589842016, 0.21465072589842016, 0.21465072589842016, 0.2444189364073357, 0.2444189364073357, 0.2444189364073357, 0.286894944324837, 0.286894944324837, 0.286894944324837, 0.28791410732075406, 0.28791410732075406, 0.28791410732075406, 0.288665474020558, 0.288665474020558, 0.288665474020558, 0.22945855198095, 0.22945855198095, 0.22945855198095, 0.19982102082297581, 0.19982102082297581, 0.19982102082297581, 0.12103828021249086, 0.12103828021249086, 0.12103828021249086, 0.1874220403361898, 0.1874220403361898, 0.1874220403361898, 0.24617784072219195, 0.24617784072219195, 0.24617784072219195, 0.2752280251020164, 0.2752280251020164, 0.2752280251020164, 0.1880647070348399, 0.1880647070348399, 0.1880647070348399, 0.1738231871514354, 0.1738231871514354, 0.1738231871514354, 0.48468137841406633, 0.48468137841406633, 0.48468137841406633, 0.6910667162082013, 0.6910667162082013, 0.6910667162082013, 0.15879721238597644, 0.15879721238597644, 0.15879721238597644, 0.746649682486173, 0.746649682486173, 0.746649682486173, 0.1689669226231103, 0.1689669226231103, 0.1689669226231103, 0.21053313703713594, 0.21053313703713594, 0.21053313703713594, 0.16560864026155453, 0.16560864026155453, 0.16560864026155453, 0.19735138827661058, 0.19735138827661058, 0.19735138827661058, 0.17825965747894357, 0.17825965747894357, 0.17825965747894357, 0.20460318845723502, 0.20460318845723502, 0.20460318845723502, 0.08741913219627273, 0.08741913219627273, 0.08741913219627273, 0.10170633761050629, 0.10170633761050629, 0.10170633761050629, 0.08130864284712969, 0.08130864284712969, 0.08130864284712969]}, "mutation_prompt": null}
{"id": "ec5db521-4ef9-48a9-b21c-7417c1639b41", "solution": "import numpy as np\n\nclass PSO_SA_Optimizer_Enhanced_V3:\n    def __init__(self, budget, dim, pop_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.w_max = 0.9\n        self.w_min = 0.4\n        self.c1_init = 1.5\n        self.c2_init = 1.5\n        self.temp_init = 1.0\n\n    def __call__(self, func):\n        np.random.seed(42)\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.array([func(pos) for pos in positions])\n        global_best_position = personal_best_positions[np.argmin(personal_best_scores)]\n        global_best_score = np.min(personal_best_scores)\n        \n        eval_count = self.pop_size\n        temperature = self.temp_init\n        \n        while eval_count < self.budget:\n            w = self.w_max - ((self.w_max - self.w_min) * (eval_count / self.budget))\n            c1 = self.c1_init * (1 - eval_count / self.budget) + 0.5\n            c2 = self.c2_init * (eval_count / self.budget) + 0.5\n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                velocities[i] = (w * velocities[i]\n                                 + c1 * r1 * (personal_best_positions[i] - positions[i])\n                                 + c2 * r2 * (global_best_position - positions[i]))\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], self.lower_bound, self.upper_bound)\n\n                score = func(positions[i])\n                eval_count += 1\n                if eval_count >= self.budget:\n                    break\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = positions[i]\n\n                if np.random.rand() < np.exp(-(score - global_best_score) / temperature):\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n                    if score < global_best_score:\n                        global_best_score = score\n                        global_best_position = positions[i]\n\n            temperature *= 0.95  # Improved annealing factor for convergence\n\n        return global_best_position, global_best_score", "name": "PSO_SA_Optimizer_Enhanced_V3", "description": "Improved adaptive search strategy in PSO-SA with dynamically updating parameters and better temperature decay.", "configspace": "", "generation": 55, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "104cf98e-8138-4906-b55a-546d6de8214b", "metadata": {"aucs": [0.5051733381719483, 0.5051733381719483, 0.5051733381719483, 0.4864875265041222, 0.4864875265041222, 0.4864875265041222, 0.5980878545878154, 0.5980878545878154, 0.5980878545878154, 0.06730631129871123, 0.06730631129871123, 0.06730631129871123, 0.09301964064620494, 0.09301964064620494, 0.09301964064620494, 0.36676061687109374, 0.36676061687109374, 0.36676061687109374, 0.1093498751795694, 0.1093498751795694, 0.1093498751795694, 0.11800704322385025, 0.11800704322385025, 0.11800704322385025, 0.11014226708156238, 0.11014226708156238, 0.11014226708156238, 0.15004683892585147, 0.15004683892585147, 0.15004683892585147, 0.13033125670061696, 0.13033125670061696, 0.13033125670061696, 0.1176642388246808, 0.1176642388246808, 0.1176642388246808, 0.9817783524568978, 0.9817783524568978, 0.9817783524568978, 0.98749484089464, 0.98749484089464, 0.98749484089464, 0.9873373389794495, 0.9873373389794495, 0.9873373389794495, 0.3193023035569802, 0.3193023035569802, 0.3193023035569802, 0.4749253967860462, 0.4749253967860462, 0.4749253967860462, 0.33952636113240786, 0.33952636113240786, 0.33952636113240786, 0.23404906009051418, 0.23404906009051418, 0.23404906009051418, 0.1867241095540818, 0.1867241095540818, 0.1867241095540818, 0.22360410437078448, 0.22360410437078448, 0.22360410437078448, 0.38917833239145483, 0.38917833239145483, 0.38917833239145483, 0.17677793426833743, 0.17677793426833743, 0.17677793426833743, 0.1866522982738108, 0.1866522982738108, 0.1866522982738108, 0.1794288919186895, 0.1794288919186895, 0.1794288919186895, 0.1713064460088295, 0.1713064460088295, 0.1713064460088295, 0.1503593130339168, 0.1503593130339168, 0.1503593130339168, 0.16896150548229494, 0.16896150548229494, 0.16896150548229494, 0.1517238337156679, 0.1517238337156679, 0.1517238337156679, 0.07801075606316066, 0.07801075606316066, 0.07801075606316066, 0.1306258192704034, 0.1306258192704034, 0.1306258192704034, 0.043036206734873295, 0.043036206734873295, 0.043036206734873295, 0.1713736095711984, 0.1713736095711984, 0.1713736095711984, 0.03478423785365725, 0.03478423785365725, 0.03478423785365725, 0.19180632709776668, 0.19180632709776668, 0.19180632709776668, 0.0590246198988168, 0.0590246198988168, 0.0590246198988168, 0.20081792517374752, 0.20081792517374752, 0.20081792517374752, 0.033491506494330636, 0.033491506494330636, 0.033491506494330636, 0.05021043193870478, 0.05021043193870478, 0.05021043193870478, 0.5003727892349492, 0.5003727892349492, 0.5003727892349492, 0.4657333813986042, 0.4657333813986042, 0.4657333813986042, 0.46917186793370924, 0.46917186793370924, 0.46917186793370924, 0.14342207768301163, 0.14342207768301163, 0.14342207768301163, 0.12145871132800168, 0.12145871132800168, 0.12145871132800168, 0.1057475206242896, 0.1057475206242896, 0.1057475206242896, 0.35642057844886454, 0.35642057844886454, 0.35642057844886454, 0.21465072589842016, 0.21465072589842016, 0.21465072589842016, 0.2444189364073357, 0.2444189364073357, 0.2444189364073357, 0.286894944324837, 0.286894944324837, 0.286894944324837, 0.28791410732075406, 0.28791410732075406, 0.28791410732075406, 0.288665474020558, 0.288665474020558, 0.288665474020558, 0.22945855198095, 0.22945855198095, 0.22945855198095, 0.19982102082297581, 0.19982102082297581, 0.19982102082297581, 0.12103828021249086, 0.12103828021249086, 0.12103828021249086, 0.1874220403361898, 0.1874220403361898, 0.1874220403361898, 0.24617784072219195, 0.24617784072219195, 0.24617784072219195, 0.2752280251020164, 0.2752280251020164, 0.2752280251020164, 0.1880647070348399, 0.1880647070348399, 0.1880647070348399, 0.1738231871514354, 0.1738231871514354, 0.1738231871514354, 0.48468137841406633, 0.48468137841406633, 0.48468137841406633, 0.6910667162082013, 0.6910667162082013, 0.6910667162082013, 0.15879721238597644, 0.15879721238597644, 0.15879721238597644, 0.746649682486173, 0.746649682486173, 0.746649682486173, 0.1689669226231103, 0.1689669226231103, 0.1689669226231103, 0.21053313703713594, 0.21053313703713594, 0.21053313703713594, 0.16560864026155453, 0.16560864026155453, 0.16560864026155453, 0.19735138827661058, 0.19735138827661058, 0.19735138827661058, 0.17825965747894357, 0.17825965747894357, 0.17825965747894357, 0.20460318845723502, 0.20460318845723502, 0.20460318845723502, 0.08741913219627273, 0.08741913219627273, 0.08741913219627273, 0.10170633761050629, 0.10170633761050629, 0.10170633761050629, 0.08130864284712969, 0.08130864284712969, 0.08130864284712969]}, "mutation_prompt": null}
{"id": "46306d76-9dbd-4e1b-af9f-482c2fc91e63", "solution": "import numpy as np\n\nclass PSO_SA_Optimizer_Enhanced_V3:\n    def __init__(self, budget, dim, pop_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.w_max = 0.9\n        self.w_min = 0.4\n        self.c1_init = 1.5\n        self.c2_init = 1.5\n        self.temp_init = 1.0\n\n    def __call__(self, func):\n        np.random.seed(42)\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.array([func(pos) for pos in positions])\n        global_best_position = personal_best_positions[np.argmin(personal_best_scores)]\n        global_best_score = np.min(personal_best_scores)\n        \n        eval_count = self.pop_size\n        temperature = self.temp_init\n        \n        while eval_count < self.budget:\n            w = self.w_max - ((self.w_max - self.w_min) * (eval_count / self.budget))\n            c1 = self.c1_init * (1 - eval_count / self.budget) + 0.5\n            c2 = self.c2_init * (eval_count / self.budget) + 0.5\n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                velocities[i] = (w * velocities[i]\n                                 + c1 * r1 * (personal_best_positions[i] - positions[i])\n                                 + c2 * r2 * (global_best_position - positions[i]))\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], self.lower_bound, self.upper_bound)\n\n                score = func(positions[i])\n                eval_count += 1\n                if eval_count >= self.budget:\n                    break\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = positions[i]\n\n                if np.random.rand() < np.exp(-(score - global_best_score) / temperature):\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n                    if score < global_best_score:\n                        global_best_score = score\n                        global_best_position = positions[i]\n\n            temperature *= 0.95  # Improved annealing factor for convergence\n\n        return global_best_position, global_best_score", "name": "PSO_SA_Optimizer_Enhanced_V3", "description": "Improved adaptive search strategy in PSO-SA with dynamically updating parameters and better temperature decay.", "configspace": "", "generation": 55, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "104cf98e-8138-4906-b55a-546d6de8214b", "metadata": {"aucs": [0.5051733381719483, 0.5051733381719483, 0.5051733381719483, 0.4864875265041222, 0.4864875265041222, 0.4864875265041222, 0.5980878545878154, 0.5980878545878154, 0.5980878545878154, 0.06730631129871123, 0.06730631129871123, 0.06730631129871123, 0.09301964064620494, 0.09301964064620494, 0.09301964064620494, 0.36676061687109374, 0.36676061687109374, 0.36676061687109374, 0.1093498751795694, 0.1093498751795694, 0.1093498751795694, 0.11800704322385025, 0.11800704322385025, 0.11800704322385025, 0.11014226708156238, 0.11014226708156238, 0.11014226708156238, 0.15004683892585147, 0.15004683892585147, 0.15004683892585147, 0.13033125670061696, 0.13033125670061696, 0.13033125670061696, 0.1176642388246808, 0.1176642388246808, 0.1176642388246808, 0.9817783524568978, 0.9817783524568978, 0.9817783524568978, 0.98749484089464, 0.98749484089464, 0.98749484089464, 0.9873373389794495, 0.9873373389794495, 0.9873373389794495, 0.3193023035569802, 0.3193023035569802, 0.3193023035569802, 0.4749253967860462, 0.4749253967860462, 0.4749253967860462, 0.33952636113240786, 0.33952636113240786, 0.33952636113240786, 0.23404906009051418, 0.23404906009051418, 0.23404906009051418, 0.1867241095540818, 0.1867241095540818, 0.1867241095540818, 0.22360410437078448, 0.22360410437078448, 0.22360410437078448, 0.38917833239145483, 0.38917833239145483, 0.38917833239145483, 0.17677793426833743, 0.17677793426833743, 0.17677793426833743, 0.1866522982738108, 0.1866522982738108, 0.1866522982738108, 0.1794288919186895, 0.1794288919186895, 0.1794288919186895, 0.1713064460088295, 0.1713064460088295, 0.1713064460088295, 0.1503593130339168, 0.1503593130339168, 0.1503593130339168, 0.16896150548229494, 0.16896150548229494, 0.16896150548229494, 0.1517238337156679, 0.1517238337156679, 0.1517238337156679, 0.07801075606316066, 0.07801075606316066, 0.07801075606316066, 0.1306258192704034, 0.1306258192704034, 0.1306258192704034, 0.043036206734873295, 0.043036206734873295, 0.043036206734873295, 0.1713736095711984, 0.1713736095711984, 0.1713736095711984, 0.03478423785365725, 0.03478423785365725, 0.03478423785365725, 0.19180632709776668, 0.19180632709776668, 0.19180632709776668, 0.0590246198988168, 0.0590246198988168, 0.0590246198988168, 0.20081792517374752, 0.20081792517374752, 0.20081792517374752, 0.033491506494330636, 0.033491506494330636, 0.033491506494330636, 0.05021043193870478, 0.05021043193870478, 0.05021043193870478, 0.5003727892349492, 0.5003727892349492, 0.5003727892349492, 0.4657333813986042, 0.4657333813986042, 0.4657333813986042, 0.46917186793370924, 0.46917186793370924, 0.46917186793370924, 0.14342207768301163, 0.14342207768301163, 0.14342207768301163, 0.12145871132800168, 0.12145871132800168, 0.12145871132800168, 0.1057475206242896, 0.1057475206242896, 0.1057475206242896, 0.35642057844886454, 0.35642057844886454, 0.35642057844886454, 0.21465072589842016, 0.21465072589842016, 0.21465072589842016, 0.2444189364073357, 0.2444189364073357, 0.2444189364073357, 0.286894944324837, 0.286894944324837, 0.286894944324837, 0.28791410732075406, 0.28791410732075406, 0.28791410732075406, 0.288665474020558, 0.288665474020558, 0.288665474020558, 0.22945855198095, 0.22945855198095, 0.22945855198095, 0.19982102082297581, 0.19982102082297581, 0.19982102082297581, 0.12103828021249086, 0.12103828021249086, 0.12103828021249086, 0.1874220403361898, 0.1874220403361898, 0.1874220403361898, 0.24617784072219195, 0.24617784072219195, 0.24617784072219195, 0.2752280251020164, 0.2752280251020164, 0.2752280251020164, 0.1880647070348399, 0.1880647070348399, 0.1880647070348399, 0.1738231871514354, 0.1738231871514354, 0.1738231871514354, 0.48468137841406633, 0.48468137841406633, 0.48468137841406633, 0.6910667162082013, 0.6910667162082013, 0.6910667162082013, 0.15879721238597644, 0.15879721238597644, 0.15879721238597644, 0.746649682486173, 0.746649682486173, 0.746649682486173, 0.1689669226231103, 0.1689669226231103, 0.1689669226231103, 0.21053313703713594, 0.21053313703713594, 0.21053313703713594, 0.16560864026155453, 0.16560864026155453, 0.16560864026155453, 0.19735138827661058, 0.19735138827661058, 0.19735138827661058, 0.17825965747894357, 0.17825965747894357, 0.17825965747894357, 0.20460318845723502, 0.20460318845723502, 0.20460318845723502, 0.08741913219627273, 0.08741913219627273, 0.08741913219627273, 0.10170633761050629, 0.10170633761050629, 0.10170633761050629, 0.08130864284712969, 0.08130864284712969, 0.08130864284712969]}, "mutation_prompt": null}
{"id": "39ac5fd9-99ac-4015-b64e-69c148eea0c2", "solution": "import numpy as np\n\nclass PSO_SA_Optimizer_Enhanced_V3:\n    def __init__(self, budget, dim, pop_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.w_max = 0.9\n        self.w_min = 0.4\n        self.c1_init = 1.5\n        self.c2_init = 1.5\n        self.temp_init = 1.0\n\n    def __call__(self, func):\n        np.random.seed(42)\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.array([func(pos) for pos in positions])\n        global_best_position = personal_best_positions[np.argmin(personal_best_scores)]\n        global_best_score = np.min(personal_best_scores)\n        \n        eval_count = self.pop_size\n        temperature = self.temp_init\n        \n        while eval_count < self.budget:\n            w = self.w_max - ((self.w_max - self.w_min) * (eval_count / self.budget))\n            c1 = self.c1_init * (1 - eval_count / self.budget) + 0.5\n            c2 = self.c2_init * (eval_count / self.budget) + 0.5\n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                velocities[i] = (w * velocities[i]\n                                 + c1 * r1 * (personal_best_positions[i] - positions[i])\n                                 + c2 * r2 * (global_best_position - positions[i]))\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], self.lower_bound, self.upper_bound)\n\n                score = func(positions[i])\n                eval_count += 1\n                if eval_count >= self.budget:\n                    break\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = positions[i]\n\n                if np.random.rand() < np.exp(-(score - global_best_score) / temperature):\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n                    if score < global_best_score:\n                        global_best_score = score\n                        global_best_position = positions[i]\n\n            temperature *= 0.95  # Improved annealing factor for convergence\n\n        return global_best_position, global_best_score", "name": "PSO_SA_Optimizer_Enhanced_V3", "description": "Improved adaptive search strategy in PSO-SA with dynamically updating parameters and better temperature decay.", "configspace": "", "generation": 55, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "104cf98e-8138-4906-b55a-546d6de8214b", "metadata": {"aucs": [0.5051733381719483, 0.5051733381719483, 0.5051733381719483, 0.4864875265041222, 0.4864875265041222, 0.4864875265041222, 0.5980878545878154, 0.5980878545878154, 0.5980878545878154, 0.06730631129871123, 0.06730631129871123, 0.06730631129871123, 0.09301964064620494, 0.09301964064620494, 0.09301964064620494, 0.36676061687109374, 0.36676061687109374, 0.36676061687109374, 0.1093498751795694, 0.1093498751795694, 0.1093498751795694, 0.11800704322385025, 0.11800704322385025, 0.11800704322385025, 0.11014226708156238, 0.11014226708156238, 0.11014226708156238, 0.15004683892585147, 0.15004683892585147, 0.15004683892585147, 0.13033125670061696, 0.13033125670061696, 0.13033125670061696, 0.1176642388246808, 0.1176642388246808, 0.1176642388246808, 0.9817783524568978, 0.9817783524568978, 0.9817783524568978, 0.98749484089464, 0.98749484089464, 0.98749484089464, 0.9873373389794495, 0.9873373389794495, 0.9873373389794495, 0.3193023035569802, 0.3193023035569802, 0.3193023035569802, 0.4749253967860462, 0.4749253967860462, 0.4749253967860462, 0.33952636113240786, 0.33952636113240786, 0.33952636113240786, 0.23404906009051418, 0.23404906009051418, 0.23404906009051418, 0.1867241095540818, 0.1867241095540818, 0.1867241095540818, 0.22360410437078448, 0.22360410437078448, 0.22360410437078448, 0.38917833239145483, 0.38917833239145483, 0.38917833239145483, 0.17677793426833743, 0.17677793426833743, 0.17677793426833743, 0.1866522982738108, 0.1866522982738108, 0.1866522982738108, 0.1794288919186895, 0.1794288919186895, 0.1794288919186895, 0.1713064460088295, 0.1713064460088295, 0.1713064460088295, 0.1503593130339168, 0.1503593130339168, 0.1503593130339168, 0.16896150548229494, 0.16896150548229494, 0.16896150548229494, 0.1517238337156679, 0.1517238337156679, 0.1517238337156679, 0.07801075606316066, 0.07801075606316066, 0.07801075606316066, 0.1306258192704034, 0.1306258192704034, 0.1306258192704034, 0.043036206734873295, 0.043036206734873295, 0.043036206734873295, 0.1713736095711984, 0.1713736095711984, 0.1713736095711984, 0.03478423785365725, 0.03478423785365725, 0.03478423785365725, 0.19180632709776668, 0.19180632709776668, 0.19180632709776668, 0.0590246198988168, 0.0590246198988168, 0.0590246198988168, 0.20081792517374752, 0.20081792517374752, 0.20081792517374752, 0.033491506494330636, 0.033491506494330636, 0.033491506494330636, 0.05021043193870478, 0.05021043193870478, 0.05021043193870478, 0.5003727892349492, 0.5003727892349492, 0.5003727892349492, 0.4657333813986042, 0.4657333813986042, 0.4657333813986042, 0.46917186793370924, 0.46917186793370924, 0.46917186793370924, 0.14342207768301163, 0.14342207768301163, 0.14342207768301163, 0.12145871132800168, 0.12145871132800168, 0.12145871132800168, 0.1057475206242896, 0.1057475206242896, 0.1057475206242896, 0.35642057844886454, 0.35642057844886454, 0.35642057844886454, 0.21465072589842016, 0.21465072589842016, 0.21465072589842016, 0.2444189364073357, 0.2444189364073357, 0.2444189364073357, 0.286894944324837, 0.286894944324837, 0.286894944324837, 0.28791410732075406, 0.28791410732075406, 0.28791410732075406, 0.288665474020558, 0.288665474020558, 0.288665474020558, 0.22945855198095, 0.22945855198095, 0.22945855198095, 0.19982102082297581, 0.19982102082297581, 0.19982102082297581, 0.12103828021249086, 0.12103828021249086, 0.12103828021249086, 0.1874220403361898, 0.1874220403361898, 0.1874220403361898, 0.24617784072219195, 0.24617784072219195, 0.24617784072219195, 0.2752280251020164, 0.2752280251020164, 0.2752280251020164, 0.1880647070348399, 0.1880647070348399, 0.1880647070348399, 0.1738231871514354, 0.1738231871514354, 0.1738231871514354, 0.48468137841406633, 0.48468137841406633, 0.48468137841406633, 0.6910667162082013, 0.6910667162082013, 0.6910667162082013, 0.15879721238597644, 0.15879721238597644, 0.15879721238597644, 0.746649682486173, 0.746649682486173, 0.746649682486173, 0.1689669226231103, 0.1689669226231103, 0.1689669226231103, 0.21053313703713594, 0.21053313703713594, 0.21053313703713594, 0.16560864026155453, 0.16560864026155453, 0.16560864026155453, 0.19735138827661058, 0.19735138827661058, 0.19735138827661058, 0.17825965747894357, 0.17825965747894357, 0.17825965747894357, 0.20460318845723502, 0.20460318845723502, 0.20460318845723502, 0.08741913219627273, 0.08741913219627273, 0.08741913219627273, 0.10170633761050629, 0.10170633761050629, 0.10170633761050629, 0.08130864284712969, 0.08130864284712969, 0.08130864284712969]}, "mutation_prompt": null}
{"id": "08f9e4fa-eaf8-48c3-8b81-0b87b5012a07", "solution": "import numpy as np\n\nclass PSO_SA_Optimizer_Enhanced_V3:\n    def __init__(self, budget, dim, pop_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.w_max = 0.9\n        self.w_min = 0.4\n        self.c1_init = 1.5\n        self.c2_init = 1.5\n        self.temp_init = 1.0\n\n    def __call__(self, func):\n        np.random.seed(42)\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.array([func(pos) for pos in positions])\n        global_best_position = personal_best_positions[np.argmin(personal_best_scores)]\n        global_best_score = np.min(personal_best_scores)\n        \n        eval_count = self.pop_size\n        temperature = self.temp_init\n        \n        while eval_count < self.budget:\n            w = self.w_max - ((self.w_max - self.w_min) * (eval_count / self.budget))\n            c1 = self.c1_init * (1 - eval_count / self.budget) + 0.5\n            c2 = self.c2_init * (eval_count / self.budget) + 0.5\n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                velocities[i] = (w * velocities[i]\n                                 + c1 * r1 * (personal_best_positions[i] - positions[i])\n                                 + c2 * r2 * (global_best_position - positions[i]))\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], self.lower_bound, self.upper_bound)\n\n                score = func(positions[i])\n                eval_count += 1\n                if eval_count >= self.budget:\n                    break\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = positions[i]\n\n                if np.random.rand() < np.exp(-(score - global_best_score) / temperature):\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n                    if score < global_best_score:\n                        global_best_score = score\n                        global_best_position = positions[i]\n\n            temperature *= 0.95  # Improved annealing factor for convergence\n\n        return global_best_position, global_best_score", "name": "PSO_SA_Optimizer_Enhanced_V3", "description": "Improved adaptive search strategy in PSO-SA with dynamically updating parameters and better temperature decay.", "configspace": "", "generation": 55, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "104cf98e-8138-4906-b55a-546d6de8214b", "metadata": {"aucs": [0.5051733381719483, 0.5051733381719483, 0.5051733381719483, 0.4864875265041222, 0.4864875265041222, 0.4864875265041222, 0.5980878545878154, 0.5980878545878154, 0.5980878545878154, 0.06730631129871123, 0.06730631129871123, 0.06730631129871123, 0.09301964064620494, 0.09301964064620494, 0.09301964064620494, 0.36676061687109374, 0.36676061687109374, 0.36676061687109374, 0.1093498751795694, 0.1093498751795694, 0.1093498751795694, 0.11800704322385025, 0.11800704322385025, 0.11800704322385025, 0.11014226708156238, 0.11014226708156238, 0.11014226708156238, 0.15004683892585147, 0.15004683892585147, 0.15004683892585147, 0.13033125670061696, 0.13033125670061696, 0.13033125670061696, 0.1176642388246808, 0.1176642388246808, 0.1176642388246808, 0.9817783524568978, 0.9817783524568978, 0.9817783524568978, 0.98749484089464, 0.98749484089464, 0.98749484089464, 0.9873373389794495, 0.9873373389794495, 0.9873373389794495, 0.3193023035569802, 0.3193023035569802, 0.3193023035569802, 0.4749253967860462, 0.4749253967860462, 0.4749253967860462, 0.33952636113240786, 0.33952636113240786, 0.33952636113240786, 0.23404906009051418, 0.23404906009051418, 0.23404906009051418, 0.1867241095540818, 0.1867241095540818, 0.1867241095540818, 0.22360410437078448, 0.22360410437078448, 0.22360410437078448, 0.38917833239145483, 0.38917833239145483, 0.38917833239145483, 0.17677793426833743, 0.17677793426833743, 0.17677793426833743, 0.1866522982738108, 0.1866522982738108, 0.1866522982738108, 0.1794288919186895, 0.1794288919186895, 0.1794288919186895, 0.1713064460088295, 0.1713064460088295, 0.1713064460088295, 0.1503593130339168, 0.1503593130339168, 0.1503593130339168, 0.16896150548229494, 0.16896150548229494, 0.16896150548229494, 0.1517238337156679, 0.1517238337156679, 0.1517238337156679, 0.07801075606316066, 0.07801075606316066, 0.07801075606316066, 0.1306258192704034, 0.1306258192704034, 0.1306258192704034, 0.043036206734873295, 0.043036206734873295, 0.043036206734873295, 0.1713736095711984, 0.1713736095711984, 0.1713736095711984, 0.03478423785365725, 0.03478423785365725, 0.03478423785365725, 0.19180632709776668, 0.19180632709776668, 0.19180632709776668, 0.0590246198988168, 0.0590246198988168, 0.0590246198988168, 0.20081792517374752, 0.20081792517374752, 0.20081792517374752, 0.033491506494330636, 0.033491506494330636, 0.033491506494330636, 0.05021043193870478, 0.05021043193870478, 0.05021043193870478, 0.5003727892349492, 0.5003727892349492, 0.5003727892349492, 0.4657333813986042, 0.4657333813986042, 0.4657333813986042, 0.46917186793370924, 0.46917186793370924, 0.46917186793370924, 0.14342207768301163, 0.14342207768301163, 0.14342207768301163, 0.12145871132800168, 0.12145871132800168, 0.12145871132800168, 0.1057475206242896, 0.1057475206242896, 0.1057475206242896, 0.35642057844886454, 0.35642057844886454, 0.35642057844886454, 0.21465072589842016, 0.21465072589842016, 0.21465072589842016, 0.2444189364073357, 0.2444189364073357, 0.2444189364073357, 0.286894944324837, 0.286894944324837, 0.286894944324837, 0.28791410732075406, 0.28791410732075406, 0.28791410732075406, 0.288665474020558, 0.288665474020558, 0.288665474020558, 0.22945855198095, 0.22945855198095, 0.22945855198095, 0.19982102082297581, 0.19982102082297581, 0.19982102082297581, 0.12103828021249086, 0.12103828021249086, 0.12103828021249086, 0.1874220403361898, 0.1874220403361898, 0.1874220403361898, 0.24617784072219195, 0.24617784072219195, 0.24617784072219195, 0.2752280251020164, 0.2752280251020164, 0.2752280251020164, 0.1880647070348399, 0.1880647070348399, 0.1880647070348399, 0.1738231871514354, 0.1738231871514354, 0.1738231871514354, 0.48468137841406633, 0.48468137841406633, 0.48468137841406633, 0.6910667162082013, 0.6910667162082013, 0.6910667162082013, 0.15879721238597644, 0.15879721238597644, 0.15879721238597644, 0.746649682486173, 0.746649682486173, 0.746649682486173, 0.1689669226231103, 0.1689669226231103, 0.1689669226231103, 0.21053313703713594, 0.21053313703713594, 0.21053313703713594, 0.16560864026155453, 0.16560864026155453, 0.16560864026155453, 0.19735138827661058, 0.19735138827661058, 0.19735138827661058, 0.17825965747894357, 0.17825965747894357, 0.17825965747894357, 0.20460318845723502, 0.20460318845723502, 0.20460318845723502, 0.08741913219627273, 0.08741913219627273, 0.08741913219627273, 0.10170633761050629, 0.10170633761050629, 0.10170633761050629, 0.08130864284712969, 0.08130864284712969, 0.08130864284712969]}, "mutation_prompt": null}
{"id": "255f0cbf-2b97-4499-b116-ba6393b0f7fb", "solution": "import numpy as np\n\nclass PSO_SA_Optimizer_Enhanced_V3:\n    def __init__(self, budget, dim, pop_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.w_max = 0.9\n        self.w_min = 0.4\n        self.c1_init = 1.5\n        self.c2_init = 1.5\n        self.temp_init = 1.0\n\n    def __call__(self, func):\n        np.random.seed(42)\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.array([func(pos) for pos in positions])\n        global_best_position = personal_best_positions[np.argmin(personal_best_scores)]\n        global_best_score = np.min(personal_best_scores)\n        \n        eval_count = self.pop_size\n        temperature = self.temp_init\n        \n        while eval_count < self.budget:\n            w = self.w_max - ((self.w_max - self.w_min) * (eval_count / self.budget))\n            c1 = self.c1_init * (1 - eval_count / self.budget) + 0.5\n            c2 = self.c2_init * (eval_count / self.budget) + 0.5\n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                velocities[i] = (w * velocities[i]\n                                 + c1 * r1 * (personal_best_positions[i] - positions[i])\n                                 + c2 * r2 * (global_best_position - positions[i]))\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], self.lower_bound, self.upper_bound)\n\n                score = func(positions[i])\n                eval_count += 1\n                if eval_count >= self.budget:\n                    break\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = positions[i]\n\n                if np.random.rand() < np.exp(-(score - global_best_score) / temperature):\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n                    if score < global_best_score:\n                        global_best_score = score\n                        global_best_position = positions[i]\n\n            temperature *= 0.95  # Improved annealing factor for convergence\n\n        return global_best_position, global_best_score", "name": "PSO_SA_Optimizer_Enhanced_V3", "description": "Improved adaptive search strategy in PSO-SA with dynamically updating parameters and better temperature decay.", "configspace": "", "generation": 55, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "104cf98e-8138-4906-b55a-546d6de8214b", "metadata": {"aucs": [0.5051733381719483, 0.5051733381719483, 0.5051733381719483, 0.4864875265041222, 0.4864875265041222, 0.4864875265041222, 0.5980878545878154, 0.5980878545878154, 0.5980878545878154, 0.06730631129871123, 0.06730631129871123, 0.06730631129871123, 0.09301964064620494, 0.09301964064620494, 0.09301964064620494, 0.36676061687109374, 0.36676061687109374, 0.36676061687109374, 0.1093498751795694, 0.1093498751795694, 0.1093498751795694, 0.11800704322385025, 0.11800704322385025, 0.11800704322385025, 0.11014226708156238, 0.11014226708156238, 0.11014226708156238, 0.15004683892585147, 0.15004683892585147, 0.15004683892585147, 0.13033125670061696, 0.13033125670061696, 0.13033125670061696, 0.1176642388246808, 0.1176642388246808, 0.1176642388246808, 0.9817783524568978, 0.9817783524568978, 0.9817783524568978, 0.98749484089464, 0.98749484089464, 0.98749484089464, 0.9873373389794495, 0.9873373389794495, 0.9873373389794495, 0.3193023035569802, 0.3193023035569802, 0.3193023035569802, 0.4749253967860462, 0.4749253967860462, 0.4749253967860462, 0.33952636113240786, 0.33952636113240786, 0.33952636113240786, 0.23404906009051418, 0.23404906009051418, 0.23404906009051418, 0.1867241095540818, 0.1867241095540818, 0.1867241095540818, 0.22360410437078448, 0.22360410437078448, 0.22360410437078448, 0.38917833239145483, 0.38917833239145483, 0.38917833239145483, 0.17677793426833743, 0.17677793426833743, 0.17677793426833743, 0.1866522982738108, 0.1866522982738108, 0.1866522982738108, 0.1794288919186895, 0.1794288919186895, 0.1794288919186895, 0.1713064460088295, 0.1713064460088295, 0.1713064460088295, 0.1503593130339168, 0.1503593130339168, 0.1503593130339168, 0.16896150548229494, 0.16896150548229494, 0.16896150548229494, 0.1517238337156679, 0.1517238337156679, 0.1517238337156679, 0.07801075606316066, 0.07801075606316066, 0.07801075606316066, 0.1306258192704034, 0.1306258192704034, 0.1306258192704034, 0.043036206734873295, 0.043036206734873295, 0.043036206734873295, 0.1713736095711984, 0.1713736095711984, 0.1713736095711984, 0.03478423785365725, 0.03478423785365725, 0.03478423785365725, 0.19180632709776668, 0.19180632709776668, 0.19180632709776668, 0.0590246198988168, 0.0590246198988168, 0.0590246198988168, 0.20081792517374752, 0.20081792517374752, 0.20081792517374752, 0.033491506494330636, 0.033491506494330636, 0.033491506494330636, 0.05021043193870478, 0.05021043193870478, 0.05021043193870478, 0.5003727892349492, 0.5003727892349492, 0.5003727892349492, 0.4657333813986042, 0.4657333813986042, 0.4657333813986042, 0.46917186793370924, 0.46917186793370924, 0.46917186793370924, 0.14342207768301163, 0.14342207768301163, 0.14342207768301163, 0.12145871132800168, 0.12145871132800168, 0.12145871132800168, 0.1057475206242896, 0.1057475206242896, 0.1057475206242896, 0.35642057844886454, 0.35642057844886454, 0.35642057844886454, 0.21465072589842016, 0.21465072589842016, 0.21465072589842016, 0.2444189364073357, 0.2444189364073357, 0.2444189364073357, 0.286894944324837, 0.286894944324837, 0.286894944324837, 0.28791410732075406, 0.28791410732075406, 0.28791410732075406, 0.288665474020558, 0.288665474020558, 0.288665474020558, 0.22945855198095, 0.22945855198095, 0.22945855198095, 0.19982102082297581, 0.19982102082297581, 0.19982102082297581, 0.12103828021249086, 0.12103828021249086, 0.12103828021249086, 0.1874220403361898, 0.1874220403361898, 0.1874220403361898, 0.24617784072219195, 0.24617784072219195, 0.24617784072219195, 0.2752280251020164, 0.2752280251020164, 0.2752280251020164, 0.1880647070348399, 0.1880647070348399, 0.1880647070348399, 0.1738231871514354, 0.1738231871514354, 0.1738231871514354, 0.48468137841406633, 0.48468137841406633, 0.48468137841406633, 0.6910667162082013, 0.6910667162082013, 0.6910667162082013, 0.15879721238597644, 0.15879721238597644, 0.15879721238597644, 0.746649682486173, 0.746649682486173, 0.746649682486173, 0.1689669226231103, 0.1689669226231103, 0.1689669226231103, 0.21053313703713594, 0.21053313703713594, 0.21053313703713594, 0.16560864026155453, 0.16560864026155453, 0.16560864026155453, 0.19735138827661058, 0.19735138827661058, 0.19735138827661058, 0.17825965747894357, 0.17825965747894357, 0.17825965747894357, 0.20460318845723502, 0.20460318845723502, 0.20460318845723502, 0.08741913219627273, 0.08741913219627273, 0.08741913219627273, 0.10170633761050629, 0.10170633761050629, 0.10170633761050629, 0.08130864284712969, 0.08130864284712969, 0.08130864284712969]}, "mutation_prompt": null}
{"id": "9f73fed1-fa60-4752-8b29-a0d315a3ccb4", "solution": "import numpy as np\n\nclass PSO_SA_Optimizer_Enhanced_V3:\n    def __init__(self, budget, dim, pop_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.w_max = 0.9\n        self.w_min = 0.4\n        self.c1_init = 1.5\n        self.c2_init = 1.5\n        self.temp_init = 1.0\n\n    def __call__(self, func):\n        np.random.seed(42)\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.array([func(pos) for pos in positions])\n        global_best_position = personal_best_positions[np.argmin(personal_best_scores)]\n        global_best_score = np.min(personal_best_scores)\n        \n        eval_count = self.pop_size\n        temperature = self.temp_init\n        \n        while eval_count < self.budget:\n            w = self.w_max - ((self.w_max - self.w_min) * (eval_count / self.budget))\n            c1 = self.c1_init * (1 - eval_count / self.budget) + 0.5\n            c2 = self.c2_init * (eval_count / self.budget) + 0.5\n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                velocities[i] = (w * velocities[i]\n                                 + c1 * r1 * (personal_best_positions[i] - positions[i])\n                                 + c2 * r2 * (global_best_position - positions[i]))\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], self.lower_bound, self.upper_bound)\n\n                score = func(positions[i])\n                eval_count += 1\n                if eval_count >= self.budget:\n                    break\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = positions[i]\n\n                if np.random.rand() < np.exp(-(score - global_best_score) / temperature):\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n                    if score < global_best_score:\n                        global_best_score = score\n                        global_best_position = positions[i]\n\n            temperature *= 0.95  # Improved annealing factor for convergence\n\n        return global_best_position, global_best_score", "name": "PSO_SA_Optimizer_Enhanced_V3", "description": "Improved adaptive search strategy in PSO-SA with dynamically updating parameters and better temperature decay.", "configspace": "", "generation": 55, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "104cf98e-8138-4906-b55a-546d6de8214b", "metadata": {"aucs": [0.5051733381719483, 0.5051733381719483, 0.5051733381719483, 0.4864875265041222, 0.4864875265041222, 0.4864875265041222, 0.5980878545878154, 0.5980878545878154, 0.5980878545878154, 0.06730631129871123, 0.06730631129871123, 0.06730631129871123, 0.09301964064620494, 0.09301964064620494, 0.09301964064620494, 0.36676061687109374, 0.36676061687109374, 0.36676061687109374, 0.1093498751795694, 0.1093498751795694, 0.1093498751795694, 0.11800704322385025, 0.11800704322385025, 0.11800704322385025, 0.11014226708156238, 0.11014226708156238, 0.11014226708156238, 0.15004683892585147, 0.15004683892585147, 0.15004683892585147, 0.13033125670061696, 0.13033125670061696, 0.13033125670061696, 0.1176642388246808, 0.1176642388246808, 0.1176642388246808, 0.9817783524568978, 0.9817783524568978, 0.9817783524568978, 0.98749484089464, 0.98749484089464, 0.98749484089464, 0.9873373389794495, 0.9873373389794495, 0.9873373389794495, 0.3193023035569802, 0.3193023035569802, 0.3193023035569802, 0.4749253967860462, 0.4749253967860462, 0.4749253967860462, 0.33952636113240786, 0.33952636113240786, 0.33952636113240786, 0.23404906009051418, 0.23404906009051418, 0.23404906009051418, 0.1867241095540818, 0.1867241095540818, 0.1867241095540818, 0.22360410437078448, 0.22360410437078448, 0.22360410437078448, 0.38917833239145483, 0.38917833239145483, 0.38917833239145483, 0.17677793426833743, 0.17677793426833743, 0.17677793426833743, 0.1866522982738108, 0.1866522982738108, 0.1866522982738108, 0.1794288919186895, 0.1794288919186895, 0.1794288919186895, 0.1713064460088295, 0.1713064460088295, 0.1713064460088295, 0.1503593130339168, 0.1503593130339168, 0.1503593130339168, 0.16896150548229494, 0.16896150548229494, 0.16896150548229494, 0.1517238337156679, 0.1517238337156679, 0.1517238337156679, 0.07801075606316066, 0.07801075606316066, 0.07801075606316066, 0.1306258192704034, 0.1306258192704034, 0.1306258192704034, 0.043036206734873295, 0.043036206734873295, 0.043036206734873295, 0.1713736095711984, 0.1713736095711984, 0.1713736095711984, 0.03478423785365725, 0.03478423785365725, 0.03478423785365725, 0.19180632709776668, 0.19180632709776668, 0.19180632709776668, 0.0590246198988168, 0.0590246198988168, 0.0590246198988168, 0.20081792517374752, 0.20081792517374752, 0.20081792517374752, 0.033491506494330636, 0.033491506494330636, 0.033491506494330636, 0.05021043193870478, 0.05021043193870478, 0.05021043193870478, 0.5003727892349492, 0.5003727892349492, 0.5003727892349492, 0.4657333813986042, 0.4657333813986042, 0.4657333813986042, 0.46917186793370924, 0.46917186793370924, 0.46917186793370924, 0.14342207768301163, 0.14342207768301163, 0.14342207768301163, 0.12145871132800168, 0.12145871132800168, 0.12145871132800168, 0.1057475206242896, 0.1057475206242896, 0.1057475206242896, 0.35642057844886454, 0.35642057844886454, 0.35642057844886454, 0.21465072589842016, 0.21465072589842016, 0.21465072589842016, 0.2444189364073357, 0.2444189364073357, 0.2444189364073357, 0.286894944324837, 0.286894944324837, 0.286894944324837, 0.28791410732075406, 0.28791410732075406, 0.28791410732075406, 0.288665474020558, 0.288665474020558, 0.288665474020558, 0.22945855198095, 0.22945855198095, 0.22945855198095, 0.19982102082297581, 0.19982102082297581, 0.19982102082297581, 0.12103828021249086, 0.12103828021249086, 0.12103828021249086, 0.1874220403361898, 0.1874220403361898, 0.1874220403361898, 0.24617784072219195, 0.24617784072219195, 0.24617784072219195, 0.2752280251020164, 0.2752280251020164, 0.2752280251020164, 0.1880647070348399, 0.1880647070348399, 0.1880647070348399, 0.1738231871514354, 0.1738231871514354, 0.1738231871514354, 0.48468137841406633, 0.48468137841406633, 0.48468137841406633, 0.6910667162082013, 0.6910667162082013, 0.6910667162082013, 0.15879721238597644, 0.15879721238597644, 0.15879721238597644, 0.746649682486173, 0.746649682486173, 0.746649682486173, 0.1689669226231103, 0.1689669226231103, 0.1689669226231103, 0.21053313703713594, 0.21053313703713594, 0.21053313703713594, 0.16560864026155453, 0.16560864026155453, 0.16560864026155453, 0.19735138827661058, 0.19735138827661058, 0.19735138827661058, 0.17825965747894357, 0.17825965747894357, 0.17825965747894357, 0.20460318845723502, 0.20460318845723502, 0.20460318845723502, 0.08741913219627273, 0.08741913219627273, 0.08741913219627273, 0.10170633761050629, 0.10170633761050629, 0.10170633761050629, 0.08130864284712969, 0.08130864284712969, 0.08130864284712969]}, "mutation_prompt": null}
{"id": "c8c71d5e-3610-4c8a-8c11-bb7abbfba936", "solution": "import numpy as np\n\nclass PSO_SA_Optimizer_Enhanced_V3:\n    def __init__(self, budget, dim, pop_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.w_max = 0.9\n        self.w_min = 0.4\n        self.c1_init = 1.5\n        self.c2_init = 1.5\n        self.temp_init = 1.0\n\n    def __call__(self, func):\n        np.random.seed(42)\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.array([func(pos) for pos in positions])\n        global_best_position = personal_best_positions[np.argmin(personal_best_scores)]\n        global_best_score = np.min(personal_best_scores)\n        \n        eval_count = self.pop_size\n        temperature = self.temp_init\n        \n        while eval_count < self.budget:\n            w = self.w_max - ((self.w_max - self.w_min) * (eval_count / self.budget))\n            c1 = self.c1_init * (1 - eval_count / self.budget) + 0.5\n            c2 = self.c2_init * (eval_count / self.budget) + 0.5\n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                velocities[i] = (w * velocities[i]\n                                 + c1 * r1 * (personal_best_positions[i] - positions[i])\n                                 + c2 * r2 * (global_best_position - positions[i]))\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], self.lower_bound, self.upper_bound)\n\n                score = func(positions[i])\n                eval_count += 1\n                if eval_count >= self.budget:\n                    break\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = positions[i]\n\n                if np.random.rand() < np.exp(-(score - global_best_score) / temperature):\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n                    if score < global_best_score:\n                        global_best_score = score\n                        global_best_position = positions[i]\n\n            temperature *= 0.95  # Improved annealing factor for convergence\n\n        return global_best_position, global_best_score", "name": "PSO_SA_Optimizer_Enhanced_V3", "description": "Improved adaptive search strategy in PSO-SA with dynamically updating parameters and better temperature decay.", "configspace": "", "generation": 55, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "104cf98e-8138-4906-b55a-546d6de8214b", "metadata": {"aucs": [0.5051733381719483, 0.5051733381719483, 0.5051733381719483, 0.4864875265041222, 0.4864875265041222, 0.4864875265041222, 0.5980878545878154, 0.5980878545878154, 0.5980878545878154, 0.06730631129871123, 0.06730631129871123, 0.06730631129871123, 0.09301964064620494, 0.09301964064620494, 0.09301964064620494, 0.36676061687109374, 0.36676061687109374, 0.36676061687109374, 0.1093498751795694, 0.1093498751795694, 0.1093498751795694, 0.11800704322385025, 0.11800704322385025, 0.11800704322385025, 0.11014226708156238, 0.11014226708156238, 0.11014226708156238, 0.15004683892585147, 0.15004683892585147, 0.15004683892585147, 0.13033125670061696, 0.13033125670061696, 0.13033125670061696, 0.1176642388246808, 0.1176642388246808, 0.1176642388246808, 0.9817783524568978, 0.9817783524568978, 0.9817783524568978, 0.98749484089464, 0.98749484089464, 0.98749484089464, 0.9873373389794495, 0.9873373389794495, 0.9873373389794495, 0.3193023035569802, 0.3193023035569802, 0.3193023035569802, 0.4749253967860462, 0.4749253967860462, 0.4749253967860462, 0.33952636113240786, 0.33952636113240786, 0.33952636113240786, 0.23404906009051418, 0.23404906009051418, 0.23404906009051418, 0.1867241095540818, 0.1867241095540818, 0.1867241095540818, 0.22360410437078448, 0.22360410437078448, 0.22360410437078448, 0.38917833239145483, 0.38917833239145483, 0.38917833239145483, 0.17677793426833743, 0.17677793426833743, 0.17677793426833743, 0.1866522982738108, 0.1866522982738108, 0.1866522982738108, 0.1794288919186895, 0.1794288919186895, 0.1794288919186895, 0.1713064460088295, 0.1713064460088295, 0.1713064460088295, 0.1503593130339168, 0.1503593130339168, 0.1503593130339168, 0.16896150548229494, 0.16896150548229494, 0.16896150548229494, 0.1517238337156679, 0.1517238337156679, 0.1517238337156679, 0.07801075606316066, 0.07801075606316066, 0.07801075606316066, 0.1306258192704034, 0.1306258192704034, 0.1306258192704034, 0.043036206734873295, 0.043036206734873295, 0.043036206734873295, 0.1713736095711984, 0.1713736095711984, 0.1713736095711984, 0.03478423785365725, 0.03478423785365725, 0.03478423785365725, 0.19180632709776668, 0.19180632709776668, 0.19180632709776668, 0.0590246198988168, 0.0590246198988168, 0.0590246198988168, 0.20081792517374752, 0.20081792517374752, 0.20081792517374752, 0.033491506494330636, 0.033491506494330636, 0.033491506494330636, 0.05021043193870478, 0.05021043193870478, 0.05021043193870478, 0.5003727892349492, 0.5003727892349492, 0.5003727892349492, 0.4657333813986042, 0.4657333813986042, 0.4657333813986042, 0.46917186793370924, 0.46917186793370924, 0.46917186793370924, 0.14342207768301163, 0.14342207768301163, 0.14342207768301163, 0.12145871132800168, 0.12145871132800168, 0.12145871132800168, 0.1057475206242896, 0.1057475206242896, 0.1057475206242896, 0.35642057844886454, 0.35642057844886454, 0.35642057844886454, 0.21465072589842016, 0.21465072589842016, 0.21465072589842016, 0.2444189364073357, 0.2444189364073357, 0.2444189364073357, 0.286894944324837, 0.286894944324837, 0.286894944324837, 0.28791410732075406, 0.28791410732075406, 0.28791410732075406, 0.288665474020558, 0.288665474020558, 0.288665474020558, 0.22945855198095, 0.22945855198095, 0.22945855198095, 0.19982102082297581, 0.19982102082297581, 0.19982102082297581, 0.12103828021249086, 0.12103828021249086, 0.12103828021249086, 0.1874220403361898, 0.1874220403361898, 0.1874220403361898, 0.24617784072219195, 0.24617784072219195, 0.24617784072219195, 0.2752280251020164, 0.2752280251020164, 0.2752280251020164, 0.1880647070348399, 0.1880647070348399, 0.1880647070348399, 0.1738231871514354, 0.1738231871514354, 0.1738231871514354, 0.48468137841406633, 0.48468137841406633, 0.48468137841406633, 0.6910667162082013, 0.6910667162082013, 0.6910667162082013, 0.15879721238597644, 0.15879721238597644, 0.15879721238597644, 0.746649682486173, 0.746649682486173, 0.746649682486173, 0.1689669226231103, 0.1689669226231103, 0.1689669226231103, 0.21053313703713594, 0.21053313703713594, 0.21053313703713594, 0.16560864026155453, 0.16560864026155453, 0.16560864026155453, 0.19735138827661058, 0.19735138827661058, 0.19735138827661058, 0.17825965747894357, 0.17825965747894357, 0.17825965747894357, 0.20460318845723502, 0.20460318845723502, 0.20460318845723502, 0.08741913219627273, 0.08741913219627273, 0.08741913219627273, 0.10170633761050629, 0.10170633761050629, 0.10170633761050629, 0.08130864284712969, 0.08130864284712969, 0.08130864284712969]}, "mutation_prompt": null}
{"id": "695326be-c77d-48fe-a996-63c07c51ef92", "solution": "import numpy as np\n\nclass PSO_SA_Optimizer_Enhanced_V3:\n    def __init__(self, budget, dim, pop_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.w_max = 0.9\n        self.w_min = 0.4\n        self.c1_init = 1.5\n        self.c2_init = 1.5\n        self.temp_init = 1.0\n\n    def __call__(self, func):\n        np.random.seed(42)\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.array([func(pos) for pos in positions])\n        global_best_position = personal_best_positions[np.argmin(personal_best_scores)]\n        global_best_score = np.min(personal_best_scores)\n        \n        eval_count = self.pop_size\n        temperature = self.temp_init\n        \n        while eval_count < self.budget:\n            w = self.w_max - ((self.w_max - self.w_min) * (eval_count / self.budget))\n            c1 = self.c1_init * (1 - eval_count / self.budget) + 0.5\n            c2 = self.c2_init * (eval_count / self.budget) + 0.5\n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                velocities[i] = (w * velocities[i]\n                                 + c1 * r1 * (personal_best_positions[i] - positions[i])\n                                 + c2 * r2 * (global_best_position - positions[i]))\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], self.lower_bound, self.upper_bound)\n\n                score = func(positions[i])\n                eval_count += 1\n                if eval_count >= self.budget:\n                    break\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = positions[i]\n\n                if np.random.rand() < np.exp(-(score - global_best_score) / temperature):\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n                    if score < global_best_score:\n                        global_best_score = score\n                        global_best_position = positions[i]\n\n            temperature *= 0.95  # Improved annealing factor for convergence\n\n        return global_best_position, global_best_score", "name": "PSO_SA_Optimizer_Enhanced_V3", "description": "Improved adaptive search strategy in PSO-SA with dynamically updating parameters and better temperature decay.", "configspace": "", "generation": 55, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "104cf98e-8138-4906-b55a-546d6de8214b", "metadata": {"aucs": [0.5051733381719483, 0.5051733381719483, 0.5051733381719483, 0.4864875265041222, 0.4864875265041222, 0.4864875265041222, 0.5980878545878154, 0.5980878545878154, 0.5980878545878154, 0.06730631129871123, 0.06730631129871123, 0.06730631129871123, 0.09301964064620494, 0.09301964064620494, 0.09301964064620494, 0.36676061687109374, 0.36676061687109374, 0.36676061687109374, 0.1093498751795694, 0.1093498751795694, 0.1093498751795694, 0.11800704322385025, 0.11800704322385025, 0.11800704322385025, 0.11014226708156238, 0.11014226708156238, 0.11014226708156238, 0.15004683892585147, 0.15004683892585147, 0.15004683892585147, 0.13033125670061696, 0.13033125670061696, 0.13033125670061696, 0.1176642388246808, 0.1176642388246808, 0.1176642388246808, 0.9817783524568978, 0.9817783524568978, 0.9817783524568978, 0.98749484089464, 0.98749484089464, 0.98749484089464, 0.9873373389794495, 0.9873373389794495, 0.9873373389794495, 0.3193023035569802, 0.3193023035569802, 0.3193023035569802, 0.4749253967860462, 0.4749253967860462, 0.4749253967860462, 0.33952636113240786, 0.33952636113240786, 0.33952636113240786, 0.23404906009051418, 0.23404906009051418, 0.23404906009051418, 0.1867241095540818, 0.1867241095540818, 0.1867241095540818, 0.22360410437078448, 0.22360410437078448, 0.22360410437078448, 0.38917833239145483, 0.38917833239145483, 0.38917833239145483, 0.17677793426833743, 0.17677793426833743, 0.17677793426833743, 0.1866522982738108, 0.1866522982738108, 0.1866522982738108, 0.1794288919186895, 0.1794288919186895, 0.1794288919186895, 0.1713064460088295, 0.1713064460088295, 0.1713064460088295, 0.1503593130339168, 0.1503593130339168, 0.1503593130339168, 0.16896150548229494, 0.16896150548229494, 0.16896150548229494, 0.1517238337156679, 0.1517238337156679, 0.1517238337156679, 0.07801075606316066, 0.07801075606316066, 0.07801075606316066, 0.1306258192704034, 0.1306258192704034, 0.1306258192704034, 0.043036206734873295, 0.043036206734873295, 0.043036206734873295, 0.1713736095711984, 0.1713736095711984, 0.1713736095711984, 0.03478423785365725, 0.03478423785365725, 0.03478423785365725, 0.19180632709776668, 0.19180632709776668, 0.19180632709776668, 0.0590246198988168, 0.0590246198988168, 0.0590246198988168, 0.20081792517374752, 0.20081792517374752, 0.20081792517374752, 0.033491506494330636, 0.033491506494330636, 0.033491506494330636, 0.05021043193870478, 0.05021043193870478, 0.05021043193870478, 0.5003727892349492, 0.5003727892349492, 0.5003727892349492, 0.4657333813986042, 0.4657333813986042, 0.4657333813986042, 0.46917186793370924, 0.46917186793370924, 0.46917186793370924, 0.14342207768301163, 0.14342207768301163, 0.14342207768301163, 0.12145871132800168, 0.12145871132800168, 0.12145871132800168, 0.1057475206242896, 0.1057475206242896, 0.1057475206242896, 0.35642057844886454, 0.35642057844886454, 0.35642057844886454, 0.21465072589842016, 0.21465072589842016, 0.21465072589842016, 0.2444189364073357, 0.2444189364073357, 0.2444189364073357, 0.286894944324837, 0.286894944324837, 0.286894944324837, 0.28791410732075406, 0.28791410732075406, 0.28791410732075406, 0.288665474020558, 0.288665474020558, 0.288665474020558, 0.22945855198095, 0.22945855198095, 0.22945855198095, 0.19982102082297581, 0.19982102082297581, 0.19982102082297581, 0.12103828021249086, 0.12103828021249086, 0.12103828021249086, 0.1874220403361898, 0.1874220403361898, 0.1874220403361898, 0.24617784072219195, 0.24617784072219195, 0.24617784072219195, 0.2752280251020164, 0.2752280251020164, 0.2752280251020164, 0.1880647070348399, 0.1880647070348399, 0.1880647070348399, 0.1738231871514354, 0.1738231871514354, 0.1738231871514354, 0.48468137841406633, 0.48468137841406633, 0.48468137841406633, 0.6910667162082013, 0.6910667162082013, 0.6910667162082013, 0.15879721238597644, 0.15879721238597644, 0.15879721238597644, 0.746649682486173, 0.746649682486173, 0.746649682486173, 0.1689669226231103, 0.1689669226231103, 0.1689669226231103, 0.21053313703713594, 0.21053313703713594, 0.21053313703713594, 0.16560864026155453, 0.16560864026155453, 0.16560864026155453, 0.19735138827661058, 0.19735138827661058, 0.19735138827661058, 0.17825965747894357, 0.17825965747894357, 0.17825965747894357, 0.20460318845723502, 0.20460318845723502, 0.20460318845723502, 0.08741913219627273, 0.08741913219627273, 0.08741913219627273, 0.10170633761050629, 0.10170633761050629, 0.10170633761050629, 0.08130864284712969, 0.08130864284712969, 0.08130864284712969]}, "mutation_prompt": null}
{"id": "61be9beb-aa01-4f3c-a901-e5f4de7e0f70", "solution": "import numpy as np\n\nclass PSO_SA_Optimizer_Enhanced_V3:\n    def __init__(self, budget, dim, pop_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.w_max = 0.9\n        self.w_min = 0.4\n        self.c1_init = 1.5\n        self.c2_init = 1.5\n        self.temp_init = 1.0\n\n    def __call__(self, func):\n        np.random.seed(42)\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.array([func(pos) for pos in positions])\n        global_best_position = personal_best_positions[np.argmin(personal_best_scores)]\n        global_best_score = np.min(personal_best_scores)\n        \n        eval_count = self.pop_size\n        temperature = self.temp_init\n        \n        while eval_count < self.budget:\n            w = self.w_max - ((self.w_max - self.w_min) * (eval_count / self.budget))\n            c1 = self.c1_init * (1 - eval_count / self.budget) + 0.5\n            c2 = self.c2_init * (eval_count / self.budget) + 0.5\n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                velocities[i] = (w * velocities[i]\n                                 + c1 * r1 * (personal_best_positions[i] - positions[i])\n                                 + c2 * r2 * (global_best_position - positions[i]))\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], self.lower_bound, self.upper_bound)\n\n                score = func(positions[i])\n                eval_count += 1\n                if eval_count >= self.budget:\n                    break\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = positions[i]\n\n                if np.random.rand() < np.exp(-(score - global_best_score) / temperature):\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n                    if score < global_best_score:\n                        global_best_score = score\n                        global_best_position = positions[i]\n\n            temperature *= 0.95  # Improved annealing factor for convergence\n\n        return global_best_position, global_best_score", "name": "PSO_SA_Optimizer_Enhanced_V3", "description": "Improved adaptive search strategy in PSO-SA with dynamically updating parameters and better temperature decay.", "configspace": "", "generation": 55, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "104cf98e-8138-4906-b55a-546d6de8214b", "metadata": {"aucs": [0.5051733381719483, 0.5051733381719483, 0.5051733381719483, 0.4864875265041222, 0.4864875265041222, 0.4864875265041222, 0.5980878545878154, 0.5980878545878154, 0.5980878545878154, 0.06730631129871123, 0.06730631129871123, 0.06730631129871123, 0.09301964064620494, 0.09301964064620494, 0.09301964064620494, 0.36676061687109374, 0.36676061687109374, 0.36676061687109374, 0.1093498751795694, 0.1093498751795694, 0.1093498751795694, 0.11800704322385025, 0.11800704322385025, 0.11800704322385025, 0.11014226708156238, 0.11014226708156238, 0.11014226708156238, 0.15004683892585147, 0.15004683892585147, 0.15004683892585147, 0.13033125670061696, 0.13033125670061696, 0.13033125670061696, 0.1176642388246808, 0.1176642388246808, 0.1176642388246808, 0.9817783524568978, 0.9817783524568978, 0.9817783524568978, 0.98749484089464, 0.98749484089464, 0.98749484089464, 0.9873373389794495, 0.9873373389794495, 0.9873373389794495, 0.3193023035569802, 0.3193023035569802, 0.3193023035569802, 0.4749253967860462, 0.4749253967860462, 0.4749253967860462, 0.33952636113240786, 0.33952636113240786, 0.33952636113240786, 0.23404906009051418, 0.23404906009051418, 0.23404906009051418, 0.1867241095540818, 0.1867241095540818, 0.1867241095540818, 0.22360410437078448, 0.22360410437078448, 0.22360410437078448, 0.38917833239145483, 0.38917833239145483, 0.38917833239145483, 0.17677793426833743, 0.17677793426833743, 0.17677793426833743, 0.1866522982738108, 0.1866522982738108, 0.1866522982738108, 0.1794288919186895, 0.1794288919186895, 0.1794288919186895, 0.1713064460088295, 0.1713064460088295, 0.1713064460088295, 0.1503593130339168, 0.1503593130339168, 0.1503593130339168, 0.16896150548229494, 0.16896150548229494, 0.16896150548229494, 0.1517238337156679, 0.1517238337156679, 0.1517238337156679, 0.07801075606316066, 0.07801075606316066, 0.07801075606316066, 0.1306258192704034, 0.1306258192704034, 0.1306258192704034, 0.043036206734873295, 0.043036206734873295, 0.043036206734873295, 0.1713736095711984, 0.1713736095711984, 0.1713736095711984, 0.03478423785365725, 0.03478423785365725, 0.03478423785365725, 0.19180632709776668, 0.19180632709776668, 0.19180632709776668, 0.0590246198988168, 0.0590246198988168, 0.0590246198988168, 0.20081792517374752, 0.20081792517374752, 0.20081792517374752, 0.033491506494330636, 0.033491506494330636, 0.033491506494330636, 0.05021043193870478, 0.05021043193870478, 0.05021043193870478, 0.5003727892349492, 0.5003727892349492, 0.5003727892349492, 0.4657333813986042, 0.4657333813986042, 0.4657333813986042, 0.46917186793370924, 0.46917186793370924, 0.46917186793370924, 0.14342207768301163, 0.14342207768301163, 0.14342207768301163, 0.12145871132800168, 0.12145871132800168, 0.12145871132800168, 0.1057475206242896, 0.1057475206242896, 0.1057475206242896, 0.35642057844886454, 0.35642057844886454, 0.35642057844886454, 0.21465072589842016, 0.21465072589842016, 0.21465072589842016, 0.2444189364073357, 0.2444189364073357, 0.2444189364073357, 0.286894944324837, 0.286894944324837, 0.286894944324837, 0.28791410732075406, 0.28791410732075406, 0.28791410732075406, 0.288665474020558, 0.288665474020558, 0.288665474020558, 0.22945855198095, 0.22945855198095, 0.22945855198095, 0.19982102082297581, 0.19982102082297581, 0.19982102082297581, 0.12103828021249086, 0.12103828021249086, 0.12103828021249086, 0.1874220403361898, 0.1874220403361898, 0.1874220403361898, 0.24617784072219195, 0.24617784072219195, 0.24617784072219195, 0.2752280251020164, 0.2752280251020164, 0.2752280251020164, 0.1880647070348399, 0.1880647070348399, 0.1880647070348399, 0.1738231871514354, 0.1738231871514354, 0.1738231871514354, 0.48468137841406633, 0.48468137841406633, 0.48468137841406633, 0.6910667162082013, 0.6910667162082013, 0.6910667162082013, 0.15879721238597644, 0.15879721238597644, 0.15879721238597644, 0.746649682486173, 0.746649682486173, 0.746649682486173, 0.1689669226231103, 0.1689669226231103, 0.1689669226231103, 0.21053313703713594, 0.21053313703713594, 0.21053313703713594, 0.16560864026155453, 0.16560864026155453, 0.16560864026155453, 0.19735138827661058, 0.19735138827661058, 0.19735138827661058, 0.17825965747894357, 0.17825965747894357, 0.17825965747894357, 0.20460318845723502, 0.20460318845723502, 0.20460318845723502, 0.08741913219627273, 0.08741913219627273, 0.08741913219627273, 0.10170633761050629, 0.10170633761050629, 0.10170633761050629, 0.08130864284712969, 0.08130864284712969, 0.08130864284712969]}, "mutation_prompt": null}
{"id": "ad7d3941-1c14-4f02-b1f0-71f676b4312b", "solution": "import numpy as np\n\nclass PSO_SA_Optimizer_Enhanced_V3:\n    def __init__(self, budget, dim, pop_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.w_max = 0.9\n        self.w_min = 0.4\n        self.c1_init = 1.5\n        self.c2_init = 1.5\n        self.temp_init = 1.0\n\n    def __call__(self, func):\n        np.random.seed(42)\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.array([func(pos) for pos in positions])\n        global_best_position = personal_best_positions[np.argmin(personal_best_scores)]\n        global_best_score = np.min(personal_best_scores)\n        \n        eval_count = self.pop_size\n        temperature = self.temp_init\n        \n        while eval_count < self.budget:\n            w = self.w_max - ((self.w_max - self.w_min) * (eval_count / self.budget))\n            c1 = self.c1_init * (1 - eval_count / self.budget) + 0.5\n            c2 = self.c2_init * (eval_count / self.budget) + 0.5\n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                velocities[i] = (w * velocities[i]\n                                 + c1 * r1 * (personal_best_positions[i] - positions[i])\n                                 + c2 * r2 * (global_best_position - positions[i]))\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], self.lower_bound, self.upper_bound)\n\n                score = func(positions[i])\n                eval_count += 1\n                if eval_count >= self.budget:\n                    break\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = positions[i]\n\n                if np.random.rand() < np.exp(-(score - global_best_score) / temperature):\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n                    if score < global_best_score:\n                        global_best_score = score\n                        global_best_position = positions[i]\n\n            temperature *= 0.95  # Improved annealing factor for convergence\n\n        return global_best_position, global_best_score", "name": "PSO_SA_Optimizer_Enhanced_V3", "description": "Improved adaptive search strategy in PSO-SA with dynamically updating parameters and better temperature decay.", "configspace": "", "generation": 55, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "104cf98e-8138-4906-b55a-546d6de8214b", "metadata": {"aucs": [0.5051733381719483, 0.5051733381719483, 0.5051733381719483, 0.4864875265041222, 0.4864875265041222, 0.4864875265041222, 0.5980878545878154, 0.5980878545878154, 0.5980878545878154, 0.06730631129871123, 0.06730631129871123, 0.06730631129871123, 0.09301964064620494, 0.09301964064620494, 0.09301964064620494, 0.36676061687109374, 0.36676061687109374, 0.36676061687109374, 0.1093498751795694, 0.1093498751795694, 0.1093498751795694, 0.11800704322385025, 0.11800704322385025, 0.11800704322385025, 0.11014226708156238, 0.11014226708156238, 0.11014226708156238, 0.15004683892585147, 0.15004683892585147, 0.15004683892585147, 0.13033125670061696, 0.13033125670061696, 0.13033125670061696, 0.1176642388246808, 0.1176642388246808, 0.1176642388246808, 0.9817783524568978, 0.9817783524568978, 0.9817783524568978, 0.98749484089464, 0.98749484089464, 0.98749484089464, 0.9873373389794495, 0.9873373389794495, 0.9873373389794495, 0.3193023035569802, 0.3193023035569802, 0.3193023035569802, 0.4749253967860462, 0.4749253967860462, 0.4749253967860462, 0.33952636113240786, 0.33952636113240786, 0.33952636113240786, 0.23404906009051418, 0.23404906009051418, 0.23404906009051418, 0.1867241095540818, 0.1867241095540818, 0.1867241095540818, 0.22360410437078448, 0.22360410437078448, 0.22360410437078448, 0.38917833239145483, 0.38917833239145483, 0.38917833239145483, 0.17677793426833743, 0.17677793426833743, 0.17677793426833743, 0.1866522982738108, 0.1866522982738108, 0.1866522982738108, 0.1794288919186895, 0.1794288919186895, 0.1794288919186895, 0.1713064460088295, 0.1713064460088295, 0.1713064460088295, 0.1503593130339168, 0.1503593130339168, 0.1503593130339168, 0.16896150548229494, 0.16896150548229494, 0.16896150548229494, 0.1517238337156679, 0.1517238337156679, 0.1517238337156679, 0.07801075606316066, 0.07801075606316066, 0.07801075606316066, 0.1306258192704034, 0.1306258192704034, 0.1306258192704034, 0.043036206734873295, 0.043036206734873295, 0.043036206734873295, 0.1713736095711984, 0.1713736095711984, 0.1713736095711984, 0.03478423785365725, 0.03478423785365725, 0.03478423785365725, 0.19180632709776668, 0.19180632709776668, 0.19180632709776668, 0.0590246198988168, 0.0590246198988168, 0.0590246198988168, 0.20081792517374752, 0.20081792517374752, 0.20081792517374752, 0.033491506494330636, 0.033491506494330636, 0.033491506494330636, 0.05021043193870478, 0.05021043193870478, 0.05021043193870478, 0.5003727892349492, 0.5003727892349492, 0.5003727892349492, 0.4657333813986042, 0.4657333813986042, 0.4657333813986042, 0.46917186793370924, 0.46917186793370924, 0.46917186793370924, 0.14342207768301163, 0.14342207768301163, 0.14342207768301163, 0.12145871132800168, 0.12145871132800168, 0.12145871132800168, 0.1057475206242896, 0.1057475206242896, 0.1057475206242896, 0.35642057844886454, 0.35642057844886454, 0.35642057844886454, 0.21465072589842016, 0.21465072589842016, 0.21465072589842016, 0.2444189364073357, 0.2444189364073357, 0.2444189364073357, 0.286894944324837, 0.286894944324837, 0.286894944324837, 0.28791410732075406, 0.28791410732075406, 0.28791410732075406, 0.288665474020558, 0.288665474020558, 0.288665474020558, 0.22945855198095, 0.22945855198095, 0.22945855198095, 0.19982102082297581, 0.19982102082297581, 0.19982102082297581, 0.12103828021249086, 0.12103828021249086, 0.12103828021249086, 0.1874220403361898, 0.1874220403361898, 0.1874220403361898, 0.24617784072219195, 0.24617784072219195, 0.24617784072219195, 0.2752280251020164, 0.2752280251020164, 0.2752280251020164, 0.1880647070348399, 0.1880647070348399, 0.1880647070348399, 0.1738231871514354, 0.1738231871514354, 0.1738231871514354, 0.48468137841406633, 0.48468137841406633, 0.48468137841406633, 0.6910667162082013, 0.6910667162082013, 0.6910667162082013, 0.15879721238597644, 0.15879721238597644, 0.15879721238597644, 0.746649682486173, 0.746649682486173, 0.746649682486173, 0.1689669226231103, 0.1689669226231103, 0.1689669226231103, 0.21053313703713594, 0.21053313703713594, 0.21053313703713594, 0.16560864026155453, 0.16560864026155453, 0.16560864026155453, 0.19735138827661058, 0.19735138827661058, 0.19735138827661058, 0.17825965747894357, 0.17825965747894357, 0.17825965747894357, 0.20460318845723502, 0.20460318845723502, 0.20460318845723502, 0.08741913219627273, 0.08741913219627273, 0.08741913219627273, 0.10170633761050629, 0.10170633761050629, 0.10170633761050629, 0.08130864284712969, 0.08130864284712969, 0.08130864284712969]}, "mutation_prompt": null}
{"id": "9846bbb0-408b-4726-9fe7-a1530d51b6b9", "solution": "import numpy as np\n\nclass PSO_SA_Optimizer_Adaptive:\n    def __init__(self, budget, dim, pop_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.w_max = 0.9\n        self.w_min = 0.4\n        self.c1_init = 1.7  # Adjusted to increase exploration\n        self.c2_init = 1.3  # Adjusted for better exploitation\n        self.temp_init = 1.0\n\n    def __call__(self, func):\n        np.random.seed(42)\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.array([func(pos) for pos in positions])\n        global_best_position = personal_best_positions[np.argmin(personal_best_scores)]\n        global_best_score = np.min(personal_best_scores)\n\n        eval_count = self.pop_size\n        temperature = self.temp_init\n\n        while eval_count < self.budget:\n            w = self.w_max - (self.w_max - self.w_min) * (eval_count / self.budget) ** 1.5\n            c1 = self.c1_init * (1 - eval_count / self.budget) + 0.3\n            c2 = self.c2_init * (eval_count / self.budget) + 0.7\n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                velocities[i] = (w * velocities[i]\n                                 + c1 * r1 * (personal_best_positions[i] - positions[i])\n                                 + c2 * r2 * (global_best_position - positions[i]))\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], self.lower_bound, self.upper_bound)\n\n                score = func(positions[i])\n                eval_count += 1\n                if eval_count >= self.budget:\n                    break\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = positions[i]\n\n                if np.random.rand() < np.exp(-(score - global_best_score) / temperature):\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n                    if score < global_best_score:\n                        global_best_score = score\n                        global_best_position = positions[i]\n\n            temperature *= 0.93  # Refined annealing process for stability\n\n        return global_best_position, global_best_score", "name": "PSO_SA_Optimizer_Adaptive", "description": "Introducing an adaptive inertia weight decay and enhanced velocity update to improve convergence efficiency.", "configspace": "", "generation": 72, "fitness": 0.25045439195603575, "feedback": "The algorithm PSO_SA_Optimizer_Adaptive got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.25 with standard deviation 0.23.", "error": "", "parent_id": "104cf98e-8138-4906-b55a-546d6de8214b", "metadata": {"aucs": [0.6131118450882375, 0.6131118450882375, 0.6131118450882375, 0.6129837393371418, 0.6129837393371418, 0.6129837393371418, 0.6080683900047671, 0.6080683900047671, 0.6080683900047671, 0.1888010998198142, 0.1888010998198142, 0.1888010998198142, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.05847180515246242, 0.05847180515246242, 0.05847180515246242, 0.12267211737308248, 0.12267211737308248, 0.12267211737308248, 0.14321429521678297, 0.14321429521678297, 0.14321429521678297, 0.12045046324559594, 0.12045046324559594, 0.12045046324559594, 0.09301651785139498, 0.09301651785139498, 0.09301651785139498, 0.0941294275165705, 0.0941294275165705, 0.0941294275165705, 0.08000342286248285, 0.08000342286248285, 0.08000342286248285, 0.9864737223259097, 0.9864737223259097, 0.9864737223259097, 0.9897852648655497, 0.9897852648655497, 0.9897852648655497, 0.9884242167174468, 0.9884242167174468, 0.9884242167174468, 0.274780907749298, 0.274780907749298, 0.274780907749298, 0.4805711989280784, 0.4805711989280784, 0.4805711989280784, 0.3075906415442028, 0.3075906415442028, 0.3075906415442028, 0.3070571817690353, 0.3070571817690353, 0.3070571817690353, 0.2061503425848169, 0.2061503425848169, 0.2061503425848169, 0.7491192470883612, 0.7491192470883612, 0.7491192470883612, 0.19136045887737174, 0.19136045887737174, 0.19136045887737174, 0.11825112882552857, 0.11825112882552857, 0.11825112882552857, 0.12634352129431003, 0.12634352129431003, 0.12634352129431003, 0.35675301777456603, 0.35675301777456603, 0.35675301777456603, 0.11559582963828074, 0.11559582963828074, 0.11559582963828074, 0.1823522291613645, 0.1823522291613645, 0.1823522291613645, 0.025757995664961175, 0.025757995664961175, 0.025757995664961175, 0.03447192019167156, 0.03447192019167156, 0.03447192019167156, 0.000991064225218219, 0.000991064225218219, 0.000991064225218219, 0.1492281370059162, 0.1492281370059162, 0.1492281370059162, 0.027591791195328397, 0.027591791195328397, 0.027591791195328397, 0.16314877863771893, 0.16314877863771893, 0.16314877863771893, 0.030609880925799415, 0.030609880925799415, 0.030609880925799415, 0.08399288252422898, 0.08399288252422898, 0.08399288252422898, 0.06536704732197962, 0.06536704732197962, 0.06536704732197962, 0.1248009098186098, 0.1248009098186098, 0.1248009098186098, 0.14344229520303708, 0.14344229520303708, 0.14344229520303708, 0.06333582487731215, 0.06333582487731215, 0.06333582487731215, 0.44170265319785895, 0.44170265319785895, 0.44170265319785895, 0.43721108182403545, 0.43721108182403545, 0.43721108182403545, 0.4403960088690585, 0.4403960088690585, 0.4403960088690585, 0.08330162379201389, 0.08330162379201389, 0.08330162379201389, 0.11266576610287171, 0.11266576610287171, 0.11266576610287171, 0.13419432945972554, 0.13419432945972554, 0.13419432945972554, 0.31609770857750574, 0.31609770857750574, 0.31609770857750574, 0.19464842502644963, 0.19464842502644963, 0.19464842502644963, 0.150127176522573, 0.150127176522573, 0.150127176522573, 0.36571496903580114, 0.36571496903580114, 0.36571496903580114, 0.2598478642141625, 0.2598478642141625, 0.2598478642141625, 0.2664149084717212, 0.2664149084717212, 0.2664149084717212, 0.17046332292037603, 0.17046332292037603, 0.17046332292037603, 0.24494234960759964, 0.24494234960759964, 0.24494234960759964, 0.12337393417317066, 0.12337393417317066, 0.12337393417317066, 0.21351614951313258, 0.21351614951313258, 0.21351614951313258, 0.2094901190226276, 0.2094901190226276, 0.2094901190226276, 0.2210512026906818, 0.2210512026906818, 0.2210512026906818, 0.2096804255545286, 0.2096804255545286, 0.2096804255545286, 0.18702987149839068, 0.18702987149839068, 0.18702987149839068, 0.20626163082314908, 0.20626163082314908, 0.20626163082314908, 0.823472813193427, 0.823472813193427, 0.823472813193427, 0.16590642445320225, 0.16590642445320225, 0.16590642445320225, 0.6798135592346484, 0.6798135592346484, 0.6798135592346484, 0.16809897906547122, 0.16809897906547122, 0.16809897906547122, 0.20439132935468596, 0.20439132935468596, 0.20439132935468596, 0.15154345439051742, 0.15154345439051742, 0.15154345439051742, 0.19122879903938128, 0.19122879903938128, 0.19122879903938128, 0.18518055978800185, 0.18518055978800185, 0.18518055978800185, 0.19675046658689987, 0.19675046658689987, 0.19675046658689987, 0.08062019983993174, 0.08062019983993174, 0.08062019983993174, 0.0876437241942638, 0.0876437241942638, 0.0876437241942638, 0.08156383059247163, 0.08156383059247163, 0.08156383059247163]}, "mutation_prompt": null}
{"id": "efb221d4-3494-4239-aec1-0d217ce7b806", "solution": "import numpy as np\n\nclass PSO_SA_Optimizer_Enhanced_V3:\n    def __init__(self, budget, dim, pop_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.w_max = 0.9\n        self.w_min = 0.4\n        self.c1_init = 1.5\n        self.c2_init = 1.5\n        self.temp_init = 1.0\n\n    def __call__(self, func):\n        np.random.seed(42)\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.array([func(pos) for pos in positions])\n        global_best_position = personal_best_positions[np.argmin(personal_best_scores)]\n        global_best_score = np.min(personal_best_scores)\n        \n        eval_count = self.pop_size\n        temperature = self.temp_init\n        \n        while eval_count < self.budget:\n            w = self.w_max - ((self.w_max - self.w_min) * (eval_count / self.budget))\n            c1 = self.c1_init * (1 - eval_count / self.budget) + 0.5\n            c2 = self.c2_init * (eval_count / self.budget) + 0.5\n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                velocities[i] = (w * velocities[i]\n                                 + c1 * r1 * (personal_best_positions[i] - positions[i])\n                                 + c2 * r2 * (global_best_position - positions[i]))\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], self.lower_bound, self.upper_bound)\n\n                score = func(positions[i])\n                eval_count += 1\n                if eval_count >= self.budget:\n                    break\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = positions[i]\n\n                if np.random.rand() < np.exp(-(score - global_best_score) / temperature):\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n                    if score < global_best_score:\n                        global_best_score = score\n                        global_best_position = positions[i]\n\n            temperature *= 0.95  # Improved annealing factor for convergence\n\n        return global_best_position, global_best_score", "name": "PSO_SA_Optimizer_Enhanced_V3", "description": "Improved adaptive search strategy in PSO-SA with dynamically updating parameters and better temperature decay.", "configspace": "", "generation": 55, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "104cf98e-8138-4906-b55a-546d6de8214b", "metadata": {"aucs": [0.5051733381719483, 0.5051733381719483, 0.5051733381719483, 0.4864875265041222, 0.4864875265041222, 0.4864875265041222, 0.5980878545878154, 0.5980878545878154, 0.5980878545878154, 0.06730631129871123, 0.06730631129871123, 0.06730631129871123, 0.09301964064620494, 0.09301964064620494, 0.09301964064620494, 0.36676061687109374, 0.36676061687109374, 0.36676061687109374, 0.1093498751795694, 0.1093498751795694, 0.1093498751795694, 0.11800704322385025, 0.11800704322385025, 0.11800704322385025, 0.11014226708156238, 0.11014226708156238, 0.11014226708156238, 0.15004683892585147, 0.15004683892585147, 0.15004683892585147, 0.13033125670061696, 0.13033125670061696, 0.13033125670061696, 0.1176642388246808, 0.1176642388246808, 0.1176642388246808, 0.9817783524568978, 0.9817783524568978, 0.9817783524568978, 0.98749484089464, 0.98749484089464, 0.98749484089464, 0.9873373389794495, 0.9873373389794495, 0.9873373389794495, 0.3193023035569802, 0.3193023035569802, 0.3193023035569802, 0.4749253967860462, 0.4749253967860462, 0.4749253967860462, 0.33952636113240786, 0.33952636113240786, 0.33952636113240786, 0.23404906009051418, 0.23404906009051418, 0.23404906009051418, 0.1867241095540818, 0.1867241095540818, 0.1867241095540818, 0.22360410437078448, 0.22360410437078448, 0.22360410437078448, 0.38917833239145483, 0.38917833239145483, 0.38917833239145483, 0.17677793426833743, 0.17677793426833743, 0.17677793426833743, 0.1866522982738108, 0.1866522982738108, 0.1866522982738108, 0.1794288919186895, 0.1794288919186895, 0.1794288919186895, 0.1713064460088295, 0.1713064460088295, 0.1713064460088295, 0.1503593130339168, 0.1503593130339168, 0.1503593130339168, 0.16896150548229494, 0.16896150548229494, 0.16896150548229494, 0.1517238337156679, 0.1517238337156679, 0.1517238337156679, 0.07801075606316066, 0.07801075606316066, 0.07801075606316066, 0.1306258192704034, 0.1306258192704034, 0.1306258192704034, 0.043036206734873295, 0.043036206734873295, 0.043036206734873295, 0.1713736095711984, 0.1713736095711984, 0.1713736095711984, 0.03478423785365725, 0.03478423785365725, 0.03478423785365725, 0.19180632709776668, 0.19180632709776668, 0.19180632709776668, 0.0590246198988168, 0.0590246198988168, 0.0590246198988168, 0.20081792517374752, 0.20081792517374752, 0.20081792517374752, 0.033491506494330636, 0.033491506494330636, 0.033491506494330636, 0.05021043193870478, 0.05021043193870478, 0.05021043193870478, 0.5003727892349492, 0.5003727892349492, 0.5003727892349492, 0.4657333813986042, 0.4657333813986042, 0.4657333813986042, 0.46917186793370924, 0.46917186793370924, 0.46917186793370924, 0.14342207768301163, 0.14342207768301163, 0.14342207768301163, 0.12145871132800168, 0.12145871132800168, 0.12145871132800168, 0.1057475206242896, 0.1057475206242896, 0.1057475206242896, 0.35642057844886454, 0.35642057844886454, 0.35642057844886454, 0.21465072589842016, 0.21465072589842016, 0.21465072589842016, 0.2444189364073357, 0.2444189364073357, 0.2444189364073357, 0.286894944324837, 0.286894944324837, 0.286894944324837, 0.28791410732075406, 0.28791410732075406, 0.28791410732075406, 0.288665474020558, 0.288665474020558, 0.288665474020558, 0.22945855198095, 0.22945855198095, 0.22945855198095, 0.19982102082297581, 0.19982102082297581, 0.19982102082297581, 0.12103828021249086, 0.12103828021249086, 0.12103828021249086, 0.1874220403361898, 0.1874220403361898, 0.1874220403361898, 0.24617784072219195, 0.24617784072219195, 0.24617784072219195, 0.2752280251020164, 0.2752280251020164, 0.2752280251020164, 0.1880647070348399, 0.1880647070348399, 0.1880647070348399, 0.1738231871514354, 0.1738231871514354, 0.1738231871514354, 0.48468137841406633, 0.48468137841406633, 0.48468137841406633, 0.6910667162082013, 0.6910667162082013, 0.6910667162082013, 0.15879721238597644, 0.15879721238597644, 0.15879721238597644, 0.746649682486173, 0.746649682486173, 0.746649682486173, 0.1689669226231103, 0.1689669226231103, 0.1689669226231103, 0.21053313703713594, 0.21053313703713594, 0.21053313703713594, 0.16560864026155453, 0.16560864026155453, 0.16560864026155453, 0.19735138827661058, 0.19735138827661058, 0.19735138827661058, 0.17825965747894357, 0.17825965747894357, 0.17825965747894357, 0.20460318845723502, 0.20460318845723502, 0.20460318845723502, 0.08741913219627273, 0.08741913219627273, 0.08741913219627273, 0.10170633761050629, 0.10170633761050629, 0.10170633761050629, 0.08130864284712969, 0.08130864284712969, 0.08130864284712969]}, "mutation_prompt": null}
{"id": "f55a2c9e-751c-43b6-8e38-9653b0377df0", "solution": "import numpy as np\n\nclass PSO_SA_Optimizer_Enhanced_V3:\n    def __init__(self, budget, dim, pop_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.w_max = 0.9\n        self.w_min = 0.4\n        self.c1_init = 1.5\n        self.c2_init = 1.5\n        self.temp_init = 1.0\n\n    def __call__(self, func):\n        np.random.seed(42)\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.array([func(pos) for pos in positions])\n        global_best_position = personal_best_positions[np.argmin(personal_best_scores)]\n        global_best_score = np.min(personal_best_scores)\n        \n        eval_count = self.pop_size\n        temperature = self.temp_init\n        \n        while eval_count < self.budget:\n            w = self.w_max - ((self.w_max - self.w_min) * (eval_count / self.budget))\n            c1 = self.c1_init * (1 - eval_count / self.budget) + 0.5\n            c2 = self.c2_init * (eval_count / self.budget) + 0.5\n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                velocities[i] = (w * velocities[i]\n                                 + c1 * r1 * (personal_best_positions[i] - positions[i])\n                                 + c2 * r2 * (global_best_position - positions[i]))\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], self.lower_bound, self.upper_bound)\n\n                score = func(positions[i])\n                eval_count += 1\n                if eval_count >= self.budget:\n                    break\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = positions[i]\n\n                if np.random.rand() < np.exp(-(score - global_best_score) / temperature):\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n                    if score < global_best_score:\n                        global_best_score = score\n                        global_best_position = positions[i]\n\n            temperature *= 0.95  # Improved annealing factor for convergence\n\n        return global_best_position, global_best_score", "name": "PSO_SA_Optimizer_Enhanced_V3", "description": "Improved adaptive search strategy in PSO-SA with dynamically updating parameters and better temperature decay.", "configspace": "", "generation": 55, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "104cf98e-8138-4906-b55a-546d6de8214b", "metadata": {"aucs": [0.5051733381719483, 0.5051733381719483, 0.5051733381719483, 0.4864875265041222, 0.4864875265041222, 0.4864875265041222, 0.5980878545878154, 0.5980878545878154, 0.5980878545878154, 0.06730631129871123, 0.06730631129871123, 0.06730631129871123, 0.09301964064620494, 0.09301964064620494, 0.09301964064620494, 0.36676061687109374, 0.36676061687109374, 0.36676061687109374, 0.1093498751795694, 0.1093498751795694, 0.1093498751795694, 0.11800704322385025, 0.11800704322385025, 0.11800704322385025, 0.11014226708156238, 0.11014226708156238, 0.11014226708156238, 0.15004683892585147, 0.15004683892585147, 0.15004683892585147, 0.13033125670061696, 0.13033125670061696, 0.13033125670061696, 0.1176642388246808, 0.1176642388246808, 0.1176642388246808, 0.9817783524568978, 0.9817783524568978, 0.9817783524568978, 0.98749484089464, 0.98749484089464, 0.98749484089464, 0.9873373389794495, 0.9873373389794495, 0.9873373389794495, 0.3193023035569802, 0.3193023035569802, 0.3193023035569802, 0.4749253967860462, 0.4749253967860462, 0.4749253967860462, 0.33952636113240786, 0.33952636113240786, 0.33952636113240786, 0.23404906009051418, 0.23404906009051418, 0.23404906009051418, 0.1867241095540818, 0.1867241095540818, 0.1867241095540818, 0.22360410437078448, 0.22360410437078448, 0.22360410437078448, 0.38917833239145483, 0.38917833239145483, 0.38917833239145483, 0.17677793426833743, 0.17677793426833743, 0.17677793426833743, 0.1866522982738108, 0.1866522982738108, 0.1866522982738108, 0.1794288919186895, 0.1794288919186895, 0.1794288919186895, 0.1713064460088295, 0.1713064460088295, 0.1713064460088295, 0.1503593130339168, 0.1503593130339168, 0.1503593130339168, 0.16896150548229494, 0.16896150548229494, 0.16896150548229494, 0.1517238337156679, 0.1517238337156679, 0.1517238337156679, 0.07801075606316066, 0.07801075606316066, 0.07801075606316066, 0.1306258192704034, 0.1306258192704034, 0.1306258192704034, 0.043036206734873295, 0.043036206734873295, 0.043036206734873295, 0.1713736095711984, 0.1713736095711984, 0.1713736095711984, 0.03478423785365725, 0.03478423785365725, 0.03478423785365725, 0.19180632709776668, 0.19180632709776668, 0.19180632709776668, 0.0590246198988168, 0.0590246198988168, 0.0590246198988168, 0.20081792517374752, 0.20081792517374752, 0.20081792517374752, 0.033491506494330636, 0.033491506494330636, 0.033491506494330636, 0.05021043193870478, 0.05021043193870478, 0.05021043193870478, 0.5003727892349492, 0.5003727892349492, 0.5003727892349492, 0.4657333813986042, 0.4657333813986042, 0.4657333813986042, 0.46917186793370924, 0.46917186793370924, 0.46917186793370924, 0.14342207768301163, 0.14342207768301163, 0.14342207768301163, 0.12145871132800168, 0.12145871132800168, 0.12145871132800168, 0.1057475206242896, 0.1057475206242896, 0.1057475206242896, 0.35642057844886454, 0.35642057844886454, 0.35642057844886454, 0.21465072589842016, 0.21465072589842016, 0.21465072589842016, 0.2444189364073357, 0.2444189364073357, 0.2444189364073357, 0.286894944324837, 0.286894944324837, 0.286894944324837, 0.28791410732075406, 0.28791410732075406, 0.28791410732075406, 0.288665474020558, 0.288665474020558, 0.288665474020558, 0.22945855198095, 0.22945855198095, 0.22945855198095, 0.19982102082297581, 0.19982102082297581, 0.19982102082297581, 0.12103828021249086, 0.12103828021249086, 0.12103828021249086, 0.1874220403361898, 0.1874220403361898, 0.1874220403361898, 0.24617784072219195, 0.24617784072219195, 0.24617784072219195, 0.2752280251020164, 0.2752280251020164, 0.2752280251020164, 0.1880647070348399, 0.1880647070348399, 0.1880647070348399, 0.1738231871514354, 0.1738231871514354, 0.1738231871514354, 0.48468137841406633, 0.48468137841406633, 0.48468137841406633, 0.6910667162082013, 0.6910667162082013, 0.6910667162082013, 0.15879721238597644, 0.15879721238597644, 0.15879721238597644, 0.746649682486173, 0.746649682486173, 0.746649682486173, 0.1689669226231103, 0.1689669226231103, 0.1689669226231103, 0.21053313703713594, 0.21053313703713594, 0.21053313703713594, 0.16560864026155453, 0.16560864026155453, 0.16560864026155453, 0.19735138827661058, 0.19735138827661058, 0.19735138827661058, 0.17825965747894357, 0.17825965747894357, 0.17825965747894357, 0.20460318845723502, 0.20460318845723502, 0.20460318845723502, 0.08741913219627273, 0.08741913219627273, 0.08741913219627273, 0.10170633761050629, 0.10170633761050629, 0.10170633761050629, 0.08130864284712969, 0.08130864284712969, 0.08130864284712969]}, "mutation_prompt": null}
{"id": "09eb3b2d-c035-419d-8c20-669c3751a7ab", "solution": "import numpy as np\n\nclass PSO_SA_Optimizer_Enhanced_V4:\n    def __init__(self, budget, dim, pop_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.w_max = 0.95  # Adjusted max inertia weight\n        self.w_min = 0.3   # Adjusted min inertia weight\n        self.c1_init = 1.3  # Adjusted initial cognitive coefficient\n        self.c2_init = 1.7  # Adjusted initial social coefficient\n        self.temp_init = 1.0\n\n    def __call__(self, func):\n        np.random.seed(42)\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.array([func(pos) for pos in positions])\n        global_best_position = personal_best_positions[np.argmin(personal_best_scores)]\n        global_best_score = np.min(personal_best_scores)\n        \n        eval_count = self.pop_size\n        temperature = self.temp_init\n        \n        while eval_count < self.budget:\n            w = self.w_max - ((self.w_max - self.w_min) * (eval_count / self.budget))\n            c1 = self.c1_init * (1 - eval_count / self.budget) + 0.6\n            c2 = self.c2_init * (eval_count / self.budget) + 0.4\n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                velocities[i] = (w * velocities[i]\n                                 + c1 * r1 * (personal_best_positions[i] - positions[i])\n                                 + c2 * r2 * (global_best_position - positions[i]))\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], self.lower_bound, self.upper_bound)\n\n                score = func(positions[i])\n                eval_count += 1\n                if eval_count >= self.budget:\n                    break\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = positions[i]\n\n                if np.random.rand() < np.exp(-(score - global_best_score) / temperature):\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n                    if score < global_best_score:\n                        global_best_score = score\n                        global_best_position = positions[i]\n\n            temperature *= 0.93  # Further improved annealing factor for better convergence\n\n        return global_best_position, global_best_score", "name": "PSO_SA_Optimizer_Enhanced_V4", "description": "Enhanced PSO-SA optimizer with improved exploration-exploitation balance by tuning inertia and cognitive parameters.", "configspace": "", "generation": 75, "fitness": 0.2727422957048986, "feedback": "The algorithm PSO_SA_Optimizer_Enhanced_V4 got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.27 with standard deviation 0.23.", "error": "", "parent_id": "104cf98e-8138-4906-b55a-546d6de8214b", "metadata": {"aucs": [0.6811867043614335, 0.6811867043614335, 0.6811867043614335, 0.6243906749366523, 0.6243906749366523, 0.6243906749366523, 0.6882078112365331, 0.6882078112365331, 0.6882078112365331, 0.2977809225570107, 0.2977809225570107, 0.2977809225570107, 0.08397519358112804, 0.08397519358112804, 0.08397519358112804, 0.12207422953407154, 0.12207422953407154, 0.12207422953407154, 0.1041865575621117, 0.1041865575621117, 0.1041865575621117, 0.1118863264364156, 0.1118863264364156, 0.1118863264364156, 0.14061384335364013, 0.14061384335364013, 0.14061384335364013, 0.10602403947432337, 0.10602403947432337, 0.10602403947432337, 0.0617277164211022, 0.0617277164211022, 0.0617277164211022, 0.11657954321205866, 0.11657954321205866, 0.11657954321205866, 0.9810151787333815, 0.9810151787333815, 0.9810151787333815, 0.9852181543181459, 0.9852181543181459, 0.9852181543181459, 0.9855259882332429, 0.9855259882332429, 0.9855259882332429, 0.4067610010271012, 0.4067610010271012, 0.4067610010271012, 0.5217388185869976, 0.5217388185869976, 0.5217388185869976, 0.2530333786416835, 0.2530333786416835, 0.2530333786416835, 0.19791945459679028, 0.19791945459679028, 0.19791945459679028, 0.20897456179118157, 0.20897456179118157, 0.20897456179118157, 0.6475692838665641, 0.6475692838665641, 0.6475692838665641, 0.3069864664688826, 0.3069864664688826, 0.3069864664688826, 0.5383229441934845, 0.5383229441934845, 0.5383229441934845, 0.13085304475245707, 0.13085304475245707, 0.13085304475245707, 0.11747285448221989, 0.11747285448221989, 0.11747285448221989, 0.12233481582867356, 0.12233481582867356, 0.12233481582867356, 0.1432134931457587, 0.1432134931457587, 0.1432134931457587, 0.07188234529795823, 0.07188234529795823, 0.07188234529795823, 0.12784359329572037, 0.12784359329572037, 0.12784359329572037, 0.11560720877238273, 0.11560720877238273, 0.11560720877238273, 0.1615376539306782, 0.1615376539306782, 0.1615376539306782, 0.1669401029137949, 0.1669401029137949, 0.1669401029137949, 0.1846891583716026, 0.1846891583716026, 0.1846891583716026, 0.03676781127777706, 0.03676781127777706, 0.03676781127777706, 0.12387495151532835, 0.12387495151532835, 0.12387495151532835, 0.07951697279233594, 0.07951697279233594, 0.07951697279233594, 0.21575742460969827, 0.21575742460969827, 0.21575742460969827, 0.1604825828495946, 0.1604825828495946, 0.1604825828495946, 0.1228961547294799, 0.1228961547294799, 0.1228961547294799, 0.5353231205995233, 0.5353231205995233, 0.5353231205995233, 0.5716701015015953, 0.5716701015015953, 0.5716701015015953, 0.45862222115439166, 0.45862222115439166, 0.45862222115439166, 0.08223019616030347, 0.08223019616030347, 0.08223019616030347, 0.13120515091519247, 0.13120515091519247, 0.13120515091519247, 0.09327114395390057, 0.09327114395390057, 0.09327114395390057, 0.22088007997467074, 0.22088007997467074, 0.22088007997467074, 0.16715958356114313, 0.16715958356114313, 0.16715958356114313, 0.15215941304289404, 0.15215941304289404, 0.15215941304289404, 0.24929648289183903, 0.24929648289183903, 0.24929648289183903, 0.22627982590715323, 0.22627982590715323, 0.22627982590715323, 0.24171568266012877, 0.24171568266012877, 0.24171568266012877, 0.2513955282519986, 0.2513955282519986, 0.2513955282519986, 0.1995555502245977, 0.1995555502245977, 0.1995555502245977, 0.13375181223571375, 0.13375181223571375, 0.13375181223571375, 0.21880627350155812, 0.21880627350155812, 0.21880627350155812, 0.22799377474964444, 0.22799377474964444, 0.22799377474964444, 0.23279445432691648, 0.23279445432691648, 0.23279445432691648, 0.2157916176503144, 0.2157916176503144, 0.2157916176503144, 0.1861224714814148, 0.1861224714814148, 0.1861224714814148, 0.18045412440919717, 0.18045412440919717, 0.18045412440919717, 0.7497764208508715, 0.7497764208508715, 0.7497764208508715, 0.1573192251958695, 0.1573192251958695, 0.1573192251958695, 0.765015204295437, 0.765015204295437, 0.765015204295437, 0.5052173756145705, 0.5052173756145705, 0.5052173756145705, 0.20376631556404046, 0.20376631556404046, 0.20376631556404046, 0.15988621073079579, 0.15988621073079579, 0.15988621073079579, 0.1995896358072795, 0.1995896358072795, 0.1995896358072795, 0.1940549454317696, 0.1940549454317696, 0.1940549454317696, 0.2033643772369893, 0.2033643772369893, 0.2033643772369893, 0.08261915821851962, 0.08261915821851962, 0.08261915821851962, 0.09868698533415499, 0.09868698533415499, 0.09868698533415499, 0.08830186562891462, 0.08830186562891462, 0.08830186562891462]}, "mutation_prompt": null}
{"id": "bd29eb6c-a095-434f-899c-43fd3a24a234", "solution": "import numpy as np\n\nclass PSO_SA_Optimizer_Enhanced_V4:\n    def __init__(self, budget, dim, pop_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.w_max = 0.95  # Adjusted max inertia weight\n        self.w_min = 0.3   # Adjusted min inertia weight\n        self.c1_init = 1.3  # Adjusted initial cognitive coefficient\n        self.c2_init = 1.7  # Adjusted initial social coefficient\n        self.temp_init = 1.0\n\n    def __call__(self, func):\n        np.random.seed(42)\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.array([func(pos) for pos in positions])\n        global_best_position = personal_best_positions[np.argmin(personal_best_scores)]\n        global_best_score = np.min(personal_best_scores)\n        \n        eval_count = self.pop_size\n        temperature = self.temp_init\n        \n        while eval_count < self.budget:\n            w = self.w_max - ((self.w_max - self.w_min) * (eval_count / self.budget))\n            c1 = self.c1_init * (1 - eval_count / self.budget) + 0.6\n            c2 = self.c2_init * (eval_count / self.budget) + 0.4\n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                velocities[i] = (w * velocities[i]\n                                 + c1 * r1 * (personal_best_positions[i] - positions[i])\n                                 + c2 * r2 * (global_best_position - positions[i]))\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], self.lower_bound, self.upper_bound)\n\n                score = func(positions[i])\n                eval_count += 1\n                if eval_count >= self.budget:\n                    break\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = positions[i]\n\n                if np.random.rand() < np.exp(-(score - global_best_score) / temperature):\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n                    if score < global_best_score:\n                        global_best_score = score\n                        global_best_position = positions[i]\n\n            temperature *= 0.93  # Further improved annealing factor for better convergence\n\n        return global_best_position, global_best_score", "name": "PSO_SA_Optimizer_Enhanced_V4", "description": "Enhanced PSO-SA optimizer with improved exploration-exploitation balance by tuning inertia and cognitive parameters.", "configspace": "", "generation": 76, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "09eb3b2d-c035-419d-8c20-669c3751a7ab", "metadata": {"aucs": [0.6811867043614335, 0.6811867043614335, 0.6811867043614335, 0.6243906749366523, 0.6243906749366523, 0.6243906749366523, 0.6882078112365331, 0.6882078112365331, 0.6882078112365331, 0.2977809225570107, 0.2977809225570107, 0.2977809225570107, 0.08397519358112804, 0.08397519358112804, 0.08397519358112804, 0.12207422953407154, 0.12207422953407154, 0.12207422953407154, 0.1041865575621117, 0.1041865575621117, 0.1041865575621117, 0.1118863264364156, 0.1118863264364156, 0.1118863264364156, 0.14061384335364013, 0.14061384335364013, 0.14061384335364013, 0.10602403947432337, 0.10602403947432337, 0.10602403947432337, 0.0617277164211022, 0.0617277164211022, 0.0617277164211022, 0.11657954321205866, 0.11657954321205866, 0.11657954321205866, 0.9810151787333815, 0.9810151787333815, 0.9810151787333815, 0.9852181543181459, 0.9852181543181459, 0.9852181543181459, 0.9855259882332429, 0.9855259882332429, 0.9855259882332429, 0.4067610010271012, 0.4067610010271012, 0.4067610010271012, 0.5217388185869976, 0.5217388185869976, 0.5217388185869976, 0.2530333786416835, 0.2530333786416835, 0.2530333786416835, 0.19791945459679028, 0.19791945459679028, 0.19791945459679028, 0.20897456179118157, 0.20897456179118157, 0.20897456179118157, 0.6475692838665641, 0.6475692838665641, 0.6475692838665641, 0.3069864664688826, 0.3069864664688826, 0.3069864664688826, 0.5383229441934845, 0.5383229441934845, 0.5383229441934845, 0.13085304475245707, 0.13085304475245707, 0.13085304475245707, 0.11747285448221989, 0.11747285448221989, 0.11747285448221989, 0.12233481582867356, 0.12233481582867356, 0.12233481582867356, 0.1432134931457587, 0.1432134931457587, 0.1432134931457587, 0.07188234529795823, 0.07188234529795823, 0.07188234529795823, 0.12784359329572037, 0.12784359329572037, 0.12784359329572037, 0.11560720877238273, 0.11560720877238273, 0.11560720877238273, 0.1615376539306782, 0.1615376539306782, 0.1615376539306782, 0.1669401029137949, 0.1669401029137949, 0.1669401029137949, 0.1846891583716026, 0.1846891583716026, 0.1846891583716026, 0.03676781127777706, 0.03676781127777706, 0.03676781127777706, 0.12387495151532835, 0.12387495151532835, 0.12387495151532835, 0.07951697279233594, 0.07951697279233594, 0.07951697279233594, 0.21575742460969827, 0.21575742460969827, 0.21575742460969827, 0.1604825828495946, 0.1604825828495946, 0.1604825828495946, 0.1228961547294799, 0.1228961547294799, 0.1228961547294799, 0.5353231205995233, 0.5353231205995233, 0.5353231205995233, 0.5716701015015953, 0.5716701015015953, 0.5716701015015953, 0.45862222115439166, 0.45862222115439166, 0.45862222115439166, 0.08223019616030347, 0.08223019616030347, 0.08223019616030347, 0.13120515091519247, 0.13120515091519247, 0.13120515091519247, 0.09327114395390057, 0.09327114395390057, 0.09327114395390057, 0.22088007997467074, 0.22088007997467074, 0.22088007997467074, 0.16715958356114313, 0.16715958356114313, 0.16715958356114313, 0.15215941304289404, 0.15215941304289404, 0.15215941304289404, 0.24929648289183903, 0.24929648289183903, 0.24929648289183903, 0.22627982590715323, 0.22627982590715323, 0.22627982590715323, 0.24171568266012877, 0.24171568266012877, 0.24171568266012877, 0.2513955282519986, 0.2513955282519986, 0.2513955282519986, 0.1995555502245977, 0.1995555502245977, 0.1995555502245977, 0.13375181223571375, 0.13375181223571375, 0.13375181223571375, 0.21880627350155812, 0.21880627350155812, 0.21880627350155812, 0.22799377474964444, 0.22799377474964444, 0.22799377474964444, 0.23279445432691648, 0.23279445432691648, 0.23279445432691648, 0.2157916176503144, 0.2157916176503144, 0.2157916176503144, 0.1861224714814148, 0.1861224714814148, 0.1861224714814148, 0.18045412440919717, 0.18045412440919717, 0.18045412440919717, 0.7497764208508715, 0.7497764208508715, 0.7497764208508715, 0.1573192251958695, 0.1573192251958695, 0.1573192251958695, 0.765015204295437, 0.765015204295437, 0.765015204295437, 0.5052173756145705, 0.5052173756145705, 0.5052173756145705, 0.20376631556404046, 0.20376631556404046, 0.20376631556404046, 0.15988621073079579, 0.15988621073079579, 0.15988621073079579, 0.1995896358072795, 0.1995896358072795, 0.1995896358072795, 0.1940549454317696, 0.1940549454317696, 0.1940549454317696, 0.2033643772369893, 0.2033643772369893, 0.2033643772369893, 0.08261915821851962, 0.08261915821851962, 0.08261915821851962, 0.09868698533415499, 0.09868698533415499, 0.09868698533415499, 0.08830186562891462, 0.08830186562891462, 0.08830186562891462]}, "mutation_prompt": null}
{"id": "7f5bf2a1-3354-4011-99c6-6eba544a4af8", "solution": "import numpy as np\n\nclass PSO_SA_Optimizer_Enhanced_V5:\n    def __init__(self, budget, dim, pop_size=30):  # Keeping consistent parameter names\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.w_max = 0.9  # Slightly reduced max inertia weight\n        self.w_min = 0.4  # Slightly increased min inertia weight\n        self.c1_init = 1.5  # Adjusted cognitive coefficient for balance\n        self.c2_init = 1.5  # Adjusted social coefficient for balance\n        self.temp_init = 1.2  # Increased initial temperature for exploration\n\n    def __call__(self, func):\n        np.random.seed(42)\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        velocities = np.random.uniform(-0.5, 0.5, (self.pop_size, self.dim))  # Reduced velocity range\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.array([func(pos) for pos in positions])\n        global_best_position = personal_best_positions[np.argmin(personal_best_scores)]\n        global_best_score = np.min(personal_best_scores)\n        \n        eval_count = self.pop_size\n        temperature = self.temp_init\n        \n        while eval_count < self.budget:\n            w = self.w_max - ((self.w_max - self.w_min) * (eval_count / self.budget))\n            c1 = self.c1_init * (1 - eval_count / self.budget) + 0.5\n            c2 = self.c2_init * (eval_count / self.budget) + 0.5\n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                velocities[i] = (w * velocities[i]\n                                 + c1 * r1 * (personal_best_positions[i] - positions[i])\n                                 + c2 * r2 * (global_best_position - positions[i]))\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], self.lower_bound, self.upper_bound)\n\n                score = func(positions[i])\n                eval_count += 1\n                if eval_count >= self.budget:\n                    break\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = positions[i]\n\n                if np.random.rand() < np.exp(-(score - global_best_score) / temperature):\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n                    if score < global_best_score:\n                        global_best_score = score\n                        global_best_position = positions[i]\n\n            temperature *= 0.9  # Adjusted annealing factor for dynamic temperature decay\n\n        return global_best_position, global_best_score", "name": "PSO_SA_Optimizer_Enhanced_V5", "description": "Improved PSO-SA optimizer with fine-tuned inertia and dynamic temperature decay for enhanced convergence.", "configspace": "", "generation": 77, "fitness": 0.2885013865691005, "feedback": "The algorithm PSO_SA_Optimizer_Enhanced_V5 got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.29 with standard deviation 0.25.", "error": "", "parent_id": "09eb3b2d-c035-419d-8c20-669c3751a7ab", "metadata": {"aucs": [0.7211156794072126, 0.7211156794072126, 0.7211156794072126, 0.7563755350493901, 0.7563755350493901, 0.7563755350493901, 0.7595921706850015, 0.7595921706850015, 0.7595921706850015, 0.11061293686701479, 0.11061293686701479, 0.11061293686701479, 0.03950144751777984, 0.03950144751777984, 0.03950144751777984, 0.07386627138127899, 0.07386627138127899, 0.07386627138127899, 0.1230245010715535, 0.1230245010715535, 0.1230245010715535, 0.11143854156806465, 0.11143854156806465, 0.11143854156806465, 0.12824919213944408, 0.12824919213944408, 0.12824919213944408, 0.05960445409902826, 0.05960445409902826, 0.05960445409902826, 0.0952885777658814, 0.0952885777658814, 0.0952885777658814, 0.07610345212832892, 0.07610345212832892, 0.07610345212832892, 0.9807070324373555, 0.9807070324373555, 0.9807070324373555, 0.9848888086857225, 0.9848888086857225, 0.9848888086857225, 0.9849382792388476, 0.9849382792388476, 0.9849382792388476, 0.2396295054942995, 0.2396295054942995, 0.2396295054942995, 0.6202859572217212, 0.6202859572217212, 0.6202859572217212, 0.2458987065465471, 0.2458987065465471, 0.2458987065465471, 0.23583218626822067, 0.23583218626822067, 0.23583218626822067, 0.20536747678983758, 0.20536747678983758, 0.20536747678983758, 0.36712052294725295, 0.36712052294725295, 0.36712052294725295, 0.29295240342080164, 0.29295240342080164, 0.29295240342080164, 0.12314755124571042, 0.12314755124571042, 0.12314755124571042, 0.1756417942504055, 0.1756417942504055, 0.1756417942504055, 0.18482735598578393, 0.18482735598578393, 0.18482735598578393, 0.1714235374306149, 0.1714235374306149, 0.1714235374306149, 0.3847536699799199, 0.3847536699799199, 0.3847536699799199, 0.06612589265628976, 0.06612589265628976, 0.06612589265628976, 0.04989923400205665, 0.04989923400205665, 0.04989923400205665, 0.0673985348449887, 0.0673985348449887, 0.0673985348449887, 0.07323997822656725, 0.07323997822656725, 0.07323997822656725, 0.44251666320269023, 0.44251666320269023, 0.44251666320269023, 0.28290999346864887, 0.28290999346864887, 0.28290999346864887, 0.0392010148219788, 0.0392010148219788, 0.0392010148219788, 0.1390103469241072, 0.1390103469241072, 0.1390103469241072, 0.20890390839305173, 0.20890390839305173, 0.20890390839305173, 0.29356367635189984, 0.29356367635189984, 0.29356367635189984, 0.26102412123579366, 0.26102412123579366, 0.26102412123579366, 0.14408992485056182, 0.14408992485056182, 0.14408992485056182, 0.5288834377685849, 0.5288834377685849, 0.5288834377685849, 0.6419018731843379, 0.6419018731843379, 0.6419018731843379, 0.6734412128143883, 0.6734412128143883, 0.6734412128143883, 0.1123800613159387, 0.1123800613159387, 0.1123800613159387, 0.07014082795353771, 0.07014082795353771, 0.07014082795353771, 0.10676719220392006, 0.10676719220392006, 0.10676719220392006, 0.2647423309373337, 0.2647423309373337, 0.2647423309373337, 0.21350118630828507, 0.21350118630828507, 0.21350118630828507, 0.2501021829011235, 0.2501021829011235, 0.2501021829011235, 0.2945291608040408, 0.2945291608040408, 0.2945291608040408, 0.22227556880215826, 0.22227556880215826, 0.22227556880215826, 0.2271726023680346, 0.2271726023680346, 0.2271726023680346, 0.2322947151859941, 0.2322947151859941, 0.2322947151859941, 0.14221891202196568, 0.14221891202196568, 0.14221891202196568, 0.15118934810070206, 0.15118934810070206, 0.15118934810070206, 0.23542410825161864, 0.23542410825161864, 0.23542410825161864, 0.23567339113037056, 0.23567339113037056, 0.23567339113037056, 0.23340413015551165, 0.23340413015551165, 0.23340413015551165, 0.18204939999208058, 0.18204939999208058, 0.18204939999208058, 0.20398815683917404, 0.20398815683917404, 0.20398815683917404, 0.6082563784972399, 0.6082563784972399, 0.6082563784972399, 0.8135504282908683, 0.8135504282908683, 0.8135504282908683, 0.16630737709990606, 0.16630737709990606, 0.16630737709990606, 0.7462541662513282, 0.7462541662513282, 0.7462541662513282, 0.1685297243351126, 0.1685297243351126, 0.1685297243351126, 0.2113793043568718, 0.2113793043568718, 0.2113793043568718, 0.6297967396268354, 0.6297967396268354, 0.6297967396268354, 0.2156380924414909, 0.2156380924414909, 0.2156380924414909, 0.21318950247862356, 0.21318950247862356, 0.21318950247862356, 0.21571981481488023, 0.21571981481488023, 0.21571981481488023, 0.07421878500495782, 0.07421878500495782, 0.07421878500495782, 0.0866509222659877, 0.0866509222659877, 0.0866509222659877, 0.08045796187038323, 0.08045796187038323, 0.08045796187038323]}, "mutation_prompt": null}
{"id": "dd70f39c-80df-40c2-a3ba-353c3f16bac4", "solution": "import numpy as np\n\nclass PSO_SA_Optimizer_Enhanced_V5:\n    def __init__(self, budget, dim, pop_size=30):  # Keeping consistent parameter names\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.w_max = 0.9  # Slightly reduced max inertia weight\n        self.w_min = 0.4  # Slightly increased min inertia weight\n        self.c1_init = 1.5  # Adjusted cognitive coefficient for balance\n        self.c2_init = 1.5  # Adjusted social coefficient for balance\n        self.temp_init = 1.2  # Increased initial temperature for exploration\n\n    def __call__(self, func):\n        np.random.seed(42)\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        velocities = np.random.uniform(-0.5, 0.5, (self.pop_size, self.dim))  # Reduced velocity range\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.array([func(pos) for pos in positions])\n        global_best_position = personal_best_positions[np.argmin(personal_best_scores)]\n        global_best_score = np.min(personal_best_scores)\n        \n        eval_count = self.pop_size\n        temperature = self.temp_init\n        \n        while eval_count < self.budget:\n            w = self.w_max - ((self.w_max - self.w_min) * (eval_count / self.budget))\n            c1 = self.c1_init * (1 - eval_count / self.budget) + 0.5\n            c2 = self.c2_init * (eval_count / self.budget) + 0.5\n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                velocities[i] = (w * velocities[i]\n                                 + c1 * r1 * (personal_best_positions[i] - positions[i])\n                                 + c2 * r2 * (global_best_position - positions[i]))\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], self.lower_bound, self.upper_bound)\n\n                score = func(positions[i])\n                eval_count += 1\n                if eval_count >= self.budget:\n                    break\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = positions[i]\n\n                if np.random.rand() < np.exp(-(score - global_best_score) / temperature):\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n                    if score < global_best_score:\n                        global_best_score = score\n                        global_best_position = positions[i]\n\n            temperature *= 0.9  # Adjusted annealing factor for dynamic temperature decay\n\n        return global_best_position, global_best_score", "name": "PSO_SA_Optimizer_Enhanced_V5", "description": "Improved PSO-SA optimizer with fine-tuned inertia and dynamic temperature decay for enhanced convergence.", "configspace": "", "generation": 78, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "7f5bf2a1-3354-4011-99c6-6eba544a4af8", "metadata": {"aucs": [0.7211156794072126, 0.7211156794072126, 0.7211156794072126, 0.7563755350493901, 0.7563755350493901, 0.7563755350493901, 0.7595921706850015, 0.7595921706850015, 0.7595921706850015, 0.11061293686701479, 0.11061293686701479, 0.11061293686701479, 0.03950144751777984, 0.03950144751777984, 0.03950144751777984, 0.07386627138127899, 0.07386627138127899, 0.07386627138127899, 0.1230245010715535, 0.1230245010715535, 0.1230245010715535, 0.11143854156806465, 0.11143854156806465, 0.11143854156806465, 0.12824919213944408, 0.12824919213944408, 0.12824919213944408, 0.05960445409902826, 0.05960445409902826, 0.05960445409902826, 0.0952885777658814, 0.0952885777658814, 0.0952885777658814, 0.07610345212832892, 0.07610345212832892, 0.07610345212832892, 0.9807070324373555, 0.9807070324373555, 0.9807070324373555, 0.9848888086857225, 0.9848888086857225, 0.9848888086857225, 0.9849382792388476, 0.9849382792388476, 0.9849382792388476, 0.2396295054942995, 0.2396295054942995, 0.2396295054942995, 0.6202859572217212, 0.6202859572217212, 0.6202859572217212, 0.2458987065465471, 0.2458987065465471, 0.2458987065465471, 0.23583218626822067, 0.23583218626822067, 0.23583218626822067, 0.20536747678983758, 0.20536747678983758, 0.20536747678983758, 0.36712052294725295, 0.36712052294725295, 0.36712052294725295, 0.29295240342080164, 0.29295240342080164, 0.29295240342080164, 0.12314755124571042, 0.12314755124571042, 0.12314755124571042, 0.1756417942504055, 0.1756417942504055, 0.1756417942504055, 0.18482735598578393, 0.18482735598578393, 0.18482735598578393, 0.1714235374306149, 0.1714235374306149, 0.1714235374306149, 0.3847536699799199, 0.3847536699799199, 0.3847536699799199, 0.06612589265628976, 0.06612589265628976, 0.06612589265628976, 0.04989923400205665, 0.04989923400205665, 0.04989923400205665, 0.0673985348449887, 0.0673985348449887, 0.0673985348449887, 0.07323997822656725, 0.07323997822656725, 0.07323997822656725, 0.44251666320269023, 0.44251666320269023, 0.44251666320269023, 0.28290999346864887, 0.28290999346864887, 0.28290999346864887, 0.0392010148219788, 0.0392010148219788, 0.0392010148219788, 0.1390103469241072, 0.1390103469241072, 0.1390103469241072, 0.20890390839305173, 0.20890390839305173, 0.20890390839305173, 0.29356367635189984, 0.29356367635189984, 0.29356367635189984, 0.26102412123579366, 0.26102412123579366, 0.26102412123579366, 0.14408992485056182, 0.14408992485056182, 0.14408992485056182, 0.5288834377685849, 0.5288834377685849, 0.5288834377685849, 0.6419018731843379, 0.6419018731843379, 0.6419018731843379, 0.6734412128143883, 0.6734412128143883, 0.6734412128143883, 0.1123800613159387, 0.1123800613159387, 0.1123800613159387, 0.07014082795353771, 0.07014082795353771, 0.07014082795353771, 0.10676719220392006, 0.10676719220392006, 0.10676719220392006, 0.2647423309373337, 0.2647423309373337, 0.2647423309373337, 0.21350118630828507, 0.21350118630828507, 0.21350118630828507, 0.2501021829011235, 0.2501021829011235, 0.2501021829011235, 0.2945291608040408, 0.2945291608040408, 0.2945291608040408, 0.22227556880215826, 0.22227556880215826, 0.22227556880215826, 0.2271726023680346, 0.2271726023680346, 0.2271726023680346, 0.2322947151859941, 0.2322947151859941, 0.2322947151859941, 0.14221891202196568, 0.14221891202196568, 0.14221891202196568, 0.15118934810070206, 0.15118934810070206, 0.15118934810070206, 0.23542410825161864, 0.23542410825161864, 0.23542410825161864, 0.23567339113037056, 0.23567339113037056, 0.23567339113037056, 0.23340413015551165, 0.23340413015551165, 0.23340413015551165, 0.18204939999208058, 0.18204939999208058, 0.18204939999208058, 0.20398815683917404, 0.20398815683917404, 0.20398815683917404, 0.6082563784972399, 0.6082563784972399, 0.6082563784972399, 0.8135504282908683, 0.8135504282908683, 0.8135504282908683, 0.16630737709990606, 0.16630737709990606, 0.16630737709990606, 0.7462541662513282, 0.7462541662513282, 0.7462541662513282, 0.1685297243351126, 0.1685297243351126, 0.1685297243351126, 0.2113793043568718, 0.2113793043568718, 0.2113793043568718, 0.6297967396268354, 0.6297967396268354, 0.6297967396268354, 0.2156380924414909, 0.2156380924414909, 0.2156380924414909, 0.21318950247862356, 0.21318950247862356, 0.21318950247862356, 0.21571981481488023, 0.21571981481488023, 0.21571981481488023, 0.07421878500495782, 0.07421878500495782, 0.07421878500495782, 0.0866509222659877, 0.0866509222659877, 0.0866509222659877, 0.08045796187038323, 0.08045796187038323, 0.08045796187038323]}, "mutation_prompt": null}
{"id": "376d31ce-e21f-4741-9568-0d0ba3ddc238", "solution": "import numpy as np\n\nclass PSO_SA_Optimizer_Enhanced_V6:\n    def __init__(self, budget, dim, pop_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.w_max = 0.85  # Adjusted max inertia weight for quicker adaptation\n        self.w_min = 0.35  # Adjusted min inertia weight for quicker adaptation\n        self.c1_init = 1.4  # Tuning cognitive coefficient for exploration\n        self.c2_init = 1.6  # Tuning social coefficient for exploitation\n        self.temp_init = 1.3  # Higher initial temperature for exploration\n\n    def __call__(self, func):\n        np.random.seed(42)\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        velocities = np.random.uniform(-0.4, 0.4, (self.pop_size, self.dim))  # Adjusted velocity range\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.array([func(pos) for pos in positions])\n        global_best_position = personal_best_positions[np.argmin(personal_best_scores)]\n        global_best_score = np.min(personal_best_scores)\n        \n        eval_count = self.pop_size\n        temperature = self.temp_init\n        \n        while eval_count < self.budget:\n            w = self.w_max - ((self.w_max - self.w_min) * (eval_count / self.budget))\n            c1 = self.c1_init * (1 - eval_count / self.budget) + 0.55\n            c2 = self.c2_init * (eval_count / self.budget) + 0.45\n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                velocities[i] = (w * velocities[i]\n                                 + c1 * r1 * (personal_best_positions[i] - positions[i])\n                                 + c2 * r2 * (global_best_position - positions[i]))\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], self.lower_bound, self.upper_bound)\n\n                score = func(positions[i])\n                eval_count += 1\n                if eval_count >= self.budget:\n                    break\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = positions[i]\n\n                if np.random.rand() < np.exp(-(score - global_best_score) / temperature):\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n                    if score < global_best_score:\n                        global_best_score = score\n                        global_best_position = positions[i]\n\n            temperature *= 0.88  # Fine-tuned annealing factor for adaptive temperature decay\n\n        return global_best_position, global_best_score", "name": "PSO_SA_Optimizer_Enhanced_V6", "description": "Enhanced adaptive PSO-SA optimizer with refined dynamic parameter adjustment for improved convergence efficiency.", "configspace": "", "generation": 79, "fitness": 0.29269732413276794, "feedback": "The algorithm PSO_SA_Optimizer_Enhanced_V6 got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.29 with standard deviation 0.25.", "error": "", "parent_id": "7f5bf2a1-3354-4011-99c6-6eba544a4af8", "metadata": {"aucs": [0.8167821930790096, 0.8167821930790096, 0.8167821930790096, 0.8143224007267236, 0.8143224007267236, 0.8143224007267236, 0.8355677960052937, 0.8355677960052937, 0.8355677960052937, 0.11887594998820494, 0.11887594998820494, 0.11887594998820494, 0.1616476462705395, 0.1616476462705395, 0.1616476462705395, 0.2293624404075113, 0.2293624404075113, 0.2293624404075113, 0.13100387068556485, 0.13100387068556485, 0.13100387068556485, 0.3347530163546938, 0.3347530163546938, 0.3347530163546938, 0.14572036904617724, 0.14572036904617724, 0.14572036904617724, 0.07061597256075891, 0.07061597256075891, 0.07061597256075891, 0.05339637868818048, 0.05339637868818048, 0.05339637868818048, 0.07087147130438332, 0.07087147130438332, 0.07087147130438332, 0.9754202403783134, 0.9754202403783134, 0.9754202403783134, 0.9821790305337643, 0.9821790305337643, 0.9821790305337643, 0.9834477199641951, 0.9834477199641951, 0.9834477199641951, 0.31600783713709557, 0.31600783713709557, 0.31600783713709557, 0.3834833995820046, 0.3834833995820046, 0.3834833995820046, 0.3677275865160786, 0.3677275865160786, 0.3677275865160786, 0.16965108282573615, 0.16965108282573615, 0.16965108282573615, 0.16241563431096961, 0.16241563431096961, 0.16241563431096961, 0.23012681019363213, 0.23012681019363213, 0.23012681019363213, 0.40766385472090816, 0.40766385472090816, 0.40766385472090816, 0.2423851168202279, 0.2423851168202279, 0.2423851168202279, 0.1318537560944678, 0.1318537560944678, 0.1318537560944678, 0.2379604796197219, 0.2379604796197219, 0.2379604796197219, 0.13192723242726057, 0.13192723242726057, 0.13192723242726057, 0.33072125245405637, 0.33072125245405637, 0.33072125245405637, 0.0648680695517001, 0.0648680695517001, 0.0648680695517001, 0.044110887371523466, 0.044110887371523466, 0.044110887371523466, 0.10663017118096318, 0.10663017118096318, 0.10663017118096318, 0.479961800985499, 0.479961800985499, 0.479961800985499, 0.30076156598948234, 0.30076156598948234, 0.30076156598948234, 0.23364652554277465, 0.23364652554277465, 0.23364652554277465, 0.04002014768183548, 0.04002014768183548, 0.04002014768183548, 0.26298916463890987, 0.26298916463890987, 0.26298916463890987, 0.15487679201894278, 0.15487679201894278, 0.15487679201894278, 0.1571352708483379, 0.1571352708483379, 0.1571352708483379, 0.03834925935338651, 0.03834925935338651, 0.03834925935338651, 0.0807751849621221, 0.0807751849621221, 0.0807751849621221, 0.520138212206239, 0.520138212206239, 0.520138212206239, 0.5558477998359908, 0.5558477998359908, 0.5558477998359908, 0.5682895440314135, 0.5682895440314135, 0.5682895440314135, 0.1053514676497238, 0.1053514676497238, 0.1053514676497238, 0.05363281581259527, 0.05363281581259527, 0.05363281581259527, 0.1585284672360322, 0.1585284672360322, 0.1585284672360322, 0.29150880570710347, 0.29150880570710347, 0.29150880570710347, 0.31486309274770785, 0.31486309274770785, 0.31486309274770785, 0.1917236496047251, 0.1917236496047251, 0.1917236496047251, 0.34033492540094734, 0.34033492540094734, 0.34033492540094734, 0.21107560476869225, 0.21107560476869225, 0.21107560476869225, 0.16775177892218307, 0.16775177892218307, 0.16775177892218307, 0.19649731060457742, 0.19649731060457742, 0.19649731060457742, 0.1723896145895618, 0.1723896145895618, 0.1723896145895618, 0.13312470287960387, 0.13312470287960387, 0.13312470287960387, 0.2127499527515604, 0.2127499527515604, 0.2127499527515604, 0.23680717677758178, 0.23680717677758178, 0.23680717677758178, 0.24236410615804438, 0.24236410615804438, 0.24236410615804438, 0.22108903238456545, 0.22108903238456545, 0.22108903238456545, 0.1802351449797116, 0.1802351449797116, 0.1802351449797116, 0.1888248956044597, 0.1888248956044597, 0.1888248956044597, 0.8483747271699531, 0.8483747271699531, 0.8483747271699531, 0.16608938213636926, 0.16608938213636926, 0.16608938213636926, 0.8084046491962622, 0.8084046491962622, 0.8084046491962622, 0.8417907063586292, 0.8417907063586292, 0.8417907063586292, 0.21035286446160217, 0.21035286446160217, 0.21035286446160217, 0.15599667862221145, 0.15599667862221145, 0.15599667862221145, 0.20330246188462542, 0.20330246188462542, 0.20330246188462542, 0.1901711347463937, 0.1901711347463937, 0.1901711347463937, 0.24705483087449043, 0.24705483087449043, 0.24705483087449043, 0.1052184149452634, 0.1052184149452634, 0.1052184149452634, 0.13111099249073033, 0.13111099249073033, 0.13111099249073033, 0.10319701519879199, 0.10319701519879199, 0.10319701519879199]}, "mutation_prompt": null}
{"id": "384eb504-0ead-4f26-a0c3-3d9526a131bf", "solution": "import numpy as np\n\nclass PSO_SA_Optimizer_Enhanced_V6:\n    def __init__(self, budget, dim, pop_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.w_max = 0.85  # Adjusted max inertia weight for quicker adaptation\n        self.w_min = 0.35  # Adjusted min inertia weight for quicker adaptation\n        self.c1_init = 1.4  # Tuning cognitive coefficient for exploration\n        self.c2_init = 1.6  # Tuning social coefficient for exploitation\n        self.temp_init = 1.3  # Higher initial temperature for exploration\n\n    def __call__(self, func):\n        np.random.seed(42)\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        velocities = np.random.uniform(-0.4, 0.4, (self.pop_size, self.dim))  # Adjusted velocity range\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.array([func(pos) for pos in positions])\n        global_best_position = personal_best_positions[np.argmin(personal_best_scores)]\n        global_best_score = np.min(personal_best_scores)\n        \n        eval_count = self.pop_size\n        temperature = self.temp_init\n        \n        while eval_count < self.budget:\n            w = self.w_max - ((self.w_max - self.w_min) * (eval_count / self.budget))\n            c1 = self.c1_init * (1 - eval_count / self.budget) + 0.55\n            c2 = self.c2_init * (eval_count / self.budget) + 0.45\n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                velocities[i] = (w * velocities[i]\n                                 + c1 * r1 * (personal_best_positions[i] - positions[i])\n                                 + c2 * r2 * (global_best_position - positions[i]))\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], self.lower_bound, self.upper_bound)\n\n                score = func(positions[i])\n                eval_count += 1\n                if eval_count >= self.budget:\n                    break\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = positions[i]\n\n                if np.random.rand() < np.exp(-(score - global_best_score) / temperature):\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n                    if score < global_best_score:\n                        global_best_score = score\n                        global_best_position = positions[i]\n\n            temperature *= 0.88  # Fine-tuned annealing factor for adaptive temperature decay\n\n        return global_best_position, global_best_score", "name": "PSO_SA_Optimizer_Enhanced_V6", "description": "Enhanced adaptive PSO-SA optimizer with refined dynamic parameter adjustment for improved convergence efficiency.", "configspace": "", "generation": 80, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "376d31ce-e21f-4741-9568-0d0ba3ddc238", "metadata": {"aucs": [0.8167821930790096, 0.8167821930790096, 0.8167821930790096, 0.8143224007267236, 0.8143224007267236, 0.8143224007267236, 0.8355677960052937, 0.8355677960052937, 0.8355677960052937, 0.11887594998820494, 0.11887594998820494, 0.11887594998820494, 0.1616476462705395, 0.1616476462705395, 0.1616476462705395, 0.2293624404075113, 0.2293624404075113, 0.2293624404075113, 0.13100387068556485, 0.13100387068556485, 0.13100387068556485, 0.3347530163546938, 0.3347530163546938, 0.3347530163546938, 0.14572036904617724, 0.14572036904617724, 0.14572036904617724, 0.07061597256075891, 0.07061597256075891, 0.07061597256075891, 0.05339637868818048, 0.05339637868818048, 0.05339637868818048, 0.07087147130438332, 0.07087147130438332, 0.07087147130438332, 0.9754202403783134, 0.9754202403783134, 0.9754202403783134, 0.9821790305337643, 0.9821790305337643, 0.9821790305337643, 0.9834477199641951, 0.9834477199641951, 0.9834477199641951, 0.31600783713709557, 0.31600783713709557, 0.31600783713709557, 0.3834833995820046, 0.3834833995820046, 0.3834833995820046, 0.3677275865160786, 0.3677275865160786, 0.3677275865160786, 0.16965108282573615, 0.16965108282573615, 0.16965108282573615, 0.16241563431096961, 0.16241563431096961, 0.16241563431096961, 0.23012681019363213, 0.23012681019363213, 0.23012681019363213, 0.40766385472090816, 0.40766385472090816, 0.40766385472090816, 0.2423851168202279, 0.2423851168202279, 0.2423851168202279, 0.1318537560944678, 0.1318537560944678, 0.1318537560944678, 0.2379604796197219, 0.2379604796197219, 0.2379604796197219, 0.13192723242726057, 0.13192723242726057, 0.13192723242726057, 0.33072125245405637, 0.33072125245405637, 0.33072125245405637, 0.0648680695517001, 0.0648680695517001, 0.0648680695517001, 0.044110887371523466, 0.044110887371523466, 0.044110887371523466, 0.10663017118096318, 0.10663017118096318, 0.10663017118096318, 0.479961800985499, 0.479961800985499, 0.479961800985499, 0.30076156598948234, 0.30076156598948234, 0.30076156598948234, 0.23364652554277465, 0.23364652554277465, 0.23364652554277465, 0.04002014768183548, 0.04002014768183548, 0.04002014768183548, 0.26298916463890987, 0.26298916463890987, 0.26298916463890987, 0.15487679201894278, 0.15487679201894278, 0.15487679201894278, 0.1571352708483379, 0.1571352708483379, 0.1571352708483379, 0.03834925935338651, 0.03834925935338651, 0.03834925935338651, 0.0807751849621221, 0.0807751849621221, 0.0807751849621221, 0.520138212206239, 0.520138212206239, 0.520138212206239, 0.5558477998359908, 0.5558477998359908, 0.5558477998359908, 0.5682895440314135, 0.5682895440314135, 0.5682895440314135, 0.1053514676497238, 0.1053514676497238, 0.1053514676497238, 0.05363281581259527, 0.05363281581259527, 0.05363281581259527, 0.1585284672360322, 0.1585284672360322, 0.1585284672360322, 0.29150880570710347, 0.29150880570710347, 0.29150880570710347, 0.31486309274770785, 0.31486309274770785, 0.31486309274770785, 0.1917236496047251, 0.1917236496047251, 0.1917236496047251, 0.34033492540094734, 0.34033492540094734, 0.34033492540094734, 0.21107560476869225, 0.21107560476869225, 0.21107560476869225, 0.16775177892218307, 0.16775177892218307, 0.16775177892218307, 0.19649731060457742, 0.19649731060457742, 0.19649731060457742, 0.1723896145895618, 0.1723896145895618, 0.1723896145895618, 0.13312470287960387, 0.13312470287960387, 0.13312470287960387, 0.2127499527515604, 0.2127499527515604, 0.2127499527515604, 0.23680717677758178, 0.23680717677758178, 0.23680717677758178, 0.24236410615804438, 0.24236410615804438, 0.24236410615804438, 0.22108903238456545, 0.22108903238456545, 0.22108903238456545, 0.1802351449797116, 0.1802351449797116, 0.1802351449797116, 0.1888248956044597, 0.1888248956044597, 0.1888248956044597, 0.8483747271699531, 0.8483747271699531, 0.8483747271699531, 0.16608938213636926, 0.16608938213636926, 0.16608938213636926, 0.8084046491962622, 0.8084046491962622, 0.8084046491962622, 0.8417907063586292, 0.8417907063586292, 0.8417907063586292, 0.21035286446160217, 0.21035286446160217, 0.21035286446160217, 0.15599667862221145, 0.15599667862221145, 0.15599667862221145, 0.20330246188462542, 0.20330246188462542, 0.20330246188462542, 0.1901711347463937, 0.1901711347463937, 0.1901711347463937, 0.24705483087449043, 0.24705483087449043, 0.24705483087449043, 0.1052184149452634, 0.1052184149452634, 0.1052184149452634, 0.13111099249073033, 0.13111099249073033, 0.13111099249073033, 0.10319701519879199, 0.10319701519879199, 0.10319701519879199]}, "mutation_prompt": null}
{"id": "748fd061-151e-4e6d-bc72-c732f7b1f201", "solution": "import numpy as np\n\nclass PSO_SA_Optimizer_Enhanced_V6:\n    def __init__(self, budget, dim, pop_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.w_max = 0.85  # Adjusted max inertia weight for quicker adaptation\n        self.w_min = 0.35  # Adjusted min inertia weight for quicker adaptation\n        self.c1_init = 1.4  # Tuning cognitive coefficient for exploration\n        self.c2_init = 1.6  # Tuning social coefficient for exploitation\n        self.temp_init = 1.3  # Higher initial temperature for exploration\n\n    def __call__(self, func):\n        np.random.seed(42)\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        velocities = np.random.uniform(-0.4, 0.4, (self.pop_size, self.dim))  # Adjusted velocity range\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.array([func(pos) for pos in positions])\n        global_best_position = personal_best_positions[np.argmin(personal_best_scores)]\n        global_best_score = np.min(personal_best_scores)\n        \n        eval_count = self.pop_size\n        temperature = self.temp_init\n        \n        while eval_count < self.budget:\n            w = self.w_max - ((self.w_max - self.w_min) * (eval_count / self.budget))\n            c1 = self.c1_init * (1 - eval_count / self.budget) + 0.55\n            c2 = self.c2_init * (eval_count / self.budget) + 0.45\n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                velocities[i] = (w * velocities[i]\n                                 + c1 * r1 * (personal_best_positions[i] - positions[i])\n                                 + c2 * r2 * (global_best_position - positions[i]))\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], self.lower_bound, self.upper_bound)\n\n                score = func(positions[i])\n                eval_count += 1\n                if eval_count >= self.budget:\n                    break\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = positions[i]\n\n                if np.random.rand() < np.exp(-(score - global_best_score) / temperature):\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n                    if score < global_best_score:\n                        global_best_score = score\n                        global_best_position = positions[i]\n\n            temperature *= 0.88  # Fine-tuned annealing factor for adaptive temperature decay\n\n        return global_best_position, global_best_score", "name": "PSO_SA_Optimizer_Enhanced_V6", "description": "Enhanced adaptive PSO-SA optimizer with refined dynamic parameter adjustment for improved convergence efficiency.", "configspace": "", "generation": 80, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "376d31ce-e21f-4741-9568-0d0ba3ddc238", "metadata": {"aucs": [0.8167821930790096, 0.8167821930790096, 0.8167821930790096, 0.8143224007267236, 0.8143224007267236, 0.8143224007267236, 0.8355677960052937, 0.8355677960052937, 0.8355677960052937, 0.11887594998820494, 0.11887594998820494, 0.11887594998820494, 0.1616476462705395, 0.1616476462705395, 0.1616476462705395, 0.2293624404075113, 0.2293624404075113, 0.2293624404075113, 0.13100387068556485, 0.13100387068556485, 0.13100387068556485, 0.3347530163546938, 0.3347530163546938, 0.3347530163546938, 0.14572036904617724, 0.14572036904617724, 0.14572036904617724, 0.07061597256075891, 0.07061597256075891, 0.07061597256075891, 0.05339637868818048, 0.05339637868818048, 0.05339637868818048, 0.07087147130438332, 0.07087147130438332, 0.07087147130438332, 0.9754202403783134, 0.9754202403783134, 0.9754202403783134, 0.9821790305337643, 0.9821790305337643, 0.9821790305337643, 0.9834477199641951, 0.9834477199641951, 0.9834477199641951, 0.31600783713709557, 0.31600783713709557, 0.31600783713709557, 0.3834833995820046, 0.3834833995820046, 0.3834833995820046, 0.3677275865160786, 0.3677275865160786, 0.3677275865160786, 0.16965108282573615, 0.16965108282573615, 0.16965108282573615, 0.16241563431096961, 0.16241563431096961, 0.16241563431096961, 0.23012681019363213, 0.23012681019363213, 0.23012681019363213, 0.40766385472090816, 0.40766385472090816, 0.40766385472090816, 0.2423851168202279, 0.2423851168202279, 0.2423851168202279, 0.1318537560944678, 0.1318537560944678, 0.1318537560944678, 0.2379604796197219, 0.2379604796197219, 0.2379604796197219, 0.13192723242726057, 0.13192723242726057, 0.13192723242726057, 0.33072125245405637, 0.33072125245405637, 0.33072125245405637, 0.0648680695517001, 0.0648680695517001, 0.0648680695517001, 0.044110887371523466, 0.044110887371523466, 0.044110887371523466, 0.10663017118096318, 0.10663017118096318, 0.10663017118096318, 0.479961800985499, 0.479961800985499, 0.479961800985499, 0.30076156598948234, 0.30076156598948234, 0.30076156598948234, 0.23364652554277465, 0.23364652554277465, 0.23364652554277465, 0.04002014768183548, 0.04002014768183548, 0.04002014768183548, 0.26298916463890987, 0.26298916463890987, 0.26298916463890987, 0.15487679201894278, 0.15487679201894278, 0.15487679201894278, 0.1571352708483379, 0.1571352708483379, 0.1571352708483379, 0.03834925935338651, 0.03834925935338651, 0.03834925935338651, 0.0807751849621221, 0.0807751849621221, 0.0807751849621221, 0.520138212206239, 0.520138212206239, 0.520138212206239, 0.5558477998359908, 0.5558477998359908, 0.5558477998359908, 0.5682895440314135, 0.5682895440314135, 0.5682895440314135, 0.1053514676497238, 0.1053514676497238, 0.1053514676497238, 0.05363281581259527, 0.05363281581259527, 0.05363281581259527, 0.1585284672360322, 0.1585284672360322, 0.1585284672360322, 0.29150880570710347, 0.29150880570710347, 0.29150880570710347, 0.31486309274770785, 0.31486309274770785, 0.31486309274770785, 0.1917236496047251, 0.1917236496047251, 0.1917236496047251, 0.34033492540094734, 0.34033492540094734, 0.34033492540094734, 0.21107560476869225, 0.21107560476869225, 0.21107560476869225, 0.16775177892218307, 0.16775177892218307, 0.16775177892218307, 0.19649731060457742, 0.19649731060457742, 0.19649731060457742, 0.1723896145895618, 0.1723896145895618, 0.1723896145895618, 0.13312470287960387, 0.13312470287960387, 0.13312470287960387, 0.2127499527515604, 0.2127499527515604, 0.2127499527515604, 0.23680717677758178, 0.23680717677758178, 0.23680717677758178, 0.24236410615804438, 0.24236410615804438, 0.24236410615804438, 0.22108903238456545, 0.22108903238456545, 0.22108903238456545, 0.1802351449797116, 0.1802351449797116, 0.1802351449797116, 0.1888248956044597, 0.1888248956044597, 0.1888248956044597, 0.8483747271699531, 0.8483747271699531, 0.8483747271699531, 0.16608938213636926, 0.16608938213636926, 0.16608938213636926, 0.8084046491962622, 0.8084046491962622, 0.8084046491962622, 0.8417907063586292, 0.8417907063586292, 0.8417907063586292, 0.21035286446160217, 0.21035286446160217, 0.21035286446160217, 0.15599667862221145, 0.15599667862221145, 0.15599667862221145, 0.20330246188462542, 0.20330246188462542, 0.20330246188462542, 0.1901711347463937, 0.1901711347463937, 0.1901711347463937, 0.24705483087449043, 0.24705483087449043, 0.24705483087449043, 0.1052184149452634, 0.1052184149452634, 0.1052184149452634, 0.13111099249073033, 0.13111099249073033, 0.13111099249073033, 0.10319701519879199, 0.10319701519879199, 0.10319701519879199]}, "mutation_prompt": null}
{"id": "c2dedb5c-991c-4273-8c0f-4c77e2a41f32", "solution": "import numpy as np\n\nclass PSO_SA_Optimizer_Enhanced_V7:\n    def __init__(self, budget, dim, pop_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.w_max = 0.9  # Slightly increased max inertia weight for better initial exploration\n        self.w_min = 0.4  # Slightly increased min inertia weight for better adaptability\n        self.c1_init = 1.5  # Enhanced cognitive coefficient for improved balance\n        self.c2_init = 1.7  # Enhanced social coefficient for better convergence\n        self.temp_init = 1.5  # Higher initial temperature for broader exploration\n\n    def __call__(self, func):\n        np.random.seed(42)\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        velocities = np.random.uniform(-0.5, 0.5, (self.pop_size, self.dim))  # Adjusted velocity range\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.array([func(pos) for pos in positions])\n        global_best_position = personal_best_positions[np.argmin(personal_best_scores)]\n        global_best_score = np.min(personal_best_scores)\n        \n        eval_count = self.pop_size\n        temperature = self.temp_init\n        \n        while eval_count < self.budget:\n            w = self.w_max - ((self.w_max - self.w_min) * (eval_count / self.budget))\n            c1 = self.c1_init * (1 - eval_count / self.budget) + 0.5\n            c2 = self.c2_init * (eval_count / self.budget) + 0.5\n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                velocities[i] = (w * velocities[i]\n                                 + c1 * r1 * (personal_best_positions[i] - positions[i])\n                                 + c2 * r2 * (global_best_position - positions[i]))\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], self.lower_bound, self.upper_bound)\n\n                score = func(positions[i])\n                eval_count += 1\n                if eval_count >= self.budget:\n                    break\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = positions[i]\n\n                if np.random.rand() < np.exp(-(score - global_best_score) / temperature):\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n                    if score < global_best_score:\n                        global_best_score = score\n                        global_best_position = positions[i]\n\n            temperature *= 0.9  # Adjusted annealing factor for enhanced convergence\n\n        return global_best_position, global_best_score", "name": "PSO_SA_Optimizer_Enhanced_V7", "description": "Enhanced adaptive PSO-SA optimizer with optimized parameter tuning and refined mechanisms for improved convergence efficiency.", "configspace": "", "generation": 82, "fitness": 0.28908608338274083, "feedback": "The algorithm PSO_SA_Optimizer_Enhanced_V7 got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.29 with standard deviation 0.24.", "error": "", "parent_id": "376d31ce-e21f-4741-9568-0d0ba3ddc238", "metadata": {"aucs": [0.7540175438564778, 0.7540175438564778, 0.7540175438564778, 0.7851899797138078, 0.7851899797138078, 0.7851899797138078, 0.6870511535385064, 0.6870511535385064, 0.6870511535385064, 0.14363486042956797, 0.14363486042956797, 0.14363486042956797, 0.11151425472009069, 0.11151425472009069, 0.11151425472009069, 0.039824971120608677, 0.039824971120608677, 0.039824971120608677, 0.11091444827325014, 0.11091444827325014, 0.11091444827325014, 0.10051128079437621, 0.10051128079437621, 0.10051128079437621, 0.11802174624279616, 0.11802174624279616, 0.11802174624279616, 0.0871663468653654, 0.0871663468653654, 0.0871663468653654, 0.06551207295708772, 0.06551207295708772, 0.06551207295708772, 0.07612557265709896, 0.07612557265709896, 0.07612557265709896, 0.9807072258295387, 0.9807072258295387, 0.9807072258295387, 0.9848906150053661, 0.9848906150053661, 0.9848906150053661, 0.9851004507346615, 0.9851004507346615, 0.9851004507346615, 0.250325695806896, 0.250325695806896, 0.250325695806896, 0.3071891038374671, 0.3071891038374671, 0.3071891038374671, 0.3002254388445461, 0.3002254388445461, 0.3002254388445461, 0.20639828031375707, 0.20639828031375707, 0.20639828031375707, 0.20571348923307808, 0.20571348923307808, 0.20571348923307808, 0.2744390927949454, 0.2744390927949454, 0.2744390927949454, 0.21727825821302438, 0.21727825821302438, 0.21727825821302438, 0.3168200059692726, 0.3168200059692726, 0.3168200059692726, 0.5692312698601487, 0.5692312698601487, 0.5692312698601487, 0.503643911792463, 0.503643911792463, 0.503643911792463, 0.14431826841551665, 0.14431826841551665, 0.14431826841551665, 0.5489037301193969, 0.5489037301193969, 0.5489037301193969, 0.07347821128518339, 0.07347821128518339, 0.07347821128518339, 0.09089430125913456, 0.09089430125913456, 0.09089430125913456, 0.11873357870083567, 0.11873357870083567, 0.11873357870083567, 0.14950927867549546, 0.14950927867549546, 0.14950927867549546, 0.09616824145656044, 0.09616824145656044, 0.09616824145656044, 0.23825203799119143, 0.23825203799119143, 0.23825203799119143, 0.03619818735523528, 0.03619818735523528, 0.03619818735523528, 0.16013860474754382, 0.16013860474754382, 0.16013860474754382, 0.22270594956113743, 0.22270594956113743, 0.22270594956113743, 0.2685291908235474, 0.2685291908235474, 0.2685291908235474, 0.04390121967049121, 0.04390121967049121, 0.04390121967049121, 0.13257766371990198, 0.13257766371990198, 0.13257766371990198, 0.4757552452190582, 0.4757552452190582, 0.4757552452190582, 0.5304415780499501, 0.5304415780499501, 0.5304415780499501, 0.6512278498143527, 0.6512278498143527, 0.6512278498143527, 0.11577460419613561, 0.11577460419613561, 0.11577460419613561, 0.09989370540880715, 0.09989370540880715, 0.09989370540880715, 0.13000620660226592, 0.13000620660226592, 0.13000620660226592, 0.30495689078145327, 0.30495689078145327, 0.30495689078145327, 0.24987067039678668, 0.24987067039678668, 0.24987067039678668, 0.2658680866359975, 0.2658680866359975, 0.2658680866359975, 0.2663528947288384, 0.2663528947288384, 0.2663528947288384, 0.2063145440763361, 0.2063145440763361, 0.2063145440763361, 0.3097824139125356, 0.3097824139125356, 0.3097824139125356, 0.2121882234332022, 0.2121882234332022, 0.2121882234332022, 0.14792754575463474, 0.14792754575463474, 0.14792754575463474, 0.13384574945309635, 0.13384574945309635, 0.13384574945309635, 0.25862730894400343, 0.25862730894400343, 0.25862730894400343, 0.1962915382875403, 0.1962915382875403, 0.1962915382875403, 0.23943699610778768, 0.23943699610778768, 0.23943699610778768, 0.24124904559489802, 0.24124904559489802, 0.24124904559489802, 0.19107893740000537, 0.19107893740000537, 0.19107893740000537, 0.5392745189599764, 0.5392745189599764, 0.5392745189599764, 0.7837739037838545, 0.7837739037838545, 0.7837739037838545, 0.1660019856465511, 0.1660019856465511, 0.1660019856465511, 0.7141657358250981, 0.7141657358250981, 0.7141657358250981, 0.1682818051902235, 0.1682818051902235, 0.1682818051902235, 0.2115811343603431, 0.2115811343603431, 0.2115811343603431, 0.6197526365060557, 0.6197526365060557, 0.6197526365060557, 0.19430625719244576, 0.19430625719244576, 0.19430625719244576, 0.21946558161573548, 0.21946558161573548, 0.21946558161573548, 0.18971043109575625, 0.18971043109575625, 0.18971043109575625, 0.09582903804815146, 0.09582903804815146, 0.09582903804815146, 0.0925455927222687, 0.0925455927222687, 0.0925455927222687, 0.08686381462782633, 0.08686381462782633, 0.08686381462782633]}, "mutation_prompt": null}
{"id": "4a1c36d2-60e4-4ee3-aa09-0be7a29f5926", "solution": "import numpy as np\n\nclass PSO_SA_Optimizer_Enhanced_V7:\n    def __init__(self, budget, dim, pop_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.w_max = 0.82  # Modified max inertia weight for dynamic range adaptation\n        self.w_min = 0.32  # Modified min inertia weight for dynamic range adaptation\n        self.c1_init = 1.5  # Adjusted cognitive coefficient for improved exploration\n        self.c2_init = 1.5  # Adjusted social coefficient for improved exploitation\n        self.temp_init = 1.2  # Altered initial temperature for balanced exploration\n\n    def __call__(self, func):\n        np.random.seed(42)\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        velocities = np.random.uniform(-0.5, 0.5, (self.pop_size, self.dim))  # Updated velocity range\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.array([func(pos) for pos in positions])\n        global_best_position = personal_best_positions[np.argmin(personal_best_scores)]\n        global_best_score = np.min(personal_best_scores)\n        \n        eval_count = self.pop_size\n        temperature = self.temp_init\n        \n        while eval_count < self.budget:\n            w = self.w_max - ((self.w_max - self.w_min) * (eval_count / self.budget))\n            c1 = self.c1_init * (1 - eval_count / self.budget) + 0.60\n            c2 = self.c2_init * (eval_count / self.budget) + 0.40\n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                velocities[i] = (w * velocities[i]\n                                 + c1 * r1 * (personal_best_positions[i] - positions[i])\n                                 + c2 * r2 * (global_best_position - positions[i]))\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], self.lower_bound, self.upper_bound)\n\n                score = func(positions[i])\n                eval_count += 1\n                if eval_count >= self.budget:\n                    break\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = positions[i]\n\n                if np.random.rand() < np.exp(-(score - global_best_score) / temperature):\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n                    if score < global_best_score:\n                        global_best_score = score\n                        global_best_position = positions[i]\n\n            temperature *= 0.85  # Adjusted annealing factor for adaptive temperature decay\n\n        return global_best_position, global_best_score", "name": "PSO_SA_Optimizer_Enhanced_V7", "description": "Modified PSO-SA optimizer with enhanced velocity updates and adaptive temperature decay for better convergence.", "configspace": "", "generation": 83, "fitness": 0.299910233810202, "feedback": "The algorithm PSO_SA_Optimizer_Enhanced_V7 got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.30 with standard deviation 0.26.", "error": "", "parent_id": "376d31ce-e21f-4741-9568-0d0ba3ddc238", "metadata": {"aucs": [0.8581616288372897, 0.8581616288372897, 0.8581616288372897, 0.8558714027376966, 0.8558714027376966, 0.8558714027376966, 0.7938251841679739, 0.7938251841679739, 0.7938251841679739, 0.3161202161339587, 0.3161202161339587, 0.3161202161339587, 0.06781517759564903, 0.06781517759564903, 0.06781517759564903, 0.014674078273694935, 0.014674078273694935, 0.014674078273694935, 0.13534141194260396, 0.13534141194260396, 0.13534141194260396, 0.1346952780148467, 0.1346952780148467, 0.1346952780148467, 0.08737846242073133, 0.08737846242073133, 0.08737846242073133, 0.06913931950237762, 0.06913931950237762, 0.06913931950237762, 0.0802238206955842, 0.0802238206955842, 0.0802238206955842, 0.08187638999884927, 0.08187638999884927, 0.08187638999884927, 0.9775854884291599, 0.9775854884291599, 0.9775854884291599, 0.9814432983398529, 0.9814432983398529, 0.9814432983398529, 0.9820766568259338, 0.9820766568259338, 0.9820766568259338, 0.3219163645728216, 0.3219163645728216, 0.3219163645728216, 0.5376093931739452, 0.5376093931739452, 0.5376093931739452, 0.4086382058705924, 0.4086382058705924, 0.4086382058705924, 0.3341613724608191, 0.3341613724608191, 0.3341613724608191, 0.19254042327402066, 0.19254042327402066, 0.19254042327402066, 0.10769246461163984, 0.10769246461163984, 0.10769246461163984, 0.30291399812535824, 0.30291399812535824, 0.30291399812535824, 0.13175204201584623, 0.13175204201584623, 0.13175204201584623, 0.18697593911698385, 0.18697593911698385, 0.18697593911698385, 0.17215389195163877, 0.17215389195163877, 0.17215389195163877, 0.1312400708228495, 0.1312400708228495, 0.1312400708228495, 0.30970133430230806, 0.30970133430230806, 0.30970133430230806, 0.06489576983035727, 0.06489576983035727, 0.06489576983035727, 0.06072198091023551, 0.06072198091023551, 0.06072198091023551, 0.11058876151883612, 0.11058876151883612, 0.11058876151883612, 0.3208991020955013, 0.3208991020955013, 0.3208991020955013, 0.18026217495344776, 0.18026217495344776, 0.18026217495344776, 0.15947911237868428, 0.15947911237868428, 0.15947911237868428, 0.047424561884309546, 0.047424561884309546, 0.047424561884309546, 0.17707103040385375, 0.17707103040385375, 0.17707103040385375, 0.33301922037993636, 0.33301922037993636, 0.33301922037993636, 0.27848996735799103, 0.27848996735799103, 0.27848996735799103, 0.03857688776182744, 0.03857688776182744, 0.03857688776182744, 0.16387507940912638, 0.16387507940912638, 0.16387507940912638, 0.7989952482098132, 0.7989952482098132, 0.7989952482098132, 0.5430082833900617, 0.5430082833900617, 0.5430082833900617, 0.5512362136716931, 0.5512362136716931, 0.5512362136716931, 0.12426376930634819, 0.12426376930634819, 0.12426376930634819, 0.08629857039651634, 0.08629857039651634, 0.08629857039651634, 0.11902447288100759, 0.11902447288100759, 0.11902447288100759, 0.3137775647667288, 0.3137775647667288, 0.3137775647667288, 0.33226438604744957, 0.33226438604744957, 0.33226438604744957, 0.3165599166282348, 0.3165599166282348, 0.3165599166282348, 0.2958073498465996, 0.2958073498465996, 0.2958073498465996, 0.2014193254970068, 0.2014193254970068, 0.2014193254970068, 0.31946735232109513, 0.31946735232109513, 0.31946735232109513, 0.19173555705870515, 0.19173555705870515, 0.19173555705870515, 0.27666290811930416, 0.27666290811930416, 0.27666290811930416, 0.13757278806570994, 0.13757278806570994, 0.13757278806570994, 0.2500945792290845, 0.2500945792290845, 0.2500945792290845, 0.2614543772320741, 0.2614543772320741, 0.2614543772320741, 0.2180837733694857, 0.2180837733694857, 0.2180837733694857, 0.22154673072340514, 0.22154673072340514, 0.22154673072340514, 0.21015583038114172, 0.21015583038114172, 0.21015583038114172, 0.23722339776194745, 0.23722339776194745, 0.23722339776194745, 0.8690458747684631, 0.8690458747684631, 0.8690458747684631, 0.16396381590114362, 0.16396381590114362, 0.16396381590114362, 0.8677361492106402, 0.8677361492106402, 0.8677361492106402, 0.8055809598306243, 0.8055809598306243, 0.8055809598306243, 0.2126013466828781, 0.2126013466828781, 0.2126013466828781, 0.15627829901343537, 0.15627829901343537, 0.15627829901343537, 0.2333283154455048, 0.2333283154455048, 0.2333283154455048, 0.19187641040375103, 0.19187641040375103, 0.19187641040375103, 0.24776423421188187, 0.24776423421188187, 0.24776423421188187, 0.12305345428048808, 0.12305345428048808, 0.12305345428048808, 0.09958587402182584, 0.09958587402182584, 0.09958587402182584, 0.10724274256136446, 0.10724274256136446, 0.10724274256136446]}, "mutation_prompt": null}
{"id": "6da0e1c5-9bf8-48de-aa8e-49f252cba3ca", "solution": "import numpy as np\n\nclass PSO_SA_Optimizer_Enhanced_V8:\n    def __init__(self, budget, dim, pop_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.w_max = 0.85  # Slight increase in max inertia weight\n        self.w_min = 0.30  # Slight decrease in min inertia weight\n        self.c1_init = 1.4  # Slightly adjusted cognitive coefficient\n        self.c2_init = 1.6  # Slightly adjusted social coefficient\n        self.temp_init = 1.0  # Adjusted initial temperature\n\n    def __call__(self, func):\n        np.random.seed(42)\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        velocities = np.random.uniform(-0.3, 0.3, (self.pop_size, self.dim))  # Modified velocity range\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.array([func(pos) for pos in positions])\n        global_best_position = personal_best_positions[np.argmin(personal_best_scores)]\n        global_best_score = np.min(personal_best_scores)\n        \n        eval_count = self.pop_size\n        temperature = self.temp_init\n        \n        while eval_count < self.budget:\n            w = self.w_max - ((self.w_max - self.w_min) * (eval_count / self.budget))\n            c1 = self.c1_init * (1 - eval_count / self.budget) + 0.55\n            c2 = self.c2_init * (eval_count / self.budget) + 0.45\n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                velocities[i] = (w * velocities[i]\n                                 + c1 * r1 * (personal_best_positions[i] - positions[i])\n                                 + c2 * r2 * (global_best_position - positions[i]))\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], self.lower_bound, self.upper_bound)\n\n                score = func(positions[i])\n                eval_count += 1\n                if eval_count >= self.budget:\n                    break\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = positions[i]\n\n                if np.random.rand() < np.exp(-(score - global_best_score) / temperature):\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n                    if score < global_best_score:\n                        global_best_score = score\n                        global_best_position = positions[i]\n\n                # Introduce a mutation operator for exploration\n                if np.random.rand() < 0.05:\n                    positions[i] = np.random.uniform(self.lower_bound, self.upper_bound, self.dim)\n\n            temperature *= 0.80  # Adjusted annealing factor\n\n        return global_best_position, global_best_score", "name": "PSO_SA_Optimizer_Enhanced_V8", "description": "Enhanced PSO-SA with dynamic coefficients and adaptive mutation for better diversity and convergence.", "configspace": "", "generation": 84, "fitness": 0.22475240584056738, "feedback": "The algorithm PSO_SA_Optimizer_Enhanced_V8 got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.22 with standard deviation 0.21.", "error": "", "parent_id": "4a1c36d2-60e4-4ee3-aa09-0be7a29f5926", "metadata": {"aucs": [0.47817827852713324, 0.47817827852713324, 0.47817827852713324, 0.5123283241328724, 0.5123283241328724, 0.5123283241328724, 0.5146919589363399, 0.5146919589363399, 0.5146919589363399, 0.019146816809824996, 0.019146816809824996, 0.019146816809824996, 0.005837145763743279, 0.005837145763743279, 0.005837145763743279, 0.02739342933644562, 0.02739342933644562, 0.02739342933644562, 0.11683656975066026, 0.11683656975066026, 0.11683656975066026, 0.11151901454646573, 0.11151901454646573, 0.11151901454646573, 0.08255089144349625, 0.08255089144349625, 0.08255089144349625, 0.11004440404875371, 0.11004440404875371, 0.11004440404875371, 0.08893603973623976, 0.08893603973623976, 0.08893603973623976, 0.08099478213199274, 0.08099478213199274, 0.08099478213199274, 0.9828430235544299, 0.9828430235544299, 0.9828430235544299, 0.9749526973864703, 0.9749526973864703, 0.9749526973864703, 0.9879333083549682, 0.9879333083549682, 0.9879333083549682, 0.3092536611898904, 0.3092536611898904, 0.3092536611898904, 0.2578657752992948, 0.2578657752992948, 0.2578657752992948, 0.31988264730102023, 0.31988264730102023, 0.31988264730102023, 0.44222407390143126, 0.44222407390143126, 0.44222407390143126, 0.18089002197083315, 0.18089002197083315, 0.18089002197083315, 0.20450062338495778, 0.20450062338495778, 0.20450062338495778, 0.19740399509162143, 0.19740399509162143, 0.19740399509162143, 0.12634939615888963, 0.12634939615888963, 0.12634939615888963, 0.16227034339338553, 0.16227034339338553, 0.16227034339338553, 0.12179763172232438, 0.12179763172232438, 0.12179763172232438, 0.14985676649830404, 0.14985676649830404, 0.14985676649830404, 0.1617549621439477, 0.1617549621439477, 0.1617549621439477, 0.030739739134810096, 0.030739739134810096, 0.030739739134810096, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.013767938048519834, 0.013767938048519834, 0.013767938048519834, 0.11280613335182665, 0.11280613335182665, 0.11280613335182665, 0.09775951561969609, 0.09775951561969609, 0.09775951561969609, 0.1438394471485892, 0.1438394471485892, 0.1438394471485892, 0.030717609065924134, 0.030717609065924134, 0.030717609065924134, 0.06287601263084164, 0.06287601263084164, 0.06287601263084164, 0.023372933614512315, 0.023372933614512315, 0.023372933614512315, 0.11721128613731069, 0.11721128613731069, 0.11721128613731069, 0.04196563273679965, 0.04196563273679965, 0.04196563273679965, 0.08951919372947303, 0.08951919372947303, 0.08951919372947303, 0.398869447703433, 0.398869447703433, 0.398869447703433, 0.414808411190983, 0.414808411190983, 0.414808411190983, 0.4605165142109078, 0.4605165142109078, 0.4605165142109078, 0.11732934145831952, 0.11732934145831952, 0.11732934145831952, 0.11507954752173555, 0.11507954752173555, 0.11507954752173555, 0.08466224530597388, 0.08466224530597388, 0.08466224530597388, 0.17364102614183174, 0.17364102614183174, 0.17364102614183174, 0.14643480352454896, 0.14643480352454896, 0.14643480352454896, 0.17599686448211738, 0.17599686448211738, 0.17599686448211738, 0.3245372808071312, 0.3245372808071312, 0.3245372808071312, 0.25966718984555337, 0.25966718984555337, 0.25966718984555337, 0.24333076550489552, 0.24333076550489552, 0.24333076550489552, 0.20498904090585646, 0.20498904090585646, 0.20498904090585646, 0.17195357849756443, 0.17195357849756443, 0.17195357849756443, 0.13023552132611627, 0.13023552132611627, 0.13023552132611627, 0.2052057889751393, 0.2052057889751393, 0.2052057889751393, 0.19209767915318166, 0.19209767915318166, 0.19209767915318166, 0.2311811172738979, 0.2311811172738979, 0.2311811172738979, 0.20480390862435416, 0.20480390862435416, 0.20480390862435416, 0.1864572043408913, 0.1864572043408913, 0.1864572043408913, 0.19795036212110084, 0.19795036212110084, 0.19795036212110084, 0.6392747871752088, 0.6392747871752088, 0.6392747871752088, 0.1616417749792437, 0.1616417749792437, 0.1616417749792437, 0.6629393331044973, 0.6629393331044973, 0.6629393331044973, 0.168223803411939, 0.168223803411939, 0.168223803411939, 0.20493056963792333, 0.20493056963792333, 0.20493056963792333, 0.3486877269068134, 0.3486877269068134, 0.3486877269068134, 0.19583407126289087, 0.19583407126289087, 0.19583407126289087, 0.19748738596557402, 0.19748738596557402, 0.19748738596557402, 0.19609855465047543, 0.19609855465047543, 0.19609855465047543, 0.08908256681188609, 0.08908256681188609, 0.08908256681188609, 0.09366548844755707, 0.09366548844755707, 0.09366548844755707, 0.09164749551726958, 0.09164749551726958, 0.09164749551726958]}, "mutation_prompt": null}
{"id": "15348967-ad87-44c5-877a-5f12f4214e30", "solution": "import numpy as np\n\nclass PSO_SA_Optimizer_Enhanced_V8:\n    def __init__(self, budget, dim, pop_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.w_max = 0.9  # Further adjusted max inertia weight for dynamic adaptation\n        self.w_min = 0.3  # Further adjusted min inertia weight for dynamic adaptation\n        self.c1_init = 1.7  # Refined cognitive coefficient for better exploration\n        self.c2_init = 1.3  # Refined social coefficient for balanced exploitation\n        self.temp_init = 1.5  # Modified initial temperature for enhanced exploration\n\n    def __call__(self, func):\n        np.random.seed(42)\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        velocities = np.random.uniform(-0.5, 0.5, (self.pop_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.array([func(pos) for pos in positions])\n        global_best_position = personal_best_positions[np.argmin(personal_best_scores)]\n        global_best_score = np.min(personal_best_scores)\n        \n        eval_count = self.pop_size\n        temperature = self.temp_init\n        \n        while eval_count < self.budget:\n            w = self.w_max - ((self.w_max - self.w_min) * (eval_count / self.budget))\n            c1 = self.c1_init * (1 - eval_count / self.budget) + 0.5\n            c2 = self.c2_init * (eval_count / self.budget) + 0.5\n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                velocities[i] = (w * velocities[i]\n                                 + c1 * r1 * (personal_best_positions[i] - positions[i])\n                                 + c2 * r2 * (global_best_position - positions[i]))\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], self.lower_bound, self.upper_bound)\n\n                score = func(positions[i])\n                eval_count += 1\n                if eval_count >= self.budget:\n                    break\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = positions[i]\n\n                if np.random.rand() < np.exp(-(score - global_best_score) / temperature):\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n                    if score < global_best_score:\n                        global_best_score = score\n                        global_best_position = positions[i]\n\n            temperature *= 0.9  # Fine-tuned annealing factor for better temperature decay\n\n        return global_best_position, global_best_score", "name": "PSO_SA_Optimizer_Enhanced_V8", "description": "Enhanced PSO-SA with adaptive parameters and annealing strategy for improved convergence and exploration.", "configspace": "", "generation": 85, "fitness": 0.2614646445955964, "feedback": "The algorithm PSO_SA_Optimizer_Enhanced_V8 got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.26 with standard deviation 0.22.", "error": "", "parent_id": "4a1c36d2-60e4-4ee3-aa09-0be7a29f5926", "metadata": {"aucs": [0.7517155515280821, 0.7517155515280821, 0.7517155515280821, 0.6146921946892168, 0.6146921946892168, 0.6146921946892168, 0.7123404983386538, 0.7123404983386538, 0.7123404983386538, 0.08105737094258125, 0.08105737094258125, 0.08105737094258125, 0.06019352378013698, 0.06019352378013698, 0.06019352378013698, 0.12614838615970747, 0.12614838615970747, 0.12614838615970747, 0.11483078038811423, 0.11483078038811423, 0.11483078038811423, 0.10135286430483392, 0.10135286430483392, 0.10135286430483392, 0.10195364858932188, 0.10195364858932188, 0.10195364858932188, 0.0871513419196337, 0.0871513419196337, 0.0871513419196337, 0.1062007647511185, 0.1062007647511185, 0.1062007647511185, 0.08800257779486997, 0.08800257779486997, 0.08800257779486997, 0.9806996134621535, 0.9806996134621535, 0.9806996134621535, 0.9848915548320981, 0.9848915548320981, 0.9848915548320981, 0.9848612551661913, 0.9848612551661913, 0.9848612551661913, 0.2753880490158206, 0.2753880490158206, 0.2753880490158206, 0.36494044310913887, 0.36494044310913887, 0.36494044310913887, 0.2822782286920412, 0.2822782286920412, 0.2822782286920412, 0.21515502579013435, 0.21515502579013435, 0.21515502579013435, 0.2160372669591677, 0.2160372669591677, 0.2160372669591677, 0.34351105171164376, 0.34351105171164376, 0.34351105171164376, 0.2347930290260617, 0.2347930290260617, 0.2347930290260617, 0.25820057509352423, 0.25820057509352423, 0.25820057509352423, 0.21576389027993326, 0.21576389027993326, 0.21576389027993326, 0.20215748931959365, 0.20215748931959365, 0.20215748931959365, 0.14091290569102943, 0.14091290569102943, 0.14091290569102943, 0.13251543607674898, 0.13251543607674898, 0.13251543607674898, 0.09726516650036954, 0.09726516650036954, 0.09726516650036954, 0.032262235068368894, 0.032262235068368894, 0.032262235068368894, 0.10383992667462649, 0.10383992667462649, 0.10383992667462649, 0.15905204048753607, 0.15905204048753607, 0.15905204048753607, 0.1138897290624924, 0.1138897290624924, 0.1138897290624924, 0.1693119964191503, 0.1693119964191503, 0.1693119964191503, 0.04654343139857664, 0.04654343139857664, 0.04654343139857664, 0.1612379056018659, 0.1612379056018659, 0.1612379056018659, 0.2361268022188615, 0.2361268022188615, 0.2361268022188615, 0.14475340093983957, 0.14475340093983957, 0.14475340093983957, 0.07635399715830626, 0.07635399715830626, 0.07635399715830626, 0.20755460170216777, 0.20755460170216777, 0.20755460170216777, 0.5271531136472405, 0.5271531136472405, 0.5271531136472405, 0.4198372959772233, 0.4198372959772233, 0.4198372959772233, 0.5607796934491334, 0.5607796934491334, 0.5607796934491334, 0.08904291648767704, 0.08904291648767704, 0.08904291648767704, 0.10911816355886383, 0.10911816355886383, 0.10911816355886383, 0.14053067737722302, 0.14053067737722302, 0.14053067737722302, 0.2024406080716723, 0.2024406080716723, 0.2024406080716723, 0.1813595486738463, 0.1813595486738463, 0.1813595486738463, 0.25963571837643096, 0.25963571837643096, 0.25963571837643096, 0.26675151421116894, 0.26675151421116894, 0.26675151421116894, 0.23592719868952705, 0.23592719868952705, 0.23592719868952705, 0.2379144889176451, 0.2379144889176451, 0.2379144889176451, 0.2152310291608488, 0.2152310291608488, 0.2152310291608488, 0.20666602185679006, 0.20666602185679006, 0.20666602185679006, 0.14026013544017046, 0.14026013544017046, 0.14026013544017046, 0.23862308433463508, 0.23862308433463508, 0.23862308433463508, 0.25094363773430184, 0.25094363773430184, 0.25094363773430184, 0.24778136373613424, 0.24778136373613424, 0.24778136373613424, 0.1978954782517368, 0.1978954782517368, 0.1978954782517368, 0.18562339169398634, 0.18562339169398634, 0.18562339169398634, 0.2432296639029886, 0.2432296639029886, 0.2432296639029886, 0.8044737223184472, 0.8044737223184472, 0.8044737223184472, 0.1659335464564956, 0.1659335464564956, 0.1659335464564956, 0.725691423482239, 0.725691423482239, 0.725691423482239, 0.16718126045844472, 0.16718126045844472, 0.16718126045844472, 0.2084100552089977, 0.2084100552089977, 0.2084100552089977, 0.4159544149668024, 0.4159544149668024, 0.4159544149668024, 0.17984169374089853, 0.17984169374089853, 0.17984169374089853, 0.19239156250935296, 0.19239156250935296, 0.19239156250935296, 0.20283704469045838, 0.20283704469045838, 0.20283704469045838, 0.09573794850049633, 0.09573794850049633, 0.09573794850049633, 0.08821626173038943, 0.08821626173038943, 0.08821626173038943, 0.09610418262696241, 0.09610418262696241, 0.09610418262696241]}, "mutation_prompt": null}
{"id": "81c0d567-f3a8-4147-a25a-44765e19710a", "solution": "import numpy as np\n\nclass PSO_SA_Optimizer_Enhanced_V8:\n    def __init__(self, budget, dim, pop_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.w_max = 0.82\n        self.w_min = 0.32\n        self.c1_init = 1.7  # Slightly adjusted cognitive coefficient\n        self.c2_init = 1.3  # Slightly adjusted social coefficient\n        self.temp_init = 1.1  # Fine-tuned initial temperature\n\n    def __call__(self, func):\n        np.random.seed(42)\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        velocities = np.random.uniform(-0.5, 0.5, (self.pop_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.array([func(pos) for pos in positions])\n        global_best_position = personal_best_positions[np.argmin(personal_best_scores)]\n        global_best_score = np.min(personal_best_scores)\n        \n        eval_count = self.pop_size\n        temperature = self.temp_init\n        \n        while eval_count < self.budget:\n            w = self.w_max - ((self.w_max - self.w_min) * (eval_count / self.budget))\n            c1 = self.c1_init * (1 - eval_count / self.budget) + 0.55  # Modified coefficient scaling\n            c2 = self.c2_init * (eval_count / self.budget) + 0.45  # Modified coefficient scaling\n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                velocities[i] = (w * velocities[i]\n                                 + c1 * r1 * (personal_best_positions[i] - positions[i])\n                                 + c2 * r2 * (global_best_position - positions[i]))\n                positions[i] += velocities[i]\n                positions[i] = np.where(positions[i] > self.upper_bound, self.upper_bound, positions[i])\n                positions[i] = np.where(positions[i] < self.lower_bound, self.lower_bound, positions[i])\n\n                score = func(positions[i])\n                eval_count += 1\n                if eval_count >= self.budget:\n                    break\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = positions[i]\n\n                if np.random.rand() < np.exp(-(score - global_best_score) / temperature):\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n                    if score < global_best_score:\n                        global_best_score = score\n                        global_best_position = positions[i]\n\n            temperature *= 0.87  # Fine-tuned annealing factor for adaptive temperature decay\n\n        return global_best_position, global_best_score", "name": "PSO_SA_Optimizer_Enhanced_V8", "description": "Enhanced PSO-SA optimizer with adaptive boundary conditions and dynamic coefficient scaling for improved convergence.", "configspace": "", "generation": 86, "fitness": 0.28528773774423793, "feedback": "The algorithm PSO_SA_Optimizer_Enhanced_V8 got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.29 with standard deviation 0.25.", "error": "", "parent_id": "4a1c36d2-60e4-4ee3-aa09-0be7a29f5926", "metadata": {"aucs": [0.800810965845196, 0.800810965845196, 0.800810965845196, 0.8511732655575911, 0.8511732655575911, 0.8511732655575911, 0.8565612353917645, 0.8565612353917645, 0.8565612353917645, 0.07506266039755605, 0.07506266039755605, 0.07506266039755605, 0.09746438112288558, 0.09746438112288558, 0.09746438112288558, 0.18589382754967632, 0.18589382754967632, 0.18589382754967632, 0.12347007806952892, 0.12347007806952892, 0.12347007806952892, 0.10991126362524628, 0.10991126362524628, 0.10991126362524628, 0.13743537181984444, 0.13743537181984444, 0.13743537181984444, 0.061693363311961735, 0.061693363311961735, 0.061693363311961735, 0.05756175053851098, 0.05756175053851098, 0.05756175053851098, 0.08164731506164391, 0.08164731506164391, 0.08164731506164391, 0.9783130235738642, 0.9783130235738642, 0.9783130235738642, 0.98212308494581, 0.98212308494581, 0.98212308494581, 0.9839040063346693, 0.9839040063346693, 0.9839040063346693, 0.45474465591299484, 0.45474465591299484, 0.45474465591299484, 0.14827501706636026, 0.14827501706636026, 0.14827501706636026, 0.41941969991622785, 0.41941969991622785, 0.41941969991622785, 0.239135600825226, 0.239135600825226, 0.239135600825226, 0.16179884197626704, 0.16179884197626704, 0.16179884197626704, 0.21983038273807065, 0.21983038273807065, 0.21983038273807065, 0.24244173781722156, 0.24244173781722156, 0.24244173781722156, 0.1724197482574109, 0.1724197482574109, 0.1724197482574109, 0.1340643947887411, 0.1340643947887411, 0.1340643947887411, 0.15246232771499157, 0.15246232771499157, 0.15246232771499157, 0.3129881146773307, 0.3129881146773307, 0.3129881146773307, 0.21659288393617926, 0.21659288393617926, 0.21659288393617926, 0.08106901706704905, 0.08106901706704905, 0.08106901706704905, 0.05200058672276042, 0.05200058672276042, 0.05200058672276042, 0.17182478489062014, 0.17182478489062014, 0.17182478489062014, 0.14594387484493232, 0.14594387484493232, 0.14594387484493232, 0.1696443023279448, 0.1696443023279448, 0.1696443023279448, 0.1321754721426387, 0.1321754721426387, 0.1321754721426387, 0.044621594935977504, 0.044621594935977504, 0.044621594935977504, 0.35623595197230895, 0.35623595197230895, 0.35623595197230895, 0.11470543211661755, 0.11470543211661755, 0.11470543211661755, 0.2020979914147667, 0.2020979914147667, 0.2020979914147667, 0.5657481300017366, 0.5657481300017366, 0.5657481300017366, 0.181667212384664, 0.181667212384664, 0.181667212384664, 0.5926823891467647, 0.5926823891467647, 0.5926823891467647, 0.5090887771833938, 0.5090887771833938, 0.5090887771833938, 0.5842577696095745, 0.5842577696095745, 0.5842577696095745, 0.12659234369049832, 0.12659234369049832, 0.12659234369049832, 0.13728002142887596, 0.13728002142887596, 0.13728002142887596, 0.11245253514315134, 0.11245253514315134, 0.11245253514315134, 0.2058925762504381, 0.2058925762504381, 0.2058925762504381, 0.16740033328700787, 0.16740033328700787, 0.16740033328700787, 0.2589793905004686, 0.2589793905004686, 0.2589793905004686, 0.31489920199830945, 0.31489920199830945, 0.31489920199830945, 0.2235481959974701, 0.2235481959974701, 0.2235481959974701, 0.18204902706960624, 0.18204902706960624, 0.18204902706960624, 0.18219043894094744, 0.18219043894094744, 0.18219043894094744, 0.18381957942285965, 0.18381957942285965, 0.18381957942285965, 0.14235936125396742, 0.14235936125396742, 0.14235936125396742, 0.24413005616070682, 0.24413005616070682, 0.24413005616070682, 0.18545394554785144, 0.18545394554785144, 0.18545394554785144, 0.25473107620873714, 0.25473107620873714, 0.25473107620873714, 0.2049929858014966, 0.2049929858014966, 0.2049929858014966, 0.206757959613029, 0.206757959613029, 0.206757959613029, 0.19475575350156815, 0.19475575350156815, 0.19475575350156815, 0.8306110014552273, 0.8306110014552273, 0.8306110014552273, 0.15738292971088452, 0.15738292971088452, 0.15738292971088452, 0.7851427319179523, 0.7851427319179523, 0.7851427319179523, 0.8151182456080113, 0.8151182456080113, 0.8151182456080113, 0.2116846980430852, 0.2116846980430852, 0.2116846980430852, 0.15603431678548896, 0.15603431678548896, 0.15603431678548896, 0.21725747259208672, 0.21725747259208672, 0.21725747259208672, 0.21560156861873614, 0.21560156861873614, 0.21560156861873614, 0.20641443674039495, 0.20641443674039495, 0.20641443674039495, 0.09417047741793538, 0.09417047741793538, 0.09417047741793538, 0.0883359907839063, 0.0883359907839063, 0.0883359907839063, 0.07371618055991114, 0.07371618055991114, 0.07371618055991114]}, "mutation_prompt": null}
{"id": "db5b36de-2498-4fae-a8f4-566fe95b4c0e", "solution": "import numpy as np\n\nclass PSO_SA_Optimizer_Enhanced_V8:\n    def __init__(self, budget, dim, pop_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.w_max = 0.85  # Slightly increased inertia weight range\n        self.w_min = 0.30  # Slightly decreased inertia weight range\n        self.c1_init = 1.4  # Adjusted cognitive coefficient for better balance\n        self.c2_init = 1.6  # Adjusted social coefficient for better cooperation\n        self.temp_init = 1.5  # Increased initial temperature for wider exploration\n\n    def __call__(self, func):\n        np.random.seed(42)\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        velocities = np.random.uniform(-0.5, 0.5, (self.pop_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.array([func(pos) for pos in positions])\n        global_best_position = personal_best_positions[np.argmin(personal_best_scores)]\n        global_best_score = np.min(personal_best_scores)\n        \n        eval_count = self.pop_size\n        temperature = self.temp_init\n        \n        while eval_count < self.budget:\n            w = self.w_max - ((self.w_max - self.w_min) * (eval_count / self.budget))\n            c1 = self.c1_init * (1 - eval_count / self.budget) + 0.50\n            c2 = self.c2_init * (eval_count / self.budget) + 0.50\n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                velocities[i] = (w * velocities[i]\n                                 + c1 * r1 * (personal_best_positions[i] - positions[i])\n                                 + c2 * r2 * (global_best_position - positions[i]))\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], self.lower_bound, self.upper_bound)\n\n                score = func(positions[i])\n                eval_count += 1\n                if eval_count >= self.budget:\n                    break\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = positions[i]\n\n                if np.random.rand() < np.exp(-(score - global_best_score) / temperature):\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n                    if score < global_best_score:\n                        global_best_score = score\n                        global_best_position = positions[i]\n\n            temperature *= 0.80  # Slightly adjusted annealing factor for more dynamic cooling\n\n        return global_best_position, global_best_score", "name": "PSO_SA_Optimizer_Enhanced_V8", "description": "Enhanced PSO-SA with dynamic inertia weight scaling and adaptive parameter adjustment for improved convergence.", "configspace": "", "generation": 87, "fitness": 0.2637991252905316, "feedback": "The algorithm PSO_SA_Optimizer_Enhanced_V8 got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.26 with standard deviation 0.25.", "error": "", "parent_id": "4a1c36d2-60e4-4ee3-aa09-0be7a29f5926", "metadata": {"aucs": [0.8637362273627421, 0.8637362273627421, 0.8637362273627421, 0.8170959049116687, 0.8170959049116687, 0.8170959049116687, 0.8360877848162813, 0.8360877848162813, 0.8360877848162813, 0.14382083042220928, 0.14382083042220928, 0.14382083042220928, 0.11246649462129388, 0.11246649462129388, 0.11246649462129388, 0.05686985284005641, 0.05686985284005641, 0.05686985284005641, 0.15033598562657968, 0.15033598562657968, 0.15033598562657968, 0.09991465244622066, 0.09991465244622066, 0.09991465244622066, 0.12503013958194664, 0.12503013958194664, 0.12503013958194664, 0.05133288168752481, 0.05133288168752481, 0.05133288168752481, 0.09013067530233143, 0.09013067530233143, 0.09013067530233143, 0.09093869582746716, 0.09093869582746716, 0.09093869582746716, 0.9798594140285722, 0.9798594140285722, 0.9798594140285722, 0.9846870833388354, 0.9846870833388354, 0.9846870833388354, 0.9845536410508647, 0.9845536410508647, 0.9845536410508647, 0.20754459175401385, 0.20754459175401385, 0.20754459175401385, 0.3469551597335595, 0.3469551597335595, 0.3469551597335595, 0.3260632138899646, 0.3260632138899646, 0.3260632138899646, 0.23967097121152459, 0.23967097121152459, 0.23967097121152459, 0.21720695982096816, 0.21720695982096816, 0.21720695982096816, 0.12908221211515392, 0.12908221211515392, 0.12908221211515392, 0.2071170182349762, 0.2071170182349762, 0.2071170182349762, 0.12798383451376194, 0.12798383451376194, 0.12798383451376194, 0.1584819805318617, 0.1584819805318617, 0.1584819805318617, 0.13230235020429282, 0.13230235020429282, 0.13230235020429282, 0.20642268050273693, 0.20642268050273693, 0.20642268050273693, 0.29626810948208715, 0.29626810948208715, 0.29626810948208715, 0.08468122467472983, 0.08468122467472983, 0.08468122467472983, 0.08669504210629098, 0.08669504210629098, 0.08669504210629098, 0.03264350475740796, 0.03264350475740796, 0.03264350475740796, 0.12159872304418418, 0.12159872304418418, 0.12159872304418418, 0.14849017090378802, 0.14849017090378802, 0.14849017090378802, 0.2025585492895733, 0.2025585492895733, 0.2025585492895733, 0.05001378970033654, 0.05001378970033654, 0.05001378970033654, 0.264867017958761, 0.264867017958761, 0.264867017958761, 0.1999330320848589, 0.1999330320848589, 0.1999330320848589, 0.14611187894626287, 0.14611187894626287, 0.14611187894626287, 0.06962894131636699, 0.06962894131636699, 0.06962894131636699, 0.10274074927870946, 0.10274074927870946, 0.10274074927870946, 0.6355900642263844, 0.6355900642263844, 0.6355900642263844, 0.5811973574906919, 0.5811973574906919, 0.5811973574906919, 0.5428902560888169, 0.5428902560888169, 0.5428902560888169, 0.1060656243504724, 0.1060656243504724, 0.1060656243504724, 0.12290930699828406, 0.12290930699828406, 0.12290930699828406, 0.10419094974769993, 0.10419094974769993, 0.10419094974769993, 0.2009336566063441, 0.2009336566063441, 0.2009336566063441, 0.1660953044886685, 0.1660953044886685, 0.1660953044886685, 0.20586492597983275, 0.20586492597983275, 0.20586492597983275, 0.310765621260352, 0.310765621260352, 0.310765621260352, 0.17421117424792543, 0.17421117424792543, 0.17421117424792543, 0.21547177061747902, 0.21547177061747902, 0.21547177061747902, 0.1785693870189463, 0.1785693870189463, 0.1785693870189463, 0.17477302366997605, 0.17477302366997605, 0.17477302366997605, 0.13635314444839064, 0.13635314444839064, 0.13635314444839064, 0.23547649364086565, 0.23547649364086565, 0.23547649364086565, 0.2829602324226007, 0.2829602324226007, 0.2829602324226007, 0.20625012110706864, 0.20625012110706864, 0.20625012110706864, 0.20277081654512896, 0.20277081654512896, 0.20277081654512896, 0.1868821440588584, 0.1868821440588584, 0.1868821440588584, 0.19536702910641057, 0.19536702910641057, 0.19536702910641057, 0.9170342738042062, 0.9170342738042062, 0.9170342738042062, 0.1661494155853297, 0.1661494155853297, 0.1661494155853297, 0.8178694838637917, 0.8178694838637917, 0.8178694838637917, 0.16858692880581405, 0.16858692880581405, 0.16858692880581405, 0.20957071564712004, 0.20957071564712004, 0.20957071564712004, 0.15626966638888984, 0.15626966638888984, 0.15626966638888984, 0.20248964961206528, 0.20248964961206528, 0.20248964961206528, 0.19980853775579266, 0.19980853775579266, 0.19980853775579266, 0.20099130146705824, 0.20099130146705824, 0.20099130146705824, 0.10001176226831288, 0.10001176226831288, 0.10001176226831288, 0.10709223804591006, 0.10709223804591006, 0.10709223804591006, 0.09015267363202362, 0.09015267363202362, 0.09015267363202362]}, "mutation_prompt": null}
{"id": "3dca72b4-96fd-4dd3-b9d4-d5f108a7702c", "solution": "import numpy as np\n\nclass PSO_SA_Optimizer_Enhanced_V8:\n    def __init__(self, budget, dim, pop_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.w_max = 0.85  # Slightly increased max inertia\n        self.w_min = 0.25  # Slightly decreased min inertia\n        self.c1_init = 1.6  # Adjusted for exploration\n        self.c2_init = 1.4  # Adjusted for exploitation\n        self.temp_init = 1.0  # Adjusted initial temperature\n\n    def __call__(self, func):\n        np.random.seed(42)\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        velocities = np.random.uniform(-0.6, 0.6, (self.pop_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.array([func(pos) for pos in positions])\n        global_best_position = personal_best_positions[np.argmin(personal_best_scores)]\n        global_best_score = np.min(personal_best_scores)\n        \n        eval_count = self.pop_size\n        temperature = self.temp_init\n        \n        while eval_count < self.budget:\n            w = self.w_max - ((self.w_max - self.w_min) * (eval_count / self.budget))\n            c1 = self.c1_init * (1 - eval_count / self.budget) + 0.55\n            c2 = self.c2_init * (eval_count / self.budget) + 0.45\n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                velocities[i] = (w * velocities[i]\n                                 + c1 * r1 * (personal_best_positions[i] - positions[i])\n                                 + c2 * r2 * (global_best_position - positions[i]))\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], self.lower_bound, self.upper_bound)\n\n                score = func(positions[i])\n                eval_count += 1\n                if eval_count >= self.budget:\n                    break\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = positions[i]\n\n                if np.random.rand() < np.exp(-(score - global_best_score) / temperature):\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n                    if score < global_best_score:\n                        global_best_score = score\n                        global_best_position = positions[i]\n\n            temperature *= 0.88  # Adjusted annealing factor\n\n        return global_best_position, global_best_score", "name": "PSO_SA_Optimizer_Enhanced_V8", "description": "Enhanced PSO-SA optimizer with dynamic inertia, adaptive coefficients, and refined convergence mechanisms for improved performance.", "configspace": "", "generation": 88, "fitness": 0.2985980523492059, "feedback": "The algorithm PSO_SA_Optimizer_Enhanced_V8 got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.30 with standard deviation 0.27.", "error": "", "parent_id": "4a1c36d2-60e4-4ee3-aa09-0be7a29f5926", "metadata": {"aucs": [0.8389886023263418, 0.8389886023263418, 0.8389886023263418, 0.8317043695742983, 0.8317043695742983, 0.8317043695742983, 0.8366208139997142, 0.8366208139997142, 0.8366208139997142, 0.6215225993415121, 0.6215225993415121, 0.6215225993415121, 0.01973101839521274, 0.01973101839521274, 0.01973101839521274, 0.5848809540443662, 0.5848809540443662, 0.5848809540443662, 0.11791259515716179, 0.11791259515716179, 0.11791259515716179, 0.07994670696651252, 0.07994670696651252, 0.07994670696651252, 0.12879757032478467, 0.12879757032478467, 0.12879757032478467, 0.07486409975781083, 0.07486409975781083, 0.07486409975781083, 0.10055491682802198, 0.10055491682802198, 0.10055491682802198, 0.10120249740767118, 0.10120249740767118, 0.10120249740767118, 0.980642433661802, 0.980642433661802, 0.980642433661802, 0.9825956687057912, 0.9825956687057912, 0.9825956687057912, 0.9846529336511256, 0.9846529336511256, 0.9846529336511256, 0.3185466106753255, 0.3185466106753255, 0.3185466106753255, 0.3627317870038562, 0.3627317870038562, 0.3627317870038562, 0.3685293571889492, 0.3685293571889492, 0.3685293571889492, 0.28020348270863815, 0.28020348270863815, 0.28020348270863815, 0.19098300615043406, 0.19098300615043406, 0.19098300615043406, 0.875360637647518, 0.875360637647518, 0.875360637647518, 0.38058736394612347, 0.38058736394612347, 0.38058736394612347, 0.13443453816797413, 0.13443453816797413, 0.13443453816797413, 0.13010937258391042, 0.13010937258391042, 0.13010937258391042, 0.12855517156004537, 0.12855517156004537, 0.12855517156004537, 0.12833123100632815, 0.12833123100632815, 0.12833123100632815, 0.12971433862276316, 0.12971433862276316, 0.12971433862276316, 0.012223514500246169, 0.012223514500246169, 0.012223514500246169, 0.0569244754039846, 0.0569244754039846, 0.0569244754039846, 0.07384807746739241, 0.07384807746739241, 0.07384807746739241, 0.21625433797693483, 0.21625433797693483, 0.21625433797693483, 0.14590637350646263, 0.14590637350646263, 0.14590637350646263, 0.19318181423109504, 0.19318181423109504, 0.19318181423109504, 0.04350479102727933, 0.04350479102727933, 0.04350479102727933, 0.21801648871954782, 0.21801648871954782, 0.21801648871954782, 0.0925762892194465, 0.0925762892194465, 0.0925762892194465, 0.18679159555234748, 0.18679159555234748, 0.18679159555234748, 0.038300658066261484, 0.038300658066261484, 0.038300658066261484, 0.12205379370858249, 0.12205379370858249, 0.12205379370858249, 0.5008033006129496, 0.5008033006129496, 0.5008033006129496, 0.5907132296503017, 0.5907132296503017, 0.5907132296503017, 0.5586297631536665, 0.5586297631536665, 0.5586297631536665, 0.15075812619011397, 0.15075812619011397, 0.15075812619011397, 0.12208467434338577, 0.12208467434338577, 0.12208467434338577, 0.08332222671910672, 0.08332222671910672, 0.08332222671910672, 0.17728702805727115, 0.17728702805727115, 0.17728702805727115, 0.23513501114070623, 0.23513501114070623, 0.23513501114070623, 0.22356688164191707, 0.22356688164191707, 0.22356688164191707, 0.2959938091377523, 0.2959938091377523, 0.2959938091377523, 0.3935630556272436, 0.3935630556272436, 0.3935630556272436, 0.2998413181331194, 0.2998413181331194, 0.2998413181331194, 0.3010446065255741, 0.3010446065255741, 0.3010446065255741, 0.13720456512718082, 0.13720456512718082, 0.13720456512718082, 0.13862408032311357, 0.13862408032311357, 0.13862408032311357, 0.23130070898169452, 0.23130070898169452, 0.23130070898169452, 0.20074433152549231, 0.20074433152549231, 0.20074433152549231, 0.25958753928587, 0.25958753928587, 0.25958753928587, 0.1786246200075955, 0.1786246200075955, 0.1786246200075955, 0.17744492611289986, 0.17744492611289986, 0.17744492611289986, 0.20716530763593366, 0.20716530763593366, 0.20716530763593366, 0.8272216895234097, 0.8272216895234097, 0.8272216895234097, 0.16438414045746463, 0.16438414045746463, 0.16438414045746463, 0.7950294302639781, 0.7950294302639781, 0.7950294302639781, 0.8396372735442189, 0.8396372735442189, 0.8396372735442189, 0.2091680006300024, 0.2091680006300024, 0.2091680006300024, 0.15550318372968797, 0.15550318372968797, 0.15550318372968797, 0.2159099463204367, 0.2159099463204367, 0.2159099463204367, 0.18297793855192102, 0.18297793855192102, 0.18297793855192102, 0.22552957550298702, 0.22552957550298702, 0.22552957550298702, 0.10749762845059085, 0.10749762845059085, 0.10749762845059085, 0.09857941605495402, 0.09857941605495402, 0.09857941605495402, 0.10189554939671075, 0.10189554939671075, 0.10189554939671075]}, "mutation_prompt": null}
{"id": "1954d2ca-0274-4620-8f64-660ad264e2f6", "solution": "import numpy as np\n\nclass PSO_SA_Optimizer_Enhanced_V8:\n    def __init__(self, budget, dim, pop_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.w_max = 0.85  # Slightly increased max inertia weight for better exploration\n        self.w_min = 0.30  # Slightly decreased min inertia weight for better exploitation\n        self.c1_init = 1.4  # Slightly adjusted cognitive coefficient for adaptive balance\n        self.c2_init = 1.6  # Slightly adjusted social coefficient for adaptive balance\n        self.temp_init = 1.1  # Modified initial temperature for refined exploration\n\n    def __call__(self, func):\n        np.random.seed(42)\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        velocities = np.random.uniform(-0.5, 0.5, (self.pop_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.array([func(pos) for pos in positions])\n        global_best_position = personal_best_positions[np.argmin(personal_best_scores)]\n        global_best_score = np.min(personal_best_scores)\n        \n        eval_count = self.pop_size\n        temperature = self.temp_init\n        \n        while eval_count < self.budget:\n            w = self.w_max - ((self.w_max - self.w_min) * (eval_count / self.budget))\n            c1 = self.c1_init * (1 - eval_count / self.budget) + 0.60\n            c2 = self.c2_init * (eval_count / self.budget) + 0.40\n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                velocities[i] = (w * velocities[i]\n                                 + c1 * r1 * (personal_best_positions[i] - positions[i])\n                                 + c2 * r2 * (global_best_position - positions[i]))\n                crossover = np.random.rand(self.dim) < 0.5\n                positions[i] = np.where(crossover, positions[i] + velocities[i], global_best_position)\n                positions[i] = np.clip(positions[i], self.lower_bound, self.upper_bound)\n\n                score = func(positions[i])\n                eval_count += 1\n                if eval_count >= self.budget:\n                    break\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = positions[i]\n\n                if np.random.rand() < np.exp(-(score - global_best_score) / temperature):\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n                    if score < global_best_score:\n                        global_best_score = score\n                        global_best_position = positions[i]\n\n            temperature *= 0.90  # Adjusted annealing factor for sustained temperature decay\n\n        return global_best_position, global_best_score", "name": "PSO_SA_Optimizer_Enhanced_V8", "description": "Enhanced PSO-SA with adaptive inertia and crossover-inspired position updates for better global search.", "configspace": "", "generation": 89, "fitness": 0.27348351285736655, "feedback": "The algorithm PSO_SA_Optimizer_Enhanced_V8 got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.27 with standard deviation 0.25.", "error": "", "parent_id": "4a1c36d2-60e4-4ee3-aa09-0be7a29f5926", "metadata": {"aucs": [0.7182481541276297, 0.7182481541276297, 0.7182481541276297, 0.6847950171209638, 0.6847950171209638, 0.6847950171209638, 0.45664518322093206, 0.45664518322093206, 0.45664518322093206, 0.6586090771954436, 0.6586090771954436, 0.6586090771954436, 0.6971233294217132, 0.6971233294217132, 0.6971233294217132, 0.6647988767044746, 0.6647988767044746, 0.6647988767044746, 0.11020215893492402, 0.11020215893492402, 0.11020215893492402, 0.15504209436751504, 0.15504209436751504, 0.15504209436751504, 0.09034087159858029, 0.09034087159858029, 0.09034087159858029, 0.09171682232366751, 0.09171682232366751, 0.09171682232366751, 0.13506379252402234, 0.13506379252402234, 0.13506379252402234, 0.10618561499968782, 0.10618561499968782, 0.10618561499968782, 0.9675710718486267, 0.9675710718486267, 0.9675710718486267, 0.9871880845496017, 0.9871880845496017, 0.9871880845496017, 0.9868236062195093, 0.9868236062195093, 0.9868236062195093, 0.6563398277074077, 0.6563398277074077, 0.6563398277074077, 0.7762072817147554, 0.7762072817147554, 0.7762072817147554, 0.7121236440645655, 0.7121236440645655, 0.7121236440645655, 0.3226078154126357, 0.3226078154126357, 0.3226078154126357, 0.1615087183826347, 0.1615087183826347, 0.1615087183826347, 0.1818342923061006, 0.1818342923061006, 0.1818342923061006, 0.26086091712277737, 0.26086091712277737, 0.26086091712277737, 0.10345767281609208, 0.10345767281609208, 0.10345767281609208, 0.12692946970646934, 0.12692946970646934, 0.12692946970646934, 0.13005120670799608, 0.13005120670799608, 0.13005120670799608, 0.33569063112392394, 0.33569063112392394, 0.33569063112392394, 0.12882466607253573, 0.12882466607253573, 0.12882466607253573, 0.02284095849518164, 0.02284095849518164, 0.02284095849518164, 0.013343267655087199, 0.013343267655087199, 0.013343267655087199, 0.005751168309681587, 0.005751168309681587, 0.005751168309681587, 0.05785866890118885, 0.05785866890118885, 0.05785866890118885, 0.03832413202484408, 0.03832413202484408, 0.03832413202484408, 0.04677121129562334, 0.04677121129562334, 0.04677121129562334, 0.0349405101138518, 0.0349405101138518, 0.0349405101138518, 0.07171008397839396, 0.07171008397839396, 0.07171008397839396, 0.30997265874196467, 0.30997265874196467, 0.30997265874196467, 0.1402149681709236, 0.1402149681709236, 0.1402149681709236, 0.09630163544118897, 0.09630163544118897, 0.09630163544118897, 0.1397903817994064, 0.1397903817994064, 0.1397903817994064, 0.5365469119039068, 0.5365469119039068, 0.5365469119039068, 0.5589340131498485, 0.5589340131498485, 0.5589340131498485, 0.5352492906736295, 0.5352492906736295, 0.5352492906736295, 0.07625407481781477, 0.07625407481781477, 0.07625407481781477, 0.07826040407397394, 0.07826040407397394, 0.07826040407397394, 0.09946797653346162, 0.09946797653346162, 0.09946797653346162, 0.1674549977071601, 0.1674549977071601, 0.1674549977071601, 0.17186974061321658, 0.17186974061321658, 0.17186974061321658, 0.25579590130678653, 0.25579590130678653, 0.25579590130678653, 0.2232255828747336, 0.2232255828747336, 0.2232255828747336, 0.2614645721144766, 0.2614645721144766, 0.2614645721144766, 0.1843636097399607, 0.1843636097399607, 0.1843636097399607, 0.17099370595482055, 0.17099370595482055, 0.17099370595482055, 0.22170685597298967, 0.22170685597298967, 0.22170685597298967, 0.13357597875084626, 0.13357597875084626, 0.13357597875084626, 0.19547571098068683, 0.19547571098068683, 0.19547571098068683, 0.22925093097011828, 0.22925093097011828, 0.22925093097011828, 0.2076375515848753, 0.2076375515848753, 0.2076375515848753, 0.2094107430282015, 0.2094107430282015, 0.2094107430282015, 0.20927512901860534, 0.20927512901860534, 0.20927512901860534, 0.19869078923436245, 0.19869078923436245, 0.19869078923436245, 0.5494104896615378, 0.5494104896615378, 0.5494104896615378, 0.16587505991250162, 0.16587505991250162, 0.16587505991250162, 0.17829188411625707, 0.17829188411625707, 0.17829188411625707, 0.21239837626433822, 0.21239837626433822, 0.21239837626433822, 0.21383158088039145, 0.21383158088039145, 0.21383158088039145, 0.15746236037180095, 0.15746236037180095, 0.15746236037180095, 0.1776203776298002, 0.1776203776298002, 0.1776203776298002, 0.2288460994635475, 0.2288460994635475, 0.2288460994635475, 0.21614660335301505, 0.21614660335301505, 0.21614660335301505, 0.10205469606432072, 0.10205469606432072, 0.10205469606432072, 0.09915005083679429, 0.09915005083679429, 0.09915005083679429, 0.08021133294908778, 0.08021133294908778, 0.08021133294908778]}, "mutation_prompt": null}
{"id": "64383f0a-3ba7-486e-a70c-ae6346269b26", "solution": "import numpy as np\n\nclass PSO_SA_Optimizer_Enhanced_V7:\n    def __init__(self, budget, dim, pop_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.w_max = 0.82  # Modified max inertia weight for dynamic range adaptation\n        self.w_min = 0.32  # Modified min inertia weight for dynamic range adaptation\n        self.c1_init = 1.5  # Adjusted cognitive coefficient for improved exploration\n        self.c2_init = 1.5  # Adjusted social coefficient for improved exploitation\n        self.temp_init = 1.2  # Altered initial temperature for balanced exploration\n\n    def __call__(self, func):\n        np.random.seed(42)\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        velocities = np.random.uniform(-0.5, 0.5, (self.pop_size, self.dim))  # Updated velocity range\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.array([func(pos) for pos in positions])\n        global_best_position = personal_best_positions[np.argmin(personal_best_scores)]\n        global_best_score = np.min(personal_best_scores)\n        \n        eval_count = self.pop_size\n        temperature = self.temp_init\n        \n        while eval_count < self.budget:\n            w = self.w_max - ((self.w_max - self.w_min) * (eval_count / self.budget))\n            c1 = self.c1_init * (1 - eval_count / self.budget) + 0.60\n            c2 = self.c2_init * (eval_count / self.budget) + 0.40\n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                velocities[i] = (w * velocities[i]\n                                 + c1 * r1 * (personal_best_positions[i] - positions[i])\n                                 + c2 * r2 * (global_best_position - positions[i]))\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], self.lower_bound, self.upper_bound)\n\n                score = func(positions[i])\n                eval_count += 1\n                if eval_count >= self.budget:\n                    break\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = positions[i]\n\n                if np.random.rand() < np.exp(-(score - global_best_score) / temperature):\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n                    if score < global_best_score:\n                        global_best_score = score\n                        global_best_position = positions[i]\n\n            temperature *= 0.85  # Adjusted annealing factor for adaptive temperature decay\n\n        return global_best_position, global_best_score", "name": "PSO_SA_Optimizer_Enhanced_V7", "description": "Modified PSO-SA optimizer with enhanced velocity updates and adaptive temperature decay for better convergence.", "configspace": "", "generation": 84, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "4a1c36d2-60e4-4ee3-aa09-0be7a29f5926", "metadata": {"aucs": [0.8581616288372897, 0.8581616288372897, 0.8581616288372897, 0.8558714027376966, 0.8558714027376966, 0.8558714027376966, 0.7938251841679739, 0.7938251841679739, 0.7938251841679739, 0.3161202161339587, 0.3161202161339587, 0.3161202161339587, 0.06781517759564903, 0.06781517759564903, 0.06781517759564903, 0.014674078273694935, 0.014674078273694935, 0.014674078273694935, 0.13534141194260396, 0.13534141194260396, 0.13534141194260396, 0.1346952780148467, 0.1346952780148467, 0.1346952780148467, 0.08737846242073133, 0.08737846242073133, 0.08737846242073133, 0.06913931950237762, 0.06913931950237762, 0.06913931950237762, 0.0802238206955842, 0.0802238206955842, 0.0802238206955842, 0.08187638999884927, 0.08187638999884927, 0.08187638999884927, 0.9775854884291599, 0.9775854884291599, 0.9775854884291599, 0.9814432983398529, 0.9814432983398529, 0.9814432983398529, 0.9820766568259338, 0.9820766568259338, 0.9820766568259338, 0.3219163645728216, 0.3219163645728216, 0.3219163645728216, 0.5376093931739452, 0.5376093931739452, 0.5376093931739452, 0.4086382058705924, 0.4086382058705924, 0.4086382058705924, 0.3341613724608191, 0.3341613724608191, 0.3341613724608191, 0.19254042327402066, 0.19254042327402066, 0.19254042327402066, 0.10769246461163984, 0.10769246461163984, 0.10769246461163984, 0.30291399812535824, 0.30291399812535824, 0.30291399812535824, 0.13175204201584623, 0.13175204201584623, 0.13175204201584623, 0.18697593911698385, 0.18697593911698385, 0.18697593911698385, 0.17215389195163877, 0.17215389195163877, 0.17215389195163877, 0.1312400708228495, 0.1312400708228495, 0.1312400708228495, 0.30970133430230806, 0.30970133430230806, 0.30970133430230806, 0.06489576983035727, 0.06489576983035727, 0.06489576983035727, 0.06072198091023551, 0.06072198091023551, 0.06072198091023551, 0.11058876151883612, 0.11058876151883612, 0.11058876151883612, 0.3208991020955013, 0.3208991020955013, 0.3208991020955013, 0.18026217495344776, 0.18026217495344776, 0.18026217495344776, 0.15947911237868428, 0.15947911237868428, 0.15947911237868428, 0.047424561884309546, 0.047424561884309546, 0.047424561884309546, 0.17707103040385375, 0.17707103040385375, 0.17707103040385375, 0.33301922037993636, 0.33301922037993636, 0.33301922037993636, 0.27848996735799103, 0.27848996735799103, 0.27848996735799103, 0.03857688776182744, 0.03857688776182744, 0.03857688776182744, 0.16387507940912638, 0.16387507940912638, 0.16387507940912638, 0.7989952482098132, 0.7989952482098132, 0.7989952482098132, 0.5430082833900617, 0.5430082833900617, 0.5430082833900617, 0.5512362136716931, 0.5512362136716931, 0.5512362136716931, 0.12426376930634819, 0.12426376930634819, 0.12426376930634819, 0.08629857039651634, 0.08629857039651634, 0.08629857039651634, 0.11902447288100759, 0.11902447288100759, 0.11902447288100759, 0.3137775647667288, 0.3137775647667288, 0.3137775647667288, 0.33226438604744957, 0.33226438604744957, 0.33226438604744957, 0.3165599166282348, 0.3165599166282348, 0.3165599166282348, 0.2958073498465996, 0.2958073498465996, 0.2958073498465996, 0.2014193254970068, 0.2014193254970068, 0.2014193254970068, 0.31946735232109513, 0.31946735232109513, 0.31946735232109513, 0.19173555705870515, 0.19173555705870515, 0.19173555705870515, 0.27666290811930416, 0.27666290811930416, 0.27666290811930416, 0.13757278806570994, 0.13757278806570994, 0.13757278806570994, 0.2500945792290845, 0.2500945792290845, 0.2500945792290845, 0.2614543772320741, 0.2614543772320741, 0.2614543772320741, 0.2180837733694857, 0.2180837733694857, 0.2180837733694857, 0.22154673072340514, 0.22154673072340514, 0.22154673072340514, 0.21015583038114172, 0.21015583038114172, 0.21015583038114172, 0.23722339776194745, 0.23722339776194745, 0.23722339776194745, 0.8690458747684631, 0.8690458747684631, 0.8690458747684631, 0.16396381590114362, 0.16396381590114362, 0.16396381590114362, 0.8677361492106402, 0.8677361492106402, 0.8677361492106402, 0.8055809598306243, 0.8055809598306243, 0.8055809598306243, 0.2126013466828781, 0.2126013466828781, 0.2126013466828781, 0.15627829901343537, 0.15627829901343537, 0.15627829901343537, 0.2333283154455048, 0.2333283154455048, 0.2333283154455048, 0.19187641040375103, 0.19187641040375103, 0.19187641040375103, 0.24776423421188187, 0.24776423421188187, 0.24776423421188187, 0.12305345428048808, 0.12305345428048808, 0.12305345428048808, 0.09958587402182584, 0.09958587402182584, 0.09958587402182584, 0.10724274256136446, 0.10724274256136446, 0.10724274256136446]}, "mutation_prompt": null}
{"id": "a49b9d33-19b2-46e5-a16a-3e8df167c970", "solution": "import numpy as np\n\nclass PSO_SA_Optimizer_Enhanced_V7:\n    def __init__(self, budget, dim, pop_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.w_max = 0.82  # Modified max inertia weight for dynamic range adaptation\n        self.w_min = 0.32  # Modified min inertia weight for dynamic range adaptation\n        self.c1_init = 1.5  # Adjusted cognitive coefficient for improved exploration\n        self.c2_init = 1.5  # Adjusted social coefficient for improved exploitation\n        self.temp_init = 1.2  # Altered initial temperature for balanced exploration\n\n    def __call__(self, func):\n        np.random.seed(42)\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        velocities = np.random.uniform(-0.5, 0.5, (self.pop_size, self.dim))  # Updated velocity range\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.array([func(pos) for pos in positions])\n        global_best_position = personal_best_positions[np.argmin(personal_best_scores)]\n        global_best_score = np.min(personal_best_scores)\n        \n        eval_count = self.pop_size\n        temperature = self.temp_init\n        \n        while eval_count < self.budget:\n            w = self.w_max - ((self.w_max - self.w_min) * (eval_count / self.budget))\n            c1 = self.c1_init * (1 - eval_count / self.budget) + 0.60\n            c2 = self.c2_init * (eval_count / self.budget) + 0.40\n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                velocities[i] = (w * velocities[i]\n                                 + c1 * r1 * (personal_best_positions[i] - positions[i])\n                                 + c2 * r2 * (global_best_position - positions[i]))\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], self.lower_bound, self.upper_bound)\n\n                score = func(positions[i])\n                eval_count += 1\n                if eval_count >= self.budget:\n                    break\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = positions[i]\n\n                if np.random.rand() < np.exp(-(score - global_best_score) / temperature):\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n                    if score < global_best_score:\n                        global_best_score = score\n                        global_best_position = positions[i]\n\n            temperature *= 0.85  # Adjusted annealing factor for adaptive temperature decay\n\n        return global_best_position, global_best_score", "name": "PSO_SA_Optimizer_Enhanced_V7", "description": "Modified PSO-SA optimizer with enhanced velocity updates and adaptive temperature decay for better convergence.", "configspace": "", "generation": 84, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "4a1c36d2-60e4-4ee3-aa09-0be7a29f5926", "metadata": {"aucs": [0.8581616288372897, 0.8581616288372897, 0.8581616288372897, 0.8558714027376966, 0.8558714027376966, 0.8558714027376966, 0.7938251841679739, 0.7938251841679739, 0.7938251841679739, 0.3161202161339587, 0.3161202161339587, 0.3161202161339587, 0.06781517759564903, 0.06781517759564903, 0.06781517759564903, 0.014674078273694935, 0.014674078273694935, 0.014674078273694935, 0.13534141194260396, 0.13534141194260396, 0.13534141194260396, 0.1346952780148467, 0.1346952780148467, 0.1346952780148467, 0.08737846242073133, 0.08737846242073133, 0.08737846242073133, 0.06913931950237762, 0.06913931950237762, 0.06913931950237762, 0.0802238206955842, 0.0802238206955842, 0.0802238206955842, 0.08187638999884927, 0.08187638999884927, 0.08187638999884927, 0.9775854884291599, 0.9775854884291599, 0.9775854884291599, 0.9814432983398529, 0.9814432983398529, 0.9814432983398529, 0.9820766568259338, 0.9820766568259338, 0.9820766568259338, 0.3219163645728216, 0.3219163645728216, 0.3219163645728216, 0.5376093931739452, 0.5376093931739452, 0.5376093931739452, 0.4086382058705924, 0.4086382058705924, 0.4086382058705924, 0.3341613724608191, 0.3341613724608191, 0.3341613724608191, 0.19254042327402066, 0.19254042327402066, 0.19254042327402066, 0.10769246461163984, 0.10769246461163984, 0.10769246461163984, 0.30291399812535824, 0.30291399812535824, 0.30291399812535824, 0.13175204201584623, 0.13175204201584623, 0.13175204201584623, 0.18697593911698385, 0.18697593911698385, 0.18697593911698385, 0.17215389195163877, 0.17215389195163877, 0.17215389195163877, 0.1312400708228495, 0.1312400708228495, 0.1312400708228495, 0.30970133430230806, 0.30970133430230806, 0.30970133430230806, 0.06489576983035727, 0.06489576983035727, 0.06489576983035727, 0.06072198091023551, 0.06072198091023551, 0.06072198091023551, 0.11058876151883612, 0.11058876151883612, 0.11058876151883612, 0.3208991020955013, 0.3208991020955013, 0.3208991020955013, 0.18026217495344776, 0.18026217495344776, 0.18026217495344776, 0.15947911237868428, 0.15947911237868428, 0.15947911237868428, 0.047424561884309546, 0.047424561884309546, 0.047424561884309546, 0.17707103040385375, 0.17707103040385375, 0.17707103040385375, 0.33301922037993636, 0.33301922037993636, 0.33301922037993636, 0.27848996735799103, 0.27848996735799103, 0.27848996735799103, 0.03857688776182744, 0.03857688776182744, 0.03857688776182744, 0.16387507940912638, 0.16387507940912638, 0.16387507940912638, 0.7989952482098132, 0.7989952482098132, 0.7989952482098132, 0.5430082833900617, 0.5430082833900617, 0.5430082833900617, 0.5512362136716931, 0.5512362136716931, 0.5512362136716931, 0.12426376930634819, 0.12426376930634819, 0.12426376930634819, 0.08629857039651634, 0.08629857039651634, 0.08629857039651634, 0.11902447288100759, 0.11902447288100759, 0.11902447288100759, 0.3137775647667288, 0.3137775647667288, 0.3137775647667288, 0.33226438604744957, 0.33226438604744957, 0.33226438604744957, 0.3165599166282348, 0.3165599166282348, 0.3165599166282348, 0.2958073498465996, 0.2958073498465996, 0.2958073498465996, 0.2014193254970068, 0.2014193254970068, 0.2014193254970068, 0.31946735232109513, 0.31946735232109513, 0.31946735232109513, 0.19173555705870515, 0.19173555705870515, 0.19173555705870515, 0.27666290811930416, 0.27666290811930416, 0.27666290811930416, 0.13757278806570994, 0.13757278806570994, 0.13757278806570994, 0.2500945792290845, 0.2500945792290845, 0.2500945792290845, 0.2614543772320741, 0.2614543772320741, 0.2614543772320741, 0.2180837733694857, 0.2180837733694857, 0.2180837733694857, 0.22154673072340514, 0.22154673072340514, 0.22154673072340514, 0.21015583038114172, 0.21015583038114172, 0.21015583038114172, 0.23722339776194745, 0.23722339776194745, 0.23722339776194745, 0.8690458747684631, 0.8690458747684631, 0.8690458747684631, 0.16396381590114362, 0.16396381590114362, 0.16396381590114362, 0.8677361492106402, 0.8677361492106402, 0.8677361492106402, 0.8055809598306243, 0.8055809598306243, 0.8055809598306243, 0.2126013466828781, 0.2126013466828781, 0.2126013466828781, 0.15627829901343537, 0.15627829901343537, 0.15627829901343537, 0.2333283154455048, 0.2333283154455048, 0.2333283154455048, 0.19187641040375103, 0.19187641040375103, 0.19187641040375103, 0.24776423421188187, 0.24776423421188187, 0.24776423421188187, 0.12305345428048808, 0.12305345428048808, 0.12305345428048808, 0.09958587402182584, 0.09958587402182584, 0.09958587402182584, 0.10724274256136446, 0.10724274256136446, 0.10724274256136446]}, "mutation_prompt": null}
{"id": "e682cea3-43b8-45e8-9ae3-94296190c041", "solution": "import numpy as np\n\nclass PSO_SA_Optimizer_Enhanced_V7:\n    def __init__(self, budget, dim, pop_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.w_max = 0.82  # Modified max inertia weight for dynamic range adaptation\n        self.w_min = 0.32  # Modified min inertia weight for dynamic range adaptation\n        self.c1_init = 1.5  # Adjusted cognitive coefficient for improved exploration\n        self.c2_init = 1.5  # Adjusted social coefficient for improved exploitation\n        self.temp_init = 1.2  # Altered initial temperature for balanced exploration\n\n    def __call__(self, func):\n        np.random.seed(42)\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        velocities = np.random.uniform(-0.5, 0.5, (self.pop_size, self.dim))  # Updated velocity range\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.array([func(pos) for pos in positions])\n        global_best_position = personal_best_positions[np.argmin(personal_best_scores)]\n        global_best_score = np.min(personal_best_scores)\n        \n        eval_count = self.pop_size\n        temperature = self.temp_init\n        \n        while eval_count < self.budget:\n            w = self.w_max - ((self.w_max - self.w_min) * (eval_count / self.budget))\n            c1 = self.c1_init * (1 - eval_count / self.budget) + 0.60\n            c2 = self.c2_init * (eval_count / self.budget) + 0.40\n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                velocities[i] = (w * velocities[i]\n                                 + c1 * r1 * (personal_best_positions[i] - positions[i])\n                                 + c2 * r2 * (global_best_position - positions[i]))\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], self.lower_bound, self.upper_bound)\n\n                score = func(positions[i])\n                eval_count += 1\n                if eval_count >= self.budget:\n                    break\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = positions[i]\n\n                if np.random.rand() < np.exp(-(score - global_best_score) / temperature):\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n                    if score < global_best_score:\n                        global_best_score = score\n                        global_best_position = positions[i]\n\n            temperature *= 0.85  # Adjusted annealing factor for adaptive temperature decay\n\n        return global_best_position, global_best_score", "name": "PSO_SA_Optimizer_Enhanced_V7", "description": "Modified PSO-SA optimizer with enhanced velocity updates and adaptive temperature decay for better convergence.", "configspace": "", "generation": 84, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "4a1c36d2-60e4-4ee3-aa09-0be7a29f5926", "metadata": {"aucs": [0.8581616288372897, 0.8581616288372897, 0.8581616288372897, 0.8558714027376966, 0.8558714027376966, 0.8558714027376966, 0.7938251841679739, 0.7938251841679739, 0.7938251841679739, 0.3161202161339587, 0.3161202161339587, 0.3161202161339587, 0.06781517759564903, 0.06781517759564903, 0.06781517759564903, 0.014674078273694935, 0.014674078273694935, 0.014674078273694935, 0.13534141194260396, 0.13534141194260396, 0.13534141194260396, 0.1346952780148467, 0.1346952780148467, 0.1346952780148467, 0.08737846242073133, 0.08737846242073133, 0.08737846242073133, 0.06913931950237762, 0.06913931950237762, 0.06913931950237762, 0.0802238206955842, 0.0802238206955842, 0.0802238206955842, 0.08187638999884927, 0.08187638999884927, 0.08187638999884927, 0.9775854884291599, 0.9775854884291599, 0.9775854884291599, 0.9814432983398529, 0.9814432983398529, 0.9814432983398529, 0.9820766568259338, 0.9820766568259338, 0.9820766568259338, 0.3219163645728216, 0.3219163645728216, 0.3219163645728216, 0.5376093931739452, 0.5376093931739452, 0.5376093931739452, 0.4086382058705924, 0.4086382058705924, 0.4086382058705924, 0.3341613724608191, 0.3341613724608191, 0.3341613724608191, 0.19254042327402066, 0.19254042327402066, 0.19254042327402066, 0.10769246461163984, 0.10769246461163984, 0.10769246461163984, 0.30291399812535824, 0.30291399812535824, 0.30291399812535824, 0.13175204201584623, 0.13175204201584623, 0.13175204201584623, 0.18697593911698385, 0.18697593911698385, 0.18697593911698385, 0.17215389195163877, 0.17215389195163877, 0.17215389195163877, 0.1312400708228495, 0.1312400708228495, 0.1312400708228495, 0.30970133430230806, 0.30970133430230806, 0.30970133430230806, 0.06489576983035727, 0.06489576983035727, 0.06489576983035727, 0.06072198091023551, 0.06072198091023551, 0.06072198091023551, 0.11058876151883612, 0.11058876151883612, 0.11058876151883612, 0.3208991020955013, 0.3208991020955013, 0.3208991020955013, 0.18026217495344776, 0.18026217495344776, 0.18026217495344776, 0.15947911237868428, 0.15947911237868428, 0.15947911237868428, 0.047424561884309546, 0.047424561884309546, 0.047424561884309546, 0.17707103040385375, 0.17707103040385375, 0.17707103040385375, 0.33301922037993636, 0.33301922037993636, 0.33301922037993636, 0.27848996735799103, 0.27848996735799103, 0.27848996735799103, 0.03857688776182744, 0.03857688776182744, 0.03857688776182744, 0.16387507940912638, 0.16387507940912638, 0.16387507940912638, 0.7989952482098132, 0.7989952482098132, 0.7989952482098132, 0.5430082833900617, 0.5430082833900617, 0.5430082833900617, 0.5512362136716931, 0.5512362136716931, 0.5512362136716931, 0.12426376930634819, 0.12426376930634819, 0.12426376930634819, 0.08629857039651634, 0.08629857039651634, 0.08629857039651634, 0.11902447288100759, 0.11902447288100759, 0.11902447288100759, 0.3137775647667288, 0.3137775647667288, 0.3137775647667288, 0.33226438604744957, 0.33226438604744957, 0.33226438604744957, 0.3165599166282348, 0.3165599166282348, 0.3165599166282348, 0.2958073498465996, 0.2958073498465996, 0.2958073498465996, 0.2014193254970068, 0.2014193254970068, 0.2014193254970068, 0.31946735232109513, 0.31946735232109513, 0.31946735232109513, 0.19173555705870515, 0.19173555705870515, 0.19173555705870515, 0.27666290811930416, 0.27666290811930416, 0.27666290811930416, 0.13757278806570994, 0.13757278806570994, 0.13757278806570994, 0.2500945792290845, 0.2500945792290845, 0.2500945792290845, 0.2614543772320741, 0.2614543772320741, 0.2614543772320741, 0.2180837733694857, 0.2180837733694857, 0.2180837733694857, 0.22154673072340514, 0.22154673072340514, 0.22154673072340514, 0.21015583038114172, 0.21015583038114172, 0.21015583038114172, 0.23722339776194745, 0.23722339776194745, 0.23722339776194745, 0.8690458747684631, 0.8690458747684631, 0.8690458747684631, 0.16396381590114362, 0.16396381590114362, 0.16396381590114362, 0.8677361492106402, 0.8677361492106402, 0.8677361492106402, 0.8055809598306243, 0.8055809598306243, 0.8055809598306243, 0.2126013466828781, 0.2126013466828781, 0.2126013466828781, 0.15627829901343537, 0.15627829901343537, 0.15627829901343537, 0.2333283154455048, 0.2333283154455048, 0.2333283154455048, 0.19187641040375103, 0.19187641040375103, 0.19187641040375103, 0.24776423421188187, 0.24776423421188187, 0.24776423421188187, 0.12305345428048808, 0.12305345428048808, 0.12305345428048808, 0.09958587402182584, 0.09958587402182584, 0.09958587402182584, 0.10724274256136446, 0.10724274256136446, 0.10724274256136446]}, "mutation_prompt": null}
{"id": "ee5988c2-a57d-4eb6-a6d2-0349db6bda7b", "solution": "import numpy as np\n\nclass PSO_SA_Optimizer_Enhanced_V8:\n    def __init__(self, budget, dim, pop_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        # Adaptive inertia weight range for better exploration versus exploitation\n        self.w_max = 0.9\n        self.w_min = 0.4\n        self.c1_init = 1.4  # Modified cognitive coefficient for enhanced local search\n        self.c2_init = 1.6  # Modified social coefficient for enhanced global search\n        self.temp_init = 1.25  # Slightly increased initial temperature for broader search\n\n    def __call__(self, func):\n        np.random.seed(42)\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        velocities = np.random.uniform(-0.4, 0.4, (self.pop_size, self.dim))  # Slightly adjusted velocity range\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.array([func(pos) for pos in positions])\n        global_best_position = personal_best_positions[np.argmin(personal_best_scores)]\n        global_best_score = np.min(personal_best_scores)\n        \n        eval_count = self.pop_size\n        temperature = self.temp_init\n        \n        while eval_count < self.budget:\n            w = self.w_max - ((self.w_max - self.w_min) * (eval_count / self.budget))\n            c1 = self.c1_init * (1 - eval_count / self.budget) + 0.65\n            c2 = self.c2_init * (eval_count / self.budget) + 0.35\n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                velocities[i] = (w * velocities[i]\n                                 + c1 * r1 * (personal_best_positions[i] - positions[i])\n                                 + c2 * r2 * (global_best_position - positions[i]))\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], self.lower_bound, self.upper_bound)\n\n                score = func(positions[i])\n                eval_count += 1\n                if eval_count >= self.budget:\n                    break\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = positions[i]\n\n                if np.random.rand() < np.exp(-(score - global_best_score) / temperature):\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n                    if score < global_best_score:\n                        global_best_score = score\n                        global_best_position = positions[i]\n\n                if i % 2 == 0:  # Hybrid mutation strategy applied every alternate particle\n                    mutation = np.random.normal(0, 0.1, self.dim)\n                    mutant_position = positions[i] + mutation\n                    mutant_position = np.clip(mutant_position, self.lower_bound, self.upper_bound)\n                    mutant_score = func(mutant_position)\n                    eval_count += 1\n                    if mutant_score < personal_best_scores[i]:\n                        personal_best_scores[i] = mutant_score\n                        personal_best_positions[i] = mutant_position\n                        if mutant_score < global_best_score:\n                            global_best_score = mutant_score\n                            global_best_position = mutant_position\n\n            temperature *= 0.83  # Increased cooling rate for more robust convergence\n\n        return global_best_position, global_best_score", "name": "PSO_SA_Optimizer_Enhanced_V8", "description": "Introduced adaptive dynamic inertia weight and hybrid mutation strategy for enhanced convergence in the PSO-SA optimizer.", "configspace": "", "generation": 93, "fitness": 0.2767015470685409, "feedback": "The algorithm PSO_SA_Optimizer_Enhanced_V8 got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28 with standard deviation 0.24.", "error": "", "parent_id": "4a1c36d2-60e4-4ee3-aa09-0be7a29f5926", "metadata": {"aucs": [0.7196568939347412, 0.7196568939347412, 0.7196568939347412, 0.5599311902250188, 0.5599311902250188, 0.5599311902250188, 0.6885258697710572, 0.6885258697710572, 0.6885258697710572, 0.06176060905084957, 0.06176060905084957, 0.06176060905084957, 0.06753164494843478, 0.06753164494843478, 0.06753164494843478, 0.04987674870671055, 0.04987674870671055, 0.04987674870671055, 0.11092462400919867, 0.11092462400919867, 0.11092462400919867, 0.10490771161829471, 0.10490771161829471, 0.10490771161829471, 0.166920274426263, 0.166920274426263, 0.166920274426263, 0.06941375658344551, 0.06941375658344551, 0.06941375658344551, 0.08843887002593442, 0.08843887002593442, 0.08843887002593442, 0.08222532952768125, 0.08222532952768125, 0.08222532952768125, 0.9551585251164775, 0.9551585251164775, 0.9551585251164775, 0.9725908496768514, 0.9725908496768514, 0.9725908496768514, 0.9692490206142909, 0.9692490206142909, 0.9692490206142909, 0.38276759337618327, 0.38276759337618327, 0.38276759337618327, 0.4110025813534327, 0.4110025813534327, 0.4110025813534327, 0.4450947740867217, 0.4450947740867217, 0.4450947740867217, 0.16597801859343997, 0.16597801859343997, 0.16597801859343997, 0.2088369900488506, 0.2088369900488506, 0.2088369900488506, 0.10733603269514003, 0.10733603269514003, 0.10733603269514003, 0.12480060596509279, 0.12480060596509279, 0.12480060596509279, 0.17967036739238884, 0.17967036739238884, 0.17967036739238884, 0.1758494474543304, 0.1758494474543304, 0.1758494474543304, 0.2009046671430813, 0.2009046671430813, 0.2009046671430813, 0.4263362089617746, 0.4263362089617746, 0.4263362089617746, 0.44638657358072753, 0.44638657358072753, 0.44638657358072753, 0.07190994977708509, 0.07190994977708509, 0.07190994977708509, 0.31166162139956377, 0.31166162139956377, 0.31166162139956377, 0.04765369518728857, 0.04765369518728857, 0.04765369518728857, 0.2766448543791248, 0.2766448543791248, 0.2766448543791248, 0.1094488939599767, 0.1094488939599767, 0.1094488939599767, 0.2426735929104885, 0.2426735929104885, 0.2426735929104885, 0.03478716236582036, 0.03478716236582036, 0.03478716236582036, 0.10440327046132114, 0.10440327046132114, 0.10440327046132114, 0.17005343462948896, 0.17005343462948896, 0.17005343462948896, 0.10505721994904982, 0.10505721994904982, 0.10505721994904982, 0.1392945846953626, 0.1392945846953626, 0.1392945846953626, 0.07722380989122357, 0.07722380989122357, 0.07722380989122357, 0.5656196952769132, 0.5656196952769132, 0.5656196952769132, 0.49502016400461, 0.49502016400461, 0.49502016400461, 0.48285448685187005, 0.48285448685187005, 0.48285448685187005, 0.11222399662934401, 0.11222399662934401, 0.11222399662934401, 0.14696468993248346, 0.14696468993248346, 0.14696468993248346, 0.11402502438470219, 0.11402502438470219, 0.11402502438470219, 0.15557341554997128, 0.15557341554997128, 0.15557341554997128, 0.19910564229432193, 0.19910564229432193, 0.19910564229432193, 0.23011231748224115, 0.23011231748224115, 0.23011231748224115, 0.2687226520807492, 0.2687226520807492, 0.2687226520807492, 0.18727732939273845, 0.18727732939273845, 0.18727732939273845, 0.21680822215177398, 0.21680822215177398, 0.21680822215177398, 0.2986923777277838, 0.2986923777277838, 0.2986923777277838, 0.1849233974111386, 0.1849233974111386, 0.1849233974111386, 0.13897293565659052, 0.13897293565659052, 0.13897293565659052, 0.22855515691980788, 0.22855515691980788, 0.22855515691980788, 0.2337624027074633, 0.2337624027074633, 0.2337624027074633, 0.2101973465273398, 0.2101973465273398, 0.2101973465273398, 0.17992394626698627, 0.17992394626698627, 0.17992394626698627, 0.18067541713027568, 0.18067541713027568, 0.18067541713027568, 0.21383041934798486, 0.21383041934798486, 0.21383041934798486, 0.840030574224674, 0.840030574224674, 0.840030574224674, 0.1630787099781692, 0.1630787099781692, 0.1630787099781692, 0.7106853079406705, 0.7106853079406705, 0.7106853079406705, 0.735006200379855, 0.735006200379855, 0.735006200379855, 0.20790574812038243, 0.20790574812038243, 0.20790574812038243, 0.732373585089185, 0.732373585089185, 0.732373585089185, 0.19100209641304255, 0.19100209641304255, 0.19100209641304255, 0.18967122004861547, 0.18967122004861547, 0.18967122004861547, 0.19927068999722009, 0.19927068999722009, 0.19927068999722009, 0.09505716487660465, 0.09505716487660465, 0.09505716487660465, 0.08883972186251754, 0.08883972186251754, 0.08883972186251754, 0.09286146578471022, 0.09286146578471022, 0.09286146578471022]}, "mutation_prompt": null}
{"id": "a3535ab9-162f-4fa3-955b-e149d9c1c1c2", "solution": "import numpy as np\n\nclass PSO_SA_Optimizer_Enhanced_V9:\n    def __init__(self, budget, dim, pop_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.w_max = 0.85  # Increased max inertia weight for better initial exploration\n        self.w_min = 0.3   # Reduced min inertia weight to enhance convergence\n        self.c1_init = 1.4  # Slightly reduced cognitive coefficient to refine local search\n        self.c2_init = 1.6  # Slightly increased social coefficient for enhanced global search\n        self.temp_init = 1.0  # Lowered initial temperature for quicker stabilization\n\n    def __call__(self, func):\n        np.random.seed(42)\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        velocities = np.random.uniform(-0.6, 0.6, (self.pop_size, self.dim))  # Expanded velocity range for dynamic adaptation\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.array([func(pos) for pos in positions])\n        global_best_position = personal_best_positions[np.argmin(personal_best_scores)]\n        global_best_score = np.min(personal_best_scores)\n        \n        eval_count = self.pop_size\n        temperature = self.temp_init\n\n        while eval_count < self.budget:\n            w = self.w_max - ((self.w_max - self.w_min) * (eval_count / self.budget))\n            c1 = self.c1_init * (1 - eval_count / self.budget) + 0.55\n            c2 = self.c2_init * (eval_count / self.budget) + 0.45\n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                velocities[i] = (w * velocities[i]\n                                 + c1 * r1 * (personal_best_positions[i] - positions[i])\n                                 + c2 * r2 * (global_best_position - positions[i]))\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], self.lower_bound, self.upper_bound)\n\n                score = func(positions[i])\n                eval_count += 1\n                if eval_count >= self.budget:\n                    break\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = positions[i]\n\n                if np.random.rand() < np.exp(-(score - global_best_score) / temperature):\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n                    if score < global_best_score:\n                        global_best_score = score\n                        global_best_position = positions[i]\n\n            temperature *= 0.88  # Modified annealing factor for adaptive temperature decay\n\n        return global_best_position, global_best_score", "name": "PSO_SA_Optimizer_Enhanced_V9", "description": "Enhanced PSO-SA optimizer with refined convergence dynamics and adjusted exploration-exploitation balance.", "configspace": "", "generation": 94, "fitness": 0.28524800114608057, "feedback": "The algorithm PSO_SA_Optimizer_Enhanced_V9 got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.29 with standard deviation 0.25.", "error": "", "parent_id": "4a1c36d2-60e4-4ee3-aa09-0be7a29f5926", "metadata": {"aucs": [0.8411237648715253, 0.8411237648715253, 0.8411237648715253, 0.7990440976774345, 0.7990440976774345, 0.7990440976774345, 0.8081385617243795, 0.8081385617243795, 0.8081385617243795, 0.08554466530371407, 0.08554466530371407, 0.08554466530371407, 0.5060807257228002, 0.5060807257228002, 0.5060807257228002, 0.14564957845676763, 0.14564957845676763, 0.14564957845676763, 0.12259784265240248, 0.12259784265240248, 0.12259784265240248, 0.08668170249637497, 0.08668170249637497, 0.08668170249637497, 0.17588384125467782, 0.17588384125467782, 0.17588384125467782, 0.06426537122642584, 0.06426537122642584, 0.06426537122642584, 0.054035482134958346, 0.054035482134958346, 0.054035482134958346, 0.0583115966065576, 0.0583115966065576, 0.0583115966065576, 0.9806499967886115, 0.9806499967886115, 0.9806499967886115, 0.9841833344933488, 0.9841833344933488, 0.9841833344933488, 0.9846624153776645, 0.9846624153776645, 0.9846624153776645, 0.26157354086131734, 0.26157354086131734, 0.26157354086131734, 0.31074306766550874, 0.31074306766550874, 0.31074306766550874, 0.35535878702391444, 0.35535878702391444, 0.35535878702391444, 0.20927801482241037, 0.20927801482241037, 0.20927801482241037, 0.20692565441253508, 0.20692565441253508, 0.20692565441253508, 0.20075367622007512, 0.20075367622007512, 0.20075367622007512, 0.29776572500645515, 0.29776572500645515, 0.29776572500645515, 0.36455761213020355, 0.36455761213020355, 0.36455761213020355, 0.15377798540212717, 0.15377798540212717, 0.15377798540212717, 0.13219394683123975, 0.13219394683123975, 0.13219394683123975, 0.12964931198595298, 0.12964931198595298, 0.12964931198595298, 0.27263847967410537, 0.27263847967410537, 0.27263847967410537, 0.11630941885537482, 0.11630941885537482, 0.11630941885537482, 0.052447753633122374, 0.052447753633122374, 0.052447753633122374, 0.05347983800290801, 0.05347983800290801, 0.05347983800290801, 0.3718997422715309, 0.3718997422715309, 0.3718997422715309, 0.06586386971206937, 0.06586386971206937, 0.06586386971206937, 0.22484595033014076, 0.22484595033014076, 0.22484595033014076, 0.04629343978023659, 0.04629343978023659, 0.04629343978023659, 0.21168883949817396, 0.21168883949817396, 0.21168883949817396, 0.08845142667068795, 0.08845142667068795, 0.08845142667068795, 0.19831773794169927, 0.19831773794169927, 0.19831773794169927, 0.038268272334283426, 0.038268272334283426, 0.038268272334283426, 0.05447049748830557, 0.05447049748830557, 0.05447049748830557, 0.5151131548850422, 0.5151131548850422, 0.5151131548850422, 0.5656151664818897, 0.5656151664818897, 0.5656151664818897, 0.4358524367744434, 0.4358524367744434, 0.4358524367744434, 0.1079715732784442, 0.1079715732784442, 0.1079715732784442, 0.14033714150786158, 0.14033714150786158, 0.14033714150786158, 0.06900144289598609, 0.06900144289598609, 0.06900144289598609, 0.25226349533374137, 0.25226349533374137, 0.25226349533374137, 0.2784159946126946, 0.2784159946126946, 0.2784159946126946, 0.27494051196572755, 0.27494051196572755, 0.27494051196572755, 0.3562647375236827, 0.3562647375236827, 0.3562647375236827, 0.38367068507792823, 0.38367068507792823, 0.38367068507792823, 0.3958055853501149, 0.3958055853501149, 0.3958055853501149, 0.19410775796713664, 0.19410775796713664, 0.19410775796713664, 0.1715292115503233, 0.1715292115503233, 0.1715292115503233, 0.1271811757624164, 0.1271811757624164, 0.1271811757624164, 0.24172570758695477, 0.24172570758695477, 0.24172570758695477, 0.2591687421029196, 0.2591687421029196, 0.2591687421029196, 0.23929717892791136, 0.23929717892791136, 0.23929717892791136, 0.18615811622229717, 0.18615811622229717, 0.18615811622229717, 0.1773096681182338, 0.1773096681182338, 0.1773096681182338, 0.2466448862393772, 0.2466448862393772, 0.2466448862393772, 0.9116742407215558, 0.9116742407215558, 0.9116742407215558, 0.16354000522661793, 0.16354000522661793, 0.16354000522661793, 0.7619095341197284, 0.7619095341197284, 0.7619095341197284, 0.7034848046858191, 0.7034848046858191, 0.7034848046858191, 0.21046816375125055, 0.21046816375125055, 0.21046816375125055, 0.1562354908665219, 0.1562354908665219, 0.1562354908665219, 0.1952634289030062, 0.1952634289030062, 0.1952634289030062, 0.2104071676586563, 0.2104071676586563, 0.2104071676586563, 0.2146212090237365, 0.2146212090237365, 0.2146212090237365, 0.07975929096103551, 0.07975929096103551, 0.07975929096103551, 0.08853508750749428, 0.08853508750749428, 0.08853508750749428, 0.10917772358523103, 0.10917772358523103, 0.10917772358523103]}, "mutation_prompt": null}
{"id": "ac5657cb-99e0-4dc5-a220-1bc43edcadd9", "solution": "import numpy as np\n\nclass PSO_SA_Optimizer_Enhanced_V8:\n    def __init__(self, budget, dim, pop_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.w_max = 0.85  # Increased max inertia weight for dynamic range adaptation\n        self.w_min = 0.3  # Reduced min inertia weight for agility\n        self.c1_init = 1.4  # Fine-tuned cognitive coefficient\n        self.c2_init = 1.6  # Fine-tuned social coefficient\n        self.temp_init = 1.1  # Slightly reduced initial temperature\n\n    def __call__(self, func):\n        np.random.seed(42)\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        velocities = np.random.uniform(-0.6, 0.6, (self.pop_size, self.dim))  # Adjusted velocity range\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.array([func(pos) for pos in positions])\n        global_best_position = personal_best_positions[np.argmin(personal_best_scores)]\n        global_best_score = np.min(personal_best_scores)\n        \n        eval_count = self.pop_size\n        temperature = self.temp_init\n        \n        while eval_count < self.budget:\n            w = self.w_max - ((self.w_max - self.w_min) * (eval_count / self.budget))\n            c1 = self.c1_init * (1 - eval_count / self.budget) + 0.5\n            c2 = self.c2_init * (eval_count / self.budget) + 0.5\n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                velocities[i] = (w * velocities[i]\n                                 + c1 * r1 * (personal_best_positions[i] - positions[i])\n                                 + c2 * r2 * (global_best_position - positions[i]))\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], self.lower_bound, self.upper_bound)\n\n                score = func(positions[i])\n                eval_count += 1\n                if eval_count >= self.budget:\n                    break\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = positions[i]\n\n                if np.random.rand() < np.exp(-(score - global_best_score) / temperature):\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n                    if score < global_best_score:\n                        global_best_score = score\n                        global_best_position = positions[i]\n\n            temperature *= 0.87  # Modified annealing factor for refined temperature decay\n\n        return global_best_position, global_best_score", "name": "PSO_SA_Optimizer_Enhanced_V8", "description": "Enhanced PSO-SA optimizer with optimized parameter adjustments and more efficient convergence logic to improve performance.", "configspace": "", "generation": 95, "fitness": 0.28335757182581467, "feedback": "The algorithm PSO_SA_Optimizer_Enhanced_V8 got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28 with standard deviation 0.25.", "error": "", "parent_id": "4a1c36d2-60e4-4ee3-aa09-0be7a29f5926", "metadata": {"aucs": [0.8267414810510605, 0.8267414810510605, 0.8267414810510605, 0.8232467493212656, 0.8232467493212656, 0.8232467493212656, 0.8056911959995692, 0.8056911959995692, 0.8056911959995692, 0.038804505315650606, 0.038804505315650606, 0.038804505315650606, 0.1447929726233389, 0.1447929726233389, 0.1447929726233389, 0.21371999251573415, 0.21371999251573415, 0.21371999251573415, 0.15133739666579726, 0.15133739666579726, 0.15133739666579726, 0.10880637248807001, 0.10880637248807001, 0.10880637248807001, 0.15094881790236103, 0.15094881790236103, 0.15094881790236103, 0.10338171447460442, 0.10338171447460442, 0.10338171447460442, 0.10277692186321619, 0.10277692186321619, 0.10277692186321619, 0.0514099214765642, 0.0514099214765642, 0.0514099214765642, 0.980600264982403, 0.980600264982403, 0.980600264982403, 0.9847442640797087, 0.9847442640797087, 0.9847442640797087, 0.9860596153992559, 0.9860596153992559, 0.9860596153992559, 0.43530759915378003, 0.43530759915378003, 0.43530759915378003, 0.3604762876414569, 0.3604762876414569, 0.3604762876414569, 0.406409335508676, 0.406409335508676, 0.406409335508676, 0.3264925079063674, 0.3264925079063674, 0.3264925079063674, 0.19294001891944723, 0.19294001891944723, 0.19294001891944723, 0.14800121544870803, 0.14800121544870803, 0.14800121544870803, 0.46153159215726713, 0.46153159215726713, 0.46153159215726713, 0.41011590700323863, 0.41011590700323863, 0.41011590700323863, 0.27912492644828524, 0.27912492644828524, 0.27912492644828524, 0.24835808470014686, 0.24835808470014686, 0.24835808470014686, 0.1725779596793522, 0.1725779596793522, 0.1725779596793522, 0.14874722675071916, 0.14874722675071916, 0.14874722675071916, 0.07893087559611611, 0.07893087559611611, 0.07893087559611611, 0.05315615877682667, 0.05315615877682667, 0.05315615877682667, 0.13657812007690195, 0.13657812007690195, 0.13657812007690195, 0.15457451980966364, 0.15457451980966364, 0.15457451980966364, 0.15114381873760452, 0.15114381873760452, 0.15114381873760452, 0.19051257368514196, 0.19051257368514196, 0.19051257368514196, 0.05602135989880519, 0.05602135989880519, 0.05602135989880519, 0.24387384026536274, 0.24387384026536274, 0.24387384026536274, 0.20607934004197748, 0.20607934004197748, 0.20607934004197748, 0.08655272700314576, 0.08655272700314576, 0.08655272700314576, 0.0715520635387733, 0.0715520635387733, 0.0715520635387733, 0.12024219138124159, 0.12024219138124159, 0.12024219138124159, 0.5093506948432271, 0.5093506948432271, 0.5093506948432271, 0.6013586113631791, 0.6013586113631791, 0.6013586113631791, 0.4910008916504124, 0.4910008916504124, 0.4910008916504124, 0.11367529478605598, 0.11367529478605598, 0.11367529478605598, 0.10549835822829567, 0.10549835822829567, 0.10549835822829567, 0.10474774732158298, 0.10474774732158298, 0.10474774732158298, 0.19463690087439534, 0.19463690087439534, 0.19463690087439534, 0.20797531441746253, 0.20797531441746253, 0.20797531441746253, 0.18840768320909052, 0.18840768320909052, 0.18840768320909052, 0.24701636711288877, 0.24701636711288877, 0.24701636711288877, 0.2390267890899379, 0.2390267890899379, 0.2390267890899379, 0.2352661126746527, 0.2352661126746527, 0.2352661126746527, 0.22036394251452263, 0.22036394251452263, 0.22036394251452263, 0.14121365039502687, 0.14121365039502687, 0.14121365039502687, 0.12416433693126383, 0.12416433693126383, 0.12416433693126383, 0.18976216137201807, 0.18976216137201807, 0.18976216137201807, 0.1912246381972854, 0.1912246381972854, 0.1912246381972854, 0.26178900267744276, 0.26178900267744276, 0.26178900267744276, 0.20270408992068678, 0.20270408992068678, 0.20270408992068678, 0.178758935681685, 0.178758935681685, 0.178758935681685, 0.19572474226933312, 0.19572474226933312, 0.19572474226933312, 0.8780055909596121, 0.8780055909596121, 0.8780055909596121, 0.1660886410987481, 0.1660886410987481, 0.1660886410987481, 0.8327639911798063, 0.8327639911798063, 0.8327639911798063, 0.37345221563090825, 0.37345221563090825, 0.37345221563090825, 0.21199986648408353, 0.21199986648408353, 0.21199986648408353, 0.4677618008437432, 0.4677618008437432, 0.4677618008437432, 0.18990628995374292, 0.18990628995374292, 0.18990628995374292, 0.21865854379708316, 0.21865854379708316, 0.21865854379708316, 0.22603612744493573, 0.22603612744493573, 0.22603612744493573, 0.092314520470547, 0.092314520470547, 0.092314520470547, 0.0836697300736694, 0.0836697300736694, 0.0836697300736694, 0.10505915170372138, 0.10505915170372138, 0.10505915170372138]}, "mutation_prompt": null}
{"id": "6864d2af-8157-4a55-8b04-1353e1562e9c", "solution": "import numpy as np\n\nclass PSO_SA_Optimizer_Enhanced_V8:\n    def __init__(self, budget, dim, pop_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.w_max = 0.82 \n        self.w_min = 0.30  # Slightly adjusted min inertia weight for better control\n        self.c1_init = 1.6  # Adjusted cognitive coefficient for enhanced exploration\n        self.c2_init = 1.4  # Adjusted social coefficient for refined exploitation\n        self.temp_init = 1.1  # Balanced initial temperature\n\n    def __call__(self, func):\n        np.random.seed(42)\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        velocities = np.random.uniform(-0.5, 0.5, (self.pop_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.array([func(pos) for pos in positions])\n        global_best_position = personal_best_positions[np.argmin(personal_best_scores)]\n        global_best_score = np.min(personal_best_scores)\n        \n        eval_count = self.pop_size\n        temperature = self.temp_init\n        \n        while eval_count < self.budget:\n            w = self.w_max - ((self.w_max - self.w_min) * (eval_count / self.budget))\n            c1 = self.c1_init * (1 - eval_count / self.budget) + 0.55  # Adjusted for dynamic balance\n            c2 = self.c2_init * (eval_count / self.budget) + 0.45  # Adjusted for dynamic balance\n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                velocities[i] = (w * velocities[i]\n                                 + c1 * r1 * (personal_best_positions[i] - positions[i])\n                                 + c2 * r2 * (global_best_position - positions[i]))\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], self.lower_bound, self.upper_bound)\n\n                score = func(positions[i])\n                eval_count += 1\n                if eval_count >= self.budget:\n                    break\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = positions[i]\n\n                if np.random.rand() < np.exp(-(score - global_best_score) / temperature):\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n                    if score < global_best_score:\n                        global_best_score = score\n                        global_best_position = positions[i]\n\n            temperature *= 0.80  # Modified annealing factor for adaptive decay\n\n        return global_best_position, global_best_score", "name": "PSO_SA_Optimizer_Enhanced_V8", "description": "Enhanced PSO-SA optimizer with dynamic parameter adjustment and improved convergence efficiency.", "configspace": "", "generation": 96, "fitness": 0.2983999941149961, "feedback": "The algorithm PSO_SA_Optimizer_Enhanced_V8 got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.30 with standard deviation 0.28.", "error": "", "parent_id": "4a1c36d2-60e4-4ee3-aa09-0be7a29f5926", "metadata": {"aucs": [0.8292904685513602, 0.8292904685513602, 0.8292904685513602, 0.8601188702280853, 0.8601188702280853, 0.8601188702280853, 0.8486764035464702, 0.8486764035464702, 0.8486764035464702, 0.0647272234944205, 0.0647272234944205, 0.0647272234944205, 0.03813560400380689, 0.03813560400380689, 0.03813560400380689, 0.15584882410767897, 0.15584882410767897, 0.15584882410767897, 0.1520749294577055, 0.1520749294577055, 0.1520749294577055, 0.12073681608343834, 0.12073681608343834, 0.12073681608343834, 0.11595585966116073, 0.11595585966116073, 0.11595585966116073, 0.060422147689388805, 0.060422147689388805, 0.060422147689388805, 0.06548143056523092, 0.06548143056523092, 0.06548143056523092, 0.05795287949254546, 0.05795287949254546, 0.05795287949254546, 0.97831655951886, 0.97831655951886, 0.97831655951886, 0.9821249591508395, 0.9821249591508395, 0.9821249591508395, 0.9839068485469114, 0.9839068485469114, 0.9839068485469114, 0.658720128456397, 0.658720128456397, 0.658720128456397, 0.413861813584959, 0.413861813584959, 0.413861813584959, 0.26366259222129995, 0.26366259222129995, 0.26366259222129995, 0.15484335248932435, 0.15484335248932435, 0.15484335248932435, 0.16197054943386757, 0.16197054943386757, 0.16197054943386757, 0.1330278253220356, 0.1330278253220356, 0.1330278253220356, 0.3700048688019233, 0.3700048688019233, 0.3700048688019233, 0.49399868021336124, 0.49399868021336124, 0.49399868021336124, 0.2109227651608646, 0.2109227651608646, 0.2109227651608646, 0.13017417046564383, 0.13017417046564383, 0.13017417046564383, 0.14380584066194002, 0.14380584066194002, 0.14380584066194002, 0.21267487599544443, 0.21267487599544443, 0.21267487599544443, 0.09083680854164411, 0.09083680854164411, 0.09083680854164411, 0.04433583033797983, 0.04433583033797983, 0.04433583033797983, 0.09052364784125966, 0.09052364784125966, 0.09052364784125966, 0.17067179644757813, 0.17067179644757813, 0.17067179644757813, 0.08545017158076984, 0.08545017158076984, 0.08545017158076984, 0.21889139895438825, 0.21889139895438825, 0.21889139895438825, 0.04443873151188049, 0.04443873151188049, 0.04443873151188049, 0.2694533320006893, 0.2694533320006893, 0.2694533320006893, 0.10060425235927106, 0.10060425235927106, 0.10060425235927106, 0.20879198689562206, 0.20879198689562206, 0.20879198689562206, 0.045463401517474855, 0.045463401517474855, 0.045463401517474855, 0.2583091577880816, 0.2583091577880816, 0.2583091577880816, 0.6367492252645629, 0.6367492252645629, 0.6367492252645629, 0.6390427059720792, 0.6390427059720792, 0.6390427059720792, 0.7211846611525994, 0.7211846611525994, 0.7211846611525994, 0.111061745947029, 0.111061745947029, 0.111061745947029, 0.12072060577773502, 0.12072060577773502, 0.12072060577773502, 0.09750406096146158, 0.09750406096146158, 0.09750406096146158, 0.1825250520016639, 0.1825250520016639, 0.1825250520016639, 0.23583219481771622, 0.23583219481771622, 0.23583219481771622, 0.1721722371153921, 0.1721722371153921, 0.1721722371153921, 0.2817007714215789, 0.2817007714215789, 0.2817007714215789, 0.18320793442314576, 0.18320793442314576, 0.18320793442314576, 0.26566380384578914, 0.26566380384578914, 0.26566380384578914, 0.21541331259674512, 0.21541331259674512, 0.21541331259674512, 0.14637491589628515, 0.14637491589628515, 0.14637491589628515, 0.1386318562448241, 0.1386318562448241, 0.1386318562448241, 0.22456396032862658, 0.22456396032862658, 0.22456396032862658, 0.2284340699716375, 0.2284340699716375, 0.2284340699716375, 0.20162890861131832, 0.20162890861131832, 0.20162890861131832, 0.19767353424420986, 0.19767353424420986, 0.19767353424420986, 0.2012827501645601, 0.2012827501645601, 0.2012827501645601, 0.2507232023778194, 0.2507232023778194, 0.2507232023778194, 0.9199993493357268, 0.9199993493357268, 0.9199993493357268, 0.7764296430825262, 0.7764296430825262, 0.7764296430825262, 0.8772643233142503, 0.8772643233142503, 0.8772643233142503, 0.8200069599287754, 0.8200069599287754, 0.8200069599287754, 0.32780720485955595, 0.32780720485955595, 0.32780720485955595, 0.15585394604178981, 0.15585394604178981, 0.15585394604178981, 0.20487134143555363, 0.20487134143555363, 0.20487134143555363, 0.19651224387394262, 0.19651224387394262, 0.19651224387394262, 0.1987905620385676, 0.1987905620385676, 0.1987905620385676, 0.07544723458652336, 0.07544723458652336, 0.07544723458652336, 0.10444084114576069, 0.10444084114576069, 0.10444084114576069, 0.08607661481836926, 0.08607661481836926, 0.08607661481836926]}, "mutation_prompt": null}
{"id": "bf176e45-f6e5-46ae-9d27-6a761562d728", "solution": "import numpy as np\n\nclass PSO_SA_Optimizer_Enhanced_V8:\n    def __init__(self, budget, dim, pop_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.w_max = 0.84  # Slightly increased max inertia weight for better global search\n        self.w_min = 0.34  # Slightly increased min inertia weight for improved stability\n        self.c1_init = 1.6  # Fine-tuned cognitive coefficient for enhanced exploration\n        self.c2_init = 1.4  # Fine-tuned social coefficient for refined exploitation\n        self.temp_init = 1.25  # Altered initial temperature for better balance\n\n    def __call__(self, func):\n        np.random.seed(42)\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        velocities = np.random.uniform(-0.3, 0.3, (self.pop_size, self.dim))  # Narrowed velocity range\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.array([func(pos) for pos in positions])\n        global_best_position = personal_best_positions[np.argmin(personal_best_scores)]\n        global_best_score = np.min(personal_best_scores)\n        \n        eval_count = self.pop_size\n        temperature = self.temp_init\n        \n        while eval_count < self.budget:\n            w = self.w_max - ((self.w_max - self.w_min) * (eval_count / self.budget))\n            c1 = self.c1_init * (1 - eval_count / self.budget) + 0.62\n            c2 = self.c2_init * (eval_count / self.budget) + 0.38\n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                velocities[i] = (w * velocities[i]\n                                 + c1 * r1 * (personal_best_positions[i] - positions[i])\n                                 + c2 * r2 * (global_best_position - positions[i]))\n                velocities[i] = np.clip(velocities[i], -1.0, 1.0)  # Clamped velocities for stability\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], self.lower_bound, self.upper_bound)\n\n                score = func(positions[i])\n                eval_count += 1\n                if eval_count >= self.budget:\n                    break\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = positions[i]\n\n                if np.random.rand() < np.exp(-(score - global_best_score) / temperature):\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n                    if score < global_best_score:\n                        global_best_score = score\n                        global_best_position = positions[i]\n\n            temperature *= 0.87  # Slightly adjusted annealing factor for adaptive decay\n\n        return global_best_position, global_best_score", "name": "PSO_SA_Optimizer_Enhanced_V8", "description": "Enhanced PSO-SA optimizer with improved velocity clamping and adaptive coefficients for better convergence efficiency.", "configspace": "", "generation": 97, "fitness": 0.2875205823458988, "feedback": "The algorithm PSO_SA_Optimizer_Enhanced_V8 got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.29 with standard deviation 0.25.", "error": "", "parent_id": "4a1c36d2-60e4-4ee3-aa09-0be7a29f5926", "metadata": {"aucs": [0.8454985482424793, 0.8454985482424793, 0.8454985482424793, 0.807405253252026, 0.807405253252026, 0.807405253252026, 0.8481889704846317, 0.8481889704846317, 0.8481889704846317, 0.1028050111817489, 0.1028050111817489, 0.1028050111817489, 0.15779335282137186, 0.15779335282137186, 0.15779335282137186, 0.29107826285652405, 0.29107826285652405, 0.29107826285652405, 0.18190740070336242, 0.18190740070336242, 0.18190740070336242, 0.13276892329621037, 0.13276892329621037, 0.13276892329621037, 0.09104764534608734, 0.09104764534608734, 0.09104764534608734, 0.08020746455056671, 0.08020746455056671, 0.08020746455056671, 0.06285407668207577, 0.06285407668207577, 0.06285407668207577, 0.09287355090133365, 0.09287355090133365, 0.09287355090133365, 0.9557796521012841, 0.9557796521012841, 0.9557796521012841, 0.9680007097346404, 0.9680007097346404, 0.9680007097346404, 0.9557133576046704, 0.9557133576046704, 0.9557133576046704, 0.48562086562314755, 0.48562086562314755, 0.48562086562314755, 0.41481034937455374, 0.41481034937455374, 0.41481034937455374, 0.40322875886662746, 0.40322875886662746, 0.40322875886662746, 0.13903168190133652, 0.13903168190133652, 0.13903168190133652, 0.19353293494593882, 0.19353293494593882, 0.19353293494593882, 0.17118772962845852, 0.17118772962845852, 0.17118772962845852, 0.2970429237370592, 0.2970429237370592, 0.2970429237370592, 0.19563653128709413, 0.19563653128709413, 0.19563653128709413, 0.19210945042447902, 0.19210945042447902, 0.19210945042447902, 0.134010438238232, 0.134010438238232, 0.134010438238232, 0.1684580087562053, 0.1684580087562053, 0.1684580087562053, 0.2083199282813869, 0.2083199282813869, 0.2083199282813869, 0.04984878884981703, 0.04984878884981703, 0.04984878884981703, 0.034000007625808015, 0.034000007625808015, 0.034000007625808015, 0.16363370428379453, 0.16363370428379453, 0.16363370428379453, 0.22568829588853334, 0.22568829588853334, 0.22568829588853334, 0.1071997684481053, 0.1071997684481053, 0.1071997684481053, 0.15384145000070626, 0.15384145000070626, 0.15384145000070626, 0.06375857361105608, 0.06375857361105608, 0.06375857361105608, 0.18504269261827488, 0.18504269261827488, 0.18504269261827488, 0.18735368727421187, 0.18735368727421187, 0.18735368727421187, 0.14530458458771844, 0.14530458458771844, 0.14530458458771844, 0.07080710270573687, 0.07080710270573687, 0.07080710270573687, 0.15231362447742158, 0.15231362447742158, 0.15231362447742158, 0.6775645060016205, 0.6775645060016205, 0.6775645060016205, 0.7483561240918032, 0.7483561240918032, 0.7483561240918032, 0.5537962932448013, 0.5537962932448013, 0.5537962932448013, 0.1463481535522032, 0.1463481535522032, 0.1463481535522032, 0.07213688893171843, 0.07213688893171843, 0.07213688893171843, 0.1120505654295062, 0.1120505654295062, 0.1120505654295062, 0.24052430937098912, 0.24052430937098912, 0.24052430937098912, 0.2227018879094771, 0.2227018879094771, 0.2227018879094771, 0.2170471494689732, 0.2170471494689732, 0.2170471494689732, 0.32375584101075205, 0.32375584101075205, 0.32375584101075205, 0.3275809131282158, 0.3275809131282158, 0.3275809131282158, 0.26446445150375475, 0.26446445150375475, 0.26446445150375475, 0.2155845275688506, 0.2155845275688506, 0.2155845275688506, 0.19667191727346922, 0.19667191727346922, 0.19667191727346922, 0.18721912494294124, 0.18721912494294124, 0.18721912494294124, 0.22936635094367197, 0.22936635094367197, 0.22936635094367197, 0.23059495113157402, 0.23059495113157402, 0.23059495113157402, 0.23032813818966147, 0.23032813818966147, 0.23032813818966147, 0.19598470254232547, 0.19598470254232547, 0.19598470254232547, 0.1783625140371634, 0.1783625140371634, 0.1783625140371634, 0.24942048288871022, 0.24942048288871022, 0.24942048288871022, 0.7721238906874407, 0.7721238906874407, 0.7721238906874407, 0.16444216824088453, 0.16444216824088453, 0.16444216824088453, 0.7608151133862119, 0.7608151133862119, 0.7608151133862119, 0.828580702946647, 0.828580702946647, 0.828580702946647, 0.2124450490065859, 0.2124450490065859, 0.2124450490065859, 0.15579431026132096, 0.15579431026132096, 0.15579431026132096, 0.1933123356704708, 0.1933123356704708, 0.1933123356704708, 0.1835825829857023, 0.1835825829857023, 0.1835825829857023, 0.17891651737870673, 0.17891651737870673, 0.17891651737870673, 0.13185840002284588, 0.13185840002284588, 0.13185840002284588, 0.08378358992394164, 0.08378358992394164, 0.08378358992394164, 0.1002634140370523, 0.1002634140370523, 0.1002634140370523]}, "mutation_prompt": null}
{"id": "bdfbf777-9d2a-4906-a7a5-8b92fc9e6172", "solution": "import numpy as np\n\nclass PSO_SA_Optimizer_Enhanced_V7:\n    def __init__(self, budget, dim, pop_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.w_max = 0.82  # Modified max inertia weight for dynamic range adaptation\n        self.w_min = 0.32  # Modified min inertia weight for dynamic range adaptation\n        self.c1_init = 1.5  # Adjusted cognitive coefficient for improved exploration\n        self.c2_init = 1.5  # Adjusted social coefficient for improved exploitation\n        self.temp_init = 1.2  # Altered initial temperature for balanced exploration\n\n    def __call__(self, func):\n        np.random.seed(42)\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        velocities = np.random.uniform(-0.5, 0.5, (self.pop_size, self.dim))  # Updated velocity range\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.array([func(pos) for pos in positions])\n        global_best_position = personal_best_positions[np.argmin(personal_best_scores)]\n        global_best_score = np.min(personal_best_scores)\n        \n        eval_count = self.pop_size\n        temperature = self.temp_init\n        \n        while eval_count < self.budget:\n            w = self.w_max - ((self.w_max - self.w_min) * (eval_count / self.budget))\n            c1 = self.c1_init * (1 - eval_count / self.budget) + 0.60\n            c2 = self.c2_init * (eval_count / self.budget) + 0.40\n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                velocities[i] = (w * velocities[i]\n                                 + c1 * r1 * (personal_best_positions[i] - positions[i])\n                                 + c2 * r2 * (global_best_position - positions[i]))\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], self.lower_bound, self.upper_bound)\n\n                score = func(positions[i])\n                eval_count += 1\n                if eval_count >= self.budget:\n                    break\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = positions[i]\n\n                if np.random.rand() < np.exp(-(score - global_best_score) / temperature):\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n                    if score < global_best_score:\n                        global_best_score = score\n                        global_best_position = positions[i]\n\n            temperature *= 0.85  # Adjusted annealing factor for adaptive temperature decay\n\n        return global_best_position, global_best_score", "name": "PSO_SA_Optimizer_Enhanced_V7", "description": "Modified PSO-SA optimizer with enhanced velocity updates and adaptive temperature decay for better convergence.", "configspace": "", "generation": 84, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "4a1c36d2-60e4-4ee3-aa09-0be7a29f5926", "metadata": {"aucs": [0.8581616288372897, 0.8581616288372897, 0.8581616288372897, 0.8558714027376966, 0.8558714027376966, 0.8558714027376966, 0.7938251841679739, 0.7938251841679739, 0.7938251841679739, 0.3161202161339587, 0.3161202161339587, 0.3161202161339587, 0.06781517759564903, 0.06781517759564903, 0.06781517759564903, 0.014674078273694935, 0.014674078273694935, 0.014674078273694935, 0.13534141194260396, 0.13534141194260396, 0.13534141194260396, 0.1346952780148467, 0.1346952780148467, 0.1346952780148467, 0.08737846242073133, 0.08737846242073133, 0.08737846242073133, 0.06913931950237762, 0.06913931950237762, 0.06913931950237762, 0.0802238206955842, 0.0802238206955842, 0.0802238206955842, 0.08187638999884927, 0.08187638999884927, 0.08187638999884927, 0.9775854884291599, 0.9775854884291599, 0.9775854884291599, 0.9814432983398529, 0.9814432983398529, 0.9814432983398529, 0.9820766568259338, 0.9820766568259338, 0.9820766568259338, 0.3219163645728216, 0.3219163645728216, 0.3219163645728216, 0.5376093931739452, 0.5376093931739452, 0.5376093931739452, 0.4086382058705924, 0.4086382058705924, 0.4086382058705924, 0.3341613724608191, 0.3341613724608191, 0.3341613724608191, 0.19254042327402066, 0.19254042327402066, 0.19254042327402066, 0.10769246461163984, 0.10769246461163984, 0.10769246461163984, 0.30291399812535824, 0.30291399812535824, 0.30291399812535824, 0.13175204201584623, 0.13175204201584623, 0.13175204201584623, 0.18697593911698385, 0.18697593911698385, 0.18697593911698385, 0.17215389195163877, 0.17215389195163877, 0.17215389195163877, 0.1312400708228495, 0.1312400708228495, 0.1312400708228495, 0.30970133430230806, 0.30970133430230806, 0.30970133430230806, 0.06489576983035727, 0.06489576983035727, 0.06489576983035727, 0.06072198091023551, 0.06072198091023551, 0.06072198091023551, 0.11058876151883612, 0.11058876151883612, 0.11058876151883612, 0.3208991020955013, 0.3208991020955013, 0.3208991020955013, 0.18026217495344776, 0.18026217495344776, 0.18026217495344776, 0.15947911237868428, 0.15947911237868428, 0.15947911237868428, 0.047424561884309546, 0.047424561884309546, 0.047424561884309546, 0.17707103040385375, 0.17707103040385375, 0.17707103040385375, 0.33301922037993636, 0.33301922037993636, 0.33301922037993636, 0.27848996735799103, 0.27848996735799103, 0.27848996735799103, 0.03857688776182744, 0.03857688776182744, 0.03857688776182744, 0.16387507940912638, 0.16387507940912638, 0.16387507940912638, 0.7989952482098132, 0.7989952482098132, 0.7989952482098132, 0.5430082833900617, 0.5430082833900617, 0.5430082833900617, 0.5512362136716931, 0.5512362136716931, 0.5512362136716931, 0.12426376930634819, 0.12426376930634819, 0.12426376930634819, 0.08629857039651634, 0.08629857039651634, 0.08629857039651634, 0.11902447288100759, 0.11902447288100759, 0.11902447288100759, 0.3137775647667288, 0.3137775647667288, 0.3137775647667288, 0.33226438604744957, 0.33226438604744957, 0.33226438604744957, 0.3165599166282348, 0.3165599166282348, 0.3165599166282348, 0.2958073498465996, 0.2958073498465996, 0.2958073498465996, 0.2014193254970068, 0.2014193254970068, 0.2014193254970068, 0.31946735232109513, 0.31946735232109513, 0.31946735232109513, 0.19173555705870515, 0.19173555705870515, 0.19173555705870515, 0.27666290811930416, 0.27666290811930416, 0.27666290811930416, 0.13757278806570994, 0.13757278806570994, 0.13757278806570994, 0.2500945792290845, 0.2500945792290845, 0.2500945792290845, 0.2614543772320741, 0.2614543772320741, 0.2614543772320741, 0.2180837733694857, 0.2180837733694857, 0.2180837733694857, 0.22154673072340514, 0.22154673072340514, 0.22154673072340514, 0.21015583038114172, 0.21015583038114172, 0.21015583038114172, 0.23722339776194745, 0.23722339776194745, 0.23722339776194745, 0.8690458747684631, 0.8690458747684631, 0.8690458747684631, 0.16396381590114362, 0.16396381590114362, 0.16396381590114362, 0.8677361492106402, 0.8677361492106402, 0.8677361492106402, 0.8055809598306243, 0.8055809598306243, 0.8055809598306243, 0.2126013466828781, 0.2126013466828781, 0.2126013466828781, 0.15627829901343537, 0.15627829901343537, 0.15627829901343537, 0.2333283154455048, 0.2333283154455048, 0.2333283154455048, 0.19187641040375103, 0.19187641040375103, 0.19187641040375103, 0.24776423421188187, 0.24776423421188187, 0.24776423421188187, 0.12305345428048808, 0.12305345428048808, 0.12305345428048808, 0.09958587402182584, 0.09958587402182584, 0.09958587402182584, 0.10724274256136446, 0.10724274256136446, 0.10724274256136446]}, "mutation_prompt": null}
{"id": "ea87d044-eb06-4759-8d85-3e39ba95c9b0", "solution": "import numpy as np\n\nclass PSO_SA_Optimizer_Enhanced_V7:\n    def __init__(self, budget, dim, pop_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.w_max = 0.82  # Modified max inertia weight for dynamic range adaptation\n        self.w_min = 0.32  # Modified min inertia weight for dynamic range adaptation\n        self.c1_init = 1.5  # Adjusted cognitive coefficient for improved exploration\n        self.c2_init = 1.5  # Adjusted social coefficient for improved exploitation\n        self.temp_init = 1.2  # Altered initial temperature for balanced exploration\n\n    def __call__(self, func):\n        np.random.seed(42)\n        positions = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        velocities = np.random.uniform(-0.5, 0.5, (self.pop_size, self.dim))  # Updated velocity range\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.array([func(pos) for pos in positions])\n        global_best_position = personal_best_positions[np.argmin(personal_best_scores)]\n        global_best_score = np.min(personal_best_scores)\n        \n        eval_count = self.pop_size\n        temperature = self.temp_init\n        \n        while eval_count < self.budget:\n            w = self.w_max - ((self.w_max - self.w_min) * (eval_count / self.budget))\n            c1 = self.c1_init * (1 - eval_count / self.budget) + 0.60\n            c2 = self.c2_init * (eval_count / self.budget) + 0.40\n            for i in range(self.pop_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                velocities[i] = (w * velocities[i]\n                                 + c1 * r1 * (personal_best_positions[i] - positions[i])\n                                 + c2 * r2 * (global_best_position - positions[i]))\n                positions[i] += velocities[i]\n                positions[i] = np.clip(positions[i], self.lower_bound, self.upper_bound)\n\n                score = func(positions[i])\n                eval_count += 1\n                if eval_count >= self.budget:\n                    break\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = positions[i]\n\n                if np.random.rand() < np.exp(-(score - global_best_score) / temperature):\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n                    if score < global_best_score:\n                        global_best_score = score\n                        global_best_position = positions[i]\n\n            temperature *= 0.85  # Adjusted annealing factor for adaptive temperature decay\n\n        return global_best_position, global_best_score", "name": "PSO_SA_Optimizer_Enhanced_V7", "description": "Modified PSO-SA optimizer with enhanced velocity updates and adaptive temperature decay for better convergence.", "configspace": "", "generation": 84, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "4a1c36d2-60e4-4ee3-aa09-0be7a29f5926", "metadata": {"aucs": [0.8581616288372897, 0.8581616288372897, 0.8581616288372897, 0.8558714027376966, 0.8558714027376966, 0.8558714027376966, 0.7938251841679739, 0.7938251841679739, 0.7938251841679739, 0.3161202161339587, 0.3161202161339587, 0.3161202161339587, 0.06781517759564903, 0.06781517759564903, 0.06781517759564903, 0.014674078273694935, 0.014674078273694935, 0.014674078273694935, 0.13534141194260396, 0.13534141194260396, 0.13534141194260396, 0.1346952780148467, 0.1346952780148467, 0.1346952780148467, 0.08737846242073133, 0.08737846242073133, 0.08737846242073133, 0.06913931950237762, 0.06913931950237762, 0.06913931950237762, 0.0802238206955842, 0.0802238206955842, 0.0802238206955842, 0.08187638999884927, 0.08187638999884927, 0.08187638999884927, 0.9775854884291599, 0.9775854884291599, 0.9775854884291599, 0.9814432983398529, 0.9814432983398529, 0.9814432983398529, 0.9820766568259338, 0.9820766568259338, 0.9820766568259338, 0.3219163645728216, 0.3219163645728216, 0.3219163645728216, 0.5376093931739452, 0.5376093931739452, 0.5376093931739452, 0.4086382058705924, 0.4086382058705924, 0.4086382058705924, 0.3341613724608191, 0.3341613724608191, 0.3341613724608191, 0.19254042327402066, 0.19254042327402066, 0.19254042327402066, 0.10769246461163984, 0.10769246461163984, 0.10769246461163984, 0.30291399812535824, 0.30291399812535824, 0.30291399812535824, 0.13175204201584623, 0.13175204201584623, 0.13175204201584623, 0.18697593911698385, 0.18697593911698385, 0.18697593911698385, 0.17215389195163877, 0.17215389195163877, 0.17215389195163877, 0.1312400708228495, 0.1312400708228495, 0.1312400708228495, 0.30970133430230806, 0.30970133430230806, 0.30970133430230806, 0.06489576983035727, 0.06489576983035727, 0.06489576983035727, 0.06072198091023551, 0.06072198091023551, 0.06072198091023551, 0.11058876151883612, 0.11058876151883612, 0.11058876151883612, 0.3208991020955013, 0.3208991020955013, 0.3208991020955013, 0.18026217495344776, 0.18026217495344776, 0.18026217495344776, 0.15947911237868428, 0.15947911237868428, 0.15947911237868428, 0.047424561884309546, 0.047424561884309546, 0.047424561884309546, 0.17707103040385375, 0.17707103040385375, 0.17707103040385375, 0.33301922037993636, 0.33301922037993636, 0.33301922037993636, 0.27848996735799103, 0.27848996735799103, 0.27848996735799103, 0.03857688776182744, 0.03857688776182744, 0.03857688776182744, 0.16387507940912638, 0.16387507940912638, 0.16387507940912638, 0.7989952482098132, 0.7989952482098132, 0.7989952482098132, 0.5430082833900617, 0.5430082833900617, 0.5430082833900617, 0.5512362136716931, 0.5512362136716931, 0.5512362136716931, 0.12426376930634819, 0.12426376930634819, 0.12426376930634819, 0.08629857039651634, 0.08629857039651634, 0.08629857039651634, 0.11902447288100759, 0.11902447288100759, 0.11902447288100759, 0.3137775647667288, 0.3137775647667288, 0.3137775647667288, 0.33226438604744957, 0.33226438604744957, 0.33226438604744957, 0.3165599166282348, 0.3165599166282348, 0.3165599166282348, 0.2958073498465996, 0.2958073498465996, 0.2958073498465996, 0.2014193254970068, 0.2014193254970068, 0.2014193254970068, 0.31946735232109513, 0.31946735232109513, 0.31946735232109513, 0.19173555705870515, 0.19173555705870515, 0.19173555705870515, 0.27666290811930416, 0.27666290811930416, 0.27666290811930416, 0.13757278806570994, 0.13757278806570994, 0.13757278806570994, 0.2500945792290845, 0.2500945792290845, 0.2500945792290845, 0.2614543772320741, 0.2614543772320741, 0.2614543772320741, 0.2180837733694857, 0.2180837733694857, 0.2180837733694857, 0.22154673072340514, 0.22154673072340514, 0.22154673072340514, 0.21015583038114172, 0.21015583038114172, 0.21015583038114172, 0.23722339776194745, 0.23722339776194745, 0.23722339776194745, 0.8690458747684631, 0.8690458747684631, 0.8690458747684631, 0.16396381590114362, 0.16396381590114362, 0.16396381590114362, 0.8677361492106402, 0.8677361492106402, 0.8677361492106402, 0.8055809598306243, 0.8055809598306243, 0.8055809598306243, 0.2126013466828781, 0.2126013466828781, 0.2126013466828781, 0.15627829901343537, 0.15627829901343537, 0.15627829901343537, 0.2333283154455048, 0.2333283154455048, 0.2333283154455048, 0.19187641040375103, 0.19187641040375103, 0.19187641040375103, 0.24776423421188187, 0.24776423421188187, 0.24776423421188187, 0.12305345428048808, 0.12305345428048808, 0.12305345428048808, 0.09958587402182584, 0.09958587402182584, 0.09958587402182584, 0.10724274256136446, 0.10724274256136446, 0.10724274256136446]}, "mutation_prompt": null}
