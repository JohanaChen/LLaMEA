{"id": "cc35849e-c096-4674-a984-3fc5a6ef4f3f", "solution": "import numpy as np\n\nclass AdaptiveGradientStochasticSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.evaluations = 0\n        self.learning_rate = 0.1\n        self.sigma = 0.1\n\n    def __call__(self, func):\n        x = np.random.uniform(self.lower_bound, self.upper_bound, self.dim)\n        best_x = x.copy()\n        best_f = func(best_x)\n        self.evaluations += 1\n\n        while self.evaluations < self.budget:\n            perturbation = np.random.normal(0, self.sigma, self.dim)\n            grad_approx = (func(x + perturbation) - func(x - perturbation)) / (2 * self.sigma)\n            self.evaluations += 2\n\n            x = x - self.learning_rate * grad_approx + perturbation\n            x = np.clip(x, self.lower_bound, self.upper_bound)\n\n            current_f = func(x)\n            self.evaluations += 1\n\n            if current_f < best_f:\n                best_f = current_f\n                best_x = x.copy()\n                self.learning_rate *= 1.2  # Adaptive increase\n                self.sigma *= 0.9  # Reduce perturbation to focus on exploitation\n            else:\n                self.learning_rate *= 0.8  # Adaptive decrease\n                self.sigma *= 1.1  # Increase perturbation for exploration\n\n        return best_x", "name": "AdaptiveGradientStochasticSearch", "description": "Gradient-Based Stochastic Search with Adaptive Step Size that combines gradient approximation and stochasticity for effective exploration and exploitation.", "configspace": "", "generation": 0, "fitness": 0.0839138884658624, "feedback": "The algorithm AdaptiveGradientStochasticSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.08 with standard deviation 0.19.", "error": "", "parent_id": null, "metadata": {"aucs": [0.08998073195420075, 0.06271675336939186, 0.08875131807856451, 0.10494277406268049, 0.09280018916089827, 0.09112693834094732, 0.1288638426464599, 0.06554447394544227, 0.07105907849757953, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.020477991420123853, 0.023304540353572656, 0.013580939962702043, 0.017274518788866433, 0.0, 0.008265519422535506, 0.0, 0.0, 0.0, 0.015226161305268215, 0.032874133507481695, 0.020434179529922436, 0.0, 0.011947285404927621, 0.0, 0.0, 0.0073079661356707515, 0.0, 0.9729019800082144, 0.9764282517738041, 0.9773327967574621, 0.9757235476326691, 0.9719199919945001, 0.9620231047078028, 0.9792004787568362, 0.973372573988543, 0.9755793587141087, 0.0, 0.0, 0.0, 0.001989524435141665, 0.0018281365899111002, 0.031124060486155014, 0.0, 0.0, 0.0, 0.11044192282611387, 0.10249699148985147, 0.11860360271926362, 0.017521515422513456, 0.00656962183887122, 9.999999999998899e-05, 0.062372258261358504, 9.999999999998899e-05, 0.0912849334670145, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.011752715529753877, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.1396112167348632, 0.12421783769642902, 0.14103244338215903, 0.10679315147984614, 0.1387916296901911, 0.09817732293631654, 0.12719501314661374, 0.09949075526341022, 0.11214895788266566, 0.014585865083758143, 0.009405807533463983, 0.053335511298185145, 0.0, 0.028138384732586563, 0.0, 0.0056031486469186875, 0.018761363751345517, 0.027938057908791225, 0.09289497683315007, 0.09288553633962471, 0.09298882555874755, 0.07622884985070266, 0.10339107993553331, 0.08929786078921786, 0.07852845632663452, 0.07276879660865687, 0.07677067817942385, 0.12068908612179152, 0.10742701554755874, 0.10705128967548738, 0.14344059472788317, 0.14783206959608686, 0.10784990049375187, 0.1273631117346915, 0.11922011318374748, 0.11351244162379848, 0.06656603129920147, 0.053679095474200356, 0.026691483647965253, 0.07476970549154738, 0.05594480916872069, 0.08278562995246708, 0.10285500844476925, 0.07035397692981282, 0.08764196550452963, 0.1354863540745328, 0.14252980027797435, 0.12140354094353056, 0.13547200639727486, 0.11588559432047074, 0.12475844993085139, 0.16131558609958807, 0.11883885863818211, 0.09484446786087752, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.00976667744786186, 0.14665717701174052, 0.0960797177461633, 0.0761239326581048, 0.07565325239499676, 0.05461947817712576, 0.06358383434806059, 0.08353689881996818, 0.05205108316764451, 0.07554536700328207, 0.0649850536817903, 0.02906392249683154, 0.05494107698589201, 0.018952763790558103, 0.08564980497969954, 0.10695440010782897, 0.022810256467945655, 0.0412131739677174, 0.06664696591121322, 0.09480636265193276, 0.146359147326369, 0.13533987883392762, 0.1632806877877092, 0.14996302121391492, 0.1768225021822749, 0.1716706583603329, 0.1529302770009463, 0.16372988774197073, 0.13784775839362684, 0.02942909441598729, 0.018485765204041993, 0.02215219582537198, 0.016613864082591134, 0.040229959076666044, 0.02982252856207468, 0.017234386334394336, 0.025772125113338595, 0.03213875331672589]}, "mutation_prompt": null}
{"id": "b7121056-e416-4259-a5ca-86a811a0655f", "solution": "import numpy as np\n\nclass AdaptiveGradientStochasticSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.evaluations = 0\n        self.learning_rate = 0.1\n        self.sigma = 0.1\n\n    def __call__(self, func):\n        x = np.random.uniform(self.lower_bound, self.upper_bound, self.dim)\n        best_x = x.copy()\n        best_f = func(best_x)\n        self.evaluations += 1\n\n        while self.evaluations < self.budget:\n            perturbation = np.random.normal(0, self.sigma, self.dim)\n            grad_approx = (func(x + perturbation) - func(x - perturbation)) / (2 * self.sigma)\n            self.evaluations += 2\n\n            x = x - self.learning_rate * grad_approx + perturbation\n            x = np.clip(x, self.lower_bound, self.upper_bound)\n\n            current_f = func(x)\n            self.evaluations += 1\n\n            if current_f < best_f:\n                best_f = current_f\n                best_x = x.copy()\n                self.learning_rate *= 1.2  # Adaptive increase\n                self.sigma *= 0.9  # Reduce perturbation to focus on exploitation\n            else:\n                self.learning_rate *= 0.8  # Adaptive decrease\n                self.sigma *= 1.1  # Increase perturbation for exploration\n\n        return best_x", "name": "AdaptiveGradientStochasticSearch", "description": "Gradient-Based Stochastic Search with Adaptive Step Size that combines gradient approximation and stochasticity for effective exploration and exploitation.", "configspace": "", "generation": 1, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "cc35849e-c096-4674-a984-3fc5a6ef4f3f", "metadata": {"aucs": [0.08998073195420075, 0.06271675336939186, 0.08875131807856451, 0.10494277406268049, 0.09280018916089827, 0.09112693834094732, 0.1288638426464599, 0.06554447394544227, 0.07105907849757953, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.020477991420123853, 0.023304540353572656, 0.013580939962702043, 0.017274518788866433, 0.0, 0.008265519422535506, 0.0, 0.0, 0.0, 0.015226161305268215, 0.032874133507481695, 0.020434179529922436, 0.0, 0.011947285404927621, 0.0, 0.0, 0.0073079661356707515, 0.0, 0.9729019800082144, 0.9764282517738041, 0.9773327967574621, 0.9757235476326691, 0.9719199919945001, 0.9620231047078028, 0.9792004787568362, 0.973372573988543, 0.9755793587141087, 0.0, 0.0, 0.0, 0.001989524435141665, 0.0018281365899111002, 0.031124060486155014, 0.0, 0.0, 0.0, 0.11044192282611387, 0.10249699148985147, 0.11860360271926362, 0.017521515422513456, 0.00656962183887122, 9.999999999998899e-05, 0.062372258261358504, 9.999999999998899e-05, 0.0912849334670145, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.011752715529753877, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.1396112167348632, 0.12421783769642902, 0.14103244338215903, 0.10679315147984614, 0.1387916296901911, 0.09817732293631654, 0.12719501314661374, 0.09949075526341022, 0.11214895788266566, 0.014585865083758143, 0.009405807533463983, 0.053335511298185145, 0.0, 0.028138384732586563, 0.0, 0.0056031486469186875, 0.018761363751345517, 0.027938057908791225, 0.09289497683315007, 0.09288553633962471, 0.09298882555874755, 0.07622884985070266, 0.10339107993553331, 0.08929786078921786, 0.07852845632663452, 0.07276879660865687, 0.07677067817942385, 0.12068908612179152, 0.10742701554755874, 0.10705128967548738, 0.14344059472788317, 0.14783206959608686, 0.10784990049375187, 0.1273631117346915, 0.11922011318374748, 0.11351244162379848, 0.06656603129920147, 0.053679095474200356, 0.026691483647965253, 0.07476970549154738, 0.05594480916872069, 0.08278562995246708, 0.10285500844476925, 0.07035397692981282, 0.08764196550452963, 0.1354863540745328, 0.14252980027797435, 0.12140354094353056, 0.13547200639727486, 0.11588559432047074, 0.12475844993085139, 0.16131558609958807, 0.11883885863818211, 0.09484446786087752, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.00976667744786186, 0.14665717701174052, 0.0960797177461633, 0.0761239326581048, 0.07565325239499676, 0.05461947817712576, 0.06358383434806059, 0.08353689881996818, 0.05205108316764451, 0.07554536700328207, 0.0649850536817903, 0.02906392249683154, 0.05494107698589201, 0.018952763790558103, 0.08564980497969954, 0.10695440010782897, 0.022810256467945655, 0.0412131739677174, 0.06664696591121322, 0.09480636265193276, 0.146359147326369, 0.13533987883392762, 0.1632806877877092, 0.14996302121391492, 0.1768225021822749, 0.1716706583603329, 0.1529302770009463, 0.16372988774197073, 0.13784775839362684, 0.02942909441598729, 0.018485765204041993, 0.02215219582537198, 0.016613864082591134, 0.040229959076666044, 0.02982252856207468, 0.017234386334394336, 0.025772125113338595, 0.03213875331672589]}, "mutation_prompt": null}
{"id": "1d99dc32-142b-43a5-b91e-23fe27239650", "solution": "import numpy as np\n\nclass AdaptiveGradientStochasticSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.evaluations = 0\n        self.learning_rate = 0.1\n        self.sigma = 0.1\n\n    def __call__(self, func):\n        x = np.random.uniform(self.lower_bound, self.upper_bound, self.dim)\n        best_x = x.copy()\n        best_f = func(best_x)\n        self.evaluations += 1\n\n        while self.evaluations < self.budget:\n            perturbation = np.random.normal(0, self.sigma, self.dim)\n            grad_approx = (func(x + perturbation) - func(x - perturbation)) / (2 * self.sigma)\n            self.evaluations += 2\n\n            x = x - self.learning_rate * grad_approx + perturbation\n            x = np.clip(x, self.lower_bound, self.upper_bound)\n\n            current_f = func(x)\n            self.evaluations += 1\n\n            if current_f < best_f:\n                best_f = current_f\n                best_x = x.copy()\n                self.learning_rate *= 1.2  # Adaptive increase\n                self.sigma *= 0.9  # Reduce perturbation to focus on exploitation\n            else:\n                self.learning_rate *= 0.8  # Adaptive decrease\n                self.sigma *= 1.1  # Increase perturbation for exploration\n\n        return best_x", "name": "AdaptiveGradientStochasticSearch", "description": "Gradient-Based Stochastic Search with Adaptive Step Size that combines gradient approximation and stochasticity for effective exploration and exploitation.", "configspace": "", "generation": 1, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "cc35849e-c096-4674-a984-3fc5a6ef4f3f", "metadata": {"aucs": [0.08998073195420075, 0.06271675336939186, 0.08875131807856451, 0.10494277406268049, 0.09280018916089827, 0.09112693834094732, 0.1288638426464599, 0.06554447394544227, 0.07105907849757953, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.020477991420123853, 0.023304540353572656, 0.013580939962702043, 0.017274518788866433, 0.0, 0.008265519422535506, 0.0, 0.0, 0.0, 0.015226161305268215, 0.032874133507481695, 0.020434179529922436, 0.0, 0.011947285404927621, 0.0, 0.0, 0.0073079661356707515, 0.0, 0.9729019800082144, 0.9764282517738041, 0.9773327967574621, 0.9757235476326691, 0.9719199919945001, 0.9620231047078028, 0.9792004787568362, 0.973372573988543, 0.9755793587141087, 0.0, 0.0, 0.0, 0.001989524435141665, 0.0018281365899111002, 0.031124060486155014, 0.0, 0.0, 0.0, 0.11044192282611387, 0.10249699148985147, 0.11860360271926362, 0.017521515422513456, 0.00656962183887122, 9.999999999998899e-05, 0.062372258261358504, 9.999999999998899e-05, 0.0912849334670145, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.011752715529753877, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.1396112167348632, 0.12421783769642902, 0.14103244338215903, 0.10679315147984614, 0.1387916296901911, 0.09817732293631654, 0.12719501314661374, 0.09949075526341022, 0.11214895788266566, 0.014585865083758143, 0.009405807533463983, 0.053335511298185145, 0.0, 0.028138384732586563, 0.0, 0.0056031486469186875, 0.018761363751345517, 0.027938057908791225, 0.09289497683315007, 0.09288553633962471, 0.09298882555874755, 0.07622884985070266, 0.10339107993553331, 0.08929786078921786, 0.07852845632663452, 0.07276879660865687, 0.07677067817942385, 0.12068908612179152, 0.10742701554755874, 0.10705128967548738, 0.14344059472788317, 0.14783206959608686, 0.10784990049375187, 0.1273631117346915, 0.11922011318374748, 0.11351244162379848, 0.06656603129920147, 0.053679095474200356, 0.026691483647965253, 0.07476970549154738, 0.05594480916872069, 0.08278562995246708, 0.10285500844476925, 0.07035397692981282, 0.08764196550452963, 0.1354863540745328, 0.14252980027797435, 0.12140354094353056, 0.13547200639727486, 0.11588559432047074, 0.12475844993085139, 0.16131558609958807, 0.11883885863818211, 0.09484446786087752, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.00976667744786186, 0.14665717701174052, 0.0960797177461633, 0.0761239326581048, 0.07565325239499676, 0.05461947817712576, 0.06358383434806059, 0.08353689881996818, 0.05205108316764451, 0.07554536700328207, 0.0649850536817903, 0.02906392249683154, 0.05494107698589201, 0.018952763790558103, 0.08564980497969954, 0.10695440010782897, 0.022810256467945655, 0.0412131739677174, 0.06664696591121322, 0.09480636265193276, 0.146359147326369, 0.13533987883392762, 0.1632806877877092, 0.14996302121391492, 0.1768225021822749, 0.1716706583603329, 0.1529302770009463, 0.16372988774197073, 0.13784775839362684, 0.02942909441598729, 0.018485765204041993, 0.02215219582537198, 0.016613864082591134, 0.040229959076666044, 0.02982252856207468, 0.017234386334394336, 0.025772125113338595, 0.03213875331672589]}, "mutation_prompt": null}
{"id": "42deada9-523f-4fb1-ad25-b49fac23676b", "solution": "import numpy as np\n\nclass AdaptiveGradientStochasticSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.evaluations = 0\n        self.learning_rate = 0.1\n        self.sigma = 0.1\n\n    def __call__(self, func):\n        x = np.random.uniform(self.lower_bound, self.upper_bound, self.dim)\n        best_x = x.copy()\n        best_f = func(best_x)\n        self.evaluations += 1\n\n        while self.evaluations < self.budget:\n            perturbation = np.random.normal(0, self.sigma, self.dim)\n            grad_approx = (func(x + perturbation) - func(x - perturbation)) / (2 * self.sigma)\n            self.evaluations += 2\n\n            x = x - self.learning_rate * grad_approx + perturbation\n            x = np.clip(x, self.lower_bound, self.upper_bound)\n\n            current_f = func(x)\n            self.evaluations += 1\n\n            if current_f < best_f:\n                best_f = current_f\n                best_x = x.copy()\n                self.learning_rate *= 1.2  # Adaptive increase\n                self.sigma *= 0.9  # Reduce perturbation to focus on exploitation\n            else:\n                self.learning_rate *= 0.8  # Adaptive decrease\n                self.sigma *= 1.1  # Increase perturbation for exploration\n\n        return best_x", "name": "AdaptiveGradientStochasticSearch", "description": "Gradient-Based Stochastic Search with Adaptive Step Size that combines gradient approximation and stochasticity for effective exploration and exploitation.", "configspace": "", "generation": 1, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "cc35849e-c096-4674-a984-3fc5a6ef4f3f", "metadata": {"aucs": [0.08998073195420075, 0.06271675336939186, 0.08875131807856451, 0.10494277406268049, 0.09280018916089827, 0.09112693834094732, 0.1288638426464599, 0.06554447394544227, 0.07105907849757953, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.020477991420123853, 0.023304540353572656, 0.013580939962702043, 0.017274518788866433, 0.0, 0.008265519422535506, 0.0, 0.0, 0.0, 0.015226161305268215, 0.032874133507481695, 0.020434179529922436, 0.0, 0.011947285404927621, 0.0, 0.0, 0.0073079661356707515, 0.0, 0.9729019800082144, 0.9764282517738041, 0.9773327967574621, 0.9757235476326691, 0.9719199919945001, 0.9620231047078028, 0.9792004787568362, 0.973372573988543, 0.9755793587141087, 0.0, 0.0, 0.0, 0.001989524435141665, 0.0018281365899111002, 0.031124060486155014, 0.0, 0.0, 0.0, 0.11044192282611387, 0.10249699148985147, 0.11860360271926362, 0.017521515422513456, 0.00656962183887122, 9.999999999998899e-05, 0.062372258261358504, 9.999999999998899e-05, 0.0912849334670145, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.011752715529753877, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.1396112167348632, 0.12421783769642902, 0.14103244338215903, 0.10679315147984614, 0.1387916296901911, 0.09817732293631654, 0.12719501314661374, 0.09949075526341022, 0.11214895788266566, 0.014585865083758143, 0.009405807533463983, 0.053335511298185145, 0.0, 0.028138384732586563, 0.0, 0.0056031486469186875, 0.018761363751345517, 0.027938057908791225, 0.09289497683315007, 0.09288553633962471, 0.09298882555874755, 0.07622884985070266, 0.10339107993553331, 0.08929786078921786, 0.07852845632663452, 0.07276879660865687, 0.07677067817942385, 0.12068908612179152, 0.10742701554755874, 0.10705128967548738, 0.14344059472788317, 0.14783206959608686, 0.10784990049375187, 0.1273631117346915, 0.11922011318374748, 0.11351244162379848, 0.06656603129920147, 0.053679095474200356, 0.026691483647965253, 0.07476970549154738, 0.05594480916872069, 0.08278562995246708, 0.10285500844476925, 0.07035397692981282, 0.08764196550452963, 0.1354863540745328, 0.14252980027797435, 0.12140354094353056, 0.13547200639727486, 0.11588559432047074, 0.12475844993085139, 0.16131558609958807, 0.11883885863818211, 0.09484446786087752, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.00976667744786186, 0.14665717701174052, 0.0960797177461633, 0.0761239326581048, 0.07565325239499676, 0.05461947817712576, 0.06358383434806059, 0.08353689881996818, 0.05205108316764451, 0.07554536700328207, 0.0649850536817903, 0.02906392249683154, 0.05494107698589201, 0.018952763790558103, 0.08564980497969954, 0.10695440010782897, 0.022810256467945655, 0.0412131739677174, 0.06664696591121322, 0.09480636265193276, 0.146359147326369, 0.13533987883392762, 0.1632806877877092, 0.14996302121391492, 0.1768225021822749, 0.1716706583603329, 0.1529302770009463, 0.16372988774197073, 0.13784775839362684, 0.02942909441598729, 0.018485765204041993, 0.02215219582537198, 0.016613864082591134, 0.040229959076666044, 0.02982252856207468, 0.017234386334394336, 0.025772125113338595, 0.03213875331672589]}, "mutation_prompt": null}
{"id": "7ad31259-067e-41cd-bc91-d73470c79aee", "solution": "import numpy as np\n\nclass AdaptiveGradientStochasticSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.evaluations = 0\n        self.learning_rate = 0.1\n        self.sigma = 0.1\n\n    def __call__(self, func):\n        x = np.random.uniform(self.lower_bound, self.upper_bound, self.dim)\n        best_x = x.copy()\n        best_f = func(best_x)\n        self.evaluations += 1\n\n        while self.evaluations < self.budget:\n            perturbation = np.random.normal(0, self.sigma, self.dim)\n            grad_approx = (func(x + perturbation) - func(x - perturbation)) / (2 * self.sigma)\n            self.evaluations += 2\n\n            x = x - self.learning_rate * grad_approx + perturbation\n            x = np.clip(x, self.lower_bound, self.upper_bound)\n\n            current_f = func(x)\n            self.evaluations += 1\n\n            if current_f < best_f:\n                best_f = current_f\n                best_x = x.copy()\n                self.learning_rate *= 1.2  # Adaptive increase\n                self.sigma *= 0.9  # Reduce perturbation to focus on exploitation\n            else:\n                self.learning_rate *= 0.8  # Adaptive decrease\n                self.sigma *= 1.1  # Increase perturbation for exploration\n\n        return best_x", "name": "AdaptiveGradientStochasticSearch", "description": "Gradient-Based Stochastic Search with Adaptive Step Size that combines gradient approximation and stochasticity for effective exploration and exploitation.", "configspace": "", "generation": 1, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "cc35849e-c096-4674-a984-3fc5a6ef4f3f", "metadata": {"aucs": [0.08998073195420075, 0.06271675336939186, 0.08875131807856451, 0.10494277406268049, 0.09280018916089827, 0.09112693834094732, 0.1288638426464599, 0.06554447394544227, 0.07105907849757953, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.020477991420123853, 0.023304540353572656, 0.013580939962702043, 0.017274518788866433, 0.0, 0.008265519422535506, 0.0, 0.0, 0.0, 0.015226161305268215, 0.032874133507481695, 0.020434179529922436, 0.0, 0.011947285404927621, 0.0, 0.0, 0.0073079661356707515, 0.0, 0.9729019800082144, 0.9764282517738041, 0.9773327967574621, 0.9757235476326691, 0.9719199919945001, 0.9620231047078028, 0.9792004787568362, 0.973372573988543, 0.9755793587141087, 0.0, 0.0, 0.0, 0.001989524435141665, 0.0018281365899111002, 0.031124060486155014, 0.0, 0.0, 0.0, 0.11044192282611387, 0.10249699148985147, 0.11860360271926362, 0.017521515422513456, 0.00656962183887122, 9.999999999998899e-05, 0.062372258261358504, 9.999999999998899e-05, 0.0912849334670145, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.011752715529753877, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.1396112167348632, 0.12421783769642902, 0.14103244338215903, 0.10679315147984614, 0.1387916296901911, 0.09817732293631654, 0.12719501314661374, 0.09949075526341022, 0.11214895788266566, 0.014585865083758143, 0.009405807533463983, 0.053335511298185145, 0.0, 0.028138384732586563, 0.0, 0.0056031486469186875, 0.018761363751345517, 0.027938057908791225, 0.09289497683315007, 0.09288553633962471, 0.09298882555874755, 0.07622884985070266, 0.10339107993553331, 0.08929786078921786, 0.07852845632663452, 0.07276879660865687, 0.07677067817942385, 0.12068908612179152, 0.10742701554755874, 0.10705128967548738, 0.14344059472788317, 0.14783206959608686, 0.10784990049375187, 0.1273631117346915, 0.11922011318374748, 0.11351244162379848, 0.06656603129920147, 0.053679095474200356, 0.026691483647965253, 0.07476970549154738, 0.05594480916872069, 0.08278562995246708, 0.10285500844476925, 0.07035397692981282, 0.08764196550452963, 0.1354863540745328, 0.14252980027797435, 0.12140354094353056, 0.13547200639727486, 0.11588559432047074, 0.12475844993085139, 0.16131558609958807, 0.11883885863818211, 0.09484446786087752, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.00976667744786186, 0.14665717701174052, 0.0960797177461633, 0.0761239326581048, 0.07565325239499676, 0.05461947817712576, 0.06358383434806059, 0.08353689881996818, 0.05205108316764451, 0.07554536700328207, 0.0649850536817903, 0.02906392249683154, 0.05494107698589201, 0.018952763790558103, 0.08564980497969954, 0.10695440010782897, 0.022810256467945655, 0.0412131739677174, 0.06664696591121322, 0.09480636265193276, 0.146359147326369, 0.13533987883392762, 0.1632806877877092, 0.14996302121391492, 0.1768225021822749, 0.1716706583603329, 0.1529302770009463, 0.16372988774197073, 0.13784775839362684, 0.02942909441598729, 0.018485765204041993, 0.02215219582537198, 0.016613864082591134, 0.040229959076666044, 0.02982252856207468, 0.017234386334394336, 0.025772125113338595, 0.03213875331672589]}, "mutation_prompt": null}
{"id": "065beb4b-5096-4c2c-9b1c-72dce5f66368", "solution": "import numpy as np\n\nclass AdaptiveGradientStochasticSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.evaluations = 0\n        self.learning_rate = 0.1\n        self.sigma = 0.1\n\n    def __call__(self, func):\n        x = np.random.uniform(self.lower_bound, self.upper_bound, self.dim)\n        best_x = x.copy()\n        best_f = func(best_x)\n        self.evaluations += 1\n\n        while self.evaluations < self.budget:\n            perturbation = np.random.normal(0, self.sigma, self.dim)\n            grad_approx = (func(x + perturbation) - func(x - perturbation)) / (2 * self.sigma)\n            self.evaluations += 2\n\n            x = x - self.learning_rate * grad_approx + perturbation\n            x = np.clip(x, self.lower_bound, self.upper_bound)\n\n            current_f = func(x)\n            self.evaluations += 1\n\n            if current_f < best_f:\n                best_f = current_f\n                best_x = x.copy()\n                self.learning_rate *= 1.2  # Adaptive increase\n                self.sigma *= 0.9  # Reduce perturbation to focus on exploitation\n            else:\n                self.learning_rate *= 0.8  # Adaptive decrease\n                self.sigma *= 1.1  # Increase perturbation for exploration\n\n        return best_x", "name": "AdaptiveGradientStochasticSearch", "description": "Gradient-Based Stochastic Search with Adaptive Step Size that combines gradient approximation and stochasticity for effective exploration and exploitation.", "configspace": "", "generation": 1, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "cc35849e-c096-4674-a984-3fc5a6ef4f3f", "metadata": {"aucs": [0.08998073195420075, 0.06271675336939186, 0.08875131807856451, 0.10494277406268049, 0.09280018916089827, 0.09112693834094732, 0.1288638426464599, 0.06554447394544227, 0.07105907849757953, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.020477991420123853, 0.023304540353572656, 0.013580939962702043, 0.017274518788866433, 0.0, 0.008265519422535506, 0.0, 0.0, 0.0, 0.015226161305268215, 0.032874133507481695, 0.020434179529922436, 0.0, 0.011947285404927621, 0.0, 0.0, 0.0073079661356707515, 0.0, 0.9729019800082144, 0.9764282517738041, 0.9773327967574621, 0.9757235476326691, 0.9719199919945001, 0.9620231047078028, 0.9792004787568362, 0.973372573988543, 0.9755793587141087, 0.0, 0.0, 0.0, 0.001989524435141665, 0.0018281365899111002, 0.031124060486155014, 0.0, 0.0, 0.0, 0.11044192282611387, 0.10249699148985147, 0.11860360271926362, 0.017521515422513456, 0.00656962183887122, 9.999999999998899e-05, 0.062372258261358504, 9.999999999998899e-05, 0.0912849334670145, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.011752715529753877, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.1396112167348632, 0.12421783769642902, 0.14103244338215903, 0.10679315147984614, 0.1387916296901911, 0.09817732293631654, 0.12719501314661374, 0.09949075526341022, 0.11214895788266566, 0.014585865083758143, 0.009405807533463983, 0.053335511298185145, 0.0, 0.028138384732586563, 0.0, 0.0056031486469186875, 0.018761363751345517, 0.027938057908791225, 0.09289497683315007, 0.09288553633962471, 0.09298882555874755, 0.07622884985070266, 0.10339107993553331, 0.08929786078921786, 0.07852845632663452, 0.07276879660865687, 0.07677067817942385, 0.12068908612179152, 0.10742701554755874, 0.10705128967548738, 0.14344059472788317, 0.14783206959608686, 0.10784990049375187, 0.1273631117346915, 0.11922011318374748, 0.11351244162379848, 0.06656603129920147, 0.053679095474200356, 0.026691483647965253, 0.07476970549154738, 0.05594480916872069, 0.08278562995246708, 0.10285500844476925, 0.07035397692981282, 0.08764196550452963, 0.1354863540745328, 0.14252980027797435, 0.12140354094353056, 0.13547200639727486, 0.11588559432047074, 0.12475844993085139, 0.16131558609958807, 0.11883885863818211, 0.09484446786087752, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.00976667744786186, 0.14665717701174052, 0.0960797177461633, 0.0761239326581048, 0.07565325239499676, 0.05461947817712576, 0.06358383434806059, 0.08353689881996818, 0.05205108316764451, 0.07554536700328207, 0.0649850536817903, 0.02906392249683154, 0.05494107698589201, 0.018952763790558103, 0.08564980497969954, 0.10695440010782897, 0.022810256467945655, 0.0412131739677174, 0.06664696591121322, 0.09480636265193276, 0.146359147326369, 0.13533987883392762, 0.1632806877877092, 0.14996302121391492, 0.1768225021822749, 0.1716706583603329, 0.1529302770009463, 0.16372988774197073, 0.13784775839362684, 0.02942909441598729, 0.018485765204041993, 0.02215219582537198, 0.016613864082591134, 0.040229959076666044, 0.02982252856207468, 0.017234386334394336, 0.025772125113338595, 0.03213875331672589]}, "mutation_prompt": null}
{"id": "efa6fb86-ccb3-45f0-9a2e-a6d42579cb35", "solution": "import numpy as np\n\nclass AdaptiveGradientStochasticSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.evaluations = 0\n        self.learning_rate = 0.1\n        self.sigma = 0.1\n\n    def __call__(self, func):\n        x = np.random.uniform(self.lower_bound, self.upper_bound, self.dim)\n        best_x = x.copy()\n        best_f = func(best_x)\n        self.evaluations += 1\n\n        while self.evaluations < self.budget:\n            perturbation = np.random.normal(0, self.sigma, self.dim)\n            grad_approx = (func(x + perturbation) - func(x - perturbation)) / (2 * self.sigma)\n            self.evaluations += 2\n\n            x = x - self.learning_rate * grad_approx + perturbation\n            x = np.clip(x, self.lower_bound, self.upper_bound)\n\n            current_f = func(x)\n            self.evaluations += 1\n\n            if current_f < best_f:\n                best_f = current_f\n                best_x = x.copy()\n                self.learning_rate *= 1.2  # Adaptive increase\n                self.sigma *= 0.9  # Reduce perturbation to focus on exploitation\n            else:\n                self.learning_rate *= 0.8  # Adaptive decrease\n                self.sigma *= 1.1  # Increase perturbation for exploration\n\n        return best_x", "name": "AdaptiveGradientStochasticSearch", "description": "Gradient-Based Stochastic Search with Adaptive Step Size that combines gradient approximation and stochasticity for effective exploration and exploitation.", "configspace": "", "generation": 1, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "cc35849e-c096-4674-a984-3fc5a6ef4f3f", "metadata": {"aucs": [0.08998073195420075, 0.06271675336939186, 0.08875131807856451, 0.10494277406268049, 0.09280018916089827, 0.09112693834094732, 0.1288638426464599, 0.06554447394544227, 0.07105907849757953, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.020477991420123853, 0.023304540353572656, 0.013580939962702043, 0.017274518788866433, 0.0, 0.008265519422535506, 0.0, 0.0, 0.0, 0.015226161305268215, 0.032874133507481695, 0.020434179529922436, 0.0, 0.011947285404927621, 0.0, 0.0, 0.0073079661356707515, 0.0, 0.9729019800082144, 0.9764282517738041, 0.9773327967574621, 0.9757235476326691, 0.9719199919945001, 0.9620231047078028, 0.9792004787568362, 0.973372573988543, 0.9755793587141087, 0.0, 0.0, 0.0, 0.001989524435141665, 0.0018281365899111002, 0.031124060486155014, 0.0, 0.0, 0.0, 0.11044192282611387, 0.10249699148985147, 0.11860360271926362, 0.017521515422513456, 0.00656962183887122, 9.999999999998899e-05, 0.062372258261358504, 9.999999999998899e-05, 0.0912849334670145, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.011752715529753877, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.1396112167348632, 0.12421783769642902, 0.14103244338215903, 0.10679315147984614, 0.1387916296901911, 0.09817732293631654, 0.12719501314661374, 0.09949075526341022, 0.11214895788266566, 0.014585865083758143, 0.009405807533463983, 0.053335511298185145, 0.0, 0.028138384732586563, 0.0, 0.0056031486469186875, 0.018761363751345517, 0.027938057908791225, 0.09289497683315007, 0.09288553633962471, 0.09298882555874755, 0.07622884985070266, 0.10339107993553331, 0.08929786078921786, 0.07852845632663452, 0.07276879660865687, 0.07677067817942385, 0.12068908612179152, 0.10742701554755874, 0.10705128967548738, 0.14344059472788317, 0.14783206959608686, 0.10784990049375187, 0.1273631117346915, 0.11922011318374748, 0.11351244162379848, 0.06656603129920147, 0.053679095474200356, 0.026691483647965253, 0.07476970549154738, 0.05594480916872069, 0.08278562995246708, 0.10285500844476925, 0.07035397692981282, 0.08764196550452963, 0.1354863540745328, 0.14252980027797435, 0.12140354094353056, 0.13547200639727486, 0.11588559432047074, 0.12475844993085139, 0.16131558609958807, 0.11883885863818211, 0.09484446786087752, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.00976667744786186, 0.14665717701174052, 0.0960797177461633, 0.0761239326581048, 0.07565325239499676, 0.05461947817712576, 0.06358383434806059, 0.08353689881996818, 0.05205108316764451, 0.07554536700328207, 0.0649850536817903, 0.02906392249683154, 0.05494107698589201, 0.018952763790558103, 0.08564980497969954, 0.10695440010782897, 0.022810256467945655, 0.0412131739677174, 0.06664696591121322, 0.09480636265193276, 0.146359147326369, 0.13533987883392762, 0.1632806877877092, 0.14996302121391492, 0.1768225021822749, 0.1716706583603329, 0.1529302770009463, 0.16372988774197073, 0.13784775839362684, 0.02942909441598729, 0.018485765204041993, 0.02215219582537198, 0.016613864082591134, 0.040229959076666044, 0.02982252856207468, 0.017234386334394336, 0.025772125113338595, 0.03213875331672589]}, "mutation_prompt": null}
{"id": "9d4a2672-4610-4afb-bec3-87dffce3a533", "solution": "import numpy as np\n\nclass AdaptiveGradientStochasticSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.evaluations = 0\n        self.learning_rate = 0.1\n        self.sigma = 0.1\n\n    def __call__(self, func):\n        x = np.random.uniform(self.lower_bound, self.upper_bound, self.dim)\n        best_x = x.copy()\n        best_f = func(best_x)\n        self.evaluations += 1\n\n        while self.evaluations < self.budget:\n            perturbation = np.random.normal(0, self.sigma, self.dim)\n            grad_approx = (func(x + perturbation) - func(x - perturbation)) / (2 * self.sigma)\n            self.evaluations += 2\n\n            x = x - self.learning_rate * grad_approx + perturbation\n            x = np.clip(x, self.lower_bound, self.upper_bound)\n\n            current_f = func(x)\n            self.evaluations += 1\n\n            if current_f < best_f:\n                best_f = current_f\n                best_x = x.copy()\n                self.learning_rate *= 1.2  # Adaptive increase\n                self.sigma *= 0.9  # Reduce perturbation to focus on exploitation\n            else:\n                self.learning_rate *= 0.8  # Adaptive decrease\n                self.sigma *= 1.1  # Increase perturbation for exploration\n\n        return best_x", "name": "AdaptiveGradientStochasticSearch", "description": "Gradient-Based Stochastic Search with Adaptive Step Size that combines gradient approximation and stochasticity for effective exploration and exploitation.", "configspace": "", "generation": 1, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "cc35849e-c096-4674-a984-3fc5a6ef4f3f", "metadata": {"aucs": [0.08998073195420075, 0.06271675336939186, 0.08875131807856451, 0.10494277406268049, 0.09280018916089827, 0.09112693834094732, 0.1288638426464599, 0.06554447394544227, 0.07105907849757953, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.020477991420123853, 0.023304540353572656, 0.013580939962702043, 0.017274518788866433, 0.0, 0.008265519422535506, 0.0, 0.0, 0.0, 0.015226161305268215, 0.032874133507481695, 0.020434179529922436, 0.0, 0.011947285404927621, 0.0, 0.0, 0.0073079661356707515, 0.0, 0.9729019800082144, 0.9764282517738041, 0.9773327967574621, 0.9757235476326691, 0.9719199919945001, 0.9620231047078028, 0.9792004787568362, 0.973372573988543, 0.9755793587141087, 0.0, 0.0, 0.0, 0.001989524435141665, 0.0018281365899111002, 0.031124060486155014, 0.0, 0.0, 0.0, 0.11044192282611387, 0.10249699148985147, 0.11860360271926362, 0.017521515422513456, 0.00656962183887122, 9.999999999998899e-05, 0.062372258261358504, 9.999999999998899e-05, 0.0912849334670145, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.011752715529753877, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.1396112167348632, 0.12421783769642902, 0.14103244338215903, 0.10679315147984614, 0.1387916296901911, 0.09817732293631654, 0.12719501314661374, 0.09949075526341022, 0.11214895788266566, 0.014585865083758143, 0.009405807533463983, 0.053335511298185145, 0.0, 0.028138384732586563, 0.0, 0.0056031486469186875, 0.018761363751345517, 0.027938057908791225, 0.09289497683315007, 0.09288553633962471, 0.09298882555874755, 0.07622884985070266, 0.10339107993553331, 0.08929786078921786, 0.07852845632663452, 0.07276879660865687, 0.07677067817942385, 0.12068908612179152, 0.10742701554755874, 0.10705128967548738, 0.14344059472788317, 0.14783206959608686, 0.10784990049375187, 0.1273631117346915, 0.11922011318374748, 0.11351244162379848, 0.06656603129920147, 0.053679095474200356, 0.026691483647965253, 0.07476970549154738, 0.05594480916872069, 0.08278562995246708, 0.10285500844476925, 0.07035397692981282, 0.08764196550452963, 0.1354863540745328, 0.14252980027797435, 0.12140354094353056, 0.13547200639727486, 0.11588559432047074, 0.12475844993085139, 0.16131558609958807, 0.11883885863818211, 0.09484446786087752, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.00976667744786186, 0.14665717701174052, 0.0960797177461633, 0.0761239326581048, 0.07565325239499676, 0.05461947817712576, 0.06358383434806059, 0.08353689881996818, 0.05205108316764451, 0.07554536700328207, 0.0649850536817903, 0.02906392249683154, 0.05494107698589201, 0.018952763790558103, 0.08564980497969954, 0.10695440010782897, 0.022810256467945655, 0.0412131739677174, 0.06664696591121322, 0.09480636265193276, 0.146359147326369, 0.13533987883392762, 0.1632806877877092, 0.14996302121391492, 0.1768225021822749, 0.1716706583603329, 0.1529302770009463, 0.16372988774197073, 0.13784775839362684, 0.02942909441598729, 0.018485765204041993, 0.02215219582537198, 0.016613864082591134, 0.040229959076666044, 0.02982252856207468, 0.017234386334394336, 0.025772125113338595, 0.03213875331672589]}, "mutation_prompt": null}
{"id": "9055204f-69ec-419f-91fe-723d940579f7", "solution": "import numpy as np\n\nclass AdaptiveGradientStochasticSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.evaluations = 0\n        self.learning_rate = 0.1\n        self.sigma = 0.1\n\n    def __call__(self, func):\n        x = np.random.uniform(self.lower_bound, self.upper_bound, self.dim)\n        best_x = x.copy()\n        best_f = func(best_x)\n        self.evaluations += 1\n\n        while self.evaluations < self.budget:\n            perturbation = np.random.normal(0, self.sigma, self.dim)\n            grad_approx = (func(x + perturbation) - func(x - perturbation)) / (2 * self.sigma)\n            self.evaluations += 2\n\n            x = x - self.learning_rate * grad_approx + perturbation\n            x = np.clip(x, self.lower_bound, self.upper_bound)\n\n            current_f = func(x)\n            self.evaluations += 1\n\n            if current_f < best_f:\n                best_f = current_f\n                best_x = x.copy()\n                self.learning_rate *= 1.2  # Adaptive increase\n                self.sigma *= 0.9  # Reduce perturbation to focus on exploitation\n            else:\n                self.learning_rate *= 0.8  # Adaptive decrease\n                self.sigma *= 1.1  # Increase perturbation for exploration\n\n        return best_x", "name": "AdaptiveGradientStochasticSearch", "description": "Gradient-Based Stochastic Search with Adaptive Step Size that combines gradient approximation and stochasticity for effective exploration and exploitation.", "configspace": "", "generation": 1, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "cc35849e-c096-4674-a984-3fc5a6ef4f3f", "metadata": {"aucs": [0.08998073195420075, 0.06271675336939186, 0.08875131807856451, 0.10494277406268049, 0.09280018916089827, 0.09112693834094732, 0.1288638426464599, 0.06554447394544227, 0.07105907849757953, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.020477991420123853, 0.023304540353572656, 0.013580939962702043, 0.017274518788866433, 0.0, 0.008265519422535506, 0.0, 0.0, 0.0, 0.015226161305268215, 0.032874133507481695, 0.020434179529922436, 0.0, 0.011947285404927621, 0.0, 0.0, 0.0073079661356707515, 0.0, 0.9729019800082144, 0.9764282517738041, 0.9773327967574621, 0.9757235476326691, 0.9719199919945001, 0.9620231047078028, 0.9792004787568362, 0.973372573988543, 0.9755793587141087, 0.0, 0.0, 0.0, 0.001989524435141665, 0.0018281365899111002, 0.031124060486155014, 0.0, 0.0, 0.0, 0.11044192282611387, 0.10249699148985147, 0.11860360271926362, 0.017521515422513456, 0.00656962183887122, 9.999999999998899e-05, 0.062372258261358504, 9.999999999998899e-05, 0.0912849334670145, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.011752715529753877, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.1396112167348632, 0.12421783769642902, 0.14103244338215903, 0.10679315147984614, 0.1387916296901911, 0.09817732293631654, 0.12719501314661374, 0.09949075526341022, 0.11214895788266566, 0.014585865083758143, 0.009405807533463983, 0.053335511298185145, 0.0, 0.028138384732586563, 0.0, 0.0056031486469186875, 0.018761363751345517, 0.027938057908791225, 0.09289497683315007, 0.09288553633962471, 0.09298882555874755, 0.07622884985070266, 0.10339107993553331, 0.08929786078921786, 0.07852845632663452, 0.07276879660865687, 0.07677067817942385, 0.12068908612179152, 0.10742701554755874, 0.10705128967548738, 0.14344059472788317, 0.14783206959608686, 0.10784990049375187, 0.1273631117346915, 0.11922011318374748, 0.11351244162379848, 0.06656603129920147, 0.053679095474200356, 0.026691483647965253, 0.07476970549154738, 0.05594480916872069, 0.08278562995246708, 0.10285500844476925, 0.07035397692981282, 0.08764196550452963, 0.1354863540745328, 0.14252980027797435, 0.12140354094353056, 0.13547200639727486, 0.11588559432047074, 0.12475844993085139, 0.16131558609958807, 0.11883885863818211, 0.09484446786087752, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.00976667744786186, 0.14665717701174052, 0.0960797177461633, 0.0761239326581048, 0.07565325239499676, 0.05461947817712576, 0.06358383434806059, 0.08353689881996818, 0.05205108316764451, 0.07554536700328207, 0.0649850536817903, 0.02906392249683154, 0.05494107698589201, 0.018952763790558103, 0.08564980497969954, 0.10695440010782897, 0.022810256467945655, 0.0412131739677174, 0.06664696591121322, 0.09480636265193276, 0.146359147326369, 0.13533987883392762, 0.1632806877877092, 0.14996302121391492, 0.1768225021822749, 0.1716706583603329, 0.1529302770009463, 0.16372988774197073, 0.13784775839362684, 0.02942909441598729, 0.018485765204041993, 0.02215219582537198, 0.016613864082591134, 0.040229959076666044, 0.02982252856207468, 0.017234386334394336, 0.025772125113338595, 0.03213875331672589]}, "mutation_prompt": null}
{"id": "773e7fe9-39ab-44bb-9d72-b7b9d2f749fc", "solution": "import numpy as np\n\nclass AdaptiveGradientStochasticSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.evaluations = 0\n        self.learning_rate = 0.1\n        self.sigma = 0.1\n\n    def __call__(self, func):\n        x = np.random.uniform(self.lower_bound, self.upper_bound, self.dim)\n        best_x = x.copy()\n        best_f = func(best_x)\n        self.evaluations += 1\n\n        while self.evaluations < self.budget:\n            perturbation = np.random.normal(0, self.sigma, self.dim)\n            grad_approx = (func(x + perturbation) - func(x - perturbation)) / (2 * self.sigma)\n            self.evaluations += 2\n\n            x = x - self.learning_rate * grad_approx + perturbation\n            x = np.clip(x, self.lower_bound, self.upper_bound)\n\n            current_f = func(x)\n            self.evaluations += 1\n\n            if current_f < best_f:\n                best_f = current_f\n                best_x = x.copy()\n                self.learning_rate *= 1.2  # Adaptive increase\n                self.sigma *= 0.9  # Reduce perturbation to focus on exploitation\n            else:\n                self.learning_rate *= 0.8  # Adaptive decrease\n                self.sigma *= 1.1  # Increase perturbation for exploration\n\n        return best_x", "name": "AdaptiveGradientStochasticSearch", "description": "Gradient-Based Stochastic Search with Adaptive Step Size that combines gradient approximation and stochasticity for effective exploration and exploitation.", "configspace": "", "generation": 1, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "cc35849e-c096-4674-a984-3fc5a6ef4f3f", "metadata": {"aucs": [0.08998073195420075, 0.06271675336939186, 0.08875131807856451, 0.10494277406268049, 0.09280018916089827, 0.09112693834094732, 0.1288638426464599, 0.06554447394544227, 0.07105907849757953, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.020477991420123853, 0.023304540353572656, 0.013580939962702043, 0.017274518788866433, 0.0, 0.008265519422535506, 0.0, 0.0, 0.0, 0.015226161305268215, 0.032874133507481695, 0.020434179529922436, 0.0, 0.011947285404927621, 0.0, 0.0, 0.0073079661356707515, 0.0, 0.9729019800082144, 0.9764282517738041, 0.9773327967574621, 0.9757235476326691, 0.9719199919945001, 0.9620231047078028, 0.9792004787568362, 0.973372573988543, 0.9755793587141087, 0.0, 0.0, 0.0, 0.001989524435141665, 0.0018281365899111002, 0.031124060486155014, 0.0, 0.0, 0.0, 0.11044192282611387, 0.10249699148985147, 0.11860360271926362, 0.017521515422513456, 0.00656962183887122, 9.999999999998899e-05, 0.062372258261358504, 9.999999999998899e-05, 0.0912849334670145, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.011752715529753877, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.1396112167348632, 0.12421783769642902, 0.14103244338215903, 0.10679315147984614, 0.1387916296901911, 0.09817732293631654, 0.12719501314661374, 0.09949075526341022, 0.11214895788266566, 0.014585865083758143, 0.009405807533463983, 0.053335511298185145, 0.0, 0.028138384732586563, 0.0, 0.0056031486469186875, 0.018761363751345517, 0.027938057908791225, 0.09289497683315007, 0.09288553633962471, 0.09298882555874755, 0.07622884985070266, 0.10339107993553331, 0.08929786078921786, 0.07852845632663452, 0.07276879660865687, 0.07677067817942385, 0.12068908612179152, 0.10742701554755874, 0.10705128967548738, 0.14344059472788317, 0.14783206959608686, 0.10784990049375187, 0.1273631117346915, 0.11922011318374748, 0.11351244162379848, 0.06656603129920147, 0.053679095474200356, 0.026691483647965253, 0.07476970549154738, 0.05594480916872069, 0.08278562995246708, 0.10285500844476925, 0.07035397692981282, 0.08764196550452963, 0.1354863540745328, 0.14252980027797435, 0.12140354094353056, 0.13547200639727486, 0.11588559432047074, 0.12475844993085139, 0.16131558609958807, 0.11883885863818211, 0.09484446786087752, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.00976667744786186, 0.14665717701174052, 0.0960797177461633, 0.0761239326581048, 0.07565325239499676, 0.05461947817712576, 0.06358383434806059, 0.08353689881996818, 0.05205108316764451, 0.07554536700328207, 0.0649850536817903, 0.02906392249683154, 0.05494107698589201, 0.018952763790558103, 0.08564980497969954, 0.10695440010782897, 0.022810256467945655, 0.0412131739677174, 0.06664696591121322, 0.09480636265193276, 0.146359147326369, 0.13533987883392762, 0.1632806877877092, 0.14996302121391492, 0.1768225021822749, 0.1716706583603329, 0.1529302770009463, 0.16372988774197073, 0.13784775839362684, 0.02942909441598729, 0.018485765204041993, 0.02215219582537198, 0.016613864082591134, 0.040229959076666044, 0.02982252856207468, 0.017234386334394336, 0.025772125113338595, 0.03213875331672589]}, "mutation_prompt": null}
{"id": "714918a1-ac15-4434-b712-68733c651d3c", "solution": "import numpy as np\n\nclass AdaptiveGradientStochasticSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.evaluations = 0\n        self.learning_rate = 0.1\n        self.sigma = 0.1\n\n    def __call__(self, func):\n        x = np.random.uniform(self.lower_bound, self.upper_bound, self.dim)\n        best_x = x.copy()\n        best_f = func(best_x)\n        self.evaluations += 1\n\n        while self.evaluations < self.budget:\n            perturbation = np.random.normal(0, self.sigma, self.dim)\n            grad_approx = (func(x + perturbation) - func(x - perturbation)) / (2 * self.sigma)\n            self.evaluations += 2\n\n            x = x - self.learning_rate * grad_approx + perturbation\n            x = np.clip(x, self.lower_bound, self.upper_bound)\n\n            current_f = func(x)\n            self.evaluations += 1\n\n            if current_f < best_f:\n                best_f = current_f\n                best_x = x.copy()\n                self.learning_rate *= 1.2  # Adaptive increase\n                self.sigma *= 0.9  # Reduce perturbation to focus on exploitation\n            else:\n                self.learning_rate *= 0.8  # Adaptive decrease\n                self.sigma *= 1.1  # Increase perturbation for exploration\n\n        return best_x", "name": "AdaptiveGradientStochasticSearch", "description": "Gradient-Based Stochastic Search with Adaptive Step Size that combines gradient approximation and stochasticity for effective exploration and exploitation.", "configspace": "", "generation": 1, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "cc35849e-c096-4674-a984-3fc5a6ef4f3f", "metadata": {"aucs": [0.08998073195420075, 0.06271675336939186, 0.08875131807856451, 0.10494277406268049, 0.09280018916089827, 0.09112693834094732, 0.1288638426464599, 0.06554447394544227, 0.07105907849757953, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.020477991420123853, 0.023304540353572656, 0.013580939962702043, 0.017274518788866433, 0.0, 0.008265519422535506, 0.0, 0.0, 0.0, 0.015226161305268215, 0.032874133507481695, 0.020434179529922436, 0.0, 0.011947285404927621, 0.0, 0.0, 0.0073079661356707515, 0.0, 0.9729019800082144, 0.9764282517738041, 0.9773327967574621, 0.9757235476326691, 0.9719199919945001, 0.9620231047078028, 0.9792004787568362, 0.973372573988543, 0.9755793587141087, 0.0, 0.0, 0.0, 0.001989524435141665, 0.0018281365899111002, 0.031124060486155014, 0.0, 0.0, 0.0, 0.11044192282611387, 0.10249699148985147, 0.11860360271926362, 0.017521515422513456, 0.00656962183887122, 9.999999999998899e-05, 0.062372258261358504, 9.999999999998899e-05, 0.0912849334670145, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.011752715529753877, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.1396112167348632, 0.12421783769642902, 0.14103244338215903, 0.10679315147984614, 0.1387916296901911, 0.09817732293631654, 0.12719501314661374, 0.09949075526341022, 0.11214895788266566, 0.014585865083758143, 0.009405807533463983, 0.053335511298185145, 0.0, 0.028138384732586563, 0.0, 0.0056031486469186875, 0.018761363751345517, 0.027938057908791225, 0.09289497683315007, 0.09288553633962471, 0.09298882555874755, 0.07622884985070266, 0.10339107993553331, 0.08929786078921786, 0.07852845632663452, 0.07276879660865687, 0.07677067817942385, 0.12068908612179152, 0.10742701554755874, 0.10705128967548738, 0.14344059472788317, 0.14783206959608686, 0.10784990049375187, 0.1273631117346915, 0.11922011318374748, 0.11351244162379848, 0.06656603129920147, 0.053679095474200356, 0.026691483647965253, 0.07476970549154738, 0.05594480916872069, 0.08278562995246708, 0.10285500844476925, 0.07035397692981282, 0.08764196550452963, 0.1354863540745328, 0.14252980027797435, 0.12140354094353056, 0.13547200639727486, 0.11588559432047074, 0.12475844993085139, 0.16131558609958807, 0.11883885863818211, 0.09484446786087752, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.00976667744786186, 0.14665717701174052, 0.0960797177461633, 0.0761239326581048, 0.07565325239499676, 0.05461947817712576, 0.06358383434806059, 0.08353689881996818, 0.05205108316764451, 0.07554536700328207, 0.0649850536817903, 0.02906392249683154, 0.05494107698589201, 0.018952763790558103, 0.08564980497969954, 0.10695440010782897, 0.022810256467945655, 0.0412131739677174, 0.06664696591121322, 0.09480636265193276, 0.146359147326369, 0.13533987883392762, 0.1632806877877092, 0.14996302121391492, 0.1768225021822749, 0.1716706583603329, 0.1529302770009463, 0.16372988774197073, 0.13784775839362684, 0.02942909441598729, 0.018485765204041993, 0.02215219582537198, 0.016613864082591134, 0.040229959076666044, 0.02982252856207468, 0.017234386334394336, 0.025772125113338595, 0.03213875331672589]}, "mutation_prompt": null}
{"id": "027019b6-bf14-4bd7-b50f-788b2d11abb5", "solution": "import numpy as np\n\nclass AdaptiveGradientStochasticSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.evaluations = 0\n        self.learning_rate = 0.1\n        self.sigma = 0.1\n\n    def __call__(self, func):\n        x = np.random.uniform(self.lower_bound, self.upper_bound, self.dim)\n        best_x = x.copy()\n        best_f = func(best_x)\n        self.evaluations += 1\n\n        while self.evaluations < self.budget:\n            perturbation = np.random.normal(0, self.sigma, self.dim)\n            grad_approx = (func(x + perturbation) - func(x - perturbation)) / (2 * self.sigma)\n            self.evaluations += 2\n\n            x = x - self.learning_rate * grad_approx + perturbation\n            x = np.clip(x, self.lower_bound, self.upper_bound)\n\n            current_f = func(x)\n            self.evaluations += 1\n\n            if current_f < best_f:\n                best_f = current_f\n                best_x = x.copy()\n                self.learning_rate *= 1.2  # Adaptive increase\n                self.sigma *= 0.9  # Reduce perturbation to focus on exploitation\n            else:\n                self.learning_rate *= 0.8  # Adaptive decrease\n                self.sigma *= 1.1  # Increase perturbation for exploration\n\n        return best_x", "name": "AdaptiveGradientStochasticSearch", "description": "Gradient-Based Stochastic Search with Adaptive Step Size that combines gradient approximation and stochasticity for effective exploration and exploitation.", "configspace": "", "generation": 1, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "cc35849e-c096-4674-a984-3fc5a6ef4f3f", "metadata": {"aucs": [0.08998073195420075, 0.06271675336939186, 0.08875131807856451, 0.10494277406268049, 0.09280018916089827, 0.09112693834094732, 0.1288638426464599, 0.06554447394544227, 0.07105907849757953, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.020477991420123853, 0.023304540353572656, 0.013580939962702043, 0.017274518788866433, 0.0, 0.008265519422535506, 0.0, 0.0, 0.0, 0.015226161305268215, 0.032874133507481695, 0.020434179529922436, 0.0, 0.011947285404927621, 0.0, 0.0, 0.0073079661356707515, 0.0, 0.9729019800082144, 0.9764282517738041, 0.9773327967574621, 0.9757235476326691, 0.9719199919945001, 0.9620231047078028, 0.9792004787568362, 0.973372573988543, 0.9755793587141087, 0.0, 0.0, 0.0, 0.001989524435141665, 0.0018281365899111002, 0.031124060486155014, 0.0, 0.0, 0.0, 0.11044192282611387, 0.10249699148985147, 0.11860360271926362, 0.017521515422513456, 0.00656962183887122, 9.999999999998899e-05, 0.062372258261358504, 9.999999999998899e-05, 0.0912849334670145, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.011752715529753877, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.1396112167348632, 0.12421783769642902, 0.14103244338215903, 0.10679315147984614, 0.1387916296901911, 0.09817732293631654, 0.12719501314661374, 0.09949075526341022, 0.11214895788266566, 0.014585865083758143, 0.009405807533463983, 0.053335511298185145, 0.0, 0.028138384732586563, 0.0, 0.0056031486469186875, 0.018761363751345517, 0.027938057908791225, 0.09289497683315007, 0.09288553633962471, 0.09298882555874755, 0.07622884985070266, 0.10339107993553331, 0.08929786078921786, 0.07852845632663452, 0.07276879660865687, 0.07677067817942385, 0.12068908612179152, 0.10742701554755874, 0.10705128967548738, 0.14344059472788317, 0.14783206959608686, 0.10784990049375187, 0.1273631117346915, 0.11922011318374748, 0.11351244162379848, 0.06656603129920147, 0.053679095474200356, 0.026691483647965253, 0.07476970549154738, 0.05594480916872069, 0.08278562995246708, 0.10285500844476925, 0.07035397692981282, 0.08764196550452963, 0.1354863540745328, 0.14252980027797435, 0.12140354094353056, 0.13547200639727486, 0.11588559432047074, 0.12475844993085139, 0.16131558609958807, 0.11883885863818211, 0.09484446786087752, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.00976667744786186, 0.14665717701174052, 0.0960797177461633, 0.0761239326581048, 0.07565325239499676, 0.05461947817712576, 0.06358383434806059, 0.08353689881996818, 0.05205108316764451, 0.07554536700328207, 0.0649850536817903, 0.02906392249683154, 0.05494107698589201, 0.018952763790558103, 0.08564980497969954, 0.10695440010782897, 0.022810256467945655, 0.0412131739677174, 0.06664696591121322, 0.09480636265193276, 0.146359147326369, 0.13533987883392762, 0.1632806877877092, 0.14996302121391492, 0.1768225021822749, 0.1716706583603329, 0.1529302770009463, 0.16372988774197073, 0.13784775839362684, 0.02942909441598729, 0.018485765204041993, 0.02215219582537198, 0.016613864082591134, 0.040229959076666044, 0.02982252856207468, 0.017234386334394336, 0.025772125113338595, 0.03213875331672589]}, "mutation_prompt": null}
{"id": "6c96df1a-fd77-4ca8-b0ac-4c2924a036a7", "solution": "import numpy as np\n\nclass AdaptiveGradientStochasticSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.evaluations = 0\n        self.learning_rate = 0.1\n        self.sigma = 0.1\n\n    def __call__(self, func):\n        x = np.random.uniform(self.lower_bound, self.upper_bound, self.dim)\n        best_x = x.copy()\n        best_f = func(best_x)\n        self.evaluations += 1\n\n        while self.evaluations < self.budget:\n            perturbation = np.random.normal(0, self.sigma, self.dim)\n            grad_approx = (func(x + perturbation) - func(x - perturbation)) / (2 * self.sigma)\n            self.evaluations += 2\n\n            x = x - self.learning_rate * grad_approx + perturbation\n            x = np.clip(x, self.lower_bound, self.upper_bound)\n\n            current_f = func(x)\n            self.evaluations += 1\n\n            if current_f < best_f:\n                best_f = current_f\n                best_x = x.copy()\n                self.learning_rate *= 1.2  # Adaptive increase\n                self.sigma *= 0.9  # Reduce perturbation to focus on exploitation\n            else:\n                self.learning_rate *= 0.8  # Adaptive decrease\n                self.sigma *= 1.1  # Increase perturbation for exploration\n\n        return best_x", "name": "AdaptiveGradientStochasticSearch", "description": "Gradient-Based Stochastic Search with Adaptive Step Size that combines gradient approximation and stochasticity for effective exploration and exploitation.", "configspace": "", "generation": 1, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "cc35849e-c096-4674-a984-3fc5a6ef4f3f", "metadata": {"aucs": [0.08998073195420075, 0.06271675336939186, 0.08875131807856451, 0.10494277406268049, 0.09280018916089827, 0.09112693834094732, 0.1288638426464599, 0.06554447394544227, 0.07105907849757953, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.020477991420123853, 0.023304540353572656, 0.013580939962702043, 0.017274518788866433, 0.0, 0.008265519422535506, 0.0, 0.0, 0.0, 0.015226161305268215, 0.032874133507481695, 0.020434179529922436, 0.0, 0.011947285404927621, 0.0, 0.0, 0.0073079661356707515, 0.0, 0.9729019800082144, 0.9764282517738041, 0.9773327967574621, 0.9757235476326691, 0.9719199919945001, 0.9620231047078028, 0.9792004787568362, 0.973372573988543, 0.9755793587141087, 0.0, 0.0, 0.0, 0.001989524435141665, 0.0018281365899111002, 0.031124060486155014, 0.0, 0.0, 0.0, 0.11044192282611387, 0.10249699148985147, 0.11860360271926362, 0.017521515422513456, 0.00656962183887122, 9.999999999998899e-05, 0.062372258261358504, 9.999999999998899e-05, 0.0912849334670145, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.011752715529753877, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.1396112167348632, 0.12421783769642902, 0.14103244338215903, 0.10679315147984614, 0.1387916296901911, 0.09817732293631654, 0.12719501314661374, 0.09949075526341022, 0.11214895788266566, 0.014585865083758143, 0.009405807533463983, 0.053335511298185145, 0.0, 0.028138384732586563, 0.0, 0.0056031486469186875, 0.018761363751345517, 0.027938057908791225, 0.09289497683315007, 0.09288553633962471, 0.09298882555874755, 0.07622884985070266, 0.10339107993553331, 0.08929786078921786, 0.07852845632663452, 0.07276879660865687, 0.07677067817942385, 0.12068908612179152, 0.10742701554755874, 0.10705128967548738, 0.14344059472788317, 0.14783206959608686, 0.10784990049375187, 0.1273631117346915, 0.11922011318374748, 0.11351244162379848, 0.06656603129920147, 0.053679095474200356, 0.026691483647965253, 0.07476970549154738, 0.05594480916872069, 0.08278562995246708, 0.10285500844476925, 0.07035397692981282, 0.08764196550452963, 0.1354863540745328, 0.14252980027797435, 0.12140354094353056, 0.13547200639727486, 0.11588559432047074, 0.12475844993085139, 0.16131558609958807, 0.11883885863818211, 0.09484446786087752, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.00976667744786186, 0.14665717701174052, 0.0960797177461633, 0.0761239326581048, 0.07565325239499676, 0.05461947817712576, 0.06358383434806059, 0.08353689881996818, 0.05205108316764451, 0.07554536700328207, 0.0649850536817903, 0.02906392249683154, 0.05494107698589201, 0.018952763790558103, 0.08564980497969954, 0.10695440010782897, 0.022810256467945655, 0.0412131739677174, 0.06664696591121322, 0.09480636265193276, 0.146359147326369, 0.13533987883392762, 0.1632806877877092, 0.14996302121391492, 0.1768225021822749, 0.1716706583603329, 0.1529302770009463, 0.16372988774197073, 0.13784775839362684, 0.02942909441598729, 0.018485765204041993, 0.02215219582537198, 0.016613864082591134, 0.040229959076666044, 0.02982252856207468, 0.017234386334394336, 0.025772125113338595, 0.03213875331672589]}, "mutation_prompt": null}
{"id": "ceed6789-9c4d-4efd-8689-bef6821db37b", "solution": "import numpy as np\n\nclass AdaptiveGradientStochasticSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.evaluations = 0\n        self.learning_rate = 0.1\n        self.sigma = 0.1\n\n    def __call__(self, func):\n        x = np.random.uniform(self.lower_bound, self.upper_bound, self.dim)\n        best_x = x.copy()\n        best_f = func(best_x)\n        self.evaluations += 1\n\n        while self.evaluations < self.budget:\n            perturbation = np.random.normal(0, self.sigma, self.dim)\n            grad_approx = (func(x + perturbation) - func(x - perturbation)) / (2 * self.sigma)\n            self.evaluations += 2\n\n            x = x - self.learning_rate * grad_approx + perturbation\n            x = np.clip(x, self.lower_bound, self.upper_bound)\n\n            current_f = func(x)\n            self.evaluations += 1\n\n            if current_f < best_f:\n                best_f = current_f\n                best_x = x.copy()\n                self.learning_rate *= 1.2  # Adaptive increase\n                self.sigma *= 0.9  # Reduce perturbation to focus on exploitation\n            else:\n                self.learning_rate *= 0.8  # Adaptive decrease\n                self.sigma *= 1.1  # Increase perturbation for exploration\n\n        return best_x", "name": "AdaptiveGradientStochasticSearch", "description": "Gradient-Based Stochastic Search with Adaptive Step Size that combines gradient approximation and stochasticity for effective exploration and exploitation.", "configspace": "", "generation": 1, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "cc35849e-c096-4674-a984-3fc5a6ef4f3f", "metadata": {"aucs": [0.08998073195420075, 0.06271675336939186, 0.08875131807856451, 0.10494277406268049, 0.09280018916089827, 0.09112693834094732, 0.1288638426464599, 0.06554447394544227, 0.07105907849757953, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.020477991420123853, 0.023304540353572656, 0.013580939962702043, 0.017274518788866433, 0.0, 0.008265519422535506, 0.0, 0.0, 0.0, 0.015226161305268215, 0.032874133507481695, 0.020434179529922436, 0.0, 0.011947285404927621, 0.0, 0.0, 0.0073079661356707515, 0.0, 0.9729019800082144, 0.9764282517738041, 0.9773327967574621, 0.9757235476326691, 0.9719199919945001, 0.9620231047078028, 0.9792004787568362, 0.973372573988543, 0.9755793587141087, 0.0, 0.0, 0.0, 0.001989524435141665, 0.0018281365899111002, 0.031124060486155014, 0.0, 0.0, 0.0, 0.11044192282611387, 0.10249699148985147, 0.11860360271926362, 0.017521515422513456, 0.00656962183887122, 9.999999999998899e-05, 0.062372258261358504, 9.999999999998899e-05, 0.0912849334670145, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.011752715529753877, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.1396112167348632, 0.12421783769642902, 0.14103244338215903, 0.10679315147984614, 0.1387916296901911, 0.09817732293631654, 0.12719501314661374, 0.09949075526341022, 0.11214895788266566, 0.014585865083758143, 0.009405807533463983, 0.053335511298185145, 0.0, 0.028138384732586563, 0.0, 0.0056031486469186875, 0.018761363751345517, 0.027938057908791225, 0.09289497683315007, 0.09288553633962471, 0.09298882555874755, 0.07622884985070266, 0.10339107993553331, 0.08929786078921786, 0.07852845632663452, 0.07276879660865687, 0.07677067817942385, 0.12068908612179152, 0.10742701554755874, 0.10705128967548738, 0.14344059472788317, 0.14783206959608686, 0.10784990049375187, 0.1273631117346915, 0.11922011318374748, 0.11351244162379848, 0.06656603129920147, 0.053679095474200356, 0.026691483647965253, 0.07476970549154738, 0.05594480916872069, 0.08278562995246708, 0.10285500844476925, 0.07035397692981282, 0.08764196550452963, 0.1354863540745328, 0.14252980027797435, 0.12140354094353056, 0.13547200639727486, 0.11588559432047074, 0.12475844993085139, 0.16131558609958807, 0.11883885863818211, 0.09484446786087752, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.00976667744786186, 0.14665717701174052, 0.0960797177461633, 0.0761239326581048, 0.07565325239499676, 0.05461947817712576, 0.06358383434806059, 0.08353689881996818, 0.05205108316764451, 0.07554536700328207, 0.0649850536817903, 0.02906392249683154, 0.05494107698589201, 0.018952763790558103, 0.08564980497969954, 0.10695440010782897, 0.022810256467945655, 0.0412131739677174, 0.06664696591121322, 0.09480636265193276, 0.146359147326369, 0.13533987883392762, 0.1632806877877092, 0.14996302121391492, 0.1768225021822749, 0.1716706583603329, 0.1529302770009463, 0.16372988774197073, 0.13784775839362684, 0.02942909441598729, 0.018485765204041993, 0.02215219582537198, 0.016613864082591134, 0.040229959076666044, 0.02982252856207468, 0.017234386334394336, 0.025772125113338595, 0.03213875331672589]}, "mutation_prompt": null}
{"id": "f03232ff-78ea-45ae-a6d8-50d0fb858bb9", "solution": "import numpy as np\n\nclass AdaptiveGradientStochasticSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.evaluations = 0\n        self.learning_rate = 0.1\n        self.sigma = 0.1\n\n    def __call__(self, func):\n        x = np.random.uniform(self.lower_bound, self.upper_bound, self.dim)\n        best_x = x.copy()\n        best_f = func(best_x)\n        self.evaluations += 1\n\n        while self.evaluations < self.budget:\n            perturbation = np.random.normal(0, self.sigma, self.dim)\n            grad_approx = (func(x + perturbation) - func(x - perturbation)) / (2 * self.sigma)\n            self.evaluations += 2\n\n            x = x - self.learning_rate * grad_approx + perturbation\n            x = np.clip(x, self.lower_bound, self.upper_bound)\n\n            current_f = func(x)\n            self.evaluations += 1\n\n            if current_f < best_f:\n                best_f = current_f\n                best_x = x.copy()\n                self.learning_rate *= 1.2  # Adaptive increase\n                self.sigma *= 0.9  # Reduce perturbation to focus on exploitation\n            else:\n                self.learning_rate *= 0.8  # Adaptive decrease\n                self.sigma *= 1.1  # Increase perturbation for exploration\n\n        return best_x", "name": "AdaptiveGradientStochasticSearch", "description": "Gradient-Based Stochastic Search with Adaptive Step Size that combines gradient approximation and stochasticity for effective exploration and exploitation.", "configspace": "", "generation": 1, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "cc35849e-c096-4674-a984-3fc5a6ef4f3f", "metadata": {"aucs": [0.08998073195420075, 0.06271675336939186, 0.08875131807856451, 0.10494277406268049, 0.09280018916089827, 0.09112693834094732, 0.1288638426464599, 0.06554447394544227, 0.07105907849757953, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.020477991420123853, 0.023304540353572656, 0.013580939962702043, 0.017274518788866433, 0.0, 0.008265519422535506, 0.0, 0.0, 0.0, 0.015226161305268215, 0.032874133507481695, 0.020434179529922436, 0.0, 0.011947285404927621, 0.0, 0.0, 0.0073079661356707515, 0.0, 0.9729019800082144, 0.9764282517738041, 0.9773327967574621, 0.9757235476326691, 0.9719199919945001, 0.9620231047078028, 0.9792004787568362, 0.973372573988543, 0.9755793587141087, 0.0, 0.0, 0.0, 0.001989524435141665, 0.0018281365899111002, 0.031124060486155014, 0.0, 0.0, 0.0, 0.11044192282611387, 0.10249699148985147, 0.11860360271926362, 0.017521515422513456, 0.00656962183887122, 9.999999999998899e-05, 0.062372258261358504, 9.999999999998899e-05, 0.0912849334670145, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.011752715529753877, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.1396112167348632, 0.12421783769642902, 0.14103244338215903, 0.10679315147984614, 0.1387916296901911, 0.09817732293631654, 0.12719501314661374, 0.09949075526341022, 0.11214895788266566, 0.014585865083758143, 0.009405807533463983, 0.053335511298185145, 0.0, 0.028138384732586563, 0.0, 0.0056031486469186875, 0.018761363751345517, 0.027938057908791225, 0.09289497683315007, 0.09288553633962471, 0.09298882555874755, 0.07622884985070266, 0.10339107993553331, 0.08929786078921786, 0.07852845632663452, 0.07276879660865687, 0.07677067817942385, 0.12068908612179152, 0.10742701554755874, 0.10705128967548738, 0.14344059472788317, 0.14783206959608686, 0.10784990049375187, 0.1273631117346915, 0.11922011318374748, 0.11351244162379848, 0.06656603129920147, 0.053679095474200356, 0.026691483647965253, 0.07476970549154738, 0.05594480916872069, 0.08278562995246708, 0.10285500844476925, 0.07035397692981282, 0.08764196550452963, 0.1354863540745328, 0.14252980027797435, 0.12140354094353056, 0.13547200639727486, 0.11588559432047074, 0.12475844993085139, 0.16131558609958807, 0.11883885863818211, 0.09484446786087752, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.00976667744786186, 0.14665717701174052, 0.0960797177461633, 0.0761239326581048, 0.07565325239499676, 0.05461947817712576, 0.06358383434806059, 0.08353689881996818, 0.05205108316764451, 0.07554536700328207, 0.0649850536817903, 0.02906392249683154, 0.05494107698589201, 0.018952763790558103, 0.08564980497969954, 0.10695440010782897, 0.022810256467945655, 0.0412131739677174, 0.06664696591121322, 0.09480636265193276, 0.146359147326369, 0.13533987883392762, 0.1632806877877092, 0.14996302121391492, 0.1768225021822749, 0.1716706583603329, 0.1529302770009463, 0.16372988774197073, 0.13784775839362684, 0.02942909441598729, 0.018485765204041993, 0.02215219582537198, 0.016613864082591134, 0.040229959076666044, 0.02982252856207468, 0.017234386334394336, 0.025772125113338595, 0.03213875331672589]}, "mutation_prompt": null}
{"id": "bd657899-6e8c-4cf1-9cde-14962926d195", "solution": "import numpy as np\n\nclass AdaptiveGradientStochasticSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.evaluations = 0\n        self.learning_rate = 0.1\n        self.sigma = 0.1\n\n    def __call__(self, func):\n        x = np.random.uniform(self.lower_bound, self.upper_bound, self.dim)\n        best_x = x.copy()\n        best_f = func(best_x)\n        self.evaluations += 1\n\n        while self.evaluations < self.budget:\n            perturbation = np.random.normal(0, self.sigma, self.dim)\n            grad_approx = (func(x + perturbation) - func(x - perturbation)) / (2 * self.sigma)\n            self.evaluations += 2\n\n            x = x - self.learning_rate * grad_approx + perturbation\n            x = np.clip(x, self.lower_bound, self.upper_bound)\n\n            current_f = func(x)\n            self.evaluations += 1\n\n            if current_f < best_f:\n                best_f = current_f\n                best_x = x.copy()\n                self.learning_rate *= 1.2  # Adaptive increase\n                self.sigma *= 0.9  # Reduce perturbation to focus on exploitation\n            else:\n                self.learning_rate *= 0.8  # Adaptive decrease\n                self.sigma *= 1.1  # Increase perturbation for exploration\n\n        return best_x", "name": "AdaptiveGradientStochasticSearch", "description": "Gradient-Based Stochastic Search with Adaptive Step Size that combines gradient approximation and stochasticity for effective exploration and exploitation.", "configspace": "", "generation": 1, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "cc35849e-c096-4674-a984-3fc5a6ef4f3f", "metadata": {"aucs": [0.08998073195420075, 0.06271675336939186, 0.08875131807856451, 0.10494277406268049, 0.09280018916089827, 0.09112693834094732, 0.1288638426464599, 0.06554447394544227, 0.07105907849757953, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.020477991420123853, 0.023304540353572656, 0.013580939962702043, 0.017274518788866433, 0.0, 0.008265519422535506, 0.0, 0.0, 0.0, 0.015226161305268215, 0.032874133507481695, 0.020434179529922436, 0.0, 0.011947285404927621, 0.0, 0.0, 0.0073079661356707515, 0.0, 0.9729019800082144, 0.9764282517738041, 0.9773327967574621, 0.9757235476326691, 0.9719199919945001, 0.9620231047078028, 0.9792004787568362, 0.973372573988543, 0.9755793587141087, 0.0, 0.0, 0.0, 0.001989524435141665, 0.0018281365899111002, 0.031124060486155014, 0.0, 0.0, 0.0, 0.11044192282611387, 0.10249699148985147, 0.11860360271926362, 0.017521515422513456, 0.00656962183887122, 9.999999999998899e-05, 0.062372258261358504, 9.999999999998899e-05, 0.0912849334670145, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.011752715529753877, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.1396112167348632, 0.12421783769642902, 0.14103244338215903, 0.10679315147984614, 0.1387916296901911, 0.09817732293631654, 0.12719501314661374, 0.09949075526341022, 0.11214895788266566, 0.014585865083758143, 0.009405807533463983, 0.053335511298185145, 0.0, 0.028138384732586563, 0.0, 0.0056031486469186875, 0.018761363751345517, 0.027938057908791225, 0.09289497683315007, 0.09288553633962471, 0.09298882555874755, 0.07622884985070266, 0.10339107993553331, 0.08929786078921786, 0.07852845632663452, 0.07276879660865687, 0.07677067817942385, 0.12068908612179152, 0.10742701554755874, 0.10705128967548738, 0.14344059472788317, 0.14783206959608686, 0.10784990049375187, 0.1273631117346915, 0.11922011318374748, 0.11351244162379848, 0.06656603129920147, 0.053679095474200356, 0.026691483647965253, 0.07476970549154738, 0.05594480916872069, 0.08278562995246708, 0.10285500844476925, 0.07035397692981282, 0.08764196550452963, 0.1354863540745328, 0.14252980027797435, 0.12140354094353056, 0.13547200639727486, 0.11588559432047074, 0.12475844993085139, 0.16131558609958807, 0.11883885863818211, 0.09484446786087752, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.00976667744786186, 0.14665717701174052, 0.0960797177461633, 0.0761239326581048, 0.07565325239499676, 0.05461947817712576, 0.06358383434806059, 0.08353689881996818, 0.05205108316764451, 0.07554536700328207, 0.0649850536817903, 0.02906392249683154, 0.05494107698589201, 0.018952763790558103, 0.08564980497969954, 0.10695440010782897, 0.022810256467945655, 0.0412131739677174, 0.06664696591121322, 0.09480636265193276, 0.146359147326369, 0.13533987883392762, 0.1632806877877092, 0.14996302121391492, 0.1768225021822749, 0.1716706583603329, 0.1529302770009463, 0.16372988774197073, 0.13784775839362684, 0.02942909441598729, 0.018485765204041993, 0.02215219582537198, 0.016613864082591134, 0.040229959076666044, 0.02982252856207468, 0.017234386334394336, 0.025772125113338595, 0.03213875331672589]}, "mutation_prompt": null}
{"id": "c3ad1d77-984e-4bc2-b2f4-ce93b2a08256", "solution": "import numpy as np\n\nclass AdaptiveGradientStochasticSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.evaluations = 0\n        self.learning_rate = 0.1\n        self.sigma = 0.1\n\n    def __call__(self, func):\n        x = np.random.uniform(self.lower_bound, self.upper_bound, self.dim)\n        best_x = x.copy()\n        best_f = func(best_x)\n        self.evaluations += 1\n\n        while self.evaluations < self.budget:\n            perturbation = np.random.normal(0, self.sigma, self.dim)\n            grad_approx = (func(x + perturbation) - func(x - perturbation)) / (2 * self.sigma)\n            self.evaluations += 2\n\n            x = x - self.learning_rate * grad_approx + perturbation\n            x = np.clip(x, self.lower_bound, self.upper_bound)\n\n            current_f = func(x)\n            self.evaluations += 1\n\n            if current_f < best_f:\n                best_f = current_f\n                best_x = x.copy()\n                self.learning_rate *= 1.2  # Adaptive increase\n                self.sigma *= 0.9  # Reduce perturbation to focus on exploitation\n            else:\n                self.learning_rate *= 0.8  # Adaptive decrease\n                self.sigma *= 1.1  # Increase perturbation for exploration\n\n        return best_x", "name": "AdaptiveGradientStochasticSearch", "description": "Gradient-Based Stochastic Search with Adaptive Step Size that combines gradient approximation and stochasticity for effective exploration and exploitation.", "configspace": "", "generation": 1, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "cc35849e-c096-4674-a984-3fc5a6ef4f3f", "metadata": {"aucs": [0.08998073195420075, 0.06271675336939186, 0.08875131807856451, 0.10494277406268049, 0.09280018916089827, 0.09112693834094732, 0.1288638426464599, 0.06554447394544227, 0.07105907849757953, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.020477991420123853, 0.023304540353572656, 0.013580939962702043, 0.017274518788866433, 0.0, 0.008265519422535506, 0.0, 0.0, 0.0, 0.015226161305268215, 0.032874133507481695, 0.020434179529922436, 0.0, 0.011947285404927621, 0.0, 0.0, 0.0073079661356707515, 0.0, 0.9729019800082144, 0.9764282517738041, 0.9773327967574621, 0.9757235476326691, 0.9719199919945001, 0.9620231047078028, 0.9792004787568362, 0.973372573988543, 0.9755793587141087, 0.0, 0.0, 0.0, 0.001989524435141665, 0.0018281365899111002, 0.031124060486155014, 0.0, 0.0, 0.0, 0.11044192282611387, 0.10249699148985147, 0.11860360271926362, 0.017521515422513456, 0.00656962183887122, 9.999999999998899e-05, 0.062372258261358504, 9.999999999998899e-05, 0.0912849334670145, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.011752715529753877, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.1396112167348632, 0.12421783769642902, 0.14103244338215903, 0.10679315147984614, 0.1387916296901911, 0.09817732293631654, 0.12719501314661374, 0.09949075526341022, 0.11214895788266566, 0.014585865083758143, 0.009405807533463983, 0.053335511298185145, 0.0, 0.028138384732586563, 0.0, 0.0056031486469186875, 0.018761363751345517, 0.027938057908791225, 0.09289497683315007, 0.09288553633962471, 0.09298882555874755, 0.07622884985070266, 0.10339107993553331, 0.08929786078921786, 0.07852845632663452, 0.07276879660865687, 0.07677067817942385, 0.12068908612179152, 0.10742701554755874, 0.10705128967548738, 0.14344059472788317, 0.14783206959608686, 0.10784990049375187, 0.1273631117346915, 0.11922011318374748, 0.11351244162379848, 0.06656603129920147, 0.053679095474200356, 0.026691483647965253, 0.07476970549154738, 0.05594480916872069, 0.08278562995246708, 0.10285500844476925, 0.07035397692981282, 0.08764196550452963, 0.1354863540745328, 0.14252980027797435, 0.12140354094353056, 0.13547200639727486, 0.11588559432047074, 0.12475844993085139, 0.16131558609958807, 0.11883885863818211, 0.09484446786087752, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.00976667744786186, 0.14665717701174052, 0.0960797177461633, 0.0761239326581048, 0.07565325239499676, 0.05461947817712576, 0.06358383434806059, 0.08353689881996818, 0.05205108316764451, 0.07554536700328207, 0.0649850536817903, 0.02906392249683154, 0.05494107698589201, 0.018952763790558103, 0.08564980497969954, 0.10695440010782897, 0.022810256467945655, 0.0412131739677174, 0.06664696591121322, 0.09480636265193276, 0.146359147326369, 0.13533987883392762, 0.1632806877877092, 0.14996302121391492, 0.1768225021822749, 0.1716706583603329, 0.1529302770009463, 0.16372988774197073, 0.13784775839362684, 0.02942909441598729, 0.018485765204041993, 0.02215219582537198, 0.016613864082591134, 0.040229959076666044, 0.02982252856207468, 0.017234386334394336, 0.025772125113338595, 0.03213875331672589]}, "mutation_prompt": null}
{"id": "164ac308-8865-4ec7-a4c4-bf1e32e222ac", "solution": "import numpy as np\n\nclass AdaptiveGradientStochasticSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.evaluations = 0\n        self.learning_rate = 0.1\n        self.sigma = 0.1\n\n    def __call__(self, func):\n        x = np.random.uniform(self.lower_bound, self.upper_bound, self.dim)\n        best_x = x.copy()\n        best_f = func(best_x)\n        self.evaluations += 1\n\n        while self.evaluations < self.budget:\n            perturbation = np.random.normal(0, self.sigma, self.dim)\n            grad_approx = (func(x + perturbation) - func(x - perturbation)) / (2 * self.sigma)\n            self.evaluations += 2\n\n            x = x - self.learning_rate * grad_approx + perturbation\n            x = np.clip(x, self.lower_bound, self.upper_bound)\n\n            current_f = func(x)\n            self.evaluations += 1\n\n            if current_f < best_f:\n                best_f = current_f\n                best_x = x.copy()\n                self.learning_rate *= 1.2  # Adaptive increase\n                self.sigma *= 0.9  # Reduce perturbation to focus on exploitation\n            else:\n                self.learning_rate *= 0.8  # Adaptive decrease\n                self.sigma *= 1.1  # Increase perturbation for exploration\n\n        return best_x", "name": "AdaptiveGradientStochasticSearch", "description": "Gradient-Based Stochastic Search with Adaptive Step Size that combines gradient approximation and stochasticity for effective exploration and exploitation.", "configspace": "", "generation": 1, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "cc35849e-c096-4674-a984-3fc5a6ef4f3f", "metadata": {"aucs": [0.08998073195420075, 0.06271675336939186, 0.08875131807856451, 0.10494277406268049, 0.09280018916089827, 0.09112693834094732, 0.1288638426464599, 0.06554447394544227, 0.07105907849757953, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.020477991420123853, 0.023304540353572656, 0.013580939962702043, 0.017274518788866433, 0.0, 0.008265519422535506, 0.0, 0.0, 0.0, 0.015226161305268215, 0.032874133507481695, 0.020434179529922436, 0.0, 0.011947285404927621, 0.0, 0.0, 0.0073079661356707515, 0.0, 0.9729019800082144, 0.9764282517738041, 0.9773327967574621, 0.9757235476326691, 0.9719199919945001, 0.9620231047078028, 0.9792004787568362, 0.973372573988543, 0.9755793587141087, 0.0, 0.0, 0.0, 0.001989524435141665, 0.0018281365899111002, 0.031124060486155014, 0.0, 0.0, 0.0, 0.11044192282611387, 0.10249699148985147, 0.11860360271926362, 0.017521515422513456, 0.00656962183887122, 9.999999999998899e-05, 0.062372258261358504, 9.999999999998899e-05, 0.0912849334670145, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.011752715529753877, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.1396112167348632, 0.12421783769642902, 0.14103244338215903, 0.10679315147984614, 0.1387916296901911, 0.09817732293631654, 0.12719501314661374, 0.09949075526341022, 0.11214895788266566, 0.014585865083758143, 0.009405807533463983, 0.053335511298185145, 0.0, 0.028138384732586563, 0.0, 0.0056031486469186875, 0.018761363751345517, 0.027938057908791225, 0.09289497683315007, 0.09288553633962471, 0.09298882555874755, 0.07622884985070266, 0.10339107993553331, 0.08929786078921786, 0.07852845632663452, 0.07276879660865687, 0.07677067817942385, 0.12068908612179152, 0.10742701554755874, 0.10705128967548738, 0.14344059472788317, 0.14783206959608686, 0.10784990049375187, 0.1273631117346915, 0.11922011318374748, 0.11351244162379848, 0.06656603129920147, 0.053679095474200356, 0.026691483647965253, 0.07476970549154738, 0.05594480916872069, 0.08278562995246708, 0.10285500844476925, 0.07035397692981282, 0.08764196550452963, 0.1354863540745328, 0.14252980027797435, 0.12140354094353056, 0.13547200639727486, 0.11588559432047074, 0.12475844993085139, 0.16131558609958807, 0.11883885863818211, 0.09484446786087752, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.00976667744786186, 0.14665717701174052, 0.0960797177461633, 0.0761239326581048, 0.07565325239499676, 0.05461947817712576, 0.06358383434806059, 0.08353689881996818, 0.05205108316764451, 0.07554536700328207, 0.0649850536817903, 0.02906392249683154, 0.05494107698589201, 0.018952763790558103, 0.08564980497969954, 0.10695440010782897, 0.022810256467945655, 0.0412131739677174, 0.06664696591121322, 0.09480636265193276, 0.146359147326369, 0.13533987883392762, 0.1632806877877092, 0.14996302121391492, 0.1768225021822749, 0.1716706583603329, 0.1529302770009463, 0.16372988774197073, 0.13784775839362684, 0.02942909441598729, 0.018485765204041993, 0.02215219582537198, 0.016613864082591134, 0.040229959076666044, 0.02982252856207468, 0.017234386334394336, 0.025772125113338595, 0.03213875331672589]}, "mutation_prompt": null}
{"id": "721b8858-74d3-4e48-82f4-570cdd06c605", "solution": "import numpy as np\n\nclass AdaptiveGradientStochasticSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.evaluations = 0\n        self.learning_rate = 0.1\n        self.sigma = 0.1\n\n    def __call__(self, func):\n        x = np.random.uniform(self.lower_bound, self.upper_bound, self.dim)\n        best_x = x.copy()\n        best_f = func(best_x)\n        self.evaluations += 1\n\n        while self.evaluations < self.budget:\n            perturbation = np.random.normal(0, self.sigma, self.dim)\n            grad_approx = (func(x + perturbation) - func(x - perturbation)) / (2 * self.sigma)\n            self.evaluations += 2\n\n            x = x - self.learning_rate * grad_approx + perturbation\n            x = np.clip(x, self.lower_bound, self.upper_bound)\n\n            current_f = func(x)\n            self.evaluations += 1\n\n            if current_f < best_f:\n                best_f = current_f\n                best_x = x.copy()\n                self.learning_rate *= 1.2  # Adaptive increase\n                self.sigma *= 0.9  # Reduce perturbation to focus on exploitation\n            else:\n                self.learning_rate *= 0.8  # Adaptive decrease\n                self.sigma *= 1.1  # Increase perturbation for exploration\n\n        return best_x", "name": "AdaptiveGradientStochasticSearch", "description": "Gradient-Based Stochastic Search with Adaptive Step Size that combines gradient approximation and stochasticity for effective exploration and exploitation.", "configspace": "", "generation": 1, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "cc35849e-c096-4674-a984-3fc5a6ef4f3f", "metadata": {"aucs": [0.08998073195420075, 0.06271675336939186, 0.08875131807856451, 0.10494277406268049, 0.09280018916089827, 0.09112693834094732, 0.1288638426464599, 0.06554447394544227, 0.07105907849757953, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.020477991420123853, 0.023304540353572656, 0.013580939962702043, 0.017274518788866433, 0.0, 0.008265519422535506, 0.0, 0.0, 0.0, 0.015226161305268215, 0.032874133507481695, 0.020434179529922436, 0.0, 0.011947285404927621, 0.0, 0.0, 0.0073079661356707515, 0.0, 0.9729019800082144, 0.9764282517738041, 0.9773327967574621, 0.9757235476326691, 0.9719199919945001, 0.9620231047078028, 0.9792004787568362, 0.973372573988543, 0.9755793587141087, 0.0, 0.0, 0.0, 0.001989524435141665, 0.0018281365899111002, 0.031124060486155014, 0.0, 0.0, 0.0, 0.11044192282611387, 0.10249699148985147, 0.11860360271926362, 0.017521515422513456, 0.00656962183887122, 9.999999999998899e-05, 0.062372258261358504, 9.999999999998899e-05, 0.0912849334670145, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.011752715529753877, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.1396112167348632, 0.12421783769642902, 0.14103244338215903, 0.10679315147984614, 0.1387916296901911, 0.09817732293631654, 0.12719501314661374, 0.09949075526341022, 0.11214895788266566, 0.014585865083758143, 0.009405807533463983, 0.053335511298185145, 0.0, 0.028138384732586563, 0.0, 0.0056031486469186875, 0.018761363751345517, 0.027938057908791225, 0.09289497683315007, 0.09288553633962471, 0.09298882555874755, 0.07622884985070266, 0.10339107993553331, 0.08929786078921786, 0.07852845632663452, 0.07276879660865687, 0.07677067817942385, 0.12068908612179152, 0.10742701554755874, 0.10705128967548738, 0.14344059472788317, 0.14783206959608686, 0.10784990049375187, 0.1273631117346915, 0.11922011318374748, 0.11351244162379848, 0.06656603129920147, 0.053679095474200356, 0.026691483647965253, 0.07476970549154738, 0.05594480916872069, 0.08278562995246708, 0.10285500844476925, 0.07035397692981282, 0.08764196550452963, 0.1354863540745328, 0.14252980027797435, 0.12140354094353056, 0.13547200639727486, 0.11588559432047074, 0.12475844993085139, 0.16131558609958807, 0.11883885863818211, 0.09484446786087752, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.00976667744786186, 0.14665717701174052, 0.0960797177461633, 0.0761239326581048, 0.07565325239499676, 0.05461947817712576, 0.06358383434806059, 0.08353689881996818, 0.05205108316764451, 0.07554536700328207, 0.0649850536817903, 0.02906392249683154, 0.05494107698589201, 0.018952763790558103, 0.08564980497969954, 0.10695440010782897, 0.022810256467945655, 0.0412131739677174, 0.06664696591121322, 0.09480636265193276, 0.146359147326369, 0.13533987883392762, 0.1632806877877092, 0.14996302121391492, 0.1768225021822749, 0.1716706583603329, 0.1529302770009463, 0.16372988774197073, 0.13784775839362684, 0.02942909441598729, 0.018485765204041993, 0.02215219582537198, 0.016613864082591134, 0.040229959076666044, 0.02982252856207468, 0.017234386334394336, 0.025772125113338595, 0.03213875331672589]}, "mutation_prompt": null}
{"id": "c54b76fb-061c-445c-aaf9-0ea099707702", "solution": "import numpy as np\n\nclass AdaptiveGradientStochasticSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.evaluations = 0\n        self.learning_rate = 0.1\n        self.sigma = 0.1\n\n    def __call__(self, func):\n        x = np.random.uniform(self.lower_bound, self.upper_bound, self.dim)\n        best_x = x.copy()\n        best_f = func(best_x)\n        self.evaluations += 1\n\n        while self.evaluations < self.budget:\n            perturbation = np.random.normal(0, self.sigma, self.dim)\n            grad_approx = (func(x + perturbation) - func(x - perturbation)) / (2 * self.sigma)\n            self.evaluations += 2\n\n            x = x - self.learning_rate * grad_approx + perturbation\n            x = np.clip(x, self.lower_bound, self.upper_bound)\n\n            current_f = func(x)\n            self.evaluations += 1\n\n            if current_f < best_f:\n                best_f = current_f\n                best_x = x.copy()\n                self.learning_rate *= 1.2  # Adaptive increase\n                self.sigma *= 0.9  # Reduce perturbation to focus on exploitation\n            else:\n                self.learning_rate *= 0.8  # Adaptive decrease\n                self.sigma *= 1.1  # Increase perturbation for exploration\n\n        return best_x", "name": "AdaptiveGradientStochasticSearch", "description": "Gradient-Based Stochastic Search with Adaptive Step Size that combines gradient approximation and stochasticity for effective exploration and exploitation.", "configspace": "", "generation": 1, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "cc35849e-c096-4674-a984-3fc5a6ef4f3f", "metadata": {"aucs": [0.08998073195420075, 0.06271675336939186, 0.08875131807856451, 0.10494277406268049, 0.09280018916089827, 0.09112693834094732, 0.1288638426464599, 0.06554447394544227, 0.07105907849757953, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.020477991420123853, 0.023304540353572656, 0.013580939962702043, 0.017274518788866433, 0.0, 0.008265519422535506, 0.0, 0.0, 0.0, 0.015226161305268215, 0.032874133507481695, 0.020434179529922436, 0.0, 0.011947285404927621, 0.0, 0.0, 0.0073079661356707515, 0.0, 0.9729019800082144, 0.9764282517738041, 0.9773327967574621, 0.9757235476326691, 0.9719199919945001, 0.9620231047078028, 0.9792004787568362, 0.973372573988543, 0.9755793587141087, 0.0, 0.0, 0.0, 0.001989524435141665, 0.0018281365899111002, 0.031124060486155014, 0.0, 0.0, 0.0, 0.11044192282611387, 0.10249699148985147, 0.11860360271926362, 0.017521515422513456, 0.00656962183887122, 9.999999999998899e-05, 0.062372258261358504, 9.999999999998899e-05, 0.0912849334670145, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.011752715529753877, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.1396112167348632, 0.12421783769642902, 0.14103244338215903, 0.10679315147984614, 0.1387916296901911, 0.09817732293631654, 0.12719501314661374, 0.09949075526341022, 0.11214895788266566, 0.014585865083758143, 0.009405807533463983, 0.053335511298185145, 0.0, 0.028138384732586563, 0.0, 0.0056031486469186875, 0.018761363751345517, 0.027938057908791225, 0.09289497683315007, 0.09288553633962471, 0.09298882555874755, 0.07622884985070266, 0.10339107993553331, 0.08929786078921786, 0.07852845632663452, 0.07276879660865687, 0.07677067817942385, 0.12068908612179152, 0.10742701554755874, 0.10705128967548738, 0.14344059472788317, 0.14783206959608686, 0.10784990049375187, 0.1273631117346915, 0.11922011318374748, 0.11351244162379848, 0.06656603129920147, 0.053679095474200356, 0.026691483647965253, 0.07476970549154738, 0.05594480916872069, 0.08278562995246708, 0.10285500844476925, 0.07035397692981282, 0.08764196550452963, 0.1354863540745328, 0.14252980027797435, 0.12140354094353056, 0.13547200639727486, 0.11588559432047074, 0.12475844993085139, 0.16131558609958807, 0.11883885863818211, 0.09484446786087752, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.00976667744786186, 0.14665717701174052, 0.0960797177461633, 0.0761239326581048, 0.07565325239499676, 0.05461947817712576, 0.06358383434806059, 0.08353689881996818, 0.05205108316764451, 0.07554536700328207, 0.0649850536817903, 0.02906392249683154, 0.05494107698589201, 0.018952763790558103, 0.08564980497969954, 0.10695440010782897, 0.022810256467945655, 0.0412131739677174, 0.06664696591121322, 0.09480636265193276, 0.146359147326369, 0.13533987883392762, 0.1632806877877092, 0.14996302121391492, 0.1768225021822749, 0.1716706583603329, 0.1529302770009463, 0.16372988774197073, 0.13784775839362684, 0.02942909441598729, 0.018485765204041993, 0.02215219582537198, 0.016613864082591134, 0.040229959076666044, 0.02982252856207468, 0.017234386334394336, 0.025772125113338595, 0.03213875331672589]}, "mutation_prompt": null}
{"id": "fd0c9d83-789d-46f4-a7e8-e8d5b66e8717", "solution": "import numpy as np\n\nclass AdaptiveGradientStochasticSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.evaluations = 0\n        self.learning_rate = 0.1\n        self.sigma = 0.1\n\n    def __call__(self, func):\n        x = np.random.uniform(self.lower_bound, self.upper_bound, self.dim)\n        best_x = x.copy()\n        best_f = func(best_x)\n        self.evaluations += 1\n\n        while self.evaluations < self.budget:\n            perturbation = np.random.normal(0, self.sigma, self.dim)\n            grad_approx = (func(x + perturbation) - func(x - perturbation)) / (2 * self.sigma)\n            self.evaluations += 2\n\n            x = x - self.learning_rate * grad_approx + perturbation\n            x = np.clip(x, self.lower_bound, self.upper_bound)\n\n            current_f = func(x)\n            self.evaluations += 1\n\n            if current_f < best_f:\n                best_f = current_f\n                best_x = x.copy()\n                self.learning_rate *= 1.2  # Adaptive increase\n                self.sigma *= 0.9  # Reduce perturbation to focus on exploitation\n            else:\n                self.learning_rate *= 0.8  # Adaptive decrease\n                self.sigma *= 1.1  # Increase perturbation for exploration\n\n        return best_x", "name": "AdaptiveGradientStochasticSearch", "description": "Gradient-Based Stochastic Search with Adaptive Step Size that combines gradient approximation and stochasticity for effective exploration and exploitation.", "configspace": "", "generation": 1, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "cc35849e-c096-4674-a984-3fc5a6ef4f3f", "metadata": {"aucs": [0.08998073195420075, 0.06271675336939186, 0.08875131807856451, 0.10494277406268049, 0.09280018916089827, 0.09112693834094732, 0.1288638426464599, 0.06554447394544227, 0.07105907849757953, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.020477991420123853, 0.023304540353572656, 0.013580939962702043, 0.017274518788866433, 0.0, 0.008265519422535506, 0.0, 0.0, 0.0, 0.015226161305268215, 0.032874133507481695, 0.020434179529922436, 0.0, 0.011947285404927621, 0.0, 0.0, 0.0073079661356707515, 0.0, 0.9729019800082144, 0.9764282517738041, 0.9773327967574621, 0.9757235476326691, 0.9719199919945001, 0.9620231047078028, 0.9792004787568362, 0.973372573988543, 0.9755793587141087, 0.0, 0.0, 0.0, 0.001989524435141665, 0.0018281365899111002, 0.031124060486155014, 0.0, 0.0, 0.0, 0.11044192282611387, 0.10249699148985147, 0.11860360271926362, 0.017521515422513456, 0.00656962183887122, 9.999999999998899e-05, 0.062372258261358504, 9.999999999998899e-05, 0.0912849334670145, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.011752715529753877, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.1396112167348632, 0.12421783769642902, 0.14103244338215903, 0.10679315147984614, 0.1387916296901911, 0.09817732293631654, 0.12719501314661374, 0.09949075526341022, 0.11214895788266566, 0.014585865083758143, 0.009405807533463983, 0.053335511298185145, 0.0, 0.028138384732586563, 0.0, 0.0056031486469186875, 0.018761363751345517, 0.027938057908791225, 0.09289497683315007, 0.09288553633962471, 0.09298882555874755, 0.07622884985070266, 0.10339107993553331, 0.08929786078921786, 0.07852845632663452, 0.07276879660865687, 0.07677067817942385, 0.12068908612179152, 0.10742701554755874, 0.10705128967548738, 0.14344059472788317, 0.14783206959608686, 0.10784990049375187, 0.1273631117346915, 0.11922011318374748, 0.11351244162379848, 0.06656603129920147, 0.053679095474200356, 0.026691483647965253, 0.07476970549154738, 0.05594480916872069, 0.08278562995246708, 0.10285500844476925, 0.07035397692981282, 0.08764196550452963, 0.1354863540745328, 0.14252980027797435, 0.12140354094353056, 0.13547200639727486, 0.11588559432047074, 0.12475844993085139, 0.16131558609958807, 0.11883885863818211, 0.09484446786087752, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.00976667744786186, 0.14665717701174052, 0.0960797177461633, 0.0761239326581048, 0.07565325239499676, 0.05461947817712576, 0.06358383434806059, 0.08353689881996818, 0.05205108316764451, 0.07554536700328207, 0.0649850536817903, 0.02906392249683154, 0.05494107698589201, 0.018952763790558103, 0.08564980497969954, 0.10695440010782897, 0.022810256467945655, 0.0412131739677174, 0.06664696591121322, 0.09480636265193276, 0.146359147326369, 0.13533987883392762, 0.1632806877877092, 0.14996302121391492, 0.1768225021822749, 0.1716706583603329, 0.1529302770009463, 0.16372988774197073, 0.13784775839362684, 0.02942909441598729, 0.018485765204041993, 0.02215219582537198, 0.016613864082591134, 0.040229959076666044, 0.02982252856207468, 0.017234386334394336, 0.025772125113338595, 0.03213875331672589]}, "mutation_prompt": null}
{"id": "7603c7fc-ac26-457e-9c9f-63f18f62aa46", "solution": "import numpy as np\n\nclass AdaptiveGradientStochasticSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.evaluations = 0\n        self.learning_rate = 0.1\n        self.sigma = 0.1\n\n    def __call__(self, func):\n        x = np.random.uniform(self.lower_bound, self.upper_bound, self.dim)\n        best_x = x.copy()\n        best_f = func(best_x)\n        self.evaluations += 1\n\n        while self.evaluations < self.budget:\n            perturbation = np.random.normal(0, self.sigma, self.dim)\n            grad_approx = (func(x + perturbation) - func(x - perturbation)) / (2 * self.sigma)\n            self.evaluations += 2\n\n            x = x - self.learning_rate * grad_approx + perturbation\n            x = np.clip(x, self.lower_bound, self.upper_bound)\n\n            current_f = func(x)\n            self.evaluations += 1\n\n            if current_f < best_f:\n                best_f = current_f\n                best_x = x.copy()\n                self.learning_rate *= 1.2  # Adaptive increase\n                self.sigma *= 0.9  # Reduce perturbation to focus on exploitation\n            else:\n                self.learning_rate *= 0.8  # Adaptive decrease\n                self.sigma *= 1.1  # Increase perturbation for exploration\n\n        return best_x", "name": "AdaptiveGradientStochasticSearch", "description": "Gradient-Based Stochastic Search with Adaptive Step Size that combines gradient approximation and stochasticity for effective exploration and exploitation.", "configspace": "", "generation": 1, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "cc35849e-c096-4674-a984-3fc5a6ef4f3f", "metadata": {"aucs": [0.08998073195420075, 0.06271675336939186, 0.08875131807856451, 0.10494277406268049, 0.09280018916089827, 0.09112693834094732, 0.1288638426464599, 0.06554447394544227, 0.07105907849757953, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.020477991420123853, 0.023304540353572656, 0.013580939962702043, 0.017274518788866433, 0.0, 0.008265519422535506, 0.0, 0.0, 0.0, 0.015226161305268215, 0.032874133507481695, 0.020434179529922436, 0.0, 0.011947285404927621, 0.0, 0.0, 0.0073079661356707515, 0.0, 0.9729019800082144, 0.9764282517738041, 0.9773327967574621, 0.9757235476326691, 0.9719199919945001, 0.9620231047078028, 0.9792004787568362, 0.973372573988543, 0.9755793587141087, 0.0, 0.0, 0.0, 0.001989524435141665, 0.0018281365899111002, 0.031124060486155014, 0.0, 0.0, 0.0, 0.11044192282611387, 0.10249699148985147, 0.11860360271926362, 0.017521515422513456, 0.00656962183887122, 9.999999999998899e-05, 0.062372258261358504, 9.999999999998899e-05, 0.0912849334670145, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.011752715529753877, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.1396112167348632, 0.12421783769642902, 0.14103244338215903, 0.10679315147984614, 0.1387916296901911, 0.09817732293631654, 0.12719501314661374, 0.09949075526341022, 0.11214895788266566, 0.014585865083758143, 0.009405807533463983, 0.053335511298185145, 0.0, 0.028138384732586563, 0.0, 0.0056031486469186875, 0.018761363751345517, 0.027938057908791225, 0.09289497683315007, 0.09288553633962471, 0.09298882555874755, 0.07622884985070266, 0.10339107993553331, 0.08929786078921786, 0.07852845632663452, 0.07276879660865687, 0.07677067817942385, 0.12068908612179152, 0.10742701554755874, 0.10705128967548738, 0.14344059472788317, 0.14783206959608686, 0.10784990049375187, 0.1273631117346915, 0.11922011318374748, 0.11351244162379848, 0.06656603129920147, 0.053679095474200356, 0.026691483647965253, 0.07476970549154738, 0.05594480916872069, 0.08278562995246708, 0.10285500844476925, 0.07035397692981282, 0.08764196550452963, 0.1354863540745328, 0.14252980027797435, 0.12140354094353056, 0.13547200639727486, 0.11588559432047074, 0.12475844993085139, 0.16131558609958807, 0.11883885863818211, 0.09484446786087752, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.00976667744786186, 0.14665717701174052, 0.0960797177461633, 0.0761239326581048, 0.07565325239499676, 0.05461947817712576, 0.06358383434806059, 0.08353689881996818, 0.05205108316764451, 0.07554536700328207, 0.0649850536817903, 0.02906392249683154, 0.05494107698589201, 0.018952763790558103, 0.08564980497969954, 0.10695440010782897, 0.022810256467945655, 0.0412131739677174, 0.06664696591121322, 0.09480636265193276, 0.146359147326369, 0.13533987883392762, 0.1632806877877092, 0.14996302121391492, 0.1768225021822749, 0.1716706583603329, 0.1529302770009463, 0.16372988774197073, 0.13784775839362684, 0.02942909441598729, 0.018485765204041993, 0.02215219582537198, 0.016613864082591134, 0.040229959076666044, 0.02982252856207468, 0.017234386334394336, 0.025772125113338595, 0.03213875331672589]}, "mutation_prompt": null}
{"id": "7e0b665e-2fab-4357-8df0-84ea7a30e476", "solution": "import numpy as np\n\nclass AdaptiveGradientStochasticSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.evaluations = 0\n        self.learning_rate = 0.1\n        self.sigma = 0.1\n\n    def __call__(self, func):\n        x = np.random.uniform(self.lower_bound, self.upper_bound, self.dim)\n        best_x = x.copy()\n        best_f = func(best_x)\n        self.evaluations += 1\n\n        while self.evaluations < self.budget:\n            perturbation = np.random.normal(0, self.sigma, self.dim)\n            grad_approx = (func(x + perturbation) - func(x - perturbation)) / (2 * self.sigma)\n            self.evaluations += 2\n\n            x = x - self.learning_rate * grad_approx + perturbation\n            x = np.clip(x, self.lower_bound, self.upper_bound)\n\n            current_f = func(x)\n            self.evaluations += 1\n\n            if current_f < best_f:\n                best_f = current_f\n                best_x = x.copy()\n                self.learning_rate *= 1.2  # Adaptive increase\n                self.sigma *= 0.9  # Reduce perturbation to focus on exploitation\n            else:\n                self.learning_rate *= 0.8  # Adaptive decrease\n                self.sigma *= 1.1  # Increase perturbation for exploration\n\n        return best_x", "name": "AdaptiveGradientStochasticSearch", "description": "Gradient-Based Stochastic Search with Adaptive Step Size that combines gradient approximation and stochasticity for effective exploration and exploitation.", "configspace": "", "generation": 1, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "cc35849e-c096-4674-a984-3fc5a6ef4f3f", "metadata": {"aucs": [0.08998073195420075, 0.06271675336939186, 0.08875131807856451, 0.10494277406268049, 0.09280018916089827, 0.09112693834094732, 0.1288638426464599, 0.06554447394544227, 0.07105907849757953, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.020477991420123853, 0.023304540353572656, 0.013580939962702043, 0.017274518788866433, 0.0, 0.008265519422535506, 0.0, 0.0, 0.0, 0.015226161305268215, 0.032874133507481695, 0.020434179529922436, 0.0, 0.011947285404927621, 0.0, 0.0, 0.0073079661356707515, 0.0, 0.9729019800082144, 0.9764282517738041, 0.9773327967574621, 0.9757235476326691, 0.9719199919945001, 0.9620231047078028, 0.9792004787568362, 0.973372573988543, 0.9755793587141087, 0.0, 0.0, 0.0, 0.001989524435141665, 0.0018281365899111002, 0.031124060486155014, 0.0, 0.0, 0.0, 0.11044192282611387, 0.10249699148985147, 0.11860360271926362, 0.017521515422513456, 0.00656962183887122, 9.999999999998899e-05, 0.062372258261358504, 9.999999999998899e-05, 0.0912849334670145, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.011752715529753877, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.1396112167348632, 0.12421783769642902, 0.14103244338215903, 0.10679315147984614, 0.1387916296901911, 0.09817732293631654, 0.12719501314661374, 0.09949075526341022, 0.11214895788266566, 0.014585865083758143, 0.009405807533463983, 0.053335511298185145, 0.0, 0.028138384732586563, 0.0, 0.0056031486469186875, 0.018761363751345517, 0.027938057908791225, 0.09289497683315007, 0.09288553633962471, 0.09298882555874755, 0.07622884985070266, 0.10339107993553331, 0.08929786078921786, 0.07852845632663452, 0.07276879660865687, 0.07677067817942385, 0.12068908612179152, 0.10742701554755874, 0.10705128967548738, 0.14344059472788317, 0.14783206959608686, 0.10784990049375187, 0.1273631117346915, 0.11922011318374748, 0.11351244162379848, 0.06656603129920147, 0.053679095474200356, 0.026691483647965253, 0.07476970549154738, 0.05594480916872069, 0.08278562995246708, 0.10285500844476925, 0.07035397692981282, 0.08764196550452963, 0.1354863540745328, 0.14252980027797435, 0.12140354094353056, 0.13547200639727486, 0.11588559432047074, 0.12475844993085139, 0.16131558609958807, 0.11883885863818211, 0.09484446786087752, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.00976667744786186, 0.14665717701174052, 0.0960797177461633, 0.0761239326581048, 0.07565325239499676, 0.05461947817712576, 0.06358383434806059, 0.08353689881996818, 0.05205108316764451, 0.07554536700328207, 0.0649850536817903, 0.02906392249683154, 0.05494107698589201, 0.018952763790558103, 0.08564980497969954, 0.10695440010782897, 0.022810256467945655, 0.0412131739677174, 0.06664696591121322, 0.09480636265193276, 0.146359147326369, 0.13533987883392762, 0.1632806877877092, 0.14996302121391492, 0.1768225021822749, 0.1716706583603329, 0.1529302770009463, 0.16372988774197073, 0.13784775839362684, 0.02942909441598729, 0.018485765204041993, 0.02215219582537198, 0.016613864082591134, 0.040229959076666044, 0.02982252856207468, 0.017234386334394336, 0.025772125113338595, 0.03213875331672589]}, "mutation_prompt": null}
{"id": "083e0e60-674f-48a4-b639-9e4c2238b5b4", "solution": "import numpy as np\n\nclass AdaptiveGradientStochasticSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.evaluations = 0\n        self.learning_rate = 0.1\n        self.sigma = 0.1\n\n    def __call__(self, func):\n        x = np.random.uniform(self.lower_bound, self.upper_bound, self.dim)\n        best_x = x.copy()\n        best_f = func(best_x)\n        self.evaluations += 1\n\n        while self.evaluations < self.budget:\n            perturbation = np.random.normal(0, self.sigma, self.dim)\n            grad_approx = (func(x + perturbation) - func(x - perturbation)) / (2 * self.sigma)\n            self.evaluations += 2\n\n            x = x - self.learning_rate * grad_approx + perturbation\n            x = np.clip(x, self.lower_bound, self.upper_bound)\n\n            current_f = func(x)\n            self.evaluations += 1\n\n            if current_f < best_f:\n                best_f = current_f\n                best_x = x.copy()\n                self.learning_rate *= 1.2  # Adaptive increase\n                self.sigma *= 0.9  # Reduce perturbation to focus on exploitation\n            else:\n                self.learning_rate *= 0.8  # Adaptive decrease\n                self.sigma *= 1.1  # Increase perturbation for exploration\n\n        return best_x", "name": "AdaptiveGradientStochasticSearch", "description": "Gradient-Based Stochastic Search with Adaptive Step Size that combines gradient approximation and stochasticity for effective exploration and exploitation.", "configspace": "", "generation": 1, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "cc35849e-c096-4674-a984-3fc5a6ef4f3f", "metadata": {"aucs": [0.08998073195420075, 0.06271675336939186, 0.08875131807856451, 0.10494277406268049, 0.09280018916089827, 0.09112693834094732, 0.1288638426464599, 0.06554447394544227, 0.07105907849757953, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.020477991420123853, 0.023304540353572656, 0.013580939962702043, 0.017274518788866433, 0.0, 0.008265519422535506, 0.0, 0.0, 0.0, 0.015226161305268215, 0.032874133507481695, 0.020434179529922436, 0.0, 0.011947285404927621, 0.0, 0.0, 0.0073079661356707515, 0.0, 0.9729019800082144, 0.9764282517738041, 0.9773327967574621, 0.9757235476326691, 0.9719199919945001, 0.9620231047078028, 0.9792004787568362, 0.973372573988543, 0.9755793587141087, 0.0, 0.0, 0.0, 0.001989524435141665, 0.0018281365899111002, 0.031124060486155014, 0.0, 0.0, 0.0, 0.11044192282611387, 0.10249699148985147, 0.11860360271926362, 0.017521515422513456, 0.00656962183887122, 9.999999999998899e-05, 0.062372258261358504, 9.999999999998899e-05, 0.0912849334670145, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.011752715529753877, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.1396112167348632, 0.12421783769642902, 0.14103244338215903, 0.10679315147984614, 0.1387916296901911, 0.09817732293631654, 0.12719501314661374, 0.09949075526341022, 0.11214895788266566, 0.014585865083758143, 0.009405807533463983, 0.053335511298185145, 0.0, 0.028138384732586563, 0.0, 0.0056031486469186875, 0.018761363751345517, 0.027938057908791225, 0.09289497683315007, 0.09288553633962471, 0.09298882555874755, 0.07622884985070266, 0.10339107993553331, 0.08929786078921786, 0.07852845632663452, 0.07276879660865687, 0.07677067817942385, 0.12068908612179152, 0.10742701554755874, 0.10705128967548738, 0.14344059472788317, 0.14783206959608686, 0.10784990049375187, 0.1273631117346915, 0.11922011318374748, 0.11351244162379848, 0.06656603129920147, 0.053679095474200356, 0.026691483647965253, 0.07476970549154738, 0.05594480916872069, 0.08278562995246708, 0.10285500844476925, 0.07035397692981282, 0.08764196550452963, 0.1354863540745328, 0.14252980027797435, 0.12140354094353056, 0.13547200639727486, 0.11588559432047074, 0.12475844993085139, 0.16131558609958807, 0.11883885863818211, 0.09484446786087752, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.00976667744786186, 0.14665717701174052, 0.0960797177461633, 0.0761239326581048, 0.07565325239499676, 0.05461947817712576, 0.06358383434806059, 0.08353689881996818, 0.05205108316764451, 0.07554536700328207, 0.0649850536817903, 0.02906392249683154, 0.05494107698589201, 0.018952763790558103, 0.08564980497969954, 0.10695440010782897, 0.022810256467945655, 0.0412131739677174, 0.06664696591121322, 0.09480636265193276, 0.146359147326369, 0.13533987883392762, 0.1632806877877092, 0.14996302121391492, 0.1768225021822749, 0.1716706583603329, 0.1529302770009463, 0.16372988774197073, 0.13784775839362684, 0.02942909441598729, 0.018485765204041993, 0.02215219582537198, 0.016613864082591134, 0.040229959076666044, 0.02982252856207468, 0.017234386334394336, 0.025772125113338595, 0.03213875331672589]}, "mutation_prompt": null}
{"id": "cf2bbe50-a030-4587-9ddf-39de57fde7da", "solution": "import numpy as np\n\nclass AdaptiveGradientStochasticSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.evaluations = 0\n        self.learning_rate = 0.1\n        self.sigma = 0.1\n\n    def __call__(self, func):\n        x = np.random.uniform(self.lower_bound, self.upper_bound, self.dim)\n        best_x = x.copy()\n        best_f = func(best_x)\n        self.evaluations += 1\n\n        while self.evaluations < self.budget:\n            perturbation = np.random.normal(0, self.sigma, self.dim)\n            grad_approx = (func(x + perturbation) - func(x - perturbation)) / (2 * self.sigma)\n            self.evaluations += 2\n\n            x = x - self.learning_rate * grad_approx + perturbation\n            x = np.clip(x, self.lower_bound, self.upper_bound)\n\n            current_f = func(x)\n            self.evaluations += 1\n\n            if current_f < best_f:\n                best_f = current_f\n                best_x = x.copy()\n                self.learning_rate *= 1.2  # Adaptive increase\n                self.sigma *= 0.9  # Reduce perturbation to focus on exploitation\n            else:\n                self.learning_rate *= 0.8  # Adaptive decrease\n                self.sigma *= 1.1  # Increase perturbation for exploration\n\n        return best_x", "name": "AdaptiveGradientStochasticSearch", "description": "Gradient-Based Stochastic Search with Adaptive Step Size that combines gradient approximation and stochasticity for effective exploration and exploitation.", "configspace": "", "generation": 1, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "cc35849e-c096-4674-a984-3fc5a6ef4f3f", "metadata": {"aucs": [0.08998073195420075, 0.06271675336939186, 0.08875131807856451, 0.10494277406268049, 0.09280018916089827, 0.09112693834094732, 0.1288638426464599, 0.06554447394544227, 0.07105907849757953, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.020477991420123853, 0.023304540353572656, 0.013580939962702043, 0.017274518788866433, 0.0, 0.008265519422535506, 0.0, 0.0, 0.0, 0.015226161305268215, 0.032874133507481695, 0.020434179529922436, 0.0, 0.011947285404927621, 0.0, 0.0, 0.0073079661356707515, 0.0, 0.9729019800082144, 0.9764282517738041, 0.9773327967574621, 0.9757235476326691, 0.9719199919945001, 0.9620231047078028, 0.9792004787568362, 0.973372573988543, 0.9755793587141087, 0.0, 0.0, 0.0, 0.001989524435141665, 0.0018281365899111002, 0.031124060486155014, 0.0, 0.0, 0.0, 0.11044192282611387, 0.10249699148985147, 0.11860360271926362, 0.017521515422513456, 0.00656962183887122, 9.999999999998899e-05, 0.062372258261358504, 9.999999999998899e-05, 0.0912849334670145, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.011752715529753877, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.1396112167348632, 0.12421783769642902, 0.14103244338215903, 0.10679315147984614, 0.1387916296901911, 0.09817732293631654, 0.12719501314661374, 0.09949075526341022, 0.11214895788266566, 0.014585865083758143, 0.009405807533463983, 0.053335511298185145, 0.0, 0.028138384732586563, 0.0, 0.0056031486469186875, 0.018761363751345517, 0.027938057908791225, 0.09289497683315007, 0.09288553633962471, 0.09298882555874755, 0.07622884985070266, 0.10339107993553331, 0.08929786078921786, 0.07852845632663452, 0.07276879660865687, 0.07677067817942385, 0.12068908612179152, 0.10742701554755874, 0.10705128967548738, 0.14344059472788317, 0.14783206959608686, 0.10784990049375187, 0.1273631117346915, 0.11922011318374748, 0.11351244162379848, 0.06656603129920147, 0.053679095474200356, 0.026691483647965253, 0.07476970549154738, 0.05594480916872069, 0.08278562995246708, 0.10285500844476925, 0.07035397692981282, 0.08764196550452963, 0.1354863540745328, 0.14252980027797435, 0.12140354094353056, 0.13547200639727486, 0.11588559432047074, 0.12475844993085139, 0.16131558609958807, 0.11883885863818211, 0.09484446786087752, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.00976667744786186, 0.14665717701174052, 0.0960797177461633, 0.0761239326581048, 0.07565325239499676, 0.05461947817712576, 0.06358383434806059, 0.08353689881996818, 0.05205108316764451, 0.07554536700328207, 0.0649850536817903, 0.02906392249683154, 0.05494107698589201, 0.018952763790558103, 0.08564980497969954, 0.10695440010782897, 0.022810256467945655, 0.0412131739677174, 0.06664696591121322, 0.09480636265193276, 0.146359147326369, 0.13533987883392762, 0.1632806877877092, 0.14996302121391492, 0.1768225021822749, 0.1716706583603329, 0.1529302770009463, 0.16372988774197073, 0.13784775839362684, 0.02942909441598729, 0.018485765204041993, 0.02215219582537198, 0.016613864082591134, 0.040229959076666044, 0.02982252856207468, 0.017234386334394336, 0.025772125113338595, 0.03213875331672589]}, "mutation_prompt": null}
{"id": "b1518d0c-7cff-4727-9800-05cd1eede2a9", "solution": "import numpy as np\n\nclass AdaptiveGradientStochasticSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.evaluations = 0\n        self.learning_rate = 0.1\n        self.sigma = 0.1\n\n    def __call__(self, func):\n        x = np.random.uniform(self.lower_bound, self.upper_bound, self.dim)\n        best_x = x.copy()\n        best_f = func(best_x)\n        self.evaluations += 1\n\n        while self.evaluations < self.budget:\n            perturbation = np.random.normal(0, self.sigma, self.dim)\n            grad_approx = (func(x + perturbation) - func(x - perturbation)) / (2 * self.sigma)\n            self.evaluations += 2\n\n            x = x - self.learning_rate * grad_approx + perturbation\n            x = np.clip(x, self.lower_bound, self.upper_bound)\n\n            current_f = func(x)\n            self.evaluations += 1\n\n            if current_f < best_f:\n                best_f = current_f\n                best_x = x.copy()\n                self.learning_rate *= 1.2  # Adaptive increase\n                self.sigma *= 0.9  # Reduce perturbation to focus on exploitation\n            else:\n                self.learning_rate *= 0.8  # Adaptive decrease\n                self.sigma *= 1.1  # Increase perturbation for exploration\n\n        return best_x", "name": "AdaptiveGradientStochasticSearch", "description": "Gradient-Based Stochastic Search with Adaptive Step Size that combines gradient approximation and stochasticity for effective exploration and exploitation.", "configspace": "", "generation": 1, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "cc35849e-c096-4674-a984-3fc5a6ef4f3f", "metadata": {"aucs": [0.08998073195420075, 0.06271675336939186, 0.08875131807856451, 0.10494277406268049, 0.09280018916089827, 0.09112693834094732, 0.1288638426464599, 0.06554447394544227, 0.07105907849757953, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.020477991420123853, 0.023304540353572656, 0.013580939962702043, 0.017274518788866433, 0.0, 0.008265519422535506, 0.0, 0.0, 0.0, 0.015226161305268215, 0.032874133507481695, 0.020434179529922436, 0.0, 0.011947285404927621, 0.0, 0.0, 0.0073079661356707515, 0.0, 0.9729019800082144, 0.9764282517738041, 0.9773327967574621, 0.9757235476326691, 0.9719199919945001, 0.9620231047078028, 0.9792004787568362, 0.973372573988543, 0.9755793587141087, 0.0, 0.0, 0.0, 0.001989524435141665, 0.0018281365899111002, 0.031124060486155014, 0.0, 0.0, 0.0, 0.11044192282611387, 0.10249699148985147, 0.11860360271926362, 0.017521515422513456, 0.00656962183887122, 9.999999999998899e-05, 0.062372258261358504, 9.999999999998899e-05, 0.0912849334670145, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.011752715529753877, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.1396112167348632, 0.12421783769642902, 0.14103244338215903, 0.10679315147984614, 0.1387916296901911, 0.09817732293631654, 0.12719501314661374, 0.09949075526341022, 0.11214895788266566, 0.014585865083758143, 0.009405807533463983, 0.053335511298185145, 0.0, 0.028138384732586563, 0.0, 0.0056031486469186875, 0.018761363751345517, 0.027938057908791225, 0.09289497683315007, 0.09288553633962471, 0.09298882555874755, 0.07622884985070266, 0.10339107993553331, 0.08929786078921786, 0.07852845632663452, 0.07276879660865687, 0.07677067817942385, 0.12068908612179152, 0.10742701554755874, 0.10705128967548738, 0.14344059472788317, 0.14783206959608686, 0.10784990049375187, 0.1273631117346915, 0.11922011318374748, 0.11351244162379848, 0.06656603129920147, 0.053679095474200356, 0.026691483647965253, 0.07476970549154738, 0.05594480916872069, 0.08278562995246708, 0.10285500844476925, 0.07035397692981282, 0.08764196550452963, 0.1354863540745328, 0.14252980027797435, 0.12140354094353056, 0.13547200639727486, 0.11588559432047074, 0.12475844993085139, 0.16131558609958807, 0.11883885863818211, 0.09484446786087752, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.00976667744786186, 0.14665717701174052, 0.0960797177461633, 0.0761239326581048, 0.07565325239499676, 0.05461947817712576, 0.06358383434806059, 0.08353689881996818, 0.05205108316764451, 0.07554536700328207, 0.0649850536817903, 0.02906392249683154, 0.05494107698589201, 0.018952763790558103, 0.08564980497969954, 0.10695440010782897, 0.022810256467945655, 0.0412131739677174, 0.06664696591121322, 0.09480636265193276, 0.146359147326369, 0.13533987883392762, 0.1632806877877092, 0.14996302121391492, 0.1768225021822749, 0.1716706583603329, 0.1529302770009463, 0.16372988774197073, 0.13784775839362684, 0.02942909441598729, 0.018485765204041993, 0.02215219582537198, 0.016613864082591134, 0.040229959076666044, 0.02982252856207468, 0.017234386334394336, 0.025772125113338595, 0.03213875331672589]}, "mutation_prompt": null}
{"id": "307a57a3-5c95-4af2-b7bc-7f4780316f2c", "solution": "import numpy as np\n\nclass AdaptiveGradientStochasticSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.evaluations = 0\n        self.learning_rate = 0.1\n        self.sigma = 0.1\n\n    def __call__(self, func):\n        x = np.random.uniform(self.lower_bound, self.upper_bound, self.dim)\n        best_x = x.copy()\n        best_f = func(best_x)\n        self.evaluations += 1\n\n        while self.evaluations < self.budget:\n            perturbation = np.random.normal(0, self.sigma, self.dim)\n            grad_approx = (func(x + perturbation) - func(x - perturbation)) / (2 * self.sigma)\n            self.evaluations += 2\n\n            x = x - self.learning_rate * grad_approx + perturbation\n            x = np.clip(x, self.lower_bound, self.upper_bound)\n\n            current_f = func(x)\n            self.evaluations += 1\n\n            if current_f < best_f:\n                best_f = current_f\n                best_x = x.copy()\n                self.learning_rate *= 1.2  # Adaptive increase\n                self.sigma *= 0.9  # Reduce perturbation to focus on exploitation\n            else:\n                self.learning_rate *= 0.8  # Adaptive decrease\n                self.sigma *= 1.1  # Increase perturbation for exploration\n\n        return best_x", "name": "AdaptiveGradientStochasticSearch", "description": "Gradient-Based Stochastic Search with Adaptive Step Size that combines gradient approximation and stochasticity for effective exploration and exploitation.", "configspace": "", "generation": 1, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "cc35849e-c096-4674-a984-3fc5a6ef4f3f", "metadata": {"aucs": [0.08998073195420075, 0.06271675336939186, 0.08875131807856451, 0.10494277406268049, 0.09280018916089827, 0.09112693834094732, 0.1288638426464599, 0.06554447394544227, 0.07105907849757953, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.020477991420123853, 0.023304540353572656, 0.013580939962702043, 0.017274518788866433, 0.0, 0.008265519422535506, 0.0, 0.0, 0.0, 0.015226161305268215, 0.032874133507481695, 0.020434179529922436, 0.0, 0.011947285404927621, 0.0, 0.0, 0.0073079661356707515, 0.0, 0.9729019800082144, 0.9764282517738041, 0.9773327967574621, 0.9757235476326691, 0.9719199919945001, 0.9620231047078028, 0.9792004787568362, 0.973372573988543, 0.9755793587141087, 0.0, 0.0, 0.0, 0.001989524435141665, 0.0018281365899111002, 0.031124060486155014, 0.0, 0.0, 0.0, 0.11044192282611387, 0.10249699148985147, 0.11860360271926362, 0.017521515422513456, 0.00656962183887122, 9.999999999998899e-05, 0.062372258261358504, 9.999999999998899e-05, 0.0912849334670145, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.011752715529753877, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.1396112167348632, 0.12421783769642902, 0.14103244338215903, 0.10679315147984614, 0.1387916296901911, 0.09817732293631654, 0.12719501314661374, 0.09949075526341022, 0.11214895788266566, 0.014585865083758143, 0.009405807533463983, 0.053335511298185145, 0.0, 0.028138384732586563, 0.0, 0.0056031486469186875, 0.018761363751345517, 0.027938057908791225, 0.09289497683315007, 0.09288553633962471, 0.09298882555874755, 0.07622884985070266, 0.10339107993553331, 0.08929786078921786, 0.07852845632663452, 0.07276879660865687, 0.07677067817942385, 0.12068908612179152, 0.10742701554755874, 0.10705128967548738, 0.14344059472788317, 0.14783206959608686, 0.10784990049375187, 0.1273631117346915, 0.11922011318374748, 0.11351244162379848, 0.06656603129920147, 0.053679095474200356, 0.026691483647965253, 0.07476970549154738, 0.05594480916872069, 0.08278562995246708, 0.10285500844476925, 0.07035397692981282, 0.08764196550452963, 0.1354863540745328, 0.14252980027797435, 0.12140354094353056, 0.13547200639727486, 0.11588559432047074, 0.12475844993085139, 0.16131558609958807, 0.11883885863818211, 0.09484446786087752, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.00976667744786186, 0.14665717701174052, 0.0960797177461633, 0.0761239326581048, 0.07565325239499676, 0.05461947817712576, 0.06358383434806059, 0.08353689881996818, 0.05205108316764451, 0.07554536700328207, 0.0649850536817903, 0.02906392249683154, 0.05494107698589201, 0.018952763790558103, 0.08564980497969954, 0.10695440010782897, 0.022810256467945655, 0.0412131739677174, 0.06664696591121322, 0.09480636265193276, 0.146359147326369, 0.13533987883392762, 0.1632806877877092, 0.14996302121391492, 0.1768225021822749, 0.1716706583603329, 0.1529302770009463, 0.16372988774197073, 0.13784775839362684, 0.02942909441598729, 0.018485765204041993, 0.02215219582537198, 0.016613864082591134, 0.040229959076666044, 0.02982252856207468, 0.017234386334394336, 0.025772125113338595, 0.03213875331672589]}, "mutation_prompt": null}
{"id": "528c9100-c4bf-4af7-8497-d4f84e24ebba", "solution": "import numpy as np\n\nclass AdaptiveGradientStochasticSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.evaluations = 0\n        self.learning_rate = 0.1\n        self.sigma = 0.1\n\n    def __call__(self, func):\n        x = np.random.uniform(self.lower_bound, self.upper_bound, self.dim)\n        best_x = x.copy()\n        best_f = func(best_x)\n        self.evaluations += 1\n\n        while self.evaluations < self.budget:\n            perturbation = np.random.normal(0, self.sigma, self.dim)\n            grad_approx = (func(x + perturbation) - func(x - perturbation)) / (2 * self.sigma)\n            self.evaluations += 2\n\n            x = x - self.learning_rate * grad_approx + perturbation\n            x = np.clip(x, self.lower_bound, self.upper_bound)\n\n            current_f = func(x)\n            self.evaluations += 1\n\n            if current_f < best_f:\n                best_f = current_f\n                best_x = x.copy()\n                self.learning_rate *= 1.2  # Adaptive increase\n                self.sigma *= 0.9  # Reduce perturbation to focus on exploitation\n            else:\n                self.learning_rate *= 0.8  # Adaptive decrease\n                self.sigma *= 1.1  # Increase perturbation for exploration\n\n        return best_x", "name": "AdaptiveGradientStochasticSearch", "description": "Gradient-Based Stochastic Search with Adaptive Step Size that combines gradient approximation and stochasticity for effective exploration and exploitation.", "configspace": "", "generation": 1, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "cc35849e-c096-4674-a984-3fc5a6ef4f3f", "metadata": {"aucs": [0.08998073195420075, 0.06271675336939186, 0.08875131807856451, 0.10494277406268049, 0.09280018916089827, 0.09112693834094732, 0.1288638426464599, 0.06554447394544227, 0.07105907849757953, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.020477991420123853, 0.023304540353572656, 0.013580939962702043, 0.017274518788866433, 0.0, 0.008265519422535506, 0.0, 0.0, 0.0, 0.015226161305268215, 0.032874133507481695, 0.020434179529922436, 0.0, 0.011947285404927621, 0.0, 0.0, 0.0073079661356707515, 0.0, 0.9729019800082144, 0.9764282517738041, 0.9773327967574621, 0.9757235476326691, 0.9719199919945001, 0.9620231047078028, 0.9792004787568362, 0.973372573988543, 0.9755793587141087, 0.0, 0.0, 0.0, 0.001989524435141665, 0.0018281365899111002, 0.031124060486155014, 0.0, 0.0, 0.0, 0.11044192282611387, 0.10249699148985147, 0.11860360271926362, 0.017521515422513456, 0.00656962183887122, 9.999999999998899e-05, 0.062372258261358504, 9.999999999998899e-05, 0.0912849334670145, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.011752715529753877, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.1396112167348632, 0.12421783769642902, 0.14103244338215903, 0.10679315147984614, 0.1387916296901911, 0.09817732293631654, 0.12719501314661374, 0.09949075526341022, 0.11214895788266566, 0.014585865083758143, 0.009405807533463983, 0.053335511298185145, 0.0, 0.028138384732586563, 0.0, 0.0056031486469186875, 0.018761363751345517, 0.027938057908791225, 0.09289497683315007, 0.09288553633962471, 0.09298882555874755, 0.07622884985070266, 0.10339107993553331, 0.08929786078921786, 0.07852845632663452, 0.07276879660865687, 0.07677067817942385, 0.12068908612179152, 0.10742701554755874, 0.10705128967548738, 0.14344059472788317, 0.14783206959608686, 0.10784990049375187, 0.1273631117346915, 0.11922011318374748, 0.11351244162379848, 0.06656603129920147, 0.053679095474200356, 0.026691483647965253, 0.07476970549154738, 0.05594480916872069, 0.08278562995246708, 0.10285500844476925, 0.07035397692981282, 0.08764196550452963, 0.1354863540745328, 0.14252980027797435, 0.12140354094353056, 0.13547200639727486, 0.11588559432047074, 0.12475844993085139, 0.16131558609958807, 0.11883885863818211, 0.09484446786087752, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.00976667744786186, 0.14665717701174052, 0.0960797177461633, 0.0761239326581048, 0.07565325239499676, 0.05461947817712576, 0.06358383434806059, 0.08353689881996818, 0.05205108316764451, 0.07554536700328207, 0.0649850536817903, 0.02906392249683154, 0.05494107698589201, 0.018952763790558103, 0.08564980497969954, 0.10695440010782897, 0.022810256467945655, 0.0412131739677174, 0.06664696591121322, 0.09480636265193276, 0.146359147326369, 0.13533987883392762, 0.1632806877877092, 0.14996302121391492, 0.1768225021822749, 0.1716706583603329, 0.1529302770009463, 0.16372988774197073, 0.13784775839362684, 0.02942909441598729, 0.018485765204041993, 0.02215219582537198, 0.016613864082591134, 0.040229959076666044, 0.02982252856207468, 0.017234386334394336, 0.025772125113338595, 0.03213875331672589]}, "mutation_prompt": null}
