{"id": "ddf9e708-0beb-4ff6-98e6-b367897fba12", "solution": "import numpy as np\n\nclass HybridDESA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.pop_size = 10 * dim\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.temp = 1000.0  # Initial temperature for simulated annealing\n        self.cooling_rate = 0.99  # Cooling rate for simulated annealing\n\n    def __call__(self, func):\n        np.random.seed(42)\n        population = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        num_evaluations = self.pop_size\n\n        def mutate(target_idx):\n            indices = [idx for idx in range(self.pop_size) if idx != target_idx]\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            mutant = population[a] + self.F * (population[b] - population[c])\n            mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n            return mutant\n\n        while num_evaluations < self.budget:\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                mutant = mutate(i)\n                trial = np.where(np.random.rand(self.dim) < self.CR, mutant, population[i])\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = trial\n\n            # Simulated Annealing part\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                neighbor = population[i] + np.random.normal(0, 0.1, self.dim)\n                neighbor = np.clip(neighbor, self.lower_bound, self.upper_bound)\n                neighbor_fitness = func(neighbor)\n                num_evaluations += 1\n                delta_fit = neighbor_fitness - fitness[i]\n\n                if delta_fit < 0 or np.random.rand() < np.exp(-delta_fit / self.temp):\n                    population[i] = neighbor\n                    fitness[i] = neighbor_fitness\n                    if neighbor_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = neighbor\n\n            self.temp *= self.cooling_rate\n\n        return best_solution", "name": "HybridDESA", "description": "A hybrid Differential Evolution and Simulated Annealing approach for efficiently exploring and exploiting the search space.", "configspace": "", "generation": 0, "fitness": 0.17257476761462967, "feedback": "The algorithm HybridDESA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.17 with standard deviation 0.16.", "error": "", "parent_id": null, "metadata": {"aucs": [0.3182147299416208, 0.3182147299416208, 0.3182147299416208, 0.32246413418149666, 0.32246413418149666, 0.32246413418149666, 0.32319506469871817, 0.32319506469871817, 0.32319506469871817, 0.08701963009086122, 0.08701963009086122, 0.08701963009086122, 0.09762090755833741, 0.09762090755833741, 0.09762090755833741, 0.0828305917751111, 0.0828305917751111, 0.0828305917751111, 0.09521506544916425, 0.09521506544916425, 0.09521506544916425, 0.07644113642222461, 0.07644113642222461, 0.07644113642222461, 0.08494603076108198, 0.08494603076108198, 0.08494603076108198, 0.07335386464826898, 0.07335386464826898, 0.07335386464826898, 0.0887909512972852, 0.0887909512972852, 0.0887909512972852, 0.06430208581270858, 0.06430208581270858, 0.06430208581270858, 0.9580506055121417, 0.9580506055121417, 0.9580506055121417, 0.6755527875572302, 0.6755527875572302, 0.6755527875572302, 0.8292174529901841, 0.8292174529901841, 0.8292174529901841, 0.1479482087224001, 0.1479482087224001, 0.1479482087224001, 0.1507710630848802, 0.1507710630848802, 0.1507710630848802, 0.14476435073369653, 0.14476435073369653, 0.14476435073369653, 0.22468242053928544, 0.22468242053928544, 0.22468242053928544, 0.23007423136066907, 0.23007423136066907, 0.23007423136066907, 0.24066370103306844, 0.24066370103306844, 0.24066370103306844, 0.09197974883203097, 0.09197974883203097, 0.09197974883203097, 0.11481522603894323, 0.11481522603894323, 0.11481522603894323, 0.09887954726408466, 0.09887954726408466, 0.09887954726408466, 0.1033282975223947, 0.1033282975223947, 0.1033282975223947, 0.08184445945846619, 0.08184445945846619, 0.08184445945846619, 0.08562154389218324, 0.08562154389218324, 0.08562154389218324, 0.05072536227567148, 0.05072536227567148, 0.05072536227567148, 0.020258137402108023, 0.020258137402108023, 0.020258137402108023, 0.026418749641179184, 0.026418749641179184, 0.026418749641179184, 0.11384547969127834, 0.11384547969127834, 0.11384547969127834, 0.10313696054134269, 0.10313696054134269, 0.10313696054134269, 0.09884120320542045, 0.09884120320542045, 0.09884120320542045, 0.0015174538180231512, 0.0015174538180231512, 0.0015174538180231512, 0.028762045531707914, 0.028762045531707914, 0.028762045531707914, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.055782190955102506, 0.055782190955102506, 0.055782190955102506, 0.06417217496769956, 0.06417217496769956, 0.06417217496769956, 0.04559293486257143, 0.04559293486257143, 0.04559293486257143, 0.3105244914674651, 0.3105244914674651, 0.3105244914674651, 0.3102428264334254, 0.3102428264334254, 0.3102428264334254, 0.2939395751893078, 0.2939395751893078, 0.2939395751893078, 0.0828326415505144, 0.0828326415505144, 0.0828326415505144, 0.0813723737764851, 0.0813723737764851, 0.0813723737764851, 0.07982987267368191, 0.07982987267368191, 0.07982987267368191, 0.19185544325250714, 0.19185544325250714, 0.19185544325250714, 0.18562431738351237, 0.18562431738351237, 0.18562431738351237, 0.1479468955022265, 0.1479468955022265, 0.1479468955022265, 0.22002151446941787, 0.22002151446941787, 0.22002151446941787, 0.20918622313582158, 0.20918622313582158, 0.20918622313582158, 0.21285525184027887, 0.21285525184027887, 0.21285525184027887, 0.14878284964048327, 0.14878284964048327, 0.14878284964048327, 0.1569106068804651, 0.1569106068804651, 0.1569106068804651, 0.16026459957523675, 0.16026459957523675, 0.16026459957523675, 0.19108635885737935, 0.19108635885737935, 0.19108635885737935, 0.1772552917738276, 0.1772552917738276, 0.1772552917738276, 0.18146686594076744, 0.18146686594076744, 0.18146686594076744, 0.1629804678518052, 0.1629804678518052, 0.1629804678518052, 0.19807965869521826, 0.19807965869521826, 0.19807965869521826, 0.16949905603107307, 0.16949905603107307, 0.16949905603107307, 0.2708009594853502, 0.2708009594853502, 0.2708009594853502, 0.2535852805243234, 0.2535852805243234, 0.2535852805243234, 0.15510708739787515, 0.15510708739787515, 0.15510708739787515, 0.17923758958243763, 0.17923758958243763, 0.17923758958243763, 0.1621839715109219, 0.1621839715109219, 0.1621839715109219, 0.2584687236414518, 0.2584687236414518, 0.2584687236414518, 0.1809273615216077, 0.1809273615216077, 0.1809273615216077, 0.17765202587621098, 0.17765202587621098, 0.17765202587621098, 0.19168524701844858, 0.19168524701844858, 0.19168524701844858, 0.07498142013801135, 0.07498142013801135, 0.07498142013801135, 0.07660145435737253, 0.07660145435737253, 0.07660145435737253, 0.06985443520778134, 0.06985443520778134, 0.06985443520778134]}, "mutation_prompt": null}
{"id": "91876856-c1f1-44e4-988a-eb9d93d1df99", "solution": "import numpy as np\n\nclass HybridDESA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.pop_size = 10 * dim\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.temp = 1000.0  # Initial temperature for simulated annealing\n        self.cooling_rate = 0.99  # Cooling rate for simulated annealing\n\n    def __call__(self, func):\n        np.random.seed(42)\n        population = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        num_evaluations = self.pop_size\n\n        def mutate(target_idx):\n            indices = [idx for idx in range(self.pop_size) if idx != target_idx]\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            mutant = population[a] + self.F * (population[b] - population[c])\n            mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n            return mutant\n\n        while num_evaluations < self.budget:\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                mutant = mutate(i)\n                trial = np.where(np.random.rand(self.dim) < self.CR, mutant, population[i])\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = trial\n\n            # Simulated Annealing part\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                neighbor = population[i] + np.random.normal(0, 0.1, self.dim)\n                neighbor = np.clip(neighbor, self.lower_bound, self.upper_bound)\n                neighbor_fitness = func(neighbor)\n                num_evaluations += 1\n                delta_fit = neighbor_fitness - fitness[i]\n\n                if delta_fit < 0 or np.random.rand() < np.exp(-delta_fit / self.temp):\n                    population[i] = neighbor\n                    fitness[i] = neighbor_fitness\n                    if neighbor_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = neighbor\n\n            self.temp *= self.cooling_rate\n\n        return best_solution", "name": "HybridDESA", "description": "A hybrid Differential Evolution and Simulated Annealing approach for efficiently exploring and exploiting the search space.", "configspace": "", "generation": 1, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "ddf9e708-0beb-4ff6-98e6-b367897fba12", "metadata": {"aucs": [0.3182147299416208, 0.3182147299416208, 0.3182147299416208, 0.32246413418149666, 0.32246413418149666, 0.32246413418149666, 0.32319506469871817, 0.32319506469871817, 0.32319506469871817, 0.08701963009086122, 0.08701963009086122, 0.08701963009086122, 0.09762090755833741, 0.09762090755833741, 0.09762090755833741, 0.0828305917751111, 0.0828305917751111, 0.0828305917751111, 0.09521506544916425, 0.09521506544916425, 0.09521506544916425, 0.07644113642222461, 0.07644113642222461, 0.07644113642222461, 0.08494603076108198, 0.08494603076108198, 0.08494603076108198, 0.07335386464826898, 0.07335386464826898, 0.07335386464826898, 0.0887909512972852, 0.0887909512972852, 0.0887909512972852, 0.06430208581270858, 0.06430208581270858, 0.06430208581270858, 0.9580506055121417, 0.9580506055121417, 0.9580506055121417, 0.6755527875572302, 0.6755527875572302, 0.6755527875572302, 0.8292174529901841, 0.8292174529901841, 0.8292174529901841, 0.1479482087224001, 0.1479482087224001, 0.1479482087224001, 0.1507710630848802, 0.1507710630848802, 0.1507710630848802, 0.14476435073369653, 0.14476435073369653, 0.14476435073369653, 0.22468242053928544, 0.22468242053928544, 0.22468242053928544, 0.23007423136066907, 0.23007423136066907, 0.23007423136066907, 0.24066370103306844, 0.24066370103306844, 0.24066370103306844, 0.09197974883203097, 0.09197974883203097, 0.09197974883203097, 0.11481522603894323, 0.11481522603894323, 0.11481522603894323, 0.09887954726408466, 0.09887954726408466, 0.09887954726408466, 0.1033282975223947, 0.1033282975223947, 0.1033282975223947, 0.08184445945846619, 0.08184445945846619, 0.08184445945846619, 0.08562154389218324, 0.08562154389218324, 0.08562154389218324, 0.05072536227567148, 0.05072536227567148, 0.05072536227567148, 0.020258137402108023, 0.020258137402108023, 0.020258137402108023, 0.026418749641179184, 0.026418749641179184, 0.026418749641179184, 0.11384547969127834, 0.11384547969127834, 0.11384547969127834, 0.10313696054134269, 0.10313696054134269, 0.10313696054134269, 0.09884120320542045, 0.09884120320542045, 0.09884120320542045, 0.0015174538180231512, 0.0015174538180231512, 0.0015174538180231512, 0.028762045531707914, 0.028762045531707914, 0.028762045531707914, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.055782190955102506, 0.055782190955102506, 0.055782190955102506, 0.06417217496769956, 0.06417217496769956, 0.06417217496769956, 0.04559293486257143, 0.04559293486257143, 0.04559293486257143, 0.3105244914674651, 0.3105244914674651, 0.3105244914674651, 0.3102428264334254, 0.3102428264334254, 0.3102428264334254, 0.2939395751893078, 0.2939395751893078, 0.2939395751893078, 0.0828326415505144, 0.0828326415505144, 0.0828326415505144, 0.0813723737764851, 0.0813723737764851, 0.0813723737764851, 0.07982987267368191, 0.07982987267368191, 0.07982987267368191, 0.19185544325250714, 0.19185544325250714, 0.19185544325250714, 0.18562431738351237, 0.18562431738351237, 0.18562431738351237, 0.1479468955022265, 0.1479468955022265, 0.1479468955022265, 0.22002151446941787, 0.22002151446941787, 0.22002151446941787, 0.20918622313582158, 0.20918622313582158, 0.20918622313582158, 0.21285525184027887, 0.21285525184027887, 0.21285525184027887, 0.14878284964048327, 0.14878284964048327, 0.14878284964048327, 0.1569106068804651, 0.1569106068804651, 0.1569106068804651, 0.16026459957523675, 0.16026459957523675, 0.16026459957523675, 0.19108635885737935, 0.19108635885737935, 0.19108635885737935, 0.1772552917738276, 0.1772552917738276, 0.1772552917738276, 0.18146686594076744, 0.18146686594076744, 0.18146686594076744, 0.1629804678518052, 0.1629804678518052, 0.1629804678518052, 0.19807965869521826, 0.19807965869521826, 0.19807965869521826, 0.16949905603107307, 0.16949905603107307, 0.16949905603107307, 0.2708009594853502, 0.2708009594853502, 0.2708009594853502, 0.2535852805243234, 0.2535852805243234, 0.2535852805243234, 0.15510708739787515, 0.15510708739787515, 0.15510708739787515, 0.17923758958243763, 0.17923758958243763, 0.17923758958243763, 0.1621839715109219, 0.1621839715109219, 0.1621839715109219, 0.2584687236414518, 0.2584687236414518, 0.2584687236414518, 0.1809273615216077, 0.1809273615216077, 0.1809273615216077, 0.17765202587621098, 0.17765202587621098, 0.17765202587621098, 0.19168524701844858, 0.19168524701844858, 0.19168524701844858, 0.07498142013801135, 0.07498142013801135, 0.07498142013801135, 0.07660145435737253, 0.07660145435737253, 0.07660145435737253, 0.06985443520778134, 0.06985443520778134, 0.06985443520778134]}, "mutation_prompt": null}
{"id": "f5d62ebd-5d1c-46e5-a286-9f68527a1c35", "solution": "import numpy as np\n\nclass HybridDESA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.pop_size = 10 * dim\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.temp = 1000.0  # Initial temperature for simulated annealing\n        self.cooling_rate = 0.99  # Cooling rate for simulated annealing\n\n    def __call__(self, func):\n        np.random.seed(42)\n        population = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        num_evaluations = self.pop_size\n\n        def mutate(target_idx):\n            indices = [idx for idx in range(self.pop_size) if idx != target_idx]\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            mutant = population[a] + self.F * (population[b] - population[c])\n            mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n            return mutant\n\n        while num_evaluations < self.budget:\n            self.CR = 0.9 - (0.5 * (num_evaluations / self.budget))  # Dynamic adjustment\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                mutant = mutate(i)\n                trial = np.where(np.random.rand(self.dim) < self.CR, mutant, population[i])\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = trial\n\n            # Simulated Annealing part\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                neighbor = population[i] + np.random.normal(0, 0.1, self.dim)\n                neighbor = np.clip(neighbor, self.lower_bound, self.upper_bound)\n                neighbor_fitness = func(neighbor)\n                num_evaluations += 1\n                delta_fit = neighbor_fitness - fitness[i]\n\n                if delta_fit < 0 or np.random.rand() < np.exp(-delta_fit / self.temp):\n                    population[i] = neighbor\n                    fitness[i] = neighbor_fitness\n                    if neighbor_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = neighbor\n\n            self.temp *= self.cooling_rate\n\n        return best_solution", "name": "HybridDESA", "description": "Improved exploration by adjusting the crossover probability dynamically based on iteration progress.", "configspace": "", "generation": 2, "fitness": 0.1735429987443044, "feedback": "The algorithm HybridDESA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.17 with standard deviation 0.17.", "error": "", "parent_id": "ddf9e708-0beb-4ff6-98e6-b367897fba12", "metadata": {"aucs": [0.32511363930885295, 0.32511363930885295, 0.32511363930885295, 0.34398170202047573, 0.34398170202047573, 0.34398170202047573, 0.31789789828484927, 0.31789789828484927, 0.31789789828484927, 0.08408190849260722, 0.08408190849260722, 0.08408190849260722, 0.09138288851100729, 0.09138288851100729, 0.09138288851100729, 0.08667038169980257, 0.08667038169980257, 0.08667038169980257, 0.09492070050612034, 0.09492070050612034, 0.09492070050612034, 0.09944385510634879, 0.09944385510634879, 0.09944385510634879, 0.09041979523727273, 0.09041979523727273, 0.09041979523727273, 0.07308163747732999, 0.07308163747732999, 0.07308163747732999, 0.06693972821899918, 0.06693972821899918, 0.06693972821899918, 0.07957772990976553, 0.07957772990976553, 0.07957772990976553, 0.9653637796919332, 0.9653637796919332, 0.9653637796919332, 0.8735834446214639, 0.8735834446214639, 0.8735834446214639, 0.8258285736303572, 0.8258285736303572, 0.8258285736303572, 0.167247682554166, 0.167247682554166, 0.167247682554166, 0.15077439465361864, 0.15077439465361864, 0.15077439465361864, 0.15368366518929055, 0.15368366518929055, 0.15368366518929055, 0.19979362035311465, 0.19979362035311465, 0.19979362035311465, 0.22733126937627135, 0.22733126937627135, 0.22733126937627135, 0.1924981396801021, 0.1924981396801021, 0.1924981396801021, 0.10013504258752004, 0.10013504258752004, 0.10013504258752004, 0.09835209801719125, 0.09835209801719125, 0.09835209801719125, 0.09085770567657148, 0.09085770567657148, 0.09085770567657148, 0.06753068672577633, 0.06753068672577633, 0.06753068672577633, 0.09069111034263566, 0.09069111034263566, 0.09069111034263566, 0.0779144119399574, 0.0779144119399574, 0.0779144119399574, 0.00019625959456148756, 0.00019625959456148756, 0.00019625959456148756, 0.009215309553113227, 0.009215309553113227, 0.009215309553113227, 0.01320029251612942, 0.01320029251612942, 0.01320029251612942, 0.11250401770324225, 0.11250401770324225, 0.11250401770324225, 0.09120423089766849, 0.09120423089766849, 0.09120423089766849, 0.08302317812292492, 0.08302317812292492, 0.08302317812292492, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.041849537414084814, 0.041849537414084814, 0.041849537414084814, 0.054219675112431864, 0.054219675112431864, 0.054219675112431864, 0.0686912290816325, 0.0686912290816325, 0.0686912290816325, 0.33959502069250536, 0.33959502069250536, 0.33959502069250536, 0.31256242423078684, 0.31256242423078684, 0.31256242423078684, 0.30464477414439606, 0.30464477414439606, 0.30464477414439606, 0.08614517049455894, 0.08614517049455894, 0.08614517049455894, 0.08935493124704952, 0.08935493124704952, 0.08935493124704952, 0.07945348374127337, 0.07945348374127337, 0.07945348374127337, 0.14515345887840636, 0.14515345887840636, 0.14515345887840636, 0.13960842600815482, 0.13960842600815482, 0.13960842600815482, 0.15874026196498436, 0.15874026196498436, 0.15874026196498436, 0.213634048328184, 0.213634048328184, 0.213634048328184, 0.20425960967245205, 0.20425960967245205, 0.20425960967245205, 0.2146248589263421, 0.2146248589263421, 0.2146248589263421, 0.15993982301172782, 0.15993982301172782, 0.15993982301172782, 0.14261992175663396, 0.14261992175663396, 0.14261992175663396, 0.16117160347100434, 0.16117160347100434, 0.16117160347100434, 0.18212437215999333, 0.18212437215999333, 0.18212437215999333, 0.2063153720644979, 0.2063153720644979, 0.2063153720644979, 0.19629537254730556, 0.19629537254730556, 0.19629537254730556, 0.16492776247779506, 0.16492776247779506, 0.16492776247779506, 0.17741723100283657, 0.17741723100283657, 0.17741723100283657, 0.16913724121951146, 0.16913724121951146, 0.16913724121951146, 0.26148452876682804, 0.26148452876682804, 0.26148452876682804, 0.2800443159200924, 0.2800443159200924, 0.2800443159200924, 0.1845698505091944, 0.1845698505091944, 0.1845698505091944, 0.17709269447192233, 0.17709269447192233, 0.17709269447192233, 0.17305182061371327, 0.17305182061371327, 0.17305182061371327, 0.30090388953153224, 0.30090388953153224, 0.30090388953153224, 0.1853006857006505, 0.1853006857006505, 0.1853006857006505, 0.1872523507443069, 0.1872523507443069, 0.1872523507443069, 0.18775313267782923, 0.18775313267782923, 0.18775313267782923, 0.07459505417932, 0.07459505417932, 0.07459505417932, 0.0629984494990784, 0.0629984494990784, 0.0629984494990784, 0.0628227491258595, 0.0628227491258595, 0.0628227491258595]}, "mutation_prompt": null}
{"id": "0976dc36-aff5-43b3-abdf-989ff08fa098", "solution": "import numpy as np\n\nclass HybridDESA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.pop_size = 10 * dim\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.temp = 1000.0  # Initial temperature for simulated annealing\n        self.cooling_rate = 0.99  # Cooling rate for simulated annealing\n\n    def __call__(self, func):\n        np.random.seed(42)\n        population = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        num_evaluations = self.pop_size\n\n        def mutate(target_idx):\n            indices = [idx for idx in range(self.pop_size) if idx != target_idx]\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            mutant = population[a] + self.F * (population[b] - population[c])\n            mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n            return mutant\n\n        while num_evaluations < self.budget:\n            self.CR = 0.9 - (0.5 * (num_evaluations / self.budget))  # Dynamic adjustment\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                mutant = mutate(i)\n                trial = np.where(np.random.rand(self.dim) < self.CR, mutant, population[i])\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = trial\n\n            # Simulated Annealing part\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                neighbor = population[i] + np.random.normal(0, 0.1, self.dim)\n                neighbor = np.clip(neighbor, self.lower_bound, self.upper_bound)\n                neighbor_fitness = func(neighbor)\n                num_evaluations += 1\n                delta_fit = neighbor_fitness - fitness[i]\n\n                if delta_fit < 0 or np.random.rand() < np.exp(-delta_fit / self.temp):\n                    population[i] = neighbor\n                    fitness[i] = neighbor_fitness\n                    if neighbor_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = neighbor\n\n            self.temp *= self.cooling_rate\n\n            # Reinitialize if population is too similar\n            if np.std(fitness) < 1e-5:\n                population = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n                fitness = np.array([func(ind) for ind in population])\n                num_evaluations += self.pop_size\n\n        return best_solution", "name": "HybridDESA", "description": "Enhanced diversity by introducing a random reinitialization of solutions when the population converges too early.", "configspace": "", "generation": 3, "fitness": 0.1735429987443044, "feedback": "The algorithm HybridDESA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.17 with standard deviation 0.17.", "error": "", "parent_id": "f5d62ebd-5d1c-46e5-a286-9f68527a1c35", "metadata": {"aucs": [0.32511363930885295, 0.32511363930885295, 0.32511363930885295, 0.34398170202047573, 0.34398170202047573, 0.34398170202047573, 0.31789789828484927, 0.31789789828484927, 0.31789789828484927, 0.08408190849260722, 0.08408190849260722, 0.08408190849260722, 0.09138288851100729, 0.09138288851100729, 0.09138288851100729, 0.08667038169980257, 0.08667038169980257, 0.08667038169980257, 0.09492070050612034, 0.09492070050612034, 0.09492070050612034, 0.09944385510634879, 0.09944385510634879, 0.09944385510634879, 0.09041979523727273, 0.09041979523727273, 0.09041979523727273, 0.07308163747732999, 0.07308163747732999, 0.07308163747732999, 0.06693972821899918, 0.06693972821899918, 0.06693972821899918, 0.07957772990976553, 0.07957772990976553, 0.07957772990976553, 0.9653637796919332, 0.9653637796919332, 0.9653637796919332, 0.8735834446214639, 0.8735834446214639, 0.8735834446214639, 0.8258285736303572, 0.8258285736303572, 0.8258285736303572, 0.167247682554166, 0.167247682554166, 0.167247682554166, 0.15077439465361864, 0.15077439465361864, 0.15077439465361864, 0.15368366518929055, 0.15368366518929055, 0.15368366518929055, 0.19979362035311465, 0.19979362035311465, 0.19979362035311465, 0.22733126937627135, 0.22733126937627135, 0.22733126937627135, 0.1924981396801021, 0.1924981396801021, 0.1924981396801021, 0.10013504258752004, 0.10013504258752004, 0.10013504258752004, 0.09835209801719125, 0.09835209801719125, 0.09835209801719125, 0.09085770567657148, 0.09085770567657148, 0.09085770567657148, 0.06753068672577633, 0.06753068672577633, 0.06753068672577633, 0.09069111034263566, 0.09069111034263566, 0.09069111034263566, 0.0779144119399574, 0.0779144119399574, 0.0779144119399574, 0.00019625959456148756, 0.00019625959456148756, 0.00019625959456148756, 0.009215309553113227, 0.009215309553113227, 0.009215309553113227, 0.01320029251612942, 0.01320029251612942, 0.01320029251612942, 0.11250401770324225, 0.11250401770324225, 0.11250401770324225, 0.09120423089766849, 0.09120423089766849, 0.09120423089766849, 0.08302317812292492, 0.08302317812292492, 0.08302317812292492, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.041849537414084814, 0.041849537414084814, 0.041849537414084814, 0.054219675112431864, 0.054219675112431864, 0.054219675112431864, 0.0686912290816325, 0.0686912290816325, 0.0686912290816325, 0.33959502069250536, 0.33959502069250536, 0.33959502069250536, 0.31256242423078684, 0.31256242423078684, 0.31256242423078684, 0.30464477414439606, 0.30464477414439606, 0.30464477414439606, 0.08614517049455894, 0.08614517049455894, 0.08614517049455894, 0.08935493124704952, 0.08935493124704952, 0.08935493124704952, 0.07945348374127337, 0.07945348374127337, 0.07945348374127337, 0.14515345887840636, 0.14515345887840636, 0.14515345887840636, 0.13960842600815482, 0.13960842600815482, 0.13960842600815482, 0.15874026196498436, 0.15874026196498436, 0.15874026196498436, 0.213634048328184, 0.213634048328184, 0.213634048328184, 0.20425960967245205, 0.20425960967245205, 0.20425960967245205, 0.2146248589263421, 0.2146248589263421, 0.2146248589263421, 0.15993982301172782, 0.15993982301172782, 0.15993982301172782, 0.14261992175663396, 0.14261992175663396, 0.14261992175663396, 0.16117160347100434, 0.16117160347100434, 0.16117160347100434, 0.18212437215999333, 0.18212437215999333, 0.18212437215999333, 0.2063153720644979, 0.2063153720644979, 0.2063153720644979, 0.19629537254730556, 0.19629537254730556, 0.19629537254730556, 0.16492776247779506, 0.16492776247779506, 0.16492776247779506, 0.17741723100283657, 0.17741723100283657, 0.17741723100283657, 0.16913724121951146, 0.16913724121951146, 0.16913724121951146, 0.26148452876682804, 0.26148452876682804, 0.26148452876682804, 0.2800443159200924, 0.2800443159200924, 0.2800443159200924, 0.1845698505091944, 0.1845698505091944, 0.1845698505091944, 0.17709269447192233, 0.17709269447192233, 0.17709269447192233, 0.17305182061371327, 0.17305182061371327, 0.17305182061371327, 0.30090388953153224, 0.30090388953153224, 0.30090388953153224, 0.1853006857006505, 0.1853006857006505, 0.1853006857006505, 0.1872523507443069, 0.1872523507443069, 0.1872523507443069, 0.18775313267782923, 0.18775313267782923, 0.18775313267782923, 0.07459505417932, 0.07459505417932, 0.07459505417932, 0.0629984494990784, 0.0629984494990784, 0.0629984494990784, 0.0628227491258595, 0.0628227491258595, 0.0628227491258595]}, "mutation_prompt": null}
{"id": "d9f2f84c-1932-4b8f-87d3-a6d0b0a1b09f", "solution": "import numpy as np\n\nclass HybridDESA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.pop_size = 10 * dim\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.temp = 1000.0  # Initial temperature for simulated annealing\n        self.cooling_rate = 0.99  # Cooling rate for simulated annealing\n\n    def __call__(self, func):\n        np.random.seed(42)\n        population = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        num_evaluations = self.pop_size\n\n        def mutate(target_idx):\n            indices = [idx for idx in range(self.pop_size) if idx != target_idx]\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            mutant = population[a] + self.F * (population[b] - population[c])\n            mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n            return mutant\n\n        while num_evaluations < self.budget:\n            self.CR = 0.9 - (0.5 * (num_evaluations / self.budget))  # Dynamic adjustment\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                mutant = mutate(i)\n                trial = np.where(np.random.rand(self.dim) < self.CR, mutant, population[i])\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = trial\n\n            # Simulated Annealing part\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                neighbor = population[i] + np.random.normal(0, 0.1 * self.cooling_rate, self.dim)  # Adjusted step size\n                neighbor = np.clip(neighbor, self.lower_bound, self.upper_bound)\n                neighbor_fitness = func(neighbor)\n                num_evaluations += 1\n                delta_fit = neighbor_fitness - fitness[i]\n\n                if delta_fit < 0 or np.random.rand() < np.exp(-delta_fit / self.temp):\n                    population[i] = neighbor\n                    fitness[i] = neighbor_fitness\n                    if neighbor_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = neighbor\n\n            self.temp *= self.cooling_rate\n\n        return best_solution", "name": "HybridDESA", "description": "Enhanced exploitation by introducing step size reduction in the simulated annealing phase.", "configspace": "", "generation": 4, "fitness": 0.17273909848196284, "feedback": "The algorithm HybridDESA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.17 with standard deviation 0.17.", "error": "", "parent_id": "f5d62ebd-5d1c-46e5-a286-9f68527a1c35", "metadata": {"aucs": [0.32890270188056747, 0.32890270188056747, 0.32890270188056747, 0.32571505501740183, 0.32571505501740183, 0.32571505501740183, 0.33922427908242847, 0.33922427908242847, 0.33922427908242847, 0.0795586334059899, 0.0795586334059899, 0.0795586334059899, 0.09331055927749676, 0.09331055927749676, 0.09331055927749676, 0.11397160032156994, 0.11397160032156994, 0.11397160032156994, 0.08189935365746637, 0.08189935365746637, 0.08189935365746637, 0.08220169589734272, 0.08220169589734272, 0.08220169589734272, 0.10210408242820301, 0.10210408242820301, 0.10210408242820301, 0.08047616591387541, 0.08047616591387541, 0.08047616591387541, 0.07225729350919008, 0.07225729350919008, 0.07225729350919008, 0.0660744379535183, 0.0660744379535183, 0.0660744379535183, 0.9653639856164172, 0.9653639856164172, 0.9653639856164172, 0.8903904522372998, 0.8903904522372998, 0.8903904522372998, 0.8327031481879122, 0.8327031481879122, 0.8327031481879122, 0.14088408379482098, 0.14088408379482098, 0.14088408379482098, 0.16236299896604578, 0.16236299896604578, 0.16236299896604578, 0.16400918707422352, 0.16400918707422352, 0.16400918707422352, 0.19206501717389024, 0.19206501717389024, 0.19206501717389024, 0.23848292708710406, 0.23848292708710406, 0.23848292708710406, 0.2531581530283813, 0.2531581530283813, 0.2531581530283813, 0.10463799170975163, 0.10463799170975163, 0.10463799170975163, 0.09818994561549321, 0.09818994561549321, 0.09818994561549321, 0.09605219773035656, 0.09605219773035656, 0.09605219773035656, 0.0811810660506449, 0.0811810660506449, 0.0811810660506449, 0.09448942106047764, 0.09448942106047764, 0.09448942106047764, 0.08300599416072163, 0.08300599416072163, 0.08300599416072163, 0.006432369952288686, 0.006432369952288686, 0.006432369952288686, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.002350147655681978, 0.002350147655681978, 0.002350147655681978, 0.10722874368433022, 0.10722874368433022, 0.10722874368433022, 0.0879204843098863, 0.0879204843098863, 0.0879204843098863, 0.11653623887570863, 0.11653623887570863, 0.11653623887570863, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.04009785459411486, 0.04009785459411486, 0.04009785459411486, 0.044743237857389984, 0.044743237857389984, 0.044743237857389984, 0.05772765395476731, 0.05772765395476731, 0.05772765395476731, 0.32080612975637246, 0.32080612975637246, 0.32080612975637246, 0.31462471977203743, 0.31462471977203743, 0.31462471977203743, 0.3098235304060155, 0.3098235304060155, 0.3098235304060155, 0.07671718117498472, 0.07671718117498472, 0.07671718117498472, 0.08394713675248111, 0.08394713675248111, 0.08394713675248111, 0.0794171463943929, 0.0794171463943929, 0.0794171463943929, 0.1448587046447143, 0.1448587046447143, 0.1448587046447143, 0.14505370925901806, 0.14505370925901806, 0.14505370925901806, 0.16171256143642698, 0.16171256143642698, 0.16171256143642698, 0.2151748571686145, 0.2151748571686145, 0.2151748571686145, 0.2014269951058285, 0.2014269951058285, 0.2014269951058285, 0.21109651570066668, 0.21109651570066668, 0.21109651570066668, 0.14328093839206502, 0.14328093839206502, 0.14328093839206502, 0.14875249009820568, 0.14875249009820568, 0.14875249009820568, 0.1575436567130971, 0.1575436567130971, 0.1575436567130971, 0.18562322107123685, 0.18562322107123685, 0.18562322107123685, 0.18539273572508075, 0.18539273572508075, 0.18539273572508075, 0.16995717570081015, 0.16995717570081015, 0.16995717570081015, 0.1617058980270898, 0.1617058980270898, 0.1617058980270898, 0.17171817158217495, 0.17171817158217495, 0.17171817158217495, 0.18510960800673515, 0.18510960800673515, 0.18510960800673515, 0.20626863858441002, 0.20626863858441002, 0.20626863858441002, 0.27971670479408217, 0.27971670479408217, 0.27971670479408217, 0.18237438619082336, 0.18237438619082336, 0.18237438619082336, 0.1744372275755266, 0.1744372275755266, 0.1744372275755266, 0.14327429115010637, 0.14327429115010637, 0.14327429115010637, 0.26729998550112455, 0.26729998550112455, 0.26729998550112455, 0.18970415155194376, 0.18970415155194376, 0.18970415155194376, 0.18713811984042494, 0.18713811984042494, 0.18713811984042494, 0.2024368661187277, 0.2024368661187277, 0.2024368661187277, 0.06757812284114983, 0.06757812284114983, 0.06757812284114983, 0.0704478883821017, 0.0704478883821017, 0.0704478883821017, 0.06668646656009714, 0.06668646656009714, 0.06668646656009714]}, "mutation_prompt": null}
{"id": "ac8e8bf7-43e5-455c-b649-879878463349", "solution": "import numpy as np\n\nclass HybridDESA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.pop_size = 10 * dim\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.temp = 1000.0  # Initial temperature for simulated annealing\n        self.cooling_rate = 0.99  # Cooling rate for simulated annealing\n\n    def __call__(self, func):\n        np.random.seed(42)\n        population = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        num_evaluations = self.pop_size\n\n        def mutate(target_idx):\n            indices = [idx for idx in range(self.pop_size) if idx != target_idx]\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            mutant = population[a] + self.F * (population[b] - population[c])\n            mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n            return mutant\n\n        while num_evaluations < self.budget:\n            self.CR = 0.9 - (0.5 * (num_evaluations / self.budget))  # Dynamic adjustment\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                mutant = mutate(i)\n                trial = np.where(np.random.rand(self.dim) < self.CR, mutant, population[i])\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = trial\n\n            # Simulated Annealing part\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                neighbor = population[i] + np.random.normal(0, 0.1 * self.temp / 1000, self.dim)\n                neighbor = np.clip(neighbor, self.lower_bound, self.upper_bound)\n                neighbor_fitness = func(neighbor)\n                num_evaluations += 1\n                delta_fit = neighbor_fitness - fitness[i]\n\n                if delta_fit < 0 or np.random.rand() < np.exp(-delta_fit / self.temp):\n                    population[i] = neighbor\n                    fitness[i] = neighbor_fitness\n                    if neighbor_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = neighbor\n\n            self.temp *= self.cooling_rate\n\n        return best_solution", "name": "HybridDESA", "description": "Enhanced local search by incorporating adaptive noise in simulated annealing neighborhood exploration.", "configspace": "", "generation": 5, "fitness": 0.17355064766706907, "feedback": "The algorithm HybridDESA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.17 with standard deviation 0.17.", "error": "", "parent_id": "f5d62ebd-5d1c-46e5-a286-9f68527a1c35", "metadata": {"aucs": [0.3466661414760661, 0.3466661414760661, 0.3466661414760661, 0.3511776331785803, 0.3511776331785803, 0.3511776331785803, 0.3550933249150291, 0.3550933249150291, 0.3550933249150291, 0.07484792661925821, 0.07484792661925821, 0.07484792661925821, 0.05743810277766859, 0.05743810277766859, 0.05743810277766859, 0.0767184810280167, 0.0767184810280167, 0.0767184810280167, 0.10680463369350424, 0.10680463369350424, 0.10680463369350424, 0.09592906907868548, 0.09592906907868548, 0.09592906907868548, 0.08996532225488718, 0.08996532225488718, 0.08996532225488718, 0.08420077021667616, 0.08420077021667616, 0.08420077021667616, 0.0709027005656111, 0.0709027005656111, 0.0709027005656111, 0.07322261394801644, 0.07322261394801644, 0.07322261394801644, 0.9653638684208881, 0.9653638684208881, 0.9653638684208881, 0.897590856058333, 0.897590856058333, 0.897590856058333, 0.7955718197835132, 0.7955718197835132, 0.7955718197835132, 0.14627574190432513, 0.14627574190432513, 0.14627574190432513, 0.15246491731852907, 0.15246491731852907, 0.15246491731852907, 0.17308979859174667, 0.17308979859174667, 0.17308979859174667, 0.1778029148164243, 0.1778029148164243, 0.1778029148164243, 0.21735341446466439, 0.21735341446466439, 0.21735341446466439, 0.2024182382859543, 0.2024182382859543, 0.2024182382859543, 0.10753520137429573, 0.10753520137429573, 0.10753520137429573, 0.10580075021547619, 0.10580075021547619, 0.10580075021547619, 0.10103637187272396, 0.10103637187272396, 0.10103637187272396, 0.08193471917128192, 0.08193471917128192, 0.08193471917128192, 0.0935212274675058, 0.0935212274675058, 0.0935212274675058, 0.07854354934820895, 0.07854354934820895, 0.07854354934820895, 0.001126561745343535, 0.001126561745343535, 0.001126561745343535, 0.02653085295553881, 0.02653085295553881, 0.02653085295553881, 0.002142249466186019, 0.002142249466186019, 0.002142249466186019, 0.11720192612985414, 0.11720192612985414, 0.11720192612985414, 0.0821821473298292, 0.0821821473298292, 0.0821821473298292, 0.10539640548574136, 0.10539640548574136, 0.10539640548574136, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.04878808267465706, 0.04878808267465706, 0.04878808267465706, 0.06561488864568343, 0.06561488864568343, 0.06561488864568343, 0.06973512825627715, 0.06973512825627715, 0.06973512825627715, 0.3351124493289359, 0.3351124493289359, 0.3351124493289359, 0.3128080508262505, 0.3128080508262505, 0.3128080508262505, 0.315129102468504, 0.315129102468504, 0.315129102468504, 0.08330864718388786, 0.08330864718388786, 0.08330864718388786, 0.08606824506870658, 0.08606824506870658, 0.08606824506870658, 0.08921972637603393, 0.08921972637603393, 0.08921972637603393, 0.14159752149405735, 0.14159752149405735, 0.14159752149405735, 0.15741141971242556, 0.15741141971242556, 0.15741141971242556, 0.16335146741349127, 0.16335146741349127, 0.16335146741349127, 0.22343034480970025, 0.22343034480970025, 0.22343034480970025, 0.19525300607030194, 0.19525300607030194, 0.19525300607030194, 0.22153987032738331, 0.22153987032738331, 0.22153987032738331, 0.1540870576507738, 0.1540870576507738, 0.1540870576507738, 0.14260928774031123, 0.14260928774031123, 0.14260928774031123, 0.15420020216391017, 0.15420020216391017, 0.15420020216391017, 0.19039834869726835, 0.19039834869726835, 0.19039834869726835, 0.1830355105278526, 0.1830355105278526, 0.1830355105278526, 0.16606035877606606, 0.16606035877606606, 0.16606035877606606, 0.1764479698922865, 0.1764479698922865, 0.1764479698922865, 0.17908140858102228, 0.17908140858102228, 0.17908140858102228, 0.1699515406492501, 0.1699515406492501, 0.1699515406492501, 0.23437486944427122, 0.23437486944427122, 0.23437486944427122, 0.2791037152080491, 0.2791037152080491, 0.2791037152080491, 0.16349512646503994, 0.16349512646503994, 0.16349512646503994, 0.15899061841047735, 0.15899061841047735, 0.15899061841047735, 0.15833138015085302, 0.15833138015085302, 0.15833138015085302, 0.30370443621530374, 0.30370443621530374, 0.30370443621530374, 0.18192218949725536, 0.18192218949725536, 0.18192218949725536, 0.18180749746874125, 0.18180749746874125, 0.18180749746874125, 0.1905877046616331, 0.1905877046616331, 0.1905877046616331, 0.07604299457794783, 0.07604299457794783, 0.07604299457794783, 0.06813368794333652, 0.06813368794333652, 0.06813368794333652, 0.06076252469266186, 0.06076252469266186, 0.06076252469266186]}, "mutation_prompt": null}
{"id": "c3bb4156-ae6d-4bc9-8d1b-34c52ef7f71b", "solution": "import numpy as np\n\nclass HybridDESA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.pop_size = 10 * dim\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.temp = 1000.0  # Initial temperature for simulated annealing\n        self.cooling_rate = 0.99  # Cooling rate for simulated annealing\n\n    def __call__(self, func):\n        np.random.seed(42)\n        population = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        num_evaluations = self.pop_size\n\n        def mutate(target_idx):\n            indices = [idx for idx in range(self.pop_size) if idx != target_idx]\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            mutant = population[a] + self.F * (population[b] - population[c])\n            mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n            return mutant\n\n        while num_evaluations < self.budget:\n            self.CR = 0.9 - (0.5 * (num_evaluations / self.budget))  # Dynamic adjustment\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                mutant = mutate(i)\n                trial = np.where(np.random.rand(self.dim) < self.CR, mutant, population[i])\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = trial\n\n            # Simulated Annealing part\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                neighbor = population[i] + np.random.normal(0, 0.1 * self.temp / 1000, self.dim)\n                neighbor = np.clip(neighbor, self.lower_bound, self.upper_bound)\n                neighbor_fitness = func(neighbor)\n                num_evaluations += 1\n                delta_fit = neighbor_fitness - fitness[i]\n\n                if delta_fit < 0 or np.random.rand() < np.exp(-delta_fit / self.temp):\n                    population[i] = neighbor\n                    fitness[i] = neighbor_fitness\n                    if neighbor_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = neighbor\n\n            self.temp *= self.cooling_rate\n\n        return best_solution", "name": "HybridDESA", "description": "Enhanced local search by incorporating adaptive noise in simulated annealing neighborhood exploration.", "configspace": "", "generation": 6, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "ac8e8bf7-43e5-455c-b649-879878463349", "metadata": {"aucs": [0.3466661414760661, 0.3466661414760661, 0.3466661414760661, 0.3511776331785803, 0.3511776331785803, 0.3511776331785803, 0.3550933249150291, 0.3550933249150291, 0.3550933249150291, 0.07484792661925821, 0.07484792661925821, 0.07484792661925821, 0.05743810277766859, 0.05743810277766859, 0.05743810277766859, 0.0767184810280167, 0.0767184810280167, 0.0767184810280167, 0.10680463369350424, 0.10680463369350424, 0.10680463369350424, 0.09592906907868548, 0.09592906907868548, 0.09592906907868548, 0.08996532225488718, 0.08996532225488718, 0.08996532225488718, 0.08420077021667616, 0.08420077021667616, 0.08420077021667616, 0.0709027005656111, 0.0709027005656111, 0.0709027005656111, 0.07322261394801644, 0.07322261394801644, 0.07322261394801644, 0.9653638684208881, 0.9653638684208881, 0.9653638684208881, 0.897590856058333, 0.897590856058333, 0.897590856058333, 0.7955718197835132, 0.7955718197835132, 0.7955718197835132, 0.14627574190432513, 0.14627574190432513, 0.14627574190432513, 0.15246491731852907, 0.15246491731852907, 0.15246491731852907, 0.17308979859174667, 0.17308979859174667, 0.17308979859174667, 0.1778029148164243, 0.1778029148164243, 0.1778029148164243, 0.21735341446466439, 0.21735341446466439, 0.21735341446466439, 0.2024182382859543, 0.2024182382859543, 0.2024182382859543, 0.10753520137429573, 0.10753520137429573, 0.10753520137429573, 0.10580075021547619, 0.10580075021547619, 0.10580075021547619, 0.10103637187272396, 0.10103637187272396, 0.10103637187272396, 0.08193471917128192, 0.08193471917128192, 0.08193471917128192, 0.0935212274675058, 0.0935212274675058, 0.0935212274675058, 0.07854354934820895, 0.07854354934820895, 0.07854354934820895, 0.001126561745343535, 0.001126561745343535, 0.001126561745343535, 0.02653085295553881, 0.02653085295553881, 0.02653085295553881, 0.002142249466186019, 0.002142249466186019, 0.002142249466186019, 0.11720192612985414, 0.11720192612985414, 0.11720192612985414, 0.0821821473298292, 0.0821821473298292, 0.0821821473298292, 0.10539640548574136, 0.10539640548574136, 0.10539640548574136, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.04878808267465706, 0.04878808267465706, 0.04878808267465706, 0.06561488864568343, 0.06561488864568343, 0.06561488864568343, 0.06973512825627715, 0.06973512825627715, 0.06973512825627715, 0.3351124493289359, 0.3351124493289359, 0.3351124493289359, 0.3128080508262505, 0.3128080508262505, 0.3128080508262505, 0.315129102468504, 0.315129102468504, 0.315129102468504, 0.08330864718388786, 0.08330864718388786, 0.08330864718388786, 0.08606824506870658, 0.08606824506870658, 0.08606824506870658, 0.08921972637603393, 0.08921972637603393, 0.08921972637603393, 0.14159752149405735, 0.14159752149405735, 0.14159752149405735, 0.15741141971242556, 0.15741141971242556, 0.15741141971242556, 0.16335146741349127, 0.16335146741349127, 0.16335146741349127, 0.22343034480970025, 0.22343034480970025, 0.22343034480970025, 0.19525300607030194, 0.19525300607030194, 0.19525300607030194, 0.22153987032738331, 0.22153987032738331, 0.22153987032738331, 0.1540870576507738, 0.1540870576507738, 0.1540870576507738, 0.14260928774031123, 0.14260928774031123, 0.14260928774031123, 0.15420020216391017, 0.15420020216391017, 0.15420020216391017, 0.19039834869726835, 0.19039834869726835, 0.19039834869726835, 0.1830355105278526, 0.1830355105278526, 0.1830355105278526, 0.16606035877606606, 0.16606035877606606, 0.16606035877606606, 0.1764479698922865, 0.1764479698922865, 0.1764479698922865, 0.17908140858102228, 0.17908140858102228, 0.17908140858102228, 0.1699515406492501, 0.1699515406492501, 0.1699515406492501, 0.23437486944427122, 0.23437486944427122, 0.23437486944427122, 0.2791037152080491, 0.2791037152080491, 0.2791037152080491, 0.16349512646503994, 0.16349512646503994, 0.16349512646503994, 0.15899061841047735, 0.15899061841047735, 0.15899061841047735, 0.15833138015085302, 0.15833138015085302, 0.15833138015085302, 0.30370443621530374, 0.30370443621530374, 0.30370443621530374, 0.18192218949725536, 0.18192218949725536, 0.18192218949725536, 0.18180749746874125, 0.18180749746874125, 0.18180749746874125, 0.1905877046616331, 0.1905877046616331, 0.1905877046616331, 0.07604299457794783, 0.07604299457794783, 0.07604299457794783, 0.06813368794333652, 0.06813368794333652, 0.06813368794333652, 0.06076252469266186, 0.06076252469266186, 0.06076252469266186]}, "mutation_prompt": null}
{"id": "11cdde92-36db-4a06-87c5-b7a6d016298a", "solution": "import numpy as np\n\nclass HybridDESA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.pop_size = 10 * dim\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.temp = 1000.0  # Initial temperature for simulated annealing\n        self.cooling_rate = 0.99  # Cooling rate for simulated annealing\n\n    def __call__(self, func):\n        np.random.seed(42)\n        population = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        num_evaluations = self.pop_size\n\n        def mutate(target_idx):\n            indices = [idx for idx in range(self.pop_size) if idx != target_idx]\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            mutant = population[a] + self.F * (population[b] - population[c])\n            mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n            return mutant\n\n        while num_evaluations < self.budget:\n            self.CR = 0.9 - (0.5 * (num_evaluations / self.budget))  # Dynamic adjustment\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                mutant = mutate(i)\n                trial = np.where(np.random.rand(self.dim) < self.CR, mutant, population[i])\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = trial\n\n            # Simulated Annealing part\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                neighbor = population[i] + np.random.normal(0, 0.1 * self.temp / 1000, self.dim)\n                neighbor = np.clip(neighbor, self.lower_bound, self.upper_bound)\n                neighbor_fitness = func(neighbor)\n                num_evaluations += 1\n                delta_fit = neighbor_fitness - fitness[i]\n\n                if delta_fit < 0 or np.random.rand() < np.exp(-delta_fit / self.temp):\n                    population[i] = neighbor\n                    fitness[i] = neighbor_fitness\n                    if neighbor_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = neighbor\n\n            self.temp *= self.cooling_rate\n\n        return best_solution", "name": "HybridDESA", "description": "Enhanced local search by incorporating adaptive noise in simulated annealing neighborhood exploration.", "configspace": "", "generation": 6, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "ac8e8bf7-43e5-455c-b649-879878463349", "metadata": {"aucs": [0.3466661414760661, 0.3466661414760661, 0.3466661414760661, 0.3511776331785803, 0.3511776331785803, 0.3511776331785803, 0.3550933249150291, 0.3550933249150291, 0.3550933249150291, 0.07484792661925821, 0.07484792661925821, 0.07484792661925821, 0.05743810277766859, 0.05743810277766859, 0.05743810277766859, 0.0767184810280167, 0.0767184810280167, 0.0767184810280167, 0.10680463369350424, 0.10680463369350424, 0.10680463369350424, 0.09592906907868548, 0.09592906907868548, 0.09592906907868548, 0.08996532225488718, 0.08996532225488718, 0.08996532225488718, 0.08420077021667616, 0.08420077021667616, 0.08420077021667616, 0.0709027005656111, 0.0709027005656111, 0.0709027005656111, 0.07322261394801644, 0.07322261394801644, 0.07322261394801644, 0.9653638684208881, 0.9653638684208881, 0.9653638684208881, 0.897590856058333, 0.897590856058333, 0.897590856058333, 0.7955718197835132, 0.7955718197835132, 0.7955718197835132, 0.14627574190432513, 0.14627574190432513, 0.14627574190432513, 0.15246491731852907, 0.15246491731852907, 0.15246491731852907, 0.17308979859174667, 0.17308979859174667, 0.17308979859174667, 0.1778029148164243, 0.1778029148164243, 0.1778029148164243, 0.21735341446466439, 0.21735341446466439, 0.21735341446466439, 0.2024182382859543, 0.2024182382859543, 0.2024182382859543, 0.10753520137429573, 0.10753520137429573, 0.10753520137429573, 0.10580075021547619, 0.10580075021547619, 0.10580075021547619, 0.10103637187272396, 0.10103637187272396, 0.10103637187272396, 0.08193471917128192, 0.08193471917128192, 0.08193471917128192, 0.0935212274675058, 0.0935212274675058, 0.0935212274675058, 0.07854354934820895, 0.07854354934820895, 0.07854354934820895, 0.001126561745343535, 0.001126561745343535, 0.001126561745343535, 0.02653085295553881, 0.02653085295553881, 0.02653085295553881, 0.002142249466186019, 0.002142249466186019, 0.002142249466186019, 0.11720192612985414, 0.11720192612985414, 0.11720192612985414, 0.0821821473298292, 0.0821821473298292, 0.0821821473298292, 0.10539640548574136, 0.10539640548574136, 0.10539640548574136, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.04878808267465706, 0.04878808267465706, 0.04878808267465706, 0.06561488864568343, 0.06561488864568343, 0.06561488864568343, 0.06973512825627715, 0.06973512825627715, 0.06973512825627715, 0.3351124493289359, 0.3351124493289359, 0.3351124493289359, 0.3128080508262505, 0.3128080508262505, 0.3128080508262505, 0.315129102468504, 0.315129102468504, 0.315129102468504, 0.08330864718388786, 0.08330864718388786, 0.08330864718388786, 0.08606824506870658, 0.08606824506870658, 0.08606824506870658, 0.08921972637603393, 0.08921972637603393, 0.08921972637603393, 0.14159752149405735, 0.14159752149405735, 0.14159752149405735, 0.15741141971242556, 0.15741141971242556, 0.15741141971242556, 0.16335146741349127, 0.16335146741349127, 0.16335146741349127, 0.22343034480970025, 0.22343034480970025, 0.22343034480970025, 0.19525300607030194, 0.19525300607030194, 0.19525300607030194, 0.22153987032738331, 0.22153987032738331, 0.22153987032738331, 0.1540870576507738, 0.1540870576507738, 0.1540870576507738, 0.14260928774031123, 0.14260928774031123, 0.14260928774031123, 0.15420020216391017, 0.15420020216391017, 0.15420020216391017, 0.19039834869726835, 0.19039834869726835, 0.19039834869726835, 0.1830355105278526, 0.1830355105278526, 0.1830355105278526, 0.16606035877606606, 0.16606035877606606, 0.16606035877606606, 0.1764479698922865, 0.1764479698922865, 0.1764479698922865, 0.17908140858102228, 0.17908140858102228, 0.17908140858102228, 0.1699515406492501, 0.1699515406492501, 0.1699515406492501, 0.23437486944427122, 0.23437486944427122, 0.23437486944427122, 0.2791037152080491, 0.2791037152080491, 0.2791037152080491, 0.16349512646503994, 0.16349512646503994, 0.16349512646503994, 0.15899061841047735, 0.15899061841047735, 0.15899061841047735, 0.15833138015085302, 0.15833138015085302, 0.15833138015085302, 0.30370443621530374, 0.30370443621530374, 0.30370443621530374, 0.18192218949725536, 0.18192218949725536, 0.18192218949725536, 0.18180749746874125, 0.18180749746874125, 0.18180749746874125, 0.1905877046616331, 0.1905877046616331, 0.1905877046616331, 0.07604299457794783, 0.07604299457794783, 0.07604299457794783, 0.06813368794333652, 0.06813368794333652, 0.06813368794333652, 0.06076252469266186, 0.06076252469266186, 0.06076252469266186]}, "mutation_prompt": null}
{"id": "bac82fae-0b03-4c11-bf7b-046bc0f250e9", "solution": "import numpy as np\n\nclass HybridDESA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.pop_size = 10 * dim\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.temp = 1000.0  # Initial temperature for simulated annealing\n        self.cooling_rate = 0.99  # Cooling rate for simulated annealing\n\n    def __call__(self, func):\n        np.random.seed(42)\n        population = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        num_evaluations = self.pop_size\n\n        def mutate(target_idx):\n            indices = [idx for idx in range(self.pop_size) if idx != target_idx]\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            mutant = population[a] + self.F * (population[b] - population[c])\n            mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n            return mutant\n\n        while num_evaluations < self.budget:\n            self.CR = 0.9 - (0.5 * (num_evaluations / self.budget))  # Dynamic adjustment\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                mutant = mutate(i)\n                trial = np.where(np.random.rand(self.dim) < self.CR, mutant, population[i])\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = trial\n\n            # Simulated Annealing part\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                neighbor = population[i] + np.random.normal(0, 0.1 * self.temp / 1000, self.dim)\n                neighbor = np.clip(neighbor, self.lower_bound, self.upper_bound)\n                neighbor_fitness = func(neighbor)\n                num_evaluations += 1\n                delta_fit = neighbor_fitness - fitness[i]\n\n                if delta_fit < 0 or np.random.rand() < np.exp(-delta_fit / self.temp):\n                    population[i] = neighbor\n                    fitness[i] = neighbor_fitness\n                    if neighbor_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = neighbor\n\n            self.temp *= self.cooling_rate\n\n        return best_solution", "name": "HybridDESA", "description": "Enhanced local search by incorporating adaptive noise in simulated annealing neighborhood exploration.", "configspace": "", "generation": 6, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "ac8e8bf7-43e5-455c-b649-879878463349", "metadata": {"aucs": [0.3466661414760661, 0.3466661414760661, 0.3466661414760661, 0.3511776331785803, 0.3511776331785803, 0.3511776331785803, 0.3550933249150291, 0.3550933249150291, 0.3550933249150291, 0.07484792661925821, 0.07484792661925821, 0.07484792661925821, 0.05743810277766859, 0.05743810277766859, 0.05743810277766859, 0.0767184810280167, 0.0767184810280167, 0.0767184810280167, 0.10680463369350424, 0.10680463369350424, 0.10680463369350424, 0.09592906907868548, 0.09592906907868548, 0.09592906907868548, 0.08996532225488718, 0.08996532225488718, 0.08996532225488718, 0.08420077021667616, 0.08420077021667616, 0.08420077021667616, 0.0709027005656111, 0.0709027005656111, 0.0709027005656111, 0.07322261394801644, 0.07322261394801644, 0.07322261394801644, 0.9653638684208881, 0.9653638684208881, 0.9653638684208881, 0.897590856058333, 0.897590856058333, 0.897590856058333, 0.7955718197835132, 0.7955718197835132, 0.7955718197835132, 0.14627574190432513, 0.14627574190432513, 0.14627574190432513, 0.15246491731852907, 0.15246491731852907, 0.15246491731852907, 0.17308979859174667, 0.17308979859174667, 0.17308979859174667, 0.1778029148164243, 0.1778029148164243, 0.1778029148164243, 0.21735341446466439, 0.21735341446466439, 0.21735341446466439, 0.2024182382859543, 0.2024182382859543, 0.2024182382859543, 0.10753520137429573, 0.10753520137429573, 0.10753520137429573, 0.10580075021547619, 0.10580075021547619, 0.10580075021547619, 0.10103637187272396, 0.10103637187272396, 0.10103637187272396, 0.08193471917128192, 0.08193471917128192, 0.08193471917128192, 0.0935212274675058, 0.0935212274675058, 0.0935212274675058, 0.07854354934820895, 0.07854354934820895, 0.07854354934820895, 0.001126561745343535, 0.001126561745343535, 0.001126561745343535, 0.02653085295553881, 0.02653085295553881, 0.02653085295553881, 0.002142249466186019, 0.002142249466186019, 0.002142249466186019, 0.11720192612985414, 0.11720192612985414, 0.11720192612985414, 0.0821821473298292, 0.0821821473298292, 0.0821821473298292, 0.10539640548574136, 0.10539640548574136, 0.10539640548574136, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.04878808267465706, 0.04878808267465706, 0.04878808267465706, 0.06561488864568343, 0.06561488864568343, 0.06561488864568343, 0.06973512825627715, 0.06973512825627715, 0.06973512825627715, 0.3351124493289359, 0.3351124493289359, 0.3351124493289359, 0.3128080508262505, 0.3128080508262505, 0.3128080508262505, 0.315129102468504, 0.315129102468504, 0.315129102468504, 0.08330864718388786, 0.08330864718388786, 0.08330864718388786, 0.08606824506870658, 0.08606824506870658, 0.08606824506870658, 0.08921972637603393, 0.08921972637603393, 0.08921972637603393, 0.14159752149405735, 0.14159752149405735, 0.14159752149405735, 0.15741141971242556, 0.15741141971242556, 0.15741141971242556, 0.16335146741349127, 0.16335146741349127, 0.16335146741349127, 0.22343034480970025, 0.22343034480970025, 0.22343034480970025, 0.19525300607030194, 0.19525300607030194, 0.19525300607030194, 0.22153987032738331, 0.22153987032738331, 0.22153987032738331, 0.1540870576507738, 0.1540870576507738, 0.1540870576507738, 0.14260928774031123, 0.14260928774031123, 0.14260928774031123, 0.15420020216391017, 0.15420020216391017, 0.15420020216391017, 0.19039834869726835, 0.19039834869726835, 0.19039834869726835, 0.1830355105278526, 0.1830355105278526, 0.1830355105278526, 0.16606035877606606, 0.16606035877606606, 0.16606035877606606, 0.1764479698922865, 0.1764479698922865, 0.1764479698922865, 0.17908140858102228, 0.17908140858102228, 0.17908140858102228, 0.1699515406492501, 0.1699515406492501, 0.1699515406492501, 0.23437486944427122, 0.23437486944427122, 0.23437486944427122, 0.2791037152080491, 0.2791037152080491, 0.2791037152080491, 0.16349512646503994, 0.16349512646503994, 0.16349512646503994, 0.15899061841047735, 0.15899061841047735, 0.15899061841047735, 0.15833138015085302, 0.15833138015085302, 0.15833138015085302, 0.30370443621530374, 0.30370443621530374, 0.30370443621530374, 0.18192218949725536, 0.18192218949725536, 0.18192218949725536, 0.18180749746874125, 0.18180749746874125, 0.18180749746874125, 0.1905877046616331, 0.1905877046616331, 0.1905877046616331, 0.07604299457794783, 0.07604299457794783, 0.07604299457794783, 0.06813368794333652, 0.06813368794333652, 0.06813368794333652, 0.06076252469266186, 0.06076252469266186, 0.06076252469266186]}, "mutation_prompt": null}
{"id": "3e7f3b21-825f-4135-be71-944124cf036d", "solution": "import numpy as np\n\nclass HybridDESA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.pop_size = 10 * dim\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.temp = 1000.0  # Initial temperature for simulated annealing\n        self.cooling_rate = 0.99  # Cooling rate for simulated annealing\n\n    def __call__(self, func):\n        np.random.seed(42)\n        population = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        num_evaluations = self.pop_size\n\n        def mutate(target_idx):\n            indices = [idx for idx in range(self.pop_size) if idx != target_idx]\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            mutant = population[a] + self.F * (population[b] - population[c])\n            mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n            return mutant\n\n        while num_evaluations < self.budget:\n            self.CR = 0.9 - (0.5 * (num_evaluations / self.budget))  # Dynamic adjustment\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                mutant = mutate(i)\n                trial = np.where(np.random.rand(self.dim) < self.CR, mutant, population[i])\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = trial\n\n            # Simulated Annealing part\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                neighbor = population[i] + np.random.normal(0, 0.1 * self.temp / 1000, self.dim)\n                neighbor = np.clip(neighbor, self.lower_bound, self.upper_bound)\n                neighbor_fitness = func(neighbor)\n                num_evaluations += 1\n                delta_fit = neighbor_fitness - fitness[i]\n\n                if delta_fit < 0 or np.random.rand() < np.exp(-delta_fit / self.temp):\n                    population[i] = neighbor\n                    fitness[i] = neighbor_fitness\n                    if neighbor_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = neighbor\n\n            self.temp *= (self.cooling_rate - 0.01 * (num_evaluations / self.budget))  # Adjusted cooling rate\n\n        return best_solution", "name": "HybridDESA", "description": "Improved local search by adjusting the cooling rate dynamically based on budget utilization.", "configspace": "", "generation": 9, "fitness": 0.18005557304144057, "feedback": "The algorithm HybridDESA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.18 with standard deviation 0.17.", "error": "", "parent_id": "ac8e8bf7-43e5-455c-b649-879878463349", "metadata": {"aucs": [0.36559194307593346, 0.36559194307593346, 0.36559194307593346, 0.36821959944843574, 0.36821959944843574, 0.36821959944843574, 0.36500602273062654, 0.36500602273062654, 0.36500602273062654, 0.10406867034643386, 0.10406867034643386, 0.10406867034643386, 0.0785534924992719, 0.0785534924992719, 0.0785534924992719, 0.07744717678893942, 0.07744717678893942, 0.07744717678893942, 0.09612354269472156, 0.09612354269472156, 0.09612354269472156, 0.10394542690874298, 0.10394542690874298, 0.10394542690874298, 0.09315842759967874, 0.09315842759967874, 0.09315842759967874, 0.08716932518423748, 0.08716932518423748, 0.08716932518423748, 0.0790354814185652, 0.0790354814185652, 0.0790354814185652, 0.07956323355111794, 0.07956323355111794, 0.07956323355111794, 0.9653638663755835, 0.9653638663755835, 0.9653638663755835, 0.897597960761129, 0.897597960761129, 0.897597960761129, 0.8235799095926116, 0.8235799095926116, 0.8235799095926116, 0.15226862536059793, 0.15226862536059793, 0.15226862536059793, 0.15127106324314732, 0.15127106324314732, 0.15127106324314732, 0.16042089919786517, 0.16042089919786517, 0.16042089919786517, 0.20038546682505887, 0.20038546682505887, 0.20038546682505887, 0.27415447623554, 0.27415447623554, 0.27415447623554, 0.23290586586776252, 0.23290586586776252, 0.23290586586776252, 0.11121227065068162, 0.11121227065068162, 0.11121227065068162, 0.14221708243666964, 0.14221708243666964, 0.14221708243666964, 0.11171784757995318, 0.11171784757995318, 0.11171784757995318, 0.06972901113020868, 0.06972901113020868, 0.06972901113020868, 0.08777635519606664, 0.08777635519606664, 0.08777635519606664, 0.08936491628130994, 0.08936491628130994, 0.08936491628130994, 0.017181704141592147, 0.017181704141592147, 0.017181704141592147, 0.008576411075592483, 0.008576411075592483, 0.008576411075592483, 0.0025693804399881115, 0.0025693804399881115, 0.0025693804399881115, 0.11079013170499807, 0.11079013170499807, 0.11079013170499807, 0.10804785989090504, 0.10804785989090504, 0.10804785989090504, 0.13581915269722045, 0.13581915269722045, 0.13581915269722045, 0.0015936086250991632, 0.0015936086250991632, 0.0015936086250991632, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.001382412281550005, 0.001382412281550005, 0.001382412281550005, 0.06198734860762256, 0.06198734860762256, 0.06198734860762256, 0.047358622606541556, 0.047358622606541556, 0.047358622606541556, 0.05781384573301862, 0.05781384573301862, 0.05781384573301862, 0.36639524124676826, 0.36639524124676826, 0.36639524124676826, 0.3154517970421481, 0.3154517970421481, 0.3154517970421481, 0.32885675182830165, 0.32885675182830165, 0.32885675182830165, 0.0940727811626888, 0.0940727811626888, 0.0940727811626888, 0.0977591930364512, 0.0977591930364512, 0.0977591930364512, 0.08383783196366679, 0.08383783196366679, 0.08383783196366679, 0.15126494392103196, 0.15126494392103196, 0.15126494392103196, 0.16539553994613776, 0.16539553994613776, 0.16539553994613776, 0.14232953188560604, 0.14232953188560604, 0.14232953188560604, 0.22172498403602836, 0.22172498403602836, 0.22172498403602836, 0.21525164945572828, 0.21525164945572828, 0.21525164945572828, 0.23713467781759934, 0.23713467781759934, 0.23713467781759934, 0.17261867416355603, 0.17261867416355603, 0.17261867416355603, 0.1803312648947667, 0.1803312648947667, 0.1803312648947667, 0.1452645962713932, 0.1452645962713932, 0.1452645962713932, 0.1898942216772449, 0.1898942216772449, 0.1898942216772449, 0.17884967733564494, 0.17884967733564494, 0.17884967733564494, 0.1945449800841993, 0.1945449800841993, 0.1945449800841993, 0.1852722245749805, 0.1852722245749805, 0.1852722245749805, 0.17216279474350593, 0.17216279474350593, 0.17216279474350593, 0.16976390523576446, 0.16976390523576446, 0.16976390523576446, 0.22434323028034853, 0.22434323028034853, 0.22434323028034853, 0.2790839969733966, 0.2790839969733966, 0.2790839969733966, 0.14258355010977974, 0.14258355010977974, 0.14258355010977974, 0.15892338744163048, 0.15892338744163048, 0.15892338744163048, 0.16071826403943412, 0.16071826403943412, 0.16071826403943412, 0.2724818454756479, 0.2724818454756479, 0.2724818454756479, 0.20034211922901424, 0.20034211922901424, 0.20034211922901424, 0.19980037818125507, 0.19980037818125507, 0.19980037818125507, 0.19253417156518016, 0.19253417156518016, 0.19253417156518016, 0.06560190259580834, 0.06560190259580834, 0.06560190259580834, 0.07352877722348217, 0.07352877722348217, 0.07352877722348217, 0.06481393676051372, 0.06481393676051372, 0.06481393676051372]}, "mutation_prompt": null}
{"id": "a23f1c4a-b558-4929-a986-f1cc48f455e1", "solution": "import numpy as np\n\nclass HybridDESA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.pop_size = 10 * dim\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.temp = 1000.0  # Initial temperature for simulated annealing\n        self.cooling_rate = 0.99  # Cooling rate for simulated annealing\n\n    def __call__(self, func):\n        np.random.seed(42)\n        population = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        num_evaluations = self.pop_size\n\n        def mutate(target_idx):\n            indices = [idx for idx in range(self.pop_size) if idx != target_idx]\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            mutant = population[a] + self.F * (population[b] - population[c])\n            mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n            return mutant\n\n        while num_evaluations < self.budget:\n            self.CR = 0.9 - (0.5 * (num_evaluations / self.budget))  # Dynamic adjustment\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                mutant = mutate(i)\n                # modified line below to adaptively adjust CR per individual\n                trial = np.where(np.random.rand(self.dim) < (self.CR if fitness[i] > fitness[best_idx] else self.CR/2), mutant, population[i])\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = trial\n\n            # Simulated Annealing part\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                neighbor = population[i] + np.random.normal(0, 0.1 * self.temp / 1000, self.dim)\n                neighbor = np.clip(neighbor, self.lower_bound, self.upper_bound)\n                neighbor_fitness = func(neighbor)\n                num_evaluations += 1\n                delta_fit = neighbor_fitness - fitness[i]\n\n                if delta_fit < 0 or np.random.rand() < np.exp(-delta_fit / self.temp):\n                    population[i] = neighbor\n                    fitness[i] = neighbor_fitness\n                    if neighbor_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = neighbor\n\n            self.temp *= (self.cooling_rate - 0.01 * (num_evaluations / self.budget))  # Adjusted cooling rate\n\n        return best_solution", "name": "HybridDESA", "description": "Enhanced trial vector generation by introducing an adaptive crossover probability based on individual fitness trends.", "configspace": "", "generation": 10, "fitness": 0.17759535119691025, "feedback": "The algorithm HybridDESA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.18 with standard deviation 0.17.", "error": "", "parent_id": "3e7f3b21-825f-4135-be71-944124cf036d", "metadata": {"aucs": [0.3566493339714363, 0.3566493339714363, 0.3566493339714363, 0.389277639418546, 0.389277639418546, 0.389277639418546, 0.37049029329979677, 0.37049029329979677, 0.37049029329979677, 0.08790086047312584, 0.08790086047312584, 0.08790086047312584, 0.1023842016380282, 0.1023842016380282, 0.1023842016380282, 0.07476608775257498, 0.07476608775257498, 0.07476608775257498, 0.09708739081025763, 0.09708739081025763, 0.09708739081025763, 0.09999507142943709, 0.09999507142943709, 0.09999507142943709, 0.09068496983182595, 0.09068496983182595, 0.09068496983182595, 0.0762289098663963, 0.0762289098663963, 0.0762289098663963, 0.10211593155529175, 0.10211593155529175, 0.10211593155529175, 0.08364300303541294, 0.08364300303541294, 0.08364300303541294, 0.9653638663755835, 0.9653638663755835, 0.9653638663755835, 0.897597960761129, 0.897597960761129, 0.897597960761129, 0.8183509600755687, 0.8183509600755687, 0.8183509600755687, 0.1374450315428698, 0.1374450315428698, 0.1374450315428698, 0.16228065147627713, 0.16228065147627713, 0.16228065147627713, 0.1711992539685696, 0.1711992539685696, 0.1711992539685696, 0.1891376801934349, 0.1891376801934349, 0.1891376801934349, 0.21135213229248162, 0.21135213229248162, 0.21135213229248162, 0.20758838792506795, 0.20758838792506795, 0.20758838792506795, 0.12091129911236187, 0.12091129911236187, 0.12091129911236187, 0.09833598234954444, 0.09833598234954444, 0.09833598234954444, 0.10811862782477966, 0.10811862782477966, 0.10811862782477966, 0.08763994116362028, 0.08763994116362028, 0.08763994116362028, 0.10300901622908187, 0.10300901622908187, 0.10300901622908187, 0.09037402052986687, 0.09037402052986687, 0.09037402052986687, 0.01719419251290455, 0.01719419251290455, 0.01719419251290455, 0.008576411075592483, 0.008576411075592483, 0.008576411075592483, 0.0025693804399881115, 0.0025693804399881115, 0.0025693804399881115, 0.11079013170499807, 0.11079013170499807, 0.11079013170499807, 0.07354806887585907, 0.07354806887585907, 0.07354806887585907, 0.13073409688038207, 0.13073409688038207, 0.13073409688038207, 0.0029494349551434373, 0.0029494349551434373, 0.0029494349551434373, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.001863853374908131, 0.001863853374908131, 0.001863853374908131, 0.05840254088074015, 0.05840254088074015, 0.05840254088074015, 0.049525456899061515, 0.049525456899061515, 0.049525456899061515, 0.05477006770572301, 0.05477006770572301, 0.05477006770572301, 0.36003982292942815, 0.36003982292942815, 0.36003982292942815, 0.33275422665650967, 0.33275422665650967, 0.33275422665650967, 0.31877183834216194, 0.31877183834216194, 0.31877183834216194, 0.09183072381613078, 0.09183072381613078, 0.09183072381613078, 0.08855777579863111, 0.08855777579863111, 0.08855777579863111, 0.08792479019524946, 0.08792479019524946, 0.08792479019524946, 0.15126494392103196, 0.15126494392103196, 0.15126494392103196, 0.1552872345288091, 0.1552872345288091, 0.1552872345288091, 0.1617891956297648, 0.1617891956297648, 0.1617891956297648, 0.21797336533717704, 0.21797336533717704, 0.21797336533717704, 0.20125196410320967, 0.20125196410320967, 0.20125196410320967, 0.23377573415689146, 0.23377573415689146, 0.23377573415689146, 0.15723236573128108, 0.15723236573128108, 0.15723236573128108, 0.1633520720642151, 0.1633520720642151, 0.1633520720642151, 0.1564457694519117, 0.1564457694519117, 0.1564457694519117, 0.19076753411543246, 0.19076753411543246, 0.19076753411543246, 0.1787634473325419, 0.1787634473325419, 0.1787634473325419, 0.17788183998666807, 0.17788183998666807, 0.17788183998666807, 0.17343031084527116, 0.17343031084527116, 0.17343031084527116, 0.18000625654747748, 0.18000625654747748, 0.18000625654747748, 0.18271818252649696, 0.18271818252649696, 0.18271818252649696, 0.22434323028034853, 0.22434323028034853, 0.22434323028034853, 0.28804078017515244, 0.28804078017515244, 0.28804078017515244, 0.14316542209176297, 0.14316542209176297, 0.14316542209176297, 0.1589268123954145, 0.1589268123954145, 0.1589268123954145, 0.16071826403943412, 0.16071826403943412, 0.16071826403943412, 0.25562347398933016, 0.25562347398933016, 0.25562347398933016, 0.20034211922901424, 0.20034211922901424, 0.20034211922901424, 0.1881891973316402, 0.1881891973316402, 0.1881891973316402, 0.19253417156518016, 0.19253417156518016, 0.19253417156518016, 0.0637663342805268, 0.0637663342805268, 0.0637663342805268, 0.06834019010290682, 0.06834019010290682, 0.06834019010290682, 0.0701037564728687, 0.0701037564728687, 0.0701037564728687]}, "mutation_prompt": null}
{"id": "19820c74-fa65-41a3-8e6b-95ce1ce88d73", "solution": "import numpy as np\n\nclass HybridDESA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.pop_size = 10 * dim\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.temp = 1000.0  # Initial temperature for simulated annealing\n        self.cooling_rate = 0.99  # Cooling rate for simulated annealing\n\n    def __call__(self, func):\n        np.random.seed(42)\n        population = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        num_evaluations = self.pop_size\n\n        def mutate(target_idx):\n            indices = [idx for idx in range(self.pop_size) if idx != target_idx]\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            dynamic_F = self.F * (1.0 - (num_evaluations / self.budget))  # Dynamic mutation scaling\n            mutant = population[a] + dynamic_F * (population[b] - population[c])\n            mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n            return mutant\n\n        while num_evaluations < self.budget:\n            self.CR = 0.9 - (0.5 * (num_evaluations / self.budget))  # Dynamic adjustment\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                mutant = mutate(i)\n                trial = np.where(np.random.rand(self.dim) < self.CR, mutant, population[i])\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = trial\n\n            # Simulated Annealing part\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                neighbor = population[i] + np.random.normal(0, 0.1 * self.temp / 1000, self.dim)\n                neighbor = np.clip(neighbor, self.lower_bound, self.upper_bound)\n                neighbor_fitness = func(neighbor)\n                num_evaluations += 1\n                delta_fit = neighbor_fitness - fitness[i]\n\n                if delta_fit < 0 or np.random.rand() < np.exp(-delta_fit / self.temp):\n                    population[i] = neighbor\n                    fitness[i] = neighbor_fitness\n                    if neighbor_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = neighbor\n\n            self.temp *= (self.cooling_rate - 0.01 * (num_evaluations / self.budget))  # Adjusted cooling rate\n\n        return best_solution", "name": "HybridDESA", "description": "Enhanced convergence by utilizing dynamic mutation scaling based on remaining budget percentage.", "configspace": "", "generation": 11, "fitness": 0.1929207793270903, "feedback": "The algorithm HybridDESA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.19 with standard deviation 0.17.", "error": "", "parent_id": "3e7f3b21-825f-4135-be71-944124cf036d", "metadata": {"aucs": [0.38951030328017455, 0.38951030328017455, 0.38951030328017455, 0.4091806226655603, 0.4091806226655603, 0.4091806226655603, 0.3974475755193786, 0.3974475755193786, 0.3974475755193786, 0.1256773915766919, 0.1256773915766919, 0.1256773915766919, 0.12709936805780475, 0.12709936805780475, 0.12709936805780475, 0.13869786775952886, 0.13869786775952886, 0.13869786775952886, 0.11642091112074271, 0.11642091112074271, 0.11642091112074271, 0.11489418376813099, 0.11489418376813099, 0.11489418376813099, 0.10897280204601067, 0.10897280204601067, 0.10897280204601067, 0.08922617450272852, 0.08922617450272852, 0.08922617450272852, 0.09244652421020616, 0.09244652421020616, 0.09244652421020616, 0.0875207941981827, 0.0875207941981827, 0.0875207941981827, 0.9653695092226641, 0.9653695092226641, 0.9653695092226641, 0.8665737929502205, 0.8665737929502205, 0.8665737929502205, 0.805112649388662, 0.805112649388662, 0.805112649388662, 0.18889663907547594, 0.18889663907547594, 0.18889663907547594, 0.18046033075978707, 0.18046033075978707, 0.18046033075978707, 0.17794351907915296, 0.17794351907915296, 0.17794351907915296, 0.24563080892645084, 0.24563080892645084, 0.24563080892645084, 0.2587195534004122, 0.2587195534004122, 0.2587195534004122, 0.2513586427564304, 0.2513586427564304, 0.2513586427564304, 0.11506626818139953, 0.11506626818139953, 0.11506626818139953, 0.11060259617635926, 0.11060259617635926, 0.11060259617635926, 0.11840144246176132, 0.11840144246176132, 0.11840144246176132, 0.10198401303355442, 0.10198401303355442, 0.10198401303355442, 0.1058220261190036, 0.1058220261190036, 0.1058220261190036, 0.09548827985694386, 0.09548827985694386, 0.09548827985694386, 0.02466098901877778, 0.02466098901877778, 0.02466098901877778, 0.027545024305597132, 0.027545024305597132, 0.027545024305597132, 0.024360366144043688, 0.024360366144043688, 0.024360366144043688, 0.15782812678105818, 0.15782812678105818, 0.15782812678105818, 0.12309346652913444, 0.12309346652913444, 0.12309346652913444, 0.09750011422425597, 0.09750011422425597, 0.09750011422425597, 0.009440001061789327, 0.009440001061789327, 0.009440001061789327, 0.006580553828085756, 0.006580553828085756, 0.006580553828085756, 0.012488660778952498, 0.012488660778952498, 0.012488660778952498, 0.08984646515702299, 0.08984646515702299, 0.08984646515702299, 0.07146223352406256, 0.07146223352406256, 0.07146223352406256, 0.08274347820624206, 0.08274347820624206, 0.08274347820624206, 0.3634801665244468, 0.3634801665244468, 0.3634801665244468, 0.3628662584949103, 0.3628662584949103, 0.3628662584949103, 0.36941661849610397, 0.36941661849610397, 0.36941661849610397, 0.10196100529582564, 0.10196100529582564, 0.10196100529582564, 0.09463260364703618, 0.09463260364703618, 0.09463260364703618, 0.09490168278230282, 0.09490168278230282, 0.09490168278230282, 0.16981440293889705, 0.16981440293889705, 0.16981440293889705, 0.14558028562357272, 0.14558028562357272, 0.14558028562357272, 0.16684408704157194, 0.16684408704157194, 0.16684408704157194, 0.25004764971270355, 0.25004764971270355, 0.25004764971270355, 0.2377743628891924, 0.2377743628891924, 0.2377743628891924, 0.2631059677858566, 0.2631059677858566, 0.2631059677858566, 0.19151951311206317, 0.19151951311206317, 0.19151951311206317, 0.17379114687863162, 0.17379114687863162, 0.17379114687863162, 0.1939561715432273, 0.1939561715432273, 0.1939561715432273, 0.1897078620111834, 0.1897078620111834, 0.1897078620111834, 0.19802803491289367, 0.19802803491289367, 0.19802803491289367, 0.2078853222064443, 0.2078853222064443, 0.2078853222064443, 0.16943213073034402, 0.16943213073034402, 0.16943213073034402, 0.18220516883494298, 0.18220516883494298, 0.18220516883494298, 0.19366600775521792, 0.19366600775521792, 0.19366600775521792, 0.16466192205800145, 0.16466192205800145, 0.16466192205800145, 0.2805842624008976, 0.2805842624008976, 0.2805842624008976, 0.2016414166554903, 0.2016414166554903, 0.2016414166554903, 0.1581172294038673, 0.1581172294038673, 0.1581172294038673, 0.18603198629871587, 0.18603198629871587, 0.18603198629871587, 0.2539268433731089, 0.2539268433731089, 0.2539268433731089, 0.18109511043037496, 0.18109511043037496, 0.18109511043037496, 0.20656840063557858, 0.20656840063557858, 0.20656840063557858, 0.19761611841977567, 0.19761611841977567, 0.19761611841977567, 0.0751771357667792, 0.0751771357667792, 0.0751771357667792, 0.07544821379804467, 0.07544821379804467, 0.07544821379804467, 0.07673695344005571, 0.07673695344005571, 0.07673695344005571]}, "mutation_prompt": null}
{"id": "04e56a3b-6a1e-4fc2-8037-ec8264a15a8e", "solution": "import numpy as np\n\nclass HybridDESA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.pop_size = 10 * dim\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.temp = 1000.0  # Initial temperature for simulated annealing\n        self.cooling_rate = 0.99  # Cooling rate for simulated annealing\n\n    def __call__(self, func):\n        np.random.seed(42)\n        population = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        num_evaluations = self.pop_size\n\n        def mutate(target_idx):\n            indices = [idx for idx in range(self.pop_size) if idx != target_idx]\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            dynamic_F = self.F * (1.0 - (num_evaluations / self.budget))  # Dynamic mutation scaling\n            mutant = population[a] + dynamic_F * (population[b] - population[c])\n            mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n            return mutant\n\n        while num_evaluations < self.budget:\n            self.CR = 0.9 - (0.5 * (num_evaluations / self.budget))  # Dynamic adjustment\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                mutant = mutate(i)\n                trial = np.where(np.random.rand(self.dim) < self.CR, mutant, population[i])\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = trial\n\n            # Simulated Annealing part\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                neighbor = population[i] + np.random.normal(0, 0.1 * self.temp / 1000, self.dim)\n                neighbor = np.clip(neighbor, self.lower_bound, self.upper_bound)\n                neighbor_fitness = func(neighbor)\n                num_evaluations += 1\n                delta_fit = neighbor_fitness - fitness[i]\n\n                if delta_fit < 0 or np.random.rand() < np.exp(-delta_fit / self.temp):\n                    population[i] = neighbor\n                    fitness[i] = neighbor_fitness\n                    if neighbor_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = neighbor\n\n            self.temp *= (self.cooling_rate - 0.01 * (num_evaluations / self.budget))  # Adjusted cooling rate\n\n        return best_solution", "name": "HybridDESA", "description": "Enhanced convergence by utilizing dynamic mutation scaling based on remaining budget percentage.", "configspace": "", "generation": 12, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "19820c74-fa65-41a3-8e6b-95ce1ce88d73", "metadata": {"aucs": [0.38951030328017455, 0.38951030328017455, 0.38951030328017455, 0.4091806226655603, 0.4091806226655603, 0.4091806226655603, 0.3974475755193786, 0.3974475755193786, 0.3974475755193786, 0.1256773915766919, 0.1256773915766919, 0.1256773915766919, 0.12709936805780475, 0.12709936805780475, 0.12709936805780475, 0.13869786775952886, 0.13869786775952886, 0.13869786775952886, 0.11642091112074271, 0.11642091112074271, 0.11642091112074271, 0.11489418376813099, 0.11489418376813099, 0.11489418376813099, 0.10897280204601067, 0.10897280204601067, 0.10897280204601067, 0.08922617450272852, 0.08922617450272852, 0.08922617450272852, 0.09244652421020616, 0.09244652421020616, 0.09244652421020616, 0.0875207941981827, 0.0875207941981827, 0.0875207941981827, 0.9653695092226641, 0.9653695092226641, 0.9653695092226641, 0.8665737929502205, 0.8665737929502205, 0.8665737929502205, 0.805112649388662, 0.805112649388662, 0.805112649388662, 0.18889663907547594, 0.18889663907547594, 0.18889663907547594, 0.18046033075978707, 0.18046033075978707, 0.18046033075978707, 0.17794351907915296, 0.17794351907915296, 0.17794351907915296, 0.24563080892645084, 0.24563080892645084, 0.24563080892645084, 0.2587195534004122, 0.2587195534004122, 0.2587195534004122, 0.2513586427564304, 0.2513586427564304, 0.2513586427564304, 0.11506626818139953, 0.11506626818139953, 0.11506626818139953, 0.11060259617635926, 0.11060259617635926, 0.11060259617635926, 0.11840144246176132, 0.11840144246176132, 0.11840144246176132, 0.10198401303355442, 0.10198401303355442, 0.10198401303355442, 0.1058220261190036, 0.1058220261190036, 0.1058220261190036, 0.09548827985694386, 0.09548827985694386, 0.09548827985694386, 0.02466098901877778, 0.02466098901877778, 0.02466098901877778, 0.027545024305597132, 0.027545024305597132, 0.027545024305597132, 0.024360366144043688, 0.024360366144043688, 0.024360366144043688, 0.15782812678105818, 0.15782812678105818, 0.15782812678105818, 0.12309346652913444, 0.12309346652913444, 0.12309346652913444, 0.09750011422425597, 0.09750011422425597, 0.09750011422425597, 0.009440001061789327, 0.009440001061789327, 0.009440001061789327, 0.006580553828085756, 0.006580553828085756, 0.006580553828085756, 0.012488660778952498, 0.012488660778952498, 0.012488660778952498, 0.08984646515702299, 0.08984646515702299, 0.08984646515702299, 0.07146223352406256, 0.07146223352406256, 0.07146223352406256, 0.08274347820624206, 0.08274347820624206, 0.08274347820624206, 0.3634801665244468, 0.3634801665244468, 0.3634801665244468, 0.3628662584949103, 0.3628662584949103, 0.3628662584949103, 0.36941661849610397, 0.36941661849610397, 0.36941661849610397, 0.10196100529582564, 0.10196100529582564, 0.10196100529582564, 0.09463260364703618, 0.09463260364703618, 0.09463260364703618, 0.09490168278230282, 0.09490168278230282, 0.09490168278230282, 0.16981440293889705, 0.16981440293889705, 0.16981440293889705, 0.14558028562357272, 0.14558028562357272, 0.14558028562357272, 0.16684408704157194, 0.16684408704157194, 0.16684408704157194, 0.25004764971270355, 0.25004764971270355, 0.25004764971270355, 0.2377743628891924, 0.2377743628891924, 0.2377743628891924, 0.2631059677858566, 0.2631059677858566, 0.2631059677858566, 0.19151951311206317, 0.19151951311206317, 0.19151951311206317, 0.17379114687863162, 0.17379114687863162, 0.17379114687863162, 0.1939561715432273, 0.1939561715432273, 0.1939561715432273, 0.1897078620111834, 0.1897078620111834, 0.1897078620111834, 0.19802803491289367, 0.19802803491289367, 0.19802803491289367, 0.2078853222064443, 0.2078853222064443, 0.2078853222064443, 0.16943213073034402, 0.16943213073034402, 0.16943213073034402, 0.18220516883494298, 0.18220516883494298, 0.18220516883494298, 0.19366600775521792, 0.19366600775521792, 0.19366600775521792, 0.16466192205800145, 0.16466192205800145, 0.16466192205800145, 0.2805842624008976, 0.2805842624008976, 0.2805842624008976, 0.2016414166554903, 0.2016414166554903, 0.2016414166554903, 0.1581172294038673, 0.1581172294038673, 0.1581172294038673, 0.18603198629871587, 0.18603198629871587, 0.18603198629871587, 0.2539268433731089, 0.2539268433731089, 0.2539268433731089, 0.18109511043037496, 0.18109511043037496, 0.18109511043037496, 0.20656840063557858, 0.20656840063557858, 0.20656840063557858, 0.19761611841977567, 0.19761611841977567, 0.19761611841977567, 0.0751771357667792, 0.0751771357667792, 0.0751771357667792, 0.07544821379804467, 0.07544821379804467, 0.07544821379804467, 0.07673695344005571, 0.07673695344005571, 0.07673695344005571]}, "mutation_prompt": null}
{"id": "e5290540-e016-4f9d-b42f-1ba8b8c3375f", "solution": "import numpy as np\n\nclass HybridDESA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.pop_size = 10 * dim\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.temp = 1000.0  # Initial temperature for simulated annealing\n        self.cooling_rate = 0.99  # Cooling rate for simulated annealing\n\n    def __call__(self, func):\n        np.random.seed(42)\n        population = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        num_evaluations = self.pop_size\n\n        def mutate(target_idx):\n            indices = [idx for idx in range(self.pop_size) if idx != target_idx]\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            dynamic_F = self.F * (1.0 - (num_evaluations / self.budget))  # Dynamic mutation scaling\n            mutant = population[a] + dynamic_F * (population[b] - population[c])\n            mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n            return mutant\n\n        while num_evaluations < self.budget:\n            self.CR = 0.9 - (0.5 * (num_evaluations / self.budget))  # Dynamic adjustment\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                mutant = mutate(i)\n                trial = np.where(np.random.rand(self.dim) < self.CR, mutant, population[i])\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = trial\n\n            # Simulated Annealing part\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                neighbor = population[i] + np.random.normal(0, 0.1 * self.temp / 1000, self.dim)\n                neighbor = np.clip(neighbor, self.lower_bound, self.upper_bound)\n                neighbor_fitness = func(neighbor)\n                num_evaluations += 1\n                delta_fit = neighbor_fitness - fitness[i]\n\n                if delta_fit < 0 or np.random.rand() < np.exp(-delta_fit / self.temp):\n                    population[i] = neighbor\n                    fitness[i] = neighbor_fitness\n                    if neighbor_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = neighbor\n\n            self.temp *= (self.cooling_rate - 0.01 * (num_evaluations / self.budget))  # Adjusted cooling rate\n\n        return best_solution", "name": "HybridDESA", "description": "Enhanced convergence by utilizing dynamic mutation scaling based on remaining budget percentage.", "configspace": "", "generation": 12, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "19820c74-fa65-41a3-8e6b-95ce1ce88d73", "metadata": {"aucs": [0.38951030328017455, 0.38951030328017455, 0.38951030328017455, 0.4091806226655603, 0.4091806226655603, 0.4091806226655603, 0.3974475755193786, 0.3974475755193786, 0.3974475755193786, 0.1256773915766919, 0.1256773915766919, 0.1256773915766919, 0.12709936805780475, 0.12709936805780475, 0.12709936805780475, 0.13869786775952886, 0.13869786775952886, 0.13869786775952886, 0.11642091112074271, 0.11642091112074271, 0.11642091112074271, 0.11489418376813099, 0.11489418376813099, 0.11489418376813099, 0.10897280204601067, 0.10897280204601067, 0.10897280204601067, 0.08922617450272852, 0.08922617450272852, 0.08922617450272852, 0.09244652421020616, 0.09244652421020616, 0.09244652421020616, 0.0875207941981827, 0.0875207941981827, 0.0875207941981827, 0.9653695092226641, 0.9653695092226641, 0.9653695092226641, 0.8665737929502205, 0.8665737929502205, 0.8665737929502205, 0.805112649388662, 0.805112649388662, 0.805112649388662, 0.18889663907547594, 0.18889663907547594, 0.18889663907547594, 0.18046033075978707, 0.18046033075978707, 0.18046033075978707, 0.17794351907915296, 0.17794351907915296, 0.17794351907915296, 0.24563080892645084, 0.24563080892645084, 0.24563080892645084, 0.2587195534004122, 0.2587195534004122, 0.2587195534004122, 0.2513586427564304, 0.2513586427564304, 0.2513586427564304, 0.11506626818139953, 0.11506626818139953, 0.11506626818139953, 0.11060259617635926, 0.11060259617635926, 0.11060259617635926, 0.11840144246176132, 0.11840144246176132, 0.11840144246176132, 0.10198401303355442, 0.10198401303355442, 0.10198401303355442, 0.1058220261190036, 0.1058220261190036, 0.1058220261190036, 0.09548827985694386, 0.09548827985694386, 0.09548827985694386, 0.02466098901877778, 0.02466098901877778, 0.02466098901877778, 0.027545024305597132, 0.027545024305597132, 0.027545024305597132, 0.024360366144043688, 0.024360366144043688, 0.024360366144043688, 0.15782812678105818, 0.15782812678105818, 0.15782812678105818, 0.12309346652913444, 0.12309346652913444, 0.12309346652913444, 0.09750011422425597, 0.09750011422425597, 0.09750011422425597, 0.009440001061789327, 0.009440001061789327, 0.009440001061789327, 0.006580553828085756, 0.006580553828085756, 0.006580553828085756, 0.012488660778952498, 0.012488660778952498, 0.012488660778952498, 0.08984646515702299, 0.08984646515702299, 0.08984646515702299, 0.07146223352406256, 0.07146223352406256, 0.07146223352406256, 0.08274347820624206, 0.08274347820624206, 0.08274347820624206, 0.3634801665244468, 0.3634801665244468, 0.3634801665244468, 0.3628662584949103, 0.3628662584949103, 0.3628662584949103, 0.36941661849610397, 0.36941661849610397, 0.36941661849610397, 0.10196100529582564, 0.10196100529582564, 0.10196100529582564, 0.09463260364703618, 0.09463260364703618, 0.09463260364703618, 0.09490168278230282, 0.09490168278230282, 0.09490168278230282, 0.16981440293889705, 0.16981440293889705, 0.16981440293889705, 0.14558028562357272, 0.14558028562357272, 0.14558028562357272, 0.16684408704157194, 0.16684408704157194, 0.16684408704157194, 0.25004764971270355, 0.25004764971270355, 0.25004764971270355, 0.2377743628891924, 0.2377743628891924, 0.2377743628891924, 0.2631059677858566, 0.2631059677858566, 0.2631059677858566, 0.19151951311206317, 0.19151951311206317, 0.19151951311206317, 0.17379114687863162, 0.17379114687863162, 0.17379114687863162, 0.1939561715432273, 0.1939561715432273, 0.1939561715432273, 0.1897078620111834, 0.1897078620111834, 0.1897078620111834, 0.19802803491289367, 0.19802803491289367, 0.19802803491289367, 0.2078853222064443, 0.2078853222064443, 0.2078853222064443, 0.16943213073034402, 0.16943213073034402, 0.16943213073034402, 0.18220516883494298, 0.18220516883494298, 0.18220516883494298, 0.19366600775521792, 0.19366600775521792, 0.19366600775521792, 0.16466192205800145, 0.16466192205800145, 0.16466192205800145, 0.2805842624008976, 0.2805842624008976, 0.2805842624008976, 0.2016414166554903, 0.2016414166554903, 0.2016414166554903, 0.1581172294038673, 0.1581172294038673, 0.1581172294038673, 0.18603198629871587, 0.18603198629871587, 0.18603198629871587, 0.2539268433731089, 0.2539268433731089, 0.2539268433731089, 0.18109511043037496, 0.18109511043037496, 0.18109511043037496, 0.20656840063557858, 0.20656840063557858, 0.20656840063557858, 0.19761611841977567, 0.19761611841977567, 0.19761611841977567, 0.0751771357667792, 0.0751771357667792, 0.0751771357667792, 0.07544821379804467, 0.07544821379804467, 0.07544821379804467, 0.07673695344005571, 0.07673695344005571, 0.07673695344005571]}, "mutation_prompt": null}
{"id": "d8d1aef1-3452-4350-8651-5a6a728d30c5", "solution": "import numpy as np\n\nclass HybridDESA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.pop_size = 10 * dim\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.temp = 1000.0  # Initial temperature for simulated annealing\n        self.cooling_rate = 0.99  # Cooling rate for simulated annealing\n\n    def __call__(self, func):\n        np.random.seed(42)\n        population = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        num_evaluations = self.pop_size\n\n        def mutate(target_idx):\n            indices = [idx for idx in range(self.pop_size) if idx != target_idx]\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            diversity = np.std(population, axis=0).mean()  # Calculate population diversity\n            dynamic_F = self.F * (1.0 - (num_evaluations / self.budget)) * diversity  # Adjusted dynamic mutation scaling\n            mutant = population[a] + dynamic_F * (population[b] - population[c])\n            mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n            return mutant\n\n        while num_evaluations < self.budget:\n            self.CR = 0.9 - (0.5 * (num_evaluations / self.budget))  # Dynamic adjustment\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                mutant = mutate(i)\n                trial = np.where(np.random.rand(self.dim) < self.CR, mutant, population[i])\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = trial\n\n            # Simulated Annealing part\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                neighbor = population[i] + np.random.normal(0, 0.1 * self.temp / 1000, self.dim)\n                neighbor = np.clip(neighbor, self.lower_bound, self.upper_bound)\n                neighbor_fitness = func(neighbor)\n                num_evaluations += 1\n                delta_fit = neighbor_fitness - fitness[i]\n\n                if delta_fit < 0 or np.random.rand() < np.exp(-delta_fit / self.temp):\n                    population[i] = neighbor\n                    fitness[i] = neighbor_fitness\n                    if neighbor_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = neighbor\n\n            self.temp *= (self.cooling_rate - 0.01 * (num_evaluations / self.budget))  # Adjusted cooling rate\n\n        return best_solution", "name": "HybridDESA", "description": "Introduced adaptive mutation scaling based on population diversity to enhance exploration and convergence.", "configspace": "", "generation": 14, "fitness": 0.17090048242960093, "feedback": "The algorithm HybridDESA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.17 with standard deviation 0.19.", "error": "", "parent_id": "19820c74-fa65-41a3-8e6b-95ce1ce88d73", "metadata": {"aucs": [0.34492949757576086, 0.34492949757576086, 0.34492949757576086, 0.41371857653442423, 0.41371857653442423, 0.41371857653442423, 0.38281229978971454, 0.38281229978971454, 0.38281229978971454, 0.11506459064798347, 0.11506459064798347, 0.11506459064798347, 0.11126524813430128, 0.11126524813430128, 0.11126524813430128, 0.10827633001839398, 0.10827633001839398, 0.10827633001839398, 0.11106488651346058, 0.11106488651346058, 0.11106488651346058, 0.1102479276246826, 0.1102479276246826, 0.1102479276246826, 0.11138631245719877, 0.11138631245719877, 0.11138631245719877, 0.07110596451746931, 0.07110596451746931, 0.07110596451746931, 0.07604225050353763, 0.07604225050353763, 0.07604225050353763, 0.07877234216943418, 0.07877234216943418, 0.07877234216943418, 0.9676580955238329, 0.9676580955238329, 0.9676580955238329, 0.9944695989024602, 0.9944695989024602, 0.9944695989024602, 0.9832829454003873, 0.9832829454003873, 0.9832829454003873, 0.1751287905616482, 0.1751287905616482, 0.1751287905616482, 0.16085992673753358, 0.16085992673753358, 0.16085992673753358, 0.14161458335025012, 0.14161458335025012, 0.14161458335025012, 0.12153581930623547, 0.12153581930623547, 0.12153581930623547, 0.16726294639234407, 0.16726294639234407, 0.16726294639234407, 0.22047255401349308, 0.22047255401349308, 0.22047255401349308, 0.09574509044263346, 0.09574509044263346, 0.09574509044263346, 0.07773048693901241, 0.07773048693901241, 0.07773048693901241, 0.10336790382712646, 0.10336790382712646, 0.10336790382712646, 0.09249038986734681, 0.09249038986734681, 0.09249038986734681, 0.06446503465488018, 0.06446503465488018, 0.06446503465488018, 0.03606710851607642, 0.03606710851607642, 0.03606710851607642, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.009257863738640792, 0.009257863738640792, 0.009257863738640792, 0.04042624352481328, 0.04042624352481328, 0.04042624352481328, 0.04990013535881266, 0.04990013535881266, 0.04990013535881266, 0.05352793113911669, 0.05352793113911669, 0.05352793113911669, 0.00015614896106252996, 0.00015614896106252996, 0.00015614896106252996, 0.012282805726379742, 0.012282805726379742, 0.012282805726379742, 0.042973910113484615, 0.042973910113484615, 0.042973910113484615, 0.07633285408391965, 0.07633285408391965, 0.07633285408391965, 0.05279744097715944, 0.05279744097715944, 0.05279744097715944, 0.07631161650910745, 0.07631161650910745, 0.07631161650910745, 0.3277828705100748, 0.3277828705100748, 0.3277828705100748, 0.31261780030270436, 0.31261780030270436, 0.31261780030270436, 0.32790541942205076, 0.32790541942205076, 0.32790541942205076, 0.10898068873329625, 0.10898068873329625, 0.10898068873329625, 0.07424312525213117, 0.07424312525213117, 0.07424312525213117, 0.06327951486332417, 0.06327951486332417, 0.06327951486332417, 0.2306083400579768, 0.2306083400579768, 0.2306083400579768, 0.1769479438727416, 0.1769479438727416, 0.1769479438727416, 0.1748890282975717, 0.1748890282975717, 0.1748890282975717, 0.19164450043226866, 0.19164450043226866, 0.19164450043226866, 0.1685090716539962, 0.1685090716539962, 0.1685090716539962, 0.21600205063756317, 0.21600205063756317, 0.21600205063756317, 0.11767305294308605, 0.11767305294308605, 0.11767305294308605, 0.1370509801269746, 0.1370509801269746, 0.1370509801269746, 0.13300664591448008, 0.13300664591448008, 0.13300664591448008, 0.15733002180671718, 0.15733002180671718, 0.15733002180671718, 0.1596079651268727, 0.1596079651268727, 0.1596079651268727, 0.1847247739431166, 0.1847247739431166, 0.1847247739431166, 0.1496778817318516, 0.1496778817318516, 0.1496778817318516, 0.12489875666963313, 0.12489875666963313, 0.12489875666963313, 0.12602613597957535, 0.12602613597957535, 0.12602613597957535, 0.2780852704031246, 0.2780852704031246, 0.2780852704031246, 0.15274095424374046, 0.15274095424374046, 0.15274095424374046, 0.13724119064634, 0.13724119064634, 0.13724119064634, 0.14863096874458714, 0.14863096874458714, 0.14863096874458714, 0.12694325690835617, 0.12694325690835617, 0.12694325690835617, 0.19053426634507753, 0.19053426634507753, 0.19053426634507753, 0.18686685964289607, 0.18686685964289607, 0.18686685964289607, 0.19203260363109498, 0.19203260363109498, 0.19203260363109498, 0.18862085320749344, 0.18862085320749344, 0.18862085320749344, 0.06219791386997586, 0.06219791386997586, 0.06219791386997586, 0.05927504454304888, 0.05927504454304888, 0.05927504454304888, 0.06725453341340704, 0.06725453341340704, 0.06725453341340704]}, "mutation_prompt": null}
{"id": "5b376164-dfcd-4a55-9d3c-6d36da4e97e6", "solution": "import numpy as np\n\nclass HybridDESA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.pop_size = 10 * dim\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.temp = 1000.0  # Initial temperature for simulated annealing\n        self.cooling_rate = 0.99  # Cooling rate for simulated annealing\n\n    def __call__(self, func):\n        np.random.seed(42)\n        population = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        num_evaluations = self.pop_size\n\n        def mutate(target_idx):\n            indices = [idx for idx in range(self.pop_size) if idx != target_idx]\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            dynamic_F = self.F * (1.0 - (num_evaluations / self.budget))  # Dynamic mutation scaling\n            mutant = population[a] + dynamic_F * (population[b] - population[c])\n            mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n            return mutant\n\n        while num_evaluations < self.budget:\n            self.CR = 0.9 - (0.5 * (num_evaluations / self.budget))  # Dynamic adjustment\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                mutant = mutate(i)\n                trial = np.where(np.random.rand(self.dim) < self.CR, mutant, population[i])\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = trial\n\n            # Simulated Annealing part\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                neighbor = population[i] + np.random.normal(0, 0.1 * self.temp / 1000, self.dim)\n                neighbor = np.clip(neighbor, self.lower_bound, self.upper_bound)\n                neighbor_fitness = func(neighbor)\n                num_evaluations += 1\n                delta_fit = neighbor_fitness - fitness[i]\n\n                if delta_fit < 0 or np.random.rand() < np.exp(-delta_fit / self.temp):\n                    population[i] = neighbor\n                    fitness[i] = neighbor_fitness\n                    if neighbor_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = neighbor\n\n            self.temp *= (self.cooling_rate - 0.01 * (num_evaluations / self.budget))  # Adjusted cooling rate\n            self.pop_size = max(4, int(10 * self.dim * (1 - num_evaluations / self.budget)))  # Adaptive population size\n\n        return best_solution", "name": "HybridDESA", "description": "Improved convergence by incorporating adaptive population size reduction based on budget utilization.", "configspace": "", "generation": 15, "fitness": 0.21783043262304266, "feedback": "The algorithm HybridDESA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.22 with standard deviation 0.17.", "error": "", "parent_id": "19820c74-fa65-41a3-8e6b-95ce1ce88d73", "metadata": {"aucs": [0.46249603996285416, 0.46249603996285416, 0.46249603996285416, 0.4527863317756421, 0.4527863317756421, 0.4527863317756421, 0.48237244949018454, 0.48237244949018454, 0.48237244949018454, 0.1663394441659608, 0.1663394441659608, 0.1663394441659608, 0.1549882095067031, 0.1549882095067031, 0.1549882095067031, 0.17472832444634456, 0.17472832444634456, 0.17472832444634456, 0.12255243565216511, 0.12255243565216511, 0.12255243565216511, 0.1321028695134513, 0.1321028695134513, 0.1321028695134513, 0.12118877838018705, 0.12118877838018705, 0.12118877838018705, 0.09782255638390835, 0.09782255638390835, 0.09782255638390835, 0.09927052328241459, 0.09927052328241459, 0.09927052328241459, 0.10809467385484983, 0.10809467385484983, 0.10809467385484983, 0.89222355424323, 0.89222355424323, 0.89222355424323, 0.8566216599705146, 0.8566216599705146, 0.8566216599705146, 0.8537698356174489, 0.8537698356174489, 0.8537698356174489, 0.2426614033938297, 0.2426614033938297, 0.2426614033938297, 0.25933927059701356, 0.25933927059701356, 0.25933927059701356, 0.2531651219853659, 0.2531651219853659, 0.2531651219853659, 0.24440965298730233, 0.24440965298730233, 0.24440965298730233, 0.293011011434523, 0.293011011434523, 0.293011011434523, 0.4469611334601671, 0.4469611334601671, 0.4469611334601671, 0.11680032166559284, 0.11680032166559284, 0.11680032166559284, 0.13441988177792352, 0.13441988177792352, 0.13441988177792352, 0.13026149508248364, 0.13026149508248364, 0.13026149508248364, 0.1066020430672967, 0.1066020430672967, 0.1066020430672967, 0.10641691619392746, 0.10641691619392746, 0.10641691619392746, 0.11919895304746386, 0.11919895304746386, 0.11919895304746386, 0.06046623351196101, 0.06046623351196101, 0.06046623351196101, 0.06241939581906897, 0.06241939581906897, 0.06241939581906897, 0.06040466785041476, 0.06040466785041476, 0.06040466785041476, 0.1393252503182728, 0.1393252503182728, 0.1393252503182728, 0.11006135595973743, 0.11006135595973743, 0.11006135595973743, 0.1265915021395485, 0.1265915021395485, 0.1265915021395485, 0.04220663101938904, 0.04220663101938904, 0.04220663101938904, 0.027265901703501405, 0.027265901703501405, 0.027265901703501405, 0.05140983961384904, 0.05140983961384904, 0.05140983961384904, 0.1334093240280263, 0.1334093240280263, 0.1334093240280263, 0.10465667548430979, 0.10465667548430979, 0.10465667548430979, 0.11972763787086682, 0.11972763787086682, 0.11972763787086682, 0.3670578147447746, 0.3670578147447746, 0.3670578147447746, 0.37479136070377583, 0.37479136070377583, 0.37479136070377583, 0.399481105729394, 0.399481105729394, 0.399481105729394, 0.10074671261856893, 0.10074671261856893, 0.10074671261856893, 0.11390377782515515, 0.11390377782515515, 0.11390377782515515, 0.11100805999546715, 0.11100805999546715, 0.11100805999546715, 0.1411399213754475, 0.1411399213754475, 0.1411399213754475, 0.17409600652204704, 0.17409600652204704, 0.17409600652204704, 0.2157447464270199, 0.2157447464270199, 0.2157447464270199, 0.28854083482054194, 0.28854083482054194, 0.28854083482054194, 0.2839772404343255, 0.2839772404343255, 0.2839772404343255, 0.29023760337643667, 0.29023760337643667, 0.29023760337643667, 0.19678517685903374, 0.19678517685903374, 0.19678517685903374, 0.19902219601187265, 0.19902219601187265, 0.19902219601187265, 0.22454874724860086, 0.22454874724860086, 0.22454874724860086, 0.19879447934574024, 0.19879447934574024, 0.19879447934574024, 0.18527087519728114, 0.18527087519728114, 0.18527087519728114, 0.21754081370301614, 0.21754081370301614, 0.21754081370301614, 0.19278070584795004, 0.19278070584795004, 0.19278070584795004, 0.20010630246523597, 0.20010630246523597, 0.20010630246523597, 0.20278809180765534, 0.20278809180765534, 0.20278809180765534, 0.49466125360900726, 0.49466125360900726, 0.49466125360900726, 0.22437277572474146, 0.22437277572474146, 0.22437277572474146, 0.2230975535128733, 0.2230975535128733, 0.2230975535128733, 0.21142244112692887, 0.21142244112692887, 0.21142244112692887, 0.17259571561302034, 0.17259571561302034, 0.17259571561302034, 0.19758552164104048, 0.19758552164104048, 0.19758552164104048, 0.18311406977500566, 0.18311406977500566, 0.18311406977500566, 0.2020853934763469, 0.2020853934763469, 0.2020853934763469, 0.18002401844501414, 0.18002401844501414, 0.18002401844501414, 0.0941473266563293, 0.0941473266563293, 0.0941473266563293, 0.08092663682335755, 0.08092663682335755, 0.08092663682335755, 0.07284656313837112, 0.07284656313837112, 0.07284656313837112]}, "mutation_prompt": null}
{"id": "2a189148-cfe4-4ead-8a5c-8cd5936dc0ae", "solution": "import numpy as np\n\nclass HybridDESA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.pop_size = 10 * dim\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.temp = 1000.0  # Initial temperature for simulated annealing\n        self.cooling_rate = 0.99  # Cooling rate for simulated annealing\n\n    def __call__(self, func):\n        np.random.seed(42)\n        population = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        num_evaluations = self.pop_size\n\n        def mutate(target_idx):\n            indices = [idx for idx in range(self.pop_size) if idx != target_idx]\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            dynamic_F = self.F * (1.0 - (num_evaluations / self.budget))  # Dynamic mutation scaling\n            mutant = population[a] + dynamic_F * (population[b] - population[c])\n            mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n            return mutant\n\n        while num_evaluations < self.budget:\n            self.CR = 0.9 - (0.6 * (num_evaluations / self.budget))  # Dynamic adjustment\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                mutant = mutate(i)\n                trial = np.where(np.random.rand(self.dim) < self.CR, mutant, population[i])\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = trial\n\n            # Simulated Annealing part\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                neighbor = population[i] + np.random.normal(0, 0.1 * self.temp / 1000, self.dim)\n                neighbor = np.clip(neighbor, self.lower_bound, self.upper_bound)\n                neighbor_fitness = func(neighbor)\n                num_evaluations += 1\n                delta_fit = neighbor_fitness - fitness[i]\n\n                if delta_fit < 0 or np.random.rand() < np.exp(-delta_fit / self.temp):\n                    population[i] = neighbor\n                    fitness[i] = neighbor_fitness\n                    if neighbor_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = neighbor\n\n            self.temp *= (self.cooling_rate - 0.01 * (num_evaluations / self.budget))  # Adjusted cooling rate\n            self.pop_size = max(4, int(10 * self.dim * (1 - num_evaluations / self.budget)))  # Adaptive population size\n\n        return best_solution", "name": "HybridDESA", "description": "Introduced a dynamic scaling factor for crossover probability adjustment to balance exploration and exploitation.", "configspace": "", "generation": 16, "fitness": 0.22058529506917784, "feedback": "The algorithm HybridDESA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.22 with standard deviation 0.18.", "error": "", "parent_id": "5b376164-dfcd-4a55-9d3c-6d36da4e97e6", "metadata": {"aucs": [0.4739815114301914, 0.4739815114301914, 0.4739815114301914, 0.4580713451873616, 0.4580713451873616, 0.4580713451873616, 0.4620130756557551, 0.4620130756557551, 0.4620130756557551, 0.17038982440279704, 0.17038982440279704, 0.17038982440279704, 0.15385152873300612, 0.15385152873300612, 0.15385152873300612, 0.1663339143461786, 0.1663339143461786, 0.1663339143461786, 0.12486335942924287, 0.12486335942924287, 0.12486335942924287, 0.10813262048403394, 0.10813262048403394, 0.10813262048403394, 0.11881270881742478, 0.11881270881742478, 0.11881270881742478, 0.10824281801852631, 0.10824281801852631, 0.10824281801852631, 0.11322547617394407, 0.11322547617394407, 0.11322547617394407, 0.10405913499750241, 0.10405913499750241, 0.10405913499750241, 0.853278449181514, 0.853278449181514, 0.853278449181514, 0.8473735328002856, 0.8473735328002856, 0.8473735328002856, 0.8800149853290414, 0.8800149853290414, 0.8800149853290414, 0.24133741314659451, 0.24133741314659451, 0.24133741314659451, 0.24718444240218929, 0.24718444240218929, 0.24718444240218929, 0.23341780277017476, 0.23341780277017476, 0.23341780277017476, 0.3439750718484226, 0.3439750718484226, 0.3439750718484226, 0.4103809775169721, 0.4103809775169721, 0.4103809775169721, 0.4633511200904816, 0.4633511200904816, 0.4633511200904816, 0.12831696777215296, 0.12831696777215296, 0.12831696777215296, 0.12159210028599376, 0.12159210028599376, 0.12159210028599376, 0.13397392204348713, 0.13397392204348713, 0.13397392204348713, 0.10253543569337886, 0.10253543569337886, 0.10253543569337886, 0.12764177756725947, 0.12764177756725947, 0.12764177756725947, 0.12644172802085796, 0.12644172802085796, 0.12644172802085796, 0.04558130172842012, 0.04558130172842012, 0.04558130172842012, 0.04473178048447968, 0.04473178048447968, 0.04473178048447968, 0.032070071593435334, 0.032070071593435334, 0.032070071593435334, 0.10103826365976853, 0.10103826365976853, 0.10103826365976853, 0.12276910875703129, 0.12276910875703129, 0.12276910875703129, 0.13936216289098324, 0.13936216289098324, 0.13936216289098324, 0.031449435153703464, 0.031449435153703464, 0.031449435153703464, 0.039555157704201926, 0.039555157704201926, 0.039555157704201926, 0.038945578913286094, 0.038945578913286094, 0.038945578913286094, 0.06955693202966884, 0.06955693202966884, 0.06955693202966884, 0.09934859166641341, 0.09934859166641341, 0.09934859166641341, 0.0828010724014897, 0.0828010724014897, 0.0828010724014897, 0.3964037824809562, 0.3964037824809562, 0.3964037824809562, 0.4281453999699457, 0.4281453999699457, 0.4281453999699457, 0.3827708880194084, 0.3827708880194084, 0.3827708880194084, 0.1047301359746381, 0.1047301359746381, 0.1047301359746381, 0.10075756308933237, 0.10075756308933237, 0.10075756308933237, 0.10143109282174423, 0.10143109282174423, 0.10143109282174423, 0.2002442689435744, 0.2002442689435744, 0.2002442689435744, 0.13599365658700002, 0.13599365658700002, 0.13599365658700002, 0.20154024037259655, 0.20154024037259655, 0.20154024037259655, 0.281901808326366, 0.281901808326366, 0.281901808326366, 0.292416731372546, 0.292416731372546, 0.292416731372546, 0.293614742175324, 0.293614742175324, 0.293614742175324, 0.20399577759668586, 0.20399577759668586, 0.20399577759668586, 0.19936064241421225, 0.19936064241421225, 0.19936064241421225, 0.22120392117735355, 0.22120392117735355, 0.22120392117735355, 0.20330017238642206, 0.20330017238642206, 0.20330017238642206, 0.19037896125160014, 0.19037896125160014, 0.19037896125160014, 0.2069963523471009, 0.2069963523471009, 0.2069963523471009, 0.2088400952495869, 0.2088400952495869, 0.2088400952495869, 0.1886904893644189, 0.1886904893644189, 0.1886904893644189, 0.2033576341792741, 0.2033576341792741, 0.2033576341792741, 0.3390971968709994, 0.3390971968709994, 0.3390971968709994, 0.3953614518219484, 0.3953614518219484, 0.3953614518219484, 0.30417065081796735, 0.30417065081796735, 0.30417065081796735, 0.19466015571668427, 0.19466015571668427, 0.19466015571668427, 0.1843802854418044, 0.1843802854418044, 0.1843802854418044, 0.24249775789581796, 0.24249775789581796, 0.24249775789581796, 0.18912521789970993, 0.18912521789970993, 0.18912521789970993, 0.18344980531936905, 0.18344980531936905, 0.18344980531936905, 0.18339191537139388, 0.18339191537139388, 0.18339191537139388, 0.08679174932959377, 0.08679174932959377, 0.08679174932959377, 0.07902304718636621, 0.07902304718636621, 0.07902304718636621, 0.08411515408141113, 0.08411515408141113, 0.08411515408141113]}, "mutation_prompt": null}
{"id": "bdcf50ad-e82e-43ef-b108-6616019fb75c", "solution": "import numpy as np\n\nclass HybridDESA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.pop_size = 10 * dim\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.temp = 1000.0  # Initial temperature for simulated annealing\n        self.cooling_rate = 0.99  # Cooling rate for simulated annealing\n\n    def __call__(self, func):\n        np.random.seed(42)\n        population = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        num_evaluations = self.pop_size\n\n        def mutate(target_idx):\n            indices = [idx for idx in range(self.pop_size) if idx != target_idx]\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            dynamic_F = self.F * (1.0 - (num_evaluations / self.budget))  # Dynamic mutation scaling\n            mutant = population[a] + dynamic_F * (population[b] - population[c])\n            mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n            return mutant\n\n        while num_evaluations < self.budget:\n            self.CR = 0.9 - (0.6 * (num_evaluations / self.budget))  # Dynamic adjustment\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                mutant = mutate(i)\n                trial = np.where(np.random.rand(self.dim) < self.CR, mutant, population[i])\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = trial\n\n            # Simulated Annealing part\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                neighbor = population[i] + np.random.normal(0, 0.1 * self.temp / 1000, self.dim)\n                neighbor = np.clip(neighbor, self.lower_bound, self.upper_bound)\n                neighbor_fitness = func(neighbor)\n                num_evaluations += 1\n                delta_fit = neighbor_fitness - fitness[i]\n\n                if delta_fit < 0 or np.random.rand() < np.exp(-delta_fit / self.temp):\n                    population[i] = neighbor\n                    fitness[i] = neighbor_fitness\n                    if neighbor_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = neighbor\n\n            self.temp *= (self.cooling_rate - 0.01 * (num_evaluations / self.budget))  # Adjusted cooling rate\n            self.pop_size = max(4, int(10 * self.dim * (1.05 - num_evaluations / self.budget)))  # Adaptive population size\n\n        return best_solution", "name": "HybridDESA", "description": "Improved exploitation by increasing mutation probability dynamically based on evaluations.", "configspace": "", "generation": 17, "fitness": -Infinity, "feedback": "An exception occurred: IndexError('index 50 is out of bounds for axis 0 with size 50').", "error": "IndexError('index 50 is out of bounds for axis 0 with size 50')", "parent_id": "2a189148-cfe4-4ead-8a5c-8cd5936dc0ae", "metadata": {}, "mutation_prompt": null}
{"id": "fd553434-e739-464a-b5c6-85f96b5470ab", "solution": "import numpy as np\n\nclass HybridDESA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.pop_size = 10 * dim\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.temp = 1000.0  # Initial temperature for simulated annealing\n        self.cooling_rate = 0.99  # Cooling rate for simulated annealing\n\n    def __call__(self, func):\n        np.random.seed(42)\n        population = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        num_evaluations = self.pop_size\n\n        def mutate(target_idx):\n            indices = [idx for idx in range(self.pop_size) if idx != target_idx]\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            dynamic_F = self.F * (1.0 - (num_evaluations / self.budget))  # Dynamic mutation scaling\n            mutant = population[a] + dynamic_F * (population[b] - population[c])\n            mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n            return mutant\n\n        while num_evaluations < self.budget:\n            self.CR = 0.9 - (0.6 * (num_evaluations / self.budget))  # Dynamic adjustment\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                mutant = mutate(i)\n                trial = np.where(np.random.rand(self.dim) < self.CR, mutant, population[i])\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = trial\n\n            # Simulated Annealing part\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                neighbor = population[i] + np.random.normal(0, 0.1 * self.temp / 1000, self.dim)\n                neighbor = np.clip(neighbor, self.lower_bound, self.upper_bound)\n                neighbor_fitness = func(neighbor)\n                num_evaluations += 1\n                delta_fit = neighbor_fitness - fitness[i]\n\n                if delta_fit < 0 or np.random.rand() < np.exp(-delta_fit / self.temp):\n                    population[i] = neighbor\n                    fitness[i] = neighbor_fitness\n                    if neighbor_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = neighbor\n\n            self.temp *= (self.cooling_rate - 0.005 * (num_evaluations / self.budget))  # Adjusted cooling rate\n            self.pop_size = max(4, int(10 * self.dim * (1 - num_evaluations / self.budget)))  # Adaptive population size\n\n        return best_solution", "name": "HybridDESA", "description": "Enhanced exploitation by modifying cooling rate to improve convergence.", "configspace": "", "generation": 18, "fitness": 0.21086198688590319, "feedback": "The algorithm HybridDESA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.21 with standard deviation 0.16.", "error": "", "parent_id": "2a189148-cfe4-4ead-8a5c-8cd5936dc0ae", "metadata": {"aucs": [0.4396102670912877, 0.4396102670912877, 0.4396102670912877, 0.44197439604233313, 0.44197439604233313, 0.44197439604233313, 0.44446971541952984, 0.44446971541952984, 0.44446971541952984, 0.15213852956676532, 0.15213852956676532, 0.15213852956676532, 0.1642036606387628, 0.1642036606387628, 0.1642036606387628, 0.14895117226044108, 0.14895117226044108, 0.14895117226044108, 0.13832681909416988, 0.13832681909416988, 0.13832681909416988, 0.10623136242205833, 0.10623136242205833, 0.10623136242205833, 0.12425143156350682, 0.12425143156350682, 0.12425143156350682, 0.09736371416129808, 0.09736371416129808, 0.09736371416129808, 0.10348312642891067, 0.10348312642891067, 0.10348312642891067, 0.10665045938517537, 0.10665045938517537, 0.10665045938517537, 0.853295061090551, 0.853295061090551, 0.853295061090551, 0.8473884230053733, 0.8473884230053733, 0.8473884230053733, 0.6910144573652686, 0.6910144573652686, 0.6910144573652686, 0.22571010881129583, 0.22571010881129583, 0.22571010881129583, 0.2282517064237808, 0.2282517064237808, 0.2282517064237808, 0.23039199611276073, 0.23039199611276073, 0.23039199611276073, 0.24280736047264395, 0.24280736047264395, 0.24280736047264395, 0.42846803845006076, 0.42846803845006076, 0.42846803845006076, 0.39689410810123404, 0.39689410810123404, 0.39689410810123404, 0.12136008679561239, 0.12136008679561239, 0.12136008679561239, 0.11595574148139154, 0.11595574148139154, 0.11595574148139154, 0.13517992162987735, 0.13517992162987735, 0.13517992162987735, 0.09718934851105954, 0.09718934851105954, 0.09718934851105954, 0.12558333369386487, 0.12558333369386487, 0.12558333369386487, 0.12594348735949135, 0.12594348735949135, 0.12594348735949135, 0.04021457471739853, 0.04021457471739853, 0.04021457471739853, 0.042149354946731776, 0.042149354946731776, 0.042149354946731776, 0.057169497526022184, 0.057169497526022184, 0.057169497526022184, 0.11212488718806346, 0.11212488718806346, 0.11212488718806346, 0.13186832630993517, 0.13186832630993517, 0.13186832630993517, 0.12924875312328443, 0.12924875312328443, 0.12924875312328443, 0.0362108207719275, 0.0362108207719275, 0.0362108207719275, 0.036367133594455536, 0.036367133594455536, 0.036367133594455536, 0.03765280993682485, 0.03765280993682485, 0.03765280993682485, 0.08527411585728428, 0.08527411585728428, 0.08527411585728428, 0.09099423424815267, 0.09099423424815267, 0.09099423424815267, 0.09094986579650122, 0.09094986579650122, 0.09094986579650122, 0.3911345960001221, 0.3911345960001221, 0.3911345960001221, 0.37244451257512845, 0.37244451257512845, 0.37244451257512845, 0.3736059284067963, 0.3736059284067963, 0.3736059284067963, 0.15453878401992827, 0.15453878401992827, 0.15453878401992827, 0.11803047442331949, 0.11803047442331949, 0.11803047442331949, 0.10501965465283625, 0.10501965465283625, 0.10501965465283625, 0.17431028369334822, 0.17431028369334822, 0.17431028369334822, 0.18136392500783682, 0.18136392500783682, 0.18136392500783682, 0.1890798194328741, 0.1890798194328741, 0.1890798194328741, 0.2765185455160474, 0.2765185455160474, 0.2765185455160474, 0.28346972760412104, 0.28346972760412104, 0.28346972760412104, 0.284446217120935, 0.284446217120935, 0.284446217120935, 0.20256294605775416, 0.20256294605775416, 0.20256294605775416, 0.19672380211788143, 0.19672380211788143, 0.19672380211788143, 0.21108952819131388, 0.21108952819131388, 0.21108952819131388, 0.18531380261381147, 0.18531380261381147, 0.18531380261381147, 0.2101420410025303, 0.2101420410025303, 0.2101420410025303, 0.24070512275918376, 0.24070512275918376, 0.24070512275918376, 0.19977135939237411, 0.19977135939237411, 0.19977135939237411, 0.18137162935556495, 0.18137162935556495, 0.18137162935556495, 0.24792510404963597, 0.24792510404963597, 0.24792510404963597, 0.2778619088913411, 0.2778619088913411, 0.2778619088913411, 0.3477828296665968, 0.3477828296665968, 0.3477828296665968, 0.1562966494861846, 0.1562966494861846, 0.1562966494861846, 0.19472483393324647, 0.19472483393324647, 0.19472483393324647, 0.1847724345890961, 0.1847724345890961, 0.1847724345890961, 0.21466299989180393, 0.21466299989180393, 0.21466299989180393, 0.18141550341708812, 0.18141550341708812, 0.18141550341708812, 0.1924549735159603, 0.1924549735159603, 0.1924549735159603, 0.18921661557316283, 0.18921661557316283, 0.18921661557316283, 0.08106614285308533, 0.08106614285308533, 0.08106614285308533, 0.07519097812513831, 0.07519097812513831, 0.07519097812513831, 0.08373717445389517, 0.08373717445389517, 0.08373717445389517]}, "mutation_prompt": null}
{"id": "398ab905-a3de-42c5-a700-546322907711", "solution": "import numpy as np\n\nclass HybridDESA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.pop_size = 10 * dim\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.temp = 1000.0  # Initial temperature for simulated annealing\n        self.cooling_rate = 0.99  # Cooling rate for simulated annealing\n\n    def __call__(self, func):\n        np.random.seed(42)\n        population = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        num_evaluations = self.pop_size\n\n        def mutate(target_idx):\n            indices = [idx for idx in range(self.pop_size) if idx != target_idx]\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            dynamic_F = self.F * (1.0 - (num_evaluations / self.budget))  # Dynamic mutation scaling\n            mutant = population[a] + dynamic_F * (population[b] - population[c])\n            mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n            return mutant\n\n        while num_evaluations < self.budget:\n            self.CR = 0.9 - (0.6 * (num_evaluations / self.budget))  # Dynamic adjustment\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                mutant = mutate(i)\n                trial = np.where(np.random.rand(self.dim) < self.CR, mutant, population[i])\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = trial\n\n            # Simulated Annealing part\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                neighbor = population[i] + np.random.normal(0, 0.1 * self.temp / 1000, self.dim)\n                neighbor = np.clip(neighbor, self.lower_bound, self.upper_bound)\n                neighbor_fitness = func(neighbor)\n                num_evaluations += 1\n                delta_fit = neighbor_fitness - fitness[i]\n\n                if delta_fit < 0 or np.random.rand() < np.exp(-delta_fit / self.temp):\n                    population[i] = neighbor\n                    fitness[i] = neighbor_fitness\n                    if neighbor_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = neighbor\n\n            self.temp *= (self.cooling_rate - 0.01 * (num_evaluations / self.budget))  # Adjusted cooling rate\n            self.pop_size = max(4, int(10 * self.dim * (1 - num_evaluations / self.budget)))  # Adaptive population size\n\n        return best_solution", "name": "HybridDESA", "description": "Introduced a dynamic scaling factor for crossover probability adjustment to balance exploration and exploitation.", "configspace": "", "generation": 17, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "2a189148-cfe4-4ead-8a5c-8cd5936dc0ae", "metadata": {"aucs": [0.4739815114301914, 0.4739815114301914, 0.4739815114301914, 0.4580713451873616, 0.4580713451873616, 0.4580713451873616, 0.4620130756557551, 0.4620130756557551, 0.4620130756557551, 0.17038982440279704, 0.17038982440279704, 0.17038982440279704, 0.15385152873300612, 0.15385152873300612, 0.15385152873300612, 0.1663339143461786, 0.1663339143461786, 0.1663339143461786, 0.12486335942924287, 0.12486335942924287, 0.12486335942924287, 0.10813262048403394, 0.10813262048403394, 0.10813262048403394, 0.11881270881742478, 0.11881270881742478, 0.11881270881742478, 0.10824281801852631, 0.10824281801852631, 0.10824281801852631, 0.11322547617394407, 0.11322547617394407, 0.11322547617394407, 0.10405913499750241, 0.10405913499750241, 0.10405913499750241, 0.853278449181514, 0.853278449181514, 0.853278449181514, 0.8473735328002856, 0.8473735328002856, 0.8473735328002856, 0.8800149853290414, 0.8800149853290414, 0.8800149853290414, 0.24133741314659451, 0.24133741314659451, 0.24133741314659451, 0.24718444240218929, 0.24718444240218929, 0.24718444240218929, 0.23341780277017476, 0.23341780277017476, 0.23341780277017476, 0.3439750718484226, 0.3439750718484226, 0.3439750718484226, 0.4103809775169721, 0.4103809775169721, 0.4103809775169721, 0.4633511200904816, 0.4633511200904816, 0.4633511200904816, 0.12831696777215296, 0.12831696777215296, 0.12831696777215296, 0.12159210028599376, 0.12159210028599376, 0.12159210028599376, 0.13397392204348713, 0.13397392204348713, 0.13397392204348713, 0.10253543569337886, 0.10253543569337886, 0.10253543569337886, 0.12764177756725947, 0.12764177756725947, 0.12764177756725947, 0.12644172802085796, 0.12644172802085796, 0.12644172802085796, 0.04558130172842012, 0.04558130172842012, 0.04558130172842012, 0.04473178048447968, 0.04473178048447968, 0.04473178048447968, 0.032070071593435334, 0.032070071593435334, 0.032070071593435334, 0.10103826365976853, 0.10103826365976853, 0.10103826365976853, 0.12276910875703129, 0.12276910875703129, 0.12276910875703129, 0.13936216289098324, 0.13936216289098324, 0.13936216289098324, 0.031449435153703464, 0.031449435153703464, 0.031449435153703464, 0.039555157704201926, 0.039555157704201926, 0.039555157704201926, 0.038945578913286094, 0.038945578913286094, 0.038945578913286094, 0.06955693202966884, 0.06955693202966884, 0.06955693202966884, 0.09934859166641341, 0.09934859166641341, 0.09934859166641341, 0.0828010724014897, 0.0828010724014897, 0.0828010724014897, 0.3964037824809562, 0.3964037824809562, 0.3964037824809562, 0.4281453999699457, 0.4281453999699457, 0.4281453999699457, 0.3827708880194084, 0.3827708880194084, 0.3827708880194084, 0.1047301359746381, 0.1047301359746381, 0.1047301359746381, 0.10075756308933237, 0.10075756308933237, 0.10075756308933237, 0.10143109282174423, 0.10143109282174423, 0.10143109282174423, 0.2002442689435744, 0.2002442689435744, 0.2002442689435744, 0.13599365658700002, 0.13599365658700002, 0.13599365658700002, 0.20154024037259655, 0.20154024037259655, 0.20154024037259655, 0.281901808326366, 0.281901808326366, 0.281901808326366, 0.292416731372546, 0.292416731372546, 0.292416731372546, 0.293614742175324, 0.293614742175324, 0.293614742175324, 0.20399577759668586, 0.20399577759668586, 0.20399577759668586, 0.19936064241421225, 0.19936064241421225, 0.19936064241421225, 0.22120392117735355, 0.22120392117735355, 0.22120392117735355, 0.20330017238642206, 0.20330017238642206, 0.20330017238642206, 0.19037896125160014, 0.19037896125160014, 0.19037896125160014, 0.2069963523471009, 0.2069963523471009, 0.2069963523471009, 0.2088400952495869, 0.2088400952495869, 0.2088400952495869, 0.1886904893644189, 0.1886904893644189, 0.1886904893644189, 0.2033576341792741, 0.2033576341792741, 0.2033576341792741, 0.3390971968709994, 0.3390971968709994, 0.3390971968709994, 0.3953614518219484, 0.3953614518219484, 0.3953614518219484, 0.30417065081796735, 0.30417065081796735, 0.30417065081796735, 0.19466015571668427, 0.19466015571668427, 0.19466015571668427, 0.1843802854418044, 0.1843802854418044, 0.1843802854418044, 0.24249775789581796, 0.24249775789581796, 0.24249775789581796, 0.18912521789970993, 0.18912521789970993, 0.18912521789970993, 0.18344980531936905, 0.18344980531936905, 0.18344980531936905, 0.18339191537139388, 0.18339191537139388, 0.18339191537139388, 0.08679174932959377, 0.08679174932959377, 0.08679174932959377, 0.07902304718636621, 0.07902304718636621, 0.07902304718636621, 0.08411515408141113, 0.08411515408141113, 0.08411515408141113]}, "mutation_prompt": null}
{"id": "cdd7a867-f2ff-4f54-a42f-1bbe60e3ec6a", "solution": "import numpy as np\n\nclass HybridDESA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.pop_size = 10 * dim\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.temp = 1000.0  # Initial temperature for simulated annealing\n        self.cooling_rate = 0.99  # Cooling rate for simulated annealing\n\n    def __call__(self, func):\n        np.random.seed(42)\n        population = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        num_evaluations = self.pop_size\n\n        def mutate(target_idx):\n            indices = [idx for idx in range(self.pop_size) if idx != target_idx]\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            dynamic_F = self.F * (1.0 - (num_evaluations / self.budget))  # Dynamic mutation scaling\n            mutant = population[a] + dynamic_F * (population[b] - population[c])\n            mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n            return mutant\n\n        while num_evaluations < self.budget:\n            self.CR = 0.9 - (0.6 * (num_evaluations / self.budget))  # Dynamic adjustment\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                mutant = mutate(i)\n                trial = np.where(np.random.rand(self.dim) < self.CR, mutant, population[i])\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = trial\n\n            # Simulated Annealing part\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                neighbor = population[i] + np.random.normal(0, 0.1 * self.temp / 1000, self.dim)\n                neighbor = np.clip(neighbor, self.lower_bound, self.upper_bound)\n                neighbor_fitness = func(neighbor)\n                num_evaluations += 1\n                delta_fit = neighbor_fitness - fitness[i]\n\n                if delta_fit < 0 or np.random.rand() < np.exp(-delta_fit / self.temp):\n                    population[i] = neighbor\n                    fitness[i] = neighbor_fitness\n                    if neighbor_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = neighbor\n\n            self.temp *= (self.cooling_rate - 0.015 * (num_evaluations / self.budget))  # Adjusted cooling rate\n            self.pop_size = max(4, int(10 * self.dim * (1 - num_evaluations / self.budget)))  # Adaptive population size\n\n        return best_solution", "name": "HybridDESA", "description": "Adjusted dynamic scaling in simulated annealing to improve convergence rate.", "configspace": "", "generation": 20, "fitness": 0.21738859713713457, "feedback": "The algorithm HybridDESA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.22 with standard deviation 0.17.", "error": "", "parent_id": "2a189148-cfe4-4ead-8a5c-8cd5936dc0ae", "metadata": {"aucs": [0.4712879515981194, 0.4712879515981194, 0.4712879515981194, 0.4802148757360877, 0.4802148757360877, 0.4802148757360877, 0.4750931968451665, 0.4750931968451665, 0.4750931968451665, 0.1800383437969113, 0.1800383437969113, 0.1800383437969113, 0.16518860687504278, 0.16518860687504278, 0.16518860687504278, 0.17119097861342414, 0.17119097861342414, 0.17119097861342414, 0.19005865018941837, 0.19005865018941837, 0.19005865018941837, 0.11695253987192122, 0.11695253987192122, 0.11695253987192122, 0.18715205286128833, 0.18715205286128833, 0.18715205286128833, 0.09442954288198302, 0.09442954288198302, 0.09442954288198302, 0.1097508042471187, 0.1097508042471187, 0.1097508042471187, 0.09660715225149419, 0.09660715225149419, 0.09660715225149419, 0.8532630392033351, 0.8532630392033351, 0.8532630392033351, 0.8298640661666343, 0.8298640661666343, 0.8298640661666343, 0.880005855144401, 0.880005855144401, 0.880005855144401, 0.2520410669576527, 0.2520410669576527, 0.2520410669576527, 0.2633141118271144, 0.2633141118271144, 0.2633141118271144, 0.24576433136494102, 0.24576433136494102, 0.24576433136494102, 0.2582783936962122, 0.2582783936962122, 0.2582783936962122, 0.29237016746619715, 0.29237016746619715, 0.29237016746619715, 0.38404218188109396, 0.38404218188109396, 0.38404218188109396, 0.12023050290200243, 0.12023050290200243, 0.12023050290200243, 0.13383595670991766, 0.13383595670991766, 0.13383595670991766, 0.1397343126024584, 0.1397343126024584, 0.1397343126024584, 0.1097558806407054, 0.1097558806407054, 0.1097558806407054, 0.10879643136186878, 0.10879643136186878, 0.10879643136186878, 0.11292948954572113, 0.11292948954572113, 0.11292948954572113, 0.05272573329225283, 0.05272573329225283, 0.05272573329225283, 0.021095296514987094, 0.021095296514987094, 0.021095296514987094, 0.03554820082584098, 0.03554820082584098, 0.03554820082584098, 0.09889453439690277, 0.09889453439690277, 0.09889453439690277, 0.11917779191108657, 0.11917779191108657, 0.11917779191108657, 0.11951674288440683, 0.11951674288440683, 0.11951674288440683, 0.049600896636942116, 0.049600896636942116, 0.049600896636942116, 0.04387797686735029, 0.04387797686735029, 0.04387797686735029, 0.03585857727332409, 0.03585857727332409, 0.03585857727332409, 0.0931324700670817, 0.0931324700670817, 0.0931324700670817, 0.11218450904771615, 0.11218450904771615, 0.11218450904771615, 0.09123828893538188, 0.09123828893538188, 0.09123828893538188, 0.3721876775659466, 0.3721876775659466, 0.3721876775659466, 0.39892958589059613, 0.39892958589059613, 0.39892958589059613, 0.37174685783921113, 0.37174685783921113, 0.37174685783921113, 0.10774372600083226, 0.10774372600083226, 0.10774372600083226, 0.10558171444324504, 0.10558171444324504, 0.10558171444324504, 0.09476539850716437, 0.09476539850716437, 0.09476539850716437, 0.14413507673739223, 0.14413507673739223, 0.14413507673739223, 0.17860367268945754, 0.17860367268945754, 0.17860367268945754, 0.1564642155417072, 0.1564642155417072, 0.1564642155417072, 0.28463415972626727, 0.28463415972626727, 0.28463415972626727, 0.28950443551442195, 0.28950443551442195, 0.28950443551442195, 0.30858020026351596, 0.30858020026351596, 0.30858020026351596, 0.2159540837372561, 0.2159540837372561, 0.2159540837372561, 0.21148550896734408, 0.21148550896734408, 0.21148550896734408, 0.21640504263472138, 0.21640504263472138, 0.21640504263472138, 0.2048447826632125, 0.2048447826632125, 0.2048447826632125, 0.19463171182021555, 0.19463171182021555, 0.19463171182021555, 0.19926534153561848, 0.19926534153561848, 0.19926534153561848, 0.19600894919763312, 0.19600894919763312, 0.19600894919763312, 0.18196948700998505, 0.18196948700998505, 0.18196948700998505, 0.1875605012348417, 0.1875605012348417, 0.1875605012348417, 0.36990004246279484, 0.36990004246279484, 0.36990004246279484, 0.510623654475556, 0.510623654475556, 0.510623654475556, 0.1799684396071498, 0.1799684396071498, 0.1799684396071498, 0.19459360472640352, 0.19459360472640352, 0.19459360472640352, 0.18472070006831554, 0.18472070006831554, 0.18472070006831554, 0.19350075997747795, 0.19350075997747795, 0.19350075997747795, 0.18756091166497812, 0.18756091166497812, 0.18756091166497812, 0.19420299133305863, 0.19420299133305863, 0.19420299133305863, 0.18800977223628612, 0.18800977223628612, 0.18800977223628612, 0.07446271821984862, 0.07446271821984862, 0.07446271821984862, 0.08139381125641443, 0.08139381125641443, 0.08139381125641443, 0.07699795643331586, 0.07699795643331586, 0.07699795643331586]}, "mutation_prompt": null}
{"id": "cf015ec9-f13b-46c6-8d5d-307db34d9a49", "solution": "import numpy as np\n\nclass HybridDESA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.pop_size = 10 * dim\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.temp = 1000.0  # Initial temperature for simulated annealing\n        self.cooling_rate = 0.99  # Cooling rate for simulated annealing\n\n    def __call__(self, func):\n        np.random.seed(42)\n        population = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        num_evaluations = self.pop_size\n\n        def mutate(target_idx):\n            indices = [idx for idx in range(self.pop_size) if idx != target_idx]\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            dynamic_F = self.F * (1.0 - (num_evaluations / self.budget))  # Dynamic mutation scaling\n            mutant = population[a] + dynamic_F * (population[b] - population[c])\n            mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n            return mutant\n\n        while num_evaluations < self.budget:\n            self.CR = 0.9 - (0.6 * (num_evaluations / self.budget))  # Dynamic adjustment\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                mutant = mutate(i)\n                trial = np.where(np.random.rand(self.dim) < self.CR, mutant, population[i])\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = trial\n\n            # Simulated Annealing part\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                neighbor = population[i] + np.random.normal(0, 0.1 * self.temp / 1000, self.dim)\n                neighbor = np.clip(neighbor, self.lower_bound, self.upper_bound)\n                neighbor_fitness = func(neighbor)\n                num_evaluations += 1\n                delta_fit = neighbor_fitness - fitness[i]\n\n                if delta_fit < 0 or np.random.rand() < np.exp(-delta_fit / self.temp):\n                    population[i] = neighbor\n                    fitness[i] = neighbor_fitness\n                    if neighbor_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = neighbor\n\n            self.temp *= (self.cooling_rate - 0.01 * (num_evaluations / self.budget))  # Adjusted cooling rate\n            self.pop_size = max(4, int(10 * self.dim * (1 - num_evaluations / self.budget)))  # Adaptive population size\n\n        return best_solution", "name": "HybridDESA", "description": "Introduced a dynamic scaling factor for crossover probability adjustment to balance exploration and exploitation.", "configspace": "", "generation": 17, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "2a189148-cfe4-4ead-8a5c-8cd5936dc0ae", "metadata": {"aucs": [0.4739815114301914, 0.4739815114301914, 0.4739815114301914, 0.4580713451873616, 0.4580713451873616, 0.4580713451873616, 0.4620130756557551, 0.4620130756557551, 0.4620130756557551, 0.17038982440279704, 0.17038982440279704, 0.17038982440279704, 0.15385152873300612, 0.15385152873300612, 0.15385152873300612, 0.1663339143461786, 0.1663339143461786, 0.1663339143461786, 0.12486335942924287, 0.12486335942924287, 0.12486335942924287, 0.10813262048403394, 0.10813262048403394, 0.10813262048403394, 0.11881270881742478, 0.11881270881742478, 0.11881270881742478, 0.10824281801852631, 0.10824281801852631, 0.10824281801852631, 0.11322547617394407, 0.11322547617394407, 0.11322547617394407, 0.10405913499750241, 0.10405913499750241, 0.10405913499750241, 0.853278449181514, 0.853278449181514, 0.853278449181514, 0.8473735328002856, 0.8473735328002856, 0.8473735328002856, 0.8800149853290414, 0.8800149853290414, 0.8800149853290414, 0.24133741314659451, 0.24133741314659451, 0.24133741314659451, 0.24718444240218929, 0.24718444240218929, 0.24718444240218929, 0.23341780277017476, 0.23341780277017476, 0.23341780277017476, 0.3439750718484226, 0.3439750718484226, 0.3439750718484226, 0.4103809775169721, 0.4103809775169721, 0.4103809775169721, 0.4633511200904816, 0.4633511200904816, 0.4633511200904816, 0.12831696777215296, 0.12831696777215296, 0.12831696777215296, 0.12159210028599376, 0.12159210028599376, 0.12159210028599376, 0.13397392204348713, 0.13397392204348713, 0.13397392204348713, 0.10253543569337886, 0.10253543569337886, 0.10253543569337886, 0.12764177756725947, 0.12764177756725947, 0.12764177756725947, 0.12644172802085796, 0.12644172802085796, 0.12644172802085796, 0.04558130172842012, 0.04558130172842012, 0.04558130172842012, 0.04473178048447968, 0.04473178048447968, 0.04473178048447968, 0.032070071593435334, 0.032070071593435334, 0.032070071593435334, 0.10103826365976853, 0.10103826365976853, 0.10103826365976853, 0.12276910875703129, 0.12276910875703129, 0.12276910875703129, 0.13936216289098324, 0.13936216289098324, 0.13936216289098324, 0.031449435153703464, 0.031449435153703464, 0.031449435153703464, 0.039555157704201926, 0.039555157704201926, 0.039555157704201926, 0.038945578913286094, 0.038945578913286094, 0.038945578913286094, 0.06955693202966884, 0.06955693202966884, 0.06955693202966884, 0.09934859166641341, 0.09934859166641341, 0.09934859166641341, 0.0828010724014897, 0.0828010724014897, 0.0828010724014897, 0.3964037824809562, 0.3964037824809562, 0.3964037824809562, 0.4281453999699457, 0.4281453999699457, 0.4281453999699457, 0.3827708880194084, 0.3827708880194084, 0.3827708880194084, 0.1047301359746381, 0.1047301359746381, 0.1047301359746381, 0.10075756308933237, 0.10075756308933237, 0.10075756308933237, 0.10143109282174423, 0.10143109282174423, 0.10143109282174423, 0.2002442689435744, 0.2002442689435744, 0.2002442689435744, 0.13599365658700002, 0.13599365658700002, 0.13599365658700002, 0.20154024037259655, 0.20154024037259655, 0.20154024037259655, 0.281901808326366, 0.281901808326366, 0.281901808326366, 0.292416731372546, 0.292416731372546, 0.292416731372546, 0.293614742175324, 0.293614742175324, 0.293614742175324, 0.20399577759668586, 0.20399577759668586, 0.20399577759668586, 0.19936064241421225, 0.19936064241421225, 0.19936064241421225, 0.22120392117735355, 0.22120392117735355, 0.22120392117735355, 0.20330017238642206, 0.20330017238642206, 0.20330017238642206, 0.19037896125160014, 0.19037896125160014, 0.19037896125160014, 0.2069963523471009, 0.2069963523471009, 0.2069963523471009, 0.2088400952495869, 0.2088400952495869, 0.2088400952495869, 0.1886904893644189, 0.1886904893644189, 0.1886904893644189, 0.2033576341792741, 0.2033576341792741, 0.2033576341792741, 0.3390971968709994, 0.3390971968709994, 0.3390971968709994, 0.3953614518219484, 0.3953614518219484, 0.3953614518219484, 0.30417065081796735, 0.30417065081796735, 0.30417065081796735, 0.19466015571668427, 0.19466015571668427, 0.19466015571668427, 0.1843802854418044, 0.1843802854418044, 0.1843802854418044, 0.24249775789581796, 0.24249775789581796, 0.24249775789581796, 0.18912521789970993, 0.18912521789970993, 0.18912521789970993, 0.18344980531936905, 0.18344980531936905, 0.18344980531936905, 0.18339191537139388, 0.18339191537139388, 0.18339191537139388, 0.08679174932959377, 0.08679174932959377, 0.08679174932959377, 0.07902304718636621, 0.07902304718636621, 0.07902304718636621, 0.08411515408141113, 0.08411515408141113, 0.08411515408141113]}, "mutation_prompt": null}
{"id": "7529d29d-1795-42fe-a7e8-1a94a8b37c07", "solution": "import numpy as np\n\nclass HybridDESA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.pop_size = 10 * dim\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.temp = 1000.0  # Initial temperature for simulated annealing\n        self.cooling_rate = 0.99  # Cooling rate for simulated annealing\n\n    def __call__(self, func):\n        np.random.seed(42)\n        population = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        num_evaluations = self.pop_size\n\n        def mutate(target_idx):\n            indices = [idx for idx in range(self.pop_size) if idx != target_idx]\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            dynamic_F = self.F * (1.0 - (num_evaluations / self.budget))  # Dynamic mutation scaling\n            mutant = population[a] + dynamic_F * (population[b] - population[c])\n            mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n            return mutant\n\n        while num_evaluations < self.budget:\n            self.CR = 0.9 - (0.6 * (num_evaluations / self.budget))  # Dynamic adjustment\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                mutant = mutate(i)\n                trial = np.where(np.random.rand(self.dim) < self.CR, mutant, population[i])\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = trial\n\n            # Simulated Annealing part\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                neighbor = population[i] + np.random.normal(0, 0.1 * self.temp / 1000, self.dim)\n                neighbor = np.clip(neighbor, self.lower_bound, self.upper_bound)\n                neighbor_fitness = func(neighbor)\n                num_evaluations += 1\n                delta_fit = neighbor_fitness - fitness[i]\n\n                if delta_fit < 0 or np.random.rand() < np.exp(-delta_fit / self.temp):\n                    population[i] = neighbor\n                    fitness[i] = neighbor_fitness\n                    if neighbor_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = neighbor\n\n            self.temp *= (self.cooling_rate - 0.01 * (num_evaluations / self.budget))  # Adjusted cooling rate\n            self.pop_size = max(4, int(10 * self.dim * (1 - num_evaluations / self.budget)))  # Adaptive population size\n\n        return best_solution", "name": "HybridDESA", "description": "Introduced a dynamic scaling factor for crossover probability adjustment to balance exploration and exploitation.", "configspace": "", "generation": 17, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "2a189148-cfe4-4ead-8a5c-8cd5936dc0ae", "metadata": {"aucs": [0.4739815114301914, 0.4739815114301914, 0.4739815114301914, 0.4580713451873616, 0.4580713451873616, 0.4580713451873616, 0.4620130756557551, 0.4620130756557551, 0.4620130756557551, 0.17038982440279704, 0.17038982440279704, 0.17038982440279704, 0.15385152873300612, 0.15385152873300612, 0.15385152873300612, 0.1663339143461786, 0.1663339143461786, 0.1663339143461786, 0.12486335942924287, 0.12486335942924287, 0.12486335942924287, 0.10813262048403394, 0.10813262048403394, 0.10813262048403394, 0.11881270881742478, 0.11881270881742478, 0.11881270881742478, 0.10824281801852631, 0.10824281801852631, 0.10824281801852631, 0.11322547617394407, 0.11322547617394407, 0.11322547617394407, 0.10405913499750241, 0.10405913499750241, 0.10405913499750241, 0.853278449181514, 0.853278449181514, 0.853278449181514, 0.8473735328002856, 0.8473735328002856, 0.8473735328002856, 0.8800149853290414, 0.8800149853290414, 0.8800149853290414, 0.24133741314659451, 0.24133741314659451, 0.24133741314659451, 0.24718444240218929, 0.24718444240218929, 0.24718444240218929, 0.23341780277017476, 0.23341780277017476, 0.23341780277017476, 0.3439750718484226, 0.3439750718484226, 0.3439750718484226, 0.4103809775169721, 0.4103809775169721, 0.4103809775169721, 0.4633511200904816, 0.4633511200904816, 0.4633511200904816, 0.12831696777215296, 0.12831696777215296, 0.12831696777215296, 0.12159210028599376, 0.12159210028599376, 0.12159210028599376, 0.13397392204348713, 0.13397392204348713, 0.13397392204348713, 0.10253543569337886, 0.10253543569337886, 0.10253543569337886, 0.12764177756725947, 0.12764177756725947, 0.12764177756725947, 0.12644172802085796, 0.12644172802085796, 0.12644172802085796, 0.04558130172842012, 0.04558130172842012, 0.04558130172842012, 0.04473178048447968, 0.04473178048447968, 0.04473178048447968, 0.032070071593435334, 0.032070071593435334, 0.032070071593435334, 0.10103826365976853, 0.10103826365976853, 0.10103826365976853, 0.12276910875703129, 0.12276910875703129, 0.12276910875703129, 0.13936216289098324, 0.13936216289098324, 0.13936216289098324, 0.031449435153703464, 0.031449435153703464, 0.031449435153703464, 0.039555157704201926, 0.039555157704201926, 0.039555157704201926, 0.038945578913286094, 0.038945578913286094, 0.038945578913286094, 0.06955693202966884, 0.06955693202966884, 0.06955693202966884, 0.09934859166641341, 0.09934859166641341, 0.09934859166641341, 0.0828010724014897, 0.0828010724014897, 0.0828010724014897, 0.3964037824809562, 0.3964037824809562, 0.3964037824809562, 0.4281453999699457, 0.4281453999699457, 0.4281453999699457, 0.3827708880194084, 0.3827708880194084, 0.3827708880194084, 0.1047301359746381, 0.1047301359746381, 0.1047301359746381, 0.10075756308933237, 0.10075756308933237, 0.10075756308933237, 0.10143109282174423, 0.10143109282174423, 0.10143109282174423, 0.2002442689435744, 0.2002442689435744, 0.2002442689435744, 0.13599365658700002, 0.13599365658700002, 0.13599365658700002, 0.20154024037259655, 0.20154024037259655, 0.20154024037259655, 0.281901808326366, 0.281901808326366, 0.281901808326366, 0.292416731372546, 0.292416731372546, 0.292416731372546, 0.293614742175324, 0.293614742175324, 0.293614742175324, 0.20399577759668586, 0.20399577759668586, 0.20399577759668586, 0.19936064241421225, 0.19936064241421225, 0.19936064241421225, 0.22120392117735355, 0.22120392117735355, 0.22120392117735355, 0.20330017238642206, 0.20330017238642206, 0.20330017238642206, 0.19037896125160014, 0.19037896125160014, 0.19037896125160014, 0.2069963523471009, 0.2069963523471009, 0.2069963523471009, 0.2088400952495869, 0.2088400952495869, 0.2088400952495869, 0.1886904893644189, 0.1886904893644189, 0.1886904893644189, 0.2033576341792741, 0.2033576341792741, 0.2033576341792741, 0.3390971968709994, 0.3390971968709994, 0.3390971968709994, 0.3953614518219484, 0.3953614518219484, 0.3953614518219484, 0.30417065081796735, 0.30417065081796735, 0.30417065081796735, 0.19466015571668427, 0.19466015571668427, 0.19466015571668427, 0.1843802854418044, 0.1843802854418044, 0.1843802854418044, 0.24249775789581796, 0.24249775789581796, 0.24249775789581796, 0.18912521789970993, 0.18912521789970993, 0.18912521789970993, 0.18344980531936905, 0.18344980531936905, 0.18344980531936905, 0.18339191537139388, 0.18339191537139388, 0.18339191537139388, 0.08679174932959377, 0.08679174932959377, 0.08679174932959377, 0.07902304718636621, 0.07902304718636621, 0.07902304718636621, 0.08411515408141113, 0.08411515408141113, 0.08411515408141113]}, "mutation_prompt": null}
{"id": "f2a938b8-96e7-4b49-bd6c-7288479c61db", "solution": "import numpy as np\n\nclass HybridDESA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.pop_size = 10 * dim\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.temp = 1000.0  # Initial temperature for simulated annealing\n        self.cooling_rate = 0.99  # Cooling rate for simulated annealing\n\n    def __call__(self, func):\n        np.random.seed(42)\n        population = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        num_evaluations = self.pop_size\n\n        def mutate(target_idx):\n            indices = [idx for idx in range(self.pop_size) if idx != target_idx]\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            improvement_factor = max(0, 1 - fitness[target_idx] / np.min(fitness))  # New line\n            dynamic_F = self.F * (1.0 - (num_evaluations / self.budget)) * (1 + 0.5 * improvement_factor)  # Modified line\n            mutant = population[a] + dynamic_F * (population[b] - population[c])\n            mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n            return mutant\n\n        while num_evaluations < self.budget:\n            self.CR = 0.9 - (0.6 * (num_evaluations / self.budget))  # Dynamic adjustment\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                mutant = mutate(i)\n                trial = np.where(np.random.rand(self.dim) < self.CR, mutant, population[i])\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = trial\n\n            # Simulated Annealing part\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                neighbor = population[i] + np.random.normal(0, 0.1 * self.temp / 1000, self.dim)\n                neighbor = np.clip(neighbor, self.lower_bound, self.upper_bound)\n                neighbor_fitness = func(neighbor)\n                num_evaluations += 1\n                delta_fit = neighbor_fitness - fitness[i]\n\n                if delta_fit < 0 or np.random.rand() < np.exp(-delta_fit / self.temp):\n                    population[i] = neighbor\n                    fitness[i] = neighbor_fitness\n                    if neighbor_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = neighbor\n\n            self.temp *= (self.cooling_rate - 0.01 * (num_evaluations / self.budget))  # Adjusted cooling rate\n            self.pop_size = max(4, int(10 * self.dim * (1 - num_evaluations / self.budget)))  # Adaptive population size\n\n        return best_solution", "name": "HybridDESA", "description": "Introduced adaptive differential weight scaling based on fitness improvement to enhance exploration and exploitation balance.", "configspace": "", "generation": 23, "fitness": 0.21374563435878025, "feedback": "The algorithm HybridDESA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.21 with standard deviation 0.18.", "error": "", "parent_id": "2a189148-cfe4-4ead-8a5c-8cd5936dc0ae", "metadata": {"aucs": [0.4739815114301914, 0.4739815114301914, 0.4739815114301914, 0.4580713451873616, 0.4580713451873616, 0.4580713451873616, 0.4586780924399477, 0.4586780924399477, 0.4586780924399477, 0.10936289517381248, 0.10936289517381248, 0.10936289517381248, 0.12916983833927442, 0.12916983833927442, 0.12916983833927442, 0.09903566520349982, 0.09903566520349982, 0.09903566520349982, 0.11512356144368463, 0.11512356144368463, 0.11512356144368463, 0.10813262048403394, 0.10813262048403394, 0.10813262048403394, 0.11881270881742478, 0.11881270881742478, 0.11881270881742478, 0.10669300182329533, 0.10669300182329533, 0.10669300182329533, 0.11322547617394407, 0.11322547617394407, 0.11322547617394407, 0.10405913499750241, 0.10405913499750241, 0.10405913499750241, 0.8843477263977283, 0.8843477263977283, 0.8843477263977283, 0.8473735328002856, 0.8473735328002856, 0.8473735328002856, 0.8800149853290414, 0.8800149853290414, 0.8800149853290414, 0.24133741314659451, 0.24133741314659451, 0.24133741314659451, 0.24718444240218929, 0.24718444240218929, 0.24718444240218929, 0.21001891609186318, 0.21001891609186318, 0.21001891609186318, 0.3439750718484226, 0.3439750718484226, 0.3439750718484226, 0.4103809775169721, 0.4103809775169721, 0.4103809775169721, 0.4633511200904816, 0.4633511200904816, 0.4633511200904816, 0.12831696777215296, 0.12831696777215296, 0.12831696777215296, 0.05838534032931819, 0.05838534032931819, 0.05838534032931819, 0.13397392204348713, 0.13397392204348713, 0.13397392204348713, 0.10253543569337886, 0.10253543569337886, 0.10253543569337886, 0.12764177756725947, 0.12764177756725947, 0.12764177756725947, 0.12644172802085796, 0.12644172802085796, 0.12644172802085796, 0.0380269954852549, 0.0380269954852549, 0.0380269954852549, 0.04473178048447968, 0.04473178048447968, 0.04473178048447968, 0.023397329787454346, 0.023397329787454346, 0.023397329787454346, 0.10103826365976853, 0.10103826365976853, 0.10103826365976853, 0.10915066958299324, 0.10915066958299324, 0.10915066958299324, 0.13936216289098324, 0.13936216289098324, 0.13936216289098324, 0.037116663827740926, 0.037116663827740926, 0.037116663827740926, 0.027781482398396062, 0.027781482398396062, 0.027781482398396062, 0.038945578913286094, 0.038945578913286094, 0.038945578913286094, 0.06955693202966884, 0.06955693202966884, 0.06955693202966884, 0.09212254652480156, 0.09212254652480156, 0.09212254652480156, 0.08698001270810851, 0.08698001270810851, 0.08698001270810851, 0.3729766133547847, 0.3729766133547847, 0.3729766133547847, 0.39509522642799844, 0.39509522642799844, 0.39509522642799844, 0.3827708880194084, 0.3827708880194084, 0.3827708880194084, 0.1047301359746381, 0.1047301359746381, 0.1047301359746381, 0.10075756308933237, 0.10075756308933237, 0.10075756308933237, 0.07463151579540028, 0.07463151579540028, 0.07463151579540028, 0.2002442689435744, 0.2002442689435744, 0.2002442689435744, 0.15216706839906669, 0.15216706839906669, 0.15216706839906669, 0.20154024037259655, 0.20154024037259655, 0.20154024037259655, 0.2774942268856445, 0.2774942268856445, 0.2774942268856445, 0.292416731372546, 0.292416731372546, 0.292416731372546, 0.293614742175324, 0.293614742175324, 0.293614742175324, 0.1403478792675119, 0.1403478792675119, 0.1403478792675119, 0.19936064241421225, 0.19936064241421225, 0.19936064241421225, 0.22120392117735355, 0.22120392117735355, 0.22120392117735355, 0.19022376960171183, 0.19022376960171183, 0.19022376960171183, 0.19037896125160014, 0.19037896125160014, 0.19037896125160014, 0.2069963523471009, 0.2069963523471009, 0.2069963523471009, 0.12462459146063287, 0.12462459146063287, 0.12462459146063287, 0.1886904893644189, 0.1886904893644189, 0.1886904893644189, 0.2033576341792741, 0.2033576341792741, 0.2033576341792741, 0.3390971968709994, 0.3390971968709994, 0.3390971968709994, 0.271980493733063, 0.271980493733063, 0.271980493733063, 0.24531044837657345, 0.24531044837657345, 0.24531044837657345, 0.1634196662404378, 0.1634196662404378, 0.1634196662404378, 0.1843802854418044, 0.1843802854418044, 0.1843802854418044, 0.4586713154896074, 0.4586713154896074, 0.4586713154896074, 0.18912521789970993, 0.18912521789970993, 0.18912521789970993, 0.18344980531936905, 0.18344980531936905, 0.18344980531936905, 0.18286220513217022, 0.18286220513217022, 0.18286220513217022, 0.08679174932959377, 0.08679174932959377, 0.08679174932959377, 0.07902304718636621, 0.07902304718636621, 0.07902304718636621, 0.08411515408141113, 0.08411515408141113, 0.08411515408141113]}, "mutation_prompt": null}
{"id": "2a708647-75ca-440e-9f3b-db8326d53448", "solution": "import numpy as np\n\nclass HybridDESA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.pop_size = 10 * dim\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.temp = 2000.0  # Initial temperature for simulated annealing\n        self.cooling_rate = 0.99  # Cooling rate for simulated annealing\n\n    def __call__(self, func):\n        np.random.seed(42)\n        population = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        num_evaluations = self.pop_size\n\n        def mutate(target_idx):\n            indices = [idx for idx in range(self.pop_size) if idx != target_idx]\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            dynamic_F = self.F * (1.0 - (num_evaluations / self.budget))  # Dynamic mutation scaling\n            mutant = population[a] + dynamic_F * (population[b] - population[c])\n            mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n            return mutant\n\n        while num_evaluations < self.budget:\n            self.CR = 0.9 - (0.6 * (num_evaluations / self.budget))  # Dynamic adjustment\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                mutant = mutate(i)\n                trial = np.where(np.random.rand(self.dim) < self.CR, mutant, population[i])\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = trial\n\n            # Simulated Annealing part\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                neighbor = population[i] + np.random.normal(0, 0.1 * self.temp / 1000, self.dim)\n                neighbor = np.clip(neighbor, self.lower_bound, self.upper_bound)\n                neighbor_fitness = func(neighbor)\n                num_evaluations += 1\n                delta_fit = neighbor_fitness - fitness[i]\n\n                if delta_fit < 0 or np.random.rand() < np.exp(-delta_fit / self.temp):\n                    population[i] = neighbor\n                    fitness[i] = neighbor_fitness\n                    if neighbor_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = neighbor\n\n            self.temp *= (self.cooling_rate - 0.01 * (num_evaluations / self.budget))  # Adjusted cooling rate\n            self.pop_size = max(4, int(10 * self.dim * (1 - num_evaluations / self.budget)))  # Adaptive population size\n\n        return best_solution", "name": "HybridDESA", "description": "Adjusted the initial temperature in simulated annealing to enhance early-stage exploration.", "configspace": "", "generation": 24, "fitness": 0.1963416338679408, "feedback": "The algorithm HybridDESA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.20 with standard deviation 0.16.", "error": "", "parent_id": "2a189148-cfe4-4ead-8a5c-8cd5936dc0ae", "metadata": {"aucs": [0.39961594555180213, 0.39961594555180213, 0.39961594555180213, 0.4132244300662644, 0.4132244300662644, 0.4132244300662644, 0.42831787929652365, 0.42831787929652365, 0.42831787929652365, 0.14554892199403113, 0.14554892199403113, 0.14554892199403113, 0.1504817775903493, 0.1504817775903493, 0.1504817775903493, 0.156257505148657, 0.156257505148657, 0.156257505148657, 0.10749537402401999, 0.10749537402401999, 0.10749537402401999, 0.11192065997519329, 0.11192065997519329, 0.11192065997519329, 0.1446683072905085, 0.1446683072905085, 0.1446683072905085, 0.08663590173770375, 0.08663590173770375, 0.08663590173770375, 0.1042587344821444, 0.1042587344821444, 0.1042587344821444, 0.08670259015829174, 0.08670259015829174, 0.08670259015829174, 0.8609005145136067, 0.8609005145136067, 0.8609005145136067, 0.8656969670575728, 0.8656969670575728, 0.8656969670575728, 0.840964089180782, 0.840964089180782, 0.840964089180782, 0.2365990293289576, 0.2365990293289576, 0.2365990293289576, 0.2233022429737953, 0.2233022429737953, 0.2233022429737953, 0.2250922663898871, 0.2250922663898871, 0.2250922663898871, 0.2374255091759896, 0.2374255091759896, 0.2374255091759896, 0.275966855469159, 0.275966855469159, 0.275966855469159, 0.2508344025185244, 0.2508344025185244, 0.2508344025185244, 0.12287711755829189, 0.12287711755829189, 0.12287711755829189, 0.1189426961539477, 0.1189426961539477, 0.1189426961539477, 0.12118730521370014, 0.12118730521370014, 0.12118730521370014, 0.10975113187719798, 0.10975113187719798, 0.10975113187719798, 0.11488376983457327, 0.11488376983457327, 0.11488376983457327, 0.09775181082868312, 0.09775181082868312, 0.09775181082868312, 0.042424051037478816, 0.042424051037478816, 0.042424051037478816, 0.05215270040701048, 0.05215270040701048, 0.05215270040701048, 0.025281438230191244, 0.025281438230191244, 0.025281438230191244, 0.11329739334402533, 0.11329739334402533, 0.11329739334402533, 0.09690749099579954, 0.09690749099579954, 0.09690749099579954, 0.11449671811650775, 0.11449671811650775, 0.11449671811650775, 0.02560896902089993, 0.02560896902089993, 0.02560896902089993, 0.02443768551117842, 0.02443768551117842, 0.02443768551117842, 0.03140526232931207, 0.03140526232931207, 0.03140526232931207, 0.07989489068842992, 0.07989489068842992, 0.07989489068842992, 0.08327905285434467, 0.08327905285434467, 0.08327905285434467, 0.09204422418712743, 0.09204422418712743, 0.09204422418712743, 0.3640138103242324, 0.3640138103242324, 0.3640138103242324, 0.35438202818567366, 0.35438202818567366, 0.35438202818567366, 0.35257336252244553, 0.35257336252244553, 0.35257336252244553, 0.11373246071108856, 0.11373246071108856, 0.11373246071108856, 0.09648000280778724, 0.09648000280778724, 0.09648000280778724, 0.10219761432214569, 0.10219761432214569, 0.10219761432214569, 0.21865543699270995, 0.21865543699270995, 0.21865543699270995, 0.1674897756597633, 0.1674897756597633, 0.1674897756597633, 0.1515781139512148, 0.1515781139512148, 0.1515781139512148, 0.2625781529408815, 0.2625781529408815, 0.2625781529408815, 0.27869273920744797, 0.27869273920744797, 0.27869273920744797, 0.2623130453036381, 0.2623130453036381, 0.2623130453036381, 0.2023286692940235, 0.2023286692940235, 0.2023286692940235, 0.19946809461219983, 0.19946809461219983, 0.19946809461219983, 0.21576011318046384, 0.21576011318046384, 0.21576011318046384, 0.1949471730007124, 0.1949471730007124, 0.1949471730007124, 0.20519231096057222, 0.20519231096057222, 0.20519231096057222, 0.18742459414599466, 0.18742459414599466, 0.18742459414599466, 0.17170419069689125, 0.17170419069689125, 0.17170419069689125, 0.17768806757189182, 0.17768806757189182, 0.17768806757189182, 0.18741285324393553, 0.18741285324393553, 0.18741285324393553, 0.15243133778398565, 0.15243133778398565, 0.15243133778398565, 0.21124557263526444, 0.21124557263526444, 0.21124557263526444, 0.18657971057784284, 0.18657971057784284, 0.18657971057784284, 0.15922213103422345, 0.15922213103422345, 0.15922213103422345, 0.14389583325889965, 0.14389583325889965, 0.14389583325889965, 0.19905929298065583, 0.19905929298065583, 0.19905929298065583, 0.19203943980683347, 0.19203943980683347, 0.19203943980683347, 0.1790979877681408, 0.1790979877681408, 0.1790979877681408, 0.18628575500526556, 0.18628575500526556, 0.18628575500526556, 0.08384669350077634, 0.08384669350077634, 0.08384669350077634, 0.07928519898221498, 0.07928519898221498, 0.07928519898221498, 0.0784624634094565, 0.0784624634094565, 0.0784624634094565]}, "mutation_prompt": null}
{"id": "b97244d4-e694-4769-8095-556c3fd8ab7d", "solution": "import numpy as np\n\nclass HybridDESA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.pop_size = 10 * dim\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.temp = 1000.0  # Initial temperature for simulated annealing\n        self.cooling_rate = 0.99  # Cooling rate for simulated annealing\n\n    def __call__(self, func):\n        np.random.seed(42)\n        population = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        num_evaluations = self.pop_size\n\n        def mutate(target_idx):\n            indices = [idx for idx in range(self.pop_size) if idx != target_idx]\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            dynamic_F = self.F * (1.0 - (num_evaluations / self.budget))  # Dynamic mutation scaling\n            mutant = population[a] + dynamic_F * (population[b] - population[c])\n            mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n            return mutant\n\n        while num_evaluations < self.budget:\n            self.CR = 0.9 - (0.6 * (num_evaluations / self.budget))  # Dynamic adjustment\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                mutant = mutate(i)\n                trial = np.where(np.random.rand(self.dim) < self.CR, mutant, population[i])\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = trial\n\n            # Simulated Annealing part\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                neighbor = population[i] + np.random.normal(0, 0.1 * self.temp / 1000, self.dim)\n                neighbor = np.clip(neighbor, self.lower_bound, self.upper_bound)\n                neighbor_fitness = func(neighbor)\n                num_evaluations += 1\n                delta_fit = neighbor_fitness - fitness[i]\n\n                if delta_fit < 0 or np.random.rand() < np.exp(-delta_fit / self.temp):\n                    population[i] = neighbor\n                    fitness[i] = neighbor_fitness\n                    if neighbor_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = neighbor\n\n            self.temp *= (self.cooling_rate - 0.01 * (num_evaluations / self.budget))  # Adjusted cooling rate\n            self.pop_size = max(4, int(10 * self.dim * (1 - num_evaluations / self.budget)))  # Adaptive population size\n\n        return best_solution", "name": "HybridDESA", "description": "Introduced a dynamic scaling factor for crossover probability adjustment to balance exploration and exploitation.", "configspace": "", "generation": 17, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "2a189148-cfe4-4ead-8a5c-8cd5936dc0ae", "metadata": {"aucs": [0.4739815114301914, 0.4739815114301914, 0.4739815114301914, 0.4580713451873616, 0.4580713451873616, 0.4580713451873616, 0.4620130756557551, 0.4620130756557551, 0.4620130756557551, 0.17038982440279704, 0.17038982440279704, 0.17038982440279704, 0.15385152873300612, 0.15385152873300612, 0.15385152873300612, 0.1663339143461786, 0.1663339143461786, 0.1663339143461786, 0.12486335942924287, 0.12486335942924287, 0.12486335942924287, 0.10813262048403394, 0.10813262048403394, 0.10813262048403394, 0.11881270881742478, 0.11881270881742478, 0.11881270881742478, 0.10824281801852631, 0.10824281801852631, 0.10824281801852631, 0.11322547617394407, 0.11322547617394407, 0.11322547617394407, 0.10405913499750241, 0.10405913499750241, 0.10405913499750241, 0.853278449181514, 0.853278449181514, 0.853278449181514, 0.8473735328002856, 0.8473735328002856, 0.8473735328002856, 0.8800149853290414, 0.8800149853290414, 0.8800149853290414, 0.24133741314659451, 0.24133741314659451, 0.24133741314659451, 0.24718444240218929, 0.24718444240218929, 0.24718444240218929, 0.23341780277017476, 0.23341780277017476, 0.23341780277017476, 0.3439750718484226, 0.3439750718484226, 0.3439750718484226, 0.4103809775169721, 0.4103809775169721, 0.4103809775169721, 0.4633511200904816, 0.4633511200904816, 0.4633511200904816, 0.12831696777215296, 0.12831696777215296, 0.12831696777215296, 0.12159210028599376, 0.12159210028599376, 0.12159210028599376, 0.13397392204348713, 0.13397392204348713, 0.13397392204348713, 0.10253543569337886, 0.10253543569337886, 0.10253543569337886, 0.12764177756725947, 0.12764177756725947, 0.12764177756725947, 0.12644172802085796, 0.12644172802085796, 0.12644172802085796, 0.04558130172842012, 0.04558130172842012, 0.04558130172842012, 0.04473178048447968, 0.04473178048447968, 0.04473178048447968, 0.032070071593435334, 0.032070071593435334, 0.032070071593435334, 0.10103826365976853, 0.10103826365976853, 0.10103826365976853, 0.12276910875703129, 0.12276910875703129, 0.12276910875703129, 0.13936216289098324, 0.13936216289098324, 0.13936216289098324, 0.031449435153703464, 0.031449435153703464, 0.031449435153703464, 0.039555157704201926, 0.039555157704201926, 0.039555157704201926, 0.038945578913286094, 0.038945578913286094, 0.038945578913286094, 0.06955693202966884, 0.06955693202966884, 0.06955693202966884, 0.09934859166641341, 0.09934859166641341, 0.09934859166641341, 0.0828010724014897, 0.0828010724014897, 0.0828010724014897, 0.3964037824809562, 0.3964037824809562, 0.3964037824809562, 0.4281453999699457, 0.4281453999699457, 0.4281453999699457, 0.3827708880194084, 0.3827708880194084, 0.3827708880194084, 0.1047301359746381, 0.1047301359746381, 0.1047301359746381, 0.10075756308933237, 0.10075756308933237, 0.10075756308933237, 0.10143109282174423, 0.10143109282174423, 0.10143109282174423, 0.2002442689435744, 0.2002442689435744, 0.2002442689435744, 0.13599365658700002, 0.13599365658700002, 0.13599365658700002, 0.20154024037259655, 0.20154024037259655, 0.20154024037259655, 0.281901808326366, 0.281901808326366, 0.281901808326366, 0.292416731372546, 0.292416731372546, 0.292416731372546, 0.293614742175324, 0.293614742175324, 0.293614742175324, 0.20399577759668586, 0.20399577759668586, 0.20399577759668586, 0.19936064241421225, 0.19936064241421225, 0.19936064241421225, 0.22120392117735355, 0.22120392117735355, 0.22120392117735355, 0.20330017238642206, 0.20330017238642206, 0.20330017238642206, 0.19037896125160014, 0.19037896125160014, 0.19037896125160014, 0.2069963523471009, 0.2069963523471009, 0.2069963523471009, 0.2088400952495869, 0.2088400952495869, 0.2088400952495869, 0.1886904893644189, 0.1886904893644189, 0.1886904893644189, 0.2033576341792741, 0.2033576341792741, 0.2033576341792741, 0.3390971968709994, 0.3390971968709994, 0.3390971968709994, 0.3953614518219484, 0.3953614518219484, 0.3953614518219484, 0.30417065081796735, 0.30417065081796735, 0.30417065081796735, 0.19466015571668427, 0.19466015571668427, 0.19466015571668427, 0.1843802854418044, 0.1843802854418044, 0.1843802854418044, 0.24249775789581796, 0.24249775789581796, 0.24249775789581796, 0.18912521789970993, 0.18912521789970993, 0.18912521789970993, 0.18344980531936905, 0.18344980531936905, 0.18344980531936905, 0.18339191537139388, 0.18339191537139388, 0.18339191537139388, 0.08679174932959377, 0.08679174932959377, 0.08679174932959377, 0.07902304718636621, 0.07902304718636621, 0.07902304718636621, 0.08411515408141113, 0.08411515408141113, 0.08411515408141113]}, "mutation_prompt": null}
{"id": "5ae2dbdc-b599-40bc-8c7a-7ca0f6473f3a", "solution": "import numpy as np\n\nclass HybridDESA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.pop_size = 10 * dim\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.temp = 1000.0  # Initial temperature for simulated annealing\n        self.cooling_rate = 0.99  # Cooling rate for simulated annealing\n\n    def __call__(self, func):\n        np.random.seed(42)\n        population = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        num_evaluations = self.pop_size\n\n        def mutate(target_idx):\n            indices = [idx for idx in range(self.pop_size) if idx != target_idx]\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            dynamic_F = self.F * (1.0 - (num_evaluations / self.budget))  # Dynamic mutation scaling\n            mutant = population[a] + dynamic_F * (population[b] - population[c])\n            mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n            return mutant\n\n        while num_evaluations < self.budget:\n            self.CR = 0.9 * (self.temp / 1000)  # Adjusted crossover rate\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                mutant = mutate(i)\n                trial = np.where(np.random.rand(self.dim) < self.CR, mutant, population[i])\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = trial\n\n            # Simulated Annealing part\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                neighbor = population[i] + np.random.normal(0, 0.1 * self.temp / 1000, self.dim)\n                neighbor = np.clip(neighbor, self.lower_bound, self.upper_bound)\n                neighbor_fitness = func(neighbor)\n                num_evaluations += 1\n                delta_fit = neighbor_fitness - fitness[i]\n\n                if delta_fit < 0 or np.random.rand() < np.exp(-delta_fit / self.temp):\n                    population[i] = neighbor\n                    fitness[i] = neighbor_fitness\n                    if neighbor_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = neighbor\n\n            self.temp *= (self.cooling_rate - 0.01 * (num_evaluations / self.budget))  # Adjusted cooling rate\n            self.pop_size = max(4, int(10 * self.dim * (1 - num_evaluations / self.budget)))  # Adaptive population size\n\n        return best_solution", "name": "HybridDESA", "description": "Introduced adaptive crossover rate based on temperature to enhance convergence speed.", "configspace": "", "generation": 26, "fitness": 0.19376462412805287, "feedback": "The algorithm HybridDESA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.19 with standard deviation 0.16.", "error": "", "parent_id": "2a189148-cfe4-4ead-8a5c-8cd5936dc0ae", "metadata": {"aucs": [0.42122673242450115, 0.42122673242450115, 0.42122673242450115, 0.4220322371546066, 0.4220322371546066, 0.4220322371546066, 0.4037098525204942, 0.4037098525204942, 0.4037098525204942, 0.1686792927390679, 0.1686792927390679, 0.1686792927390679, 0.12633144469044477, 0.12633144469044477, 0.12633144469044477, 0.15424749825871809, 0.15424749825871809, 0.15424749825871809, 0.15050586260193688, 0.15050586260193688, 0.15050586260193688, 0.11916688542773013, 0.11916688542773013, 0.11916688542773013, 0.11228117705445839, 0.11228117705445839, 0.11228117705445839, 0.09951847356402266, 0.09951847356402266, 0.09951847356402266, 0.1073203570152782, 0.1073203570152782, 0.1073203570152782, 0.09752986704473043, 0.09752986704473043, 0.09752986704473043, 0.6441296958992497, 0.6441296958992497, 0.6441296958992497, 0.8767384509655326, 0.8767384509655326, 0.8767384509655326, 0.8348394043231654, 0.8348394043231654, 0.8348394043231654, 0.195386784914242, 0.195386784914242, 0.195386784914242, 0.194602217142548, 0.194602217142548, 0.194602217142548, 0.19038612055583948, 0.19038612055583948, 0.19038612055583948, 0.23594764435009685, 0.23594764435009685, 0.23594764435009685, 0.2623793177511192, 0.2623793177511192, 0.2623793177511192, 0.24718005365328266, 0.24718005365328266, 0.24718005365328266, 0.12044819695448772, 0.12044819695448772, 0.12044819695448772, 0.12671433044501224, 0.12671433044501224, 0.12671433044501224, 0.11465395024451686, 0.11465395024451686, 0.11465395024451686, 0.0907293322854622, 0.0907293322854622, 0.0907293322854622, 0.11487680283838309, 0.11487680283838309, 0.11487680283838309, 0.10344555123026056, 0.10344555123026056, 0.10344555123026056, 0.008187273130135897, 0.008187273130135897, 0.008187273130135897, 0.001863990415289285, 0.001863990415289285, 0.001863990415289285, 0.02842384540192655, 0.02842384540192655, 0.02842384540192655, 0.10292383662227689, 0.10292383662227689, 0.10292383662227689, 0.12044753882944892, 0.12044753882944892, 0.12044753882944892, 0.1163417478326837, 0.1163417478326837, 0.1163417478326837, 0.027201311271419004, 0.027201311271419004, 0.027201311271419004, 0.03690032082043504, 0.03690032082043504, 0.03690032082043504, 0.030670106153086962, 0.030670106153086962, 0.030670106153086962, 0.08531653642055248, 0.08531653642055248, 0.08531653642055248, 0.06327876880244165, 0.06327876880244165, 0.06327876880244165, 0.06746059625541423, 0.06746059625541423, 0.06746059625541423, 0.3542350914519299, 0.3542350914519299, 0.3542350914519299, 0.3728076298417836, 0.3728076298417836, 0.3728076298417836, 0.3615830136853354, 0.3615830136853354, 0.3615830136853354, 0.12134689887556305, 0.12134689887556305, 0.12134689887556305, 0.10378727103072383, 0.10378727103072383, 0.10378727103072383, 0.09334941535445507, 0.09334941535445507, 0.09334941535445507, 0.16900280403015588, 0.16900280403015588, 0.16900280403015588, 0.16236500517535113, 0.16236500517535113, 0.16236500517535113, 0.1482182704814755, 0.1482182704814755, 0.1482182704814755, 0.26745654712554734, 0.26745654712554734, 0.26745654712554734, 0.24728277070574178, 0.24728277070574178, 0.24728277070574178, 0.265930583238171, 0.265930583238171, 0.265930583238171, 0.18973595093121398, 0.18973595093121398, 0.18973595093121398, 0.17944980754546846, 0.17944980754546846, 0.17944980754546846, 0.1912937637867631, 0.1912937637867631, 0.1912937637867631, 0.20662516783030282, 0.20662516783030282, 0.20662516783030282, 0.17886813035409577, 0.17886813035409577, 0.17886813035409577, 0.20236858712572314, 0.20236858712572314, 0.20236858712572314, 0.19736968077932404, 0.19736968077932404, 0.19736968077932404, 0.1886668828316942, 0.1886668828316942, 0.1886668828316942, 0.19945346425298216, 0.19945346425298216, 0.19945346425298216, 0.16860065785267542, 0.16860065785267542, 0.16860065785267542, 0.27685352072435476, 0.27685352072435476, 0.27685352072435476, 0.19181651147595447, 0.19181651147595447, 0.19181651147595447, 0.33002223481896764, 0.33002223481896764, 0.33002223481896764, 0.17154187227586248, 0.17154187227586248, 0.17154187227586248, 0.20091195705014797, 0.20091195705014797, 0.20091195705014797, 0.17677693892293134, 0.17677693892293134, 0.17677693892293134, 0.19836538886563482, 0.19836538886563482, 0.19836538886563482, 0.1932025609747443, 0.1932025609747443, 0.1932025609747443, 0.06661230577210031, 0.06661230577210031, 0.06661230577210031, 0.07704818521770407, 0.07704818521770407, 0.07704818521770407, 0.07408066083062781, 0.07408066083062781, 0.07408066083062781]}, "mutation_prompt": null}
{"id": "054f266f-6b85-4e8c-9943-26407740e78d", "solution": "import numpy as np\n\nclass HybridDESA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.pop_size = 10 * dim\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.temp = 1000.0  # Initial temperature for simulated annealing\n        self.cooling_rate = 0.99  # Cooling rate for simulated annealing\n\n    def __call__(self, func):\n        np.random.seed(42)\n        population = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        num_evaluations = self.pop_size\n\n        def mutate(target_idx):\n            indices = [idx for idx in range(self.pop_size) if idx != target_idx]\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            dynamic_F = self.F * (1.0 - (num_evaluations / self.budget))  # Dynamic mutation scaling\n            mutant = population[a] + dynamic_F * (population[b] - population[c])\n            mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n            return mutant\n\n        while num_evaluations < self.budget:\n            self.CR = 0.9 - (0.6 * (num_evaluations / self.budget))  # Dynamic adjustment\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                mutant = mutate(i)\n                trial = np.where(np.random.rand(self.dim) < self.CR, mutant, population[i])\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = trial\n\n            # Simulated Annealing part\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                neighbor = population[i] + np.random.normal(0, 0.1 * self.temp / 1000, self.dim)\n                neighbor = np.clip(neighbor, self.lower_bound, self.upper_bound)\n                neighbor_fitness = func(neighbor)\n                num_evaluations += 1\n                delta_fit = neighbor_fitness - fitness[i]\n\n                if delta_fit < 0 or np.random.rand() < np.exp(-delta_fit / self.temp):\n                    population[i] = neighbor\n                    fitness[i] = neighbor_fitness\n                    if neighbor_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = neighbor\n\n            self.temp *= (self.cooling_rate - 0.01 * (num_evaluations / self.budget))  # Adjusted cooling rate\n            self.pop_size = max(4, int(10 * self.dim * (1 - num_evaluations / self.budget)))  # Adaptive population size\n\n        return best_solution", "name": "HybridDESA", "description": "Introduced a dynamic scaling factor for crossover probability adjustment to balance exploration and exploitation.", "configspace": "", "generation": 17, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "2a189148-cfe4-4ead-8a5c-8cd5936dc0ae", "metadata": {"aucs": [0.4739815114301914, 0.4739815114301914, 0.4739815114301914, 0.4580713451873616, 0.4580713451873616, 0.4580713451873616, 0.4620130756557551, 0.4620130756557551, 0.4620130756557551, 0.17038982440279704, 0.17038982440279704, 0.17038982440279704, 0.15385152873300612, 0.15385152873300612, 0.15385152873300612, 0.1663339143461786, 0.1663339143461786, 0.1663339143461786, 0.12486335942924287, 0.12486335942924287, 0.12486335942924287, 0.10813262048403394, 0.10813262048403394, 0.10813262048403394, 0.11881270881742478, 0.11881270881742478, 0.11881270881742478, 0.10824281801852631, 0.10824281801852631, 0.10824281801852631, 0.11322547617394407, 0.11322547617394407, 0.11322547617394407, 0.10405913499750241, 0.10405913499750241, 0.10405913499750241, 0.853278449181514, 0.853278449181514, 0.853278449181514, 0.8473735328002856, 0.8473735328002856, 0.8473735328002856, 0.8800149853290414, 0.8800149853290414, 0.8800149853290414, 0.24133741314659451, 0.24133741314659451, 0.24133741314659451, 0.24718444240218929, 0.24718444240218929, 0.24718444240218929, 0.23341780277017476, 0.23341780277017476, 0.23341780277017476, 0.3439750718484226, 0.3439750718484226, 0.3439750718484226, 0.4103809775169721, 0.4103809775169721, 0.4103809775169721, 0.4633511200904816, 0.4633511200904816, 0.4633511200904816, 0.12831696777215296, 0.12831696777215296, 0.12831696777215296, 0.12159210028599376, 0.12159210028599376, 0.12159210028599376, 0.13397392204348713, 0.13397392204348713, 0.13397392204348713, 0.10253543569337886, 0.10253543569337886, 0.10253543569337886, 0.12764177756725947, 0.12764177756725947, 0.12764177756725947, 0.12644172802085796, 0.12644172802085796, 0.12644172802085796, 0.04558130172842012, 0.04558130172842012, 0.04558130172842012, 0.04473178048447968, 0.04473178048447968, 0.04473178048447968, 0.032070071593435334, 0.032070071593435334, 0.032070071593435334, 0.10103826365976853, 0.10103826365976853, 0.10103826365976853, 0.12276910875703129, 0.12276910875703129, 0.12276910875703129, 0.13936216289098324, 0.13936216289098324, 0.13936216289098324, 0.031449435153703464, 0.031449435153703464, 0.031449435153703464, 0.039555157704201926, 0.039555157704201926, 0.039555157704201926, 0.038945578913286094, 0.038945578913286094, 0.038945578913286094, 0.06955693202966884, 0.06955693202966884, 0.06955693202966884, 0.09934859166641341, 0.09934859166641341, 0.09934859166641341, 0.0828010724014897, 0.0828010724014897, 0.0828010724014897, 0.3964037824809562, 0.3964037824809562, 0.3964037824809562, 0.4281453999699457, 0.4281453999699457, 0.4281453999699457, 0.3827708880194084, 0.3827708880194084, 0.3827708880194084, 0.1047301359746381, 0.1047301359746381, 0.1047301359746381, 0.10075756308933237, 0.10075756308933237, 0.10075756308933237, 0.10143109282174423, 0.10143109282174423, 0.10143109282174423, 0.2002442689435744, 0.2002442689435744, 0.2002442689435744, 0.13599365658700002, 0.13599365658700002, 0.13599365658700002, 0.20154024037259655, 0.20154024037259655, 0.20154024037259655, 0.281901808326366, 0.281901808326366, 0.281901808326366, 0.292416731372546, 0.292416731372546, 0.292416731372546, 0.293614742175324, 0.293614742175324, 0.293614742175324, 0.20399577759668586, 0.20399577759668586, 0.20399577759668586, 0.19936064241421225, 0.19936064241421225, 0.19936064241421225, 0.22120392117735355, 0.22120392117735355, 0.22120392117735355, 0.20330017238642206, 0.20330017238642206, 0.20330017238642206, 0.19037896125160014, 0.19037896125160014, 0.19037896125160014, 0.2069963523471009, 0.2069963523471009, 0.2069963523471009, 0.2088400952495869, 0.2088400952495869, 0.2088400952495869, 0.1886904893644189, 0.1886904893644189, 0.1886904893644189, 0.2033576341792741, 0.2033576341792741, 0.2033576341792741, 0.3390971968709994, 0.3390971968709994, 0.3390971968709994, 0.3953614518219484, 0.3953614518219484, 0.3953614518219484, 0.30417065081796735, 0.30417065081796735, 0.30417065081796735, 0.19466015571668427, 0.19466015571668427, 0.19466015571668427, 0.1843802854418044, 0.1843802854418044, 0.1843802854418044, 0.24249775789581796, 0.24249775789581796, 0.24249775789581796, 0.18912521789970993, 0.18912521789970993, 0.18912521789970993, 0.18344980531936905, 0.18344980531936905, 0.18344980531936905, 0.18339191537139388, 0.18339191537139388, 0.18339191537139388, 0.08679174932959377, 0.08679174932959377, 0.08679174932959377, 0.07902304718636621, 0.07902304718636621, 0.07902304718636621, 0.08411515408141113, 0.08411515408141113, 0.08411515408141113]}, "mutation_prompt": null}
{"id": "8ff9fada-192a-401c-87a6-84d901bbd89e", "solution": "import numpy as np\n\nclass HybridDESA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.pop_size = 10 * dim\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.temp = 1000.0  # Initial temperature for simulated annealing\n        self.cooling_rate = 0.99  # Cooling rate for simulated annealing\n        self.learning_rate = 0.1  # New learning rate for adaptive mutation\n\n    def __call__(self, func):\n        np.random.seed(42)\n        population = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        num_evaluations = self.pop_size\n\n        def mutate(target_idx):\n            indices = [idx for idx in range(self.pop_size) if idx != target_idx]\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            dynamic_F = self.F * (1.0 - (num_evaluations / self.budget))  # Dynamic mutation scaling\n            mutant = population[a] + dynamic_F * (population[b] - population[c]) + \\\n                     self.learning_rate * (population[a] - best_solution)  # Adaptive mutation\n            mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n            return mutant\n\n        while num_evaluations < self.budget:\n            self.CR = 0.9 - (0.6 * (num_evaluations / self.budget))  # Dynamic adjustment\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                mutant = mutate(i)\n                trial = np.where(np.random.rand(self.dim) < self.CR, mutant, population[i])\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = trial\n\n            # Simulated Annealing part\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                neighbor = population[i] + np.random.normal(0, 0.1 * self.temp / 1000, self.dim)\n                neighbor = np.clip(neighbor, self.lower_bound, self.upper_bound)\n                neighbor_fitness = func(neighbor)\n                num_evaluations += 1\n                delta_fit = neighbor_fitness - fitness[i]\n\n                if delta_fit < 0 or np.random.rand() < np.exp(-delta_fit / self.temp):\n                    population[i] = neighbor\n                    fitness[i] = neighbor_fitness\n                    if neighbor_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = neighbor\n\n            self.temp *= (self.cooling_rate - 0.01 * (num_evaluations / self.budget))  # Adjusted cooling rate\n            self.pop_size = max(4, int(10 * self.dim * (1 - num_evaluations / self.budget)))  # Adaptive population size\n\n        return best_solution", "name": "HybridDESA", "description": "Incorporate an adaptive mutation strategy by introducing a learning rate to improve convergence.", "configspace": "", "generation": 28, "fitness": 0.19558839994494684, "feedback": "The algorithm HybridDESA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.20 with standard deviation 0.15.", "error": "", "parent_id": "2a189148-cfe4-4ead-8a5c-8cd5936dc0ae", "metadata": {"aucs": [0.44597738731032155, 0.44597738731032155, 0.44597738731032155, 0.43446311910651414, 0.43446311910651414, 0.43446311910651414, 0.4411432105300751, 0.4411432105300751, 0.4411432105300751, 0.15611589590110508, 0.15611589590110508, 0.15611589590110508, 0.15236420374745685, 0.15236420374745685, 0.15236420374745685, 0.14600882298091478, 0.14600882298091478, 0.14600882298091478, 0.1263112279400862, 0.1263112279400862, 0.1263112279400862, 0.11578274785860698, 0.11578274785860698, 0.11578274785860698, 0.12550155844816724, 0.12550155844816724, 0.12550155844816724, 0.08610109210587902, 0.08610109210587902, 0.08610109210587902, 0.08356445682041413, 0.08356445682041413, 0.08356445682041413, 0.0992263077250587, 0.0992263077250587, 0.0992263077250587, 0.7107304952228595, 0.7107304952228595, 0.7107304952228595, 0.7063708215205537, 0.7063708215205537, 0.7063708215205537, 0.7790049857752979, 0.7790049857752979, 0.7790049857752979, 0.2325779051025475, 0.2325779051025475, 0.2325779051025475, 0.209811925785643, 0.209811925785643, 0.209811925785643, 0.23880467857821686, 0.23880467857821686, 0.23880467857821686, 0.24839720225451734, 0.24839720225451734, 0.24839720225451734, 0.23770500591381216, 0.23770500591381216, 0.23770500591381216, 0.37386941346161007, 0.37386941346161007, 0.37386941346161007, 0.11445030780069787, 0.11445030780069787, 0.11445030780069787, 0.12559877217603554, 0.12559877217603554, 0.12559877217603554, 0.11584253363256614, 0.11584253363256614, 0.11584253363256614, 0.09169727906748903, 0.09169727906748903, 0.09169727906748903, 0.1267885432125493, 0.1267885432125493, 0.1267885432125493, 0.1148874103704729, 0.1148874103704729, 0.1148874103704729, 0.03802477841125251, 0.03802477841125251, 0.03802477841125251, 0.011927819696249009, 0.011927819696249009, 0.011927819696249009, 0.022759067504141584, 0.022759067504141584, 0.022759067504141584, 0.08488197308113066, 0.08488197308113066, 0.08488197308113066, 0.10722693727227073, 0.10722693727227073, 0.10722693727227073, 0.11959534968446883, 0.11959534968446883, 0.11959534968446883, 0.03314567531094781, 0.03314567531094781, 0.03314567531094781, 0.02745922653883115, 0.02745922653883115, 0.02745922653883115, 0.03709634907719883, 0.03709634907719883, 0.03709634907719883, 0.08480581488945771, 0.08480581488945771, 0.08480581488945771, 0.08949989119584167, 0.08949989119584167, 0.08949989119584167, 0.09167200500745143, 0.09167200500745143, 0.09167200500745143, 0.38230025066570217, 0.38230025066570217, 0.38230025066570217, 0.370674127674426, 0.370674127674426, 0.370674127674426, 0.3693166591425172, 0.3693166591425172, 0.3693166591425172, 0.10842660638045076, 0.10842660638045076, 0.10842660638045076, 0.10173713104407611, 0.10173713104407611, 0.10173713104407611, 0.11301310152248101, 0.11301310152248101, 0.11301310152248101, 0.1411716181121907, 0.1411716181121907, 0.1411716181121907, 0.1568087630209717, 0.1568087630209717, 0.1568087630209717, 0.1701058920421381, 0.1701058920421381, 0.1701058920421381, 0.27483770213675585, 0.27483770213675585, 0.27483770213675585, 0.24559577825836443, 0.24559577825836443, 0.24559577825836443, 0.28000720418077396, 0.28000720418077396, 0.28000720418077396, 0.2121303366871492, 0.2121303366871492, 0.2121303366871492, 0.16627232384938384, 0.16627232384938384, 0.16627232384938384, 0.18862231824304387, 0.18862231824304387, 0.18862231824304387, 0.18458416509134634, 0.18458416509134634, 0.18458416509134634, 0.18704630061913863, 0.18704630061913863, 0.18704630061913863, 0.1896340280291724, 0.1896340280291724, 0.1896340280291724, 0.194207241486554, 0.194207241486554, 0.194207241486554, 0.17816563244732841, 0.17816563244732841, 0.17816563244732841, 0.17643046600089496, 0.17643046600089496, 0.17643046600089496, 0.4139264749119852, 0.4139264749119852, 0.4139264749119852, 0.15836929398264965, 0.15836929398264965, 0.15836929398264965, 0.16420370509436577, 0.16420370509436577, 0.16420370509436577, 0.15648654983082488, 0.15648654983082488, 0.15648654983082488, 0.17222188836525187, 0.17222188836525187, 0.17222188836525187, 0.2448850301179788, 0.2448850301179788, 0.2448850301179788, 0.1882955405648733, 0.1882955405648733, 0.1882955405648733, 0.1837623207396939, 0.1837623207396939, 0.1837623207396939, 0.19108017180477255, 0.19108017180477255, 0.19108017180477255, 0.0819032861691339, 0.0819032861691339, 0.0819032861691339, 0.07415336256373883, 0.07415336256373883, 0.07415336256373883, 0.07479732723733468, 0.07479732723733468, 0.07479732723733468]}, "mutation_prompt": null}
{"id": "01b3f11a-be32-4dd4-9895-ba5b170a998c", "solution": "import numpy as np\n\nclass HybridDESA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.pop_size = 10 * dim\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.temp = 1000.0  # Initial temperature for simulated annealing\n        self.cooling_rate = 0.99  # Cooling rate for simulated annealing\n\n    def __call__(self, func):\n        np.random.seed(42)\n        population = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        num_evaluations = self.pop_size\n\n        def mutate(target_idx):\n            indices = [idx for idx in range(self.pop_size) if idx != target_idx]\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            dynamic_F = self.F * (1.0 - (num_evaluations / self.budget))  # Dynamic mutation scaling\n            mutant = population[a] + dynamic_F * (population[b] - population[c])\n            mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n            return mutant\n\n        while num_evaluations < self.budget:\n            self.CR = 0.9 - (0.6 * (num_evaluations / self.budget))  # Dynamic adjustment\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                mutant = mutate(i)\n                trial = np.where(np.random.rand(self.dim) < self.CR, mutant, population[i])\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = trial\n\n            # Simulated Annealing part\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                neighbor = population[i] + np.random.normal(0, 0.1 * self.temp / 1000, self.dim)\n                neighbor = np.clip(neighbor, self.lower_bound, self.upper_bound)\n                neighbor_fitness = func(neighbor)\n                num_evaluations += 1\n                delta_fit = neighbor_fitness - fitness[i]\n\n                if delta_fit < 0 or np.random.rand() < np.exp(-delta_fit / self.temp):\n                    population[i] = neighbor\n                    fitness[i] = neighbor_fitness\n                    if neighbor_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = neighbor\n\n            self.temp *= (self.cooling_rate - 0.01 * (num_evaluations / self.budget))  # Adjusted cooling rate\n            self.pop_size = max(4, int(10 * self.dim * (1 - num_evaluations / self.budget)))  # Adaptive population size\n\n        return best_solution", "name": "HybridDESA", "description": "Introduced a dynamic scaling factor for crossover probability adjustment to balance exploration and exploitation.", "configspace": "", "generation": 17, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "2a189148-cfe4-4ead-8a5c-8cd5936dc0ae", "metadata": {"aucs": [0.4739815114301914, 0.4739815114301914, 0.4739815114301914, 0.4580713451873616, 0.4580713451873616, 0.4580713451873616, 0.4620130756557551, 0.4620130756557551, 0.4620130756557551, 0.17038982440279704, 0.17038982440279704, 0.17038982440279704, 0.15385152873300612, 0.15385152873300612, 0.15385152873300612, 0.1663339143461786, 0.1663339143461786, 0.1663339143461786, 0.12486335942924287, 0.12486335942924287, 0.12486335942924287, 0.10813262048403394, 0.10813262048403394, 0.10813262048403394, 0.11881270881742478, 0.11881270881742478, 0.11881270881742478, 0.10824281801852631, 0.10824281801852631, 0.10824281801852631, 0.11322547617394407, 0.11322547617394407, 0.11322547617394407, 0.10405913499750241, 0.10405913499750241, 0.10405913499750241, 0.853278449181514, 0.853278449181514, 0.853278449181514, 0.8473735328002856, 0.8473735328002856, 0.8473735328002856, 0.8800149853290414, 0.8800149853290414, 0.8800149853290414, 0.24133741314659451, 0.24133741314659451, 0.24133741314659451, 0.24718444240218929, 0.24718444240218929, 0.24718444240218929, 0.23341780277017476, 0.23341780277017476, 0.23341780277017476, 0.3439750718484226, 0.3439750718484226, 0.3439750718484226, 0.4103809775169721, 0.4103809775169721, 0.4103809775169721, 0.4633511200904816, 0.4633511200904816, 0.4633511200904816, 0.12831696777215296, 0.12831696777215296, 0.12831696777215296, 0.12159210028599376, 0.12159210028599376, 0.12159210028599376, 0.13397392204348713, 0.13397392204348713, 0.13397392204348713, 0.10253543569337886, 0.10253543569337886, 0.10253543569337886, 0.12764177756725947, 0.12764177756725947, 0.12764177756725947, 0.12644172802085796, 0.12644172802085796, 0.12644172802085796, 0.04558130172842012, 0.04558130172842012, 0.04558130172842012, 0.04473178048447968, 0.04473178048447968, 0.04473178048447968, 0.032070071593435334, 0.032070071593435334, 0.032070071593435334, 0.10103826365976853, 0.10103826365976853, 0.10103826365976853, 0.12276910875703129, 0.12276910875703129, 0.12276910875703129, 0.13936216289098324, 0.13936216289098324, 0.13936216289098324, 0.031449435153703464, 0.031449435153703464, 0.031449435153703464, 0.039555157704201926, 0.039555157704201926, 0.039555157704201926, 0.038945578913286094, 0.038945578913286094, 0.038945578913286094, 0.06955693202966884, 0.06955693202966884, 0.06955693202966884, 0.09934859166641341, 0.09934859166641341, 0.09934859166641341, 0.0828010724014897, 0.0828010724014897, 0.0828010724014897, 0.3964037824809562, 0.3964037824809562, 0.3964037824809562, 0.4281453999699457, 0.4281453999699457, 0.4281453999699457, 0.3827708880194084, 0.3827708880194084, 0.3827708880194084, 0.1047301359746381, 0.1047301359746381, 0.1047301359746381, 0.10075756308933237, 0.10075756308933237, 0.10075756308933237, 0.10143109282174423, 0.10143109282174423, 0.10143109282174423, 0.2002442689435744, 0.2002442689435744, 0.2002442689435744, 0.13599365658700002, 0.13599365658700002, 0.13599365658700002, 0.20154024037259655, 0.20154024037259655, 0.20154024037259655, 0.281901808326366, 0.281901808326366, 0.281901808326366, 0.292416731372546, 0.292416731372546, 0.292416731372546, 0.293614742175324, 0.293614742175324, 0.293614742175324, 0.20399577759668586, 0.20399577759668586, 0.20399577759668586, 0.19936064241421225, 0.19936064241421225, 0.19936064241421225, 0.22120392117735355, 0.22120392117735355, 0.22120392117735355, 0.20330017238642206, 0.20330017238642206, 0.20330017238642206, 0.19037896125160014, 0.19037896125160014, 0.19037896125160014, 0.2069963523471009, 0.2069963523471009, 0.2069963523471009, 0.2088400952495869, 0.2088400952495869, 0.2088400952495869, 0.1886904893644189, 0.1886904893644189, 0.1886904893644189, 0.2033576341792741, 0.2033576341792741, 0.2033576341792741, 0.3390971968709994, 0.3390971968709994, 0.3390971968709994, 0.3953614518219484, 0.3953614518219484, 0.3953614518219484, 0.30417065081796735, 0.30417065081796735, 0.30417065081796735, 0.19466015571668427, 0.19466015571668427, 0.19466015571668427, 0.1843802854418044, 0.1843802854418044, 0.1843802854418044, 0.24249775789581796, 0.24249775789581796, 0.24249775789581796, 0.18912521789970993, 0.18912521789970993, 0.18912521789970993, 0.18344980531936905, 0.18344980531936905, 0.18344980531936905, 0.18339191537139388, 0.18339191537139388, 0.18339191537139388, 0.08679174932959377, 0.08679174932959377, 0.08679174932959377, 0.07902304718636621, 0.07902304718636621, 0.07902304718636621, 0.08411515408141113, 0.08411515408141113, 0.08411515408141113]}, "mutation_prompt": null}
{"id": "d1a32125-0637-4471-8835-0d79dd25c1d1", "solution": "import numpy as np\n\nclass HybridDESA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.pop_size = 10 * dim\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.temp = 1000.0  # Initial temperature for simulated annealing\n        self.cooling_rate = 0.99  # Cooling rate for simulated annealing\n\n    def __call__(self, func):\n        np.random.seed(42)\n        population = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        num_evaluations = self.pop_size\n\n        def mutate(target_idx):\n            indices = [idx for idx in range(self.pop_size) if idx != target_idx]\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            dynamic_F = self.F * (1.0 - (num_evaluations / self.budget))  # Dynamic mutation scaling\n            mutant = population[a] + dynamic_F * (population[b] - population[c])\n            mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n            return mutant\n\n        while num_evaluations < self.budget:\n            self.CR = 0.9 - (0.6 * (num_evaluations / self.budget))  # Dynamic adjustment\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                mutant = mutate(i)\n                trial = np.where(np.random.rand(self.dim) < self.CR, mutant, population[i])\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = trial\n\n            # Simulated Annealing part\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                neighbor = population[i] + np.random.normal(0, 0.1 * self.temp / 1000, self.dim)\n                neighbor = np.clip(neighbor, self.lower_bound, self.upper_bound)\n                neighbor_fitness = func(neighbor)\n                num_evaluations += 1\n                delta_fit = neighbor_fitness - fitness[i]\n\n                if delta_fit < 0 or np.random.rand() < np.exp(-delta_fit / self.temp):\n                    population[i] = neighbor\n                    fitness[i] = neighbor_fitness\n                    if neighbor_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = neighbor\n\n            self.temp *= (self.cooling_rate - 0.01 * (num_evaluations / self.budget))  # Adjusted cooling rate\n            self.pop_size = max(4, int(10 * self.dim * (1 - num_evaluations / self.budget)))  # Adaptive population size\n\n        return best_solution", "name": "HybridDESA", "description": "Introduced a dynamic scaling factor for crossover probability adjustment to balance exploration and exploitation.", "configspace": "", "generation": 17, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "2a189148-cfe4-4ead-8a5c-8cd5936dc0ae", "metadata": {"aucs": [0.4739815114301914, 0.4739815114301914, 0.4739815114301914, 0.4580713451873616, 0.4580713451873616, 0.4580713451873616, 0.4620130756557551, 0.4620130756557551, 0.4620130756557551, 0.17038982440279704, 0.17038982440279704, 0.17038982440279704, 0.15385152873300612, 0.15385152873300612, 0.15385152873300612, 0.1663339143461786, 0.1663339143461786, 0.1663339143461786, 0.12486335942924287, 0.12486335942924287, 0.12486335942924287, 0.10813262048403394, 0.10813262048403394, 0.10813262048403394, 0.11881270881742478, 0.11881270881742478, 0.11881270881742478, 0.10824281801852631, 0.10824281801852631, 0.10824281801852631, 0.11322547617394407, 0.11322547617394407, 0.11322547617394407, 0.10405913499750241, 0.10405913499750241, 0.10405913499750241, 0.853278449181514, 0.853278449181514, 0.853278449181514, 0.8473735328002856, 0.8473735328002856, 0.8473735328002856, 0.8800149853290414, 0.8800149853290414, 0.8800149853290414, 0.24133741314659451, 0.24133741314659451, 0.24133741314659451, 0.24718444240218929, 0.24718444240218929, 0.24718444240218929, 0.23341780277017476, 0.23341780277017476, 0.23341780277017476, 0.3439750718484226, 0.3439750718484226, 0.3439750718484226, 0.4103809775169721, 0.4103809775169721, 0.4103809775169721, 0.4633511200904816, 0.4633511200904816, 0.4633511200904816, 0.12831696777215296, 0.12831696777215296, 0.12831696777215296, 0.12159210028599376, 0.12159210028599376, 0.12159210028599376, 0.13397392204348713, 0.13397392204348713, 0.13397392204348713, 0.10253543569337886, 0.10253543569337886, 0.10253543569337886, 0.12764177756725947, 0.12764177756725947, 0.12764177756725947, 0.12644172802085796, 0.12644172802085796, 0.12644172802085796, 0.04558130172842012, 0.04558130172842012, 0.04558130172842012, 0.04473178048447968, 0.04473178048447968, 0.04473178048447968, 0.032070071593435334, 0.032070071593435334, 0.032070071593435334, 0.10103826365976853, 0.10103826365976853, 0.10103826365976853, 0.12276910875703129, 0.12276910875703129, 0.12276910875703129, 0.13936216289098324, 0.13936216289098324, 0.13936216289098324, 0.031449435153703464, 0.031449435153703464, 0.031449435153703464, 0.039555157704201926, 0.039555157704201926, 0.039555157704201926, 0.038945578913286094, 0.038945578913286094, 0.038945578913286094, 0.06955693202966884, 0.06955693202966884, 0.06955693202966884, 0.09934859166641341, 0.09934859166641341, 0.09934859166641341, 0.0828010724014897, 0.0828010724014897, 0.0828010724014897, 0.3964037824809562, 0.3964037824809562, 0.3964037824809562, 0.4281453999699457, 0.4281453999699457, 0.4281453999699457, 0.3827708880194084, 0.3827708880194084, 0.3827708880194084, 0.1047301359746381, 0.1047301359746381, 0.1047301359746381, 0.10075756308933237, 0.10075756308933237, 0.10075756308933237, 0.10143109282174423, 0.10143109282174423, 0.10143109282174423, 0.2002442689435744, 0.2002442689435744, 0.2002442689435744, 0.13599365658700002, 0.13599365658700002, 0.13599365658700002, 0.20154024037259655, 0.20154024037259655, 0.20154024037259655, 0.281901808326366, 0.281901808326366, 0.281901808326366, 0.292416731372546, 0.292416731372546, 0.292416731372546, 0.293614742175324, 0.293614742175324, 0.293614742175324, 0.20399577759668586, 0.20399577759668586, 0.20399577759668586, 0.19936064241421225, 0.19936064241421225, 0.19936064241421225, 0.22120392117735355, 0.22120392117735355, 0.22120392117735355, 0.20330017238642206, 0.20330017238642206, 0.20330017238642206, 0.19037896125160014, 0.19037896125160014, 0.19037896125160014, 0.2069963523471009, 0.2069963523471009, 0.2069963523471009, 0.2088400952495869, 0.2088400952495869, 0.2088400952495869, 0.1886904893644189, 0.1886904893644189, 0.1886904893644189, 0.2033576341792741, 0.2033576341792741, 0.2033576341792741, 0.3390971968709994, 0.3390971968709994, 0.3390971968709994, 0.3953614518219484, 0.3953614518219484, 0.3953614518219484, 0.30417065081796735, 0.30417065081796735, 0.30417065081796735, 0.19466015571668427, 0.19466015571668427, 0.19466015571668427, 0.1843802854418044, 0.1843802854418044, 0.1843802854418044, 0.24249775789581796, 0.24249775789581796, 0.24249775789581796, 0.18912521789970993, 0.18912521789970993, 0.18912521789970993, 0.18344980531936905, 0.18344980531936905, 0.18344980531936905, 0.18339191537139388, 0.18339191537139388, 0.18339191537139388, 0.08679174932959377, 0.08679174932959377, 0.08679174932959377, 0.07902304718636621, 0.07902304718636621, 0.07902304718636621, 0.08411515408141113, 0.08411515408141113, 0.08411515408141113]}, "mutation_prompt": null}
{"id": "de8433e9-acce-4f6c-9564-457f6fa59fb1", "solution": "import numpy as np\n\nclass HybridDESA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.pop_size = 10 * dim\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.temp = 1000.0  # Initial temperature for simulated annealing\n        self.cooling_rate = 0.99  # Cooling rate for simulated annealing\n\n    def __call__(self, func):\n        np.random.seed(42)\n        population = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        num_evaluations = self.pop_size\n\n        def mutate(target_idx):\n            indices = [idx for idx in range(self.pop_size) if idx != target_idx]\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            dynamic_F = self.F * (1.0 - (num_evaluations / self.budget))  # Dynamic mutation scaling\n            mutant = population[a] + dynamic_F * (population[b] - population[c])\n            mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n            return mutant\n\n        while num_evaluations < self.budget:\n            self.CR = 0.9 - (0.6 * (num_evaluations / self.budget))  # Dynamic adjustment\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                mutant = mutate(i)\n                trial = np.where(np.random.rand(self.dim) < self.CR, mutant, population[i])\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = trial\n\n            # Simulated Annealing part\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                neighbor = population[i] + np.random.normal(0, 0.1 * self.temp / 1000, self.dim)\n                neighbor = np.clip(neighbor, self.lower_bound, self.upper_bound)\n                neighbor_fitness = func(neighbor)\n                num_evaluations += 1\n                delta_fit = neighbor_fitness - fitness[i]\n\n                if delta_fit < 0 or np.random.rand() < np.exp(-delta_fit / self.temp):\n                    population[i] = neighbor\n                    fitness[i] = neighbor_fitness\n                    if neighbor_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = neighbor\n\n            self.temp *= (self.cooling_rate - 0.01 * (num_evaluations / self.budget))  # Adjusted cooling rate\n            self.pop_size = max(4, int(8 * self.dim * (1 - num_evaluations / self.budget)))  # Adaptive population size\n\n        return best_solution", "name": "HybridDESA", "description": "Enhanced adaptive population size strategy to improve convergence speed.", "configspace": "", "generation": 31, "fitness": 0.22859164585215735, "feedback": "The algorithm HybridDESA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.23 with standard deviation 0.18.", "error": "", "parent_id": "2a189148-cfe4-4ead-8a5c-8cd5936dc0ae", "metadata": {"aucs": [0.4803978287136904, 0.4803978287136904, 0.4803978287136904, 0.48637001682661396, 0.48637001682661396, 0.48637001682661396, 0.48277428464572925, 0.48277428464572925, 0.48277428464572925, 0.17945966552297543, 0.17945966552297543, 0.17945966552297543, 0.18382717777073476, 0.18382717777073476, 0.18382717777073476, 0.17192383116080467, 0.17192383116080467, 0.17192383116080467, 0.12770726756301043, 0.12770726756301043, 0.12770726756301043, 0.1543117882938293, 0.1543117882938293, 0.1543117882938293, 0.12192603517853062, 0.12192603517853062, 0.12192603517853062, 0.11128350872841786, 0.11128350872841786, 0.11128350872841786, 0.11058442364378629, 0.11058442364378629, 0.11058442364378629, 0.10507247597559288, 0.10507247597559288, 0.10507247597559288, 0.9169095589631192, 0.9169095589631192, 0.9169095589631192, 0.8410405421925882, 0.8410405421925882, 0.8410405421925882, 0.7947088736454208, 0.7947088736454208, 0.7947088736454208, 0.27579067450332284, 0.27579067450332284, 0.27579067450332284, 0.2631493092026419, 0.2631493092026419, 0.2631493092026419, 0.2625150822536768, 0.2625150822536768, 0.2625150822536768, 0.3038945294125428, 0.3038945294125428, 0.3038945294125428, 0.48460429084339895, 0.48460429084339895, 0.48460429084339895, 0.5082606935617336, 0.5082606935617336, 0.5082606935617336, 0.12456025044720065, 0.12456025044720065, 0.12456025044720065, 0.13677218157817372, 0.13677218157817372, 0.13677218157817372, 0.13952513092839103, 0.13952513092839103, 0.13952513092839103, 0.09580282974611032, 0.09580282974611032, 0.09580282974611032, 0.1372678660794936, 0.1372678660794936, 0.1372678660794936, 0.12969140822557113, 0.12969140822557113, 0.12969140822557113, 0.067801777208368, 0.067801777208368, 0.067801777208368, 0.05007538100006237, 0.05007538100006237, 0.05007538100006237, 0.05632082845863862, 0.05632082845863862, 0.05632082845863862, 0.11746153089078215, 0.11746153089078215, 0.11746153089078215, 0.1083091183759507, 0.1083091183759507, 0.1083091183759507, 0.11213413772776348, 0.11213413772776348, 0.11213413772776348, 0.04265691874092847, 0.04265691874092847, 0.04265691874092847, 0.051371654126801, 0.051371654126801, 0.051371654126801, 0.09431678787847853, 0.09431678787847853, 0.09431678787847853, 0.12527039864745493, 0.12527039864745493, 0.12527039864745493, 0.12594320767179235, 0.12594320767179235, 0.12594320767179235, 0.10376719806125456, 0.10376719806125456, 0.10376719806125456, 0.4106487325729282, 0.4106487325729282, 0.4106487325729282, 0.4006003233955462, 0.4006003233955462, 0.4006003233955462, 0.4067830618323576, 0.4067830618323576, 0.4067830618323576, 0.15326975229646667, 0.15326975229646667, 0.15326975229646667, 0.11249511676218571, 0.11249511676218571, 0.11249511676218571, 0.10715857294479625, 0.10715857294479625, 0.10715857294479625, 0.1378232692818543, 0.1378232692818543, 0.1378232692818543, 0.1377066874528884, 0.1377066874528884, 0.1377066874528884, 0.18618326816033293, 0.18618326816033293, 0.18618326816033293, 0.29267565480483126, 0.29267565480483126, 0.29267565480483126, 0.30686697501419213, 0.30686697501419213, 0.30686697501419213, 0.3124708758902104, 0.3124708758902104, 0.3124708758902104, 0.24087302187445625, 0.24087302187445625, 0.24087302187445625, 0.18055983453836832, 0.18055983453836832, 0.18055983453836832, 0.22395756189905747, 0.22395756189905747, 0.22395756189905747, 0.20079618008136957, 0.20079618008136957, 0.20079618008136957, 0.21690664228023704, 0.21690664228023704, 0.21690664228023704, 0.21623102098877, 0.21623102098877, 0.21623102098877, 0.20538354942451686, 0.20538354942451686, 0.20538354942451686, 0.2640035154681123, 0.2640035154681123, 0.2640035154681123, 0.2817644801285576, 0.2817644801285576, 0.2817644801285576, 0.1624024419431025, 0.1624024419431025, 0.1624024419431025, 0.1742746574098295, 0.1742746574098295, 0.1742746574098295, 0.5340755470732428, 0.5340755470732428, 0.5340755470732428, 0.1605161584087733, 0.1605161584087733, 0.1605161584087733, 0.24777227252037182, 0.24777227252037182, 0.24777227252037182, 0.19120669969901882, 0.19120669969901882, 0.19120669969901882, 0.1813052562563292, 0.1813052562563292, 0.1813052562563292, 0.19993427708307487, 0.19993427708307487, 0.19993427708307487, 0.1908162113166404, 0.1908162113166404, 0.1908162113166404, 0.07961485962677584, 0.07961485962677584, 0.07961485962677584, 0.08003199655401427, 0.08003199655401427, 0.08003199655401427, 0.07592956197674305, 0.07592956197674305, 0.07592956197674305]}, "mutation_prompt": null}
{"id": "91d59a85-c9de-4b29-ae4f-4f43e3b14ea5", "solution": "import numpy as np\n\nclass HybridDESA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.pop_size = 10 * dim\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.temp = 1000.0  # Initial temperature for simulated annealing\n        self.cooling_rate = 0.99  # Cooling rate for simulated annealing\n\n    def __call__(self, func):\n        np.random.seed(42)\n        population = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        num_evaluations = self.pop_size\n\n        def mutate(target_idx):\n            indices = [idx for idx in range(self.pop_size) if idx != target_idx]\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            dynamic_F = self.F * (1.0 - (num_evaluations / self.budget))  # Dynamic mutation scaling\n            mutant = population[a] + dynamic_F * (population[b] - population[c])\n            mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n            return mutant\n\n        while num_evaluations < self.budget:\n            self.CR = 0.9 - (0.6 * (num_evaluations / self.budget))  # Dynamic adjustment\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                mutant = mutate(i)\n                trial = np.where(np.random.rand(self.dim) < self.CR, mutant, population[i])\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = trial\n\n            # Simulated Annealing part\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                neighbor = population[i] + np.random.normal(0, 0.1 * self.temp / 1000, self.dim)\n                neighbor = np.clip(neighbor, self.lower_bound, self.upper_bound)\n                neighbor_fitness = func(neighbor)\n                num_evaluations += 1\n                delta_fit = neighbor_fitness - fitness[i]\n\n                if delta_fit < 0 or np.random.rand() < np.exp(-delta_fit / self.temp):\n                    population[i] = neighbor\n                    fitness[i] = neighbor_fitness\n                    if neighbor_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = neighbor\n\n            self.temp *= (self.cooling_rate - 0.01 * (num_evaluations / self.budget))  # Adjusted cooling rate\n            self.pop_size = max(4, int(8 * self.dim * (1 - num_evaluations / self.budget)))  # Adaptive population size\n\n        return best_solution", "name": "HybridDESA", "description": "Enhanced adaptive population size strategy to improve convergence speed.", "configspace": "", "generation": 32, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "de8433e9-acce-4f6c-9564-457f6fa59fb1", "metadata": {"aucs": [0.4803978287136904, 0.4803978287136904, 0.4803978287136904, 0.48637001682661396, 0.48637001682661396, 0.48637001682661396, 0.48277428464572925, 0.48277428464572925, 0.48277428464572925, 0.17945966552297543, 0.17945966552297543, 0.17945966552297543, 0.18382717777073476, 0.18382717777073476, 0.18382717777073476, 0.17192383116080467, 0.17192383116080467, 0.17192383116080467, 0.12770726756301043, 0.12770726756301043, 0.12770726756301043, 0.1543117882938293, 0.1543117882938293, 0.1543117882938293, 0.12192603517853062, 0.12192603517853062, 0.12192603517853062, 0.11128350872841786, 0.11128350872841786, 0.11128350872841786, 0.11058442364378629, 0.11058442364378629, 0.11058442364378629, 0.10507247597559288, 0.10507247597559288, 0.10507247597559288, 0.9169095589631192, 0.9169095589631192, 0.9169095589631192, 0.8410405421925882, 0.8410405421925882, 0.8410405421925882, 0.7947088736454208, 0.7947088736454208, 0.7947088736454208, 0.27579067450332284, 0.27579067450332284, 0.27579067450332284, 0.2631493092026419, 0.2631493092026419, 0.2631493092026419, 0.2625150822536768, 0.2625150822536768, 0.2625150822536768, 0.3038945294125428, 0.3038945294125428, 0.3038945294125428, 0.48460429084339895, 0.48460429084339895, 0.48460429084339895, 0.5082606935617336, 0.5082606935617336, 0.5082606935617336, 0.12456025044720065, 0.12456025044720065, 0.12456025044720065, 0.13677218157817372, 0.13677218157817372, 0.13677218157817372, 0.13952513092839103, 0.13952513092839103, 0.13952513092839103, 0.09580282974611032, 0.09580282974611032, 0.09580282974611032, 0.1372678660794936, 0.1372678660794936, 0.1372678660794936, 0.12969140822557113, 0.12969140822557113, 0.12969140822557113, 0.067801777208368, 0.067801777208368, 0.067801777208368, 0.05007538100006237, 0.05007538100006237, 0.05007538100006237, 0.05632082845863862, 0.05632082845863862, 0.05632082845863862, 0.11746153089078215, 0.11746153089078215, 0.11746153089078215, 0.1083091183759507, 0.1083091183759507, 0.1083091183759507, 0.11213413772776348, 0.11213413772776348, 0.11213413772776348, 0.04265691874092847, 0.04265691874092847, 0.04265691874092847, 0.051371654126801, 0.051371654126801, 0.051371654126801, 0.09431678787847853, 0.09431678787847853, 0.09431678787847853, 0.12527039864745493, 0.12527039864745493, 0.12527039864745493, 0.12594320767179235, 0.12594320767179235, 0.12594320767179235, 0.10376719806125456, 0.10376719806125456, 0.10376719806125456, 0.4106487325729282, 0.4106487325729282, 0.4106487325729282, 0.4006003233955462, 0.4006003233955462, 0.4006003233955462, 0.4067830618323576, 0.4067830618323576, 0.4067830618323576, 0.15326975229646667, 0.15326975229646667, 0.15326975229646667, 0.11249511676218571, 0.11249511676218571, 0.11249511676218571, 0.10715857294479625, 0.10715857294479625, 0.10715857294479625, 0.1378232692818543, 0.1378232692818543, 0.1378232692818543, 0.1377066874528884, 0.1377066874528884, 0.1377066874528884, 0.18618326816033293, 0.18618326816033293, 0.18618326816033293, 0.29267565480483126, 0.29267565480483126, 0.29267565480483126, 0.30686697501419213, 0.30686697501419213, 0.30686697501419213, 0.3124708758902104, 0.3124708758902104, 0.3124708758902104, 0.24087302187445625, 0.24087302187445625, 0.24087302187445625, 0.18055983453836832, 0.18055983453836832, 0.18055983453836832, 0.22395756189905747, 0.22395756189905747, 0.22395756189905747, 0.20079618008136957, 0.20079618008136957, 0.20079618008136957, 0.21690664228023704, 0.21690664228023704, 0.21690664228023704, 0.21623102098877, 0.21623102098877, 0.21623102098877, 0.20538354942451686, 0.20538354942451686, 0.20538354942451686, 0.2640035154681123, 0.2640035154681123, 0.2640035154681123, 0.2817644801285576, 0.2817644801285576, 0.2817644801285576, 0.1624024419431025, 0.1624024419431025, 0.1624024419431025, 0.1742746574098295, 0.1742746574098295, 0.1742746574098295, 0.5340755470732428, 0.5340755470732428, 0.5340755470732428, 0.1605161584087733, 0.1605161584087733, 0.1605161584087733, 0.24777227252037182, 0.24777227252037182, 0.24777227252037182, 0.19120669969901882, 0.19120669969901882, 0.19120669969901882, 0.1813052562563292, 0.1813052562563292, 0.1813052562563292, 0.19993427708307487, 0.19993427708307487, 0.19993427708307487, 0.1908162113166404, 0.1908162113166404, 0.1908162113166404, 0.07961485962677584, 0.07961485962677584, 0.07961485962677584, 0.08003199655401427, 0.08003199655401427, 0.08003199655401427, 0.07592956197674305, 0.07592956197674305, 0.07592956197674305]}, "mutation_prompt": null}
{"id": "25a20b83-a4cb-4ed7-83ab-f557f0d1bd60", "solution": "import numpy as np\n\nclass HybridDESA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.pop_size = 10 * dim\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.temp = 1000.0  # Initial temperature for simulated annealing\n        self.cooling_rate = 0.99  # Cooling rate for simulated annealing\n\n    def __call__(self, func):\n        np.random.seed(42)\n        population = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        num_evaluations = self.pop_size\n\n        def mutate(target_idx):\n            indices = [idx for idx in range(self.pop_size) if idx != target_idx]\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            dynamic_F = self.F * (1.0 - (num_evaluations / self.budget))  # Dynamic mutation scaling\n            mutant = population[a] + dynamic_F * (population[b] - population[c])\n            mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n            return mutant\n\n        while num_evaluations < self.budget:\n            self.CR = 0.9 - (0.6 * (num_evaluations / self.budget))  # Dynamic adjustment\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                mutant = mutate(i)\n                trial = np.where(np.random.rand(self.dim) < self.CR, mutant, population[i])\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = trial\n\n            # Simulated Annealing part\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                neighbor = population[i] + np.random.normal(0, 0.1 * self.temp / 1000, self.dim)\n                neighbor = np.clip(neighbor, self.lower_bound, self.upper_bound)\n                neighbor_fitness = func(neighbor)\n                num_evaluations += 1\n                delta_fit = neighbor_fitness - fitness[i]\n\n                if delta_fit < 0 or np.random.rand() < np.exp(-delta_fit / self.temp):\n                    population[i] = neighbor\n                    fitness[i] = neighbor_fitness\n                    if neighbor_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = neighbor\n\n            self.temp *= (self.cooling_rate - 0.005 * (num_evaluations / self.budget))  # Adjusted cooling rate\n            self.pop_size = max(4, int(8 * self.dim * (1 - num_evaluations / self.budget)))  # Adaptive population size\n\n        return best_solution", "name": "HybridDESA", "description": "Refined adaptive cooling rate to enhance the balance between exploration and exploitation.", "configspace": "", "generation": 33, "fitness": 0.21707874481939934, "feedback": "The algorithm HybridDESA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.22 with standard deviation 0.17.", "error": "", "parent_id": "de8433e9-acce-4f6c-9564-457f6fa59fb1", "metadata": {"aucs": [0.4744089204320717, 0.4744089204320717, 0.4744089204320717, 0.46240056049573175, 0.46240056049573175, 0.46240056049573175, 0.47341625171967905, 0.47341625171967905, 0.47341625171967905, 0.18640077960936807, 0.18640077960936807, 0.18640077960936807, 0.18319913737083326, 0.18319913737083326, 0.18319913737083326, 0.1633807187728633, 0.1633807187728633, 0.1633807187728633, 0.18878778950921227, 0.18878778950921227, 0.18878778950921227, 0.09817217897877184, 0.09817217897877184, 0.09817217897877184, 0.12354150882027393, 0.12354150882027393, 0.12354150882027393, 0.09023652393056003, 0.09023652393056003, 0.09023652393056003, 0.10679273294094693, 0.10679273294094693, 0.10679273294094693, 0.10267431418402939, 0.10267431418402939, 0.10267431418402939, 0.916909994723062, 0.916909994723062, 0.916909994723062, 0.8410427686560393, 0.8410427686560393, 0.8410427686560393, 0.8215875367657142, 0.8215875367657142, 0.8215875367657142, 0.2662521536952399, 0.2662521536952399, 0.2662521536952399, 0.2683068575681935, 0.2683068575681935, 0.2683068575681935, 0.264709045849308, 0.264709045849308, 0.264709045849308, 0.4692387342801584, 0.4692387342801584, 0.4692387342801584, 0.4210743542898465, 0.4210743542898465, 0.4210743542898465, 0.29996516417240826, 0.29996516417240826, 0.29996516417240826, 0.13273164266545268, 0.13273164266545268, 0.13273164266545268, 0.14451273704472678, 0.14451273704472678, 0.14451273704472678, 0.1316005891922778, 0.1316005891922778, 0.1316005891922778, 0.11089433001376037, 0.11089433001376037, 0.11089433001376037, 0.13237069968824688, 0.13237069968824688, 0.13237069968824688, 0.12747065672240876, 0.12747065672240876, 0.12747065672240876, 0.045270703333836204, 0.045270703333836204, 0.045270703333836204, 0.05168280580538376, 0.05168280580538376, 0.05168280580538376, 0.04373699380393936, 0.04373699380393936, 0.04373699380393936, 0.13717563007827416, 0.13717563007827416, 0.13717563007827416, 0.06846677155408198, 0.06846677155408198, 0.06846677155408198, 0.13654824461780712, 0.13654824461780712, 0.13654824461780712, 0.03574014569679296, 0.03574014569679296, 0.03574014569679296, 0.04181072589790513, 0.04181072589790513, 0.04181072589790513, 0.037227993470671183, 0.037227993470671183, 0.037227993470671183, 0.09806467607496738, 0.09806467607496738, 0.09806467607496738, 0.10556646796941027, 0.10556646796941027, 0.10556646796941027, 0.10245123795450761, 0.10245123795450761, 0.10245123795450761, 0.4113886864083087, 0.4113886864083087, 0.4113886864083087, 0.3975022820553138, 0.3975022820553138, 0.3975022820553138, 0.40241488329601094, 0.40241488329601094, 0.40241488329601094, 0.10906311163028126, 0.10906311163028126, 0.10906311163028126, 0.10069139524672499, 0.10069139524672499, 0.10069139524672499, 0.09872810238331842, 0.09872810238331842, 0.09872810238331842, 0.1630185342498789, 0.1630185342498789, 0.1630185342498789, 0.13880236377576438, 0.13880236377576438, 0.13880236377576438, 0.16544140527663898, 0.16544140527663898, 0.16544140527663898, 0.27817458891059055, 0.27817458891059055, 0.27817458891059055, 0.2778725058391289, 0.2778725058391289, 0.2778725058391289, 0.2906433601535746, 0.2906433601535746, 0.2906433601535746, 0.21901776661425976, 0.21901776661425976, 0.21901776661425976, 0.22935144462936274, 0.22935144462936274, 0.22935144462936274, 0.21464967688623915, 0.21464967688623915, 0.21464967688623915, 0.20348378476886286, 0.20348378476886286, 0.20348378476886286, 0.18640844276859259, 0.18640844276859259, 0.18640844276859259, 0.19968277727040262, 0.19968277727040262, 0.19968277727040262, 0.18197633686649295, 0.18197633686649295, 0.18197633686649295, 0.2088085186335389, 0.2088085186335389, 0.2088085186335389, 0.2608902513275272, 0.2608902513275272, 0.2608902513275272, 0.1774806497006739, 0.1774806497006739, 0.1774806497006739, 0.1740343282973208, 0.1740343282973208, 0.1740343282973208, 0.28880651347800745, 0.28880651347800745, 0.28880651347800745, 0.1604855141057573, 0.1604855141057573, 0.1604855141057573, 0.18745657899975887, 0.18745657899975887, 0.18745657899975887, 0.19126804616054416, 0.19126804616054416, 0.19126804616054416, 0.19010037358201115, 0.19010037358201115, 0.19010037358201115, 0.17870666094649967, 0.17870666094649967, 0.17870666094649967, 0.1788555614589723, 0.1788555614589723, 0.1788555614589723, 0.08430565660174272, 0.08430565660174272, 0.08430565660174272, 0.09004463975134547, 0.09004463975134547, 0.09004463975134547, 0.08429380657454533, 0.08429380657454533, 0.08429380657454533]}, "mutation_prompt": null}
{"id": "a23abc51-cac8-4819-b530-c9b5e6c87530", "solution": "import numpy as np\n\nclass HybridDESA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.pop_size = 10 * dim\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.temp = 1000.0  # Initial temperature for simulated annealing\n        self.cooling_rate = 0.99  # Cooling rate for simulated annealing\n\n    def __call__(self, func):\n        np.random.seed(42)\n        population = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        num_evaluations = self.pop_size\n\n        def mutate(target_idx):\n            indices = [idx for idx in range(self.pop_size) if idx != target_idx]\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            dynamic_F = self.F * (1.0 - (num_evaluations / self.budget))  # Dynamic mutation scaling\n            mutant = population[a] + dynamic_F * (population[b] - population[c])\n            mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n            return mutant\n\n        while num_evaluations < self.budget:\n            self.CR = 0.9 - (0.5 * (num_evaluations / self.budget))  # Dynamic adjustment with enhanced exploration\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                mutant = mutate(i)\n                trial = np.where(np.random.rand(self.dim) < self.CR, mutant, population[i])\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = trial\n\n            # Simulated Annealing part\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                neighbor = population[i] + np.random.normal(0, 0.1 * self.temp / 1000, self.dim)\n                neighbor = np.clip(neighbor, self.lower_bound, self.upper_bound)\n                neighbor_fitness = func(neighbor)\n                num_evaluations += 1\n                delta_fit = neighbor_fitness - fitness[i]\n\n                if delta_fit < 0 or np.random.rand() < np.exp(-delta_fit / self.temp):\n                    population[i] = neighbor\n                    fitness[i] = neighbor_fitness\n                    if neighbor_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = neighbor\n\n            self.temp *= (self.cooling_rate - 0.01 * (num_evaluations / self.budget))  # Adjusted cooling rate\n            self.pop_size = max(4, int(8 * self.dim * (1 - num_evaluations / self.budget)))  # Adaptive population size\n\n        return best_solution", "name": "HybridDESA", "description": "Introduce a dynamic crossover probability adjustment to enhance exploration-exploitation balance.", "configspace": "", "generation": 34, "fitness": 0.22877128996775012, "feedback": "The algorithm HybridDESA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.23 with standard deviation 0.17.", "error": "", "parent_id": "de8433e9-acce-4f6c-9564-457f6fa59fb1", "metadata": {"aucs": [0.4933307569063823, 0.4933307569063823, 0.4933307569063823, 0.486302594801032, 0.486302594801032, 0.486302594801032, 0.4964680403102998, 0.4964680403102998, 0.4964680403102998, 0.14340492288608964, 0.14340492288608964, 0.14340492288608964, 0.1422724251857611, 0.1422724251857611, 0.1422724251857611, 0.16165522246329345, 0.16165522246329345, 0.16165522246329345, 0.12393775082031844, 0.12393775082031844, 0.12393775082031844, 0.12765933102801597, 0.12765933102801597, 0.12765933102801597, 0.20375654338917404, 0.20375654338917404, 0.20375654338917404, 0.0920841054098861, 0.0920841054098861, 0.0920841054098861, 0.10022505172180085, 0.10022505172180085, 0.10022505172180085, 0.10097089880597687, 0.10097089880597687, 0.10097089880597687, 0.7992041166385038, 0.7992041166385038, 0.7992041166385038, 0.7579844574770246, 0.7579844574770246, 0.7579844574770246, 0.7737581596887949, 0.7737581596887949, 0.7737581596887949, 0.28248354486540084, 0.28248354486540084, 0.28248354486540084, 0.282674282520579, 0.282674282520579, 0.282674282520579, 0.28484282590696064, 0.28484282590696064, 0.28484282590696064, 0.2477350768058978, 0.2477350768058978, 0.2477350768058978, 0.237134617525955, 0.237134617525955, 0.237134617525955, 0.5451365546244145, 0.5451365546244145, 0.5451365546244145, 0.12783816734412123, 0.12783816734412123, 0.12783816734412123, 0.12574152059045207, 0.12574152059045207, 0.12574152059045207, 0.12632135467586758, 0.12632135467586758, 0.12632135467586758, 0.13578031704396287, 0.13578031704396287, 0.13578031704396287, 0.12201859771190504, 0.12201859771190504, 0.12201859771190504, 0.13421444142737615, 0.13421444142737615, 0.13421444142737615, 0.08856711966563091, 0.08856711966563091, 0.08856711966563091, 0.056388941119007385, 0.056388941119007385, 0.056388941119007385, 0.04570311717095721, 0.04570311717095721, 0.04570311717095721, 0.15850012106421596, 0.15850012106421596, 0.15850012106421596, 0.10033817853138516, 0.10033817853138516, 0.10033817853138516, 0.11273852385815974, 0.11273852385815974, 0.11273852385815974, 0.048617488822171095, 0.048617488822171095, 0.048617488822171095, 0.03716194485287394, 0.03716194485287394, 0.03716194485287394, 0.051200970563792136, 0.051200970563792136, 0.051200970563792136, 0.12763542759695323, 0.12763542759695323, 0.12763542759695323, 0.11139396244910182, 0.11139396244910182, 0.11139396244910182, 0.10285323609250896, 0.10285323609250896, 0.10285323609250896, 0.4156220858285443, 0.4156220858285443, 0.4156220858285443, 0.41272255275136727, 0.41272255275136727, 0.41272255275136727, 0.42282518010998715, 0.42282518010998715, 0.42282518010998715, 0.10704494424892741, 0.10704494424892741, 0.10704494424892741, 0.12610176361361303, 0.12610176361361303, 0.12610176361361303, 0.12766715918892935, 0.12766715918892935, 0.12766715918892935, 0.17320406832110713, 0.17320406832110713, 0.17320406832110713, 0.15315830157665133, 0.15315830157665133, 0.15315830157665133, 0.21136080206692454, 0.21136080206692454, 0.21136080206692454, 0.3028030661717035, 0.3028030661717035, 0.3028030661717035, 0.3040970811034772, 0.3040970811034772, 0.3040970811034772, 0.31464764311072513, 0.31464764311072513, 0.31464764311072513, 0.24680674733440022, 0.24680674733440022, 0.24680674733440022, 0.19555008241045257, 0.19555008241045257, 0.19555008241045257, 0.23291985879123478, 0.23291985879123478, 0.23291985879123478, 0.21735514494443253, 0.21735514494443253, 0.21735514494443253, 0.1878427171211784, 0.1878427171211784, 0.1878427171211784, 0.20481832023262403, 0.20481832023262403, 0.20481832023262403, 0.31686050696082024, 0.31686050696082024, 0.31686050696082024, 0.1857281694049977, 0.1857281694049977, 0.1857281694049977, 0.20722338285808206, 0.20722338285808206, 0.20722338285808206, 0.5737733000582486, 0.5737733000582486, 0.5737733000582486, 0.24658391305718264, 0.24658391305718264, 0.24658391305718264, 0.2203240111667989, 0.2203240111667989, 0.2203240111667989, 0.19435002598241335, 0.19435002598241335, 0.19435002598241335, 0.2593360928281354, 0.2593360928281354, 0.2593360928281354, 0.37939833507252774, 0.37939833507252774, 0.37939833507252774, 0.2031622182143844, 0.2031622182143844, 0.2031622182143844, 0.1864831910694391, 0.1864831910694391, 0.1864831910694391, 0.1936926537367577, 0.1936926537367577, 0.1936926537367577, 0.08529388696491058, 0.08529388696491058, 0.08529388696491058, 0.0837976732411414, 0.0837976732411414, 0.0837976732411414, 0.08294328777388438, 0.08294328777388438, 0.08294328777388438]}, "mutation_prompt": null}
{"id": "89e24f1c-7e25-422e-bb0e-1885f2a88b93", "solution": "import numpy as np\n\nclass HybridDESA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.pop_size = 10 * dim\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.temp = 1000.0  # Initial temperature for simulated annealing\n        self.cooling_rate = 0.99  # Cooling rate for simulated annealing\n\n    def __call__(self, func):\n        np.random.seed(42)\n        population = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        num_evaluations = self.pop_size\n\n        def mutate(target_idx):\n            indices = [idx for idx in range(self.pop_size) if idx != target_idx]\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            dynamic_F = self.F * (1.0 - ((num_evaluations / self.budget)**1.2))  # Dynamic mutation scaling\n            mutant = population[a] + dynamic_F * (population[b] - population[c])\n            mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n            return mutant\n\n        while num_evaluations < self.budget:\n            self.CR = 0.9 - (0.5 * (num_evaluations / self.budget))  # Dynamic adjustment with enhanced exploration\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                mutant = mutate(i)\n                trial = np.where(np.random.rand(self.dim) < self.CR, mutant, population[i])\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = trial\n\n            # Simulated Annealing part\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                neighbor = population[i] + np.random.normal(0, 0.1 * self.temp / 1000, self.dim)\n                neighbor = np.clip(neighbor, self.lower_bound, self.upper_bound)\n                neighbor_fitness = func(neighbor)\n                num_evaluations += 1\n                delta_fit = neighbor_fitness - fitness[i]\n\n                if delta_fit < 0 or np.random.rand() < np.exp(-delta_fit / self.temp):\n                    population[i] = neighbor\n                    fitness[i] = neighbor_fitness\n                    if neighbor_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = neighbor\n\n            self.temp *= (self.cooling_rate - 0.01 * (num_evaluations / self.budget))  # Adjusted cooling rate\n            self.pop_size = max(4, int(8 * self.dim * (1 - num_evaluations / self.budget)))  # Adaptive population size\n\n        return best_solution", "name": "HybridDESA", "description": "Introduce adaptive dynamic scaling of differential weight to improve convergence speed.", "configspace": "", "generation": 35, "fitness": 0.22152087409012633, "feedback": "The algorithm HybridDESA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.22 with standard deviation 0.17.", "error": "", "parent_id": "a23abc51-cac8-4819-b530-c9b5e6c87530", "metadata": {"aucs": [0.49345389498021097, 0.49345389498021097, 0.49345389498021097, 0.4815710084731155, 0.4815710084731155, 0.4815710084731155, 0.4823790739343783, 0.4823790739343783, 0.4823790739343783, 0.17690871378180328, 0.17690871378180328, 0.17690871378180328, 0.15999172129011552, 0.15999172129011552, 0.15999172129011552, 0.19331403776446732, 0.19331403776446732, 0.19331403776446732, 0.1235446565201671, 0.1235446565201671, 0.1235446565201671, 0.11407455950330292, 0.11407455950330292, 0.11407455950330292, 0.11786496303614136, 0.11786496303614136, 0.11786496303614136, 0.10414252416075054, 0.10414252416075054, 0.10414252416075054, 0.10708945028405759, 0.10708945028405759, 0.10708945028405759, 0.09815821514115564, 0.09815821514115564, 0.09815821514115564, 0.8832384127434012, 0.8832384127434012, 0.8832384127434012, 0.654316636527595, 0.654316636527595, 0.654316636527595, 0.9298483252765416, 0.9298483252765416, 0.9298483252765416, 0.2703930980208007, 0.2703930980208007, 0.2703930980208007, 0.28942555691519667, 0.28942555691519667, 0.28942555691519667, 0.2856835123214312, 0.2856835123214312, 0.2856835123214312, 0.4296664205042887, 0.4296664205042887, 0.4296664205042887, 0.4716319308015051, 0.4716319308015051, 0.4716319308015051, 0.47568342945244957, 0.47568342945244957, 0.47568342945244957, 0.13457701958587676, 0.13457701958587676, 0.13457701958587676, 0.13263480948365525, 0.13263480948365525, 0.13263480948365525, 0.13228297526965638, 0.13228297526965638, 0.13228297526965638, 0.13399886258822313, 0.13399886258822313, 0.13399886258822313, 0.12251210957609893, 0.12251210957609893, 0.12251210957609893, 0.10930260402356362, 0.10930260402356362, 0.10930260402356362, 0.040621839850326635, 0.040621839850326635, 0.040621839850326635, 0.047332484206943026, 0.047332484206943026, 0.047332484206943026, 0.06535457480139661, 0.06535457480139661, 0.06535457480139661, 0.11586025078539208, 0.11586025078539208, 0.11586025078539208, 0.11172234666059633, 0.11172234666059633, 0.11172234666059633, 0.13416171448526215, 0.13416171448526215, 0.13416171448526215, 0.044159109839208344, 0.044159109839208344, 0.044159109839208344, 0.05150699809263426, 0.05150699809263426, 0.05150699809263426, 0.04305969738347015, 0.04305969738347015, 0.04305969738347015, 0.11442308700328119, 0.11442308700328119, 0.11442308700328119, 0.12283355155881204, 0.12283355155881204, 0.12283355155881204, 0.10667072304203884, 0.10667072304203884, 0.10667072304203884, 0.41505735275626143, 0.41505735275626143, 0.41505735275626143, 0.399102149559477, 0.399102149559477, 0.399102149559477, 0.4006876055521019, 0.4006876055521019, 0.4006876055521019, 0.11086714087097749, 0.11086714087097749, 0.11086714087097749, 0.11007127108888348, 0.11007127108888348, 0.11007127108888348, 0.10324306697560737, 0.10324306697560737, 0.10324306697560737, 0.15193019554477738, 0.15193019554477738, 0.15193019554477738, 0.13908824893662963, 0.13908824893662963, 0.13908824893662963, 0.21329178377095936, 0.21329178377095936, 0.21329178377095936, 0.27653378614085367, 0.27653378614085367, 0.27653378614085367, 0.2899480768778543, 0.2899480768778543, 0.2899480768778543, 0.305335291725762, 0.305335291725762, 0.305335291725762, 0.2219684125368172, 0.2219684125368172, 0.2219684125368172, 0.2076674752188995, 0.2076674752188995, 0.2076674752188995, 0.23626572740707974, 0.23626572740707974, 0.23626572740707974, 0.20881054912521269, 0.20881054912521269, 0.20881054912521269, 0.22909868927651011, 0.22909868927651011, 0.22909868927651011, 0.2058172218902008, 0.2058172218902008, 0.2058172218902008, 0.2585042188880423, 0.2585042188880423, 0.2585042188880423, 0.18548198505880287, 0.18548198505880287, 0.18548198505880287, 0.18418240674616893, 0.18418240674616893, 0.18418240674616893, 0.1624890996957734, 0.1624890996957734, 0.1624890996957734, 0.16859514014993526, 0.16859514014993526, 0.16859514014993526, 0.27401129644903466, 0.27401129644903466, 0.27401129644903466, 0.16073013384975277, 0.16073013384975277, 0.16073013384975277, 0.16938070652263648, 0.16938070652263648, 0.16938070652263648, 0.24613795137585892, 0.24613795137585892, 0.24613795137585892, 0.1866905296985667, 0.1866905296985667, 0.1866905296985667, 0.19327687472935673, 0.19327687472935673, 0.19327687472935673, 0.188530785975762, 0.188530785975762, 0.188530785975762, 0.08614113376796706, 0.08614113376796706, 0.08614113376796706, 0.07626745910864718, 0.07626745910864718, 0.07626745910864718, 0.07890426754861635, 0.07890426754861635, 0.07890426754861635]}, "mutation_prompt": null}
{"id": "d5b8f280-8823-441b-9780-86ab415a11c0", "solution": "import numpy as np\n\nclass HybridDESA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.pop_size = 10 * dim\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.temp = 1000.0  # Initial temperature for simulated annealing\n        self.cooling_rate = 0.99  # Cooling rate for simulated annealing\n\n    def __call__(self, func):\n        np.random.seed(42)\n        population = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        num_evaluations = self.pop_size\n\n        def mutate(target_idx):\n            indices = [idx for idx in range(self.pop_size) if idx != target_idx]\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            dynamic_F = self.F * (1.0 - (num_evaluations / self.budget))  # Dynamic mutation scaling\n            mutant = population[a] + dynamic_F * (population[b] - population[c])\n            mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n            return mutant\n\n        while num_evaluations < self.budget:\n            self.CR = 0.9 - (0.5 * (num_evaluations / self.budget))  # Dynamic adjustment with enhanced exploration\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                mutant = mutate(i)\n                trial = np.where(np.random.rand(self.dim) < self.CR, mutant, population[i])\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = trial\n\n            # Simulated Annealing part\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                neighbor = population[i] + np.random.normal(0, 0.1 * self.temp / 1000, self.dim)\n                neighbor = np.clip(neighbor, self.lower_bound, self.upper_bound)\n                neighbor_fitness = func(neighbor)\n                num_evaluations += 1\n                delta_fit = neighbor_fitness - fitness[i]\n\n                if delta_fit < 0 or np.random.rand() < np.exp(-delta_fit / self.temp):\n                    population[i] = neighbor\n                    fitness[i] = neighbor_fitness\n                    if neighbor_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = neighbor\n\n            self.temp *= (self.cooling_rate - 0.01 * (num_evaluations / self.budget))  # Adjusted cooling rate\n            self.pop_size = max(4, int(8 * self.dim * (1 - num_evaluations / self.budget)))  # Adaptive population size\n\n        return best_solution", "name": "HybridDESA", "description": "Introduce a dynamic crossover probability adjustment to enhance exploration-exploitation balance.", "configspace": "", "generation": 35, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "a23abc51-cac8-4819-b530-c9b5e6c87530", "metadata": {"aucs": [0.4933307569063823, 0.4933307569063823, 0.4933307569063823, 0.486302594801032, 0.486302594801032, 0.486302594801032, 0.4964680403102998, 0.4964680403102998, 0.4964680403102998, 0.14340492288608964, 0.14340492288608964, 0.14340492288608964, 0.1422724251857611, 0.1422724251857611, 0.1422724251857611, 0.16165522246329345, 0.16165522246329345, 0.16165522246329345, 0.12393775082031844, 0.12393775082031844, 0.12393775082031844, 0.12765933102801597, 0.12765933102801597, 0.12765933102801597, 0.20375654338917404, 0.20375654338917404, 0.20375654338917404, 0.0920841054098861, 0.0920841054098861, 0.0920841054098861, 0.10022505172180085, 0.10022505172180085, 0.10022505172180085, 0.10097089880597687, 0.10097089880597687, 0.10097089880597687, 0.7992041166385038, 0.7992041166385038, 0.7992041166385038, 0.7579844574770246, 0.7579844574770246, 0.7579844574770246, 0.7737581596887949, 0.7737581596887949, 0.7737581596887949, 0.28248354486540084, 0.28248354486540084, 0.28248354486540084, 0.282674282520579, 0.282674282520579, 0.282674282520579, 0.28484282590696064, 0.28484282590696064, 0.28484282590696064, 0.2477350768058978, 0.2477350768058978, 0.2477350768058978, 0.237134617525955, 0.237134617525955, 0.237134617525955, 0.5451365546244145, 0.5451365546244145, 0.5451365546244145, 0.12783816734412123, 0.12783816734412123, 0.12783816734412123, 0.12574152059045207, 0.12574152059045207, 0.12574152059045207, 0.12632135467586758, 0.12632135467586758, 0.12632135467586758, 0.13578031704396287, 0.13578031704396287, 0.13578031704396287, 0.12201859771190504, 0.12201859771190504, 0.12201859771190504, 0.13421444142737615, 0.13421444142737615, 0.13421444142737615, 0.08856711966563091, 0.08856711966563091, 0.08856711966563091, 0.056388941119007385, 0.056388941119007385, 0.056388941119007385, 0.04570311717095721, 0.04570311717095721, 0.04570311717095721, 0.15850012106421596, 0.15850012106421596, 0.15850012106421596, 0.10033817853138516, 0.10033817853138516, 0.10033817853138516, 0.11273852385815974, 0.11273852385815974, 0.11273852385815974, 0.048617488822171095, 0.048617488822171095, 0.048617488822171095, 0.03716194485287394, 0.03716194485287394, 0.03716194485287394, 0.051200970563792136, 0.051200970563792136, 0.051200970563792136, 0.12763542759695323, 0.12763542759695323, 0.12763542759695323, 0.11139396244910182, 0.11139396244910182, 0.11139396244910182, 0.10285323609250896, 0.10285323609250896, 0.10285323609250896, 0.4156220858285443, 0.4156220858285443, 0.4156220858285443, 0.41272255275136727, 0.41272255275136727, 0.41272255275136727, 0.42282518010998715, 0.42282518010998715, 0.42282518010998715, 0.10704494424892741, 0.10704494424892741, 0.10704494424892741, 0.12610176361361303, 0.12610176361361303, 0.12610176361361303, 0.12766715918892935, 0.12766715918892935, 0.12766715918892935, 0.17320406832110713, 0.17320406832110713, 0.17320406832110713, 0.15315830157665133, 0.15315830157665133, 0.15315830157665133, 0.21136080206692454, 0.21136080206692454, 0.21136080206692454, 0.3028030661717035, 0.3028030661717035, 0.3028030661717035, 0.3040970811034772, 0.3040970811034772, 0.3040970811034772, 0.31464764311072513, 0.31464764311072513, 0.31464764311072513, 0.24680674733440022, 0.24680674733440022, 0.24680674733440022, 0.19555008241045257, 0.19555008241045257, 0.19555008241045257, 0.23291985879123478, 0.23291985879123478, 0.23291985879123478, 0.21735514494443253, 0.21735514494443253, 0.21735514494443253, 0.1878427171211784, 0.1878427171211784, 0.1878427171211784, 0.20481832023262403, 0.20481832023262403, 0.20481832023262403, 0.31686050696082024, 0.31686050696082024, 0.31686050696082024, 0.1857281694049977, 0.1857281694049977, 0.1857281694049977, 0.20722338285808206, 0.20722338285808206, 0.20722338285808206, 0.5737733000582486, 0.5737733000582486, 0.5737733000582486, 0.24658391305718264, 0.24658391305718264, 0.24658391305718264, 0.2203240111667989, 0.2203240111667989, 0.2203240111667989, 0.19435002598241335, 0.19435002598241335, 0.19435002598241335, 0.2593360928281354, 0.2593360928281354, 0.2593360928281354, 0.37939833507252774, 0.37939833507252774, 0.37939833507252774, 0.2031622182143844, 0.2031622182143844, 0.2031622182143844, 0.1864831910694391, 0.1864831910694391, 0.1864831910694391, 0.1936926537367577, 0.1936926537367577, 0.1936926537367577, 0.08529388696491058, 0.08529388696491058, 0.08529388696491058, 0.0837976732411414, 0.0837976732411414, 0.0837976732411414, 0.08294328777388438, 0.08294328777388438, 0.08294328777388438]}, "mutation_prompt": null}
{"id": "123056e3-c27d-41f6-bc53-56742054bfee", "solution": "import numpy as np\n\nclass HybridDESA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.pop_size = 10 * dim\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.temp = 1000.0  # Initial temperature for simulated annealing\n        self.cooling_rate = 0.99  # Cooling rate for simulated annealing\n\n    def __call__(self, func):\n        np.random.seed(42)\n        population = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        num_evaluations = self.pop_size\n\n        def mutate(target_idx):\n            indices = [idx for idx in range(self.pop_size) if idx != target_idx]\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            proximity_factor = np.linalg.norm(population[target_idx] - best_solution) / self.dim\n            dynamic_F = self.F * (1.0 - (num_evaluations / self.budget)) * proximity_factor  # Adaptive mutation scaling\n            mutant = population[a] + dynamic_F * (population[b] - population[c])\n            mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n            return mutant\n\n        while num_evaluations < self.budget:\n            self.CR = 0.9 - (0.5 * (num_evaluations / self.budget))  # Dynamic adjustment with enhanced exploration\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                mutant = mutate(i)\n                trial = np.where(np.random.rand(self.dim) < self.CR, mutant, population[i])\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = trial\n\n            # Simulated Annealing part\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                neighbor = population[i] + np.random.normal(0, 0.1 * self.temp / 1000, self.dim)\n                neighbor = np.clip(neighbor, self.lower_bound, self.upper_bound)\n                neighbor_fitness = func(neighbor)\n                num_evaluations += 1\n                delta_fit = neighbor_fitness - fitness[i]\n\n                if delta_fit < 0 or np.random.rand() < np.exp(-delta_fit / self.temp):\n                    population[i] = neighbor\n                    fitness[i] = neighbor_fitness\n                    if neighbor_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = neighbor\n\n            self.temp *= (self.cooling_rate - 0.01 * (num_evaluations / self.budget))  # Adjusted cooling rate\n            self.pop_size = max(4, int(8 * self.dim * (1 - num_evaluations / self.budget)))  # Adaptive population size\n\n        return best_solution", "name": "HybridDESA", "description": "Introduce adaptive mutation scaling based on best solution proximity to improve convergence.", "configspace": "", "generation": 37, "fitness": 0.2270631885844787, "feedback": "The algorithm HybridDESA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.23 with standard deviation 0.19.", "error": "", "parent_id": "a23abc51-cac8-4819-b530-c9b5e6c87530", "metadata": {"aucs": [0.515838037497566, 0.515838037497566, 0.515838037497566, 0.5194366341796561, 0.5194366341796561, 0.5194366341796561, 0.5144450677015125, 0.5144450677015125, 0.5144450677015125, 0.16229942767588723, 0.16229942767588723, 0.16229942767588723, 0.20270873270629186, 0.20270873270629186, 0.20270873270629186, 0.178556481064282, 0.178556481064282, 0.178556481064282, 0.12822588616493447, 0.12822588616493447, 0.12822588616493447, 0.11268231017053654, 0.11268231017053654, 0.11268231017053654, 0.15123445119054157, 0.15123445119054157, 0.15123445119054157, 0.10492488703566383, 0.10492488703566383, 0.10492488703566383, 0.10780263409163038, 0.10780263409163038, 0.10780263409163038, 0.11112448491308202, 0.11112448491308202, 0.11112448491308202, 0.8653459057540779, 0.8653459057540779, 0.8653459057540779, 0.9908917400231935, 0.9908917400231935, 0.9908917400231935, 0.9757033347569765, 0.9757033347569765, 0.9757033347569765, 0.3054291963379251, 0.3054291963379251, 0.3054291963379251, 0.2815826945549963, 0.2815826945549963, 0.2815826945549963, 0.24677541065955932, 0.24677541065955932, 0.24677541065955932, 0.2237469460430841, 0.2237469460430841, 0.2237469460430841, 0.5795716912670807, 0.5795716912670807, 0.5795716912670807, 0.2998389884197912, 0.2998389884197912, 0.2998389884197912, 0.1620045531706633, 0.1620045531706633, 0.1620045531706633, 0.18119415785529058, 0.18119415785529058, 0.18119415785529058, 0.15455987210447797, 0.15455987210447797, 0.15455987210447797, 0.11125355686262683, 0.11125355686262683, 0.11125355686262683, 0.15432052326184476, 0.15432052326184476, 0.15432052326184476, 0.14137968815389756, 0.14137968815389756, 0.14137968815389756, 0.053237925529003416, 0.053237925529003416, 0.053237925529003416, 0.04573299992348745, 0.04573299992348745, 0.04573299992348745, 0.04003332545663685, 0.04003332545663685, 0.04003332545663685, 0.13713257741230622, 0.13713257741230622, 0.13713257741230622, 0.10001761595210423, 0.10001761595210423, 0.10001761595210423, 0.0987330326788931, 0.0987330326788931, 0.0987330326788931, 0.08301530156514147, 0.08301530156514147, 0.08301530156514147, 0.07857211407458708, 0.07857211407458708, 0.07857211407458708, 0.08461846940606843, 0.08461846940606843, 0.08461846940606843, 0.07780350698006389, 0.07780350698006389, 0.07780350698006389, 0.09853425695450169, 0.09853425695450169, 0.09853425695450169, 0.1316263636873638, 0.1316263636873638, 0.1316263636873638, 0.4200023443735511, 0.4200023443735511, 0.4200023443735511, 0.39873613341764236, 0.39873613341764236, 0.39873613341764236, 0.3992175621491353, 0.3992175621491353, 0.3992175621491353, 0.11312891844776662, 0.11312891844776662, 0.11312891844776662, 0.130948461530772, 0.130948461530772, 0.130948461530772, 0.12706630811522068, 0.12706630811522068, 0.12706630811522068, 0.14418300352105484, 0.14418300352105484, 0.14418300352105484, 0.17602883773211542, 0.17602883773211542, 0.17602883773211542, 0.21400361169932913, 0.21400361169932913, 0.21400361169932913, 0.29348086429114095, 0.29348086429114095, 0.29348086429114095, 0.29319156574914385, 0.29319156574914385, 0.29319156574914385, 0.30550384770407535, 0.30550384770407535, 0.30550384770407535, 0.20567640164229772, 0.20567640164229772, 0.20567640164229772, 0.21581126065286338, 0.21581126065286338, 0.21581126065286338, 0.21420249443653772, 0.21420249443653772, 0.21420249443653772, 0.2059548298526973, 0.2059548298526973, 0.2059548298526973, 0.19954674868751088, 0.19954674868751088, 0.19954674868751088, 0.2251506803310015, 0.2251506803310015, 0.2251506803310015, 0.29025612968060566, 0.29025612968060566, 0.29025612968060566, 0.1726278100701284, 0.1726278100701284, 0.1726278100701284, 0.1941856468682196, 0.1941856468682196, 0.1941856468682196, 0.1537666685415957, 0.1537666685415957, 0.1537666685415957, 0.22675420385600453, 0.22675420385600453, 0.22675420385600453, 0.1697211190559923, 0.1697211190559923, 0.1697211190559923, 0.16860585174810294, 0.16860585174810294, 0.16860585174810294, 0.15982334738363024, 0.15982334738363024, 0.15982334738363024, 0.1998308755071726, 0.1998308755071726, 0.1998308755071726, 0.18116614808192177, 0.18116614808192177, 0.18116614808192177, 0.19962551703774634, 0.19962551703774634, 0.19962551703774634, 0.1775550099237957, 0.1775550099237957, 0.1775550099237957, 0.07800880755493511, 0.07800880755493511, 0.07800880755493511, 0.0947748820858011, 0.0947748820858011, 0.0947748820858011, 0.08807890311573152, 0.08807890311573152, 0.08807890311573152]}, "mutation_prompt": null}
{"id": "08cbd95f-f6fc-4c89-8a70-ad228c827fc6", "solution": "import numpy as np\n\nclass HybridDESA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.pop_size = 10 * dim\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.temp = 1000.0  # Initial temperature for simulated annealing\n        self.cooling_rate = 0.99  # Cooling rate for simulated annealing\n\n    def __call__(self, func):\n        np.random.seed(42)\n        population = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        num_evaluations = self.pop_size\n\n        def mutate(target_idx):\n            indices = [idx for idx in range(self.pop_size) if idx != target_idx]\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            dynamic_F = self.F * (1.0 - (num_evaluations / self.budget))  # Dynamic mutation scaling\n            mutant = population[a] + dynamic_F * (population[b] - population[c])\n            mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n            return mutant\n\n        while num_evaluations < self.budget:\n            self.CR = 0.9 - (0.5 * (num_evaluations / self.budget))  # Dynamic adjustment with enhanced exploration\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                mutant = mutate(i)\n                trial = np.where(np.random.rand(self.dim) < self.CR, mutant, population[i])\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = trial\n\n            # Simulated Annealing part\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                neighbor = population[i] + np.random.normal(0, 0.1 * self.temp / 1000, self.dim)\n                neighbor = np.clip(neighbor, self.lower_bound, self.upper_bound)\n                neighbor_fitness = func(neighbor)\n                num_evaluations += 1\n                delta_fit = neighbor_fitness - fitness[i]\n\n                if delta_fit < 0 or np.random.rand() < np.exp(-delta_fit / self.temp):\n                    population[i] = neighbor\n                    fitness[i] = neighbor_fitness\n                    if neighbor_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = neighbor\n\n            self.temp *= (self.cooling_rate - 0.01 * (num_evaluations / self.budget))  # Adjusted cooling rate\n            self.pop_size = max(4, int(8 * self.dim * (1 - num_evaluations / self.budget)))  # Adaptive population size\n\n        return best_solution", "name": "HybridDESA", "description": "Introduce a dynamic crossover probability adjustment to enhance exploration-exploitation balance.", "configspace": "", "generation": 35, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "a23abc51-cac8-4819-b530-c9b5e6c87530", "metadata": {"aucs": [0.4933307569063823, 0.4933307569063823, 0.4933307569063823, 0.486302594801032, 0.486302594801032, 0.486302594801032, 0.4964680403102998, 0.4964680403102998, 0.4964680403102998, 0.14340492288608964, 0.14340492288608964, 0.14340492288608964, 0.1422724251857611, 0.1422724251857611, 0.1422724251857611, 0.16165522246329345, 0.16165522246329345, 0.16165522246329345, 0.12393775082031844, 0.12393775082031844, 0.12393775082031844, 0.12765933102801597, 0.12765933102801597, 0.12765933102801597, 0.20375654338917404, 0.20375654338917404, 0.20375654338917404, 0.0920841054098861, 0.0920841054098861, 0.0920841054098861, 0.10022505172180085, 0.10022505172180085, 0.10022505172180085, 0.10097089880597687, 0.10097089880597687, 0.10097089880597687, 0.7992041166385038, 0.7992041166385038, 0.7992041166385038, 0.7579844574770246, 0.7579844574770246, 0.7579844574770246, 0.7737581596887949, 0.7737581596887949, 0.7737581596887949, 0.28248354486540084, 0.28248354486540084, 0.28248354486540084, 0.282674282520579, 0.282674282520579, 0.282674282520579, 0.28484282590696064, 0.28484282590696064, 0.28484282590696064, 0.2477350768058978, 0.2477350768058978, 0.2477350768058978, 0.237134617525955, 0.237134617525955, 0.237134617525955, 0.5451365546244145, 0.5451365546244145, 0.5451365546244145, 0.12783816734412123, 0.12783816734412123, 0.12783816734412123, 0.12574152059045207, 0.12574152059045207, 0.12574152059045207, 0.12632135467586758, 0.12632135467586758, 0.12632135467586758, 0.13578031704396287, 0.13578031704396287, 0.13578031704396287, 0.12201859771190504, 0.12201859771190504, 0.12201859771190504, 0.13421444142737615, 0.13421444142737615, 0.13421444142737615, 0.08856711966563091, 0.08856711966563091, 0.08856711966563091, 0.056388941119007385, 0.056388941119007385, 0.056388941119007385, 0.04570311717095721, 0.04570311717095721, 0.04570311717095721, 0.15850012106421596, 0.15850012106421596, 0.15850012106421596, 0.10033817853138516, 0.10033817853138516, 0.10033817853138516, 0.11273852385815974, 0.11273852385815974, 0.11273852385815974, 0.048617488822171095, 0.048617488822171095, 0.048617488822171095, 0.03716194485287394, 0.03716194485287394, 0.03716194485287394, 0.051200970563792136, 0.051200970563792136, 0.051200970563792136, 0.12763542759695323, 0.12763542759695323, 0.12763542759695323, 0.11139396244910182, 0.11139396244910182, 0.11139396244910182, 0.10285323609250896, 0.10285323609250896, 0.10285323609250896, 0.4156220858285443, 0.4156220858285443, 0.4156220858285443, 0.41272255275136727, 0.41272255275136727, 0.41272255275136727, 0.42282518010998715, 0.42282518010998715, 0.42282518010998715, 0.10704494424892741, 0.10704494424892741, 0.10704494424892741, 0.12610176361361303, 0.12610176361361303, 0.12610176361361303, 0.12766715918892935, 0.12766715918892935, 0.12766715918892935, 0.17320406832110713, 0.17320406832110713, 0.17320406832110713, 0.15315830157665133, 0.15315830157665133, 0.15315830157665133, 0.21136080206692454, 0.21136080206692454, 0.21136080206692454, 0.3028030661717035, 0.3028030661717035, 0.3028030661717035, 0.3040970811034772, 0.3040970811034772, 0.3040970811034772, 0.31464764311072513, 0.31464764311072513, 0.31464764311072513, 0.24680674733440022, 0.24680674733440022, 0.24680674733440022, 0.19555008241045257, 0.19555008241045257, 0.19555008241045257, 0.23291985879123478, 0.23291985879123478, 0.23291985879123478, 0.21735514494443253, 0.21735514494443253, 0.21735514494443253, 0.1878427171211784, 0.1878427171211784, 0.1878427171211784, 0.20481832023262403, 0.20481832023262403, 0.20481832023262403, 0.31686050696082024, 0.31686050696082024, 0.31686050696082024, 0.1857281694049977, 0.1857281694049977, 0.1857281694049977, 0.20722338285808206, 0.20722338285808206, 0.20722338285808206, 0.5737733000582486, 0.5737733000582486, 0.5737733000582486, 0.24658391305718264, 0.24658391305718264, 0.24658391305718264, 0.2203240111667989, 0.2203240111667989, 0.2203240111667989, 0.19435002598241335, 0.19435002598241335, 0.19435002598241335, 0.2593360928281354, 0.2593360928281354, 0.2593360928281354, 0.37939833507252774, 0.37939833507252774, 0.37939833507252774, 0.2031622182143844, 0.2031622182143844, 0.2031622182143844, 0.1864831910694391, 0.1864831910694391, 0.1864831910694391, 0.1936926537367577, 0.1936926537367577, 0.1936926537367577, 0.08529388696491058, 0.08529388696491058, 0.08529388696491058, 0.0837976732411414, 0.0837976732411414, 0.0837976732411414, 0.08294328777388438, 0.08294328777388438, 0.08294328777388438]}, "mutation_prompt": null}
{"id": "27d66c69-b064-4c3e-ad02-22565e257111", "solution": "import numpy as np\n\nclass HybridDESA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.pop_size = 10 * dim\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.temp = 1000.0  # Initial temperature for simulated annealing\n        self.cooling_rate = 0.99  # Cooling rate for simulated annealing\n\n    def __call__(self, func):\n        np.random.seed(42)\n        population = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        num_evaluations = self.pop_size\n\n        def mutate(target_idx):\n            indices = [idx for idx in range(self.pop_size) if idx != target_idx]\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            dynamic_F = self.F * (1.0 - (num_evaluations / self.budget))  # Dynamic mutation scaling\n            mutant = population[a] + dynamic_F * (population[b] - population[c])\n            mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n            return mutant\n\n        while num_evaluations < self.budget:\n            self.CR = 0.9 - (0.5 * (num_evaluations / self.budget))  # Dynamic adjustment with enhanced exploration\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                mutant = mutate(i)\n                trial = np.where(np.random.rand(self.dim) < self.CR, mutant, population[i])\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = trial\n\n            # Simulated Annealing part\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                neighbor = population[i] + np.random.normal(0, 0.1 * self.temp / 1000, self.dim)\n                neighbor = np.clip(neighbor, self.lower_bound, self.upper_bound)\n                neighbor_fitness = func(neighbor)\n                num_evaluations += 1\n                delta_fit = neighbor_fitness - fitness[i]\n\n                if delta_fit < 0 or np.random.rand() < np.exp(-delta_fit / self.temp):\n                    population[i] = neighbor\n                    fitness[i] = neighbor_fitness\n                    if neighbor_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = neighbor\n\n            self.temp *= (self.cooling_rate - 0.01 * (num_evaluations / self.budget))  # Adjusted cooling rate\n            self.pop_size = max(4, int(8 * self.dim * (1 - num_evaluations / self.budget)))  # Adaptive population size\n\n        return best_solution", "name": "HybridDESA", "description": "Introduce a dynamic crossover probability adjustment to enhance exploration-exploitation balance.", "configspace": "", "generation": 35, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "a23abc51-cac8-4819-b530-c9b5e6c87530", "metadata": {"aucs": [0.4933307569063823, 0.4933307569063823, 0.4933307569063823, 0.486302594801032, 0.486302594801032, 0.486302594801032, 0.4964680403102998, 0.4964680403102998, 0.4964680403102998, 0.14340492288608964, 0.14340492288608964, 0.14340492288608964, 0.1422724251857611, 0.1422724251857611, 0.1422724251857611, 0.16165522246329345, 0.16165522246329345, 0.16165522246329345, 0.12393775082031844, 0.12393775082031844, 0.12393775082031844, 0.12765933102801597, 0.12765933102801597, 0.12765933102801597, 0.20375654338917404, 0.20375654338917404, 0.20375654338917404, 0.0920841054098861, 0.0920841054098861, 0.0920841054098861, 0.10022505172180085, 0.10022505172180085, 0.10022505172180085, 0.10097089880597687, 0.10097089880597687, 0.10097089880597687, 0.7992041166385038, 0.7992041166385038, 0.7992041166385038, 0.7579844574770246, 0.7579844574770246, 0.7579844574770246, 0.7737581596887949, 0.7737581596887949, 0.7737581596887949, 0.28248354486540084, 0.28248354486540084, 0.28248354486540084, 0.282674282520579, 0.282674282520579, 0.282674282520579, 0.28484282590696064, 0.28484282590696064, 0.28484282590696064, 0.2477350768058978, 0.2477350768058978, 0.2477350768058978, 0.237134617525955, 0.237134617525955, 0.237134617525955, 0.5451365546244145, 0.5451365546244145, 0.5451365546244145, 0.12783816734412123, 0.12783816734412123, 0.12783816734412123, 0.12574152059045207, 0.12574152059045207, 0.12574152059045207, 0.12632135467586758, 0.12632135467586758, 0.12632135467586758, 0.13578031704396287, 0.13578031704396287, 0.13578031704396287, 0.12201859771190504, 0.12201859771190504, 0.12201859771190504, 0.13421444142737615, 0.13421444142737615, 0.13421444142737615, 0.08856711966563091, 0.08856711966563091, 0.08856711966563091, 0.056388941119007385, 0.056388941119007385, 0.056388941119007385, 0.04570311717095721, 0.04570311717095721, 0.04570311717095721, 0.15850012106421596, 0.15850012106421596, 0.15850012106421596, 0.10033817853138516, 0.10033817853138516, 0.10033817853138516, 0.11273852385815974, 0.11273852385815974, 0.11273852385815974, 0.048617488822171095, 0.048617488822171095, 0.048617488822171095, 0.03716194485287394, 0.03716194485287394, 0.03716194485287394, 0.051200970563792136, 0.051200970563792136, 0.051200970563792136, 0.12763542759695323, 0.12763542759695323, 0.12763542759695323, 0.11139396244910182, 0.11139396244910182, 0.11139396244910182, 0.10285323609250896, 0.10285323609250896, 0.10285323609250896, 0.4156220858285443, 0.4156220858285443, 0.4156220858285443, 0.41272255275136727, 0.41272255275136727, 0.41272255275136727, 0.42282518010998715, 0.42282518010998715, 0.42282518010998715, 0.10704494424892741, 0.10704494424892741, 0.10704494424892741, 0.12610176361361303, 0.12610176361361303, 0.12610176361361303, 0.12766715918892935, 0.12766715918892935, 0.12766715918892935, 0.17320406832110713, 0.17320406832110713, 0.17320406832110713, 0.15315830157665133, 0.15315830157665133, 0.15315830157665133, 0.21136080206692454, 0.21136080206692454, 0.21136080206692454, 0.3028030661717035, 0.3028030661717035, 0.3028030661717035, 0.3040970811034772, 0.3040970811034772, 0.3040970811034772, 0.31464764311072513, 0.31464764311072513, 0.31464764311072513, 0.24680674733440022, 0.24680674733440022, 0.24680674733440022, 0.19555008241045257, 0.19555008241045257, 0.19555008241045257, 0.23291985879123478, 0.23291985879123478, 0.23291985879123478, 0.21735514494443253, 0.21735514494443253, 0.21735514494443253, 0.1878427171211784, 0.1878427171211784, 0.1878427171211784, 0.20481832023262403, 0.20481832023262403, 0.20481832023262403, 0.31686050696082024, 0.31686050696082024, 0.31686050696082024, 0.1857281694049977, 0.1857281694049977, 0.1857281694049977, 0.20722338285808206, 0.20722338285808206, 0.20722338285808206, 0.5737733000582486, 0.5737733000582486, 0.5737733000582486, 0.24658391305718264, 0.24658391305718264, 0.24658391305718264, 0.2203240111667989, 0.2203240111667989, 0.2203240111667989, 0.19435002598241335, 0.19435002598241335, 0.19435002598241335, 0.2593360928281354, 0.2593360928281354, 0.2593360928281354, 0.37939833507252774, 0.37939833507252774, 0.37939833507252774, 0.2031622182143844, 0.2031622182143844, 0.2031622182143844, 0.1864831910694391, 0.1864831910694391, 0.1864831910694391, 0.1936926537367577, 0.1936926537367577, 0.1936926537367577, 0.08529388696491058, 0.08529388696491058, 0.08529388696491058, 0.0837976732411414, 0.0837976732411414, 0.0837976732411414, 0.08294328777388438, 0.08294328777388438, 0.08294328777388438]}, "mutation_prompt": null}
{"id": "36b7c46f-6a02-4f40-b5e7-a665c8586cce", "solution": "import numpy as np\n\nclass HybridDESA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.pop_size = 10 * dim\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.temp = 1000.0  # Initial temperature for simulated annealing\n        self.cooling_rate = 0.99  # Cooling rate for simulated annealing\n\n    def __call__(self, func):\n        np.random.seed(42)\n        population = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        num_evaluations = self.pop_size\n\n        def mutate(target_idx):\n            indices = [idx for idx in range(self.pop_size) if idx != target_idx]\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            dynamic_F = self.F * (1.0 - (num_evaluations / self.budget))  # Dynamic mutation scaling\n            mutant = population[a] + dynamic_F * (population[b] - population[c])\n            mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n            return mutant\n\n        while num_evaluations < self.budget:\n            self.CR = 0.9 - (0.5 * (num_evaluations / self.budget))  # Dynamic adjustment with enhanced exploration\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                mutant = mutate(i)\n                trial = np.where(np.random.rand(self.dim) < self.CR, mutant, population[i])\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = trial\n\n            # Simulated Annealing part\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                neighbor = population[i] + np.random.normal(0, 0.1 * self.temp / 1000, self.dim)\n                neighbor = np.clip(neighbor, self.lower_bound, self.upper_bound)\n                neighbor_fitness = func(neighbor)\n                num_evaluations += 1\n                delta_fit = neighbor_fitness - fitness[i]\n\n                if delta_fit < 0 or np.random.rand() < np.exp(-delta_fit / self.temp):\n                    population[i] = neighbor\n                    fitness[i] = neighbor_fitness\n                    if neighbor_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = neighbor\n\n            self.temp *= (self.cooling_rate - 0.01 * (num_evaluations / self.budget))  # Adjusted cooling rate\n            self.pop_size = max(4, int(8 * self.dim * (1 - num_evaluations / self.budget)))  # Adaptive population size\n\n        return best_solution", "name": "HybridDESA", "description": "Introduce a dynamic crossover probability adjustment to enhance exploration-exploitation balance.", "configspace": "", "generation": 35, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "a23abc51-cac8-4819-b530-c9b5e6c87530", "metadata": {"aucs": [0.4933307569063823, 0.4933307569063823, 0.4933307569063823, 0.486302594801032, 0.486302594801032, 0.486302594801032, 0.4964680403102998, 0.4964680403102998, 0.4964680403102998, 0.14340492288608964, 0.14340492288608964, 0.14340492288608964, 0.1422724251857611, 0.1422724251857611, 0.1422724251857611, 0.16165522246329345, 0.16165522246329345, 0.16165522246329345, 0.12393775082031844, 0.12393775082031844, 0.12393775082031844, 0.12765933102801597, 0.12765933102801597, 0.12765933102801597, 0.20375654338917404, 0.20375654338917404, 0.20375654338917404, 0.0920841054098861, 0.0920841054098861, 0.0920841054098861, 0.10022505172180085, 0.10022505172180085, 0.10022505172180085, 0.10097089880597687, 0.10097089880597687, 0.10097089880597687, 0.7992041166385038, 0.7992041166385038, 0.7992041166385038, 0.7579844574770246, 0.7579844574770246, 0.7579844574770246, 0.7737581596887949, 0.7737581596887949, 0.7737581596887949, 0.28248354486540084, 0.28248354486540084, 0.28248354486540084, 0.282674282520579, 0.282674282520579, 0.282674282520579, 0.28484282590696064, 0.28484282590696064, 0.28484282590696064, 0.2477350768058978, 0.2477350768058978, 0.2477350768058978, 0.237134617525955, 0.237134617525955, 0.237134617525955, 0.5451365546244145, 0.5451365546244145, 0.5451365546244145, 0.12783816734412123, 0.12783816734412123, 0.12783816734412123, 0.12574152059045207, 0.12574152059045207, 0.12574152059045207, 0.12632135467586758, 0.12632135467586758, 0.12632135467586758, 0.13578031704396287, 0.13578031704396287, 0.13578031704396287, 0.12201859771190504, 0.12201859771190504, 0.12201859771190504, 0.13421444142737615, 0.13421444142737615, 0.13421444142737615, 0.08856711966563091, 0.08856711966563091, 0.08856711966563091, 0.056388941119007385, 0.056388941119007385, 0.056388941119007385, 0.04570311717095721, 0.04570311717095721, 0.04570311717095721, 0.15850012106421596, 0.15850012106421596, 0.15850012106421596, 0.10033817853138516, 0.10033817853138516, 0.10033817853138516, 0.11273852385815974, 0.11273852385815974, 0.11273852385815974, 0.048617488822171095, 0.048617488822171095, 0.048617488822171095, 0.03716194485287394, 0.03716194485287394, 0.03716194485287394, 0.051200970563792136, 0.051200970563792136, 0.051200970563792136, 0.12763542759695323, 0.12763542759695323, 0.12763542759695323, 0.11139396244910182, 0.11139396244910182, 0.11139396244910182, 0.10285323609250896, 0.10285323609250896, 0.10285323609250896, 0.4156220858285443, 0.4156220858285443, 0.4156220858285443, 0.41272255275136727, 0.41272255275136727, 0.41272255275136727, 0.42282518010998715, 0.42282518010998715, 0.42282518010998715, 0.10704494424892741, 0.10704494424892741, 0.10704494424892741, 0.12610176361361303, 0.12610176361361303, 0.12610176361361303, 0.12766715918892935, 0.12766715918892935, 0.12766715918892935, 0.17320406832110713, 0.17320406832110713, 0.17320406832110713, 0.15315830157665133, 0.15315830157665133, 0.15315830157665133, 0.21136080206692454, 0.21136080206692454, 0.21136080206692454, 0.3028030661717035, 0.3028030661717035, 0.3028030661717035, 0.3040970811034772, 0.3040970811034772, 0.3040970811034772, 0.31464764311072513, 0.31464764311072513, 0.31464764311072513, 0.24680674733440022, 0.24680674733440022, 0.24680674733440022, 0.19555008241045257, 0.19555008241045257, 0.19555008241045257, 0.23291985879123478, 0.23291985879123478, 0.23291985879123478, 0.21735514494443253, 0.21735514494443253, 0.21735514494443253, 0.1878427171211784, 0.1878427171211784, 0.1878427171211784, 0.20481832023262403, 0.20481832023262403, 0.20481832023262403, 0.31686050696082024, 0.31686050696082024, 0.31686050696082024, 0.1857281694049977, 0.1857281694049977, 0.1857281694049977, 0.20722338285808206, 0.20722338285808206, 0.20722338285808206, 0.5737733000582486, 0.5737733000582486, 0.5737733000582486, 0.24658391305718264, 0.24658391305718264, 0.24658391305718264, 0.2203240111667989, 0.2203240111667989, 0.2203240111667989, 0.19435002598241335, 0.19435002598241335, 0.19435002598241335, 0.2593360928281354, 0.2593360928281354, 0.2593360928281354, 0.37939833507252774, 0.37939833507252774, 0.37939833507252774, 0.2031622182143844, 0.2031622182143844, 0.2031622182143844, 0.1864831910694391, 0.1864831910694391, 0.1864831910694391, 0.1936926537367577, 0.1936926537367577, 0.1936926537367577, 0.08529388696491058, 0.08529388696491058, 0.08529388696491058, 0.0837976732411414, 0.0837976732411414, 0.0837976732411414, 0.08294328777388438, 0.08294328777388438, 0.08294328777388438]}, "mutation_prompt": null}
{"id": "0b253849-9fd2-4ea7-9ca7-f174b783e51c", "solution": "import numpy as np\n\nclass HybridDESA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.pop_size = 10 * dim\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.temp = 1000.0  # Initial temperature for simulated annealing\n        self.cooling_rate = 0.99  # Cooling rate for simulated annealing\n\n    def __call__(self, func):\n        np.random.seed(42)\n        population = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        num_evaluations = self.pop_size\n\n        def mutate(target_idx):\n            indices = [idx for idx in range(self.pop_size) if idx != target_idx]\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            dynamic_F = self.F * (1.0 - (num_evaluations / self.budget))  # Dynamic mutation scaling\n            mutant = population[a] + dynamic_F * (population[b] - population[c])\n            mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n            return mutant\n\n        while num_evaluations < self.budget:\n            self.CR = 0.9 - (0.5 * (num_evaluations / self.budget))  # Dynamic adjustment with enhanced exploration\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                mutant = mutate(i)\n                trial = np.where(np.random.rand(self.dim) < self.CR, mutant, population[i])\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = trial\n\n            # Simulated Annealing part\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                neighbor = population[i] + np.random.normal(0, 0.1 * self.temp / 1000, self.dim)\n                neighbor = np.clip(neighbor, self.lower_bound, self.upper_bound)\n                neighbor_fitness = func(neighbor)\n                num_evaluations += 1\n                delta_fit = neighbor_fitness - fitness[i]\n\n                if delta_fit < 0 or np.random.rand() < np.exp(-delta_fit / self.temp):\n                    population[i] = neighbor\n                    fitness[i] = neighbor_fitness\n                    if neighbor_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = neighbor\n\n            self.temp *= (self.cooling_rate - 0.01 * (num_evaluations / self.budget))  # Adjusted cooling rate\n            self.pop_size = max(4, int(8 * self.dim * (1 - num_evaluations / self.budget)))  # Adaptive population size\n\n        return best_solution", "name": "HybridDESA", "description": "Introduce a dynamic crossover probability adjustment to enhance exploration-exploitation balance.", "configspace": "", "generation": 35, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "a23abc51-cac8-4819-b530-c9b5e6c87530", "metadata": {"aucs": [0.4933307569063823, 0.4933307569063823, 0.4933307569063823, 0.486302594801032, 0.486302594801032, 0.486302594801032, 0.4964680403102998, 0.4964680403102998, 0.4964680403102998, 0.14340492288608964, 0.14340492288608964, 0.14340492288608964, 0.1422724251857611, 0.1422724251857611, 0.1422724251857611, 0.16165522246329345, 0.16165522246329345, 0.16165522246329345, 0.12393775082031844, 0.12393775082031844, 0.12393775082031844, 0.12765933102801597, 0.12765933102801597, 0.12765933102801597, 0.20375654338917404, 0.20375654338917404, 0.20375654338917404, 0.0920841054098861, 0.0920841054098861, 0.0920841054098861, 0.10022505172180085, 0.10022505172180085, 0.10022505172180085, 0.10097089880597687, 0.10097089880597687, 0.10097089880597687, 0.7992041166385038, 0.7992041166385038, 0.7992041166385038, 0.7579844574770246, 0.7579844574770246, 0.7579844574770246, 0.7737581596887949, 0.7737581596887949, 0.7737581596887949, 0.28248354486540084, 0.28248354486540084, 0.28248354486540084, 0.282674282520579, 0.282674282520579, 0.282674282520579, 0.28484282590696064, 0.28484282590696064, 0.28484282590696064, 0.2477350768058978, 0.2477350768058978, 0.2477350768058978, 0.237134617525955, 0.237134617525955, 0.237134617525955, 0.5451365546244145, 0.5451365546244145, 0.5451365546244145, 0.12783816734412123, 0.12783816734412123, 0.12783816734412123, 0.12574152059045207, 0.12574152059045207, 0.12574152059045207, 0.12632135467586758, 0.12632135467586758, 0.12632135467586758, 0.13578031704396287, 0.13578031704396287, 0.13578031704396287, 0.12201859771190504, 0.12201859771190504, 0.12201859771190504, 0.13421444142737615, 0.13421444142737615, 0.13421444142737615, 0.08856711966563091, 0.08856711966563091, 0.08856711966563091, 0.056388941119007385, 0.056388941119007385, 0.056388941119007385, 0.04570311717095721, 0.04570311717095721, 0.04570311717095721, 0.15850012106421596, 0.15850012106421596, 0.15850012106421596, 0.10033817853138516, 0.10033817853138516, 0.10033817853138516, 0.11273852385815974, 0.11273852385815974, 0.11273852385815974, 0.048617488822171095, 0.048617488822171095, 0.048617488822171095, 0.03716194485287394, 0.03716194485287394, 0.03716194485287394, 0.051200970563792136, 0.051200970563792136, 0.051200970563792136, 0.12763542759695323, 0.12763542759695323, 0.12763542759695323, 0.11139396244910182, 0.11139396244910182, 0.11139396244910182, 0.10285323609250896, 0.10285323609250896, 0.10285323609250896, 0.4156220858285443, 0.4156220858285443, 0.4156220858285443, 0.41272255275136727, 0.41272255275136727, 0.41272255275136727, 0.42282518010998715, 0.42282518010998715, 0.42282518010998715, 0.10704494424892741, 0.10704494424892741, 0.10704494424892741, 0.12610176361361303, 0.12610176361361303, 0.12610176361361303, 0.12766715918892935, 0.12766715918892935, 0.12766715918892935, 0.17320406832110713, 0.17320406832110713, 0.17320406832110713, 0.15315830157665133, 0.15315830157665133, 0.15315830157665133, 0.21136080206692454, 0.21136080206692454, 0.21136080206692454, 0.3028030661717035, 0.3028030661717035, 0.3028030661717035, 0.3040970811034772, 0.3040970811034772, 0.3040970811034772, 0.31464764311072513, 0.31464764311072513, 0.31464764311072513, 0.24680674733440022, 0.24680674733440022, 0.24680674733440022, 0.19555008241045257, 0.19555008241045257, 0.19555008241045257, 0.23291985879123478, 0.23291985879123478, 0.23291985879123478, 0.21735514494443253, 0.21735514494443253, 0.21735514494443253, 0.1878427171211784, 0.1878427171211784, 0.1878427171211784, 0.20481832023262403, 0.20481832023262403, 0.20481832023262403, 0.31686050696082024, 0.31686050696082024, 0.31686050696082024, 0.1857281694049977, 0.1857281694049977, 0.1857281694049977, 0.20722338285808206, 0.20722338285808206, 0.20722338285808206, 0.5737733000582486, 0.5737733000582486, 0.5737733000582486, 0.24658391305718264, 0.24658391305718264, 0.24658391305718264, 0.2203240111667989, 0.2203240111667989, 0.2203240111667989, 0.19435002598241335, 0.19435002598241335, 0.19435002598241335, 0.2593360928281354, 0.2593360928281354, 0.2593360928281354, 0.37939833507252774, 0.37939833507252774, 0.37939833507252774, 0.2031622182143844, 0.2031622182143844, 0.2031622182143844, 0.1864831910694391, 0.1864831910694391, 0.1864831910694391, 0.1936926537367577, 0.1936926537367577, 0.1936926537367577, 0.08529388696491058, 0.08529388696491058, 0.08529388696491058, 0.0837976732411414, 0.0837976732411414, 0.0837976732411414, 0.08294328777388438, 0.08294328777388438, 0.08294328777388438]}, "mutation_prompt": null}
{"id": "9f5a3f14-2246-47b1-8696-5ac1b0f148cf", "solution": "import numpy as np\n\nclass HybridDESA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.pop_size = 10 * dim\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.temp = 1000.0  # Initial temperature for simulated annealing\n        self.cooling_rate = 0.99  # Cooling rate for simulated annealing\n\n    def __call__(self, func):\n        np.random.seed(42)\n        population = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        num_evaluations = self.pop_size\n\n        def mutate(target_idx):\n            indices = [idx for idx in range(self.pop_size) if idx != target_idx]\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            dynamic_F = self.F * (1.0 - (num_evaluations / self.budget)) * (0.5 + fitness[best_idx] / np.mean(fitness))  # Adaptive adjustment\n            mutant = population[a] + dynamic_F * (population[b] - population[c])\n            mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n            return mutant\n\n        while num_evaluations < self.budget:\n            self.CR = 0.9 - (0.5 * (num_evaluations / self.budget))  # Dynamic adjustment with enhanced exploration\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                mutant = mutate(i)\n                trial = np.where(np.random.rand(self.dim) < self.CR, mutant, population[i])\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = trial\n\n            # Simulated Annealing part\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                neighbor = population[i] + np.random.normal(0, 0.1 * self.temp / 1000, self.dim)\n                neighbor = np.clip(neighbor, self.lower_bound, self.upper_bound)\n                neighbor_fitness = func(neighbor)\n                num_evaluations += 1\n                delta_fit = neighbor_fitness - fitness[i]\n\n                if delta_fit < 0 or np.random.rand() < np.exp(-delta_fit / self.temp):\n                    population[i] = neighbor\n                    fitness[i] = neighbor_fitness\n                    if neighbor_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = neighbor\n\n            self.temp *= (self.cooling_rate - 0.01 * (num_evaluations / self.budget))  # Adjusted cooling rate\n            self.pop_size = max(4, int(8 * self.dim * (1 - num_evaluations / self.budget)))  # Adaptive population size\n\n        return best_solution", "name": "HybridDESA", "description": "Introduce an adaptive differential weight adjustment to improve convergence speed.", "configspace": "", "generation": 42, "fitness": 0.22803150778007494, "feedback": "The algorithm HybridDESA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.23 with standard deviation 0.18.", "error": "", "parent_id": "a23abc51-cac8-4819-b530-c9b5e6c87530", "metadata": {"aucs": [0.4639080293525373, 0.4639080293525373, 0.4639080293525373, 0.4599712405385553, 0.4599712405385553, 0.4599712405385553, 0.4554264179291394, 0.4554264179291394, 0.4554264179291394, 0.13326940526341569, 0.13326940526341569, 0.13326940526341569, 0.209309575796515, 0.209309575796515, 0.209309575796515, 0.1894127677014018, 0.1894127677014018, 0.1894127677014018, 0.1009939686588992, 0.1009939686588992, 0.1009939686588992, 0.12582617746600866, 0.12582617746600866, 0.12582617746600866, 0.12501043234951525, 0.12501043234951525, 0.12501043234951525, 0.05454004123190648, 0.05454004123190648, 0.05454004123190648, 0.11405746160485575, 0.11405746160485575, 0.11405746160485575, 0.0949799642307475, 0.0949799642307475, 0.0949799642307475, 0.8698389185080023, 0.8698389185080023, 0.8698389185080023, 0.9397901921668528, 0.9397901921668528, 0.9397901921668528, 0.7902658563834063, 0.7902658563834063, 0.7902658563834063, 0.31537288213926806, 0.31537288213926806, 0.31537288213926806, 0.31968181559804776, 0.31968181559804776, 0.31968181559804776, 0.30274997295033734, 0.30274997295033734, 0.30274997295033734, 0.4628136200831824, 0.4628136200831824, 0.4628136200831824, 0.6365366455736963, 0.6365366455736963, 0.6365366455736963, 0.27186305032519376, 0.27186305032519376, 0.27186305032519376, 0.1540581289421138, 0.1540581289421138, 0.1540581289421138, 0.1489808804760825, 0.1489808804760825, 0.1489808804760825, 0.15135825846067186, 0.15135825846067186, 0.15135825846067186, 0.1471807212876065, 0.1471807212876065, 0.1471807212876065, 0.1508461502137869, 0.1508461502137869, 0.1508461502137869, 0.16136996434778794, 0.16136996434778794, 0.16136996434778794, 0.09656063511394863, 0.09656063511394863, 0.09656063511394863, 0.06895110512092817, 0.06895110512092817, 0.06895110512092817, 0.04578595987459799, 0.04578595987459799, 0.04578595987459799, 0.11529101390484953, 0.11529101390484953, 0.11529101390484953, 0.04097782402364114, 0.04097782402364114, 0.04097782402364114, 0.12668571202788426, 0.12668571202788426, 0.12668571202788426, 0.06519927666619585, 0.06519927666619585, 0.06519927666619585, 0.05602451458199609, 0.05602451458199609, 0.05602451458199609, 0.086791407011374, 0.086791407011374, 0.086791407011374, 0.13651964182733245, 0.13651964182733245, 0.13651964182733245, 0.14577803459828043, 0.14577803459828043, 0.14577803459828043, 0.10128915782643633, 0.10128915782643633, 0.10128915782643633, 0.3533941100184995, 0.3533941100184995, 0.3533941100184995, 0.3545980808073115, 0.3545980808073115, 0.3545980808073115, 0.3767470027444394, 0.3767470027444394, 0.3767470027444394, 0.09430001188038406, 0.09430001188038406, 0.09430001188038406, 0.10304074050291057, 0.10304074050291057, 0.10304074050291057, 0.12476678753039705, 0.12476678753039705, 0.12476678753039705, 0.16485752374593754, 0.16485752374593754, 0.16485752374593754, 0.1927514187076732, 0.1927514187076732, 0.1927514187076732, 0.16807501264946512, 0.16807501264946512, 0.16807501264946512, 0.15021220074734032, 0.15021220074734032, 0.15021220074734032, 0.29028389668384247, 0.29028389668384247, 0.29028389668384247, 0.2709329607465849, 0.2709329607465849, 0.2709329607465849, 0.23849415480066094, 0.23849415480066094, 0.23849415480066094, 0.23154379175896123, 0.23154379175896123, 0.23154379175896123, 0.2230151232512919, 0.2230151232512919, 0.2230151232512919, 0.1894191709668327, 0.1894191709668327, 0.1894191709668327, 0.19237111356941228, 0.19237111356941228, 0.19237111356941228, 0.19419093958366984, 0.19419093958366984, 0.19419093958366984, 0.22842824642300674, 0.22842824642300674, 0.22842824642300674, 0.1980919294786262, 0.1980919294786262, 0.1980919294786262, 0.2174654298204176, 0.2174654298204176, 0.2174654298204176, 0.6077478939026039, 0.6077478939026039, 0.6077478939026039, 0.20597270288790892, 0.20597270288790892, 0.20597270288790892, 0.24968806269488986, 0.24968806269488986, 0.24968806269488986, 0.1824661117651819, 0.1824661117651819, 0.1824661117651819, 0.18619980550373794, 0.18619980550373794, 0.18619980550373794, 0.1900540139030723, 0.1900540139030723, 0.1900540139030723, 0.19483601220396518, 0.19483601220396518, 0.19483601220396518, 0.18692749015158538, 0.18692749015158538, 0.18692749015158538, 0.181956762346014, 0.181956762346014, 0.181956762346014, 0.07458886169784273, 0.07458886169784273, 0.07458886169784273, 0.07967486056111084, 0.07967486056111084, 0.07967486056111084, 0.08590951397282098, 0.08590951397282098, 0.08590951397282098]}, "mutation_prompt": null}
{"id": "c27dd26c-6ae1-4f50-94a9-6a890777abbb", "solution": "import numpy as np\n\nclass HybridDESA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.pop_size = 10 * dim\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.temp = 1000.0  # Initial temperature for simulated annealing\n        self.cooling_rate = 0.99  # Cooling rate for simulated annealing\n\n    def __call__(self, func):\n        np.random.seed(42)\n        population = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        num_evaluations = self.pop_size\n\n        def mutate(target_idx):\n            indices = [idx for idx in range(self.pop_size) if idx != target_idx]\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            dynamic_F = self.F * (1.0 - (num_evaluations / self.budget)) * np.exp(-num_evaluations / self.budget)  # Decay factor added\n            mutant = population[a] + dynamic_F * (population[b] - population[c])\n            mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n            return mutant\n\n        while num_evaluations < self.budget:\n            self.CR = 0.9 - (0.5 * (num_evaluations / self.budget))  # Dynamic adjustment with enhanced exploration\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                mutant = mutate(i)\n                trial = np.where(np.random.rand(self.dim) < self.CR, mutant, population[i])\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = trial\n\n            # Simulated Annealing part\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                neighbor = population[i] + np.random.normal(0, 0.1 * self.temp / 1000, self.dim)\n                neighbor = np.clip(neighbor, self.lower_bound, self.upper_bound)\n                neighbor_fitness = func(neighbor)\n                num_evaluations += 1\n                delta_fit = neighbor_fitness - fitness[i]\n\n                if delta_fit < 0 or np.random.rand() < np.exp(-delta_fit / self.temp):\n                    population[i] = neighbor\n                    fitness[i] = neighbor_fitness\n                    if neighbor_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = neighbor\n\n            self.temp *= (self.cooling_rate - 0.01 * (num_evaluations / self.budget))  # Adjusted cooling rate\n            self.pop_size = max(4, int(8 * self.dim * (1 - num_evaluations / self.budget)))  # Adaptive population size\n\n        return best_solution", "name": "HybridDESA", "description": "Introduce a decay factor to decrease mutation scaling over time for refined convergence.", "configspace": "", "generation": 43, "fitness": 0.24127563731886387, "feedback": "The algorithm HybridDESA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.24 with standard deviation 0.19.", "error": "", "parent_id": "a23abc51-cac8-4819-b530-c9b5e6c87530", "metadata": {"aucs": [0.5089506057635013, 0.5089506057635013, 0.5089506057635013, 0.5048047526127155, 0.5048047526127155, 0.5048047526127155, 0.49663185778912877, 0.49663185778912877, 0.49663185778912877, 0.20667341342081347, 0.20667341342081347, 0.20667341342081347, 0.188859568172146, 0.188859568172146, 0.188859568172146, 0.18201945549140375, 0.18201945549140375, 0.18201945549140375, 0.13819228650371262, 0.13819228650371262, 0.13819228650371262, 0.12656571169603703, 0.12656571169603703, 0.12656571169603703, 0.1431303702171144, 0.1431303702171144, 0.1431303702171144, 0.09812855909251061, 0.09812855909251061, 0.09812855909251061, 0.1000045926522658, 0.1000045926522658, 0.1000045926522658, 0.11798771270452357, 0.11798771270452357, 0.11798771270452357, 0.87224705441059, 0.87224705441059, 0.87224705441059, 0.8433556763985367, 0.8433556763985367, 0.8433556763985367, 0.8348580934595236, 0.8348580934595236, 0.8348580934595236, 0.29066979781995883, 0.29066979781995883, 0.29066979781995883, 0.2858377235693186, 0.2858377235693186, 0.2858377235693186, 0.2988596582533407, 0.2988596582533407, 0.2988596582533407, 0.31884017099546913, 0.31884017099546913, 0.31884017099546913, 0.5804488407125017, 0.5804488407125017, 0.5804488407125017, 0.5597042151968271, 0.5597042151968271, 0.5597042151968271, 0.13092396021370334, 0.13092396021370334, 0.13092396021370334, 0.13475492497149266, 0.13475492497149266, 0.13475492497149266, 0.13843244308960512, 0.13843244308960512, 0.13843244308960512, 0.14385503228561225, 0.14385503228561225, 0.14385503228561225, 0.12864516501939993, 0.12864516501939993, 0.12864516501939993, 0.12678067181662722, 0.12678067181662722, 0.12678067181662722, 0.09596153771927562, 0.09596153771927562, 0.09596153771927562, 0.05208672393222169, 0.05208672393222169, 0.05208672393222169, 0.038080668467628365, 0.038080668467628365, 0.038080668467628365, 0.14724130513221312, 0.14724130513221312, 0.14724130513221312, 0.12970094466802762, 0.12970094466802762, 0.12970094466802762, 0.1230299952086612, 0.1230299952086612, 0.1230299952086612, 0.05561880410835507, 0.05561880410835507, 0.05561880410835507, 0.06145109920447178, 0.06145109920447178, 0.06145109920447178, 0.035531539140631896, 0.035531539140631896, 0.035531539140631896, 0.13788984829631268, 0.13788984829631268, 0.13788984829631268, 0.1149980610320408, 0.1149980610320408, 0.1149980610320408, 0.12600372520080905, 0.12600372520080905, 0.12600372520080905, 0.4132895792503958, 0.4132895792503958, 0.4132895792503958, 0.4003052661437817, 0.4003052661437817, 0.4003052661437817, 0.418123670725303, 0.418123670725303, 0.418123670725303, 0.11169514917872891, 0.11169514917872891, 0.11169514917872891, 0.14174465194281916, 0.14174465194281916, 0.14174465194281916, 0.12607777593735847, 0.12607777593735847, 0.12607777593735847, 0.14677581518256322, 0.14677581518256322, 0.14677581518256322, 0.197598206960383, 0.197598206960383, 0.197598206960383, 0.2059380399105174, 0.2059380399105174, 0.2059380399105174, 0.31162821939491225, 0.31162821939491225, 0.31162821939491225, 0.30582058899683084, 0.30582058899683084, 0.30582058899683084, 0.3169677640370484, 0.3169677640370484, 0.3169677640370484, 0.2617708309524237, 0.2617708309524237, 0.2617708309524237, 0.21239427682102263, 0.21239427682102263, 0.21239427682102263, 0.25155064741125777, 0.25155064741125777, 0.25155064741125777, 0.21436363936978087, 0.21436363936978087, 0.21436363936978087, 0.2398642379674345, 0.2398642379674345, 0.2398642379674345, 0.21878346241075086, 0.21878346241075086, 0.21878346241075086, 0.19141714357148043, 0.19141714357148043, 0.19141714357148043, 0.21067807839143138, 0.21067807839143138, 0.21067807839143138, 0.1865348378834295, 0.1865348378834295, 0.1865348378834295, 0.6865686059569746, 0.6865686059569746, 0.6865686059569746, 0.21202308408878456, 0.21202308408878456, 0.21202308408878456, 0.4330079002915218, 0.4330079002915218, 0.4330079002915218, 0.189213055700997, 0.189213055700997, 0.189213055700997, 0.18893232694167317, 0.18893232694167317, 0.18893232694167317, 0.14724846234458377, 0.14724846234458377, 0.14724846234458377, 0.18384345406251346, 0.18384345406251346, 0.18384345406251346, 0.19491102944326744, 0.19491102944326744, 0.19491102944326744, 0.19302913760403384, 0.19302913760403384, 0.19302913760403384, 0.07715822122970806, 0.07715822122970806, 0.07715822122970806, 0.08565447224934974, 0.08565447224934974, 0.08565447224934974, 0.07917768816610826, 0.07917768816610826, 0.07917768816610826]}, "mutation_prompt": null}
{"id": "c176d0bd-4aae-4e8c-834f-03a1f6367d83", "solution": "import numpy as np\n\nclass HybridDESA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.pop_size = 10 * dim\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.temp = 1000.0  # Initial temperature for simulated annealing\n        self.cooling_rate = 0.99  # Cooling rate for simulated annealing\n\n    def __call__(self, func):\n        np.random.seed(42)\n        population = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        num_evaluations = self.pop_size\n\n        def mutate(target_idx):\n            indices = [idx for idx in range(self.pop_size) if idx != target_idx]\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            dynamic_F = self.F * (1.0 - (num_evaluations / self.budget)) * np.exp(-num_evaluations / self.budget)  # Decay factor added\n            mutant = population[a] + dynamic_F * (population[b] - population[c])\n            mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n            return mutant\n\n        while num_evaluations < self.budget:\n            self.CR = 0.9 - (0.3 * (num_evaluations / self.budget))  # Dynamic adjustment with enhanced exploration\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                mutant = mutate(i)\n                trial = np.where(np.random.rand(self.dim) < self.CR, mutant, population[i])\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = trial\n\n            # Simulated Annealing part\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                neighbor = population[i] + np.random.normal(0, 0.1 * self.temp / 1000, self.dim)\n                neighbor = np.clip(neighbor, self.lower_bound, self.upper_bound)\n                neighbor_fitness = func(neighbor)\n                num_evaluations += 1\n                delta_fit = neighbor_fitness - fitness[i]\n\n                if delta_fit < 0 or np.random.rand() < np.exp(-delta_fit / self.temp):\n                    population[i] = neighbor\n                    fitness[i] = neighbor_fitness\n                    if neighbor_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = neighbor\n\n            self.temp *= (self.cooling_rate - 0.01 * (num_evaluations / self.budget))  # Adjusted cooling rate\n            self.pop_size = max(4, int(8 * self.dim * (1 - num_evaluations / self.budget)))  # Adaptive population size\n\n        return best_solution", "name": "HybridDESA", "description": "Integrate dynamic crossover probability decay to balance exploration and exploitation phases.", "configspace": "", "generation": 44, "fitness": 0.2464517947461025, "feedback": "The algorithm HybridDESA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.25 with standard deviation 0.18.", "error": "", "parent_id": "c27dd26c-6ae1-4f50-94a9-6a890777abbb", "metadata": {"aucs": [0.5035239764140618, 0.5035239764140618, 0.5035239764140618, 0.5099778223952196, 0.5099778223952196, 0.5099778223952196, 0.5311241022996918, 0.5311241022996918, 0.5311241022996918, 0.16709877559680042, 0.16709877559680042, 0.16709877559680042, 0.17114773316261367, 0.17114773316261367, 0.17114773316261367, 0.20360328209178136, 0.20360328209178136, 0.20360328209178136, 0.12766382005040033, 0.12766382005040033, 0.12766382005040033, 0.12262805673716881, 0.12262805673716881, 0.12262805673716881, 0.12047001700479865, 0.12047001700479865, 0.12047001700479865, 0.1135217848922444, 0.1135217848922444, 0.1135217848922444, 0.1089382447689724, 0.1089382447689724, 0.1089382447689724, 0.10695357397455496, 0.10695357397455496, 0.10695357397455496, 0.8670529987539095, 0.8670529987539095, 0.8670529987539095, 0.8618632630453595, 0.8618632630453595, 0.8618632630453595, 0.7733300082991207, 0.7733300082991207, 0.7733300082991207, 0.3028012425943405, 0.3028012425943405, 0.3028012425943405, 0.3018415732091342, 0.3018415732091342, 0.3018415732091342, 0.2926104733018048, 0.2926104733018048, 0.2926104733018048, 0.31372002664767507, 0.31372002664767507, 0.31372002664767507, 0.34077103872783354, 0.34077103872783354, 0.34077103872783354, 0.346626858314946, 0.346626858314946, 0.346626858314946, 0.14258307287734528, 0.14258307287734528, 0.14258307287734528, 0.14108609496651148, 0.14108609496651148, 0.14108609496651148, 0.14237163128987262, 0.14237163128987262, 0.14237163128987262, 0.15061062256043733, 0.15061062256043733, 0.15061062256043733, 0.14723451353485273, 0.14723451353485273, 0.14723451353485273, 0.15352864002344746, 0.15352864002344746, 0.15352864002344746, 0.05105311502428722, 0.05105311502428722, 0.05105311502428722, 0.08175672388436328, 0.08175672388436328, 0.08175672388436328, 0.0919702169156249, 0.0919702169156249, 0.0919702169156249, 0.15377299286410706, 0.15377299286410706, 0.15377299286410706, 0.13263326132860898, 0.13263326132860898, 0.13263326132860898, 0.15323012718963636, 0.15323012718963636, 0.15323012718963636, 0.06210609435553072, 0.06210609435553072, 0.06210609435553072, 0.1030196445103978, 0.1030196445103978, 0.1030196445103978, 0.049865189297059964, 0.049865189297059964, 0.049865189297059964, 0.11499924878924672, 0.11499924878924672, 0.11499924878924672, 0.12664543715391652, 0.12664543715391652, 0.12664543715391652, 0.13201157722857826, 0.13201157722857826, 0.13201157722857826, 0.41569034169194696, 0.41569034169194696, 0.41569034169194696, 0.41566571969055754, 0.41566571969055754, 0.41566571969055754, 0.4260409325501332, 0.4260409325501332, 0.4260409325501332, 0.11738468113058387, 0.11738468113058387, 0.11738468113058387, 0.2143364851369698, 0.2143364851369698, 0.2143364851369698, 0.12315120517130529, 0.12315120517130529, 0.12315120517130529, 0.17562440760725273, 0.17562440760725273, 0.17562440760725273, 0.20338220422841036, 0.20338220422841036, 0.20338220422841036, 0.21173674789439545, 0.21173674789439545, 0.21173674789439545, 0.30602378773363414, 0.30602378773363414, 0.30602378773363414, 0.30080690449570047, 0.30080690449570047, 0.30080690449570047, 0.3084757136495043, 0.3084757136495043, 0.3084757136495043, 0.22892627914686248, 0.22892627914686248, 0.22892627914686248, 0.22614058020677208, 0.22614058020677208, 0.22614058020677208, 0.26008349475927495, 0.26008349475927495, 0.26008349475927495, 0.22143807137140814, 0.22143807137140814, 0.22143807137140814, 0.23173218456519584, 0.23173218456519584, 0.23173218456519584, 0.2128022044973188, 0.2128022044973188, 0.2128022044973188, 0.19312614226557967, 0.19312614226557967, 0.19312614226557967, 0.3046652234612095, 0.3046652234612095, 0.3046652234612095, 0.20193317060241667, 0.20193317060241667, 0.20193317060241667, 0.6178674377394057, 0.6178674377394057, 0.6178674377394057, 0.5679974388766922, 0.5679974388766922, 0.5679974388766922, 0.5688694038379118, 0.5688694038379118, 0.5688694038379118, 0.19454003128547348, 0.19454003128547348, 0.19454003128547348, 0.18823999649266965, 0.18823999649266965, 0.18823999649266965, 0.15069641874396889, 0.15069641874396889, 0.15069641874396889, 0.18201152256865138, 0.18201152256865138, 0.18201152256865138, 0.19390082141074438, 0.19390082141074438, 0.19390082141074438, 0.1986266621420807, 0.1986266621420807, 0.1986266621420807, 0.08927243413886377, 0.08927243413886377, 0.08927243413886377, 0.08774677595619051, 0.08774677595619051, 0.08774677595619051, 0.08784692059404076, 0.08784692059404076, 0.08784692059404076]}, "mutation_prompt": null}
{"id": "fe5aac61-5427-4be5-9a43-77f033304011", "solution": "import numpy as np\n\nclass HybridDESA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.pop_size = 10 * dim\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.temp = 1000.0  # Initial temperature for simulated annealing\n        self.cooling_rate = 0.99  # Cooling rate for simulated annealing\n\n    def __call__(self, func):\n        np.random.seed(42)\n        population = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        num_evaluations = self.pop_size\n\n        def mutate(target_idx):\n            indices = [idx for idx in range(self.pop_size) if idx != target_idx]\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            dynamic_F = self.F * np.sin((num_evaluations / self.budget) * np.pi / 2)  # Updated decay factor\n            mutant = population[a] + dynamic_F * (population[b] - population[c])\n            mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n            return mutant\n\n        while num_evaluations < self.budget:\n            self.CR = 0.9 - (0.3 * (num_evaluations / self.budget))  # Dynamic adjustment with enhanced exploration\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                mutant = mutate(i)\n                trial = np.where(np.random.rand(self.dim) < self.CR, mutant, population[i])\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = trial\n\n            # Simulated Annealing part\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                neighbor = population[i] + np.random.normal(0, 0.1 * self.temp / 1000, self.dim)\n                neighbor = np.clip(neighbor, self.lower_bound, self.upper_bound)\n                neighbor_fitness = func(neighbor)\n                num_evaluations += 1\n                delta_fit = neighbor_fitness - fitness[i]\n\n                if delta_fit < 0 or np.random.rand() < np.exp(-delta_fit / self.temp):\n                    population[i] = neighbor\n                    fitness[i] = neighbor_fitness\n                    if neighbor_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = neighbor\n\n            self.temp *= (self.cooling_rate - 0.01 * (num_evaluations / self.budget))  # Adjusted cooling rate\n            self.pop_size = max(4, int(8 * self.dim * (1 - num_evaluations / self.budget)))  # Adaptive population size\n\n        return best_solution", "name": "HybridDESA", "description": "Enhance exploration by introducing a more dynamic adjustment of the differential weight to better adapt to varying landscapes.", "configspace": "", "generation": 45, "fitness": 0.21594819641468618, "feedback": "The algorithm HybridDESA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.22 with standard deviation 0.15.", "error": "", "parent_id": "c176d0bd-4aae-4e8c-834f-03a1f6367d83", "metadata": {"aucs": [0.49563806127000554, 0.49563806127000554, 0.49563806127000554, 0.49195028890864, 0.49195028890864, 0.49195028890864, 0.5107637661272031, 0.5107637661272031, 0.5107637661272031, 0.11663485423438269, 0.11663485423438269, 0.11663485423438269, 0.09226294472976226, 0.09226294472976226, 0.09226294472976226, 0.1755646079501867, 0.1755646079501867, 0.1755646079501867, 0.13878361104509673, 0.13878361104509673, 0.13878361104509673, 0.13662342560241814, 0.13662342560241814, 0.13662342560241814, 0.11853439494522355, 0.11853439494522355, 0.11853439494522355, 0.08540944756345104, 0.08540944756345104, 0.08540944756345104, 0.039874407951743684, 0.039874407951743684, 0.039874407951743684, 0.06575346047241437, 0.06575346047241437, 0.06575346047241437, 0.6882682803532769, 0.6882682803532769, 0.6882682803532769, 0.6462224839784834, 0.6462224839784834, 0.6462224839784834, 0.6460688956235917, 0.6460688956235917, 0.6460688956235917, 0.2698198929863356, 0.2698198929863356, 0.2698198929863356, 0.2665648546257001, 0.2665648546257001, 0.2665648546257001, 0.2904470104351854, 0.2904470104351854, 0.2904470104351854, 0.10682354038833486, 0.10682354038833486, 0.10682354038833486, 0.1978113274232699, 0.1978113274232699, 0.1978113274232699, 0.290616443725906, 0.290616443725906, 0.290616443725906, 0.16315512190933057, 0.16315512190933057, 0.16315512190933057, 0.13622616462854453, 0.13622616462854453, 0.13622616462854453, 0.1808618729227186, 0.1808618729227186, 0.1808618729227186, 0.16466401428819466, 0.16466401428819466, 0.16466401428819466, 0.1620174159894484, 0.1620174159894484, 0.1620174159894484, 0.16936856205345685, 0.16936856205345685, 0.16936856205345685, 0.10630498334898908, 0.10630498334898908, 0.10630498334898908, 0.02959333631372807, 0.02959333631372807, 0.02959333631372807, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.1037035581096063, 0.1037035581096063, 0.1037035581096063, 0.08148718588533632, 0.08148718588533632, 0.08148718588533632, 0.0684168677199567, 0.0684168677199567, 0.0684168677199567, 0.07635572105639044, 0.07635572105639044, 0.07635572105639044, 0.09687260663550856, 0.09687260663550856, 0.09687260663550856, 0.12345714099774363, 0.12345714099774363, 0.12345714099774363, 0.13390088139128908, 0.13390088139128908, 0.13390088139128908, 0.11980541420407242, 0.11980541420407242, 0.11980541420407242, 0.11889445839440538, 0.11889445839440538, 0.11889445839440538, 0.42646723615373605, 0.42646723615373605, 0.42646723615373605, 0.3608597387818595, 0.3608597387818595, 0.3608597387818595, 0.42259660488085216, 0.42259660488085216, 0.42259660488085216, 0.13711399369547983, 0.13711399369547983, 0.13711399369547983, 0.11854407767813824, 0.11854407767813824, 0.11854407767813824, 0.10572039831138191, 0.10572039831138191, 0.10572039831138191, 0.28698074935809603, 0.28698074935809603, 0.28698074935809603, 0.21830514486406505, 0.21830514486406505, 0.21830514486406505, 0.28829269424502857, 0.28829269424502857, 0.28829269424502857, 0.29139225424811255, 0.29139225424811255, 0.29139225424811255, 0.20410982744585626, 0.20410982744585626, 0.20410982744585626, 0.30870503584968734, 0.30870503584968734, 0.30870503584968734, 0.23001286911633667, 0.23001286911633667, 0.23001286911633667, 0.18807948108092898, 0.18807948108092898, 0.18807948108092898, 0.2002317274440889, 0.2002317274440889, 0.2002317274440889, 0.2175571259708562, 0.2175571259708562, 0.2175571259708562, 0.1972419395213778, 0.1972419395213778, 0.1972419395213778, 0.20904286222993873, 0.20904286222993873, 0.20904286222993873, 0.31890095504460814, 0.31890095504460814, 0.31890095504460814, 0.20394777103779593, 0.20394777103779593, 0.20394777103779593, 0.2397305189980613, 0.2397305189980613, 0.2397305189980613, 0.7212242910794977, 0.7212242910794977, 0.7212242910794977, 0.1963799732472361, 0.1963799732472361, 0.1963799732472361, 0.16429871128954, 0.16429871128954, 0.16429871128954, 0.2025009890004308, 0.2025009890004308, 0.2025009890004308, 0.20749108736806743, 0.20749108736806743, 0.20749108736806743, 0.1513853176730523, 0.1513853176730523, 0.1513853176730523, 0.17805363877217573, 0.17805363877217573, 0.17805363877217573, 0.1852973065035627, 0.1852973065035627, 0.1852973065035627, 0.1894898964190609, 0.1894898964190609, 0.1894898964190609, 0.09946194920575269, 0.09946194920575269, 0.09946194920575269, 0.08891067981397516, 0.08891067981397516, 0.08891067981397516, 0.08432198933943436, 0.08432198933943436, 0.08432198933943436]}, "mutation_prompt": null}
{"id": "b40b66d9-e1f6-4c87-a9ee-b5c4cd5b57e8", "solution": "import numpy as np\n\nclass HybridDESA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.pop_size = 10 * dim\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.temp = 1000.0  # Initial temperature for simulated annealing\n        self.cooling_rate = 0.99  # Cooling rate for simulated annealing\n\n    def __call__(self, func):\n        np.random.seed(42)\n        population = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        num_evaluations = self.pop_size\n\n        def mutate(target_idx):\n            indices = [idx for idx in range(self.pop_size) if idx != target_idx]\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            dynamic_F = self.F * (1.0 - (num_evaluations / self.budget)) * np.exp(-num_evaluations / self.budget)  # Decay factor added\n            mutant = population[a] + dynamic_F * (population[b] - population[c])\n            mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n            return mutant\n\n        while num_evaluations < self.budget:\n            self.CR = 0.9 - (0.3 * (num_evaluations / self.budget))  # Dynamic adjustment with enhanced exploration\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                mutant = mutate(i)\n                trial = np.where(np.random.rand(self.dim) < self.CR, mutant, 0.5 * (population[i] + best_solution))  # Refined trial selection\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = trial\n\n            # Simulated Annealing part\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                neighbor = population[i] + np.random.normal(0, 0.1 * self.temp / 1000, self.dim)\n                neighbor = np.clip(neighbor, self.lower_bound, self.upper_bound)\n                neighbor_fitness = func(neighbor)\n                num_evaluations += 1\n                delta_fit = neighbor_fitness - fitness[i]\n\n                if delta_fit < 0 or np.random.rand() < np.exp(-delta_fit / self.temp):\n                    population[i] = neighbor\n                    fitness[i] = neighbor_fitness\n                    if neighbor_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = neighbor\n\n            self.temp *= (self.cooling_rate - 0.01 * (num_evaluations / self.budget))  # Adjusted cooling rate\n            self.pop_size = max(4, int(8 * self.dim * (1 - num_evaluations / self.budget)))  # Adaptive population size\n\n        return best_solution", "name": "HybridDESA", "description": "Enhance exploitation by refining trial vector selection strategy.", "configspace": "", "generation": 46, "fitness": 0.26162707527461637, "feedback": "The algorithm HybridDESA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.26 with standard deviation 0.19.", "error": "", "parent_id": "c176d0bd-4aae-4e8c-834f-03a1f6367d83", "metadata": {"aucs": [0.5439164519622353, 0.5439164519622353, 0.5439164519622353, 0.5433829283626338, 0.5433829283626338, 0.5433829283626338, 0.5388850171149873, 0.5388850171149873, 0.5388850171149873, 0.21454918393252387, 0.21454918393252387, 0.21454918393252387, 0.18767797890026672, 0.18767797890026672, 0.18767797890026672, 0.21756376333183391, 0.21756376333183391, 0.21756376333183391, 0.26002111591282284, 0.26002111591282284, 0.26002111591282284, 0.12961581925294396, 0.12961581925294396, 0.12961581925294396, 0.1303902521987459, 0.1303902521987459, 0.1303902521987459, 0.09701082408027739, 0.09701082408027739, 0.09701082408027739, 0.12436717614293669, 0.12436717614293669, 0.12436717614293669, 0.09417828589746857, 0.09417828589746857, 0.09417828589746857, 0.8644705008816079, 0.8644705008816079, 0.8644705008816079, 0.8800314205698251, 0.8800314205698251, 0.8800314205698251, 0.8846186483924845, 0.8846186483924845, 0.8846186483924845, 0.37207337702382404, 0.37207337702382404, 0.37207337702382404, 0.35090275697490503, 0.35090275697490503, 0.35090275697490503, 0.34788948953836263, 0.34788948953836263, 0.34788948953836263, 0.7054011983320436, 0.7054011983320436, 0.7054011983320436, 0.582456115306947, 0.582456115306947, 0.582456115306947, 0.26433938312948735, 0.26433938312948735, 0.26433938312948735, 0.1578768528779605, 0.1578768528779605, 0.1578768528779605, 0.1741530994720102, 0.1741530994720102, 0.1741530994720102, 0.1627096794557883, 0.1627096794557883, 0.1627096794557883, 0.10730000593477385, 0.10730000593477385, 0.10730000593477385, 0.15591521986291368, 0.15591521986291368, 0.15591521986291368, 0.1834455420034916, 0.1834455420034916, 0.1834455420034916, 0.08512476426602611, 0.08512476426602611, 0.08512476426602611, 0.13775867893438953, 0.13775867893438953, 0.13775867893438953, 0.06950568278259739, 0.06950568278259739, 0.06950568278259739, 0.1364272732806927, 0.1364272732806927, 0.1364272732806927, 0.11655164148611508, 0.11655164148611508, 0.11655164148611508, 0.1768413390467416, 0.1768413390467416, 0.1768413390467416, 0.08835959622215539, 0.08835959622215539, 0.08835959622215539, 0.18852125266274367, 0.18852125266274367, 0.18852125266274367, 0.14937853240243093, 0.14937853240243093, 0.14937853240243093, 0.1528141778845169, 0.1528141778845169, 0.1528141778845169, 0.14506191613051778, 0.14506191613051778, 0.14506191613051778, 0.164930055625083, 0.164930055625083, 0.164930055625083, 0.456536405271196, 0.456536405271196, 0.456536405271196, 0.42182792636819, 0.42182792636819, 0.42182792636819, 0.4280883237596367, 0.4280883237596367, 0.4280883237596367, 0.2609723999401543, 0.2609723999401543, 0.2609723999401543, 0.13328864662656437, 0.13328864662656437, 0.13328864662656437, 0.12760299403853093, 0.12760299403853093, 0.12760299403853093, 0.15588048668863463, 0.15588048668863463, 0.15588048668863463, 0.23604635866939605, 0.23604635866939605, 0.23604635866939605, 0.18921232931157328, 0.18921232931157328, 0.18921232931157328, 0.33002396434194803, 0.33002396434194803, 0.33002396434194803, 0.33090636293353637, 0.33090636293353637, 0.33090636293353637, 0.3388587669035036, 0.3388587669035036, 0.3388587669035036, 0.2628045900736997, 0.2628045900736997, 0.2628045900736997, 0.2651740209739273, 0.2651740209739273, 0.2651740209739273, 0.2757172420392333, 0.2757172420392333, 0.2757172420392333, 0.2201538782701158, 0.2201538782701158, 0.2201538782701158, 0.22249584594266147, 0.22249584594266147, 0.22249584594266147, 0.20123552814415657, 0.20123552814415657, 0.20123552814415657, 0.19832073436145192, 0.19832073436145192, 0.19832073436145192, 0.19666261468792856, 0.19666261468792856, 0.19666261468792856, 0.20149554867493258, 0.20149554867493258, 0.20149554867493258, 0.686081248894497, 0.686081248894497, 0.686081248894497, 0.1563333000332967, 0.1563333000332967, 0.1563333000332967, 0.19770402242731, 0.19770402242731, 0.19770402242731, 0.16511297334529718, 0.16511297334529718, 0.16511297334529718, 0.19991668918973204, 0.19991668918973204, 0.19991668918973204, 0.2048294639645306, 0.2048294639645306, 0.2048294639645306, 0.20537962345992644, 0.20537962345992644, 0.20537962345992644, 0.20406743835401708, 0.20406743835401708, 0.20406743835401708, 0.20738581992871385, 0.20738581992871385, 0.20738581992871385, 0.09132517915779736, 0.09132517915779736, 0.09132517915779736, 0.09039945654823056, 0.09039945654823056, 0.09039945654823056, 0.08889223887794984, 0.08889223887794984, 0.08889223887794984]}, "mutation_prompt": null}
{"id": "5574f710-b8fb-40f9-a21e-7d2afac40270", "solution": "import numpy as np\n\nclass HybridDESA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.pop_size = 10 * dim\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.temp = 1000.0  # Initial temperature for simulated annealing\n        self.cooling_rate = 0.99  # Cooling rate for simulated annealing\n\n    def __call__(self, func):\n        np.random.seed(42)\n        population = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        num_evaluations = self.pop_size\n\n        def mutate(target_idx):\n            indices = [idx for idx in range(self.pop_size) if idx != target_idx]\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            dynamic_F = self.F * (1.0 - (num_evaluations / self.budget)) * np.exp(-num_evaluations / self.budget)\n            dynamic_F *= (self.temp / 1000.0)  # Modified line: Temperature-dependent mutation factor\n            mutant = population[a] + dynamic_F * (population[b] - population[c])\n            mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n            return mutant\n\n        while num_evaluations < self.budget:\n            self.CR = 0.9 - (0.3 * (num_evaluations / self.budget))\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                mutant = mutate(i)\n                trial = np.where(np.random.rand(self.dim) < self.CR, mutant, 0.5 * (population[i] + best_solution))\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = trial\n\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                neighbor = population[i] + np.random.normal(0, 0.1 * self.temp / 1000, self.dim)\n                neighbor = np.clip(neighbor, self.lower_bound, self.upper_bound)\n                neighbor_fitness = func(neighbor)\n                num_evaluations += 1\n                delta_fit = neighbor_fitness - fitness[i]\n\n                if delta_fit < 0 or np.random.rand() < np.exp(-delta_fit / self.temp):\n                    population[i] = neighbor\n                    fitness[i] = neighbor_fitness\n                    if neighbor_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = neighbor\n\n            self.temp *= (self.cooling_rate - 0.01 * (num_evaluations / self.budget))\n            self.pop_size = max(4, int(8 * self.dim * (1 - num_evaluations / self.budget)))\n\n        return best_solution", "name": "HybridDESA", "description": "Incorporate a temperature-dependent mutation factor to enhance exploration during early search phases.", "configspace": "", "generation": 47, "fitness": 0.2678111390429435, "feedback": "The algorithm HybridDESA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.27 with standard deviation 0.19.", "error": "", "parent_id": "b40b66d9-e1f6-4c87-a9ee-b5c4cd5b57e8", "metadata": {"aucs": [0.5421937017571719, 0.5421937017571719, 0.5421937017571719, 0.5602329286166772, 0.5602329286166772, 0.5602329286166772, 0.5486840517156681, 0.5486840517156681, 0.5486840517156681, 0.224794382539645, 0.224794382539645, 0.224794382539645, 0.24354620789461146, 0.24354620789461146, 0.24354620789461146, 0.2985700234794808, 0.2985700234794808, 0.2985700234794808, 0.15588078770782854, 0.15588078770782854, 0.15588078770782854, 0.13778739984766142, 0.13778739984766142, 0.13778739984766142, 0.10660746801388599, 0.10660746801388599, 0.10660746801388599, 0.10542295741654073, 0.10542295741654073, 0.10542295741654073, 0.11394423943544751, 0.11394423943544751, 0.11394423943544751, 0.12799333758547837, 0.12799333758547837, 0.12799333758547837, 0.7547811837904082, 0.7547811837904082, 0.7547811837904082, 0.8554901281323539, 0.8554901281323539, 0.8554901281323539, 0.8758763993808825, 0.8758763993808825, 0.8758763993808825, 0.3569046652602593, 0.3569046652602593, 0.3569046652602593, 0.35019506654164445, 0.35019506654164445, 0.35019506654164445, 0.3388358892045805, 0.3388358892045805, 0.3388358892045805, 0.7228276941589196, 0.7228276941589196, 0.7228276941589196, 0.6886548016575028, 0.6886548016575028, 0.6886548016575028, 0.6448702537633678, 0.6448702537633678, 0.6448702537633678, 0.15136011837783392, 0.15136011837783392, 0.15136011837783392, 0.1597169209680791, 0.1597169209680791, 0.1597169209680791, 0.183081274980047, 0.183081274980047, 0.183081274980047, 0.1139177776783108, 0.1139177776783108, 0.1139177776783108, 0.16463214547570182, 0.16463214547570182, 0.16463214547570182, 0.14980832624165852, 0.14980832624165852, 0.14980832624165852, 0.10022773436004206, 0.10022773436004206, 0.10022773436004206, 0.03116857085356728, 0.03116857085356728, 0.03116857085356728, 0.10708726364328336, 0.10708726364328336, 0.10708726364328336, 0.18441637232973596, 0.18441637232973596, 0.18441637232973596, 0.10950629060135486, 0.10950629060135486, 0.10950629060135486, 0.1801774296483365, 0.1801774296483365, 0.1801774296483365, 0.10627269004375761, 0.10627269004375761, 0.10627269004375761, 0.19639099929074988, 0.19639099929074988, 0.19639099929074988, 0.17658660382609648, 0.17658660382609648, 0.17658660382609648, 0.07362819525179132, 0.07362819525179132, 0.07362819525179132, 0.1338591622909444, 0.1338591622909444, 0.1338591622909444, 0.16723949310098218, 0.16723949310098218, 0.16723949310098218, 0.4675506796200075, 0.4675506796200075, 0.4675506796200075, 0.47363547003426343, 0.47363547003426343, 0.47363547003426343, 0.43713003470676737, 0.43713003470676737, 0.43713003470676737, 0.27250362922225435, 0.27250362922225435, 0.27250362922225435, 0.1431671152841989, 0.1431671152841989, 0.1431671152841989, 0.13655828207418164, 0.13655828207418164, 0.13655828207418164, 0.2092918336497841, 0.2092918336497841, 0.2092918336497841, 0.3075726965516099, 0.3075726965516099, 0.3075726965516099, 0.2520327466945782, 0.2520327466945782, 0.2520327466945782, 0.3408462875801054, 0.3408462875801054, 0.3408462875801054, 0.32805408750254206, 0.32805408750254206, 0.32805408750254206, 0.34890256193812297, 0.34890256193812297, 0.34890256193812297, 0.2657822910045774, 0.2657822910045774, 0.2657822910045774, 0.27773922038033294, 0.27773922038033294, 0.27773922038033294, 0.275738331562712, 0.275738331562712, 0.275738331562712, 0.22517412994331487, 0.22517412994331487, 0.22517412994331487, 0.22425587714201312, 0.22425587714201312, 0.22425587714201312, 0.24805072621710056, 0.24805072621710056, 0.24805072621710056, 0.37123729041185594, 0.37123729041185594, 0.37123729041185594, 0.21083401245209288, 0.21083401245209288, 0.21083401245209288, 0.18509715495794365, 0.18509715495794365, 0.18509715495794365, 0.18197771746559555, 0.18197771746559555, 0.18197771746559555, 0.18259026260510847, 0.18259026260510847, 0.18259026260510847, 0.16619050598732488, 0.16619050598732488, 0.16619050598732488, 0.20210768482248864, 0.20210768482248864, 0.20210768482248864, 0.19661699325785642, 0.19661699325785642, 0.19661699325785642, 0.20558686226391654, 0.20558686226391654, 0.20558686226391654, 0.20348813844034652, 0.20348813844034652, 0.20348813844034652, 0.210535374473671, 0.210535374473671, 0.210535374473671, 0.2082203588490592, 0.2082203588490592, 0.2082203588490592, 0.09269956078295782, 0.09269956078295782, 0.09269956078295782, 0.09191824469254084, 0.09191824469254084, 0.09191824469254084, 0.09614491165841732, 0.09614491165841732, 0.09614491165841732]}, "mutation_prompt": null}
{"id": "3168ea34-41ff-4ea1-89a4-c6a83a217861", "solution": "import numpy as np\n\nclass HybridDESA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.pop_size = 10 * dim\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.temp = 1000.0  # Initial temperature for simulated annealing\n        self.cooling_rate = 0.99  # Cooling rate for simulated annealing\n\n    def __call__(self, func):\n        np.random.seed(42)\n        population = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        num_evaluations = self.pop_size\n\n        def mutate(target_idx):\n            indices = [idx for idx in range(self.pop_size) if idx != target_idx]\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            dynamic_F = self.F * (1.0 - (num_evaluations / self.budget)) * np.exp(-num_evaluations / self.budget)\n            dynamic_F *= (self.temp / 1000.0)  # Modified line: Temperature-dependent mutation factor\n            mutant = population[a] + dynamic_F * (population[b] - population[c])\n            mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n            return mutant\n\n        while num_evaluations < self.budget:\n            self.CR = 0.9 - (0.3 * (num_evaluations / self.budget))\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                mutant = mutate(i)\n                trial = np.where(np.random.rand(self.dim) < self.CR, mutant, 0.5 * (population[i] + best_solution))\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = trial\n\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                neighbor = population[i] + np.random.normal(0, 0.1 * self.temp / 1000, self.dim)\n                neighbor = np.clip(neighbor, self.lower_bound, self.upper_bound)\n                neighbor_fitness = func(neighbor)\n                num_evaluations += 1\n                delta_fit = neighbor_fitness - fitness[i]\n\n                if delta_fit < 0 or np.random.rand() < np.exp(-delta_fit / self.temp):\n                    population[i] = neighbor\n                    fitness[i] = neighbor_fitness\n                    if neighbor_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = neighbor\n\n            self.temp *= (self.cooling_rate - 0.01 * (num_evaluations / self.budget))\n            self.pop_size = max(4, int(8 * self.dim * (1 - num_evaluations / self.budget)))\n\n        return best_solution", "name": "HybridDESA", "description": "Incorporate a temperature-dependent mutation factor to enhance exploration during early search phases.", "configspace": "", "generation": 48, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "5574f710-b8fb-40f9-a21e-7d2afac40270", "metadata": {"aucs": [0.5421937017571719, 0.5421937017571719, 0.5421937017571719, 0.5602329286166772, 0.5602329286166772, 0.5602329286166772, 0.5486840517156681, 0.5486840517156681, 0.5486840517156681, 0.224794382539645, 0.224794382539645, 0.224794382539645, 0.24354620789461146, 0.24354620789461146, 0.24354620789461146, 0.2985700234794808, 0.2985700234794808, 0.2985700234794808, 0.15588078770782854, 0.15588078770782854, 0.15588078770782854, 0.13778739984766142, 0.13778739984766142, 0.13778739984766142, 0.10660746801388599, 0.10660746801388599, 0.10660746801388599, 0.10542295741654073, 0.10542295741654073, 0.10542295741654073, 0.11394423943544751, 0.11394423943544751, 0.11394423943544751, 0.12799333758547837, 0.12799333758547837, 0.12799333758547837, 0.7547811837904082, 0.7547811837904082, 0.7547811837904082, 0.8554901281323539, 0.8554901281323539, 0.8554901281323539, 0.8758763993808825, 0.8758763993808825, 0.8758763993808825, 0.3569046652602593, 0.3569046652602593, 0.3569046652602593, 0.35019506654164445, 0.35019506654164445, 0.35019506654164445, 0.3388358892045805, 0.3388358892045805, 0.3388358892045805, 0.7228276941589196, 0.7228276941589196, 0.7228276941589196, 0.6886548016575028, 0.6886548016575028, 0.6886548016575028, 0.6448702537633678, 0.6448702537633678, 0.6448702537633678, 0.15136011837783392, 0.15136011837783392, 0.15136011837783392, 0.1597169209680791, 0.1597169209680791, 0.1597169209680791, 0.183081274980047, 0.183081274980047, 0.183081274980047, 0.1139177776783108, 0.1139177776783108, 0.1139177776783108, 0.16463214547570182, 0.16463214547570182, 0.16463214547570182, 0.14980832624165852, 0.14980832624165852, 0.14980832624165852, 0.10022773436004206, 0.10022773436004206, 0.10022773436004206, 0.03116857085356728, 0.03116857085356728, 0.03116857085356728, 0.10708726364328336, 0.10708726364328336, 0.10708726364328336, 0.18441637232973596, 0.18441637232973596, 0.18441637232973596, 0.10950629060135486, 0.10950629060135486, 0.10950629060135486, 0.1801774296483365, 0.1801774296483365, 0.1801774296483365, 0.10627269004375761, 0.10627269004375761, 0.10627269004375761, 0.19639099929074988, 0.19639099929074988, 0.19639099929074988, 0.17658660382609648, 0.17658660382609648, 0.17658660382609648, 0.07362819525179132, 0.07362819525179132, 0.07362819525179132, 0.1338591622909444, 0.1338591622909444, 0.1338591622909444, 0.16723949310098218, 0.16723949310098218, 0.16723949310098218, 0.4675506796200075, 0.4675506796200075, 0.4675506796200075, 0.47363547003426343, 0.47363547003426343, 0.47363547003426343, 0.43713003470676737, 0.43713003470676737, 0.43713003470676737, 0.27250362922225435, 0.27250362922225435, 0.27250362922225435, 0.1431671152841989, 0.1431671152841989, 0.1431671152841989, 0.13655828207418164, 0.13655828207418164, 0.13655828207418164, 0.2092918336497841, 0.2092918336497841, 0.2092918336497841, 0.3075726965516099, 0.3075726965516099, 0.3075726965516099, 0.2520327466945782, 0.2520327466945782, 0.2520327466945782, 0.3408462875801054, 0.3408462875801054, 0.3408462875801054, 0.32805408750254206, 0.32805408750254206, 0.32805408750254206, 0.34890256193812297, 0.34890256193812297, 0.34890256193812297, 0.2657822910045774, 0.2657822910045774, 0.2657822910045774, 0.27773922038033294, 0.27773922038033294, 0.27773922038033294, 0.275738331562712, 0.275738331562712, 0.275738331562712, 0.22517412994331487, 0.22517412994331487, 0.22517412994331487, 0.22425587714201312, 0.22425587714201312, 0.22425587714201312, 0.24805072621710056, 0.24805072621710056, 0.24805072621710056, 0.37123729041185594, 0.37123729041185594, 0.37123729041185594, 0.21083401245209288, 0.21083401245209288, 0.21083401245209288, 0.18509715495794365, 0.18509715495794365, 0.18509715495794365, 0.18197771746559555, 0.18197771746559555, 0.18197771746559555, 0.18259026260510847, 0.18259026260510847, 0.18259026260510847, 0.16619050598732488, 0.16619050598732488, 0.16619050598732488, 0.20210768482248864, 0.20210768482248864, 0.20210768482248864, 0.19661699325785642, 0.19661699325785642, 0.19661699325785642, 0.20558686226391654, 0.20558686226391654, 0.20558686226391654, 0.20348813844034652, 0.20348813844034652, 0.20348813844034652, 0.210535374473671, 0.210535374473671, 0.210535374473671, 0.2082203588490592, 0.2082203588490592, 0.2082203588490592, 0.09269956078295782, 0.09269956078295782, 0.09269956078295782, 0.09191824469254084, 0.09191824469254084, 0.09191824469254084, 0.09614491165841732, 0.09614491165841732, 0.09614491165841732]}, "mutation_prompt": null}
{"id": "7c2cdd96-a24a-4bdc-96df-83fbafae318e", "solution": "import numpy as np\n\nclass HybridDESA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.pop_size = 10 * dim\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.temp = 1000.0  # Initial temperature for simulated annealing\n        self.cooling_rate = 0.99  # Cooling rate for simulated annealing\n\n    def __call__(self, func):\n        np.random.seed(42)\n        population = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        num_evaluations = self.pop_size\n\n        def mutate(target_idx):\n            indices = [idx for idx in range(self.pop_size) if idx != target_idx]\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            dynamic_F = self.F * (1.0 - (num_evaluations / self.budget)) * np.exp(-num_evaluations / self.budget)\n            dynamic_F *= (self.temp / 1000.0) * (1 + 0.1 * (num_evaluations / self.budget))  # Modified line: Adaptive scaling to mutation factor\n            mutant = population[a] + dynamic_F * (population[b] - population[c])\n            mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n            return mutant\n\n        while num_evaluations < self.budget:\n            self.CR = 0.9 - (0.3 * (num_evaluations / self.budget))\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                mutant = mutate(i)\n                trial = np.where(np.random.rand(self.dim) < self.CR, mutant, 0.5 * (population[i] + best_solution))\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = trial\n\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                neighbor = population[i] + np.random.normal(0, 0.1 * self.temp / 1000, self.dim)\n                neighbor = np.clip(neighbor, self.lower_bound, self.upper_bound)\n                neighbor_fitness = func(neighbor)\n                num_evaluations += 1\n                delta_fit = neighbor_fitness - fitness[i]\n\n                if delta_fit < 0 or np.random.rand() < np.exp(-delta_fit / self.temp):\n                    population[i] = neighbor\n                    fitness[i] = neighbor_fitness\n                    if neighbor_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = neighbor\n\n            self.temp *= (self.cooling_rate - 0.01 * (num_evaluations / self.budget))\n            self.pop_size = max(4, int(8 * self.dim * (1 - num_evaluations / self.budget)))\n\n        return best_solution", "name": "HybridDESA", "description": "Introduce adaptive scaling to mutation factor based on function evaluation ratio to improve exploitation capabilities.", "configspace": "", "generation": 49, "fitness": 0.2681305376547231, "feedback": "The algorithm HybridDESA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.27 with standard deviation 0.19.", "error": "", "parent_id": "5574f710-b8fb-40f9-a21e-7d2afac40270", "metadata": {"aucs": [0.5442606585864098, 0.5442606585864098, 0.5442606585864098, 0.5594667505839126, 0.5594667505839126, 0.5594667505839126, 0.5619670043110119, 0.5619670043110119, 0.5619670043110119, 0.21747217979971056, 0.21747217979971056, 0.21747217979971056, 0.301145005068842, 0.301145005068842, 0.301145005068842, 0.2701774299619817, 0.2701774299619817, 0.2701774299619817, 0.12380717926710627, 0.12380717926710627, 0.12380717926710627, 0.2707857807301146, 0.2707857807301146, 0.2707857807301146, 0.12434041080760361, 0.12434041080760361, 0.12434041080760361, 0.09990514563760045, 0.09990514563760045, 0.09990514563760045, 0.11400070825966435, 0.11400070825966435, 0.11400070825966435, 0.10067172072753439, 0.10067172072753439, 0.10067172072753439, 0.8661251149491918, 0.8661251149491918, 0.8661251149491918, 0.7290713983164432, 0.7290713983164432, 0.7290713983164432, 0.8436918710960647, 0.8436918710960647, 0.8436918710960647, 0.36712616727641867, 0.36712616727641867, 0.36712616727641867, 0.3636131714545058, 0.3636131714545058, 0.3636131714545058, 0.3659024547958438, 0.3659024547958438, 0.3659024547958438, 0.6310787403199133, 0.6310787403199133, 0.6310787403199133, 0.6768815727847995, 0.6768815727847995, 0.6768815727847995, 0.5994986583901672, 0.5994986583901672, 0.5994986583901672, 0.16395592167529593, 0.16395592167529593, 0.16395592167529593, 0.16248216297557505, 0.16248216297557505, 0.16248216297557505, 0.15316082201437908, 0.15316082201437908, 0.15316082201437908, 0.13422969468819068, 0.13422969468819068, 0.13422969468819068, 0.15994447909387, 0.15994447909387, 0.15994447909387, 0.17925306879841263, 0.17925306879841263, 0.17925306879841263, 0.11693011259923936, 0.11693011259923936, 0.11693011259923936, 0.13639781006130602, 0.13639781006130602, 0.13639781006130602, 0.0958292613408166, 0.0958292613408166, 0.0958292613408166, 0.10006033384570234, 0.10006033384570234, 0.10006033384570234, 0.1772039837170689, 0.1772039837170689, 0.1772039837170689, 0.12716325765900838, 0.12716325765900838, 0.12716325765900838, 0.05686359512133354, 0.05686359512133354, 0.05686359512133354, 0.11681310492106911, 0.11681310492106911, 0.11681310492106911, 0.15179585825375674, 0.15179585825375674, 0.15179585825375674, 0.10382739431942212, 0.10382739431942212, 0.10382739431942212, 0.14537585172280887, 0.14537585172280887, 0.14537585172280887, 0.15717930997441565, 0.15717930997441565, 0.15717930997441565, 0.4528669556728523, 0.4528669556728523, 0.4528669556728523, 0.458952743718657, 0.458952743718657, 0.458952743718657, 0.4620892965273501, 0.4620892965273501, 0.4620892965273501, 0.14033997840350254, 0.14033997840350254, 0.14033997840350254, 0.2682290973070668, 0.2682290973070668, 0.2682290973070668, 0.1490239265232367, 0.1490239265232367, 0.1490239265232367, 0.18401715372093796, 0.18401715372093796, 0.18401715372093796, 0.17696361157596752, 0.17696361157596752, 0.17696361157596752, 0.21923037373927678, 0.21923037373927678, 0.21923037373927678, 0.3303506444060237, 0.3303506444060237, 0.3303506444060237, 0.3324590475247433, 0.3324590475247433, 0.3324590475247433, 0.33682868711350433, 0.33682868711350433, 0.33682868711350433, 0.27600513773960433, 0.27600513773960433, 0.27600513773960433, 0.27728610841027, 0.27728610841027, 0.27728610841027, 0.2777719329986905, 0.2777719329986905, 0.2777719329986905, 0.2245954506033757, 0.2245954506033757, 0.2245954506033757, 0.2326601224038437, 0.2326601224038437, 0.2326601224038437, 0.2677386403983355, 0.2677386403983355, 0.2677386403983355, 0.17856169785671994, 0.17856169785671994, 0.17856169785671994, 0.1858441067890847, 0.1858441067890847, 0.1858441067890847, 0.16780382235241265, 0.16780382235241265, 0.16780382235241265, 0.7358333621066688, 0.7358333621066688, 0.7358333621066688, 0.15611265274709496, 0.15611265274709496, 0.15611265274709496, 0.15358931980043466, 0.15358931980043466, 0.15358931980043466, 0.20138745427599292, 0.20138745427599292, 0.20138745427599292, 0.19671548035487985, 0.19671548035487985, 0.19671548035487985, 0.2043569815436096, 0.2043569815436096, 0.2043569815436096, 0.20557195626092395, 0.20557195626092395, 0.20557195626092395, 0.20788996694542194, 0.20788996694542194, 0.20788996694542194, 0.19607587685120376, 0.19607587685120376, 0.19607587685120376, 0.09142243954669904, 0.09142243954669904, 0.09142243954669904, 0.09273474322018704, 0.09273474322018704, 0.09273474322018704, 0.09463479579497758, 0.09463479579497758, 0.09463479579497758]}, "mutation_prompt": null}
{"id": "720d77bc-d3d4-48b5-a805-8336d68d6fc9", "solution": "import numpy as np\n\nclass HybridDESA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.pop_size = 10 * dim\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.temp = 1000.0  # Initial temperature for simulated annealing\n        self.cooling_rate = 0.99  # Cooling rate for simulated annealing\n\n    def __call__(self, func):\n        np.random.seed(42)\n        population = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        num_evaluations = self.pop_size\n\n        def mutate(target_idx):\n            indices = [idx for idx in range(self.pop_size) if idx != target_idx]\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            dynamic_F = self.F * (1.0 - (num_evaluations / self.budget)) * np.exp(-num_evaluations / self.budget)\n            dynamic_F *= (self.temp / 1000.0) * (1 + 0.1 * (num_evaluations / self.budget))  # Modified line: Adaptive scaling to mutation factor\n            mutant = population[a] + dynamic_F * (population[b] - population[c])\n            mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n            return mutant\n\n        while num_evaluations < self.budget:\n            self.CR = 0.9 - (0.3 * (num_evaluations / self.budget))\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                mutant = mutate(i)\n                trial = np.where(np.random.rand(self.dim) < self.CR, mutant, 0.5 * (population[i] + best_solution))\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = trial\n\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                neighbor = population[i] + np.random.normal(0, 0.1 * self.temp / 1000, self.dim)\n                neighbor = np.clip(neighbor, self.lower_bound, self.upper_bound)\n                neighbor_fitness = func(neighbor)\n                num_evaluations += 1\n                delta_fit = neighbor_fitness - fitness[i]\n\n                if delta_fit < 0 or np.random.rand() < np.exp(-delta_fit / self.temp):\n                    population[i] = neighbor\n                    fitness[i] = neighbor_fitness\n                    if neighbor_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = neighbor\n\n            self.temp *= (self.cooling_rate - 0.01 * (num_evaluations / self.budget))\n            self.pop_size = max(4, int(8 * self.dim * (1 - num_evaluations / self.budget)))\n\n        return best_solution", "name": "HybridDESA", "description": "Introduce adaptive scaling to mutation factor based on function evaluation ratio to improve exploitation capabilities.", "configspace": "", "generation": 50, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "7c2cdd96-a24a-4bdc-96df-83fbafae318e", "metadata": {"aucs": [0.5442606585864098, 0.5442606585864098, 0.5442606585864098, 0.5594667505839126, 0.5594667505839126, 0.5594667505839126, 0.5619670043110119, 0.5619670043110119, 0.5619670043110119, 0.21747217979971056, 0.21747217979971056, 0.21747217979971056, 0.301145005068842, 0.301145005068842, 0.301145005068842, 0.2701774299619817, 0.2701774299619817, 0.2701774299619817, 0.12380717926710627, 0.12380717926710627, 0.12380717926710627, 0.2707857807301146, 0.2707857807301146, 0.2707857807301146, 0.12434041080760361, 0.12434041080760361, 0.12434041080760361, 0.09990514563760045, 0.09990514563760045, 0.09990514563760045, 0.11400070825966435, 0.11400070825966435, 0.11400070825966435, 0.10067172072753439, 0.10067172072753439, 0.10067172072753439, 0.8661251149491918, 0.8661251149491918, 0.8661251149491918, 0.7290713983164432, 0.7290713983164432, 0.7290713983164432, 0.8436918710960647, 0.8436918710960647, 0.8436918710960647, 0.36712616727641867, 0.36712616727641867, 0.36712616727641867, 0.3636131714545058, 0.3636131714545058, 0.3636131714545058, 0.3659024547958438, 0.3659024547958438, 0.3659024547958438, 0.6310787403199133, 0.6310787403199133, 0.6310787403199133, 0.6768815727847995, 0.6768815727847995, 0.6768815727847995, 0.5994986583901672, 0.5994986583901672, 0.5994986583901672, 0.16395592167529593, 0.16395592167529593, 0.16395592167529593, 0.16248216297557505, 0.16248216297557505, 0.16248216297557505, 0.15316082201437908, 0.15316082201437908, 0.15316082201437908, 0.13422969468819068, 0.13422969468819068, 0.13422969468819068, 0.15994447909387, 0.15994447909387, 0.15994447909387, 0.17925306879841263, 0.17925306879841263, 0.17925306879841263, 0.11693011259923936, 0.11693011259923936, 0.11693011259923936, 0.13639781006130602, 0.13639781006130602, 0.13639781006130602, 0.0958292613408166, 0.0958292613408166, 0.0958292613408166, 0.10006033384570234, 0.10006033384570234, 0.10006033384570234, 0.1772039837170689, 0.1772039837170689, 0.1772039837170689, 0.12716325765900838, 0.12716325765900838, 0.12716325765900838, 0.05686359512133354, 0.05686359512133354, 0.05686359512133354, 0.11681310492106911, 0.11681310492106911, 0.11681310492106911, 0.15179585825375674, 0.15179585825375674, 0.15179585825375674, 0.10382739431942212, 0.10382739431942212, 0.10382739431942212, 0.14537585172280887, 0.14537585172280887, 0.14537585172280887, 0.15717930997441565, 0.15717930997441565, 0.15717930997441565, 0.4528669556728523, 0.4528669556728523, 0.4528669556728523, 0.458952743718657, 0.458952743718657, 0.458952743718657, 0.4620892965273501, 0.4620892965273501, 0.4620892965273501, 0.14033997840350254, 0.14033997840350254, 0.14033997840350254, 0.2682290973070668, 0.2682290973070668, 0.2682290973070668, 0.1490239265232367, 0.1490239265232367, 0.1490239265232367, 0.18401715372093796, 0.18401715372093796, 0.18401715372093796, 0.17696361157596752, 0.17696361157596752, 0.17696361157596752, 0.21923037373927678, 0.21923037373927678, 0.21923037373927678, 0.3303506444060237, 0.3303506444060237, 0.3303506444060237, 0.3324590475247433, 0.3324590475247433, 0.3324590475247433, 0.33682868711350433, 0.33682868711350433, 0.33682868711350433, 0.27600513773960433, 0.27600513773960433, 0.27600513773960433, 0.27728610841027, 0.27728610841027, 0.27728610841027, 0.2777719329986905, 0.2777719329986905, 0.2777719329986905, 0.2245954506033757, 0.2245954506033757, 0.2245954506033757, 0.2326601224038437, 0.2326601224038437, 0.2326601224038437, 0.2677386403983355, 0.2677386403983355, 0.2677386403983355, 0.17856169785671994, 0.17856169785671994, 0.17856169785671994, 0.1858441067890847, 0.1858441067890847, 0.1858441067890847, 0.16780382235241265, 0.16780382235241265, 0.16780382235241265, 0.7358333621066688, 0.7358333621066688, 0.7358333621066688, 0.15611265274709496, 0.15611265274709496, 0.15611265274709496, 0.15358931980043466, 0.15358931980043466, 0.15358931980043466, 0.20138745427599292, 0.20138745427599292, 0.20138745427599292, 0.19671548035487985, 0.19671548035487985, 0.19671548035487985, 0.2043569815436096, 0.2043569815436096, 0.2043569815436096, 0.20557195626092395, 0.20557195626092395, 0.20557195626092395, 0.20788996694542194, 0.20788996694542194, 0.20788996694542194, 0.19607587685120376, 0.19607587685120376, 0.19607587685120376, 0.09142243954669904, 0.09142243954669904, 0.09142243954669904, 0.09273474322018704, 0.09273474322018704, 0.09273474322018704, 0.09463479579497758, 0.09463479579497758, 0.09463479579497758]}, "mutation_prompt": null}
{"id": "fb189d99-87b7-4bc1-b675-18c905c04da8", "solution": "import numpy as np\n\nclass HybridDESA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.pop_size = 10 * dim\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.temp = 1000.0  # Initial temperature for simulated annealing\n        self.cooling_rate = 0.99  # Cooling rate for simulated annealing\n\n    def __call__(self, func):\n        np.random.seed(42)\n        population = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        num_evaluations = self.pop_size\n\n        def mutate(target_idx):\n            indices = [idx for idx in range(self.pop_size) if idx != target_idx]\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            dynamic_F = self.F * (1.0 - (num_evaluations / self.budget)) * np.exp(-num_evaluations / self.budget)\n            dynamic_F *= (self.temp / 1000.0) * (1 + 0.1 * (num_evaluations / self.budget))  # Modified line: Adaptive scaling to mutation factor\n            mutant = population[a] + dynamic_F * (population[b] - population[c])\n            mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n            return mutant\n\n        while num_evaluations < self.budget:\n            self.CR = 0.9 - (0.3 * (num_evaluations / self.budget))\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                mutant = mutate(i)\n                trial = np.where(np.random.rand(self.dim) < self.CR, mutant, 0.5 * (population[i] + best_solution))\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = trial\n\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                neighbor = population[i] + np.random.normal(0, 0.1 * self.temp / 1000, self.dim)\n                neighbor = np.clip(neighbor, self.lower_bound, self.upper_bound)\n                neighbor_fitness = func(neighbor)\n                num_evaluations += 1\n                delta_fit = neighbor_fitness - fitness[i]\n\n                if delta_fit < 0 or np.random.rand() < np.exp(-delta_fit / self.temp):\n                    population[i] = neighbor\n                    fitness[i] = neighbor_fitness\n                    if neighbor_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = neighbor\n\n            self.temp *= (self.cooling_rate - 0.01 * (num_evaluations / self.budget))\n            self.pop_size = max(4, int(8 * self.dim * (1 - num_evaluations / self.budget)))\n\n        return best_solution", "name": "HybridDESA", "description": "Introduce adaptive scaling to mutation factor based on function evaluation ratio to improve exploitation capabilities.", "configspace": "", "generation": 50, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "7c2cdd96-a24a-4bdc-96df-83fbafae318e", "metadata": {"aucs": [0.5442606585864098, 0.5442606585864098, 0.5442606585864098, 0.5594667505839126, 0.5594667505839126, 0.5594667505839126, 0.5619670043110119, 0.5619670043110119, 0.5619670043110119, 0.21747217979971056, 0.21747217979971056, 0.21747217979971056, 0.301145005068842, 0.301145005068842, 0.301145005068842, 0.2701774299619817, 0.2701774299619817, 0.2701774299619817, 0.12380717926710627, 0.12380717926710627, 0.12380717926710627, 0.2707857807301146, 0.2707857807301146, 0.2707857807301146, 0.12434041080760361, 0.12434041080760361, 0.12434041080760361, 0.09990514563760045, 0.09990514563760045, 0.09990514563760045, 0.11400070825966435, 0.11400070825966435, 0.11400070825966435, 0.10067172072753439, 0.10067172072753439, 0.10067172072753439, 0.8661251149491918, 0.8661251149491918, 0.8661251149491918, 0.7290713983164432, 0.7290713983164432, 0.7290713983164432, 0.8436918710960647, 0.8436918710960647, 0.8436918710960647, 0.36712616727641867, 0.36712616727641867, 0.36712616727641867, 0.3636131714545058, 0.3636131714545058, 0.3636131714545058, 0.3659024547958438, 0.3659024547958438, 0.3659024547958438, 0.6310787403199133, 0.6310787403199133, 0.6310787403199133, 0.6768815727847995, 0.6768815727847995, 0.6768815727847995, 0.5994986583901672, 0.5994986583901672, 0.5994986583901672, 0.16395592167529593, 0.16395592167529593, 0.16395592167529593, 0.16248216297557505, 0.16248216297557505, 0.16248216297557505, 0.15316082201437908, 0.15316082201437908, 0.15316082201437908, 0.13422969468819068, 0.13422969468819068, 0.13422969468819068, 0.15994447909387, 0.15994447909387, 0.15994447909387, 0.17925306879841263, 0.17925306879841263, 0.17925306879841263, 0.11693011259923936, 0.11693011259923936, 0.11693011259923936, 0.13639781006130602, 0.13639781006130602, 0.13639781006130602, 0.0958292613408166, 0.0958292613408166, 0.0958292613408166, 0.10006033384570234, 0.10006033384570234, 0.10006033384570234, 0.1772039837170689, 0.1772039837170689, 0.1772039837170689, 0.12716325765900838, 0.12716325765900838, 0.12716325765900838, 0.05686359512133354, 0.05686359512133354, 0.05686359512133354, 0.11681310492106911, 0.11681310492106911, 0.11681310492106911, 0.15179585825375674, 0.15179585825375674, 0.15179585825375674, 0.10382739431942212, 0.10382739431942212, 0.10382739431942212, 0.14537585172280887, 0.14537585172280887, 0.14537585172280887, 0.15717930997441565, 0.15717930997441565, 0.15717930997441565, 0.4528669556728523, 0.4528669556728523, 0.4528669556728523, 0.458952743718657, 0.458952743718657, 0.458952743718657, 0.4620892965273501, 0.4620892965273501, 0.4620892965273501, 0.14033997840350254, 0.14033997840350254, 0.14033997840350254, 0.2682290973070668, 0.2682290973070668, 0.2682290973070668, 0.1490239265232367, 0.1490239265232367, 0.1490239265232367, 0.18401715372093796, 0.18401715372093796, 0.18401715372093796, 0.17696361157596752, 0.17696361157596752, 0.17696361157596752, 0.21923037373927678, 0.21923037373927678, 0.21923037373927678, 0.3303506444060237, 0.3303506444060237, 0.3303506444060237, 0.3324590475247433, 0.3324590475247433, 0.3324590475247433, 0.33682868711350433, 0.33682868711350433, 0.33682868711350433, 0.27600513773960433, 0.27600513773960433, 0.27600513773960433, 0.27728610841027, 0.27728610841027, 0.27728610841027, 0.2777719329986905, 0.2777719329986905, 0.2777719329986905, 0.2245954506033757, 0.2245954506033757, 0.2245954506033757, 0.2326601224038437, 0.2326601224038437, 0.2326601224038437, 0.2677386403983355, 0.2677386403983355, 0.2677386403983355, 0.17856169785671994, 0.17856169785671994, 0.17856169785671994, 0.1858441067890847, 0.1858441067890847, 0.1858441067890847, 0.16780382235241265, 0.16780382235241265, 0.16780382235241265, 0.7358333621066688, 0.7358333621066688, 0.7358333621066688, 0.15611265274709496, 0.15611265274709496, 0.15611265274709496, 0.15358931980043466, 0.15358931980043466, 0.15358931980043466, 0.20138745427599292, 0.20138745427599292, 0.20138745427599292, 0.19671548035487985, 0.19671548035487985, 0.19671548035487985, 0.2043569815436096, 0.2043569815436096, 0.2043569815436096, 0.20557195626092395, 0.20557195626092395, 0.20557195626092395, 0.20788996694542194, 0.20788996694542194, 0.20788996694542194, 0.19607587685120376, 0.19607587685120376, 0.19607587685120376, 0.09142243954669904, 0.09142243954669904, 0.09142243954669904, 0.09273474322018704, 0.09273474322018704, 0.09273474322018704, 0.09463479579497758, 0.09463479579497758, 0.09463479579497758]}, "mutation_prompt": null}
{"id": "7ad31e93-2263-4d31-927c-f7dbe61baa8e", "solution": "import numpy as np\n\nclass HybridDESA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.pop_size = 10 * dim\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.temp = 1000.0  # Initial temperature for simulated annealing\n        self.cooling_rate = 0.99  # Cooling rate for simulated annealing\n\n    def __call__(self, func):\n        np.random.seed(42)\n        population = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        num_evaluations = self.pop_size\n\n        def mutate(target_idx):\n            indices = [idx for idx in range(self.pop_size) if idx != target_idx]\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            dynamic_F = self.F * (1.0 - (num_evaluations / self.budget)) * np.exp(-num_evaluations / self.budget)\n            dynamic_F *= (self.temp / 1000.0) * (1 + 0.1 * (num_evaluations / self.budget))  # Modified line: Adaptive scaling to mutation factor\n            mutant = population[a] + dynamic_F * (population[b] - population[c])\n            mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n            return mutant\n\n        while num_evaluations < self.budget:\n            self.CR = 0.9 - (0.3 * (num_evaluations / self.budget))\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                mutant = mutate(i)\n                trial = np.where(np.random.rand(self.dim) < self.CR, mutant, 0.5 * (population[i] + best_solution))\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = trial\n\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                neighbor = population[i] + np.random.normal(0, 0.1 * self.temp / 1000, self.dim)\n                neighbor = np.clip(neighbor, self.lower_bound, self.upper_bound)\n                neighbor_fitness = func(neighbor)\n                num_evaluations += 1\n                delta_fit = neighbor_fitness - fitness[i]\n\n                if delta_fit < 0 or np.random.rand() < np.exp(-delta_fit / self.temp):\n                    population[i] = neighbor\n                    fitness[i] = neighbor_fitness\n                    if neighbor_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = neighbor\n\n            self.temp *= (self.cooling_rate - 0.01 * (num_evaluations / self.budget))\n            self.pop_size = max(4, int(8 * self.dim * (1 - num_evaluations / self.budget)))\n\n        return best_solution", "name": "HybridDESA", "description": "Introduce adaptive scaling to mutation factor based on function evaluation ratio to improve exploitation capabilities.", "configspace": "", "generation": 50, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "7c2cdd96-a24a-4bdc-96df-83fbafae318e", "metadata": {"aucs": [0.5442606585864098, 0.5442606585864098, 0.5442606585864098, 0.5594667505839126, 0.5594667505839126, 0.5594667505839126, 0.5619670043110119, 0.5619670043110119, 0.5619670043110119, 0.21747217979971056, 0.21747217979971056, 0.21747217979971056, 0.301145005068842, 0.301145005068842, 0.301145005068842, 0.2701774299619817, 0.2701774299619817, 0.2701774299619817, 0.12380717926710627, 0.12380717926710627, 0.12380717926710627, 0.2707857807301146, 0.2707857807301146, 0.2707857807301146, 0.12434041080760361, 0.12434041080760361, 0.12434041080760361, 0.09990514563760045, 0.09990514563760045, 0.09990514563760045, 0.11400070825966435, 0.11400070825966435, 0.11400070825966435, 0.10067172072753439, 0.10067172072753439, 0.10067172072753439, 0.8661251149491918, 0.8661251149491918, 0.8661251149491918, 0.7290713983164432, 0.7290713983164432, 0.7290713983164432, 0.8436918710960647, 0.8436918710960647, 0.8436918710960647, 0.36712616727641867, 0.36712616727641867, 0.36712616727641867, 0.3636131714545058, 0.3636131714545058, 0.3636131714545058, 0.3659024547958438, 0.3659024547958438, 0.3659024547958438, 0.6310787403199133, 0.6310787403199133, 0.6310787403199133, 0.6768815727847995, 0.6768815727847995, 0.6768815727847995, 0.5994986583901672, 0.5994986583901672, 0.5994986583901672, 0.16395592167529593, 0.16395592167529593, 0.16395592167529593, 0.16248216297557505, 0.16248216297557505, 0.16248216297557505, 0.15316082201437908, 0.15316082201437908, 0.15316082201437908, 0.13422969468819068, 0.13422969468819068, 0.13422969468819068, 0.15994447909387, 0.15994447909387, 0.15994447909387, 0.17925306879841263, 0.17925306879841263, 0.17925306879841263, 0.11693011259923936, 0.11693011259923936, 0.11693011259923936, 0.13639781006130602, 0.13639781006130602, 0.13639781006130602, 0.0958292613408166, 0.0958292613408166, 0.0958292613408166, 0.10006033384570234, 0.10006033384570234, 0.10006033384570234, 0.1772039837170689, 0.1772039837170689, 0.1772039837170689, 0.12716325765900838, 0.12716325765900838, 0.12716325765900838, 0.05686359512133354, 0.05686359512133354, 0.05686359512133354, 0.11681310492106911, 0.11681310492106911, 0.11681310492106911, 0.15179585825375674, 0.15179585825375674, 0.15179585825375674, 0.10382739431942212, 0.10382739431942212, 0.10382739431942212, 0.14537585172280887, 0.14537585172280887, 0.14537585172280887, 0.15717930997441565, 0.15717930997441565, 0.15717930997441565, 0.4528669556728523, 0.4528669556728523, 0.4528669556728523, 0.458952743718657, 0.458952743718657, 0.458952743718657, 0.4620892965273501, 0.4620892965273501, 0.4620892965273501, 0.14033997840350254, 0.14033997840350254, 0.14033997840350254, 0.2682290973070668, 0.2682290973070668, 0.2682290973070668, 0.1490239265232367, 0.1490239265232367, 0.1490239265232367, 0.18401715372093796, 0.18401715372093796, 0.18401715372093796, 0.17696361157596752, 0.17696361157596752, 0.17696361157596752, 0.21923037373927678, 0.21923037373927678, 0.21923037373927678, 0.3303506444060237, 0.3303506444060237, 0.3303506444060237, 0.3324590475247433, 0.3324590475247433, 0.3324590475247433, 0.33682868711350433, 0.33682868711350433, 0.33682868711350433, 0.27600513773960433, 0.27600513773960433, 0.27600513773960433, 0.27728610841027, 0.27728610841027, 0.27728610841027, 0.2777719329986905, 0.2777719329986905, 0.2777719329986905, 0.2245954506033757, 0.2245954506033757, 0.2245954506033757, 0.2326601224038437, 0.2326601224038437, 0.2326601224038437, 0.2677386403983355, 0.2677386403983355, 0.2677386403983355, 0.17856169785671994, 0.17856169785671994, 0.17856169785671994, 0.1858441067890847, 0.1858441067890847, 0.1858441067890847, 0.16780382235241265, 0.16780382235241265, 0.16780382235241265, 0.7358333621066688, 0.7358333621066688, 0.7358333621066688, 0.15611265274709496, 0.15611265274709496, 0.15611265274709496, 0.15358931980043466, 0.15358931980043466, 0.15358931980043466, 0.20138745427599292, 0.20138745427599292, 0.20138745427599292, 0.19671548035487985, 0.19671548035487985, 0.19671548035487985, 0.2043569815436096, 0.2043569815436096, 0.2043569815436096, 0.20557195626092395, 0.20557195626092395, 0.20557195626092395, 0.20788996694542194, 0.20788996694542194, 0.20788996694542194, 0.19607587685120376, 0.19607587685120376, 0.19607587685120376, 0.09142243954669904, 0.09142243954669904, 0.09142243954669904, 0.09273474322018704, 0.09273474322018704, 0.09273474322018704, 0.09463479579497758, 0.09463479579497758, 0.09463479579497758]}, "mutation_prompt": null}
{"id": "081fd4c8-1ca6-47b9-b5d4-e1c7be4b45c4", "solution": "import numpy as np\n\nclass HybridDESA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.pop_size = 10 * dim\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.temp = 1000.0  # Initial temperature for simulated annealing\n        self.cooling_rate = 0.995  # Cooling rate for simulated annealing (Modified line)\n\n    def __call__(self, func):\n        np.random.seed(42)\n        population = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        num_evaluations = self.pop_size\n\n        def mutate(target_idx):\n            indices = [idx for idx in range(self.pop_size) if idx != target_idx]\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            dynamic_F = self.F * (1.0 - (num_evaluations / self.budget)) * np.exp(-num_evaluations / self.budget)\n            dynamic_F *= (self.temp / 1000.0) * (1 + 0.1 * (num_evaluations / self.budget))\n            mutant = population[a] + dynamic_F * (population[b] - population[c])\n            mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n            return mutant\n\n        while num_evaluations < self.budget:\n            self.CR = 0.9 - (0.3 * (num_evaluations / self.budget))\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                mutant = mutate(i)\n                trial = np.where(np.random.rand(self.dim) < self.CR, mutant, 0.5 * (population[i] + best_solution))\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = trial\n\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                neighbor = population[i] + np.random.normal(0, 0.1 * self.temp / 1000, self.dim)\n                neighbor = np.clip(neighbor, self.lower_bound, self.upper_bound)\n                neighbor_fitness = func(neighbor)\n                num_evaluations += 1\n                delta_fit = neighbor_fitness - fitness[i]\n\n                if delta_fit < 0 or np.random.rand() < np.exp(-delta_fit / self.temp):\n                    population[i] = neighbor\n                    fitness[i] = neighbor_fitness\n                    if neighbor_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = neighbor\n\n            self.temp *= (self.cooling_rate - 0.01 * (num_evaluations / self.budget))\n            self.pop_size = max(4, int(8 * self.dim * (1 - num_evaluations / self.budget)))\n\n        return best_solution", "name": "HybridDESA", "description": "Fine-tune the temperature cooling rate to enhance balance between exploration and exploitation.", "configspace": "", "generation": 53, "fitness": 0.2503181655084389, "feedback": "The algorithm HybridDESA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.25 with standard deviation 0.17.", "error": "", "parent_id": "7c2cdd96-a24a-4bdc-96df-83fbafae318e", "metadata": {"aucs": [0.506626556753752, 0.506626556753752, 0.506626556753752, 0.5093131519119807, 0.5093131519119807, 0.5093131519119807, 0.5361146479513238, 0.5361146479513238, 0.5361146479513238, 0.246766740868533, 0.246766740868533, 0.246766740868533, 0.25779188408606624, 0.25779188408606624, 0.25779188408606624, 0.2724922932491126, 0.2724922932491126, 0.2724922932491126, 0.1146228817308722, 0.1146228817308722, 0.1146228817308722, 0.1382152945002827, 0.1382152945002827, 0.1382152945002827, 0.13029064475187568, 0.13029064475187568, 0.13029064475187568, 0.10099414288161368, 0.10099414288161368, 0.10099414288161368, 0.11403299449191362, 0.11403299449191362, 0.11403299449191362, 0.10977370464671476, 0.10977370464671476, 0.10977370464671476, 0.8279874621666241, 0.8279874621666241, 0.8279874621666241, 0.7456340363838958, 0.7456340363838958, 0.7456340363838958, 0.8089970745358371, 0.8089970745358371, 0.8089970745358371, 0.3161218024794199, 0.3161218024794199, 0.3161218024794199, 0.31121455621660976, 0.31121455621660976, 0.31121455621660976, 0.3132110137903821, 0.3132110137903821, 0.3132110137903821, 0.23318358452139365, 0.23318358452139365, 0.23318358452139365, 0.5230740182154872, 0.5230740182154872, 0.5230740182154872, 0.701142517058519, 0.701142517058519, 0.701142517058519, 0.15443043457563177, 0.15443043457563177, 0.15443043457563177, 0.16683873211809042, 0.16683873211809042, 0.16683873211809042, 0.1477840947157134, 0.1477840947157134, 0.1477840947157134, 0.10911649956406078, 0.10911649956406078, 0.10911649956406078, 0.14744994915788245, 0.14744994915788245, 0.14744994915788245, 0.15759293992235412, 0.15759293992235412, 0.15759293992235412, 0.08299177979770411, 0.08299177979770411, 0.08299177979770411, 0.09759532650864444, 0.09759532650864444, 0.09759532650864444, 0.0896326604954083, 0.0896326604954083, 0.0896326604954083, 0.1313937982663902, 0.1313937982663902, 0.1313937982663902, 0.12344708024415774, 0.12344708024415774, 0.12344708024415774, 0.1539698016416574, 0.1539698016416574, 0.1539698016416574, 0.13112470459661763, 0.13112470459661763, 0.13112470459661763, 0.19370436502846833, 0.19370436502846833, 0.19370436502846833, 0.16305414801269946, 0.16305414801269946, 0.16305414801269946, 0.12766648554951898, 0.12766648554951898, 0.12766648554951898, 0.15274110341114533, 0.15274110341114533, 0.15274110341114533, 0.13060969446602155, 0.13060969446602155, 0.13060969446602155, 0.44954273246562615, 0.44954273246562615, 0.44954273246562615, 0.4257416957671383, 0.4257416957671383, 0.4257416957671383, 0.41850901618608594, 0.41850901618608594, 0.41850901618608594, 0.12716199003498774, 0.12716199003498774, 0.12716199003498774, 0.2149119682669789, 0.2149119682669789, 0.2149119682669789, 0.13024391975333516, 0.13024391975333516, 0.13024391975333516, 0.1777911528669469, 0.1777911528669469, 0.1777911528669469, 0.26927973241835157, 0.26927973241835157, 0.26927973241835157, 0.2067015492115163, 0.2067015492115163, 0.2067015492115163, 0.31066400455191057, 0.31066400455191057, 0.31066400455191057, 0.30760959573204927, 0.30760959573204927, 0.30760959573204927, 0.3172965516320845, 0.3172965516320845, 0.3172965516320845, 0.25694133342177206, 0.25694133342177206, 0.25694133342177206, 0.2515862807898569, 0.2515862807898569, 0.2515862807898569, 0.2605352548142297, 0.2605352548142297, 0.2605352548142297, 0.23496448026479178, 0.23496448026479178, 0.23496448026479178, 0.20184108116131716, 0.20184108116131716, 0.20184108116131716, 0.22568349454367764, 0.22568349454367764, 0.22568349454367764, 0.32204761388038017, 0.32204761388038017, 0.32204761388038017, 0.18441332764935947, 0.18441332764935947, 0.18441332764935947, 0.19834175123397424, 0.19834175123397424, 0.19834175123397424, 0.16569149517753412, 0.16569149517753412, 0.16569149517753412, 0.18071576159146074, 0.18071576159146074, 0.18071576159146074, 0.5453295147475598, 0.5453295147475598, 0.5453295147475598, 0.16523797409569796, 0.16523797409569796, 0.16523797409569796, 0.12642946464355487, 0.12642946464355487, 0.12642946464355487, 0.20578241986401524, 0.20578241986401524, 0.20578241986401524, 0.20233675175094634, 0.20233675175094634, 0.20233675175094634, 0.1948737405804557, 0.1948737405804557, 0.1948737405804557, 0.19133049775797817, 0.19133049775797817, 0.19133049775797817, 0.09346853824516077, 0.09346853824516077, 0.09346853824516077, 0.0898677376554049, 0.0898677376554049, 0.0898677376554049, 0.09129089258708156, 0.09129089258708156, 0.09129089258708156]}, "mutation_prompt": null}
{"id": "da397cfa-6b8e-4def-9a0c-5b7744fc8332", "solution": "import numpy as np\n\nclass HybridDESA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.pop_size = 10 * dim\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.temp = 1000.0  # Initial temperature for simulated annealing\n        self.cooling_rate = 0.99  # Cooling rate for simulated annealing\n\n    def __call__(self, func):\n        np.random.seed(42)\n        population = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        num_evaluations = self.pop_size\n\n        def mutate(target_idx):\n            indices = [idx for idx in range(self.pop_size) if idx != target_idx]\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            dynamic_F = self.F * (1.0 - (num_evaluations / self.budget)) * np.exp(-num_evaluations / self.budget)\n            dynamic_F *= (self.temp / 1000.0) * (1 + 0.1 * (num_evaluations / self.budget))  # Modified line: Adaptive scaling to mutation factor\n            mutant = population[a] + dynamic_F * (population[b] - population[c])\n            mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n            return mutant\n\n        while num_evaluations < self.budget:\n            self.CR = 0.9 - (0.3 * (num_evaluations / self.budget))\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                mutant = mutate(i)\n                trial = np.where(np.random.rand(self.dim) < self.CR, mutant, 0.5 * (population[i] + best_solution))\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = trial\n\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                neighbor = population[i] + np.random.normal(0, 0.1 * self.temp / 1000, self.dim)\n                neighbor = np.clip(neighbor, self.lower_bound, self.upper_bound)\n                neighbor_fitness = func(neighbor)\n                num_evaluations += 1\n                delta_fit = neighbor_fitness - fitness[i]\n\n                if delta_fit < 0 or np.random.rand() < np.exp(-delta_fit / self.temp):\n                    population[i] = neighbor\n                    fitness[i] = neighbor_fitness\n                    if neighbor_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = neighbor\n\n            self.temp *= (self.cooling_rate - 0.01 * (num_evaluations / self.budget))\n            self.pop_size = max(4, int(8 * self.dim * (1 - num_evaluations / self.budget)))\n\n        return best_solution", "name": "HybridDESA", "description": "Introduce adaptive scaling to mutation factor based on function evaluation ratio to improve exploitation capabilities.", "configspace": "", "generation": 50, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "7c2cdd96-a24a-4bdc-96df-83fbafae318e", "metadata": {"aucs": [0.5442606585864098, 0.5442606585864098, 0.5442606585864098, 0.5594667505839126, 0.5594667505839126, 0.5594667505839126, 0.5619670043110119, 0.5619670043110119, 0.5619670043110119, 0.21747217979971056, 0.21747217979971056, 0.21747217979971056, 0.301145005068842, 0.301145005068842, 0.301145005068842, 0.2701774299619817, 0.2701774299619817, 0.2701774299619817, 0.12380717926710627, 0.12380717926710627, 0.12380717926710627, 0.2707857807301146, 0.2707857807301146, 0.2707857807301146, 0.12434041080760361, 0.12434041080760361, 0.12434041080760361, 0.09990514563760045, 0.09990514563760045, 0.09990514563760045, 0.11400070825966435, 0.11400070825966435, 0.11400070825966435, 0.10067172072753439, 0.10067172072753439, 0.10067172072753439, 0.8661251149491918, 0.8661251149491918, 0.8661251149491918, 0.7290713983164432, 0.7290713983164432, 0.7290713983164432, 0.8436918710960647, 0.8436918710960647, 0.8436918710960647, 0.36712616727641867, 0.36712616727641867, 0.36712616727641867, 0.3636131714545058, 0.3636131714545058, 0.3636131714545058, 0.3659024547958438, 0.3659024547958438, 0.3659024547958438, 0.6310787403199133, 0.6310787403199133, 0.6310787403199133, 0.6768815727847995, 0.6768815727847995, 0.6768815727847995, 0.5994986583901672, 0.5994986583901672, 0.5994986583901672, 0.16395592167529593, 0.16395592167529593, 0.16395592167529593, 0.16248216297557505, 0.16248216297557505, 0.16248216297557505, 0.15316082201437908, 0.15316082201437908, 0.15316082201437908, 0.13422969468819068, 0.13422969468819068, 0.13422969468819068, 0.15994447909387, 0.15994447909387, 0.15994447909387, 0.17925306879841263, 0.17925306879841263, 0.17925306879841263, 0.11693011259923936, 0.11693011259923936, 0.11693011259923936, 0.13639781006130602, 0.13639781006130602, 0.13639781006130602, 0.0958292613408166, 0.0958292613408166, 0.0958292613408166, 0.10006033384570234, 0.10006033384570234, 0.10006033384570234, 0.1772039837170689, 0.1772039837170689, 0.1772039837170689, 0.12716325765900838, 0.12716325765900838, 0.12716325765900838, 0.05686359512133354, 0.05686359512133354, 0.05686359512133354, 0.11681310492106911, 0.11681310492106911, 0.11681310492106911, 0.15179585825375674, 0.15179585825375674, 0.15179585825375674, 0.10382739431942212, 0.10382739431942212, 0.10382739431942212, 0.14537585172280887, 0.14537585172280887, 0.14537585172280887, 0.15717930997441565, 0.15717930997441565, 0.15717930997441565, 0.4528669556728523, 0.4528669556728523, 0.4528669556728523, 0.458952743718657, 0.458952743718657, 0.458952743718657, 0.4620892965273501, 0.4620892965273501, 0.4620892965273501, 0.14033997840350254, 0.14033997840350254, 0.14033997840350254, 0.2682290973070668, 0.2682290973070668, 0.2682290973070668, 0.1490239265232367, 0.1490239265232367, 0.1490239265232367, 0.18401715372093796, 0.18401715372093796, 0.18401715372093796, 0.17696361157596752, 0.17696361157596752, 0.17696361157596752, 0.21923037373927678, 0.21923037373927678, 0.21923037373927678, 0.3303506444060237, 0.3303506444060237, 0.3303506444060237, 0.3324590475247433, 0.3324590475247433, 0.3324590475247433, 0.33682868711350433, 0.33682868711350433, 0.33682868711350433, 0.27600513773960433, 0.27600513773960433, 0.27600513773960433, 0.27728610841027, 0.27728610841027, 0.27728610841027, 0.2777719329986905, 0.2777719329986905, 0.2777719329986905, 0.2245954506033757, 0.2245954506033757, 0.2245954506033757, 0.2326601224038437, 0.2326601224038437, 0.2326601224038437, 0.2677386403983355, 0.2677386403983355, 0.2677386403983355, 0.17856169785671994, 0.17856169785671994, 0.17856169785671994, 0.1858441067890847, 0.1858441067890847, 0.1858441067890847, 0.16780382235241265, 0.16780382235241265, 0.16780382235241265, 0.7358333621066688, 0.7358333621066688, 0.7358333621066688, 0.15611265274709496, 0.15611265274709496, 0.15611265274709496, 0.15358931980043466, 0.15358931980043466, 0.15358931980043466, 0.20138745427599292, 0.20138745427599292, 0.20138745427599292, 0.19671548035487985, 0.19671548035487985, 0.19671548035487985, 0.2043569815436096, 0.2043569815436096, 0.2043569815436096, 0.20557195626092395, 0.20557195626092395, 0.20557195626092395, 0.20788996694542194, 0.20788996694542194, 0.20788996694542194, 0.19607587685120376, 0.19607587685120376, 0.19607587685120376, 0.09142243954669904, 0.09142243954669904, 0.09142243954669904, 0.09273474322018704, 0.09273474322018704, 0.09273474322018704, 0.09463479579497758, 0.09463479579497758, 0.09463479579497758]}, "mutation_prompt": null}
{"id": "ff03f636-9f0c-4b06-8411-b47e8c0ddf45", "solution": "import numpy as np\n\nclass HybridDESA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.pop_size = 10 * dim\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.temp = 1000.0  # Initial temperature for simulated annealing\n        self.cooling_rate = 0.99  # Cooling rate for simulated annealing\n\n    def __call__(self, func):\n        np.random.seed(42)\n        population = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        num_evaluations = self.pop_size\n\n        def mutate(target_idx):\n            indices = [idx for idx in range(self.pop_size) if idx != target_idx]\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            dynamic_F = self.F * (1.0 - (num_evaluations / self.budget)) * np.exp(-num_evaluations / self.budget)\n            dynamic_F *= (self.temp / 1000.0) * (1 + 0.1 * (num_evaluations / self.budget))  # Modified line: Adaptive scaling to mutation factor\n            mutant = population[a] + dynamic_F * (population[b] - population[c])\n            mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n            return mutant\n\n        while num_evaluations < self.budget:\n            self.CR = 0.9 - (0.3 * (num_evaluations / self.budget))\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                mutant = mutate(i)\n                trial = np.where(np.random.rand(self.dim) < self.CR, mutant, 0.5 * (population[i] + best_solution))\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = trial\n\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                neighbor = population[i] + np.random.normal(0, 0.1 * self.temp / 1000, self.dim)\n                neighbor = np.clip(neighbor, self.lower_bound, self.upper_bound)\n                neighbor_fitness = func(neighbor)\n                num_evaluations += 1\n                delta_fit = neighbor_fitness - fitness[i]\n\n                if delta_fit < 0 or np.random.rand() < np.exp(-delta_fit / self.temp):\n                    population[i] = neighbor\n                    fitness[i] = neighbor_fitness\n                    if neighbor_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = neighbor\n\n            self.temp *= (self.cooling_rate - 0.01 * (num_evaluations / self.budget))\n            self.pop_size = max(4, int(8 * self.dim * (1 - num_evaluations / self.budget)))\n\n        return best_solution", "name": "HybridDESA", "description": "Introduce adaptive scaling to mutation factor based on function evaluation ratio to improve exploitation capabilities.", "configspace": "", "generation": 50, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "7c2cdd96-a24a-4bdc-96df-83fbafae318e", "metadata": {"aucs": [0.5442606585864098, 0.5442606585864098, 0.5442606585864098, 0.5594667505839126, 0.5594667505839126, 0.5594667505839126, 0.5619670043110119, 0.5619670043110119, 0.5619670043110119, 0.21747217979971056, 0.21747217979971056, 0.21747217979971056, 0.301145005068842, 0.301145005068842, 0.301145005068842, 0.2701774299619817, 0.2701774299619817, 0.2701774299619817, 0.12380717926710627, 0.12380717926710627, 0.12380717926710627, 0.2707857807301146, 0.2707857807301146, 0.2707857807301146, 0.12434041080760361, 0.12434041080760361, 0.12434041080760361, 0.09990514563760045, 0.09990514563760045, 0.09990514563760045, 0.11400070825966435, 0.11400070825966435, 0.11400070825966435, 0.10067172072753439, 0.10067172072753439, 0.10067172072753439, 0.8661251149491918, 0.8661251149491918, 0.8661251149491918, 0.7290713983164432, 0.7290713983164432, 0.7290713983164432, 0.8436918710960647, 0.8436918710960647, 0.8436918710960647, 0.36712616727641867, 0.36712616727641867, 0.36712616727641867, 0.3636131714545058, 0.3636131714545058, 0.3636131714545058, 0.3659024547958438, 0.3659024547958438, 0.3659024547958438, 0.6310787403199133, 0.6310787403199133, 0.6310787403199133, 0.6768815727847995, 0.6768815727847995, 0.6768815727847995, 0.5994986583901672, 0.5994986583901672, 0.5994986583901672, 0.16395592167529593, 0.16395592167529593, 0.16395592167529593, 0.16248216297557505, 0.16248216297557505, 0.16248216297557505, 0.15316082201437908, 0.15316082201437908, 0.15316082201437908, 0.13422969468819068, 0.13422969468819068, 0.13422969468819068, 0.15994447909387, 0.15994447909387, 0.15994447909387, 0.17925306879841263, 0.17925306879841263, 0.17925306879841263, 0.11693011259923936, 0.11693011259923936, 0.11693011259923936, 0.13639781006130602, 0.13639781006130602, 0.13639781006130602, 0.0958292613408166, 0.0958292613408166, 0.0958292613408166, 0.10006033384570234, 0.10006033384570234, 0.10006033384570234, 0.1772039837170689, 0.1772039837170689, 0.1772039837170689, 0.12716325765900838, 0.12716325765900838, 0.12716325765900838, 0.05686359512133354, 0.05686359512133354, 0.05686359512133354, 0.11681310492106911, 0.11681310492106911, 0.11681310492106911, 0.15179585825375674, 0.15179585825375674, 0.15179585825375674, 0.10382739431942212, 0.10382739431942212, 0.10382739431942212, 0.14537585172280887, 0.14537585172280887, 0.14537585172280887, 0.15717930997441565, 0.15717930997441565, 0.15717930997441565, 0.4528669556728523, 0.4528669556728523, 0.4528669556728523, 0.458952743718657, 0.458952743718657, 0.458952743718657, 0.4620892965273501, 0.4620892965273501, 0.4620892965273501, 0.14033997840350254, 0.14033997840350254, 0.14033997840350254, 0.2682290973070668, 0.2682290973070668, 0.2682290973070668, 0.1490239265232367, 0.1490239265232367, 0.1490239265232367, 0.18401715372093796, 0.18401715372093796, 0.18401715372093796, 0.17696361157596752, 0.17696361157596752, 0.17696361157596752, 0.21923037373927678, 0.21923037373927678, 0.21923037373927678, 0.3303506444060237, 0.3303506444060237, 0.3303506444060237, 0.3324590475247433, 0.3324590475247433, 0.3324590475247433, 0.33682868711350433, 0.33682868711350433, 0.33682868711350433, 0.27600513773960433, 0.27600513773960433, 0.27600513773960433, 0.27728610841027, 0.27728610841027, 0.27728610841027, 0.2777719329986905, 0.2777719329986905, 0.2777719329986905, 0.2245954506033757, 0.2245954506033757, 0.2245954506033757, 0.2326601224038437, 0.2326601224038437, 0.2326601224038437, 0.2677386403983355, 0.2677386403983355, 0.2677386403983355, 0.17856169785671994, 0.17856169785671994, 0.17856169785671994, 0.1858441067890847, 0.1858441067890847, 0.1858441067890847, 0.16780382235241265, 0.16780382235241265, 0.16780382235241265, 0.7358333621066688, 0.7358333621066688, 0.7358333621066688, 0.15611265274709496, 0.15611265274709496, 0.15611265274709496, 0.15358931980043466, 0.15358931980043466, 0.15358931980043466, 0.20138745427599292, 0.20138745427599292, 0.20138745427599292, 0.19671548035487985, 0.19671548035487985, 0.19671548035487985, 0.2043569815436096, 0.2043569815436096, 0.2043569815436096, 0.20557195626092395, 0.20557195626092395, 0.20557195626092395, 0.20788996694542194, 0.20788996694542194, 0.20788996694542194, 0.19607587685120376, 0.19607587685120376, 0.19607587685120376, 0.09142243954669904, 0.09142243954669904, 0.09142243954669904, 0.09273474322018704, 0.09273474322018704, 0.09273474322018704, 0.09463479579497758, 0.09463479579497758, 0.09463479579497758]}, "mutation_prompt": null}
{"id": "6874c423-a073-4df5-be64-3802243bb457", "solution": "import numpy as np\n\nclass HybridDESA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.pop_size = 10 * dim\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.temp = 1000.0  # Initial temperature for simulated annealing\n        self.cooling_rate = 0.99  # Cooling rate for simulated annealing\n\n    def __call__(self, func):\n        np.random.seed(42)\n        population = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        num_evaluations = self.pop_size\n\n        def mutate(target_idx):\n            indices = [idx for idx in range(self.pop_size) if idx != target_idx]\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            dynamic_F = self.F * (1.0 - (num_evaluations / self.budget)) * np.exp(-num_evaluations / self.budget)\n            dynamic_F *= (self.temp / 1000.0) * (1 + 0.15 * (num_evaluations / self.budget))  # Modified line: Enhanced exploration adjustment\n            mutant = population[a] + dynamic_F * (population[b] - population[c])\n            mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n            return mutant\n\n        while num_evaluations < self.budget:\n            self.CR = 0.9 - (0.3 * (num_evaluations / self.budget))\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                mutant = mutate(i)\n                trial = np.where(np.random.rand(self.dim) < self.CR, mutant, 0.5 * (population[i] + best_solution))\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = trial\n\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                neighbor = population[i] + np.random.normal(0, 0.1 * self.temp / 1000, self.dim)\n                neighbor = np.clip(neighbor, self.lower_bound, self.upper_bound)\n                neighbor_fitness = func(neighbor)\n                num_evaluations += 1\n                delta_fit = neighbor_fitness - fitness[i]\n\n                if delta_fit < 0 or np.random.rand() < np.exp(-delta_fit / self.temp):\n                    population[i] = neighbor\n                    fitness[i] = neighbor_fitness\n                    if neighbor_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = neighbor\n\n            self.temp *= (self.cooling_rate - 0.01 * (num_evaluations / self.budget))\n            self.pop_size = max(4, int(8 * self.dim * (1 - num_evaluations / self.budget)))\n\n        return best_solution", "name": "HybridDESA", "description": "Enhance exploration by adjusting the dynamic scaling factor in mutation.", "configspace": "", "generation": 56, "fitness": 0.2632756519108384, "feedback": "The algorithm HybridDESA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.26 with standard deviation 0.18.", "error": "", "parent_id": "7c2cdd96-a24a-4bdc-96df-83fbafae318e", "metadata": {"aucs": [0.5382028361657692, 0.5382028361657692, 0.5382028361657692, 0.5725305641578575, 0.5725305641578575, 0.5725305641578575, 0.5500323883512265, 0.5500323883512265, 0.5500323883512265, 0.1110402586702649, 0.1110402586702649, 0.1110402586702649, 0.2790659868642007, 0.2790659868642007, 0.2790659868642007, 0.26237976892241965, 0.26237976892241965, 0.26237976892241965, 0.13720105134267946, 0.13720105134267946, 0.13720105134267946, 0.1392805616063999, 0.1392805616063999, 0.1392805616063999, 0.14474470585712862, 0.14474470585712862, 0.14474470585712862, 0.09831397849261425, 0.09831397849261425, 0.09831397849261425, 0.09997588885345998, 0.09997588885345998, 0.09997588885345998, 0.11519775563722245, 0.11519775563722245, 0.11519775563722245, 0.7723875288190702, 0.7723875288190702, 0.7723875288190702, 0.6888098953642192, 0.6888098953642192, 0.6888098953642192, 0.8483848572694973, 0.8483848572694973, 0.8483848572694973, 0.35602990259842104, 0.35602990259842104, 0.35602990259842104, 0.36463369087482556, 0.36463369087482556, 0.36463369087482556, 0.3327881826274728, 0.3327881826274728, 0.3327881826274728, 0.5960440163046553, 0.5960440163046553, 0.5960440163046553, 0.6685185794368904, 0.6685185794368904, 0.6685185794368904, 0.5226530361845401, 0.5226530361845401, 0.5226530361845401, 0.15400957929262316, 0.15400957929262316, 0.15400957929262316, 0.16100267135230917, 0.16100267135230917, 0.16100267135230917, 0.17916067802686142, 0.17916067802686142, 0.17916067802686142, 0.1118607390932046, 0.1118607390932046, 0.1118607390932046, 0.1590138142384081, 0.1590138142384081, 0.1590138142384081, 0.1714412404542729, 0.1714412404542729, 0.1714412404542729, 0.13272345190424295, 0.13272345190424295, 0.13272345190424295, 0.09777533292313034, 0.09777533292313034, 0.09777533292313034, 0.0957597080764645, 0.0957597080764645, 0.0957597080764645, 0.10965401334368252, 0.10965401334368252, 0.10965401334368252, 0.11386104210269388, 0.11386104210269388, 0.11386104210269388, 0.14159369355144935, 0.14159369355144935, 0.14159369355144935, 0.07245021880612634, 0.07245021880612634, 0.07245021880612634, 0.1059943955903796, 0.1059943955903796, 0.1059943955903796, 0.22396040320001886, 0.22396040320001886, 0.22396040320001886, 0.15790066208411213, 0.15790066208411213, 0.15790066208411213, 0.15362821777042968, 0.15362821777042968, 0.15362821777042968, 0.16830502516039592, 0.16830502516039592, 0.16830502516039592, 0.44625117124460834, 0.44625117124460834, 0.44625117124460834, 0.4373694748389797, 0.4373694748389797, 0.4373694748389797, 0.44277991024870933, 0.44277991024870933, 0.44277991024870933, 0.26820920952507277, 0.26820920952507277, 0.26820920952507277, 0.10828583532034974, 0.10828583532034974, 0.10828583532034974, 0.12346436538808025, 0.12346436538808025, 0.12346436538808025, 0.2811341838124104, 0.2811341838124104, 0.2811341838124104, 0.28296174891776793, 0.28296174891776793, 0.28296174891776793, 0.21853033071490668, 0.21853033071490668, 0.21853033071490668, 0.33331798257187306, 0.33331798257187306, 0.33331798257187306, 0.3422374250360969, 0.3422374250360969, 0.3422374250360969, 0.3337621456878057, 0.3337621456878057, 0.3337621456878057, 0.2830732156712181, 0.2830732156712181, 0.2830732156712181, 0.2711392323767734, 0.2711392323767734, 0.2711392323767734, 0.2822307481382379, 0.2822307481382379, 0.2822307481382379, 0.22926697781918637, 0.22926697781918637, 0.22926697781918637, 0.2229512277346365, 0.2229512277346365, 0.2229512277346365, 0.2102922145203665, 0.2102922145203665, 0.2102922145203665, 0.19833008704320443, 0.19833008704320443, 0.19833008704320443, 0.19864470508508236, 0.19864470508508236, 0.19864470508508236, 0.20222073147904196, 0.20222073147904196, 0.20222073147904196, 0.7174774102062611, 0.7174774102062611, 0.7174774102062611, 0.16020067052369613, 0.16020067052369613, 0.16020067052369613, 0.16596108295119827, 0.16596108295119827, 0.16596108295119827, 0.16607058159029364, 0.16607058159029364, 0.16607058159029364, 0.19669239502825409, 0.19669239502825409, 0.19669239502825409, 0.20493075778235836, 0.20493075778235836, 0.20493075778235836, 0.21113234056163188, 0.21113234056163188, 0.21113234056163188, 0.22458564919136148, 0.22458564919136148, 0.22458564919136148, 0.20372866649322963, 0.20372866649322963, 0.20372866649322963, 0.09112987336269307, 0.09112987336269307, 0.09112987336269307, 0.09193503862925023, 0.09193503862925023, 0.09193503862925023, 0.09723922671409002, 0.09723922671409002, 0.09723922671409002]}, "mutation_prompt": null}
{"id": "1cd08e7d-95f0-41ce-bc3d-1a95477c647a", "solution": "import numpy as np\n\nclass HybridDESA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.pop_size = 10 * dim\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.temp = 1000.0  # Initial temperature for simulated annealing\n        self.cooling_rate = 0.99  # Cooling rate for simulated annealing\n\n    def __call__(self, func):\n        np.random.seed(42)\n        population = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        num_evaluations = self.pop_size\n\n        def mutate(target_idx):\n            indices = [idx for idx in range(self.pop_size) if idx != target_idx]\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            dynamic_F = self.F * (1.0 - (num_evaluations / self.budget))**2 * np.exp(-num_evaluations / self.budget) # Modified line: Adjusted dynamic_F calculation for improved balance\n            dynamic_F *= (self.temp / 1000.0) * (1 + 0.1 * (num_evaluations / self.budget))\n            mutant = population[a] + dynamic_F * (population[b] - population[c])\n            mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n            return mutant\n\n        while num_evaluations < self.budget:\n            self.CR = 0.9 - (0.3 * (num_evaluations / self.budget))\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                mutant = mutate(i)\n                trial = np.where(np.random.rand(self.dim) < self.CR, mutant, 0.5 * (population[i] + best_solution))\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = trial\n\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                neighbor = population[i] + np.random.normal(0, 0.1 * self.temp / 1000, self.dim)\n                neighbor = np.clip(neighbor, self.lower_bound, self.upper_bound)\n                neighbor_fitness = func(neighbor)\n                num_evaluations += 1\n                delta_fit = neighbor_fitness - fitness[i]\n\n                if delta_fit < 0 or np.random.rand() < np.exp(-delta_fit / self.temp):\n                    population[i] = neighbor\n                    fitness[i] = neighbor_fitness\n                    if neighbor_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = neighbor\n\n            self.temp *= (self.cooling_rate - 0.01 * (num_evaluations / self.budget))\n            self.pop_size = max(4, int(8 * self.dim * (1 - num_evaluations / self.budget)))\n\n        return best_solution", "name": "HybridDESA", "description": "Tune mutation factor adaptation by incorporating exponential decay based on budget consumption for enhanced exploration-exploitation balance.", "configspace": "", "generation": 57, "fitness": 0.26398468896257604, "feedback": "The algorithm HybridDESA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.26 with standard deviation 0.19.", "error": "", "parent_id": "7c2cdd96-a24a-4bdc-96df-83fbafae318e", "metadata": {"aucs": [0.5451892282860127, 0.5451892282860127, 0.5451892282860127, 0.5472786572672239, 0.5472786572672239, 0.5472786572672239, 0.5480778959104624, 0.5480778959104624, 0.5480778959104624, 0.22627797586989695, 0.22627797586989695, 0.22627797586989695, 0.2277564485719712, 0.2277564485719712, 0.2277564485719712, 0.38164803556954385, 0.38164803556954385, 0.38164803556954385, 0.1561705340953553, 0.1561705340953553, 0.1561705340953553, 0.10560498018790165, 0.10560498018790165, 0.10560498018790165, 0.12177103326990635, 0.12177103326990635, 0.12177103326990635, 0.09697473665419087, 0.09697473665419087, 0.09697473665419087, 0.10656029302476977, 0.10656029302476977, 0.10656029302476977, 0.09988176930706427, 0.09988176930706427, 0.09988176930706427, 0.851967862957562, 0.851967862957562, 0.851967862957562, 0.7056791716659887, 0.7056791716659887, 0.7056791716659887, 0.7765444948843357, 0.7765444948843357, 0.7765444948843357, 0.36392694918230695, 0.36392694918230695, 0.36392694918230695, 0.3542000261529863, 0.3542000261529863, 0.3542000261529863, 0.3560885253714986, 0.3560885253714986, 0.3560885253714986, 0.6286169595168118, 0.6286169595168118, 0.6286169595168118, 0.7278185951631868, 0.7278185951631868, 0.7278185951631868, 0.7007777709365774, 0.7007777709365774, 0.7007777709365774, 0.14803469077473896, 0.14803469077473896, 0.14803469077473896, 0.1706874557057888, 0.1706874557057888, 0.1706874557057888, 0.16461533797172523, 0.16461533797172523, 0.16461533797172523, 0.14562406586092969, 0.14562406586092969, 0.14562406586092969, 0.16836494681629233, 0.16836494681629233, 0.16836494681629233, 0.1477427093340633, 0.1477427093340633, 0.1477427093340633, 0.06795599350564385, 0.06795599350564385, 0.06795599350564385, 0.03534788637109265, 0.03534788637109265, 0.03534788637109265, 0.0504699443606742, 0.0504699443606742, 0.0504699443606742, 0.10762443275547562, 0.10762443275547562, 0.10762443275547562, 0.18492106626881455, 0.18492106626881455, 0.18492106626881455, 0.1722883704827065, 0.1722883704827065, 0.1722883704827065, 0.08726264473318601, 0.08726264473318601, 0.08726264473318601, 0.204885752231308, 0.204885752231308, 0.204885752231308, 0.11571793287149412, 0.11571793287149412, 0.11571793287149412, 0.13835444138802755, 0.13835444138802755, 0.13835444138802755, 0.14941367638838143, 0.14941367638838143, 0.14941367638838143, 0.1339264547747414, 0.1339264547747414, 0.1339264547747414, 0.4504736206263743, 0.4504736206263743, 0.4504736206263743, 0.4686852971202802, 0.4686852971202802, 0.4686852971202802, 0.4393467334742165, 0.4393467334742165, 0.4393467334742165, 0.11100266599081077, 0.11100266599081077, 0.11100266599081077, 0.1412008499768337, 0.1412008499768337, 0.1412008499768337, 0.1279144548113561, 0.1279144548113561, 0.1279144548113561, 0.21499363660298565, 0.21499363660298565, 0.21499363660298565, 0.1795685981532531, 0.1795685981532531, 0.1795685981532531, 0.25677229503830745, 0.25677229503830745, 0.25677229503830745, 0.3348024240053121, 0.3348024240053121, 0.3348024240053121, 0.3284217613788185, 0.3284217613788185, 0.3284217613788185, 0.3407124154159312, 0.3407124154159312, 0.3407124154159312, 0.2709987195976866, 0.2709987195976866, 0.2709987195976866, 0.2139008108168955, 0.2139008108168955, 0.2139008108168955, 0.2759428096891442, 0.2759428096891442, 0.2759428096891442, 0.23990956445650913, 0.23990956445650913, 0.23990956445650913, 0.22389243265736192, 0.22389243265736192, 0.22389243265736192, 0.24819714504338342, 0.24819714504338342, 0.24819714504338342, 0.17897763647146148, 0.17897763647146148, 0.17897763647146148, 0.18398603705440586, 0.18398603705440586, 0.18398603705440586, 0.1882489473609692, 0.1882489473609692, 0.1882489473609692, 0.7701990774531032, 0.7701990774531032, 0.7701990774531032, 0.15664372245690517, 0.15664372245690517, 0.15664372245690517, 0.14976516285281882, 0.14976516285281882, 0.14976516285281882, 0.1646966114403674, 0.1646966114403674, 0.1646966114403674, 0.1962595151908375, 0.1962595151908375, 0.1962595151908375, 0.23305126331807435, 0.23305126331807435, 0.23305126331807435, 0.19504101827098475, 0.19504101827098475, 0.19504101827098475, 0.207570582408094, 0.207570582408094, 0.207570582408094, 0.20652613224718208, 0.20652613224718208, 0.20652613224718208, 0.0945594448226107, 0.0945594448226107, 0.0945594448226107, 0.09560792228793413, 0.09560792228793413, 0.09560792228793413, 0.09697855037362646, 0.09697855037362646, 0.09697855037362646]}, "mutation_prompt": null}
{"id": "e44c9cff-8a6b-4f92-bf4c-9b97ee9489eb", "solution": "import numpy as np\n\nclass HybridDESA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.pop_size = 10 * dim\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.temp = 1000.0  # Initial temperature for simulated annealing\n        self.cooling_rate = 0.99  # Cooling rate for simulated annealing\n\n    def __call__(self, func):\n        np.random.seed(42)\n        population = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        num_evaluations = self.pop_size\n\n        def mutate(target_idx):\n            indices = [idx for idx in range(self.pop_size) if idx != target_idx]\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            dynamic_F = self.F * (1.0 - (num_evaluations / self.budget)) * np.exp(-num_evaluations / self.budget)\n            dynamic_F *= (self.temp / 1000.0) * (1 + 0.1 * (num_evaluations / self.budget))  # Modified line: Adaptive scaling to mutation factor\n            mutant = population[a] + dynamic_F * (population[b] - population[c])\n            mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n            return mutant\n\n        while num_evaluations < self.budget:\n            self.CR = 0.9 - (0.3 * (num_evaluations / self.budget)) - 0.1 * ((fitness[best_idx] - np.mean(fitness)) / (np.std(fitness) + 1e-9))  # Line changed: Added fitness improvement impact on CR\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                mutant = mutate(i)\n                trial = np.where(np.random.rand(self.dim) < self.CR, mutant, 0.5 * (population[i] + best_solution))\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = trial\n\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                neighbor = population[i] + np.random.normal(0, 0.1 * self.temp / 1000, self.dim)\n                neighbor = np.clip(neighbor, self.lower_bound, self.upper_bound)\n                neighbor_fitness = func(neighbor)\n                num_evaluations += 1\n                delta_fit = neighbor_fitness - fitness[i]\n\n                if delta_fit < 0 or np.random.rand() < np.exp(-delta_fit / self.temp):\n                    population[i] = neighbor\n                    fitness[i] = neighbor_fitness\n                    if neighbor_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = neighbor\n\n            self.temp *= (self.cooling_rate - 0.01 * (num_evaluations / self.budget))\n            self.pop_size = max(4, int(8 * self.dim * (1 - num_evaluations / self.budget)))\n\n        return best_solution", "name": "HybridDESA", "description": "Enhance exploration by adjusting crossover probability dynamically based on the fitness improvement rate.", "configspace": "", "generation": 58, "fitness": 0.2445924374990595, "feedback": "The algorithm HybridDESA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.24 with standard deviation 0.16.", "error": "", "parent_id": "7c2cdd96-a24a-4bdc-96df-83fbafae318e", "metadata": {"aucs": [0.5378142448695324, 0.5378142448695324, 0.5378142448695324, 0.5391421619207917, 0.5391421619207917, 0.5391421619207917, 0.5459985080152487, 0.5459985080152487, 0.5459985080152487, 0.23840726180053995, 0.23840726180053995, 0.23840726180053995, 0.3111238790505597, 0.3111238790505597, 0.3111238790505597, 0.27351479356379127, 0.27351479356379127, 0.27351479356379127, 0.15204174673866688, 0.15204174673866688, 0.15204174673866688, 0.10402314939136026, 0.10402314939136026, 0.10402314939136026, 0.17909685265572028, 0.17909685265572028, 0.17909685265572028, 0.10145352048802092, 0.10145352048802092, 0.10145352048802092, 0.12431309295807713, 0.12431309295807713, 0.12431309295807713, 0.09114411871538919, 0.09114411871538919, 0.09114411871538919, 0.7135405058493669, 0.7135405058493669, 0.7135405058493669, 0.858109864109402, 0.858109864109402, 0.858109864109402, 0.7084210693931847, 0.7084210693931847, 0.7084210693931847, 0.35464106827374475, 0.35464106827374475, 0.35464106827374475, 0.34857382030602024, 0.34857382030602024, 0.34857382030602024, 0.35541552985474123, 0.35541552985474123, 0.35541552985474123, 0.5673389872377372, 0.5673389872377372, 0.5673389872377372, 0.20933802750748498, 0.20933802750748498, 0.20933802750748498, 0.24195175519925316, 0.24195175519925316, 0.24195175519925316, 0.18162710008805039, 0.18162710008805039, 0.18162710008805039, 0.1170617904840614, 0.1170617904840614, 0.1170617904840614, 0.16205691534138333, 0.16205691534138333, 0.16205691534138333, 0.12006009160411335, 0.12006009160411335, 0.12006009160411335, 0.16529445160992307, 0.16529445160992307, 0.16529445160992307, 0.1673800325330631, 0.1673800325330631, 0.1673800325330631, 0.16547950584665527, 0.16547950584665527, 0.16547950584665527, 0.11346298528229803, 0.11346298528229803, 0.11346298528229803, 0.1776044336631576, 0.1776044336631576, 0.1776044336631576, 0.1481841426878895, 0.1481841426878895, 0.1481841426878895, 0.12507567282228615, 0.12507567282228615, 0.12507567282228615, 0.13634996164626023, 0.13634996164626023, 0.13634996164626023, 0.05498826166538018, 0.05498826166538018, 0.05498826166538018, 0.10296749813263939, 0.10296749813263939, 0.10296749813263939, 0.15214470579988992, 0.15214470579988992, 0.15214470579988992, 0.15415296378660226, 0.15415296378660226, 0.15415296378660226, 0.14404382511253588, 0.14404382511253588, 0.14404382511253588, 0.12948267870050167, 0.12948267870050167, 0.12948267870050167, 0.45094734217912114, 0.45094734217912114, 0.45094734217912114, 0.45976565140304915, 0.45976565140304915, 0.45976565140304915, 0.4435419478130054, 0.4435419478130054, 0.4435419478130054, 0.13681350012028526, 0.13681350012028526, 0.13681350012028526, 0.11985282994878743, 0.11985282994878743, 0.11985282994878743, 0.13224297126542228, 0.13224297126542228, 0.13224297126542228, 0.28171867005315077, 0.28171867005315077, 0.28171867005315077, 0.24262805714918267, 0.24262805714918267, 0.24262805714918267, 0.19578287004564954, 0.19578287004564954, 0.19578287004564954, 0.33886384512614054, 0.33886384512614054, 0.33886384512614054, 0.3306799729249503, 0.3306799729249503, 0.3306799729249503, 0.33860145063185954, 0.33860145063185954, 0.33860145063185954, 0.2718221270110729, 0.2718221270110729, 0.2718221270110729, 0.27345181353705394, 0.27345181353705394, 0.27345181353705394, 0.21316063622754777, 0.21316063622754777, 0.21316063622754777, 0.2164905252592655, 0.2164905252592655, 0.2164905252592655, 0.2527543394213666, 0.2527543394213666, 0.2527543394213666, 0.24973235774182356, 0.24973235774182356, 0.24973235774182356, 0.22221725076861765, 0.22221725076861765, 0.22221725076861765, 0.1863808886770545, 0.1863808886770545, 0.1863808886770545, 0.17187441323798613, 0.17187441323798613, 0.17187441323798613, 0.1848423223087451, 0.1848423223087451, 0.1848423223087451, 0.15342873628742915, 0.15342873628742915, 0.15342873628742915, 0.183202870060574, 0.183202870060574, 0.183202870060574, 0.16744353412206803, 0.16744353412206803, 0.16744353412206803, 0.18831819180939702, 0.18831819180939702, 0.18831819180939702, 0.21661953502917708, 0.21661953502917708, 0.21661953502917708, 0.2135886850760097, 0.2135886850760097, 0.2135886850760097, 0.2001704250902976, 0.2001704250902976, 0.2001704250902976, 0.19953342639672444, 0.19953342639672444, 0.19953342639672444, 0.11282652480668198, 0.11282652480668198, 0.11282652480668198, 0.09291427865426471, 0.09291427865426471, 0.09291427865426471, 0.09562253504319962, 0.09562253504319962, 0.09562253504319962]}, "mutation_prompt": null}
{"id": "57c2d366-b840-4316-81ac-9ea3e22becb2", "solution": "import numpy as np\n\nclass HybridDESA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.pop_size = 10 * dim\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.temp = 1000.0  # Initial temperature for simulated annealing\n        self.cooling_rate = 0.99  # Cooling rate for simulated annealing\n\n    def __call__(self, func):\n        np.random.seed(42)\n        population = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        num_evaluations = self.pop_size\n\n        def mutate(target_idx):\n            indices = [idx for idx in range(self.pop_size) if idx != target_idx]\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            dynamic_F = self.F * (1.0 - (num_evaluations / self.budget)) * np.exp(-num_evaluations / self.budget)\n            dynamic_F *= (self.temp / 1000.0) * (1 + 0.1 * (num_evaluations / self.budget))  # Modified line: Adaptive scaling to mutation factor\n            mutant = population[a] + dynamic_F * (population[b] - population[c])\n            mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n            return mutant\n\n        while num_evaluations < self.budget:\n            self.CR = 0.9 - (0.3 * (num_evaluations / self.budget))\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                mutant = mutate(i)\n                trial = np.where(np.random.rand(self.dim) < self.CR, mutant, 0.5 * (population[i] + best_solution))\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = trial\n\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                neighbor = population[i] + np.random.normal(0, 0.1 * self.temp / 1000, self.dim)\n                neighbor = np.clip(neighbor, self.lower_bound, self.upper_bound)\n                neighbor_fitness = func(neighbor)\n                num_evaluations += 1\n                delta_fit = neighbor_fitness - fitness[i]\n\n                if delta_fit < 0 or np.random.rand() < np.exp(-delta_fit / self.temp):\n                    population[i] = neighbor\n                    fitness[i] = neighbor_fitness\n                    if neighbor_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = neighbor\n\n            self.temp *= (self.cooling_rate - 0.01 * (num_evaluations / self.budget))\n            self.pop_size = max(4, int(8 * self.dim * (1 - num_evaluations / self.budget)))\n\n        return best_solution", "name": "HybridDESA", "description": "Introduce adaptive scaling to mutation factor based on function evaluation ratio to improve exploitation capabilities.", "configspace": "", "generation": 50, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "7c2cdd96-a24a-4bdc-96df-83fbafae318e", "metadata": {"aucs": [0.5442606585864098, 0.5442606585864098, 0.5442606585864098, 0.5594667505839126, 0.5594667505839126, 0.5594667505839126, 0.5619670043110119, 0.5619670043110119, 0.5619670043110119, 0.21747217979971056, 0.21747217979971056, 0.21747217979971056, 0.301145005068842, 0.301145005068842, 0.301145005068842, 0.2701774299619817, 0.2701774299619817, 0.2701774299619817, 0.12380717926710627, 0.12380717926710627, 0.12380717926710627, 0.2707857807301146, 0.2707857807301146, 0.2707857807301146, 0.12434041080760361, 0.12434041080760361, 0.12434041080760361, 0.09990514563760045, 0.09990514563760045, 0.09990514563760045, 0.11400070825966435, 0.11400070825966435, 0.11400070825966435, 0.10067172072753439, 0.10067172072753439, 0.10067172072753439, 0.8661251149491918, 0.8661251149491918, 0.8661251149491918, 0.7290713983164432, 0.7290713983164432, 0.7290713983164432, 0.8436918710960647, 0.8436918710960647, 0.8436918710960647, 0.36712616727641867, 0.36712616727641867, 0.36712616727641867, 0.3636131714545058, 0.3636131714545058, 0.3636131714545058, 0.3659024547958438, 0.3659024547958438, 0.3659024547958438, 0.6310787403199133, 0.6310787403199133, 0.6310787403199133, 0.6768815727847995, 0.6768815727847995, 0.6768815727847995, 0.5994986583901672, 0.5994986583901672, 0.5994986583901672, 0.16395592167529593, 0.16395592167529593, 0.16395592167529593, 0.16248216297557505, 0.16248216297557505, 0.16248216297557505, 0.15316082201437908, 0.15316082201437908, 0.15316082201437908, 0.13422969468819068, 0.13422969468819068, 0.13422969468819068, 0.15994447909387, 0.15994447909387, 0.15994447909387, 0.17925306879841263, 0.17925306879841263, 0.17925306879841263, 0.11693011259923936, 0.11693011259923936, 0.11693011259923936, 0.13639781006130602, 0.13639781006130602, 0.13639781006130602, 0.0958292613408166, 0.0958292613408166, 0.0958292613408166, 0.10006033384570234, 0.10006033384570234, 0.10006033384570234, 0.1772039837170689, 0.1772039837170689, 0.1772039837170689, 0.12716325765900838, 0.12716325765900838, 0.12716325765900838, 0.05686359512133354, 0.05686359512133354, 0.05686359512133354, 0.11681310492106911, 0.11681310492106911, 0.11681310492106911, 0.15179585825375674, 0.15179585825375674, 0.15179585825375674, 0.10382739431942212, 0.10382739431942212, 0.10382739431942212, 0.14537585172280887, 0.14537585172280887, 0.14537585172280887, 0.15717930997441565, 0.15717930997441565, 0.15717930997441565, 0.4528669556728523, 0.4528669556728523, 0.4528669556728523, 0.458952743718657, 0.458952743718657, 0.458952743718657, 0.4620892965273501, 0.4620892965273501, 0.4620892965273501, 0.14033997840350254, 0.14033997840350254, 0.14033997840350254, 0.2682290973070668, 0.2682290973070668, 0.2682290973070668, 0.1490239265232367, 0.1490239265232367, 0.1490239265232367, 0.18401715372093796, 0.18401715372093796, 0.18401715372093796, 0.17696361157596752, 0.17696361157596752, 0.17696361157596752, 0.21923037373927678, 0.21923037373927678, 0.21923037373927678, 0.3303506444060237, 0.3303506444060237, 0.3303506444060237, 0.3324590475247433, 0.3324590475247433, 0.3324590475247433, 0.33682868711350433, 0.33682868711350433, 0.33682868711350433, 0.27600513773960433, 0.27600513773960433, 0.27600513773960433, 0.27728610841027, 0.27728610841027, 0.27728610841027, 0.2777719329986905, 0.2777719329986905, 0.2777719329986905, 0.2245954506033757, 0.2245954506033757, 0.2245954506033757, 0.2326601224038437, 0.2326601224038437, 0.2326601224038437, 0.2677386403983355, 0.2677386403983355, 0.2677386403983355, 0.17856169785671994, 0.17856169785671994, 0.17856169785671994, 0.1858441067890847, 0.1858441067890847, 0.1858441067890847, 0.16780382235241265, 0.16780382235241265, 0.16780382235241265, 0.7358333621066688, 0.7358333621066688, 0.7358333621066688, 0.15611265274709496, 0.15611265274709496, 0.15611265274709496, 0.15358931980043466, 0.15358931980043466, 0.15358931980043466, 0.20138745427599292, 0.20138745427599292, 0.20138745427599292, 0.19671548035487985, 0.19671548035487985, 0.19671548035487985, 0.2043569815436096, 0.2043569815436096, 0.2043569815436096, 0.20557195626092395, 0.20557195626092395, 0.20557195626092395, 0.20788996694542194, 0.20788996694542194, 0.20788996694542194, 0.19607587685120376, 0.19607587685120376, 0.19607587685120376, 0.09142243954669904, 0.09142243954669904, 0.09142243954669904, 0.09273474322018704, 0.09273474322018704, 0.09273474322018704, 0.09463479579497758, 0.09463479579497758, 0.09463479579497758]}, "mutation_prompt": null}
{"id": "ef8c6814-a40e-4a66-bc4d-2bc80b177f19", "solution": "import numpy as np\n\nclass HybridDESA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.pop_size = 10 * dim\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.temp = 1000.0  # Initial temperature for simulated annealing\n        self.cooling_rate = 0.99  # Cooling rate for simulated annealing\n\n    def __call__(self, func):\n        np.random.seed(42)\n        population = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        num_evaluations = self.pop_size\n\n        def mutate(target_idx):\n            indices = [idx for idx in range(self.pop_size) if idx != target_idx]\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            dynamic_F = self.F * (1.0 - (num_evaluations / self.budget)) * np.exp(-num_evaluations / self.budget)\n            dynamic_F *= (self.temp / 1000.0) * (1 + 0.1 * (num_evaluations / self.budget))\n            mutant = population[a] + dynamic_F * (population[b] - population[c])\n            mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n            return mutant\n\n        while num_evaluations < self.budget:\n            self.CR = 0.9 - (0.3 * (num_evaluations / self.budget))\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                mutant = mutate(i)\n                random_vector = np.random.uniform(self.lower_bound, self.upper_bound, self.dim)\n                trial = np.where(np.random.rand(self.dim) < self.CR, mutant, 0.3 * (population[i] + best_solution + random_vector))  # Modified line: Include random vector for trial blending\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = trial\n\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                neighbor = population[i] + np.random.normal(0, 0.1 * self.temp / 1000, self.dim)\n                neighbor = np.clip(neighbor, self.lower_bound, self.upper_bound)\n                neighbor_fitness = func(neighbor)\n                num_evaluations += 1\n                delta_fit = neighbor_fitness - fitness[i]\n\n                if delta_fit < 0 or np.random.rand() < np.exp(-delta_fit / self.temp):\n                    population[i] = neighbor\n                    fitness[i] = neighbor_fitness\n                    if neighbor_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = neighbor\n\n            self.temp *= (self.cooling_rate - 0.01 * (num_evaluations / self.budget))\n            self.pop_size = max(4, int(8 * self.dim * (1 - num_evaluations / self.budget)))\n\n        return best_solution", "name": "HybridDESA", "description": "Increase exploration by modifying the trial vector blending strategy to incorporate a random vector in addition to the mutant and current best solutions.", "configspace": "", "generation": 60, "fitness": 0.20826680341235443, "feedback": "The algorithm HybridDESA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.21 with standard deviation 0.13.", "error": "", "parent_id": "7c2cdd96-a24a-4bdc-96df-83fbafae318e", "metadata": {"aucs": [0.4496776030901276, 0.4496776030901276, 0.4496776030901276, 0.41561790530655485, 0.41561790530655485, 0.41561790530655485, 0.44208499946273183, 0.44208499946273183, 0.44208499946273183, 0.1501073529810446, 0.1501073529810446, 0.1501073529810446, 0.14290258209641904, 0.14290258209641904, 0.14290258209641904, 0.1598742748557832, 0.1598742748557832, 0.1598742748557832, 0.13382460672329688, 0.13382460672329688, 0.13382460672329688, 0.1288391517670192, 0.1288391517670192, 0.1288391517670192, 0.12699597805973917, 0.12699597805973917, 0.12699597805973917, 0.10235556889879827, 0.10235556889879827, 0.10235556889879827, 0.11026287272909518, 0.11026287272909518, 0.11026287272909518, 0.1034057134788392, 0.1034057134788392, 0.1034057134788392, 0.25954901083835813, 0.25954901083835813, 0.25954901083835813, 0.7352675983427804, 0.7352675983427804, 0.7352675983427804, 0.1964959658227493, 0.1964959658227493, 0.1964959658227493, 0.22847536044770145, 0.22847536044770145, 0.22847536044770145, 0.21937565785242685, 0.21937565785242685, 0.21937565785242685, 0.2351943111493725, 0.2351943111493725, 0.2351943111493725, 0.2581724223669626, 0.2581724223669626, 0.2581724223669626, 0.27930393204480375, 0.27930393204480375, 0.27930393204480375, 0.32393312813203623, 0.32393312813203623, 0.32393312813203623, 0.13848698348543653, 0.13848698348543653, 0.13848698348543653, 0.13584131504358976, 0.13584131504358976, 0.13584131504358976, 0.14001320812388496, 0.14001320812388496, 0.14001320812388496, 0.13075841085652717, 0.13075841085652717, 0.13075841085652717, 0.1403761376870689, 0.1403761376870689, 0.1403761376870689, 0.17086779522686846, 0.17086779522686846, 0.17086779522686846, 0.06108116306333378, 0.06108116306333378, 0.06108116306333378, 0.06549870438181393, 0.06549870438181393, 0.06549870438181393, 0.10764522904909668, 0.10764522904909668, 0.10764522904909668, 0.11638443704273727, 0.11638443704273727, 0.11638443704273727, 0.13200256213675565, 0.13200256213675565, 0.13200256213675565, 0.15409508058744759, 0.15409508058744759, 0.15409508058744759, 0.0388832005738855, 0.0388832005738855, 0.0388832005738855, 0.0850434014215502, 0.0850434014215502, 0.0850434014215502, 0.057993899492309375, 0.057993899492309375, 0.057993899492309375, 0.09411765299999475, 0.09411765299999475, 0.09411765299999475, 0.10425993859313931, 0.10425993859313931, 0.10425993859313931, 0.08662856134912678, 0.08662856134912678, 0.08662856134912678, 0.40051687008214176, 0.40051687008214176, 0.40051687008214176, 0.4076761210639359, 0.4076761210639359, 0.4076761210639359, 0.38864667738066705, 0.38864667738066705, 0.38864667738066705, 0.09539852321894682, 0.09539852321894682, 0.09539852321894682, 0.09503541701247853, 0.09503541701247853, 0.09503541701247853, 0.11946704000291408, 0.11946704000291408, 0.11946704000291408, 0.2528957563087214, 0.2528957563087214, 0.2528957563087214, 0.1829247701977793, 0.1829247701977793, 0.1829247701977793, 0.16953504004262254, 0.16953504004262254, 0.16953504004262254, 0.2641472141078032, 0.2641472141078032, 0.2641472141078032, 0.277347248201238, 0.277347248201238, 0.277347248201238, 0.2897625392755774, 0.2897625392755774, 0.2897625392755774, 0.2326343804105251, 0.2326343804105251, 0.2326343804105251, 0.22220176586181428, 0.22220176586181428, 0.22220176586181428, 0.2174681123532386, 0.2174681123532386, 0.2174681123532386, 0.22884358360515766, 0.22884358360515766, 0.22884358360515766, 0.22350348533721653, 0.22350348533721653, 0.22350348533721653, 0.2188502250715656, 0.2188502250715656, 0.2188502250715656, 0.1932543628468928, 0.1932543628468928, 0.1932543628468928, 0.23542660171232155, 0.23542660171232155, 0.23542660171232155, 0.18312492418503323, 0.18312492418503323, 0.18312492418503323, 0.5342923839435229, 0.5342923839435229, 0.5342923839435229, 0.43746912401982074, 0.43746912401982074, 0.43746912401982074, 0.5210946017168772, 0.5210946017168772, 0.5210946017168772, 0.16231929109180676, 0.16231929109180676, 0.16231929109180676, 0.27804428568695616, 0.27804428568695616, 0.27804428568695616, 0.2009559964775397, 0.2009559964775397, 0.2009559964775397, 0.18222297365123108, 0.18222297365123108, 0.18222297365123108, 0.18290040125091245, 0.18290040125091245, 0.18290040125091245, 0.1898437114747601, 0.1898437114747601, 0.1898437114747601, 0.07604569149635243, 0.07604569149635243, 0.07604569149635243, 0.09203963958929395, 0.09203963958929395, 0.09203963958929395, 0.07762740945064739, 0.07762740945064739, 0.07762740945064739]}, "mutation_prompt": null}
{"id": "4b826ffb-8595-4be7-b182-5ad83abe1f15", "solution": "import numpy as np\n\nclass HybridDESA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.pop_size = 10 * dim\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.temp = 1000.0  # Initial temperature for simulated annealing\n        self.cooling_rate = 0.99  # Cooling rate for simulated annealing\n\n    def __call__(self, func):\n        np.random.seed(42)\n        population = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        num_evaluations = self.pop_size\n\n        def mutate(target_idx):\n            indices = [idx for idx in range(self.pop_size) if idx != target_idx]\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            dynamic_F = self.F * (1.0 - (num_evaluations / self.budget)) * np.exp(-num_evaluations / self.budget)\n            dynamic_F *= (self.temp / 1000.0) * (1 + 0.1 * (num_evaluations / self.budget))  # Modified line: Adaptive scaling to mutation factor\n            mutant = population[a] + dynamic_F * (population[b] - population[c])\n            mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n            return mutant\n\n        while num_evaluations < self.budget:\n            self.CR = 0.9 - (0.3 * (num_evaluations / self.budget))\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                mutant = mutate(i)\n                trial = np.where(np.random.rand(self.dim) < self.CR, mutant, 0.5 * (population[i] + best_solution))\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = trial\n\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                neighbor = population[i] + np.random.normal(0, 0.1 * self.temp / 1000, self.dim)\n                neighbor = np.clip(neighbor, self.lower_bound, self.upper_bound)\n                neighbor_fitness = func(neighbor)\n                num_evaluations += 1\n                delta_fit = neighbor_fitness - fitness[i]\n\n                if delta_fit < 0 or np.random.rand() < np.exp(-delta_fit / self.temp):\n                    population[i] = neighbor\n                    fitness[i] = neighbor_fitness\n                    if neighbor_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = neighbor\n\n            self.temp *= (self.cooling_rate - 0.01 * (num_evaluations / self.budget))\n            self.pop_size = max(4, int(8 * self.dim * (1 - num_evaluations / self.budget)))\n\n        return best_solution", "name": "HybridDESA", "description": "Introduce adaptive scaling to mutation factor based on function evaluation ratio to improve exploitation capabilities.", "configspace": "", "generation": 50, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "7c2cdd96-a24a-4bdc-96df-83fbafae318e", "metadata": {"aucs": [0.5442606585864098, 0.5442606585864098, 0.5442606585864098, 0.5594667505839126, 0.5594667505839126, 0.5594667505839126, 0.5619670043110119, 0.5619670043110119, 0.5619670043110119, 0.21747217979971056, 0.21747217979971056, 0.21747217979971056, 0.301145005068842, 0.301145005068842, 0.301145005068842, 0.2701774299619817, 0.2701774299619817, 0.2701774299619817, 0.12380717926710627, 0.12380717926710627, 0.12380717926710627, 0.2707857807301146, 0.2707857807301146, 0.2707857807301146, 0.12434041080760361, 0.12434041080760361, 0.12434041080760361, 0.09990514563760045, 0.09990514563760045, 0.09990514563760045, 0.11400070825966435, 0.11400070825966435, 0.11400070825966435, 0.10067172072753439, 0.10067172072753439, 0.10067172072753439, 0.8661251149491918, 0.8661251149491918, 0.8661251149491918, 0.7290713983164432, 0.7290713983164432, 0.7290713983164432, 0.8436918710960647, 0.8436918710960647, 0.8436918710960647, 0.36712616727641867, 0.36712616727641867, 0.36712616727641867, 0.3636131714545058, 0.3636131714545058, 0.3636131714545058, 0.3659024547958438, 0.3659024547958438, 0.3659024547958438, 0.6310787403199133, 0.6310787403199133, 0.6310787403199133, 0.6768815727847995, 0.6768815727847995, 0.6768815727847995, 0.5994986583901672, 0.5994986583901672, 0.5994986583901672, 0.16395592167529593, 0.16395592167529593, 0.16395592167529593, 0.16248216297557505, 0.16248216297557505, 0.16248216297557505, 0.15316082201437908, 0.15316082201437908, 0.15316082201437908, 0.13422969468819068, 0.13422969468819068, 0.13422969468819068, 0.15994447909387, 0.15994447909387, 0.15994447909387, 0.17925306879841263, 0.17925306879841263, 0.17925306879841263, 0.11693011259923936, 0.11693011259923936, 0.11693011259923936, 0.13639781006130602, 0.13639781006130602, 0.13639781006130602, 0.0958292613408166, 0.0958292613408166, 0.0958292613408166, 0.10006033384570234, 0.10006033384570234, 0.10006033384570234, 0.1772039837170689, 0.1772039837170689, 0.1772039837170689, 0.12716325765900838, 0.12716325765900838, 0.12716325765900838, 0.05686359512133354, 0.05686359512133354, 0.05686359512133354, 0.11681310492106911, 0.11681310492106911, 0.11681310492106911, 0.15179585825375674, 0.15179585825375674, 0.15179585825375674, 0.10382739431942212, 0.10382739431942212, 0.10382739431942212, 0.14537585172280887, 0.14537585172280887, 0.14537585172280887, 0.15717930997441565, 0.15717930997441565, 0.15717930997441565, 0.4528669556728523, 0.4528669556728523, 0.4528669556728523, 0.458952743718657, 0.458952743718657, 0.458952743718657, 0.4620892965273501, 0.4620892965273501, 0.4620892965273501, 0.14033997840350254, 0.14033997840350254, 0.14033997840350254, 0.2682290973070668, 0.2682290973070668, 0.2682290973070668, 0.1490239265232367, 0.1490239265232367, 0.1490239265232367, 0.18401715372093796, 0.18401715372093796, 0.18401715372093796, 0.17696361157596752, 0.17696361157596752, 0.17696361157596752, 0.21923037373927678, 0.21923037373927678, 0.21923037373927678, 0.3303506444060237, 0.3303506444060237, 0.3303506444060237, 0.3324590475247433, 0.3324590475247433, 0.3324590475247433, 0.33682868711350433, 0.33682868711350433, 0.33682868711350433, 0.27600513773960433, 0.27600513773960433, 0.27600513773960433, 0.27728610841027, 0.27728610841027, 0.27728610841027, 0.2777719329986905, 0.2777719329986905, 0.2777719329986905, 0.2245954506033757, 0.2245954506033757, 0.2245954506033757, 0.2326601224038437, 0.2326601224038437, 0.2326601224038437, 0.2677386403983355, 0.2677386403983355, 0.2677386403983355, 0.17856169785671994, 0.17856169785671994, 0.17856169785671994, 0.1858441067890847, 0.1858441067890847, 0.1858441067890847, 0.16780382235241265, 0.16780382235241265, 0.16780382235241265, 0.7358333621066688, 0.7358333621066688, 0.7358333621066688, 0.15611265274709496, 0.15611265274709496, 0.15611265274709496, 0.15358931980043466, 0.15358931980043466, 0.15358931980043466, 0.20138745427599292, 0.20138745427599292, 0.20138745427599292, 0.19671548035487985, 0.19671548035487985, 0.19671548035487985, 0.2043569815436096, 0.2043569815436096, 0.2043569815436096, 0.20557195626092395, 0.20557195626092395, 0.20557195626092395, 0.20788996694542194, 0.20788996694542194, 0.20788996694542194, 0.19607587685120376, 0.19607587685120376, 0.19607587685120376, 0.09142243954669904, 0.09142243954669904, 0.09142243954669904, 0.09273474322018704, 0.09273474322018704, 0.09273474322018704, 0.09463479579497758, 0.09463479579497758, 0.09463479579497758]}, "mutation_prompt": null}
{"id": "ba6bec03-35bc-47f6-92da-256729321735", "solution": "import numpy as np\n\nclass HybridDESA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.pop_size = 10 * dim\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.temp = 1000.0  # Initial temperature for simulated annealing\n        self.cooling_rate = 0.99  # Cooling rate for simulated annealing\n\n    def __call__(self, func):\n        np.random.seed(42)\n        population = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        num_evaluations = self.pop_size\n\n        def mutate(target_idx):\n            indices = [idx for idx in range(self.pop_size) if idx != target_idx]\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            dynamic_F = self.F * (1.0 - (num_evaluations / self.budget)) * np.exp(-num_evaluations / self.budget)\n            dynamic_F *= (self.temp / 1000.0) * (1 + 0.1 * (num_evaluations / self.budget))  # Modified line: Adaptive scaling to mutation factor\n            mutant = population[a] + dynamic_F * (population[b] - population[c])\n            mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n            return mutant\n\n        while num_evaluations < self.budget:\n            self.CR = 0.9 - (0.3 * (num_evaluations / self.budget)) + 0.1 * np.tanh(num_evaluations / self.budget)  # Modified line: Introduce learning rate update to crossover probability\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                mutant = mutate(i)\n                trial = np.where(np.random.rand(self.dim) < self.CR, mutant, 0.5 * (population[i] + best_solution))\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = trial\n\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                neighbor = population[i] + np.random.normal(0, 0.1 * self.temp / 1000, self.dim)\n                neighbor = np.clip(neighbor, self.lower_bound, self.upper_bound)\n                neighbor_fitness = func(neighbor)\n                num_evaluations += 1\n                delta_fit = neighbor_fitness - fitness[i]\n\n                if delta_fit < 0 or np.random.rand() < np.exp(-delta_fit / self.temp):\n                    population[i] = neighbor\n                    fitness[i] = neighbor_fitness\n                    if neighbor_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = neighbor\n\n            self.temp *= (self.cooling_rate - 0.01 * (num_evaluations / self.budget))\n            self.pop_size = max(4, int(8 * self.dim * (1 - num_evaluations / self.budget)))\n\n        return best_solution", "name": "HybridDESA", "description": "Introduce learning rate based update to the crossover probability to enhance exploration capabilities.", "configspace": "", "generation": 62, "fitness": 0.27456111215513973, "feedback": "The algorithm HybridDESA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.27 with standard deviation 0.19.", "error": "", "parent_id": "7c2cdd96-a24a-4bdc-96df-83fbafae318e", "metadata": {"aucs": [0.5542661297327882, 0.5542661297327882, 0.5542661297327882, 0.5502131781010412, 0.5502131781010412, 0.5502131781010412, 0.5542576829571606, 0.5542576829571606, 0.5542576829571606, 0.3214986489793178, 0.3214986489793178, 0.3214986489793178, 0.3065416684084221, 0.3065416684084221, 0.3065416684084221, 0.3158065919396942, 0.3158065919396942, 0.3158065919396942, 0.13838181317138243, 0.13838181317138243, 0.13838181317138243, 0.12545327945625218, 0.12545327945625218, 0.12545327945625218, 0.12341091429570572, 0.12341091429570572, 0.12341091429570572, 0.10743269946665945, 0.10743269946665945, 0.10743269946665945, 0.1047695610119097, 0.1047695610119097, 0.1047695610119097, 0.11944414538050097, 0.11944414538050097, 0.11944414538050097, 0.8779320540888479, 0.8779320540888479, 0.8779320540888479, 0.877898188844995, 0.877898188844995, 0.877898188844995, 0.780456822675078, 0.780456822675078, 0.780456822675078, 0.3563309678652393, 0.3563309678652393, 0.3563309678652393, 0.3648861206411198, 0.3648861206411198, 0.3648861206411198, 0.3566431522159045, 0.3566431522159045, 0.3566431522159045, 0.6706862034126804, 0.6706862034126804, 0.6706862034126804, 0.3423519789978463, 0.3423519789978463, 0.3423519789978463, 0.72668247747501, 0.72668247747501, 0.72668247747501, 0.16919472358197873, 0.16919472358197873, 0.16919472358197873, 0.1651292859417527, 0.1651292859417527, 0.1651292859417527, 0.14989293282023441, 0.14989293282023441, 0.14989293282023441, 0.14250677278277657, 0.14250677278277657, 0.14250677278277657, 0.1589362233709246, 0.1589362233709246, 0.1589362233709246, 0.11857286877661266, 0.11857286877661266, 0.11857286877661266, 0.07046504581570368, 0.07046504581570368, 0.07046504581570368, 0.09291593469393078, 0.09291593469393078, 0.09291593469393078, 0.10976115636052908, 0.10976115636052908, 0.10976115636052908, 0.15193954057356018, 0.15193954057356018, 0.15193954057356018, 0.1246100015034457, 0.1246100015034457, 0.1246100015034457, 0.15848011734590806, 0.15848011734590806, 0.15848011734590806, 0.2108696082750796, 0.2108696082750796, 0.2108696082750796, 0.17218218373984961, 0.17218218373984961, 0.17218218373984961, 0.2329385394943635, 0.2329385394943635, 0.2329385394943635, 0.10591689517605296, 0.10591689517605296, 0.10591689517605296, 0.1552473120636909, 0.1552473120636909, 0.1552473120636909, 0.12199788623767405, 0.12199788623767405, 0.12199788623767405, 0.4593083307916008, 0.4593083307916008, 0.4593083307916008, 0.4628141347100536, 0.4628141347100536, 0.4628141347100536, 0.4549634008249209, 0.4549634008249209, 0.4549634008249209, 0.14035267481144242, 0.14035267481144242, 0.14035267481144242, 0.2647056139184356, 0.2647056139184356, 0.2647056139184356, 0.13663480456036958, 0.13663480456036958, 0.13663480456036958, 0.20334182760801733, 0.20334182760801733, 0.20334182760801733, 0.17681323754407496, 0.17681323754407496, 0.17681323754407496, 0.21871292997332525, 0.21871292997332525, 0.21871292997332525, 0.32825394007153186, 0.32825394007153186, 0.32825394007153186, 0.3326823931838664, 0.3326823931838664, 0.3326823931838664, 0.3421300043168062, 0.3421300043168062, 0.3421300043168062, 0.2724755833403435, 0.2724755833403435, 0.2724755833403435, 0.20678815163767184, 0.20678815163767184, 0.20678815163767184, 0.27220142835892625, 0.27220142835892625, 0.27220142835892625, 0.22900362693501475, 0.22900362693501475, 0.22900362693501475, 0.23990890494473383, 0.23990890494473383, 0.23990890494473383, 0.23540240144705793, 0.23540240144705793, 0.23540240144705793, 0.19425768105254093, 0.19425768105254093, 0.19425768105254093, 0.18912798567843125, 0.18912798567843125, 0.18912798567843125, 0.19106985300632762, 0.19106985300632762, 0.19106985300632762, 0.7025277924101051, 0.7025277924101051, 0.7025277924101051, 0.15133519477843094, 0.15133519477843094, 0.15133519477843094, 0.6240958961911556, 0.6240958961911556, 0.6240958961911556, 0.16573198475929618, 0.16573198475929618, 0.16573198475929618, 0.19732013670547166, 0.19732013670547166, 0.19732013670547166, 0.20412171108102173, 0.20412171108102173, 0.20412171108102173, 0.20894043819099617, 0.20894043819099617, 0.20894043819099617, 0.1933116987478113, 0.1933116987478113, 0.1933116987478113, 0.1933601729649982, 0.1933601729649982, 0.1933601729649982, 0.09564479257847158, 0.09564479257847158, 0.09564479257847158, 0.09537324264215674, 0.09537324264215674, 0.09537324264215674, 0.09878679773303067, 0.09878679773303067, 0.09878679773303067]}, "mutation_prompt": null}
{"id": "c246ea8a-544d-4be9-9a56-f13db7b1ba04", "solution": "import numpy as np\n\nclass HybridDESA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.pop_size = 10 * dim\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.temp = 1000.0  # Initial temperature for simulated annealing\n        self.cooling_rate = 0.99  # Cooling rate for simulated annealing\n\n    def __call__(self, func):\n        np.random.seed(42)\n        population = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        num_evaluations = self.pop_size\n\n        def mutate(target_idx):\n            indices = [idx for idx in range(self.pop_size) if idx != target_idx]\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            dynamic_F = self.F * (1.0 - (num_evaluations / self.budget)) * np.exp(-num_evaluations / self.budget)\n            dynamic_F *= (self.temp / 1000.0) * (1 + 0.1 * (num_evaluations / self.budget))  # Modified line: Adaptive scaling to mutation factor\n            mutant = population[a] + dynamic_F * (population[b] - population[c])\n            mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n            return mutant\n\n        while num_evaluations < self.budget:\n            self.CR = 0.9 - (0.3 * (num_evaluations / self.budget)) + 0.1 * np.tanh(num_evaluations / self.budget)  # Modified line: Introduce learning rate update to crossover probability\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                mutant = mutate(i)\n                trial = np.where(np.random.rand(self.dim) < self.CR, mutant, 0.5 * (population[i] + best_solution))\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = trial\n\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                neighbor = population[i] + np.random.normal(0, 0.1 * self.temp / 1000, self.dim)\n                neighbor = np.clip(neighbor, self.lower_bound, self.upper_bound)\n                neighbor_fitness = func(neighbor)\n                num_evaluations += 1\n                delta_fit = neighbor_fitness - fitness[i]\n\n                if delta_fit < 0 or np.random.rand() < np.exp(-delta_fit / self.temp):\n                    population[i] = neighbor\n                    fitness[i] = neighbor_fitness\n                    if neighbor_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = neighbor\n\n            self.temp *= (self.cooling_rate - 0.01 * (num_evaluations / self.budget))\n            self.pop_size = max(4, int(8 * self.dim * (1 - num_evaluations / self.budget)))\n\n        return best_solution", "name": "HybridDESA", "description": "Introduce learning rate based update to the crossover probability to enhance exploration capabilities.", "configspace": "", "generation": 63, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "ba6bec03-35bc-47f6-92da-256729321735", "metadata": {"aucs": [0.5542661297327882, 0.5542661297327882, 0.5542661297327882, 0.5502131781010412, 0.5502131781010412, 0.5502131781010412, 0.5542576829571606, 0.5542576829571606, 0.5542576829571606, 0.3214986489793178, 0.3214986489793178, 0.3214986489793178, 0.3065416684084221, 0.3065416684084221, 0.3065416684084221, 0.3158065919396942, 0.3158065919396942, 0.3158065919396942, 0.13838181317138243, 0.13838181317138243, 0.13838181317138243, 0.12545327945625218, 0.12545327945625218, 0.12545327945625218, 0.12341091429570572, 0.12341091429570572, 0.12341091429570572, 0.10743269946665945, 0.10743269946665945, 0.10743269946665945, 0.1047695610119097, 0.1047695610119097, 0.1047695610119097, 0.11944414538050097, 0.11944414538050097, 0.11944414538050097, 0.8779320540888479, 0.8779320540888479, 0.8779320540888479, 0.877898188844995, 0.877898188844995, 0.877898188844995, 0.780456822675078, 0.780456822675078, 0.780456822675078, 0.3563309678652393, 0.3563309678652393, 0.3563309678652393, 0.3648861206411198, 0.3648861206411198, 0.3648861206411198, 0.3566431522159045, 0.3566431522159045, 0.3566431522159045, 0.6706862034126804, 0.6706862034126804, 0.6706862034126804, 0.3423519789978463, 0.3423519789978463, 0.3423519789978463, 0.72668247747501, 0.72668247747501, 0.72668247747501, 0.16919472358197873, 0.16919472358197873, 0.16919472358197873, 0.1651292859417527, 0.1651292859417527, 0.1651292859417527, 0.14989293282023441, 0.14989293282023441, 0.14989293282023441, 0.14250677278277657, 0.14250677278277657, 0.14250677278277657, 0.1589362233709246, 0.1589362233709246, 0.1589362233709246, 0.11857286877661266, 0.11857286877661266, 0.11857286877661266, 0.07046504581570368, 0.07046504581570368, 0.07046504581570368, 0.09291593469393078, 0.09291593469393078, 0.09291593469393078, 0.10976115636052908, 0.10976115636052908, 0.10976115636052908, 0.15193954057356018, 0.15193954057356018, 0.15193954057356018, 0.1246100015034457, 0.1246100015034457, 0.1246100015034457, 0.15848011734590806, 0.15848011734590806, 0.15848011734590806, 0.2108696082750796, 0.2108696082750796, 0.2108696082750796, 0.17218218373984961, 0.17218218373984961, 0.17218218373984961, 0.2329385394943635, 0.2329385394943635, 0.2329385394943635, 0.10591689517605296, 0.10591689517605296, 0.10591689517605296, 0.1552473120636909, 0.1552473120636909, 0.1552473120636909, 0.12199788623767405, 0.12199788623767405, 0.12199788623767405, 0.4593083307916008, 0.4593083307916008, 0.4593083307916008, 0.4628141347100536, 0.4628141347100536, 0.4628141347100536, 0.4549634008249209, 0.4549634008249209, 0.4549634008249209, 0.14035267481144242, 0.14035267481144242, 0.14035267481144242, 0.2647056139184356, 0.2647056139184356, 0.2647056139184356, 0.13663480456036958, 0.13663480456036958, 0.13663480456036958, 0.20334182760801733, 0.20334182760801733, 0.20334182760801733, 0.17681323754407496, 0.17681323754407496, 0.17681323754407496, 0.21871292997332525, 0.21871292997332525, 0.21871292997332525, 0.32825394007153186, 0.32825394007153186, 0.32825394007153186, 0.3326823931838664, 0.3326823931838664, 0.3326823931838664, 0.3421300043168062, 0.3421300043168062, 0.3421300043168062, 0.2724755833403435, 0.2724755833403435, 0.2724755833403435, 0.20678815163767184, 0.20678815163767184, 0.20678815163767184, 0.27220142835892625, 0.27220142835892625, 0.27220142835892625, 0.22900362693501475, 0.22900362693501475, 0.22900362693501475, 0.23990890494473383, 0.23990890494473383, 0.23990890494473383, 0.23540240144705793, 0.23540240144705793, 0.23540240144705793, 0.19425768105254093, 0.19425768105254093, 0.19425768105254093, 0.18912798567843125, 0.18912798567843125, 0.18912798567843125, 0.19106985300632762, 0.19106985300632762, 0.19106985300632762, 0.7025277924101051, 0.7025277924101051, 0.7025277924101051, 0.15133519477843094, 0.15133519477843094, 0.15133519477843094, 0.6240958961911556, 0.6240958961911556, 0.6240958961911556, 0.16573198475929618, 0.16573198475929618, 0.16573198475929618, 0.19732013670547166, 0.19732013670547166, 0.19732013670547166, 0.20412171108102173, 0.20412171108102173, 0.20412171108102173, 0.20894043819099617, 0.20894043819099617, 0.20894043819099617, 0.1933116987478113, 0.1933116987478113, 0.1933116987478113, 0.1933601729649982, 0.1933601729649982, 0.1933601729649982, 0.09564479257847158, 0.09564479257847158, 0.09564479257847158, 0.09537324264215674, 0.09537324264215674, 0.09537324264215674, 0.09878679773303067, 0.09878679773303067, 0.09878679773303067]}, "mutation_prompt": null}
{"id": "977870ba-d12e-482c-90e4-866482ce8b46", "solution": "import numpy as np\n\nclass HybridDESA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.pop_size = 10 * dim\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.temp = 1000.0  # Initial temperature for simulated annealing\n        self.cooling_rate = 0.99  # Cooling rate for simulated annealing\n\n    def __call__(self, func):\n        np.random.seed(42)\n        population = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        num_evaluations = self.pop_size\n\n        def mutate(target_idx):\n            indices = [idx for idx in range(self.pop_size) if idx != target_idx]\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            dynamic_F = self.F * (1.0 - (num_evaluations / self.budget)) * np.exp(-num_evaluations / self.budget)\n            dynamic_F *= (self.temp / 1000.0) * (1 + 0.2 * (num_evaluations / self.budget))  # Modified line: More aggressive scaling to mutation factor\n            mutant = population[a] + dynamic_F * (population[b] - population[c])\n            mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n            return mutant\n\n        while num_evaluations < self.budget:\n            self.CR = 0.9 - (0.3 * (num_evaluations / self.budget)) + 0.1 * np.tanh(num_evaluations / self.budget)\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                mutant = mutate(i)\n                trial = np.where(np.random.rand(self.dim) < self.CR, mutant, 0.5 * (population[i] + best_solution))\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = trial\n\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                neighbor = population[i] + np.random.normal(0, 0.1 * self.temp / 1000, self.dim)\n                neighbor = np.clip(neighbor, self.lower_bound, self.upper_bound)\n                neighbor_fitness = func(neighbor)\n                num_evaluations += 1\n                delta_fit = neighbor_fitness - fitness[i]\n\n                if delta_fit < 0 or np.random.rand() < np.exp(-delta_fit / self.temp):\n                    population[i] = neighbor\n                    fitness[i] = neighbor_fitness\n                    if neighbor_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = neighbor\n\n            self.temp *= (self.cooling_rate - 0.01 * (num_evaluations / self.budget))\n            self.pop_size = max(4, int(8 * self.dim * (1 - num_evaluations / self.budget)))\n\n        return best_solution", "name": "HybridDESA", "description": "Enhance exploration by using an adaptive mutation factor that scales more aggressively with the diminishing temperature.", "configspace": "", "generation": 64, "fitness": 0.2556694720948214, "feedback": "The algorithm HybridDESA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.26 with standard deviation 0.17.", "error": "", "parent_id": "ba6bec03-35bc-47f6-92da-256729321735", "metadata": {"aucs": [0.5357903148127101, 0.5357903148127101, 0.5357903148127101, 0.5368800992473355, 0.5368800992473355, 0.5368800992473355, 0.5447774436220897, 0.5447774436220897, 0.5447774436220897, 0.23262131355591797, 0.23262131355591797, 0.23262131355591797, 0.23934662396589423, 0.23934662396589423, 0.23934662396589423, 0.30177895274597855, 0.30177895274597855, 0.30177895274597855, 0.1317228885940498, 0.1317228885940498, 0.1317228885940498, 0.16493732404223616, 0.16493732404223616, 0.16493732404223616, 0.13887271596677941, 0.13887271596677941, 0.13887271596677941, 0.10232580696230931, 0.10232580696230931, 0.10232580696230931, 0.11169380267347839, 0.11169380267347839, 0.11169380267347839, 0.11360736039220587, 0.11360736039220587, 0.11360736039220587, 0.666270797410036, 0.666270797410036, 0.666270797410036, 0.787790618446924, 0.787790618446924, 0.787790618446924, 0.8761842520873959, 0.8761842520873959, 0.8761842520873959, 0.3525320949373859, 0.3525320949373859, 0.3525320949373859, 0.3406152756425809, 0.3406152756425809, 0.3406152756425809, 0.3625319606751397, 0.3625319606751397, 0.3625319606751397, 0.3509114172800548, 0.3509114172800548, 0.3509114172800548, 0.7194000901936302, 0.7194000901936302, 0.7194000901936302, 0.33173288053513283, 0.33173288053513283, 0.33173288053513283, 0.1452663037056463, 0.1452663037056463, 0.1452663037056463, 0.18111607532447993, 0.18111607532447993, 0.18111607532447993, 0.16102732524560692, 0.16102732524560692, 0.16102732524560692, 0.1734493890582306, 0.1734493890582306, 0.1734493890582306, 0.14234516387709795, 0.14234516387709795, 0.14234516387709795, 0.1693087168278724, 0.1693087168278724, 0.1693087168278724, 0.11498064704151456, 0.11498064704151456, 0.11498064704151456, 0.0719788260032117, 0.0719788260032117, 0.0719788260032117, 0.08347138247690167, 0.08347138247690167, 0.08347138247690167, 0.12655229211585506, 0.12655229211585506, 0.12655229211585506, 0.11967784651253854, 0.11967784651253854, 0.11967784651253854, 0.15062119940115082, 0.15062119940115082, 0.15062119940115082, 0.17664694515780466, 0.17664694515780466, 0.17664694515780466, 0.12653955315888832, 0.12653955315888832, 0.12653955315888832, 0.09209406853235302, 0.09209406853235302, 0.09209406853235302, 0.15112126125657432, 0.15112126125657432, 0.15112126125657432, 0.14735420673799982, 0.14735420673799982, 0.14735420673799982, 0.1546536899503319, 0.1546536899503319, 0.1546536899503319, 0.44328328906683223, 0.44328328906683223, 0.44328328906683223, 0.4518203451202082, 0.4518203451202082, 0.4518203451202082, 0.43767732470623877, 0.43767732470623877, 0.43767732470623877, 0.15297207626361564, 0.15297207626361564, 0.15297207626361564, 0.2683220091274088, 0.2683220091274088, 0.2683220091274088, 0.1368690902223536, 0.1368690902223536, 0.1368690902223536, 0.20787579053988425, 0.20787579053988425, 0.20787579053988425, 0.1967591018838717, 0.1967591018838717, 0.1967591018838717, 0.2374198731629824, 0.2374198731629824, 0.2374198731629824, 0.3375737969845972, 0.3375737969845972, 0.3375737969845972, 0.32098169968550805, 0.32098169968550805, 0.32098169968550805, 0.3389568424555094, 0.3389568424555094, 0.3389568424555094, 0.27425101488038384, 0.27425101488038384, 0.27425101488038384, 0.28238431144838894, 0.28238431144838894, 0.28238431144838894, 0.27722152449869475, 0.27722152449869475, 0.27722152449869475, 0.2100118712364616, 0.2100118712364616, 0.2100118712364616, 0.21332913172888668, 0.21332913172888668, 0.21332913172888668, 0.24489679611928172, 0.24489679611928172, 0.24489679611928172, 0.20165813830230195, 0.20165813830230195, 0.20165813830230195, 0.18986163959051605, 0.18986163959051605, 0.18986163959051605, 0.17260054642910072, 0.17260054642910072, 0.17260054642910072, 0.16168532246535783, 0.16168532246535783, 0.16168532246535783, 0.18570236267254436, 0.18570236267254436, 0.18570236267254436, 0.6012786808202915, 0.6012786808202915, 0.6012786808202915, 0.16536272027494958, 0.16536272027494958, 0.16536272027494958, 0.19891017980199144, 0.19891017980199144, 0.19891017980199144, 0.20484935775199697, 0.20484935775199697, 0.20484935775199697, 0.18391218868315962, 0.18391218868315962, 0.18391218868315962, 0.20302678463327717, 0.20302678463327717, 0.20302678463327717, 0.19778585176927166, 0.19778585176927166, 0.19778585176927166, 0.10085469888509835, 0.10085469888509835, 0.10085469888509835, 0.09011760340988562, 0.09011760340988562, 0.09011760340988562, 0.08746110003296181, 0.08746110003296181, 0.08746110003296181]}, "mutation_prompt": null}
{"id": "25a3c3bc-a816-4076-adea-f2c002a977e5", "solution": "import numpy as np\n\nclass HybridDESA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.pop_size = 10 * dim\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.temp = 1000.0  # Initial temperature for simulated annealing\n        self.cooling_rate = 0.99  # Cooling rate for simulated annealing\n\n    def __call__(self, func):\n        np.random.seed(42)\n        population = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        num_evaluations = self.pop_size\n\n        def mutate(target_idx):\n            indices = [idx for idx in range(self.pop_size) if idx != target_idx]\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            dynamic_F = self.F * (1.0 - (num_evaluations / self.budget)) * np.exp(-num_evaluations / self.budget)\n            dynamic_F *= (self.temp / 1000.0) * (1 + 0.1 * (num_evaluations / self.budget))  # Modified line: Adaptive scaling to mutation factor\n            mutant = population[a] + dynamic_F * (population[b] - population[c])\n            mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n            return mutant\n\n        while num_evaluations < self.budget:\n            self.CR = 0.9 - (0.3 * (num_evaluations / self.budget)) + 0.1 * np.tanh(num_evaluations / self.budget)  # Modified line: Introduce learning rate update to crossover probability\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                mutant = mutate(i)\n                trial = np.where(np.random.rand(self.dim) < self.CR, mutant, 0.5 * (population[i] + best_solution))\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = trial\n\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                neighbor = population[i] + np.random.normal(0, 0.1 * self.temp / 1000, self.dim)\n                neighbor = np.clip(neighbor, self.lower_bound, self.upper_bound)\n                neighbor_fitness = func(neighbor)\n                num_evaluations += 1\n                delta_fit = neighbor_fitness - fitness[i]\n\n                if delta_fit < 0 or np.random.rand() < np.exp(-delta_fit / self.temp):\n                    population[i] = neighbor\n                    fitness[i] = neighbor_fitness\n                    if neighbor_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = neighbor\n\n            self.temp *= (self.cooling_rate - 0.01 * (num_evaluations / self.budget))\n            self.pop_size = max(4, int(8 * self.dim * (1 - num_evaluations / self.budget)))\n\n        return best_solution", "name": "HybridDESA", "description": "Introduce learning rate based update to the crossover probability to enhance exploration capabilities.", "configspace": "", "generation": 63, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "ba6bec03-35bc-47f6-92da-256729321735", "metadata": {"aucs": [0.5542661297327882, 0.5542661297327882, 0.5542661297327882, 0.5502131781010412, 0.5502131781010412, 0.5502131781010412, 0.5542576829571606, 0.5542576829571606, 0.5542576829571606, 0.3214986489793178, 0.3214986489793178, 0.3214986489793178, 0.3065416684084221, 0.3065416684084221, 0.3065416684084221, 0.3158065919396942, 0.3158065919396942, 0.3158065919396942, 0.13838181317138243, 0.13838181317138243, 0.13838181317138243, 0.12545327945625218, 0.12545327945625218, 0.12545327945625218, 0.12341091429570572, 0.12341091429570572, 0.12341091429570572, 0.10743269946665945, 0.10743269946665945, 0.10743269946665945, 0.1047695610119097, 0.1047695610119097, 0.1047695610119097, 0.11944414538050097, 0.11944414538050097, 0.11944414538050097, 0.8779320540888479, 0.8779320540888479, 0.8779320540888479, 0.877898188844995, 0.877898188844995, 0.877898188844995, 0.780456822675078, 0.780456822675078, 0.780456822675078, 0.3563309678652393, 0.3563309678652393, 0.3563309678652393, 0.3648861206411198, 0.3648861206411198, 0.3648861206411198, 0.3566431522159045, 0.3566431522159045, 0.3566431522159045, 0.6706862034126804, 0.6706862034126804, 0.6706862034126804, 0.3423519789978463, 0.3423519789978463, 0.3423519789978463, 0.72668247747501, 0.72668247747501, 0.72668247747501, 0.16919472358197873, 0.16919472358197873, 0.16919472358197873, 0.1651292859417527, 0.1651292859417527, 0.1651292859417527, 0.14989293282023441, 0.14989293282023441, 0.14989293282023441, 0.14250677278277657, 0.14250677278277657, 0.14250677278277657, 0.1589362233709246, 0.1589362233709246, 0.1589362233709246, 0.11857286877661266, 0.11857286877661266, 0.11857286877661266, 0.07046504581570368, 0.07046504581570368, 0.07046504581570368, 0.09291593469393078, 0.09291593469393078, 0.09291593469393078, 0.10976115636052908, 0.10976115636052908, 0.10976115636052908, 0.15193954057356018, 0.15193954057356018, 0.15193954057356018, 0.1246100015034457, 0.1246100015034457, 0.1246100015034457, 0.15848011734590806, 0.15848011734590806, 0.15848011734590806, 0.2108696082750796, 0.2108696082750796, 0.2108696082750796, 0.17218218373984961, 0.17218218373984961, 0.17218218373984961, 0.2329385394943635, 0.2329385394943635, 0.2329385394943635, 0.10591689517605296, 0.10591689517605296, 0.10591689517605296, 0.1552473120636909, 0.1552473120636909, 0.1552473120636909, 0.12199788623767405, 0.12199788623767405, 0.12199788623767405, 0.4593083307916008, 0.4593083307916008, 0.4593083307916008, 0.4628141347100536, 0.4628141347100536, 0.4628141347100536, 0.4549634008249209, 0.4549634008249209, 0.4549634008249209, 0.14035267481144242, 0.14035267481144242, 0.14035267481144242, 0.2647056139184356, 0.2647056139184356, 0.2647056139184356, 0.13663480456036958, 0.13663480456036958, 0.13663480456036958, 0.20334182760801733, 0.20334182760801733, 0.20334182760801733, 0.17681323754407496, 0.17681323754407496, 0.17681323754407496, 0.21871292997332525, 0.21871292997332525, 0.21871292997332525, 0.32825394007153186, 0.32825394007153186, 0.32825394007153186, 0.3326823931838664, 0.3326823931838664, 0.3326823931838664, 0.3421300043168062, 0.3421300043168062, 0.3421300043168062, 0.2724755833403435, 0.2724755833403435, 0.2724755833403435, 0.20678815163767184, 0.20678815163767184, 0.20678815163767184, 0.27220142835892625, 0.27220142835892625, 0.27220142835892625, 0.22900362693501475, 0.22900362693501475, 0.22900362693501475, 0.23990890494473383, 0.23990890494473383, 0.23990890494473383, 0.23540240144705793, 0.23540240144705793, 0.23540240144705793, 0.19425768105254093, 0.19425768105254093, 0.19425768105254093, 0.18912798567843125, 0.18912798567843125, 0.18912798567843125, 0.19106985300632762, 0.19106985300632762, 0.19106985300632762, 0.7025277924101051, 0.7025277924101051, 0.7025277924101051, 0.15133519477843094, 0.15133519477843094, 0.15133519477843094, 0.6240958961911556, 0.6240958961911556, 0.6240958961911556, 0.16573198475929618, 0.16573198475929618, 0.16573198475929618, 0.19732013670547166, 0.19732013670547166, 0.19732013670547166, 0.20412171108102173, 0.20412171108102173, 0.20412171108102173, 0.20894043819099617, 0.20894043819099617, 0.20894043819099617, 0.1933116987478113, 0.1933116987478113, 0.1933116987478113, 0.1933601729649982, 0.1933601729649982, 0.1933601729649982, 0.09564479257847158, 0.09564479257847158, 0.09564479257847158, 0.09537324264215674, 0.09537324264215674, 0.09537324264215674, 0.09878679773303067, 0.09878679773303067, 0.09878679773303067]}, "mutation_prompt": null}
{"id": "7a754119-00a9-4b28-871e-0d902b5dd330", "solution": "import numpy as np\n\nclass HybridDESA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.pop_size = 10 * dim\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.temp = 1000.0  # Initial temperature for simulated annealing\n        self.cooling_rate = 0.99  # Cooling rate for simulated annealing\n\n    def __call__(self, func):\n        np.random.seed(42)\n        population = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        num_evaluations = self.pop_size\n\n        def mutate(target_idx):\n            indices = [idx for idx in range(self.pop_size) if idx != target_idx]\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            dynamic_F = self.F * (1.0 - (num_evaluations / self.budget)) * np.exp(-num_evaluations / self.budget)\n            dynamic_F *= (self.temp / 1000.0) * (1 + 0.1 * (num_evaluations / self.budget))  # Modified line: Adaptive scaling to mutation factor\n            mutant = population[a] + dynamic_F * (population[b] - population[c])\n            mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n            return mutant\n\n        while num_evaluations < self.budget:\n            self.CR = 0.9 - (0.3 * (num_evaluations / self.budget)) + 0.1 * np.tanh(num_evaluations / self.budget)  # Modified line: Introduce learning rate update to crossover probability\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                mutant = mutate(i)\n                trial = np.where(np.random.rand(self.dim) < self.CR, mutant, 0.5 * (population[i] + best_solution))\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = trial\n\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                neighbor = population[i] + np.random.normal(0, 0.1 * self.temp / 1000, self.dim)\n                neighbor = np.clip(neighbor, self.lower_bound, self.upper_bound)\n                neighbor_fitness = func(neighbor)\n                num_evaluations += 1\n                delta_fit = neighbor_fitness - fitness[i]\n\n                if delta_fit < 0 or np.random.rand() < np.exp(-delta_fit / self.temp):\n                    population[i] = neighbor\n                    fitness[i] = neighbor_fitness\n                    if neighbor_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = neighbor\n\n            self.temp *= (self.cooling_rate - 0.01 * (num_evaluations / self.budget))\n            self.pop_size = max(4, int(8 * self.dim * (1 - num_evaluations / self.budget)))\n\n        return best_solution", "name": "HybridDESA", "description": "Introduce learning rate based update to the crossover probability to enhance exploration capabilities.", "configspace": "", "generation": 63, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "ba6bec03-35bc-47f6-92da-256729321735", "metadata": {"aucs": [0.5542661297327882, 0.5542661297327882, 0.5542661297327882, 0.5502131781010412, 0.5502131781010412, 0.5502131781010412, 0.5542576829571606, 0.5542576829571606, 0.5542576829571606, 0.3214986489793178, 0.3214986489793178, 0.3214986489793178, 0.3065416684084221, 0.3065416684084221, 0.3065416684084221, 0.3158065919396942, 0.3158065919396942, 0.3158065919396942, 0.13838181317138243, 0.13838181317138243, 0.13838181317138243, 0.12545327945625218, 0.12545327945625218, 0.12545327945625218, 0.12341091429570572, 0.12341091429570572, 0.12341091429570572, 0.10743269946665945, 0.10743269946665945, 0.10743269946665945, 0.1047695610119097, 0.1047695610119097, 0.1047695610119097, 0.11944414538050097, 0.11944414538050097, 0.11944414538050097, 0.8779320540888479, 0.8779320540888479, 0.8779320540888479, 0.877898188844995, 0.877898188844995, 0.877898188844995, 0.780456822675078, 0.780456822675078, 0.780456822675078, 0.3563309678652393, 0.3563309678652393, 0.3563309678652393, 0.3648861206411198, 0.3648861206411198, 0.3648861206411198, 0.3566431522159045, 0.3566431522159045, 0.3566431522159045, 0.6706862034126804, 0.6706862034126804, 0.6706862034126804, 0.3423519789978463, 0.3423519789978463, 0.3423519789978463, 0.72668247747501, 0.72668247747501, 0.72668247747501, 0.16919472358197873, 0.16919472358197873, 0.16919472358197873, 0.1651292859417527, 0.1651292859417527, 0.1651292859417527, 0.14989293282023441, 0.14989293282023441, 0.14989293282023441, 0.14250677278277657, 0.14250677278277657, 0.14250677278277657, 0.1589362233709246, 0.1589362233709246, 0.1589362233709246, 0.11857286877661266, 0.11857286877661266, 0.11857286877661266, 0.07046504581570368, 0.07046504581570368, 0.07046504581570368, 0.09291593469393078, 0.09291593469393078, 0.09291593469393078, 0.10976115636052908, 0.10976115636052908, 0.10976115636052908, 0.15193954057356018, 0.15193954057356018, 0.15193954057356018, 0.1246100015034457, 0.1246100015034457, 0.1246100015034457, 0.15848011734590806, 0.15848011734590806, 0.15848011734590806, 0.2108696082750796, 0.2108696082750796, 0.2108696082750796, 0.17218218373984961, 0.17218218373984961, 0.17218218373984961, 0.2329385394943635, 0.2329385394943635, 0.2329385394943635, 0.10591689517605296, 0.10591689517605296, 0.10591689517605296, 0.1552473120636909, 0.1552473120636909, 0.1552473120636909, 0.12199788623767405, 0.12199788623767405, 0.12199788623767405, 0.4593083307916008, 0.4593083307916008, 0.4593083307916008, 0.4628141347100536, 0.4628141347100536, 0.4628141347100536, 0.4549634008249209, 0.4549634008249209, 0.4549634008249209, 0.14035267481144242, 0.14035267481144242, 0.14035267481144242, 0.2647056139184356, 0.2647056139184356, 0.2647056139184356, 0.13663480456036958, 0.13663480456036958, 0.13663480456036958, 0.20334182760801733, 0.20334182760801733, 0.20334182760801733, 0.17681323754407496, 0.17681323754407496, 0.17681323754407496, 0.21871292997332525, 0.21871292997332525, 0.21871292997332525, 0.32825394007153186, 0.32825394007153186, 0.32825394007153186, 0.3326823931838664, 0.3326823931838664, 0.3326823931838664, 0.3421300043168062, 0.3421300043168062, 0.3421300043168062, 0.2724755833403435, 0.2724755833403435, 0.2724755833403435, 0.20678815163767184, 0.20678815163767184, 0.20678815163767184, 0.27220142835892625, 0.27220142835892625, 0.27220142835892625, 0.22900362693501475, 0.22900362693501475, 0.22900362693501475, 0.23990890494473383, 0.23990890494473383, 0.23990890494473383, 0.23540240144705793, 0.23540240144705793, 0.23540240144705793, 0.19425768105254093, 0.19425768105254093, 0.19425768105254093, 0.18912798567843125, 0.18912798567843125, 0.18912798567843125, 0.19106985300632762, 0.19106985300632762, 0.19106985300632762, 0.7025277924101051, 0.7025277924101051, 0.7025277924101051, 0.15133519477843094, 0.15133519477843094, 0.15133519477843094, 0.6240958961911556, 0.6240958961911556, 0.6240958961911556, 0.16573198475929618, 0.16573198475929618, 0.16573198475929618, 0.19732013670547166, 0.19732013670547166, 0.19732013670547166, 0.20412171108102173, 0.20412171108102173, 0.20412171108102173, 0.20894043819099617, 0.20894043819099617, 0.20894043819099617, 0.1933116987478113, 0.1933116987478113, 0.1933116987478113, 0.1933601729649982, 0.1933601729649982, 0.1933601729649982, 0.09564479257847158, 0.09564479257847158, 0.09564479257847158, 0.09537324264215674, 0.09537324264215674, 0.09537324264215674, 0.09878679773303067, 0.09878679773303067, 0.09878679773303067]}, "mutation_prompt": null}
{"id": "ed6cdd98-6386-4a91-a27b-163264a148d8", "solution": "import numpy as np\n\nclass HybridDESA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.pop_size = 10 * dim\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.temp = 1000.0  # Initial temperature for simulated annealing\n        self.cooling_rate = 0.99  # Cooling rate for simulated annealing\n\n    def __call__(self, func):\n        np.random.seed(42)\n        population = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        num_evaluations = self.pop_size\n\n        def mutate(target_idx):\n            indices = [idx for idx in range(self.pop_size) if idx != target_idx]\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            dynamic_F = self.F * (1.0 - (num_evaluations / self.budget)) * np.exp(-num_evaluations / self.budget)\n            dynamic_F *= (self.temp / 1000.0) * (1 + 0.1 * (num_evaluations / self.budget))  # Modified line: Adaptive scaling to mutation factor\n            mutant = population[a] + dynamic_F * (population[b] - population[c])\n            mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n            return mutant\n\n        while num_evaluations < self.budget:\n            self.CR = 0.9 - (0.3 * (num_evaluations / self.budget)) + 0.1 * np.tanh(num_evaluations / self.budget)  # Modified line: Introduce learning rate update to crossover probability\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                mutant = mutate(i)\n                trial = np.where(np.random.rand(self.dim) < self.CR, mutant, 0.5 * (population[i] + best_solution))\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = trial\n\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                neighbor = population[i] + np.random.normal(0, 0.1 * self.temp / 1000, self.dim)\n                neighbor = np.clip(neighbor, self.lower_bound, self.upper_bound)\n                neighbor_fitness = func(neighbor)\n                num_evaluations += 1\n                delta_fit = neighbor_fitness - fitness[i]\n\n                if delta_fit < 0 or np.random.rand() < np.exp(-delta_fit / self.temp):\n                    population[i] = neighbor\n                    fitness[i] = neighbor_fitness\n                    if neighbor_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = neighbor\n\n            self.temp *= (self.cooling_rate - 0.01 * (num_evaluations / self.budget))\n            self.pop_size = max(4, int(8 * self.dim * (1 - num_evaluations / self.budget)))\n\n        return best_solution", "name": "HybridDESA", "description": "Introduce learning rate based update to the crossover probability to enhance exploration capabilities.", "configspace": "", "generation": 63, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "ba6bec03-35bc-47f6-92da-256729321735", "metadata": {"aucs": [0.5542661297327882, 0.5542661297327882, 0.5542661297327882, 0.5502131781010412, 0.5502131781010412, 0.5502131781010412, 0.5542576829571606, 0.5542576829571606, 0.5542576829571606, 0.3214986489793178, 0.3214986489793178, 0.3214986489793178, 0.3065416684084221, 0.3065416684084221, 0.3065416684084221, 0.3158065919396942, 0.3158065919396942, 0.3158065919396942, 0.13838181317138243, 0.13838181317138243, 0.13838181317138243, 0.12545327945625218, 0.12545327945625218, 0.12545327945625218, 0.12341091429570572, 0.12341091429570572, 0.12341091429570572, 0.10743269946665945, 0.10743269946665945, 0.10743269946665945, 0.1047695610119097, 0.1047695610119097, 0.1047695610119097, 0.11944414538050097, 0.11944414538050097, 0.11944414538050097, 0.8779320540888479, 0.8779320540888479, 0.8779320540888479, 0.877898188844995, 0.877898188844995, 0.877898188844995, 0.780456822675078, 0.780456822675078, 0.780456822675078, 0.3563309678652393, 0.3563309678652393, 0.3563309678652393, 0.3648861206411198, 0.3648861206411198, 0.3648861206411198, 0.3566431522159045, 0.3566431522159045, 0.3566431522159045, 0.6706862034126804, 0.6706862034126804, 0.6706862034126804, 0.3423519789978463, 0.3423519789978463, 0.3423519789978463, 0.72668247747501, 0.72668247747501, 0.72668247747501, 0.16919472358197873, 0.16919472358197873, 0.16919472358197873, 0.1651292859417527, 0.1651292859417527, 0.1651292859417527, 0.14989293282023441, 0.14989293282023441, 0.14989293282023441, 0.14250677278277657, 0.14250677278277657, 0.14250677278277657, 0.1589362233709246, 0.1589362233709246, 0.1589362233709246, 0.11857286877661266, 0.11857286877661266, 0.11857286877661266, 0.07046504581570368, 0.07046504581570368, 0.07046504581570368, 0.09291593469393078, 0.09291593469393078, 0.09291593469393078, 0.10976115636052908, 0.10976115636052908, 0.10976115636052908, 0.15193954057356018, 0.15193954057356018, 0.15193954057356018, 0.1246100015034457, 0.1246100015034457, 0.1246100015034457, 0.15848011734590806, 0.15848011734590806, 0.15848011734590806, 0.2108696082750796, 0.2108696082750796, 0.2108696082750796, 0.17218218373984961, 0.17218218373984961, 0.17218218373984961, 0.2329385394943635, 0.2329385394943635, 0.2329385394943635, 0.10591689517605296, 0.10591689517605296, 0.10591689517605296, 0.1552473120636909, 0.1552473120636909, 0.1552473120636909, 0.12199788623767405, 0.12199788623767405, 0.12199788623767405, 0.4593083307916008, 0.4593083307916008, 0.4593083307916008, 0.4628141347100536, 0.4628141347100536, 0.4628141347100536, 0.4549634008249209, 0.4549634008249209, 0.4549634008249209, 0.14035267481144242, 0.14035267481144242, 0.14035267481144242, 0.2647056139184356, 0.2647056139184356, 0.2647056139184356, 0.13663480456036958, 0.13663480456036958, 0.13663480456036958, 0.20334182760801733, 0.20334182760801733, 0.20334182760801733, 0.17681323754407496, 0.17681323754407496, 0.17681323754407496, 0.21871292997332525, 0.21871292997332525, 0.21871292997332525, 0.32825394007153186, 0.32825394007153186, 0.32825394007153186, 0.3326823931838664, 0.3326823931838664, 0.3326823931838664, 0.3421300043168062, 0.3421300043168062, 0.3421300043168062, 0.2724755833403435, 0.2724755833403435, 0.2724755833403435, 0.20678815163767184, 0.20678815163767184, 0.20678815163767184, 0.27220142835892625, 0.27220142835892625, 0.27220142835892625, 0.22900362693501475, 0.22900362693501475, 0.22900362693501475, 0.23990890494473383, 0.23990890494473383, 0.23990890494473383, 0.23540240144705793, 0.23540240144705793, 0.23540240144705793, 0.19425768105254093, 0.19425768105254093, 0.19425768105254093, 0.18912798567843125, 0.18912798567843125, 0.18912798567843125, 0.19106985300632762, 0.19106985300632762, 0.19106985300632762, 0.7025277924101051, 0.7025277924101051, 0.7025277924101051, 0.15133519477843094, 0.15133519477843094, 0.15133519477843094, 0.6240958961911556, 0.6240958961911556, 0.6240958961911556, 0.16573198475929618, 0.16573198475929618, 0.16573198475929618, 0.19732013670547166, 0.19732013670547166, 0.19732013670547166, 0.20412171108102173, 0.20412171108102173, 0.20412171108102173, 0.20894043819099617, 0.20894043819099617, 0.20894043819099617, 0.1933116987478113, 0.1933116987478113, 0.1933116987478113, 0.1933601729649982, 0.1933601729649982, 0.1933601729649982, 0.09564479257847158, 0.09564479257847158, 0.09564479257847158, 0.09537324264215674, 0.09537324264215674, 0.09537324264215674, 0.09878679773303067, 0.09878679773303067, 0.09878679773303067]}, "mutation_prompt": null}
{"id": "87350a0d-3b3a-4fb4-bad2-d9b3446681ce", "solution": "import numpy as np\n\nclass HybridDESA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.pop_size = 10 * dim\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.temp = 1000.0  # Initial temperature for simulated annealing\n        self.cooling_rate = 0.99  # Cooling rate for simulated annealing\n\n    def __call__(self, func):\n        np.random.seed(42)\n        population = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        num_evaluations = self.pop_size\n\n        def mutate(target_idx):\n            indices = [idx for idx in range(self.pop_size) if idx != target_idx]\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            dynamic_F = self.F * (1.0 - (num_evaluations / self.budget)) * np.exp(-num_evaluations / self.budget)\n            dynamic_F *= (self.temp / 1000.0) * (1 + 0.1 * (num_evaluations / self.budget))  # Adaptive scaling to mutation factor\n            mutant = population[a] + dynamic_F * (population[b] - population[c])\n            mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n            return mutant\n\n        while num_evaluations < self.budget:\n            self.CR = 0.9 - (0.3 * (num_evaluations / self.budget)) + 0.1 * np.tanh(num_evaluations / self.budget)  # Introduce learning rate update to crossover probability\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                mutant = mutate(i)\n                trial = np.where(np.random.rand(self.dim) < self.CR, mutant, 0.5 * (population[i] + best_solution))\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = trial\n\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                neighbor = population[i] + np.random.normal(0, 0.1 * self.temp / 1000, self.dim)\n                neighbor = np.clip(neighbor, self.lower_bound, self.upper_bound)\n                neighbor_fitness = func(neighbor)\n                num_evaluations += 1\n                delta_fit = neighbor_fitness - fitness[i]\n\n                if delta_fit < 0 or np.random.rand() < np.exp(-delta_fit / self.temp):\n                    population[i] = neighbor\n                    fitness[i] = neighbor_fitness\n                    if neighbor_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = neighbor\n\n            self.temp *= (self.cooling_rate - 0.01 * (num_evaluations / self.budget) - 0.005 * (fitness[best_idx] / np.mean(fitness)))  # Modified line: Adaptive cooling based on fitness improvement\n            self.pop_size = max(4, int(8 * self.dim * (1 - num_evaluations / self.budget)))\n\n        return best_solution", "name": "HybridDESA", "description": "Introduce adaptive cooling to temperature based on both budget and fitness improvements for enhanced convergence.", "configspace": "", "generation": 68, "fitness": 0.2660575805479971, "feedback": "The algorithm HybridDESA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.27 with standard deviation 0.19.", "error": "", "parent_id": "ba6bec03-35bc-47f6-92da-256729321735", "metadata": {"aucs": [0.5806340253084112, 0.5806340253084112, 0.5806340253084112, 0.6049188255333394, 0.6049188255333394, 0.6049188255333394, 0.5924530287535688, 0.5924530287535688, 0.5924530287535688, 0.2282406683104069, 0.2282406683104069, 0.2282406683104069, 0.2656318433179816, 0.2656318433179816, 0.2656318433179816, 0.23810662866671273, 0.23810662866671273, 0.23810662866671273, 0.12007583067941874, 0.12007583067941874, 0.12007583067941874, 0.14277014292459933, 0.14277014292459933, 0.14277014292459933, 0.11213361368630392, 0.11213361368630392, 0.11213361368630392, 0.12310588682684942, 0.12310588682684942, 0.12310588682684942, 0.10743154189242865, 0.10743154189242865, 0.10743154189242865, 0.09766976964619023, 0.09766976964619023, 0.09766976964619023, 0.7036606724676342, 0.7036606724676342, 0.7036606724676342, 0.8067874248527135, 0.8067874248527135, 0.8067874248527135, 0.8220901001712331, 0.8220901001712331, 0.8220901001712331, 0.36817845099499347, 0.36817845099499347, 0.36817845099499347, 0.3633372368652996, 0.3633372368652996, 0.3633372368652996, 0.3532514020653371, 0.3532514020653371, 0.3532514020653371, 0.6685515278498306, 0.6685515278498306, 0.6685515278498306, 0.20285721126416756, 0.20285721126416756, 0.20285721126416756, 0.7000078745331965, 0.7000078745331965, 0.7000078745331965, 0.1574477097010396, 0.1574477097010396, 0.1574477097010396, 0.16565877600395051, 0.16565877600395051, 0.16565877600395051, 0.1490555012162662, 0.1490555012162662, 0.1490555012162662, 0.14557338384722496, 0.14557338384722496, 0.14557338384722496, 0.15892235927797282, 0.15892235927797282, 0.15892235927797282, 0.15292032960735347, 0.15292032960735347, 0.15292032960735347, 0.12528574158845418, 0.12528574158845418, 0.12528574158845418, 0.06671655489836426, 0.06671655489836426, 0.06671655489836426, 0.055119350005811474, 0.055119350005811474, 0.055119350005811474, 0.14258714633426683, 0.14258714633426683, 0.14258714633426683, 0.11474291875112541, 0.11474291875112541, 0.11474291875112541, 0.15849221142758363, 0.15849221142758363, 0.15849221142758363, 0.193726951860975, 0.193726951860975, 0.193726951860975, 0.13057337436664607, 0.13057337436664607, 0.13057337436664607, 0.10675991113429628, 0.10675991113429628, 0.10675991113429628, 0.15897978440000493, 0.15897978440000493, 0.15897978440000493, 0.15127492464891112, 0.15127492464891112, 0.15127492464891112, 0.134939479484521, 0.134939479484521, 0.134939479484521, 0.43707675405121804, 0.43707675405121804, 0.43707675405121804, 0.4756464972415485, 0.4756464972415485, 0.4756464972415485, 0.45734079022487073, 0.45734079022487073, 0.45734079022487073, 0.11152706386534195, 0.11152706386534195, 0.11152706386534195, 0.1306017416882279, 0.1306017416882279, 0.1306017416882279, 0.12749383840620243, 0.12749383840620243, 0.12749383840620243, 0.22493070340214072, 0.22493070340214072, 0.22493070340214072, 0.2274525478392435, 0.2274525478392435, 0.2274525478392435, 0.23903600148954296, 0.23903600148954296, 0.23903600148954296, 0.3708280680622502, 0.3708280680622502, 0.3708280680622502, 0.3575146696484368, 0.3575146696484368, 0.3575146696484368, 0.343646394715727, 0.343646394715727, 0.343646394715727, 0.2459008024459094, 0.2459008024459094, 0.2459008024459094, 0.26723411860919355, 0.26723411860919355, 0.26723411860919355, 0.2833304562654907, 0.2833304562654907, 0.2833304562654907, 0.23282457489298503, 0.23282457489298503, 0.23282457489298503, 0.20223117093716003, 0.20223117093716003, 0.20223117093716003, 0.23273413095717144, 0.23273413095717144, 0.23273413095717144, 0.1878013436517464, 0.1878013436517464, 0.1878013436517464, 0.20081530671190184, 0.20081530671190184, 0.20081530671190184, 0.19702790782852986, 0.19702790782852986, 0.19702790782852986, 0.7962063749464807, 0.7962063749464807, 0.7962063749464807, 0.15365254139266904, 0.15365254139266904, 0.15365254139266904, 0.48665546250524494, 0.48665546250524494, 0.48665546250524494, 0.16580510728774223, 0.16580510728774223, 0.16580510728774223, 0.19750374002597815, 0.19750374002597815, 0.19750374002597815, 0.20642364673819036, 0.20642364673819036, 0.20642364673819036, 0.21918958551304424, 0.21918958551304424, 0.21918958551304424, 0.20564541032729888, 0.20564541032729888, 0.20564541032729888, 0.211094373834952, 0.211094373834952, 0.211094373834952, 0.1006478520570514, 0.1006478520570514, 0.1006478520570514, 0.08871707135046514, 0.08871707135046514, 0.08871707135046514, 0.10093963537645234, 0.10093963537645234, 0.10093963537645234]}, "mutation_prompt": null}
{"id": "0cf05637-07c2-4b11-8952-e63922fe4d54", "solution": "import numpy as np\n\nclass HybridDESA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.pop_size = 10 * dim\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.temp = 1000.0  # Initial temperature for simulated annealing\n        self.cooling_rate = 0.99  # Cooling rate for simulated annealing\n\n    def __call__(self, func):\n        np.random.seed(42)\n        population = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        num_evaluations = self.pop_size\n\n        def mutate(target_idx):\n            indices = [idx for idx in range(self.pop_size) if idx != target_idx]\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            dynamic_F = self.F * (1.0 - (num_evaluations / self.budget)) * np.exp(-num_evaluations / self.budget)\n            dynamic_F *= (self.temp / 1000.0) * (1 + 0.1 * (num_evaluations / self.budget))  # Modified line: Adaptive scaling to mutation factor\n            dynamic_F *= 0.95 * num_evaluations / self.budget + 0.05  # Smoothed adaptive scaling factor\n            mutant = population[a] + dynamic_F * (population[b] - population[c])\n            mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n            return mutant\n\n        while num_evaluations < self.budget:\n            self.CR = 0.9 - (0.3 * (num_evaluations / self.budget)) + 0.1 * np.tanh(num_evaluations / self.budget)  # Modified line: Introduce learning rate update to crossover probability\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                mutant = mutate(i)\n                trial = np.where(np.random.rand(self.dim) < self.CR, mutant, 0.5 * (population[i] + best_solution))\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = trial\n\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                neighbor = population[i] + np.random.normal(0, 0.1 * self.temp / 1000, self.dim)\n                neighbor = np.clip(neighbor, self.lower_bound, self.upper_bound)\n                neighbor_fitness = func(neighbor)\n                num_evaluations += 1\n                delta_fit = neighbor_fitness - fitness[i]\n\n                if delta_fit < 0 or np.random.rand() < np.exp(-delta_fit / self.temp):\n                    population[i] = neighbor\n                    fitness[i] = neighbor_fitness\n                    if neighbor_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = neighbor\n\n            self.temp *= (self.cooling_rate - 0.01 * (num_evaluations / self.budget))\n            self.pop_size = max(4, int(8 * self.dim * (1 - num_evaluations / self.budget)))\n\n        return best_solution", "name": "HybridDESA", "description": "Introduce adaptive mutation scaling factor smoothing to balance exploration and exploitation.", "configspace": "", "generation": 69, "fitness": 0.2039703735888072, "feedback": "The algorithm HybridDESA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.20 with standard deviation 0.14.", "error": "", "parent_id": "ba6bec03-35bc-47f6-92da-256729321735", "metadata": {"aucs": [0.5585264263775938, 0.5585264263775938, 0.5585264263775938, 0.5093602643545583, 0.5093602643545583, 0.5093602643545583, 0.552495397074182, 0.552495397074182, 0.552495397074182, 0.17770810409912974, 0.17770810409912974, 0.17770810409912974, 0.13439541811590083, 0.13439541811590083, 0.13439541811590083, 0.12145022170223363, 0.12145022170223363, 0.12145022170223363, 0.14835036203972884, 0.14835036203972884, 0.14835036203972884, 0.1478916910502549, 0.1478916910502549, 0.1478916910502549, 0.0799873260302405, 0.0799873260302405, 0.0799873260302405, 0.06064522224210689, 0.06064522224210689, 0.06064522224210689, 0.0767720930762571, 0.0767720930762571, 0.0767720930762571, 0.09302877425509792, 0.09302877425509792, 0.09302877425509792, 0.1363890828354566, 0.1363890828354566, 0.1363890828354566, 0.14092295195133409, 0.14092295195133409, 0.14092295195133409, 0.1209571647073655, 0.1209571647073655, 0.1209571647073655, 0.2894240154576878, 0.2894240154576878, 0.2894240154576878, 0.32715669398397407, 0.32715669398397407, 0.32715669398397407, 0.3366851664576028, 0.3366851664576028, 0.3366851664576028, 0.09949094618784526, 0.09949094618784526, 0.09949094618784526, 0.15995975310673094, 0.15995975310673094, 0.15995975310673094, 0.11503873289328725, 0.11503873289328725, 0.11503873289328725, 0.1530898894510747, 0.1530898894510747, 0.1530898894510747, 0.16239588146226636, 0.16239588146226636, 0.16239588146226636, 0.2781469143440053, 0.2781469143440053, 0.2781469143440053, 0.1451218439873746, 0.1451218439873746, 0.1451218439873746, 0.18612063026889314, 0.18612063026889314, 0.18612063026889314, 0.16782889823206804, 0.16782889823206804, 0.16782889823206804, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.13255832864225614, 0.13255832864225614, 0.13255832864225614, 0.02895532797217204, 0.02895532797217204, 0.02895532797217204, 0.05579119022640566, 0.05579119022640566, 0.05579119022640566, 0.07148812553261574, 0.07148812553261574, 0.07148812553261574, 0.07565517767597663, 0.07565517767597663, 0.07565517767597663, 0.20124850617277257, 0.20124850617277257, 0.20124850617277257, 0.10652100250706587, 0.10652100250706587, 0.10652100250706587, 0.08282682047957057, 0.08282682047957057, 0.08282682047957057, 0.12977641527397976, 0.12977641527397976, 0.12977641527397976, 0.5021167125956894, 0.5021167125956894, 0.5021167125956894, 0.43986523254953014, 0.43986523254953014, 0.43986523254953014, 0.44973413413979235, 0.44973413413979235, 0.44973413413979235, 0.13317530514713927, 0.13317530514713927, 0.13317530514713927, 0.12597330933558037, 0.12597330933558037, 0.12597330933558037, 0.28701925840869147, 0.28701925840869147, 0.28701925840869147, 0.3742265669910356, 0.3742265669910356, 0.3742265669910356, 0.2038024923086732, 0.2038024923086732, 0.2038024923086732, 0.3177540916848063, 0.3177540916848063, 0.3177540916848063, 0.3515869817676278, 0.3515869817676278, 0.3515869817676278, 0.21715627484476496, 0.21715627484476496, 0.21715627484476496, 0.34439331392217687, 0.34439331392217687, 0.34439331392217687, 0.23499878850402633, 0.23499878850402633, 0.23499878850402633, 0.19559303961531893, 0.19559303961531893, 0.19559303961531893, 0.19307075599715617, 0.19307075599715617, 0.19307075599715617, 0.2276190218744084, 0.2276190218744084, 0.2276190218744084, 0.23705374021594328, 0.23705374021594328, 0.23705374021594328, 0.23915849085038077, 0.23915849085038077, 0.23915849085038077, 0.4139987710301559, 0.4139987710301559, 0.4139987710301559, 0.19053637290786785, 0.19053637290786785, 0.19053637290786785, 0.1890000807487251, 0.1890000807487251, 0.1890000807487251, 0.7438422411320703, 0.7438422411320703, 0.7438422411320703, 0.15690703560214803, 0.15690703560214803, 0.15690703560214803, 0.16794335292973195, 0.16794335292973195, 0.16794335292973195, 0.11011357070948757, 0.11011357070948757, 0.11011357070948757, 0.2093979552110602, 0.2093979552110602, 0.2093979552110602, 0.18048347307493018, 0.18048347307493018, 0.18048347307493018, 0.19629370306491323, 0.19629370306491323, 0.19629370306491323, 0.2003678764620762, 0.2003678764620762, 0.2003678764620762, 0.19288623656300696, 0.19288623656300696, 0.19288623656300696, 0.10134978318886967, 0.10134978318886967, 0.10134978318886967, 0.09421025760191659, 0.09421025760191659, 0.09421025760191659, 0.0997779171873523, 0.0997779171873523, 0.0997779171873523]}, "mutation_prompt": null}
{"id": "ef87e164-1e7d-43b1-84e3-dcdc6b19d3da", "solution": "import numpy as np\n\nclass HybridDESA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.pop_size = 10 * dim\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.temp = 1000.0  # Initial temperature for simulated annealing\n        self.cooling_rate = 0.99  # Cooling rate for simulated annealing\n\n    def __call__(self, func):\n        np.random.seed(42)\n        population = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        num_evaluations = self.pop_size\n\n        def mutate(target_idx):\n            indices = [idx for idx in range(self.pop_size) if idx != target_idx]\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            dynamic_F = self.F * (1.0 - (num_evaluations / self.budget)) * np.exp(-num_evaluations / self.budget)\n            dynamic_F *= (self.temp / 1000.0) * (1 + 0.1 * (num_evaluations / self.budget))  # Modified line: Adaptive scaling to mutation factor\n            mutant = population[a] + dynamic_F * (population[b] - population[c])\n            mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n            return mutant\n\n        while num_evaluations < self.budget:\n            self.CR = 0.9 - (0.3 * (num_evaluations / self.budget)) + 0.1 * np.tanh(num_evaluations / self.budget)  # Modified line: Introduce learning rate update to crossover probability\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                mutant = mutate(i)\n                trial = np.where(np.random.rand(self.dim) < self.CR, mutant, 0.5 * (population[i] + best_solution))\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = trial\n\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                neighbor = population[i] + np.random.normal(0, 0.1 * self.temp / 1000, self.dim)\n                neighbor = np.clip(neighbor, self.lower_bound, self.upper_bound)\n                neighbor_fitness = func(neighbor)\n                num_evaluations += 1\n                delta_fit = neighbor_fitness - fitness[i]\n\n                if delta_fit < 0 or np.random.rand() < np.exp(-delta_fit / self.temp):\n                    population[i] = neighbor\n                    fitness[i] = neighbor_fitness\n                    if neighbor_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = neighbor\n\n            self.temp *= (self.cooling_rate - 0.01 * np.log1p(num_evaluations / self.budget))  # Modified line: Improved cooling strategy with log transformation\n            self.pop_size = max(4, int(8 * self.dim * (1 - num_evaluations / self.budget)))\n\n        return best_solution", "name": "HybridDESA", "description": "Enhance adaptive scaling with log transformation for better exploration-exploitation balance.", "configspace": "", "generation": 70, "fitness": 0.26738078558970624, "feedback": "The algorithm HybridDESA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.27 with standard deviation 0.19.", "error": "", "parent_id": "ba6bec03-35bc-47f6-92da-256729321735", "metadata": {"aucs": [0.5566381169163441, 0.5566381169163441, 0.5566381169163441, 0.5368622135568856, 0.5368622135568856, 0.5368622135568856, 0.5287324307344943, 0.5287324307344943, 0.5287324307344943, 0.17495649395744184, 0.17495649395744184, 0.17495649395744184, 0.31975289497500803, 0.31975289497500803, 0.31975289497500803, 0.32768809301342106, 0.32768809301342106, 0.32768809301342106, 0.157758620608349, 0.157758620608349, 0.157758620608349, 0.13549554683035803, 0.13549554683035803, 0.13549554683035803, 0.13959627308573286, 0.13959627308573286, 0.13959627308573286, 0.09847717058801742, 0.09847717058801742, 0.09847717058801742, 0.09708863436357529, 0.09708863436357529, 0.09708863436357529, 0.08810584007526123, 0.08810584007526123, 0.08810584007526123, 0.8779323642530403, 0.8779323642530403, 0.8779323642530403, 0.8778847898666995, 0.8778847898666995, 0.8778847898666995, 0.7804585497496003, 0.7804585497496003, 0.7804585497496003, 0.3523033417195659, 0.3523033417195659, 0.3523033417195659, 0.3474087217271561, 0.3474087217271561, 0.3474087217271561, 0.34812098652264134, 0.34812098652264134, 0.34812098652264134, 0.34247839114001266, 0.34247839114001266, 0.34247839114001266, 0.5635772646140328, 0.5635772646140328, 0.5635772646140328, 0.7133290644077157, 0.7133290644077157, 0.7133290644077157, 0.1648052426580765, 0.1648052426580765, 0.1648052426580765, 0.17221495824181254, 0.17221495824181254, 0.17221495824181254, 0.1544031667582294, 0.1544031667582294, 0.1544031667582294, 0.15383141691349866, 0.15383141691349866, 0.15383141691349866, 0.17071109926322847, 0.17071109926322847, 0.17071109926322847, 0.17000675693211764, 0.17000675693211764, 0.17000675693211764, 0.13117358714143335, 0.13117358714143335, 0.13117358714143335, 0.13054628851220984, 0.13054628851220984, 0.13054628851220984, 0.09698004877658295, 0.09698004877658295, 0.09698004877658295, 0.1615453648794355, 0.1615453648794355, 0.1615453648794355, 0.1136436054176746, 0.1136436054176746, 0.1136436054176746, 0.147872834710696, 0.147872834710696, 0.147872834710696, 0.17619422541650132, 0.17619422541650132, 0.17619422541650132, 0.10523005286609532, 0.10523005286609532, 0.10523005286609532, 0.12555013479130195, 0.12555013479130195, 0.12555013479130195, 0.08649906597348656, 0.08649906597348656, 0.08649906597348656, 0.15844860112896963, 0.15844860112896963, 0.15844860112896963, 0.1495438598808816, 0.1495438598808816, 0.1495438598808816, 0.4621467859733811, 0.4621467859733811, 0.4621467859733811, 0.4497184670009885, 0.4497184670009885, 0.4497184670009885, 0.45270535513228916, 0.45270535513228916, 0.45270535513228916, 0.1367592431538749, 0.1367592431538749, 0.1367592431538749, 0.15023340707779276, 0.15023340707779276, 0.15023340707779276, 0.15193655966541886, 0.15193655966541886, 0.15193655966541886, 0.1660327114116622, 0.1660327114116622, 0.1660327114116622, 0.21079811174088758, 0.21079811174088758, 0.21079811174088758, 0.18677766602078705, 0.18677766602078705, 0.18677766602078705, 0.326403794030238, 0.326403794030238, 0.326403794030238, 0.3314142872533423, 0.3314142872533423, 0.3314142872533423, 0.3278262310117177, 0.3278262310117177, 0.3278262310117177, 0.2687805229831868, 0.2687805229831868, 0.2687805229831868, 0.25192363968363785, 0.25192363968363785, 0.25192363968363785, 0.27664256302721957, 0.27664256302721957, 0.27664256302721957, 0.2125901405984152, 0.2125901405984152, 0.2125901405984152, 0.21725204466669346, 0.21725204466669346, 0.21725204466669346, 0.2555003126739631, 0.2555003126739631, 0.2555003126739631, 0.19241436144083357, 0.19241436144083357, 0.19241436144083357, 0.17815631802910248, 0.17815631802910248, 0.17815631802910248, 0.197269601087525, 0.197269601087525, 0.197269601087525, 0.7002974621020006, 0.7002974621020006, 0.7002974621020006, 0.15136555224667358, 0.15136555224667358, 0.15136555224667358, 0.6084728500401908, 0.6084728500401908, 0.6084728500401908, 0.16572398069533278, 0.16572398069533278, 0.16572398069533278, 0.19683170541194506, 0.19683170541194506, 0.19683170541194506, 0.2040548777010056, 0.2040548777010056, 0.2040548777010056, 0.20508571107971785, 0.20508571107971785, 0.20508571107971785, 0.20162072708311585, 0.20162072708311585, 0.20162072708311585, 0.1982682799293496, 0.1982682799293496, 0.1982682799293496, 0.09695568151578515, 0.09695568151578515, 0.09695568151578515, 0.09462642472094085, 0.09462642472094085, 0.09462642472094085, 0.09098507330224836, 0.09098507330224836, 0.09098507330224836]}, "mutation_prompt": null}
{"id": "36a2a41d-1c99-4df2-bdf6-86ba6c63ec53", "solution": "import numpy as np\n\nclass HybridDESA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.pop_size = 10 * dim\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.temp = 1000.0  # Initial temperature for simulated annealing\n        self.cooling_rate = 0.99  # Cooling rate for simulated annealing\n\n    def __call__(self, func):\n        np.random.seed(42)\n        population = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        num_evaluations = self.pop_size\n\n        def mutate(target_idx):\n            indices = [idx for idx in range(self.pop_size) if idx != target_idx]\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            # Modified line: Introduce dynamic adjustment to differential weight\n            dynamic_F = self.F * (1.0 - (num_evaluations / self.budget)) * np.exp(-num_evaluations / self.budget) * (1 + 0.2 * np.sin(num_evaluations / self.budget * np.pi))  \n            dynamic_F *= (self.temp / 1000.0) * (1 + 0.1 * (num_evaluations / self.budget)) \n            mutant = population[a] + dynamic_F * (population[b] - population[c])\n            mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n            return mutant\n\n        while num_evaluations < self.budget:\n            self.CR = 0.9 - (0.3 * (num_evaluations / self.budget)) + 0.1 * np.tanh(num_evaluations / self.budget)  \n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                mutant = mutate(i)\n                trial = np.where(np.random.rand(self.dim) < self.CR, mutant, 0.5 * (population[i] + best_solution))\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = trial\n\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                neighbor = population[i] + np.random.normal(0, 0.1 * self.temp / 1000, self.dim)\n                neighbor = np.clip(neighbor, self.lower_bound, self.upper_bound)\n                neighbor_fitness = func(neighbor)\n                num_evaluations += 1\n                delta_fit = neighbor_fitness - fitness[i]\n\n                if delta_fit < 0 or np.random.rand() < np.exp(-delta_fit / self.temp):\n                    population[i] = neighbor\n                    fitness[i] = neighbor_fitness\n                    if neighbor_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = neighbor\n\n            self.temp *= (self.cooling_rate - 0.01 * (num_evaluations / self.budget))\n            self.pop_size = max(4, int(8 * self.dim * (1 - num_evaluations / self.budget)))\n\n        return best_solution", "name": "HybridDESA", "description": "Introduce a dynamic adjustment to the differential weight to balance exploration and exploitation.", "configspace": "", "generation": 71, "fitness": 0.25574376693735995, "feedback": "The algorithm HybridDESA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.26 with standard deviation 0.19.", "error": "", "parent_id": "ba6bec03-35bc-47f6-92da-256729321735", "metadata": {"aucs": [0.5447595880523763, 0.5447595880523763, 0.5447595880523763, 0.5475296703897466, 0.5475296703897466, 0.5475296703897466, 0.5568938727631593, 0.5568938727631593, 0.5568938727631593, 0.28300762584228134, 0.28300762584228134, 0.28300762584228134, 0.19445812835280218, 0.19445812835280218, 0.19445812835280218, 0.28404818824033695, 0.28404818824033695, 0.28404818824033695, 0.1164659368146247, 0.1164659368146247, 0.1164659368146247, 0.1236034825281398, 0.1236034825281398, 0.1236034825281398, 0.15567393137447894, 0.15567393137447894, 0.15567393137447894, 0.11854737686476657, 0.11854737686476657, 0.11854737686476657, 0.10304273820761733, 0.10304273820761733, 0.10304273820761733, 0.1129307657956159, 0.1129307657956159, 0.1129307657956159, 0.9096360118012654, 0.9096360118012654, 0.9096360118012654, 0.8985947231145863, 0.8985947231145863, 0.8985947231145863, 0.8442887940773571, 0.8442887940773571, 0.8442887940773571, 0.3394666424712164, 0.3394666424712164, 0.3394666424712164, 0.3387495415441185, 0.3387495415441185, 0.3387495415441185, 0.3466745954785183, 0.3466745954785183, 0.3466745954785183, 0.23822547058913446, 0.23822547058913446, 0.23822547058913446, 0.62971306996546, 0.62971306996546, 0.62971306996546, 0.22314858824342232, 0.22314858824342232, 0.22314858824342232, 0.16423281036413862, 0.16423281036413862, 0.16423281036413862, 0.15307394510027628, 0.15307394510027628, 0.15307394510027628, 0.1722131100061639, 0.1722131100061639, 0.1722131100061639, 0.13938663068991486, 0.13938663068991486, 0.13938663068991486, 0.17849600510549513, 0.17849600510549513, 0.17849600510549513, 0.1642614346166421, 0.1642614346166421, 0.1642614346166421, 0.06704549286169081, 0.06704549286169081, 0.06704549286169081, 0.11349319083291687, 0.11349319083291687, 0.11349319083291687, 0.10392855668847512, 0.10392855668847512, 0.10392855668847512, 0.18666339537634835, 0.18666339537634835, 0.18666339537634835, 0.11846243503165443, 0.11846243503165443, 0.11846243503165443, 0.11875957902630108, 0.11875957902630108, 0.11875957902630108, 0.048913457155894347, 0.048913457155894347, 0.048913457155894347, 0.07912418196932558, 0.07912418196932558, 0.07912418196932558, 0.11351612903298969, 0.11351612903298969, 0.11351612903298969, 0.1409168236667141, 0.1409168236667141, 0.1409168236667141, 0.1302427415873364, 0.1302427415873364, 0.1302427415873364, 0.16078023454583212, 0.16078023454583212, 0.16078023454583212, 0.46134662749730426, 0.46134662749730426, 0.46134662749730426, 0.4263531039339439, 0.4263531039339439, 0.4263531039339439, 0.4339198599458963, 0.4339198599458963, 0.4339198599458963, 0.27073020746668863, 0.27073020746668863, 0.27073020746668863, 0.1373517230507849, 0.1373517230507849, 0.1373517230507849, 0.1317251794102967, 0.1317251794102967, 0.1317251794102967, 0.20493073660534067, 0.20493073660534067, 0.20493073660534067, 0.2503373629845205, 0.2503373629845205, 0.2503373629845205, 0.2051027232342254, 0.2051027232342254, 0.2051027232342254, 0.3321900657613889, 0.3321900657613889, 0.3321900657613889, 0.3357365116721852, 0.3357365116721852, 0.3357365116721852, 0.3364293587264191, 0.3364293587264191, 0.3364293587264191, 0.2623369214060546, 0.2623369214060546, 0.2623369214060546, 0.26960183621348843, 0.26960183621348843, 0.26960183621348843, 0.25825284460010167, 0.25825284460010167, 0.25825284460010167, 0.24543976807205636, 0.24543976807205636, 0.24543976807205636, 0.2067477740828464, 0.2067477740828464, 0.2067477740828464, 0.22854823022441617, 0.22854823022441617, 0.22854823022441617, 0.18370642140925197, 0.18370642140925197, 0.18370642140925197, 0.3619704699624079, 0.3619704699624079, 0.3619704699624079, 0.22153505904123239, 0.22153505904123239, 0.22153505904123239, 0.16361100816003016, 0.16361100816003016, 0.16361100816003016, 0.16057932125943986, 0.16057932125943986, 0.16057932125943986, 0.15915999845247042, 0.15915999845247042, 0.15915999845247042, 0.16550947813381156, 0.16550947813381156, 0.16550947813381156, 0.1972678292663772, 0.1972678292663772, 0.1972678292663772, 0.6333578510631578, 0.6333578510631578, 0.6333578510631578, 0.22040026226184273, 0.22040026226184273, 0.22040026226184273, 0.1948858703594497, 0.1948858703594497, 0.1948858703594497, 0.20765497834092506, 0.20765497834092506, 0.20765497834092506, 0.09553016308357853, 0.09553016308357853, 0.09553016308357853, 0.09461569151244498, 0.09461569151244498, 0.09461569151244498, 0.09371708612640373, 0.09371708612640373, 0.09371708612640373]}, "mutation_prompt": null}
{"id": "1003954f-6c39-4918-be27-eeb55834b8e9", "solution": "import numpy as np\n\nclass HybridDESA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.pop_size = 10 * dim\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.temp = 1000.0  # Initial temperature for simulated annealing\n        self.cooling_rate = 0.99  # Cooling rate for simulated annealing\n\n    def __call__(self, func):\n        np.random.seed(42)\n        population = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        num_evaluations = self.pop_size\n\n        def mutate(target_idx):\n            indices = [idx for idx in range(self.pop_size) if idx != target_idx]\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            dynamic_F = self.F * np.cos(np.pi * (num_evaluations / self.budget) / 2)  # Modified line: Nonlinear decay function for mutation factor\n            dynamic_F *= (self.temp / 1000.0) * (1 + 0.1 * (num_evaluations / self.budget))\n            mutant = population[a] + dynamic_F * (population[b] - population[c])\n            mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n            return mutant\n\n        while num_evaluations < self.budget:\n            self.CR = 0.9 - (0.3 * (num_evaluations / self.budget)) + 0.1 * np.tanh(num_evaluations / self.budget)\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                mutant = mutate(i)\n                trial = np.where(np.random.rand(self.dim) < self.CR, mutant, 0.5 * (population[i] + best_solution))\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = trial\n\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                neighbor = population[i] + np.random.normal(0, 0.1 * self.temp / 1000, self.dim)\n                neighbor = np.clip(neighbor, self.lower_bound, self.upper_bound)\n                neighbor_fitness = func(neighbor)\n                num_evaluations += 1\n                delta_fit = neighbor_fitness - fitness[i]\n\n                if delta_fit < 0 or np.random.rand() < np.exp(-delta_fit / self.temp):\n                    population[i] = neighbor\n                    fitness[i] = neighbor_fitness\n                    if neighbor_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = neighbor\n\n            self.temp *= (self.cooling_rate - 0.01 * (num_evaluations / self.budget))\n            self.pop_size = max(4, int(8 * self.dim * (1 - num_evaluations / self.budget)))\n\n        return best_solution", "name": "HybridDESA", "description": "Enhance the mutation factor to utilize a nonlinear decay function for better exploration-exploitation balance.", "configspace": "", "generation": 72, "fitness": 0.2506392206824168, "feedback": "The algorithm HybridDESA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.25 with standard deviation 0.18.", "error": "", "parent_id": "ba6bec03-35bc-47f6-92da-256729321735", "metadata": {"aucs": [0.5387391544211126, 0.5387391544211126, 0.5387391544211126, 0.5333058314559735, 0.5333058314559735, 0.5333058314559735, 0.525625234587649, 0.525625234587649, 0.525625234587649, 0.23172703445492904, 0.23172703445492904, 0.23172703445492904, 0.17397422099742832, 0.17397422099742832, 0.17397422099742832, 0.278299206691926, 0.278299206691926, 0.278299206691926, 0.13258040116872105, 0.13258040116872105, 0.13258040116872105, 0.13713202291969917, 0.13713202291969917, 0.13713202291969917, 0.15028306217060183, 0.15028306217060183, 0.15028306217060183, 0.09567362336492913, 0.09567362336492913, 0.09567362336492913, 0.09586616393063285, 0.09586616393063285, 0.09586616393063285, 0.11043022453375684, 0.11043022453375684, 0.11043022453375684, 0.8705833944585827, 0.8705833944585827, 0.8705833944585827, 0.9482936518039126, 0.9482936518039126, 0.9482936518039126, 0.8104335689876464, 0.8104335689876464, 0.8104335689876464, 0.34367247551775937, 0.34367247551775937, 0.34367247551775937, 0.3411363930795974, 0.3411363930795974, 0.3411363930795974, 0.348395550518092, 0.348395550518092, 0.348395550518092, 0.29995103725293515, 0.29995103725293515, 0.29995103725293515, 0.26212310112499804, 0.26212310112499804, 0.26212310112499804, 0.6017385835546937, 0.6017385835546937, 0.6017385835546937, 0.15620022731959615, 0.15620022731959615, 0.15620022731959615, 0.14158394891960802, 0.14158394891960802, 0.14158394891960802, 0.15066929354125091, 0.15066929354125091, 0.15066929354125091, 0.10622990088810325, 0.10622990088810325, 0.10622990088810325, 0.152778061999167, 0.152778061999167, 0.152778061999167, 0.15119584260451902, 0.15119584260451902, 0.15119584260451902, 0.07208538514937224, 0.07208538514937224, 0.07208538514937224, 0.09551292465555594, 0.09551292465555594, 0.09551292465555594, 0.1032225139347791, 0.1032225139347791, 0.1032225139347791, 0.17253214770943792, 0.17253214770943792, 0.17253214770943792, 0.10259295668015056, 0.10259295668015056, 0.10259295668015056, 0.18443138939552917, 0.18443138939552917, 0.18443138939552917, 0.11878211640471947, 0.11878211640471947, 0.11878211640471947, 0.06549407724613288, 0.06549407724613288, 0.06549407724613288, 0.18181473195707243, 0.18181473195707243, 0.18181473195707243, 0.1510141115483462, 0.1510141115483462, 0.1510141115483462, 0.13938977012620457, 0.13938977012620457, 0.13938977012620457, 0.127183228457461, 0.127183228457461, 0.127183228457461, 0.43427368485197915, 0.43427368485197915, 0.43427368485197915, 0.457486972167763, 0.457486972167763, 0.457486972167763, 0.4564333487309731, 0.4564333487309731, 0.4564333487309731, 0.12278007996513274, 0.12278007996513274, 0.12278007996513274, 0.25026801948764965, 0.25026801948764965, 0.25026801948764965, 0.1290712046874407, 0.1290712046874407, 0.1290712046874407, 0.21676223375990122, 0.21676223375990122, 0.21676223375990122, 0.31100918659816257, 0.31100918659816257, 0.31100918659816257, 0.18120311205899742, 0.18120311205899742, 0.18120311205899742, 0.3324441409244163, 0.3324441409244163, 0.3324441409244163, 0.3256112013236844, 0.3256112013236844, 0.3256112013236844, 0.3291026922041752, 0.3291026922041752, 0.3291026922041752, 0.2248370405987652, 0.2248370405987652, 0.2248370405987652, 0.26939176049667535, 0.26939176049667535, 0.26939176049667535, 0.26731147086787155, 0.26731147086787155, 0.26731147086787155, 0.21285125113358594, 0.21285125113358594, 0.21285125113358594, 0.23758052864278734, 0.23758052864278734, 0.23758052864278734, 0.2667947190378046, 0.2667947190378046, 0.2667947190378046, 0.2094530290920118, 0.2094530290920118, 0.2094530290920118, 0.1893273113530929, 0.1893273113530929, 0.1893273113530929, 0.22179682918306753, 0.22179682918306753, 0.22179682918306753, 0.17766967578708026, 0.17766967578708026, 0.17766967578708026, 0.17261893990882926, 0.17261893990882926, 0.17261893990882926, 0.18641703342825744, 0.18641703342825744, 0.18641703342825744, 0.20247694991525744, 0.20247694991525744, 0.20247694991525744, 0.2129403454310601, 0.2129403454310601, 0.2129403454310601, 0.36481913246241027, 0.36481913246241027, 0.36481913246241027, 0.19240706585014555, 0.19240706585014555, 0.19240706585014555, 0.20660661344797038, 0.20660661344797038, 0.20660661344797038, 0.19998826796469915, 0.19998826796469915, 0.19998826796469915, 0.09853773222048745, 0.09853773222048745, 0.09853773222048745, 0.09262701019932607, 0.09262701019932607, 0.09262701019932607, 0.09044873981996737, 0.09044873981996737, 0.09044873981996737]}, "mutation_prompt": null}
{"id": "f19698e7-d6a2-45f2-ae09-5151a82fe97e", "solution": "import numpy as np\n\nclass HybridDESA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.pop_size = 10 * dim\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.temp = 1000.0  # Initial temperature for simulated annealing\n        self.cooling_rate = 0.99  # Cooling rate for simulated annealing\n\n    def __call__(self, func):\n        np.random.seed(42)\n        population = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        num_evaluations = self.pop_size\n\n        def mutate(target_idx):\n            indices = [idx for idx in range(self.pop_size) if idx != target_idx]\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            dynamic_F = self.F * (1.0 - (num_evaluations / self.budget)) * np.exp(-num_evaluations / self.budget)\n            dynamic_F *= (self.temp / 1000.0) * (1 + 0.1 * (num_evaluations / self.budget))  # Modified line: Adaptive scaling to mutation factor\n            mutant = population[a] + dynamic_F * (population[b] - population[c])\n            mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n            return mutant\n\n        while num_evaluations < self.budget:\n            self.CR = 0.9 - (0.3 * (num_evaluations / self.budget)) + 0.1 * np.tanh(num_evaluations / self.budget)  # Modified line: Introduce learning rate update to crossover probability\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                mutant = mutate(i)\n                trial = np.where(np.random.rand(self.dim) < self.CR, mutant, 0.5 * (population[i] + best_solution))\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = trial\n\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                neighbor = population[i] + np.random.normal(0, 0.1 * self.temp / 1000, self.dim)\n                neighbor = np.clip(neighbor, self.lower_bound, self.upper_bound)\n                neighbor_fitness = func(neighbor)\n                num_evaluations += 1\n                delta_fit = neighbor_fitness - fitness[i]\n\n                if delta_fit < 0 or np.random.rand() < np.exp(-delta_fit / self.temp):\n                    population[i] = neighbor\n                    fitness[i] = neighbor_fitness\n                    if neighbor_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = neighbor\n\n            self.temp *= (self.cooling_rate - 0.01 * (num_evaluations / self.budget))\n            self.pop_size = max(4, int(8 * self.dim * (1 - num_evaluations / self.budget)))\n\n        return best_solution", "name": "HybridDESA", "description": "Introduce learning rate based update to the crossover probability to enhance exploration capabilities.", "configspace": "", "generation": 63, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "ba6bec03-35bc-47f6-92da-256729321735", "metadata": {"aucs": [0.5542661297327882, 0.5542661297327882, 0.5542661297327882, 0.5502131781010412, 0.5502131781010412, 0.5502131781010412, 0.5542576829571606, 0.5542576829571606, 0.5542576829571606, 0.3214986489793178, 0.3214986489793178, 0.3214986489793178, 0.3065416684084221, 0.3065416684084221, 0.3065416684084221, 0.3158065919396942, 0.3158065919396942, 0.3158065919396942, 0.13838181317138243, 0.13838181317138243, 0.13838181317138243, 0.12545327945625218, 0.12545327945625218, 0.12545327945625218, 0.12341091429570572, 0.12341091429570572, 0.12341091429570572, 0.10743269946665945, 0.10743269946665945, 0.10743269946665945, 0.1047695610119097, 0.1047695610119097, 0.1047695610119097, 0.11944414538050097, 0.11944414538050097, 0.11944414538050097, 0.8779320540888479, 0.8779320540888479, 0.8779320540888479, 0.877898188844995, 0.877898188844995, 0.877898188844995, 0.780456822675078, 0.780456822675078, 0.780456822675078, 0.3563309678652393, 0.3563309678652393, 0.3563309678652393, 0.3648861206411198, 0.3648861206411198, 0.3648861206411198, 0.3566431522159045, 0.3566431522159045, 0.3566431522159045, 0.6706862034126804, 0.6706862034126804, 0.6706862034126804, 0.3423519789978463, 0.3423519789978463, 0.3423519789978463, 0.72668247747501, 0.72668247747501, 0.72668247747501, 0.16919472358197873, 0.16919472358197873, 0.16919472358197873, 0.1651292859417527, 0.1651292859417527, 0.1651292859417527, 0.14989293282023441, 0.14989293282023441, 0.14989293282023441, 0.14250677278277657, 0.14250677278277657, 0.14250677278277657, 0.1589362233709246, 0.1589362233709246, 0.1589362233709246, 0.11857286877661266, 0.11857286877661266, 0.11857286877661266, 0.07046504581570368, 0.07046504581570368, 0.07046504581570368, 0.09291593469393078, 0.09291593469393078, 0.09291593469393078, 0.10976115636052908, 0.10976115636052908, 0.10976115636052908, 0.15193954057356018, 0.15193954057356018, 0.15193954057356018, 0.1246100015034457, 0.1246100015034457, 0.1246100015034457, 0.15848011734590806, 0.15848011734590806, 0.15848011734590806, 0.2108696082750796, 0.2108696082750796, 0.2108696082750796, 0.17218218373984961, 0.17218218373984961, 0.17218218373984961, 0.2329385394943635, 0.2329385394943635, 0.2329385394943635, 0.10591689517605296, 0.10591689517605296, 0.10591689517605296, 0.1552473120636909, 0.1552473120636909, 0.1552473120636909, 0.12199788623767405, 0.12199788623767405, 0.12199788623767405, 0.4593083307916008, 0.4593083307916008, 0.4593083307916008, 0.4628141347100536, 0.4628141347100536, 0.4628141347100536, 0.4549634008249209, 0.4549634008249209, 0.4549634008249209, 0.14035267481144242, 0.14035267481144242, 0.14035267481144242, 0.2647056139184356, 0.2647056139184356, 0.2647056139184356, 0.13663480456036958, 0.13663480456036958, 0.13663480456036958, 0.20334182760801733, 0.20334182760801733, 0.20334182760801733, 0.17681323754407496, 0.17681323754407496, 0.17681323754407496, 0.21871292997332525, 0.21871292997332525, 0.21871292997332525, 0.32825394007153186, 0.32825394007153186, 0.32825394007153186, 0.3326823931838664, 0.3326823931838664, 0.3326823931838664, 0.3421300043168062, 0.3421300043168062, 0.3421300043168062, 0.2724755833403435, 0.2724755833403435, 0.2724755833403435, 0.20678815163767184, 0.20678815163767184, 0.20678815163767184, 0.27220142835892625, 0.27220142835892625, 0.27220142835892625, 0.22900362693501475, 0.22900362693501475, 0.22900362693501475, 0.23990890494473383, 0.23990890494473383, 0.23990890494473383, 0.23540240144705793, 0.23540240144705793, 0.23540240144705793, 0.19425768105254093, 0.19425768105254093, 0.19425768105254093, 0.18912798567843125, 0.18912798567843125, 0.18912798567843125, 0.19106985300632762, 0.19106985300632762, 0.19106985300632762, 0.7025277924101051, 0.7025277924101051, 0.7025277924101051, 0.15133519477843094, 0.15133519477843094, 0.15133519477843094, 0.6240958961911556, 0.6240958961911556, 0.6240958961911556, 0.16573198475929618, 0.16573198475929618, 0.16573198475929618, 0.19732013670547166, 0.19732013670547166, 0.19732013670547166, 0.20412171108102173, 0.20412171108102173, 0.20412171108102173, 0.20894043819099617, 0.20894043819099617, 0.20894043819099617, 0.1933116987478113, 0.1933116987478113, 0.1933116987478113, 0.1933601729649982, 0.1933601729649982, 0.1933601729649982, 0.09564479257847158, 0.09564479257847158, 0.09564479257847158, 0.09537324264215674, 0.09537324264215674, 0.09537324264215674, 0.09878679773303067, 0.09878679773303067, 0.09878679773303067]}, "mutation_prompt": null}
{"id": "5dba6508-8694-4be4-ab51-b42462476359", "solution": "import numpy as np\n\nclass HybridDESA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.pop_size = 10 * dim\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.temp = 1000.0  # Initial temperature for simulated annealing\n        self.cooling_rate = 0.99  # Cooling rate for simulated annealing\n\n    def __call__(self, func):\n        np.random.seed(42)\n        population = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        num_evaluations = self.pop_size\n\n        def mutate(target_idx):\n            indices = [idx for idx in range(self.pop_size) if idx != target_idx]\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            dynamic_F = self.F * (1.0 - (num_evaluations / self.budget)) * np.exp(-num_evaluations / self.budget)\n            dynamic_F *= (self.temp / 1000.0) * (1 + 0.1 * (num_evaluations / self.budget))  # Modified line: Adaptive scaling to mutation factor\n            mutant = population[a] + dynamic_F * (population[b] - population[c])\n            mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n            return mutant\n\n        while num_evaluations < self.budget:\n            self.CR = 0.9 - (0.3 * (num_evaluations / self.budget)) + 0.1 * np.tanh(num_evaluations / self.budget)  # Modified line: Introduce learning rate update to crossover probability\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                mutant = mutate(i)\n                trial = np.where(np.random.rand(self.dim) < self.CR, mutant, 0.5 * (population[i] + best_solution))\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = trial\n\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                neighbor = population[i] + np.random.normal(0, 0.1 * self.temp / 1000, self.dim)\n                neighbor = np.clip(neighbor, self.lower_bound, self.upper_bound)\n                neighbor_fitness = func(neighbor)\n                num_evaluations += 1\n                delta_fit = neighbor_fitness - fitness[i]\n\n                if delta_fit < 0 or np.random.rand() < np.exp(-delta_fit / self.temp):\n                    population[i] = neighbor\n                    fitness[i] = neighbor_fitness\n                    if neighbor_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = neighbor\n\n            self.temp *= (self.cooling_rate - 0.01 * (num_evaluations / self.budget))\n            self.pop_size = max(4, int(8 * self.dim * (1 - num_evaluations / self.budget)))\n\n        return best_solution", "name": "HybridDESA", "description": "Introduce learning rate based update to the crossover probability to enhance exploration capabilities.", "configspace": "", "generation": 63, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "ba6bec03-35bc-47f6-92da-256729321735", "metadata": {"aucs": [0.5542661297327882, 0.5542661297327882, 0.5542661297327882, 0.5502131781010412, 0.5502131781010412, 0.5502131781010412, 0.5542576829571606, 0.5542576829571606, 0.5542576829571606, 0.3214986489793178, 0.3214986489793178, 0.3214986489793178, 0.3065416684084221, 0.3065416684084221, 0.3065416684084221, 0.3158065919396942, 0.3158065919396942, 0.3158065919396942, 0.13838181317138243, 0.13838181317138243, 0.13838181317138243, 0.12545327945625218, 0.12545327945625218, 0.12545327945625218, 0.12341091429570572, 0.12341091429570572, 0.12341091429570572, 0.10743269946665945, 0.10743269946665945, 0.10743269946665945, 0.1047695610119097, 0.1047695610119097, 0.1047695610119097, 0.11944414538050097, 0.11944414538050097, 0.11944414538050097, 0.8779320540888479, 0.8779320540888479, 0.8779320540888479, 0.877898188844995, 0.877898188844995, 0.877898188844995, 0.780456822675078, 0.780456822675078, 0.780456822675078, 0.3563309678652393, 0.3563309678652393, 0.3563309678652393, 0.3648861206411198, 0.3648861206411198, 0.3648861206411198, 0.3566431522159045, 0.3566431522159045, 0.3566431522159045, 0.6706862034126804, 0.6706862034126804, 0.6706862034126804, 0.3423519789978463, 0.3423519789978463, 0.3423519789978463, 0.72668247747501, 0.72668247747501, 0.72668247747501, 0.16919472358197873, 0.16919472358197873, 0.16919472358197873, 0.1651292859417527, 0.1651292859417527, 0.1651292859417527, 0.14989293282023441, 0.14989293282023441, 0.14989293282023441, 0.14250677278277657, 0.14250677278277657, 0.14250677278277657, 0.1589362233709246, 0.1589362233709246, 0.1589362233709246, 0.11857286877661266, 0.11857286877661266, 0.11857286877661266, 0.07046504581570368, 0.07046504581570368, 0.07046504581570368, 0.09291593469393078, 0.09291593469393078, 0.09291593469393078, 0.10976115636052908, 0.10976115636052908, 0.10976115636052908, 0.15193954057356018, 0.15193954057356018, 0.15193954057356018, 0.1246100015034457, 0.1246100015034457, 0.1246100015034457, 0.15848011734590806, 0.15848011734590806, 0.15848011734590806, 0.2108696082750796, 0.2108696082750796, 0.2108696082750796, 0.17218218373984961, 0.17218218373984961, 0.17218218373984961, 0.2329385394943635, 0.2329385394943635, 0.2329385394943635, 0.10591689517605296, 0.10591689517605296, 0.10591689517605296, 0.1552473120636909, 0.1552473120636909, 0.1552473120636909, 0.12199788623767405, 0.12199788623767405, 0.12199788623767405, 0.4593083307916008, 0.4593083307916008, 0.4593083307916008, 0.4628141347100536, 0.4628141347100536, 0.4628141347100536, 0.4549634008249209, 0.4549634008249209, 0.4549634008249209, 0.14035267481144242, 0.14035267481144242, 0.14035267481144242, 0.2647056139184356, 0.2647056139184356, 0.2647056139184356, 0.13663480456036958, 0.13663480456036958, 0.13663480456036958, 0.20334182760801733, 0.20334182760801733, 0.20334182760801733, 0.17681323754407496, 0.17681323754407496, 0.17681323754407496, 0.21871292997332525, 0.21871292997332525, 0.21871292997332525, 0.32825394007153186, 0.32825394007153186, 0.32825394007153186, 0.3326823931838664, 0.3326823931838664, 0.3326823931838664, 0.3421300043168062, 0.3421300043168062, 0.3421300043168062, 0.2724755833403435, 0.2724755833403435, 0.2724755833403435, 0.20678815163767184, 0.20678815163767184, 0.20678815163767184, 0.27220142835892625, 0.27220142835892625, 0.27220142835892625, 0.22900362693501475, 0.22900362693501475, 0.22900362693501475, 0.23990890494473383, 0.23990890494473383, 0.23990890494473383, 0.23540240144705793, 0.23540240144705793, 0.23540240144705793, 0.19425768105254093, 0.19425768105254093, 0.19425768105254093, 0.18912798567843125, 0.18912798567843125, 0.18912798567843125, 0.19106985300632762, 0.19106985300632762, 0.19106985300632762, 0.7025277924101051, 0.7025277924101051, 0.7025277924101051, 0.15133519477843094, 0.15133519477843094, 0.15133519477843094, 0.6240958961911556, 0.6240958961911556, 0.6240958961911556, 0.16573198475929618, 0.16573198475929618, 0.16573198475929618, 0.19732013670547166, 0.19732013670547166, 0.19732013670547166, 0.20412171108102173, 0.20412171108102173, 0.20412171108102173, 0.20894043819099617, 0.20894043819099617, 0.20894043819099617, 0.1933116987478113, 0.1933116987478113, 0.1933116987478113, 0.1933601729649982, 0.1933601729649982, 0.1933601729649982, 0.09564479257847158, 0.09564479257847158, 0.09564479257847158, 0.09537324264215674, 0.09537324264215674, 0.09537324264215674, 0.09878679773303067, 0.09878679773303067, 0.09878679773303067]}, "mutation_prompt": null}
{"id": "43fb441b-4537-4bfc-9f46-b62f0ab8a3df", "solution": "import numpy as np\n\nclass HybridDESA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.pop_size = 10 * dim\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.temp = 1000.0  # Initial temperature for simulated annealing\n        self.cooling_rate = 0.99  # Cooling rate for simulated annealing\n\n    def __call__(self, func):\n        np.random.seed(42)\n        population = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        num_evaluations = self.pop_size\n\n        def mutate(target_idx):\n            indices = [idx for idx in range(self.pop_size) if idx != target_idx]\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            dynamic_F = self.F * (1.0 - (num_evaluations / self.budget)) * np.exp(-num_evaluations / self.budget)\n            dynamic_F *= (self.temp / 1000.0) * (1 + 0.1 * (num_evaluations / self.budget))  # Modified line: Adaptive scaling to mutation factor\n            mutant = population[a] + dynamic_F * (population[b] - population[c])\n            mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n            return mutant\n\n        while num_evaluations < self.budget:\n            self.CR = 0.9 - (0.3 * (num_evaluations / self.budget)) + 0.1 * np.tanh(num_evaluations / self.budget)  # Modified line: Introduce learning rate update to crossover probability\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                mutant = mutate(i)\n                trial = np.where(np.random.rand(self.dim) < self.CR, mutant, 0.5 * (population[i] + best_solution))\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = trial\n\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                neighbor = population[i] + np.random.normal(0, 0.1 * self.temp / 1000, self.dim)\n                neighbor = np.clip(neighbor, self.lower_bound, self.upper_bound)\n                neighbor_fitness = func(neighbor)\n                num_evaluations += 1\n                delta_fit = neighbor_fitness - fitness[i]\n\n                if delta_fit < 0 or np.random.rand() < np.exp(-delta_fit / self.temp):\n                    population[i] = neighbor\n                    fitness[i] = neighbor_fitness\n                    if neighbor_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = neighbor\n\n            self.temp *= (self.cooling_rate - 0.01 * (num_evaluations / self.budget))\n            self.pop_size = max(4, int(8 * self.dim * (1 - num_evaluations / self.budget)))\n\n        return best_solution", "name": "HybridDESA", "description": "Introduce learning rate based update to the crossover probability to enhance exploration capabilities.", "configspace": "", "generation": 63, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "ba6bec03-35bc-47f6-92da-256729321735", "metadata": {"aucs": [0.5542661297327882, 0.5542661297327882, 0.5542661297327882, 0.5502131781010412, 0.5502131781010412, 0.5502131781010412, 0.5542576829571606, 0.5542576829571606, 0.5542576829571606, 0.3214986489793178, 0.3214986489793178, 0.3214986489793178, 0.3065416684084221, 0.3065416684084221, 0.3065416684084221, 0.3158065919396942, 0.3158065919396942, 0.3158065919396942, 0.13838181317138243, 0.13838181317138243, 0.13838181317138243, 0.12545327945625218, 0.12545327945625218, 0.12545327945625218, 0.12341091429570572, 0.12341091429570572, 0.12341091429570572, 0.10743269946665945, 0.10743269946665945, 0.10743269946665945, 0.1047695610119097, 0.1047695610119097, 0.1047695610119097, 0.11944414538050097, 0.11944414538050097, 0.11944414538050097, 0.8779320540888479, 0.8779320540888479, 0.8779320540888479, 0.877898188844995, 0.877898188844995, 0.877898188844995, 0.780456822675078, 0.780456822675078, 0.780456822675078, 0.3563309678652393, 0.3563309678652393, 0.3563309678652393, 0.3648861206411198, 0.3648861206411198, 0.3648861206411198, 0.3566431522159045, 0.3566431522159045, 0.3566431522159045, 0.6706862034126804, 0.6706862034126804, 0.6706862034126804, 0.3423519789978463, 0.3423519789978463, 0.3423519789978463, 0.72668247747501, 0.72668247747501, 0.72668247747501, 0.16919472358197873, 0.16919472358197873, 0.16919472358197873, 0.1651292859417527, 0.1651292859417527, 0.1651292859417527, 0.14989293282023441, 0.14989293282023441, 0.14989293282023441, 0.14250677278277657, 0.14250677278277657, 0.14250677278277657, 0.1589362233709246, 0.1589362233709246, 0.1589362233709246, 0.11857286877661266, 0.11857286877661266, 0.11857286877661266, 0.07046504581570368, 0.07046504581570368, 0.07046504581570368, 0.09291593469393078, 0.09291593469393078, 0.09291593469393078, 0.10976115636052908, 0.10976115636052908, 0.10976115636052908, 0.15193954057356018, 0.15193954057356018, 0.15193954057356018, 0.1246100015034457, 0.1246100015034457, 0.1246100015034457, 0.15848011734590806, 0.15848011734590806, 0.15848011734590806, 0.2108696082750796, 0.2108696082750796, 0.2108696082750796, 0.17218218373984961, 0.17218218373984961, 0.17218218373984961, 0.2329385394943635, 0.2329385394943635, 0.2329385394943635, 0.10591689517605296, 0.10591689517605296, 0.10591689517605296, 0.1552473120636909, 0.1552473120636909, 0.1552473120636909, 0.12199788623767405, 0.12199788623767405, 0.12199788623767405, 0.4593083307916008, 0.4593083307916008, 0.4593083307916008, 0.4628141347100536, 0.4628141347100536, 0.4628141347100536, 0.4549634008249209, 0.4549634008249209, 0.4549634008249209, 0.14035267481144242, 0.14035267481144242, 0.14035267481144242, 0.2647056139184356, 0.2647056139184356, 0.2647056139184356, 0.13663480456036958, 0.13663480456036958, 0.13663480456036958, 0.20334182760801733, 0.20334182760801733, 0.20334182760801733, 0.17681323754407496, 0.17681323754407496, 0.17681323754407496, 0.21871292997332525, 0.21871292997332525, 0.21871292997332525, 0.32825394007153186, 0.32825394007153186, 0.32825394007153186, 0.3326823931838664, 0.3326823931838664, 0.3326823931838664, 0.3421300043168062, 0.3421300043168062, 0.3421300043168062, 0.2724755833403435, 0.2724755833403435, 0.2724755833403435, 0.20678815163767184, 0.20678815163767184, 0.20678815163767184, 0.27220142835892625, 0.27220142835892625, 0.27220142835892625, 0.22900362693501475, 0.22900362693501475, 0.22900362693501475, 0.23990890494473383, 0.23990890494473383, 0.23990890494473383, 0.23540240144705793, 0.23540240144705793, 0.23540240144705793, 0.19425768105254093, 0.19425768105254093, 0.19425768105254093, 0.18912798567843125, 0.18912798567843125, 0.18912798567843125, 0.19106985300632762, 0.19106985300632762, 0.19106985300632762, 0.7025277924101051, 0.7025277924101051, 0.7025277924101051, 0.15133519477843094, 0.15133519477843094, 0.15133519477843094, 0.6240958961911556, 0.6240958961911556, 0.6240958961911556, 0.16573198475929618, 0.16573198475929618, 0.16573198475929618, 0.19732013670547166, 0.19732013670547166, 0.19732013670547166, 0.20412171108102173, 0.20412171108102173, 0.20412171108102173, 0.20894043819099617, 0.20894043819099617, 0.20894043819099617, 0.1933116987478113, 0.1933116987478113, 0.1933116987478113, 0.1933601729649982, 0.1933601729649982, 0.1933601729649982, 0.09564479257847158, 0.09564479257847158, 0.09564479257847158, 0.09537324264215674, 0.09537324264215674, 0.09537324264215674, 0.09878679773303067, 0.09878679773303067, 0.09878679773303067]}, "mutation_prompt": null}
{"id": "60f574e8-c9a3-4555-a635-909ddedf2c1b", "solution": "import numpy as np\n\nclass HybridDESA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.pop_size = 10 * dim\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.temp = 1000.0  # Initial temperature for simulated annealing\n        self.cooling_rate = 0.99  # Cooling rate for simulated annealing\n\n    def __call__(self, func):\n        np.random.seed(42)\n        population = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        num_evaluations = self.pop_size\n\n        def mutate(target_idx):\n            indices = [idx for idx in range(self.pop_size) if idx != target_idx]\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            dynamic_F = self.F * (1.0 - (num_evaluations / self.budget)) * np.exp(-num_evaluations / self.budget)\n            dynamic_F *= (self.temp / 1000.0) * (1 + 0.1 * (num_evaluations / self.budget))  # Modified line: Adaptive scaling to mutation factor\n            mutant = population[a] + dynamic_F * (population[b] - population[c])\n            mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n            return mutant\n\n        while num_evaluations < self.budget:\n            self.CR = 0.9 - (0.3 * (num_evaluations / self.budget)) + 0.1 * np.tanh(num_evaluations / self.budget)  # Modified line: Introduce learning rate update to crossover probability\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                mutant = mutate(i)\n                trial = np.where(np.random.rand(self.dim) < self.CR, mutant, 0.5 * (population[i] + best_solution))\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = trial\n\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                neighbor = population[i] + np.random.normal(0, 0.1 * self.temp / 1000, self.dim)\n                neighbor = np.clip(neighbor, self.lower_bound, self.upper_bound)\n                neighbor_fitness = func(neighbor)\n                num_evaluations += 1\n                delta_fit = neighbor_fitness - fitness[i]\n\n                if delta_fit < 0 or np.random.rand() < np.exp(-delta_fit / self.temp):\n                    population[i] = neighbor\n                    fitness[i] = neighbor_fitness\n                    if neighbor_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = neighbor\n\n            self.temp *= (self.cooling_rate - 0.01 * (num_evaluations / self.budget))\n            self.pop_size = max(4, int(8 * self.dim * (1 - num_evaluations / self.budget)))\n\n        return best_solution", "name": "HybridDESA", "description": "Introduce learning rate based update to the crossover probability to enhance exploration capabilities.", "configspace": "", "generation": 63, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "ba6bec03-35bc-47f6-92da-256729321735", "metadata": {"aucs": [0.5542661297327882, 0.5542661297327882, 0.5542661297327882, 0.5502131781010412, 0.5502131781010412, 0.5502131781010412, 0.5542576829571606, 0.5542576829571606, 0.5542576829571606, 0.3214986489793178, 0.3214986489793178, 0.3214986489793178, 0.3065416684084221, 0.3065416684084221, 0.3065416684084221, 0.3158065919396942, 0.3158065919396942, 0.3158065919396942, 0.13838181317138243, 0.13838181317138243, 0.13838181317138243, 0.12545327945625218, 0.12545327945625218, 0.12545327945625218, 0.12341091429570572, 0.12341091429570572, 0.12341091429570572, 0.10743269946665945, 0.10743269946665945, 0.10743269946665945, 0.1047695610119097, 0.1047695610119097, 0.1047695610119097, 0.11944414538050097, 0.11944414538050097, 0.11944414538050097, 0.8779320540888479, 0.8779320540888479, 0.8779320540888479, 0.877898188844995, 0.877898188844995, 0.877898188844995, 0.780456822675078, 0.780456822675078, 0.780456822675078, 0.3563309678652393, 0.3563309678652393, 0.3563309678652393, 0.3648861206411198, 0.3648861206411198, 0.3648861206411198, 0.3566431522159045, 0.3566431522159045, 0.3566431522159045, 0.6706862034126804, 0.6706862034126804, 0.6706862034126804, 0.3423519789978463, 0.3423519789978463, 0.3423519789978463, 0.72668247747501, 0.72668247747501, 0.72668247747501, 0.16919472358197873, 0.16919472358197873, 0.16919472358197873, 0.1651292859417527, 0.1651292859417527, 0.1651292859417527, 0.14989293282023441, 0.14989293282023441, 0.14989293282023441, 0.14250677278277657, 0.14250677278277657, 0.14250677278277657, 0.1589362233709246, 0.1589362233709246, 0.1589362233709246, 0.11857286877661266, 0.11857286877661266, 0.11857286877661266, 0.07046504581570368, 0.07046504581570368, 0.07046504581570368, 0.09291593469393078, 0.09291593469393078, 0.09291593469393078, 0.10976115636052908, 0.10976115636052908, 0.10976115636052908, 0.15193954057356018, 0.15193954057356018, 0.15193954057356018, 0.1246100015034457, 0.1246100015034457, 0.1246100015034457, 0.15848011734590806, 0.15848011734590806, 0.15848011734590806, 0.2108696082750796, 0.2108696082750796, 0.2108696082750796, 0.17218218373984961, 0.17218218373984961, 0.17218218373984961, 0.2329385394943635, 0.2329385394943635, 0.2329385394943635, 0.10591689517605296, 0.10591689517605296, 0.10591689517605296, 0.1552473120636909, 0.1552473120636909, 0.1552473120636909, 0.12199788623767405, 0.12199788623767405, 0.12199788623767405, 0.4593083307916008, 0.4593083307916008, 0.4593083307916008, 0.4628141347100536, 0.4628141347100536, 0.4628141347100536, 0.4549634008249209, 0.4549634008249209, 0.4549634008249209, 0.14035267481144242, 0.14035267481144242, 0.14035267481144242, 0.2647056139184356, 0.2647056139184356, 0.2647056139184356, 0.13663480456036958, 0.13663480456036958, 0.13663480456036958, 0.20334182760801733, 0.20334182760801733, 0.20334182760801733, 0.17681323754407496, 0.17681323754407496, 0.17681323754407496, 0.21871292997332525, 0.21871292997332525, 0.21871292997332525, 0.32825394007153186, 0.32825394007153186, 0.32825394007153186, 0.3326823931838664, 0.3326823931838664, 0.3326823931838664, 0.3421300043168062, 0.3421300043168062, 0.3421300043168062, 0.2724755833403435, 0.2724755833403435, 0.2724755833403435, 0.20678815163767184, 0.20678815163767184, 0.20678815163767184, 0.27220142835892625, 0.27220142835892625, 0.27220142835892625, 0.22900362693501475, 0.22900362693501475, 0.22900362693501475, 0.23990890494473383, 0.23990890494473383, 0.23990890494473383, 0.23540240144705793, 0.23540240144705793, 0.23540240144705793, 0.19425768105254093, 0.19425768105254093, 0.19425768105254093, 0.18912798567843125, 0.18912798567843125, 0.18912798567843125, 0.19106985300632762, 0.19106985300632762, 0.19106985300632762, 0.7025277924101051, 0.7025277924101051, 0.7025277924101051, 0.15133519477843094, 0.15133519477843094, 0.15133519477843094, 0.6240958961911556, 0.6240958961911556, 0.6240958961911556, 0.16573198475929618, 0.16573198475929618, 0.16573198475929618, 0.19732013670547166, 0.19732013670547166, 0.19732013670547166, 0.20412171108102173, 0.20412171108102173, 0.20412171108102173, 0.20894043819099617, 0.20894043819099617, 0.20894043819099617, 0.1933116987478113, 0.1933116987478113, 0.1933116987478113, 0.1933601729649982, 0.1933601729649982, 0.1933601729649982, 0.09564479257847158, 0.09564479257847158, 0.09564479257847158, 0.09537324264215674, 0.09537324264215674, 0.09537324264215674, 0.09878679773303067, 0.09878679773303067, 0.09878679773303067]}, "mutation_prompt": null}
{"id": "6703d2d2-3eca-462d-ad24-f9e4d5bb5721", "solution": "import numpy as np\n\nclass HybridDESA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.pop_size = 10 * dim\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.temp = 1000.0  # Initial temperature for simulated annealing\n        self.cooling_rate = 0.99  # Cooling rate for simulated annealing\n\n    def __call__(self, func):\n        np.random.seed(42)\n        population = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        num_evaluations = self.pop_size\n\n        def mutate(target_idx):\n            indices = [idx for idx in range(self.pop_size) if idx != target_idx]\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            dynamic_F = self.F * (1.0 - (num_evaluations / self.budget)) * np.exp(-num_evaluations / self.budget)\n            dynamic_F *= (self.temp / 1000.0) * (1 + 0.1 * (num_evaluations / self.budget))  # Modified line: Adaptive scaling to mutation factor\n            mutant = population[a] + dynamic_F * (population[b] - population[c])\n            mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n            return mutant\n\n        while num_evaluations < self.budget:\n            self.CR = 0.9 - (0.3 * (num_evaluations / self.budget)) + 0.1 * np.tanh(num_evaluations / self.budget)  # Modified line: Introduce learning rate update to crossover probability\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                mutant = mutate(i)\n                trial = np.where(np.random.rand(self.dim) < self.CR, mutant, 0.5 * (population[i] + best_solution))\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = trial\n\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                neighbor = population[i] + np.random.normal(0, 0.1 * self.temp / 1000, self.dim)\n                neighbor = np.clip(neighbor, self.lower_bound, self.upper_bound)\n                neighbor_fitness = func(neighbor)\n                num_evaluations += 1\n                delta_fit = neighbor_fitness - fitness[i]\n\n                if delta_fit < 0 or np.random.rand() < np.exp(-delta_fit / self.temp):\n                    population[i] = neighbor\n                    fitness[i] = neighbor_fitness\n                    if neighbor_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = neighbor\n\n            self.temp *= (self.cooling_rate - 0.01 * (num_evaluations / self.budget))\n            self.pop_size = max(4, int(8 * self.dim * (1 - num_evaluations / self.budget)))\n\n        return best_solution", "name": "HybridDESA", "description": "Introduce learning rate based update to the crossover probability to enhance exploration capabilities.", "configspace": "", "generation": 63, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "ba6bec03-35bc-47f6-92da-256729321735", "metadata": {"aucs": [0.5542661297327882, 0.5542661297327882, 0.5542661297327882, 0.5502131781010412, 0.5502131781010412, 0.5502131781010412, 0.5542576829571606, 0.5542576829571606, 0.5542576829571606, 0.3214986489793178, 0.3214986489793178, 0.3214986489793178, 0.3065416684084221, 0.3065416684084221, 0.3065416684084221, 0.3158065919396942, 0.3158065919396942, 0.3158065919396942, 0.13838181317138243, 0.13838181317138243, 0.13838181317138243, 0.12545327945625218, 0.12545327945625218, 0.12545327945625218, 0.12341091429570572, 0.12341091429570572, 0.12341091429570572, 0.10743269946665945, 0.10743269946665945, 0.10743269946665945, 0.1047695610119097, 0.1047695610119097, 0.1047695610119097, 0.11944414538050097, 0.11944414538050097, 0.11944414538050097, 0.8779320540888479, 0.8779320540888479, 0.8779320540888479, 0.877898188844995, 0.877898188844995, 0.877898188844995, 0.780456822675078, 0.780456822675078, 0.780456822675078, 0.3563309678652393, 0.3563309678652393, 0.3563309678652393, 0.3648861206411198, 0.3648861206411198, 0.3648861206411198, 0.3566431522159045, 0.3566431522159045, 0.3566431522159045, 0.6706862034126804, 0.6706862034126804, 0.6706862034126804, 0.3423519789978463, 0.3423519789978463, 0.3423519789978463, 0.72668247747501, 0.72668247747501, 0.72668247747501, 0.16919472358197873, 0.16919472358197873, 0.16919472358197873, 0.1651292859417527, 0.1651292859417527, 0.1651292859417527, 0.14989293282023441, 0.14989293282023441, 0.14989293282023441, 0.14250677278277657, 0.14250677278277657, 0.14250677278277657, 0.1589362233709246, 0.1589362233709246, 0.1589362233709246, 0.11857286877661266, 0.11857286877661266, 0.11857286877661266, 0.07046504581570368, 0.07046504581570368, 0.07046504581570368, 0.09291593469393078, 0.09291593469393078, 0.09291593469393078, 0.10976115636052908, 0.10976115636052908, 0.10976115636052908, 0.15193954057356018, 0.15193954057356018, 0.15193954057356018, 0.1246100015034457, 0.1246100015034457, 0.1246100015034457, 0.15848011734590806, 0.15848011734590806, 0.15848011734590806, 0.2108696082750796, 0.2108696082750796, 0.2108696082750796, 0.17218218373984961, 0.17218218373984961, 0.17218218373984961, 0.2329385394943635, 0.2329385394943635, 0.2329385394943635, 0.10591689517605296, 0.10591689517605296, 0.10591689517605296, 0.1552473120636909, 0.1552473120636909, 0.1552473120636909, 0.12199788623767405, 0.12199788623767405, 0.12199788623767405, 0.4593083307916008, 0.4593083307916008, 0.4593083307916008, 0.4628141347100536, 0.4628141347100536, 0.4628141347100536, 0.4549634008249209, 0.4549634008249209, 0.4549634008249209, 0.14035267481144242, 0.14035267481144242, 0.14035267481144242, 0.2647056139184356, 0.2647056139184356, 0.2647056139184356, 0.13663480456036958, 0.13663480456036958, 0.13663480456036958, 0.20334182760801733, 0.20334182760801733, 0.20334182760801733, 0.17681323754407496, 0.17681323754407496, 0.17681323754407496, 0.21871292997332525, 0.21871292997332525, 0.21871292997332525, 0.32825394007153186, 0.32825394007153186, 0.32825394007153186, 0.3326823931838664, 0.3326823931838664, 0.3326823931838664, 0.3421300043168062, 0.3421300043168062, 0.3421300043168062, 0.2724755833403435, 0.2724755833403435, 0.2724755833403435, 0.20678815163767184, 0.20678815163767184, 0.20678815163767184, 0.27220142835892625, 0.27220142835892625, 0.27220142835892625, 0.22900362693501475, 0.22900362693501475, 0.22900362693501475, 0.23990890494473383, 0.23990890494473383, 0.23990890494473383, 0.23540240144705793, 0.23540240144705793, 0.23540240144705793, 0.19425768105254093, 0.19425768105254093, 0.19425768105254093, 0.18912798567843125, 0.18912798567843125, 0.18912798567843125, 0.19106985300632762, 0.19106985300632762, 0.19106985300632762, 0.7025277924101051, 0.7025277924101051, 0.7025277924101051, 0.15133519477843094, 0.15133519477843094, 0.15133519477843094, 0.6240958961911556, 0.6240958961911556, 0.6240958961911556, 0.16573198475929618, 0.16573198475929618, 0.16573198475929618, 0.19732013670547166, 0.19732013670547166, 0.19732013670547166, 0.20412171108102173, 0.20412171108102173, 0.20412171108102173, 0.20894043819099617, 0.20894043819099617, 0.20894043819099617, 0.1933116987478113, 0.1933116987478113, 0.1933116987478113, 0.1933601729649982, 0.1933601729649982, 0.1933601729649982, 0.09564479257847158, 0.09564479257847158, 0.09564479257847158, 0.09537324264215674, 0.09537324264215674, 0.09537324264215674, 0.09878679773303067, 0.09878679773303067, 0.09878679773303067]}, "mutation_prompt": null}
{"id": "63bebae4-af0f-4057-83e3-aff2f7d60a75", "solution": "import numpy as np\n\nclass HybridDESA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.pop_size = 10 * dim\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.temp = 1000.0  # Initial temperature for simulated annealing\n        self.cooling_rate = 0.99  # Cooling rate for simulated annealing\n\n    def __call__(self, func):\n        np.random.seed(42)\n        population = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        num_evaluations = self.pop_size\n\n        def mutate(target_idx):\n            indices = [idx for idx in range(self.pop_size) if idx != target_idx]\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            dynamic_F = self.F * (1.0 - (num_evaluations / self.budget)) * np.exp(-num_evaluations / self.budget)\n            dynamic_F *= (self.temp / 1000.0) * (1 + 0.1 * (num_evaluations / self.budget))  # Modified line: Adaptive scaling to mutation factor\n            mutant = population[a] + dynamic_F * (population[b] - population[c])\n            mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n            return mutant\n\n        while num_evaluations < self.budget:\n            self.CR = 0.9 - (0.3 * (num_evaluations / self.budget)) + 0.1 * np.tanh(num_evaluations / self.budget)  # Modified line: Introduce learning rate update to crossover probability\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                mutant = mutate(i)\n                trial = np.where(np.random.rand(self.dim) < self.CR, mutant, 0.5 * (population[i] + best_solution))\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = trial\n\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                neighbor = population[i] + np.random.normal(0, 0.1 * self.temp / 1000, self.dim)\n                neighbor = np.clip(neighbor, self.lower_bound, self.upper_bound)\n                neighbor_fitness = func(neighbor)\n                num_evaluations += 1\n                delta_fit = neighbor_fitness - fitness[i]\n\n                if delta_fit < 0 or np.random.rand() < np.exp(-delta_fit / self.temp):\n                    population[i] = neighbor\n                    fitness[i] = neighbor_fitness\n                    if neighbor_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = neighbor\n\n            self.temp *= (self.cooling_rate - 0.01 * (num_evaluations / self.budget))\n            self.pop_size = max(4, int(8 * self.dim * (1 - num_evaluations / self.budget)))\n\n        return best_solution", "name": "HybridDESA", "description": "Introduce learning rate based update to the crossover probability to enhance exploration capabilities.", "configspace": "", "generation": 63, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "ba6bec03-35bc-47f6-92da-256729321735", "metadata": {"aucs": [0.5542661297327882, 0.5542661297327882, 0.5542661297327882, 0.5502131781010412, 0.5502131781010412, 0.5502131781010412, 0.5542576829571606, 0.5542576829571606, 0.5542576829571606, 0.3214986489793178, 0.3214986489793178, 0.3214986489793178, 0.3065416684084221, 0.3065416684084221, 0.3065416684084221, 0.3158065919396942, 0.3158065919396942, 0.3158065919396942, 0.13838181317138243, 0.13838181317138243, 0.13838181317138243, 0.12545327945625218, 0.12545327945625218, 0.12545327945625218, 0.12341091429570572, 0.12341091429570572, 0.12341091429570572, 0.10743269946665945, 0.10743269946665945, 0.10743269946665945, 0.1047695610119097, 0.1047695610119097, 0.1047695610119097, 0.11944414538050097, 0.11944414538050097, 0.11944414538050097, 0.8779320540888479, 0.8779320540888479, 0.8779320540888479, 0.877898188844995, 0.877898188844995, 0.877898188844995, 0.780456822675078, 0.780456822675078, 0.780456822675078, 0.3563309678652393, 0.3563309678652393, 0.3563309678652393, 0.3648861206411198, 0.3648861206411198, 0.3648861206411198, 0.3566431522159045, 0.3566431522159045, 0.3566431522159045, 0.6706862034126804, 0.6706862034126804, 0.6706862034126804, 0.3423519789978463, 0.3423519789978463, 0.3423519789978463, 0.72668247747501, 0.72668247747501, 0.72668247747501, 0.16919472358197873, 0.16919472358197873, 0.16919472358197873, 0.1651292859417527, 0.1651292859417527, 0.1651292859417527, 0.14989293282023441, 0.14989293282023441, 0.14989293282023441, 0.14250677278277657, 0.14250677278277657, 0.14250677278277657, 0.1589362233709246, 0.1589362233709246, 0.1589362233709246, 0.11857286877661266, 0.11857286877661266, 0.11857286877661266, 0.07046504581570368, 0.07046504581570368, 0.07046504581570368, 0.09291593469393078, 0.09291593469393078, 0.09291593469393078, 0.10976115636052908, 0.10976115636052908, 0.10976115636052908, 0.15193954057356018, 0.15193954057356018, 0.15193954057356018, 0.1246100015034457, 0.1246100015034457, 0.1246100015034457, 0.15848011734590806, 0.15848011734590806, 0.15848011734590806, 0.2108696082750796, 0.2108696082750796, 0.2108696082750796, 0.17218218373984961, 0.17218218373984961, 0.17218218373984961, 0.2329385394943635, 0.2329385394943635, 0.2329385394943635, 0.10591689517605296, 0.10591689517605296, 0.10591689517605296, 0.1552473120636909, 0.1552473120636909, 0.1552473120636909, 0.12199788623767405, 0.12199788623767405, 0.12199788623767405, 0.4593083307916008, 0.4593083307916008, 0.4593083307916008, 0.4628141347100536, 0.4628141347100536, 0.4628141347100536, 0.4549634008249209, 0.4549634008249209, 0.4549634008249209, 0.14035267481144242, 0.14035267481144242, 0.14035267481144242, 0.2647056139184356, 0.2647056139184356, 0.2647056139184356, 0.13663480456036958, 0.13663480456036958, 0.13663480456036958, 0.20334182760801733, 0.20334182760801733, 0.20334182760801733, 0.17681323754407496, 0.17681323754407496, 0.17681323754407496, 0.21871292997332525, 0.21871292997332525, 0.21871292997332525, 0.32825394007153186, 0.32825394007153186, 0.32825394007153186, 0.3326823931838664, 0.3326823931838664, 0.3326823931838664, 0.3421300043168062, 0.3421300043168062, 0.3421300043168062, 0.2724755833403435, 0.2724755833403435, 0.2724755833403435, 0.20678815163767184, 0.20678815163767184, 0.20678815163767184, 0.27220142835892625, 0.27220142835892625, 0.27220142835892625, 0.22900362693501475, 0.22900362693501475, 0.22900362693501475, 0.23990890494473383, 0.23990890494473383, 0.23990890494473383, 0.23540240144705793, 0.23540240144705793, 0.23540240144705793, 0.19425768105254093, 0.19425768105254093, 0.19425768105254093, 0.18912798567843125, 0.18912798567843125, 0.18912798567843125, 0.19106985300632762, 0.19106985300632762, 0.19106985300632762, 0.7025277924101051, 0.7025277924101051, 0.7025277924101051, 0.15133519477843094, 0.15133519477843094, 0.15133519477843094, 0.6240958961911556, 0.6240958961911556, 0.6240958961911556, 0.16573198475929618, 0.16573198475929618, 0.16573198475929618, 0.19732013670547166, 0.19732013670547166, 0.19732013670547166, 0.20412171108102173, 0.20412171108102173, 0.20412171108102173, 0.20894043819099617, 0.20894043819099617, 0.20894043819099617, 0.1933116987478113, 0.1933116987478113, 0.1933116987478113, 0.1933601729649982, 0.1933601729649982, 0.1933601729649982, 0.09564479257847158, 0.09564479257847158, 0.09564479257847158, 0.09537324264215674, 0.09537324264215674, 0.09537324264215674, 0.09878679773303067, 0.09878679773303067, 0.09878679773303067]}, "mutation_prompt": null}
{"id": "af538310-f61a-44b6-99ca-28cfa128ef74", "solution": "import numpy as np\n\nclass HybridDESA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.pop_size = 10 * dim\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.temp = 1000.0  # Initial temperature for simulated annealing\n        self.cooling_rate = 0.99  # Cooling rate for simulated annealing\n\n    def __call__(self, func):\n        np.random.seed(42)\n        population = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        num_evaluations = self.pop_size\n\n        def mutate(target_idx):\n            indices = [idx for idx in range(self.pop_size) if idx != target_idx]\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            target_fitness = fitness[target_idx]  # New line: Get fitness of target solution\n            dynamic_F = self.F * (1.0 - (num_evaluations / self.budget)) * np.exp(-num_evaluations / self.budget)\n            dynamic_F *= (self.temp / 1000.0) * (1 + 0.1 * (num_evaluations / self.budget))\n            dynamic_F *= (1 - target_fitness / max(fitness))  # Modified line: Adaptive scaling based on target fitness\n            mutant = population[a] + dynamic_F * (population[b] - population[c])\n            mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n            return mutant\n\n        while num_evaluations < self.budget:\n            self.CR = 0.9 - (0.3 * (num_evaluations / self.budget)) + 0.1 * np.tanh(num_evaluations / self.budget)\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                mutant = mutate(i)\n                trial = np.where(np.random.rand(self.dim) < self.CR, mutant, 0.5 * (population[i] + best_solution))\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = trial\n\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                neighbor = population[i] + np.random.normal(0, 0.1 * self.temp / 1000, self.dim)\n                neighbor = np.clip(neighbor, self.lower_bound, self.upper_bound)\n                neighbor_fitness = func(neighbor)\n                num_evaluations += 1\n                delta_fit = neighbor_fitness - fitness[i]\n\n                if delta_fit < 0 or np.random.rand() < np.exp(-delta_fit / self.temp):\n                    population[i] = neighbor\n                    fitness[i] = neighbor_fitness\n                    if neighbor_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = neighbor\n\n            self.temp *= (self.cooling_rate - 0.01 * (num_evaluations / self.budget))\n            self.pop_size = max(4, int(8 * self.dim * (1 - num_evaluations / self.budget)))\n\n        return best_solution", "name": "HybridDESA", "description": "Introduce adaptive mutation scaling based on target solution's fitness to improve exploitation.", "configspace": "", "generation": 79, "fitness": 0.2587220448049101, "feedback": "The algorithm HybridDESA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.26 with standard deviation 0.18.", "error": "", "parent_id": "ba6bec03-35bc-47f6-92da-256729321735", "metadata": {"aucs": [0.5782973249328076, 0.5782973249328076, 0.5782973249328076, 0.524860723453954, 0.524860723453954, 0.524860723453954, 0.544533730312426, 0.544533730312426, 0.544533730312426, 0.2619252867196413, 0.2619252867196413, 0.2619252867196413, 0.12614061410244326, 0.12614061410244326, 0.12614061410244326, 0.16054589218372406, 0.16054589218372406, 0.16054589218372406, 0.09865603612270768, 0.09865603612270768, 0.09865603612270768, 0.1412935124867305, 0.1412935124867305, 0.1412935124867305, 0.14538743530873366, 0.14538743530873366, 0.14538743530873366, 0.12366986665881707, 0.12366986665881707, 0.12366986665881707, 0.11759550604981284, 0.11759550604981284, 0.11759550604981284, 0.13035319979555138, 0.13035319979555138, 0.13035319979555138, 0.8646894508893163, 0.8646894508893163, 0.8646894508893163, 0.11734301644898604, 0.11734301644898604, 0.11734301644898604, 0.34086959018247953, 0.34086959018247953, 0.34086959018247953, 0.3548177915239187, 0.3548177915239187, 0.3548177915239187, 0.3682921591889625, 0.3682921591889625, 0.3682921591889625, 0.3567792343016253, 0.3567792343016253, 0.3567792343016253, 0.7218915771841741, 0.7218915771841741, 0.7218915771841741, 0.7067923562032445, 0.7067923562032445, 0.7067923562032445, 0.36335105968530756, 0.36335105968530756, 0.36335105968530756, 0.17407296749758283, 0.17407296749758283, 0.17407296749758283, 0.1671449506676792, 0.1671449506676792, 0.1671449506676792, 0.19890731504747083, 0.19890731504747083, 0.19890731504747083, 0.1587223134381367, 0.1587223134381367, 0.1587223134381367, 0.15639951122402485, 0.15639951122402485, 0.15639951122402485, 0.14984592797545704, 0.14984592797545704, 0.14984592797545704, 0.0877656529281019, 0.0877656529281019, 0.0877656529281019, 0.10248217179699604, 0.10248217179699604, 0.10248217179699604, 0.11693319432389193, 0.11693319432389193, 0.11693319432389193, 0.2094521885093683, 0.2094521885093683, 0.2094521885093683, 0.08240166904326995, 0.08240166904326995, 0.08240166904326995, 0.1339430343982373, 0.1339430343982373, 0.1339430343982373, 0.1310322380573825, 0.1310322380573825, 0.1310322380573825, 0.22702228380246903, 0.22702228380246903, 0.22702228380246903, 0.21561292947491362, 0.21561292947491362, 0.21561292947491362, 0.16214011205428103, 0.16214011205428103, 0.16214011205428103, 0.15581749936410272, 0.15581749936410272, 0.15581749936410272, 0.1541897005981222, 0.1541897005981222, 0.1541897005981222, 0.4311036972710237, 0.4311036972710237, 0.4311036972710237, 0.4481516297704634, 0.4481516297704634, 0.4481516297704634, 0.4397804697988297, 0.4397804697988297, 0.4397804697988297, 0.13504190815365613, 0.13504190815365613, 0.13504190815365613, 0.1423637549027943, 0.1423637549027943, 0.1423637549027943, 0.27008045726616425, 0.27008045726616425, 0.27008045726616425, 0.33173479630098623, 0.33173479630098623, 0.33173479630098623, 0.2008673859979767, 0.2008673859979767, 0.2008673859979767, 0.33103068127623037, 0.33103068127623037, 0.33103068127623037, 0.30964885223719607, 0.30964885223719607, 0.30964885223719607, 0.35413116132059497, 0.35413116132059497, 0.35413116132059497, 0.20478792282064529, 0.20478792282064529, 0.20478792282064529, 0.2585173636984046, 0.2585173636984046, 0.2585173636984046, 0.23293820345298855, 0.23293820345298855, 0.23293820345298855, 0.24833959603405242, 0.24833959603405242, 0.24833959603405242, 0.18083732419922083, 0.18083732419922083, 0.18083732419922083, 0.21918945266559997, 0.21918945266559997, 0.21918945266559997, 0.24387238316824977, 0.24387238316824977, 0.24387238316824977, 0.18806312625378852, 0.18806312625378852, 0.18806312625378852, 0.1881175265572722, 0.1881175265572722, 0.1881175265572722, 0.20941628766684794, 0.20941628766684794, 0.20941628766684794, 0.8009038842086308, 0.8009038842086308, 0.8009038842086308, 0.17571710840423638, 0.17571710840423638, 0.17571710840423638, 0.7878118943856979, 0.7878118943856979, 0.7878118943856979, 0.10907731732728021, 0.10907731732728021, 0.10907731732728021, 0.2065773552131347, 0.2065773552131347, 0.2065773552131347, 0.14944223599710837, 0.14944223599710837, 0.14944223599710837, 0.1982437909537228, 0.1982437909537228, 0.1982437909537228, 0.20632309166049834, 0.20632309166049834, 0.20632309166049834, 0.20502225131708096, 0.20502225131708096, 0.20502225131708096, 0.09531332637157464, 0.09531332637157464, 0.09531332637157464, 0.10082899407306245, 0.10082899407306245, 0.10082899407306245, 0.0927409712916637, 0.0927409712916637, 0.0927409712916637]}, "mutation_prompt": null}
{"id": "9316e821-5d36-489e-8292-153bfeb4e270", "solution": "import numpy as np\n\nclass HybridDESA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.pop_size = 10 * dim\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.temp = 1000.0  # Initial temperature for simulated annealing\n        self.cooling_rate = 0.99  # Cooling rate for simulated annealing\n\n    def __call__(self, func):\n        np.random.seed(42)\n        population = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        num_evaluations = self.pop_size\n\n        def mutate(target_idx):\n            indices = [idx for idx in range(self.pop_size) if idx != target_idx]\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            dynamic_F = self.F * (1.0 - (num_evaluations / self.budget)) * np.exp(-num_evaluations / self.budget)\n            dynamic_F *= (self.temp / 1000.0) * (1 + 0.1 * (num_evaluations / self.budget))  # Modified line: Adaptive scaling to mutation factor\n            mutant = population[a] + dynamic_F * (population[b] - population[c])\n            mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n            return mutant\n\n        while num_evaluations < self.budget:\n            self.CR = 0.9 - (0.3 * (num_evaluations / self.budget)) + 0.1 * np.tanh(num_evaluations / self.budget)  # Modified line: Introduce learning rate update to crossover probability\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                mutant = mutate(i)\n                trial = np.where(np.random.rand(self.dim) < self.CR, mutant, 0.5 * (population[i] + best_solution))\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = trial\n\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                neighbor = population[i] + np.random.normal(0, 0.1 * self.temp / 1000, self.dim)\n                neighbor = np.clip(neighbor, self.lower_bound, self.upper_bound)\n                neighbor_fitness = func(neighbor)\n                num_evaluations += 1\n                delta_fit = neighbor_fitness - fitness[i]\n\n                if delta_fit < 0 or np.random.rand() < np.exp(-delta_fit / self.temp):\n                    population[i] = neighbor\n                    fitness[i] = neighbor_fitness\n                    if neighbor_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = neighbor\n\n            self.temp *= (self.cooling_rate - 0.01 * (num_evaluations / self.budget))\n            self.pop_size = max(4, int(8 * self.dim * (1 - num_evaluations / self.budget)))\n\n        return best_solution", "name": "HybridDESA", "description": "Introduce learning rate based update to the crossover probability to enhance exploration capabilities.", "configspace": "", "generation": 63, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "ba6bec03-35bc-47f6-92da-256729321735", "metadata": {"aucs": [0.5542661297327882, 0.5542661297327882, 0.5542661297327882, 0.5502131781010412, 0.5502131781010412, 0.5502131781010412, 0.5542576829571606, 0.5542576829571606, 0.5542576829571606, 0.3214986489793178, 0.3214986489793178, 0.3214986489793178, 0.3065416684084221, 0.3065416684084221, 0.3065416684084221, 0.3158065919396942, 0.3158065919396942, 0.3158065919396942, 0.13838181317138243, 0.13838181317138243, 0.13838181317138243, 0.12545327945625218, 0.12545327945625218, 0.12545327945625218, 0.12341091429570572, 0.12341091429570572, 0.12341091429570572, 0.10743269946665945, 0.10743269946665945, 0.10743269946665945, 0.1047695610119097, 0.1047695610119097, 0.1047695610119097, 0.11944414538050097, 0.11944414538050097, 0.11944414538050097, 0.8779320540888479, 0.8779320540888479, 0.8779320540888479, 0.877898188844995, 0.877898188844995, 0.877898188844995, 0.780456822675078, 0.780456822675078, 0.780456822675078, 0.3563309678652393, 0.3563309678652393, 0.3563309678652393, 0.3648861206411198, 0.3648861206411198, 0.3648861206411198, 0.3566431522159045, 0.3566431522159045, 0.3566431522159045, 0.6706862034126804, 0.6706862034126804, 0.6706862034126804, 0.3423519789978463, 0.3423519789978463, 0.3423519789978463, 0.72668247747501, 0.72668247747501, 0.72668247747501, 0.16919472358197873, 0.16919472358197873, 0.16919472358197873, 0.1651292859417527, 0.1651292859417527, 0.1651292859417527, 0.14989293282023441, 0.14989293282023441, 0.14989293282023441, 0.14250677278277657, 0.14250677278277657, 0.14250677278277657, 0.1589362233709246, 0.1589362233709246, 0.1589362233709246, 0.11857286877661266, 0.11857286877661266, 0.11857286877661266, 0.07046504581570368, 0.07046504581570368, 0.07046504581570368, 0.09291593469393078, 0.09291593469393078, 0.09291593469393078, 0.10976115636052908, 0.10976115636052908, 0.10976115636052908, 0.15193954057356018, 0.15193954057356018, 0.15193954057356018, 0.1246100015034457, 0.1246100015034457, 0.1246100015034457, 0.15848011734590806, 0.15848011734590806, 0.15848011734590806, 0.2108696082750796, 0.2108696082750796, 0.2108696082750796, 0.17218218373984961, 0.17218218373984961, 0.17218218373984961, 0.2329385394943635, 0.2329385394943635, 0.2329385394943635, 0.10591689517605296, 0.10591689517605296, 0.10591689517605296, 0.1552473120636909, 0.1552473120636909, 0.1552473120636909, 0.12199788623767405, 0.12199788623767405, 0.12199788623767405, 0.4593083307916008, 0.4593083307916008, 0.4593083307916008, 0.4628141347100536, 0.4628141347100536, 0.4628141347100536, 0.4549634008249209, 0.4549634008249209, 0.4549634008249209, 0.14035267481144242, 0.14035267481144242, 0.14035267481144242, 0.2647056139184356, 0.2647056139184356, 0.2647056139184356, 0.13663480456036958, 0.13663480456036958, 0.13663480456036958, 0.20334182760801733, 0.20334182760801733, 0.20334182760801733, 0.17681323754407496, 0.17681323754407496, 0.17681323754407496, 0.21871292997332525, 0.21871292997332525, 0.21871292997332525, 0.32825394007153186, 0.32825394007153186, 0.32825394007153186, 0.3326823931838664, 0.3326823931838664, 0.3326823931838664, 0.3421300043168062, 0.3421300043168062, 0.3421300043168062, 0.2724755833403435, 0.2724755833403435, 0.2724755833403435, 0.20678815163767184, 0.20678815163767184, 0.20678815163767184, 0.27220142835892625, 0.27220142835892625, 0.27220142835892625, 0.22900362693501475, 0.22900362693501475, 0.22900362693501475, 0.23990890494473383, 0.23990890494473383, 0.23990890494473383, 0.23540240144705793, 0.23540240144705793, 0.23540240144705793, 0.19425768105254093, 0.19425768105254093, 0.19425768105254093, 0.18912798567843125, 0.18912798567843125, 0.18912798567843125, 0.19106985300632762, 0.19106985300632762, 0.19106985300632762, 0.7025277924101051, 0.7025277924101051, 0.7025277924101051, 0.15133519477843094, 0.15133519477843094, 0.15133519477843094, 0.6240958961911556, 0.6240958961911556, 0.6240958961911556, 0.16573198475929618, 0.16573198475929618, 0.16573198475929618, 0.19732013670547166, 0.19732013670547166, 0.19732013670547166, 0.20412171108102173, 0.20412171108102173, 0.20412171108102173, 0.20894043819099617, 0.20894043819099617, 0.20894043819099617, 0.1933116987478113, 0.1933116987478113, 0.1933116987478113, 0.1933601729649982, 0.1933601729649982, 0.1933601729649982, 0.09564479257847158, 0.09564479257847158, 0.09564479257847158, 0.09537324264215674, 0.09537324264215674, 0.09537324264215674, 0.09878679773303067, 0.09878679773303067, 0.09878679773303067]}, "mutation_prompt": null}
{"id": "e87edc72-00a6-4e19-9539-65566f6a31ec", "solution": "import numpy as np\n\nclass HybridDESA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.pop_size = 10 * dim\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.temp = 1000.0  # Initial temperature for simulated annealing\n        self.cooling_rate = 0.99  # Cooling rate for simulated annealing\n\n    def __call__(self, func):\n        np.random.seed(42)\n        population = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        num_evaluations = self.pop_size\n\n        def mutate(target_idx):\n            indices = [idx for idx in range(self.pop_size) if idx != target_idx]\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            dynamic_F = self.F * (1.0 - (num_evaluations / self.budget)) * np.exp(-num_evaluations / self.budget)\n            dynamic_F *= (self.temp / 1000.0) * (1 + 0.1 * (num_evaluations / self.budget))\n            mutant = population[a] + dynamic_F * (population[b] - population[c])\n            mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n            return mutant\n\n        while num_evaluations < self.budget:\n            self.CR = 0.9 - (0.3 * (num_evaluations / self.budget)) + 0.1 * np.sin(np.pi * num_evaluations / self.budget)  # Modified line: Introduce sinusoidal adaptation to crossover probability\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                mutant = mutate(i)\n                trial = np.where(np.random.rand(self.dim) < self.CR, mutant, 0.5 * (population[i] + best_solution))\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = trial\n\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                neighbor = population[i] + np.random.normal(0, 0.1 * self.temp / 1000, self.dim)\n                neighbor = np.clip(neighbor, self.lower_bound, self.upper_bound)\n                neighbor_fitness = func(neighbor)\n                num_evaluations += 1\n                delta_fit = neighbor_fitness - fitness[i]\n\n                if delta_fit < 0 or np.random.rand() < np.exp(-delta_fit / self.temp):\n                    population[i] = neighbor\n                    fitness[i] = neighbor_fitness\n                    if neighbor_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = neighbor\n\n            self.temp *= (self.cooling_rate - 0.01 * (num_evaluations / self.budget))\n            self.pop_size = max(4, int(8 * self.dim * (1 - num_evaluations / self.budget)))\n\n        return best_solution", "name": "HybridDESA", "description": "Introduce a sinusoidal adaptation to the crossover probability to enhance convergence diversity.", "configspace": "", "generation": 81, "fitness": 0.25121647531668745, "feedback": "The algorithm HybridDESA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.25 with standard deviation 0.17.", "error": "", "parent_id": "ba6bec03-35bc-47f6-92da-256729321735", "metadata": {"aucs": [0.5435723100117531, 0.5435723100117531, 0.5435723100117531, 0.5420391722178892, 0.5420391722178892, 0.5420391722178892, 0.5550770188044081, 0.5550770188044081, 0.5550770188044081, 0.26933883946908477, 0.26933883946908477, 0.26933883946908477, 0.24614594193642858, 0.24614594193642858, 0.24614594193642858, 0.26961755853977043, 0.26961755853977043, 0.26961755853977043, 0.13576429455692196, 0.13576429455692196, 0.13576429455692196, 0.13667789450152956, 0.13667789450152956, 0.13667789450152956, 0.15113374452678385, 0.15113374452678385, 0.15113374452678385, 0.09643131825022211, 0.09643131825022211, 0.09643131825022211, 0.1117939353240105, 0.1117939353240105, 0.1117939353240105, 0.09542615504792484, 0.09542615504792484, 0.09542615504792484, 0.7796083055972401, 0.7796083055972401, 0.7796083055972401, 0.8978409998863075, 0.8978409998863075, 0.8978409998863075, 0.7989677350140915, 0.7989677350140915, 0.7989677350140915, 0.35343188046201146, 0.35343188046201146, 0.35343188046201146, 0.35435742572816, 0.35435742572816, 0.35435742572816, 0.34184119242217426, 0.34184119242217426, 0.34184119242217426, 0.3424173197837206, 0.3424173197837206, 0.3424173197837206, 0.673098488356638, 0.673098488356638, 0.673098488356638, 0.25477301440988787, 0.25477301440988787, 0.25477301440988787, 0.15535885623180423, 0.15535885623180423, 0.15535885623180423, 0.16425843926490746, 0.16425843926490746, 0.16425843926490746, 0.18561533267278163, 0.18561533267278163, 0.18561533267278163, 0.14730765069568874, 0.14730765069568874, 0.14730765069568874, 0.16541017482162623, 0.16541017482162623, 0.16541017482162623, 0.15232209850470668, 0.15232209850470668, 0.15232209850470668, 0.15486044241131036, 0.15486044241131036, 0.15486044241131036, 0.10133594249742361, 0.10133594249742361, 0.10133594249742361, 0.15562212688137556, 0.15562212688137556, 0.15562212688137556, 0.1865106514129551, 0.1865106514129551, 0.1865106514129551, 0.1929141460002599, 0.1929141460002599, 0.1929141460002599, 0.10203614638428371, 0.10203614638428371, 0.10203614638428371, 0.10995041504145753, 0.10995041504145753, 0.10995041504145753, 0.07184809902885259, 0.07184809902885259, 0.07184809902885259, 0.11655820804527539, 0.11655820804527539, 0.11655820804527539, 0.15345174448048893, 0.15345174448048893, 0.15345174448048893, 0.13569934743064938, 0.13569934743064938, 0.13569934743064938, 0.10354625955317465, 0.10354625955317465, 0.10354625955317465, 0.42449379806552745, 0.42449379806552745, 0.42449379806552745, 0.4302879531586917, 0.4302879531586917, 0.4302879531586917, 0.4537112907340781, 0.4537112907340781, 0.4537112907340781, 0.123665481851709, 0.123665481851709, 0.123665481851709, 0.1510723941340375, 0.1510723941340375, 0.1510723941340375, 0.12460744516007294, 0.12460744516007294, 0.12460744516007294, 0.2743912393963578, 0.2743912393963578, 0.2743912393963578, 0.17866056609081593, 0.17866056609081593, 0.17866056609081593, 0.2552470951324445, 0.2552470951324445, 0.2552470951324445, 0.3155383333631866, 0.3155383333631866, 0.3155383333631866, 0.32398172481241605, 0.32398172481241605, 0.32398172481241605, 0.33774776755251545, 0.33774776755251545, 0.33774776755251545, 0.23337176363892398, 0.23337176363892398, 0.23337176363892398, 0.2508756476820825, 0.2508756476820825, 0.2508756476820825, 0.27653615099003914, 0.27653615099003914, 0.27653615099003914, 0.22768050064202117, 0.22768050064202117, 0.22768050064202117, 0.23478092340810575, 0.23478092340810575, 0.23478092340810575, 0.2228184350270379, 0.2228184350270379, 0.2228184350270379, 0.18766588838203146, 0.18766588838203146, 0.18766588838203146, 0.3693772451181906, 0.3693772451181906, 0.3693772451181906, 0.220623698512465, 0.220623698512465, 0.220623698512465, 0.16451576352154496, 0.16451576352154496, 0.16451576352154496, 0.15671775332704707, 0.15671775332704707, 0.15671775332704707, 0.1545239752498916, 0.1545239752498916, 0.1545239752498916, 0.20247895921301684, 0.20247895921301684, 0.20247895921301684, 0.19992742521223683, 0.19992742521223683, 0.19992742521223683, 0.20290020646285378, 0.20290020646285378, 0.20290020646285378, 0.19812051258710328, 0.19812051258710328, 0.19812051258710328, 0.19761699486013617, 0.19761699486013617, 0.19761699486013617, 0.2112242730354773, 0.2112242730354773, 0.2112242730354773, 0.09602184108997969, 0.09602184108997969, 0.09602184108997969, 0.08580200924989889, 0.08580200924989889, 0.08580200924989889, 0.09664053993358124, 0.09664053993358124, 0.09664053993358124]}, "mutation_prompt": null}
{"id": "4c0a1443-a501-45ea-9977-21cd2d36f77b", "solution": "import numpy as np\n\nclass HybridDESA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.pop_size = 10 * dim\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.temp = 1000.0  # Initial temperature for simulated annealing\n        self.cooling_rate = 0.99  # Cooling rate for simulated annealing\n\n    def __call__(self, func):\n        np.random.seed(42)\n        population = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        num_evaluations = self.pop_size\n\n        def mutate(target_idx):\n            indices = [idx for idx in range(self.pop_size) if idx != target_idx]\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            dynamic_F = self.F * (1.0 - (num_evaluations / self.budget)) * np.exp(-num_evaluations / self.budget)\n            dynamic_F *= (self.temp / 1000.0) * (1 + 0.1 * (num_evaluations / self.budget))\n            mutant = population[a] + dynamic_F * (population[b] - population[c])\n            mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n            return mutant\n\n        while num_evaluations < self.budget:\n            self.CR = 0.9 - (0.3 * (num_evaluations / self.budget)) + 0.1 * np.tanh(num_evaluations / self.budget)\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                mutant = mutate(i)\n                trial = np.where(np.random.rand(self.dim) < self.CR, mutant, 0.5 * (population[i] + best_solution))\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = trial\n\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                neighbor = population[i] + np.random.normal(0, 0.1 * self.temp / 1000, self.dim)\n                neighbor = np.clip(neighbor, self.lower_bound, self.upper_bound)\n                neighbor_fitness = func(neighbor)\n                num_evaluations += 1\n                delta_fit = neighbor_fitness - fitness[i]\n\n                if delta_fit < 0 or np.random.rand() < np.exp(-delta_fit / self.temp):\n                    population[i] = neighbor\n                    fitness[i] = neighbor_fitness\n                    if neighbor_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = neighbor\n\n            self.temp *= (self.cooling_rate - 0.01 * (num_evaluations / self.budget))\n            self.pop_size = max(4, int(8 * self.dim * (1 - num_evaluations / self.budget)))\n\n        return best_solution", "name": "HybridDESA", "description": "Introduce dynamic scaling to trial vector generation probability for improved balance between exploration and exploitation.", "configspace": "", "generation": 82, "fitness": 0.27456111215513973, "feedback": "The algorithm HybridDESA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.27 with standard deviation 0.19.", "error": "", "parent_id": "ba6bec03-35bc-47f6-92da-256729321735", "metadata": {"aucs": [0.5542661297327882, 0.5542661297327882, 0.5542661297327882, 0.5502131781010412, 0.5502131781010412, 0.5502131781010412, 0.5542576829571606, 0.5542576829571606, 0.5542576829571606, 0.3214986489793178, 0.3214986489793178, 0.3214986489793178, 0.3065416684084221, 0.3065416684084221, 0.3065416684084221, 0.3158065919396942, 0.3158065919396942, 0.3158065919396942, 0.13838181317138243, 0.13838181317138243, 0.13838181317138243, 0.12545327945625218, 0.12545327945625218, 0.12545327945625218, 0.12341091429570572, 0.12341091429570572, 0.12341091429570572, 0.10743269946665945, 0.10743269946665945, 0.10743269946665945, 0.1047695610119097, 0.1047695610119097, 0.1047695610119097, 0.11944414538050097, 0.11944414538050097, 0.11944414538050097, 0.8779320540888479, 0.8779320540888479, 0.8779320540888479, 0.877898188844995, 0.877898188844995, 0.877898188844995, 0.780456822675078, 0.780456822675078, 0.780456822675078, 0.3563309678652393, 0.3563309678652393, 0.3563309678652393, 0.3648861206411198, 0.3648861206411198, 0.3648861206411198, 0.3566431522159045, 0.3566431522159045, 0.3566431522159045, 0.6706862034126804, 0.6706862034126804, 0.6706862034126804, 0.3423519789978463, 0.3423519789978463, 0.3423519789978463, 0.72668247747501, 0.72668247747501, 0.72668247747501, 0.16919472358197873, 0.16919472358197873, 0.16919472358197873, 0.1651292859417527, 0.1651292859417527, 0.1651292859417527, 0.14989293282023441, 0.14989293282023441, 0.14989293282023441, 0.14250677278277657, 0.14250677278277657, 0.14250677278277657, 0.1589362233709246, 0.1589362233709246, 0.1589362233709246, 0.11857286877661266, 0.11857286877661266, 0.11857286877661266, 0.07046504581570368, 0.07046504581570368, 0.07046504581570368, 0.09291593469393078, 0.09291593469393078, 0.09291593469393078, 0.10976115636052908, 0.10976115636052908, 0.10976115636052908, 0.15193954057356018, 0.15193954057356018, 0.15193954057356018, 0.1246100015034457, 0.1246100015034457, 0.1246100015034457, 0.15848011734590806, 0.15848011734590806, 0.15848011734590806, 0.2108696082750796, 0.2108696082750796, 0.2108696082750796, 0.17218218373984961, 0.17218218373984961, 0.17218218373984961, 0.2329385394943635, 0.2329385394943635, 0.2329385394943635, 0.10591689517605296, 0.10591689517605296, 0.10591689517605296, 0.1552473120636909, 0.1552473120636909, 0.1552473120636909, 0.12199788623767405, 0.12199788623767405, 0.12199788623767405, 0.4593083307916008, 0.4593083307916008, 0.4593083307916008, 0.4628141347100536, 0.4628141347100536, 0.4628141347100536, 0.4549634008249209, 0.4549634008249209, 0.4549634008249209, 0.14035267481144242, 0.14035267481144242, 0.14035267481144242, 0.2647056139184356, 0.2647056139184356, 0.2647056139184356, 0.13663480456036958, 0.13663480456036958, 0.13663480456036958, 0.20334182760801733, 0.20334182760801733, 0.20334182760801733, 0.17681323754407496, 0.17681323754407496, 0.17681323754407496, 0.21871292997332525, 0.21871292997332525, 0.21871292997332525, 0.32825394007153186, 0.32825394007153186, 0.32825394007153186, 0.3326823931838664, 0.3326823931838664, 0.3326823931838664, 0.3421300043168062, 0.3421300043168062, 0.3421300043168062, 0.2724755833403435, 0.2724755833403435, 0.2724755833403435, 0.20678815163767184, 0.20678815163767184, 0.20678815163767184, 0.27220142835892625, 0.27220142835892625, 0.27220142835892625, 0.22900362693501475, 0.22900362693501475, 0.22900362693501475, 0.23990890494473383, 0.23990890494473383, 0.23990890494473383, 0.23540240144705793, 0.23540240144705793, 0.23540240144705793, 0.19425768105254093, 0.19425768105254093, 0.19425768105254093, 0.18912798567843125, 0.18912798567843125, 0.18912798567843125, 0.19106985300632762, 0.19106985300632762, 0.19106985300632762, 0.7025277924101051, 0.7025277924101051, 0.7025277924101051, 0.15133519477843094, 0.15133519477843094, 0.15133519477843094, 0.6240958961911556, 0.6240958961911556, 0.6240958961911556, 0.16573198475929618, 0.16573198475929618, 0.16573198475929618, 0.19732013670547166, 0.19732013670547166, 0.19732013670547166, 0.20412171108102173, 0.20412171108102173, 0.20412171108102173, 0.20894043819099617, 0.20894043819099617, 0.20894043819099617, 0.1933116987478113, 0.1933116987478113, 0.1933116987478113, 0.1933601729649982, 0.1933601729649982, 0.1933601729649982, 0.09564479257847158, 0.09564479257847158, 0.09564479257847158, 0.09537324264215674, 0.09537324264215674, 0.09537324264215674, 0.09878679773303067, 0.09878679773303067, 0.09878679773303067]}, "mutation_prompt": null}
{"id": "e73ec8dc-768a-4c4d-9180-e3e2abf0037d", "solution": "import numpy as np\n\nclass HybridDESA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.pop_size = 10 * dim\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.temp = 1000.0  # Initial temperature for simulated annealing\n        self.cooling_rate = 0.99  # Cooling rate for simulated annealing\n\n    def __call__(self, func):\n        np.random.seed(42)\n        population = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        num_evaluations = self.pop_size\n\n        def mutate(target_idx):\n            indices = [idx for idx in range(self.pop_size) if idx != target_idx]\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            dynamic_F = self.F * (1.0 - (num_evaluations / self.budget)) * np.exp(-num_evaluations / self.budget)\n            dynamic_F *= (self.temp / 1000.0) * (1 + 0.1 * (num_evaluations / self.budget))  # Modified line: Adaptive scaling to mutation factor\n            mutant = population[a] + dynamic_F * (population[b] - population[c])\n            mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n            return mutant\n\n        while num_evaluations < self.budget:\n            self.CR = 0.9 - (0.3 * (num_evaluations / self.budget)) + 0.1 * np.tanh(num_evaluations / self.budget)  # Modified line: Introduce learning rate update to crossover probability\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                mutant = mutate(i)\n                trial = np.where(np.random.rand(self.dim) < self.CR, mutant, 0.5 * (population[i] + best_solution))\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = trial\n\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                neighbor = population[i] + np.random.normal(0, 0.1 * self.temp / 1000, self.dim)\n                neighbor = np.clip(neighbor, self.lower_bound, self.upper_bound)\n                neighbor_fitness = func(neighbor)\n                num_evaluations += 1\n                delta_fit = neighbor_fitness - fitness[i]\n\n                if delta_fit < 0 or np.random.rand() < np.exp(-delta_fit / self.temp):\n                    population[i] = neighbor\n                    fitness[i] = neighbor_fitness\n                    if neighbor_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = neighbor\n\n            self.temp *= (self.cooling_rate - 0.01 * (num_evaluations / self.budget))\n            self.pop_size = max(4, int(8 * self.dim * (1 - num_evaluations / self.budget)))\n\n        return best_solution", "name": "HybridDESA", "description": "Introduce learning rate based update to the crossover probability to enhance exploration capabilities.", "configspace": "", "generation": 63, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "ba6bec03-35bc-47f6-92da-256729321735", "metadata": {"aucs": [0.5542661297327882, 0.5542661297327882, 0.5542661297327882, 0.5502131781010412, 0.5502131781010412, 0.5502131781010412, 0.5542576829571606, 0.5542576829571606, 0.5542576829571606, 0.3214986489793178, 0.3214986489793178, 0.3214986489793178, 0.3065416684084221, 0.3065416684084221, 0.3065416684084221, 0.3158065919396942, 0.3158065919396942, 0.3158065919396942, 0.13838181317138243, 0.13838181317138243, 0.13838181317138243, 0.12545327945625218, 0.12545327945625218, 0.12545327945625218, 0.12341091429570572, 0.12341091429570572, 0.12341091429570572, 0.10743269946665945, 0.10743269946665945, 0.10743269946665945, 0.1047695610119097, 0.1047695610119097, 0.1047695610119097, 0.11944414538050097, 0.11944414538050097, 0.11944414538050097, 0.8779320540888479, 0.8779320540888479, 0.8779320540888479, 0.877898188844995, 0.877898188844995, 0.877898188844995, 0.780456822675078, 0.780456822675078, 0.780456822675078, 0.3563309678652393, 0.3563309678652393, 0.3563309678652393, 0.3648861206411198, 0.3648861206411198, 0.3648861206411198, 0.3566431522159045, 0.3566431522159045, 0.3566431522159045, 0.6706862034126804, 0.6706862034126804, 0.6706862034126804, 0.3423519789978463, 0.3423519789978463, 0.3423519789978463, 0.72668247747501, 0.72668247747501, 0.72668247747501, 0.16919472358197873, 0.16919472358197873, 0.16919472358197873, 0.1651292859417527, 0.1651292859417527, 0.1651292859417527, 0.14989293282023441, 0.14989293282023441, 0.14989293282023441, 0.14250677278277657, 0.14250677278277657, 0.14250677278277657, 0.1589362233709246, 0.1589362233709246, 0.1589362233709246, 0.11857286877661266, 0.11857286877661266, 0.11857286877661266, 0.07046504581570368, 0.07046504581570368, 0.07046504581570368, 0.09291593469393078, 0.09291593469393078, 0.09291593469393078, 0.10976115636052908, 0.10976115636052908, 0.10976115636052908, 0.15193954057356018, 0.15193954057356018, 0.15193954057356018, 0.1246100015034457, 0.1246100015034457, 0.1246100015034457, 0.15848011734590806, 0.15848011734590806, 0.15848011734590806, 0.2108696082750796, 0.2108696082750796, 0.2108696082750796, 0.17218218373984961, 0.17218218373984961, 0.17218218373984961, 0.2329385394943635, 0.2329385394943635, 0.2329385394943635, 0.10591689517605296, 0.10591689517605296, 0.10591689517605296, 0.1552473120636909, 0.1552473120636909, 0.1552473120636909, 0.12199788623767405, 0.12199788623767405, 0.12199788623767405, 0.4593083307916008, 0.4593083307916008, 0.4593083307916008, 0.4628141347100536, 0.4628141347100536, 0.4628141347100536, 0.4549634008249209, 0.4549634008249209, 0.4549634008249209, 0.14035267481144242, 0.14035267481144242, 0.14035267481144242, 0.2647056139184356, 0.2647056139184356, 0.2647056139184356, 0.13663480456036958, 0.13663480456036958, 0.13663480456036958, 0.20334182760801733, 0.20334182760801733, 0.20334182760801733, 0.17681323754407496, 0.17681323754407496, 0.17681323754407496, 0.21871292997332525, 0.21871292997332525, 0.21871292997332525, 0.32825394007153186, 0.32825394007153186, 0.32825394007153186, 0.3326823931838664, 0.3326823931838664, 0.3326823931838664, 0.3421300043168062, 0.3421300043168062, 0.3421300043168062, 0.2724755833403435, 0.2724755833403435, 0.2724755833403435, 0.20678815163767184, 0.20678815163767184, 0.20678815163767184, 0.27220142835892625, 0.27220142835892625, 0.27220142835892625, 0.22900362693501475, 0.22900362693501475, 0.22900362693501475, 0.23990890494473383, 0.23990890494473383, 0.23990890494473383, 0.23540240144705793, 0.23540240144705793, 0.23540240144705793, 0.19425768105254093, 0.19425768105254093, 0.19425768105254093, 0.18912798567843125, 0.18912798567843125, 0.18912798567843125, 0.19106985300632762, 0.19106985300632762, 0.19106985300632762, 0.7025277924101051, 0.7025277924101051, 0.7025277924101051, 0.15133519477843094, 0.15133519477843094, 0.15133519477843094, 0.6240958961911556, 0.6240958961911556, 0.6240958961911556, 0.16573198475929618, 0.16573198475929618, 0.16573198475929618, 0.19732013670547166, 0.19732013670547166, 0.19732013670547166, 0.20412171108102173, 0.20412171108102173, 0.20412171108102173, 0.20894043819099617, 0.20894043819099617, 0.20894043819099617, 0.1933116987478113, 0.1933116987478113, 0.1933116987478113, 0.1933601729649982, 0.1933601729649982, 0.1933601729649982, 0.09564479257847158, 0.09564479257847158, 0.09564479257847158, 0.09537324264215674, 0.09537324264215674, 0.09537324264215674, 0.09878679773303067, 0.09878679773303067, 0.09878679773303067]}, "mutation_prompt": null}
{"id": "fa1395ef-0db0-4891-8a4a-058839f6884a", "solution": "import numpy as np\n\nclass HybridDESA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.pop_size = 10 * dim\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.temp = 1000.0  # Initial temperature for simulated annealing\n        self.cooling_rate = 0.99  # Cooling rate for simulated annealing\n\n    def __call__(self, func):\n        np.random.seed(42)\n        population = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        num_evaluations = self.pop_size\n\n        def mutate(target_idx):\n            indices = [idx for idx in range(self.pop_size) if idx != target_idx]\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            dynamic_F = self.F * (1.0 - (num_evaluations / self.budget)) * np.exp(-num_evaluations / self.budget)\n            dynamic_F *= (self.temp / 1000.0) * (1 + 0.1 * (num_evaluations / self.budget))  # Modified line: Adaptive scaling to mutation factor\n            mutant = population[a] + dynamic_F * (population[b] - population[c])\n            mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n            return mutant\n\n        while num_evaluations < self.budget:\n            self.CR = 0.9 - (0.3 * (num_evaluations / self.budget)) + 0.1 * np.tanh(num_evaluations / self.budget)  # Introduce learning rate update to crossover probability\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                mutant = mutate(i)\n                perturbation = np.random.normal(0, 0.1 * (1 - num_evaluations / self.budget), self.dim)  # Modified line: Introduce adaptive perturbation\n                trial = np.where(np.random.rand(self.dim) < self.CR, mutant + perturbation, 0.5 * (population[i] + best_solution))\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = trial\n\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                neighbor = population[i] + np.random.normal(0, 0.1 * self.temp / 1000, self.dim)\n                neighbor = np.clip(neighbor, self.lower_bound, self.upper_bound)\n                neighbor_fitness = func(neighbor)\n                num_evaluations += 1\n                delta_fit = neighbor_fitness - fitness[i]\n\n                if delta_fit < 0 or np.random.rand() < np.exp(-delta_fit / self.temp):\n                    population[i] = neighbor\n                    fitness[i] = neighbor_fitness\n                    if neighbor_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = neighbor\n\n            self.temp *= (self.cooling_rate - 0.01 * (num_evaluations / self.budget))\n            self.pop_size = max(4, int(8 * self.dim * (1 - num_evaluations / self.budget)))\n\n        return best_solution", "name": "HybridDESA", "description": "Enhance diversity by using adaptive perturbation in trial vector generation.", "configspace": "", "generation": 84, "fitness": 0.24428288169840875, "feedback": "The algorithm HybridDESA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.24 with standard deviation 0.19.", "error": "", "parent_id": "ba6bec03-35bc-47f6-92da-256729321735", "metadata": {"aucs": [0.5036503527743579, 0.5036503527743579, 0.5036503527743579, 0.48765421266153897, 0.48765421266153897, 0.48765421266153897, 0.4985580612179087, 0.4985580612179087, 0.4985580612179087, 0.1308605479534385, 0.1308605479534385, 0.1308605479534385, 0.15091363468524854, 0.15091363468524854, 0.15091363468524854, 0.08628502451396902, 0.08628502451396902, 0.08628502451396902, 0.13172128263783967, 0.13172128263783967, 0.13172128263783967, 0.12215135744849792, 0.12215135744849792, 0.12215135744849792, 0.1378367536014934, 0.1378367536014934, 0.1378367536014934, 0.10257681867903445, 0.10257681867903445, 0.10257681867903445, 0.11747439326790043, 0.11747439326790043, 0.11747439326790043, 0.11332043100672473, 0.11332043100672473, 0.11332043100672473, 0.8266221937417799, 0.8266221937417799, 0.8266221937417799, 0.8123234162786865, 0.8123234162786865, 0.8123234162786865, 0.9040843679801333, 0.9040843679801333, 0.9040843679801333, 0.3186085130032331, 0.3186085130032331, 0.3186085130032331, 0.3218365455569636, 0.3218365455569636, 0.3218365455569636, 0.29852210989002337, 0.29852210989002337, 0.29852210989002337, 0.6259049331171854, 0.6259049331171854, 0.6259049331171854, 0.2613265528660689, 0.2613265528660689, 0.2613265528660689, 0.2444760645255677, 0.2444760645255677, 0.2444760645255677, 0.16733610438370106, 0.16733610438370106, 0.16733610438370106, 0.1554586952566238, 0.1554586952566238, 0.1554586952566238, 0.15233141136850825, 0.15233141136850825, 0.15233141136850825, 0.16834438477816072, 0.16834438477816072, 0.16834438477816072, 0.15878425771799798, 0.15878425771799798, 0.15878425771799798, 0.17494201199159654, 0.17494201199159654, 0.17494201199159654, 0.07433957040424755, 0.07433957040424755, 0.07433957040424755, 0.034217367661747655, 0.034217367661747655, 0.034217367661747655, 0.035415862206870985, 0.035415862206870985, 0.035415862206870985, 0.0861766919203878, 0.0861766919203878, 0.0861766919203878, 0.12029846883428452, 0.12029846883428452, 0.12029846883428452, 0.10536952444712189, 0.10536952444712189, 0.10536952444712189, 0.04320156673539022, 0.04320156673539022, 0.04320156673539022, 0.041668341080761784, 0.041668341080761784, 0.041668341080761784, 0.05189109715050677, 0.05189109715050677, 0.05189109715050677, 0.13015313703994835, 0.13015313703994835, 0.13015313703994835, 0.14196400807107223, 0.14196400807107223, 0.14196400807107223, 0.08692046739282089, 0.08692046739282089, 0.08692046739282089, 0.4375089251644576, 0.4375089251644576, 0.4375089251644576, 0.42552166173935035, 0.42552166173935035, 0.42552166173935035, 0.4398162186164093, 0.4398162186164093, 0.4398162186164093, 0.22518746536323542, 0.22518746536323542, 0.22518746536323542, 0.12996347519698137, 0.12996347519698137, 0.12996347519698137, 0.1403792397192739, 0.1403792397192739, 0.1403792397192739, 0.25758135561755424, 0.25758135561755424, 0.25758135561755424, 0.1829149125174473, 0.1829149125174473, 0.1829149125174473, 0.2791955070091683, 0.2791955070091683, 0.2791955070091683, 0.32285830119571435, 0.32285830119571435, 0.32285830119571435, 0.31387997173257787, 0.31387997173257787, 0.31387997173257787, 0.32194515433289095, 0.32194515433289095, 0.32194515433289095, 0.2580971638778271, 0.2580971638778271, 0.2580971638778271, 0.21577067296598607, 0.21577067296598607, 0.21577067296598607, 0.2613526802189532, 0.2613526802189532, 0.2613526802189532, 0.22069690656077467, 0.22069690656077467, 0.22069690656077467, 0.20585980062323395, 0.20585980062323395, 0.20585980062323395, 0.22942878419352297, 0.22942878419352297, 0.22942878419352297, 0.1895460205259304, 0.1895460205259304, 0.1895460205259304, 0.21541644316344954, 0.21541644316344954, 0.21541644316344954, 0.19897533551408586, 0.19897533551408586, 0.19897533551408586, 0.7019050690571993, 0.7019050690571993, 0.7019050690571993, 0.18375569129727742, 0.18375569129727742, 0.18375569129727742, 0.6335990517297045, 0.6335990517297045, 0.6335990517297045, 0.20139737296749805, 0.20139737296749805, 0.20139737296749805, 0.19671256138612447, 0.19671256138612447, 0.19671256138612447, 0.20552318202551245, 0.20552318202551245, 0.20552318202551245, 0.18744072866527284, 0.18744072866527284, 0.18744072866527284, 0.20047456781984152, 0.20047456781984152, 0.20047456781984152, 0.19963762718762512, 0.19963762718762512, 0.19963762718762512, 0.09216476453831601, 0.09216476453831601, 0.09216476453831601, 0.08802612458753578, 0.08802612458753578, 0.08802612458753578, 0.10031420932535473, 0.10031420932535473, 0.10031420932535473]}, "mutation_prompt": null}
{"id": "e91a2ade-3299-466e-a6c9-2c5f4ce3ec77", "solution": "import numpy as np\n\nclass HybridDESA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.pop_size = 10 * dim\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.temp = 1000.0  # Initial temperature for simulated annealing\n        self.cooling_rate = 0.99  # Cooling rate for simulated annealing\n\n    def __call__(self, func):\n        np.random.seed(42)\n        population = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        num_evaluations = self.pop_size\n\n        def mutate(target_idx):\n            indices = [idx for idx in range(self.pop_size) if idx != target_idx]\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            dynamic_F = self.F * (1.0 - (num_evaluations / self.budget)) * np.exp(-num_evaluations / self.budget)\n            dynamic_F *= (self.temp / 1000.0) * (1 + 0.1 * (num_evaluations / self.budget))  # Adaptive scaling to mutation factor\n            dynamic_F *= np.random.rand()  # Modified line: Introduce random scaling to enhance mutation diversity\n            mutant = population[a] + dynamic_F * (population[b] - population[c])\n            mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n            return mutant\n\n        while num_evaluations < self.budget:\n            self.CR = 0.9 - (0.3 * (num_evaluations / self.budget)) + 0.1 * np.tanh(num_evaluations / self.budget)  # Introduce learning rate update to crossover probability\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                mutant = mutate(i)\n                trial = np.where(np.random.rand(self.dim) < self.CR, mutant, 0.5 * (population[i] + best_solution))\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = trial\n\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                neighbor = population[i] + np.random.normal(0, 0.1 * self.temp / 1000, self.dim)\n                neighbor = np.clip(neighbor, self.lower_bound, self.upper_bound)\n                neighbor_fitness = func(neighbor)\n                num_evaluations += 1\n                delta_fit = neighbor_fitness - fitness[i]\n\n                if delta_fit < 0 or np.random.rand() < np.exp(-delta_fit / self.temp):\n                    population[i] = neighbor\n                    fitness[i] = neighbor_fitness\n                    if neighbor_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = neighbor\n\n            self.temp *= (self.cooling_rate - 0.01 * (num_evaluations / self.budget))\n            self.pop_size = max(4, int(8 * self.dim * (1 - num_evaluations / self.budget)))\n\n        return best_solution", "name": "HybridDESA", "description": "Enhance mutation diversity by introducing random scaling to the differential weight.", "configspace": "", "generation": 85, "fitness": 0.25840999168110756, "feedback": "The algorithm HybridDESA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.26 with standard deviation 0.19.", "error": "", "parent_id": "ba6bec03-35bc-47f6-92da-256729321735", "metadata": {"aucs": [0.570064605666701, 0.570064605666701, 0.570064605666701, 0.5672224263573151, 0.5672224263573151, 0.5672224263573151, 0.552138973624003, 0.552138973624003, 0.552138973624003, 0.18494579754777873, 0.18494579754777873, 0.18494579754777873, 0.04708876099987569, 0.04708876099987569, 0.04708876099987569, 0.30975090502601754, 0.30975090502601754, 0.30975090502601754, 0.1655181658086622, 0.1655181658086622, 0.1655181658086622, 0.132777552395814, 0.132777552395814, 0.132777552395814, 0.16790609385761346, 0.16790609385761346, 0.16790609385761346, 0.10668453932218058, 0.10668453932218058, 0.10668453932218058, 0.08905546750163196, 0.08905546750163196, 0.08905546750163196, 0.10092002820721846, 0.10092002820721846, 0.10092002820721846, 0.8413512302730681, 0.8413512302730681, 0.8413512302730681, 0.9039132579165295, 0.9039132579165295, 0.9039132579165295, 0.7648066615480718, 0.7648066615480718, 0.7648066615480718, 0.3400456746645566, 0.3400456746645566, 0.3400456746645566, 0.3718591214364638, 0.3718591214364638, 0.3718591214364638, 0.35728308972631084, 0.35728308972631084, 0.35728308972631084, 0.22027020183765422, 0.22027020183765422, 0.22027020183765422, 0.6290979742893911, 0.6290979742893911, 0.6290979742893911, 0.23334309380738705, 0.23334309380738705, 0.23334309380738705, 0.1789600850235622, 0.1789600850235622, 0.1789600850235622, 0.12373725455890872, 0.12373725455890872, 0.12373725455890872, 0.15123191709935657, 0.15123191709935657, 0.15123191709935657, 0.14864505114427962, 0.14864505114427962, 0.14864505114427962, 0.19205542542165877, 0.19205542542165877, 0.19205542542165877, 0.1680764331035196, 0.1680764331035196, 0.1680764331035196, 0.1951111916767161, 0.1951111916767161, 0.1951111916767161, 0.2026349526721385, 0.2026349526721385, 0.2026349526721385, 0.03264762782920094, 0.03264762782920094, 0.03264762782920094, 0.14851759303070555, 0.14851759303070555, 0.14851759303070555, 0.05026151387885491, 0.05026151387885491, 0.05026151387885491, 0.10703233383336486, 0.10703233383336486, 0.10703233383336486, 0.07491459644159826, 0.07491459644159826, 0.07491459644159826, 0.14315006229586547, 0.14315006229586547, 0.14315006229586547, 0.11780716538157343, 0.11780716538157343, 0.11780716538157343, 0.1712262164607642, 0.1712262164607642, 0.1712262164607642, 0.13933232129524498, 0.13933232129524498, 0.13933232129524498, 0.13036323357558743, 0.13036323357558743, 0.13036323357558743, 0.45257875722711527, 0.45257875722711527, 0.45257875722711527, 0.45897878933109093, 0.45897878933109093, 0.45897878933109093, 0.4776227743295135, 0.4776227743295135, 0.4776227743295135, 0.16476069049012199, 0.16476069049012199, 0.16476069049012199, 0.1428131427536098, 0.1428131427536098, 0.1428131427536098, 0.12410086685842325, 0.12410086685842325, 0.12410086685842325, 0.3008009096766857, 0.3008009096766857, 0.3008009096766857, 0.18523276482047002, 0.18523276482047002, 0.18523276482047002, 0.3198187231752059, 0.3198187231752059, 0.3198187231752059, 0.3386530970352134, 0.3386530970352134, 0.3386530970352134, 0.33916775100136587, 0.33916775100136587, 0.33916775100136587, 0.33820820891874626, 0.33820820891874626, 0.33820820891874626, 0.2816743575321602, 0.2816743575321602, 0.2816743575321602, 0.2284626807339587, 0.2284626807339587, 0.2284626807339587, 0.28459951389931804, 0.28459951389931804, 0.28459951389931804, 0.2163791952508063, 0.2163791952508063, 0.2163791952508063, 0.22108350551171585, 0.22108350551171585, 0.22108350551171585, 0.2676442008385368, 0.2676442008385368, 0.2676442008385368, 0.3959672311828387, 0.3959672311828387, 0.3959672311828387, 0.19852075216853105, 0.19852075216853105, 0.19852075216853105, 0.1763670785078526, 0.1763670785078526, 0.1763670785078526, 0.7756882626642632, 0.7756882626642632, 0.7756882626642632, 0.1937548778534317, 0.1937548778534317, 0.1937548778534317, 0.15823235831855476, 0.15823235831855476, 0.15823235831855476, 0.16419359823098734, 0.16419359823098734, 0.16419359823098734, 0.20737274237034742, 0.20737274237034742, 0.20737274237034742, 0.15070130169942075, 0.15070130169942075, 0.15070130169942075, 0.2042179832319938, 0.2042179832319938, 0.2042179832319938, 0.1987347183614785, 0.1987347183614785, 0.1987347183614785, 0.19556928193573409, 0.19556928193573409, 0.19556928193573409, 0.10296014339319448, 0.10296014339319448, 0.10296014339319448, 0.11173571674097127, 0.11173571674097127, 0.11173571674097127, 0.09717482845893977, 0.09717482845893977, 0.09717482845893977]}, "mutation_prompt": null}
{"id": "784b319f-f3cc-4813-a0f9-efb79c57b0a5", "solution": "import numpy as np\n\nclass HybridDESA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.pop_size = 10 * dim\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.temp = 1000.0  # Initial temperature for simulated annealing\n        self.cooling_rate = 0.99  # Cooling rate for simulated annealing\n\n    def __call__(self, func):\n        np.random.seed(42)\n        population = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        num_evaluations = self.pop_size\n\n        def mutate(target_idx):\n            indices = [idx for idx in range(self.pop_size) if idx != target_idx]\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            dynamic_F = self.F * (1.0 - (num_evaluations / self.budget)) * np.exp(-num_evaluations / self.budget)\n            dynamic_F *= (self.temp / 1000.0) * (1 + 0.1 * (num_evaluations / self.budget))  # Modified line: Adaptive scaling to mutation factor\n            mutant = population[a] + dynamic_F * (population[b] - population[c])\n            mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n            return mutant\n\n        while num_evaluations < self.budget:\n            self.CR = 0.9 - (0.3 * (num_evaluations / self.budget)) + 0.1 * np.tanh(num_evaluations / self.budget) * (self.temp / 1000.0)  # Modified line: Adapt CR by temperature for better exploitation\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                mutant = mutate(i)\n                trial = np.where(np.random.rand(self.dim) < self.CR, mutant, 0.5 * (population[i] + best_solution))\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = trial\n\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                neighbor = population[i] + np.random.normal(0, 0.1 * self.temp / 1000, self.dim)\n                neighbor = np.clip(neighbor, self.lower_bound, self.upper_bound)\n                neighbor_fitness = func(neighbor)\n                num_evaluations += 1\n                delta_fit = neighbor_fitness - fitness[i]\n\n                if delta_fit < 0 or np.random.rand() < np.exp(-delta_fit / self.temp):\n                    population[i] = neighbor\n                    fitness[i] = neighbor_fitness\n                    if neighbor_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = neighbor\n\n            self.temp *= (self.cooling_rate - 0.01 * (num_evaluations / self.budget))\n            self.pop_size = max(4, int(8 * self.dim * (1 - num_evaluations / self.budget)))\n\n        return best_solution", "name": "HybridDESA", "description": "Enhance exploitation by adapting crossover probability based on the temperature for simulated annealing.", "configspace": "", "generation": 86, "fitness": 0.2628467929919302, "feedback": "The algorithm HybridDESA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.26 with standard deviation 0.19.", "error": "", "parent_id": "ba6bec03-35bc-47f6-92da-256729321735", "metadata": {"aucs": [0.5307073479063396, 0.5307073479063396, 0.5307073479063396, 0.5444872574380498, 0.5444872574380498, 0.5444872574380498, 0.5425680120644962, 0.5425680120644962, 0.5425680120644962, 0.24319124346198118, 0.24319124346198118, 0.24319124346198118, 0.23874328349922835, 0.23874328349922835, 0.23874328349922835, 0.31517635658527565, 0.31517635658527565, 0.31517635658527565, 0.1414137223430646, 0.1414137223430646, 0.1414137223430646, 0.13519663722628916, 0.13519663722628916, 0.13519663722628916, 0.1601524787747325, 0.1601524787747325, 0.1601524787747325, 0.09743587812511179, 0.09743587812511179, 0.09743587812511179, 0.11208463887795583, 0.11208463887795583, 0.11208463887795583, 0.10831640959145117, 0.10831640959145117, 0.10831640959145117, 0.8733566959561315, 0.8733566959561315, 0.8733566959561315, 0.877898188844995, 0.877898188844995, 0.877898188844995, 0.8187753679712755, 0.8187753679712755, 0.8187753679712755, 0.35545312733404266, 0.35545312733404266, 0.35545312733404266, 0.3512710808619438, 0.3512710808619438, 0.3512710808619438, 0.36256246581985885, 0.36256246581985885, 0.36256246581985885, 0.7180455512293773, 0.7180455512293773, 0.7180455512293773, 0.7581499314740805, 0.7581499314740805, 0.7581499314740805, 0.6850907931823962, 0.6850907931823962, 0.6850907931823962, 0.15973064095375356, 0.15973064095375356, 0.15973064095375356, 0.1799049810292056, 0.1799049810292056, 0.1799049810292056, 0.1716627364197446, 0.1716627364197446, 0.1716627364197446, 0.15100043576671907, 0.15100043576671907, 0.15100043576671907, 0.16974377403283014, 0.16974377403283014, 0.16974377403283014, 0.16135869040779927, 0.16135869040779927, 0.16135869040779927, 0.09722261049540282, 0.09722261049540282, 0.09722261049540282, 0.06314581701647781, 0.06314581701647781, 0.06314581701647781, 0.09209908560571511, 0.09209908560571511, 0.09209908560571511, 0.19881417902628296, 0.19881417902628296, 0.19881417902628296, 0.12652848438583564, 0.12652848438583564, 0.12652848438583564, 0.13940586718066206, 0.13940586718066206, 0.13940586718066206, 0.14731459312531403, 0.14731459312531403, 0.14731459312531403, 0.07760265728567717, 0.07760265728567717, 0.07760265728567717, 0.1141267633127061, 0.1141267633127061, 0.1141267633127061, 0.12724523135121457, 0.12724523135121457, 0.12724523135121457, 0.17561681767019166, 0.17561681767019166, 0.17561681767019166, 0.15795225275379743, 0.15795225275379743, 0.15795225275379743, 0.4448345333460241, 0.4448345333460241, 0.4448345333460241, 0.45407365474252415, 0.45407365474252415, 0.45407365474252415, 0.4388936559841601, 0.4388936559841601, 0.4388936559841601, 0.12510009110767806, 0.12510009110767806, 0.12510009110767806, 0.14861677659128458, 0.14861677659128458, 0.14861677659128458, 0.1565548893256684, 0.1565548893256684, 0.1565548893256684, 0.326253307545776, 0.326253307545776, 0.326253307545776, 0.17836371538063267, 0.17836371538063267, 0.17836371538063267, 0.19933576748546078, 0.19933576748546078, 0.19933576748546078, 0.32679486701944105, 0.32679486701944105, 0.32679486701944105, 0.33204051332894957, 0.33204051332894957, 0.33204051332894957, 0.3323848251524141, 0.3323848251524141, 0.3323848251524141, 0.27746577261952066, 0.27746577261952066, 0.27746577261952066, 0.27499032478791896, 0.27499032478791896, 0.27499032478791896, 0.27778781613104, 0.27778781613104, 0.27778781613104, 0.2145438142498357, 0.2145438142498357, 0.2145438142498357, 0.22998394029735936, 0.22998394029735936, 0.22998394029735936, 0.21060768429124987, 0.21060768429124987, 0.21060768429124987, 0.17708330491918567, 0.17708330491918567, 0.17708330491918567, 0.18164507291367327, 0.18164507291367327, 0.18164507291367327, 0.20921698156996504, 0.20921698156996504, 0.20921698156996504, 0.1626565393713486, 0.1626565393713486, 0.1626565393713486, 0.1509100477816694, 0.1509100477816694, 0.1509100477816694, 0.15240064524290575, 0.15240064524290575, 0.15240064524290575, 0.16572963520019535, 0.16572963520019535, 0.16572963520019535, 0.19728701858023168, 0.19728701858023168, 0.19728701858023168, 0.20392360870423898, 0.20392360870423898, 0.20392360870423898, 0.19877678122464681, 0.19877678122464681, 0.19877678122464681, 0.2078559440613046, 0.2078559440613046, 0.2078559440613046, 0.20008299837202426, 0.20008299837202426, 0.20008299837202426, 0.09879579644789105, 0.09879579644789105, 0.09879579644789105, 0.09785473955779833, 0.09785473955779833, 0.09785473955779833, 0.09157194569754956, 0.09157194569754956, 0.09157194569754956]}, "mutation_prompt": null}
{"id": "7a24056f-da3c-4955-9741-967746644287", "solution": "import numpy as np\n\nclass HybridDESA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.pop_size = 10 * dim\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.temp = 1000.0  # Initial temperature for simulated annealing\n        self.cooling_rate = 0.99  # Cooling rate for simulated annealing\n\n    def __call__(self, func):\n        np.random.seed(42)\n        population = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        num_evaluations = self.pop_size\n\n        def mutate(target_idx):\n            indices = [idx for idx in range(self.pop_size) if idx != target_idx]\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            dynamic_F = self.F * (1.0 - (num_evaluations / self.budget)) * np.exp(-num_evaluations / self.budget)\n            dynamic_F *= (self.temp / 1000.0) * (1 + 0.1 * (num_evaluations / self.budget))\n            mutant = population[a] + dynamic_F * (population[b] - population[c])\n            mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n            return mutant\n\n        while num_evaluations < self.budget:\n            self.CR = 0.9 - (0.3 * (num_evaluations / self.budget)) + 0.1 * np.tanh(num_evaluations / self.budget)\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                mutant = mutate(i)\n                trial = np.where(np.random.rand(self.dim) < self.CR, mutant, 0.5 * (population[i] + best_solution))\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = trial\n\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                neighbor = population[i] + np.random.normal(0, 0.1 * self.temp / 1000, self.dim)\n                neighbor = np.clip(neighbor, self.lower_bound, self.upper_bound)\n                neighbor_fitness = func(neighbor)\n                num_evaluations += 1\n                delta_fit = neighbor_fitness - fitness[i]\n\n                if delta_fit < 0 or np.random.rand() < np.exp(-delta_fit / self.temp):\n                    population[i] = neighbor\n                    fitness[i] = neighbor_fitness\n                    if neighbor_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = neighbor\n\n            self.temp *= (self.cooling_rate - 0.01 * (num_evaluations / self.budget))\n            self.pop_size = max(4, int(8 * self.dim * (1 - num_evaluations / self.budget)))\n\n        return best_solution", "name": "HybridDESA", "description": "Integrate adaptive mutation probability scaling to enhance exploration dynamics.", "configspace": "", "generation": 87, "fitness": 0.27456111215513973, "feedback": "The algorithm HybridDESA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.27 with standard deviation 0.19.", "error": "", "parent_id": "ba6bec03-35bc-47f6-92da-256729321735", "metadata": {"aucs": [0.5542661297327882, 0.5542661297327882, 0.5542661297327882, 0.5502131781010412, 0.5502131781010412, 0.5502131781010412, 0.5542576829571606, 0.5542576829571606, 0.5542576829571606, 0.3214986489793178, 0.3214986489793178, 0.3214986489793178, 0.3065416684084221, 0.3065416684084221, 0.3065416684084221, 0.3158065919396942, 0.3158065919396942, 0.3158065919396942, 0.13838181317138243, 0.13838181317138243, 0.13838181317138243, 0.12545327945625218, 0.12545327945625218, 0.12545327945625218, 0.12341091429570572, 0.12341091429570572, 0.12341091429570572, 0.10743269946665945, 0.10743269946665945, 0.10743269946665945, 0.1047695610119097, 0.1047695610119097, 0.1047695610119097, 0.11944414538050097, 0.11944414538050097, 0.11944414538050097, 0.8779320540888479, 0.8779320540888479, 0.8779320540888479, 0.877898188844995, 0.877898188844995, 0.877898188844995, 0.780456822675078, 0.780456822675078, 0.780456822675078, 0.3563309678652393, 0.3563309678652393, 0.3563309678652393, 0.3648861206411198, 0.3648861206411198, 0.3648861206411198, 0.3566431522159045, 0.3566431522159045, 0.3566431522159045, 0.6706862034126804, 0.6706862034126804, 0.6706862034126804, 0.3423519789978463, 0.3423519789978463, 0.3423519789978463, 0.72668247747501, 0.72668247747501, 0.72668247747501, 0.16919472358197873, 0.16919472358197873, 0.16919472358197873, 0.1651292859417527, 0.1651292859417527, 0.1651292859417527, 0.14989293282023441, 0.14989293282023441, 0.14989293282023441, 0.14250677278277657, 0.14250677278277657, 0.14250677278277657, 0.1589362233709246, 0.1589362233709246, 0.1589362233709246, 0.11857286877661266, 0.11857286877661266, 0.11857286877661266, 0.07046504581570368, 0.07046504581570368, 0.07046504581570368, 0.09291593469393078, 0.09291593469393078, 0.09291593469393078, 0.10976115636052908, 0.10976115636052908, 0.10976115636052908, 0.15193954057356018, 0.15193954057356018, 0.15193954057356018, 0.1246100015034457, 0.1246100015034457, 0.1246100015034457, 0.15848011734590806, 0.15848011734590806, 0.15848011734590806, 0.2108696082750796, 0.2108696082750796, 0.2108696082750796, 0.17218218373984961, 0.17218218373984961, 0.17218218373984961, 0.2329385394943635, 0.2329385394943635, 0.2329385394943635, 0.10591689517605296, 0.10591689517605296, 0.10591689517605296, 0.1552473120636909, 0.1552473120636909, 0.1552473120636909, 0.12199788623767405, 0.12199788623767405, 0.12199788623767405, 0.4593083307916008, 0.4593083307916008, 0.4593083307916008, 0.4628141347100536, 0.4628141347100536, 0.4628141347100536, 0.4549634008249209, 0.4549634008249209, 0.4549634008249209, 0.14035267481144242, 0.14035267481144242, 0.14035267481144242, 0.2647056139184356, 0.2647056139184356, 0.2647056139184356, 0.13663480456036958, 0.13663480456036958, 0.13663480456036958, 0.20334182760801733, 0.20334182760801733, 0.20334182760801733, 0.17681323754407496, 0.17681323754407496, 0.17681323754407496, 0.21871292997332525, 0.21871292997332525, 0.21871292997332525, 0.32825394007153186, 0.32825394007153186, 0.32825394007153186, 0.3326823931838664, 0.3326823931838664, 0.3326823931838664, 0.3421300043168062, 0.3421300043168062, 0.3421300043168062, 0.2724755833403435, 0.2724755833403435, 0.2724755833403435, 0.20678815163767184, 0.20678815163767184, 0.20678815163767184, 0.27220142835892625, 0.27220142835892625, 0.27220142835892625, 0.22900362693501475, 0.22900362693501475, 0.22900362693501475, 0.23990890494473383, 0.23990890494473383, 0.23990890494473383, 0.23540240144705793, 0.23540240144705793, 0.23540240144705793, 0.19425768105254093, 0.19425768105254093, 0.19425768105254093, 0.18912798567843125, 0.18912798567843125, 0.18912798567843125, 0.19106985300632762, 0.19106985300632762, 0.19106985300632762, 0.7025277924101051, 0.7025277924101051, 0.7025277924101051, 0.15133519477843094, 0.15133519477843094, 0.15133519477843094, 0.6240958961911556, 0.6240958961911556, 0.6240958961911556, 0.16573198475929618, 0.16573198475929618, 0.16573198475929618, 0.19732013670547166, 0.19732013670547166, 0.19732013670547166, 0.20412171108102173, 0.20412171108102173, 0.20412171108102173, 0.20894043819099617, 0.20894043819099617, 0.20894043819099617, 0.1933116987478113, 0.1933116987478113, 0.1933116987478113, 0.1933601729649982, 0.1933601729649982, 0.1933601729649982, 0.09564479257847158, 0.09564479257847158, 0.09564479257847158, 0.09537324264215674, 0.09537324264215674, 0.09537324264215674, 0.09878679773303067, 0.09878679773303067, 0.09878679773303067]}, "mutation_prompt": null}
{"id": "25012977-ef28-4324-99ba-a42f57e66072", "solution": "import numpy as np\n\nclass HybridDESA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.pop_size = 10 * dim\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.temp = 1000.0  # Initial temperature for simulated annealing\n        self.cooling_rate = 0.99  # Cooling rate for simulated annealing\n\n    def __call__(self, func):\n        np.random.seed(42)\n        population = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        num_evaluations = self.pop_size\n\n        def mutate(target_idx):\n            indices = [idx for idx in range(self.pop_size) if idx != target_idx]\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            dynamic_F = self.F * (1.0 - (num_evaluations / self.budget)) * np.exp(-num_evaluations / self.budget)\n            # Modified line: Added sinusoidal component to mutation factor\n            dynamic_F *= (self.temp / 1000.0) * (1 + 0.1 * (num_evaluations / self.budget)) * (1 + 0.1 * np.sin(2 * np.pi * num_evaluations / self.budget))  \n            mutant = population[a] + dynamic_F * (population[b] - population[c])\n            mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n            return mutant\n\n        while num_evaluations < self.budget:\n            self.CR = 0.9 - (0.3 * (num_evaluations / self.budget)) + 0.1 * np.tanh(num_evaluations / self.budget)\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                mutant = mutate(i)\n                trial = np.where(np.random.rand(self.dim) < self.CR, mutant, 0.5 * (population[i] + best_solution))\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = trial\n\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                neighbor = population[i] + np.random.normal(0, 0.1 * self.temp / 1000, self.dim)\n                neighbor = np.clip(neighbor, self.lower_bound, self.upper_bound)\n                neighbor_fitness = func(neighbor)\n                num_evaluations += 1\n                delta_fit = neighbor_fitness - fitness[i]\n\n                if delta_fit < 0 or np.random.rand() < np.exp(-delta_fit / self.temp):\n                    population[i] = neighbor\n                    fitness[i] = neighbor_fitness\n                    if neighbor_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = neighbor\n\n            self.temp *= (self.cooling_rate - 0.01 * (num_evaluations / self.budget))\n            self.pop_size = max(4, int(8 * self.dim * (1 - num_evaluations / self.budget)))\n\n        return best_solution", "name": "HybridDESA", "description": "Enhance mutation diversity by introducing a sinusoidal component to the mutation factor.", "configspace": "", "generation": 88, "fitness": 0.2694631129217099, "feedback": "The algorithm HybridDESA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.27 with standard deviation 0.20.", "error": "", "parent_id": "ba6bec03-35bc-47f6-92da-256729321735", "metadata": {"aucs": [0.5484182613719948, 0.5484182613719948, 0.5484182613719948, 0.5421545695607146, 0.5421545695607146, 0.5421545695607146, 0.5476518224207647, 0.5476518224207647, 0.5476518224207647, 0.29462883514367044, 0.29462883514367044, 0.29462883514367044, 0.22099597644873914, 0.22099597644873914, 0.22099597644873914, 0.2842599090910858, 0.2842599090910858, 0.2842599090910858, 0.2701279192198526, 0.2701279192198526, 0.2701279192198526, 0.12789977465931568, 0.12789977465931568, 0.12789977465931568, 0.125344794135841, 0.125344794135841, 0.125344794135841, 0.12260409164046981, 0.12260409164046981, 0.12260409164046981, 0.11662775919347224, 0.11662775919347224, 0.11662775919347224, 0.10265722146878853, 0.10265722146878853, 0.10265722146878853, 0.909636971713379, 0.909636971713379, 0.909636971713379, 0.898524623923353, 0.898524623923353, 0.898524623923353, 0.7911511244447704, 0.7911511244447704, 0.7911511244447704, 0.3431966714940057, 0.3431966714940057, 0.3431966714940057, 0.3364875815119134, 0.3364875815119134, 0.3364875815119134, 0.34321113912870116, 0.34321113912870116, 0.34321113912870116, 0.6554181359301293, 0.6554181359301293, 0.6554181359301293, 0.7215095393675554, 0.7215095393675554, 0.7215095393675554, 0.686608804202893, 0.686608804202893, 0.686608804202893, 0.14861810666874098, 0.14861810666874098, 0.14861810666874098, 0.16812442797664284, 0.16812442797664284, 0.16812442797664284, 0.1553859015250919, 0.1553859015250919, 0.1553859015250919, 0.1568523797096898, 0.1568523797096898, 0.1568523797096898, 0.15602034475429571, 0.15602034475429571, 0.15602034475429571, 0.16305819521917853, 0.16305819521917853, 0.16305819521917853, 0.13682543846611128, 0.13682543846611128, 0.13682543846611128, 0.10075883027319088, 0.10075883027319088, 0.10075883027319088, 0.06387596376226234, 0.06387596376226234, 0.06387596376226234, 0.10807287558564327, 0.10807287558564327, 0.10807287558564327, 0.184195813652535, 0.184195813652535, 0.184195813652535, 0.13641065099785044, 0.13641065099785044, 0.13641065099785044, 0.05974564148933592, 0.05974564148933592, 0.05974564148933592, 0.07837892573029293, 0.07837892573029293, 0.07837892573029293, 0.20011799640016115, 0.20011799640016115, 0.20011799640016115, 0.14239982212185387, 0.14239982212185387, 0.14239982212185387, 0.14640959363051842, 0.14640959363051842, 0.14640959363051842, 0.1513430426600445, 0.1513430426600445, 0.1513430426600445, 0.4565259760845317, 0.4565259760845317, 0.4565259760845317, 0.45449258484954425, 0.45449258484954425, 0.45449258484954425, 0.4367876560548398, 0.4367876560548398, 0.4367876560548398, 0.13535316457579882, 0.13535316457579882, 0.13535316457579882, 0.15388723387250214, 0.15388723387250214, 0.15388723387250214, 0.26417267145155077, 0.26417267145155077, 0.26417267145155077, 0.23198542235338404, 0.23198542235338404, 0.23198542235338404, 0.21090988722370996, 0.21090988722370996, 0.21090988722370996, 0.22069543226644384, 0.22069543226644384, 0.22069543226644384, 0.33285559803773157, 0.33285559803773157, 0.33285559803773157, 0.3351588619288013, 0.3351588619288013, 0.3351588619288013, 0.32662646750816826, 0.32662646750816826, 0.32662646750816826, 0.27384861363344704, 0.27384861363344704, 0.27384861363344704, 0.24767108034289242, 0.24767108034289242, 0.24767108034289242, 0.26572437069152455, 0.26572437069152455, 0.26572437069152455, 0.23588484848433078, 0.23588484848433078, 0.23588484848433078, 0.21163547420140605, 0.21163547420140605, 0.21163547420140605, 0.2209276252026109, 0.2209276252026109, 0.2209276252026109, 0.20213386277554257, 0.20213386277554257, 0.20213386277554257, 0.1963377607815413, 0.1963377607815413, 0.1963377607815413, 0.19396246125523386, 0.19396246125523386, 0.19396246125523386, 0.16362339104308576, 0.16362339104308576, 0.16362339104308576, 0.16008073741796436, 0.16008073741796436, 0.16008073741796436, 0.15472031573856748, 0.15472031573856748, 0.15472031573856748, 0.1655196134492566, 0.1655196134492566, 0.1655196134492566, 0.194892543855947, 0.194892543855947, 0.194892543855947, 0.6208602194655508, 0.6208602194655508, 0.6208602194655508, 0.1931123453305832, 0.1931123453305832, 0.1931123453305832, 0.21169596691135484, 0.21169596691135484, 0.21169596691135484, 0.2024294847571605, 0.2024294847571605, 0.2024294847571605, 0.09695646208943054, 0.09695646208943054, 0.09695646208943054, 0.09113245965345029, 0.09113245965345029, 0.09113245965345029, 0.09303406038037554, 0.09303406038037554, 0.09303406038037554]}, "mutation_prompt": null}
{"id": "d4cdc998-91d1-4dd4-b33e-bb2cd34cd87f", "solution": "import numpy as np\n\nclass HybridDESA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.pop_size = 10 * dim\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.temp = 1000.0  # Initial temperature for simulated annealing\n        self.cooling_rate = 0.99  # Cooling rate for simulated annealing\n\n    def __call__(self, func):\n        np.random.seed(42)\n        population = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        num_evaluations = self.pop_size\n\n        def mutate(target_idx):\n            indices = [idx for idx in range(self.pop_size) if idx != target_idx]\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            dynamic_F = self.F * (1.0 - (num_evaluations / self.budget)) * np.exp(-num_evaluations / self.budget)\n            dynamic_F *= (self.temp / 1000.0) * (1 + 0.1 * (num_evaluations / self.budget))  # Modified line: Adaptive scaling to mutation factor\n            mutant = population[a] + dynamic_F * (population[b] - population[c])\n            mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n            return mutant\n\n        while num_evaluations < self.budget:\n            self.CR = 0.9 - (0.3 * (num_evaluations / self.budget)) + 0.1 * np.tanh(num_evaluations / self.budget)  # Modified line: Introduce learning rate update to crossover probability\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                mutant = mutate(i)\n                trial = np.where(np.random.rand(self.dim) < self.CR, mutant, 0.5 * (population[i] + (best_solution + mutant) / 2))  # Modified line: Improved trial vector creation\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = trial\n\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                neighbor = population[i] + np.random.normal(0, 0.1 * self.temp / 1000, self.dim)\n                neighbor = np.clip(neighbor, self.lower_bound, self.upper_bound)\n                neighbor_fitness = func(neighbor)\n                num_evaluations += 1\n                delta_fit = neighbor_fitness - fitness[i]\n\n                if delta_fit < 0 or np.random.rand() < np.exp(-delta_fit / self.temp):\n                    population[i] = neighbor\n                    fitness[i] = neighbor_fitness\n                    if neighbor_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = neighbor\n\n            self.temp *= (self.cooling_rate - 0.01 * (num_evaluations / self.budget))\n            self.pop_size = max(4, int(8 * self.dim * (1 - num_evaluations / self.budget)))\n\n        return best_solution", "name": "HybridDESA", "description": "Enhance local search capabilities by improving trial vector creation.", "configspace": "", "generation": 89, "fitness": 0.2621923796637408, "feedback": "The algorithm HybridDESA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.26 with standard deviation 0.17.", "error": "", "parent_id": "ba6bec03-35bc-47f6-92da-256729321735", "metadata": {"aucs": [0.5369459677456585, 0.5369459677456585, 0.5369459677456585, 0.5502486382442875, 0.5502486382442875, 0.5502486382442875, 0.5472403824999599, 0.5472403824999599, 0.5472403824999599, 0.2585383962206784, 0.2585383962206784, 0.2585383962206784, 0.12096691187927189, 0.12096691187927189, 0.12096691187927189, 0.2246799856389674, 0.2246799856389674, 0.2246799856389674, 0.12174335053087704, 0.12174335053087704, 0.12174335053087704, 0.13899384748483756, 0.13899384748483756, 0.13899384748483756, 0.266741030349789, 0.266741030349789, 0.266741030349789, 0.10023762838776018, 0.10023762838776018, 0.10023762838776018, 0.09623938063881265, 0.09623938063881265, 0.09623938063881265, 0.1039554369892397, 0.1039554369892397, 0.1039554369892397, 0.6422995163053229, 0.6422995163053229, 0.6422995163053229, 0.7959645745095396, 0.7959645745095396, 0.7959645745095396, 0.5068888632177286, 0.5068888632177286, 0.5068888632177286, 0.33803237683110543, 0.33803237683110543, 0.33803237683110543, 0.34424678823715715, 0.34424678823715715, 0.34424678823715715, 0.33464646013504185, 0.33464646013504185, 0.33464646013504185, 0.642161733697988, 0.642161733697988, 0.642161733697988, 0.7137652170192929, 0.7137652170192929, 0.7137652170192929, 0.6617226430389487, 0.6617226430389487, 0.6617226430389487, 0.14166851311495032, 0.14166851311495032, 0.14166851311495032, 0.1476317425752952, 0.1476317425752952, 0.1476317425752952, 0.16912716319689314, 0.16912716319689314, 0.16912716319689314, 0.15748388095729893, 0.15748388095729893, 0.15748388095729893, 0.15725535637390753, 0.15725535637390753, 0.15725535637390753, 0.14450452025189298, 0.14450452025189298, 0.14450452025189298, 0.17114403309245152, 0.17114403309245152, 0.17114403309245152, 0.1044710288659002, 0.1044710288659002, 0.1044710288659002, 0.07545074902886317, 0.07545074902886317, 0.07545074902886317, 0.15988967909123453, 0.15988967909123453, 0.15988967909123453, 0.1020790529487493, 0.1020790529487493, 0.1020790529487493, 0.17355087523063595, 0.17355087523063595, 0.17355087523063595, 0.15352268239740374, 0.15352268239740374, 0.15352268239740374, 0.14130440161853697, 0.14130440161853697, 0.14130440161853697, 0.17375438937710408, 0.17375438937710408, 0.17375438937710408, 0.14410306720676058, 0.14410306720676058, 0.14410306720676058, 0.15608866427298929, 0.15608866427298929, 0.15608866427298929, 0.14754659826050176, 0.14754659826050176, 0.14754659826050176, 0.45134681371261975, 0.45134681371261975, 0.45134681371261975, 0.4536511650618291, 0.4536511650618291, 0.4536511650618291, 0.44999784769659024, 0.44999784769659024, 0.44999784769659024, 0.2657981666218424, 0.2657981666218424, 0.2657981666218424, 0.26115400157937374, 0.26115400157937374, 0.26115400157937374, 0.1464369871124076, 0.1464369871124076, 0.1464369871124076, 0.26224105553508803, 0.26224105553508803, 0.26224105553508803, 0.20740038977137487, 0.20740038977137487, 0.20740038977137487, 0.19833045694429763, 0.19833045694429763, 0.19833045694429763, 0.3369889049256609, 0.3369889049256609, 0.3369889049256609, 0.33479428849028936, 0.33479428849028936, 0.33479428849028936, 0.32981396540174324, 0.32981396540174324, 0.32981396540174324, 0.27331271404768254, 0.27331271404768254, 0.27331271404768254, 0.2630543065874964, 0.2630543065874964, 0.2630543065874964, 0.2800434444921136, 0.2800434444921136, 0.2800434444921136, 0.24504517677613236, 0.24504517677613236, 0.24504517677613236, 0.21540201497137434, 0.21540201497137434, 0.21540201497137434, 0.2052132575983967, 0.2052132575983967, 0.2052132575983967, 0.17475302321424335, 0.17475302321424335, 0.17475302321424335, 0.17864121767584662, 0.17864121767584662, 0.17864121767584662, 0.18331970585205404, 0.18331970585205404, 0.18331970585205404, 0.7528210961183484, 0.7528210961183484, 0.7528210961183484, 0.1613408595265151, 0.1613408595265151, 0.1613408595265151, 0.15945227672809548, 0.15945227672809548, 0.15945227672809548, 0.16340781084092582, 0.16340781084092582, 0.16340781084092582, 0.19710460340877534, 0.19710460340877534, 0.19710460340877534, 0.14444947396361407, 0.14444947396361407, 0.14444947396361407, 0.20896403408517428, 0.20896403408517428, 0.20896403408517428, 0.20468560180826523, 0.20468560180826523, 0.20468560180826523, 0.21655776828124063, 0.21655776828124063, 0.21655776828124063, 0.08991851870638679, 0.08991851870638679, 0.08991851870638679, 0.09865634353794628, 0.09865634353794628, 0.09865634353794628, 0.09691851725196043, 0.09691851725196043, 0.09691851725196043]}, "mutation_prompt": null}
{"id": "01767670-e395-4387-86fa-dbcfdce1409b", "solution": "import numpy as np\n\nclass HybridDESA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.pop_size = 10 * dim\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.temp = 1000.0  # Initial temperature for simulated annealing\n        self.cooling_rate = 0.99  # Cooling rate for simulated annealing\n\n    def __call__(self, func):\n        np.random.seed(42)\n        population = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        num_evaluations = self.pop_size\n\n        def mutate(target_idx):\n            indices = [idx for idx in range(self.pop_size) if idx != target_idx]\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            dynamic_F = self.F * (1.0 - (num_evaluations / self.budget)) * np.exp(-num_evaluations / self.budget)\n            dynamic_F *= (self.temp / 1000.0) * (1 + 0.1 * (num_evaluations / self.budget))  # Modified line: Adaptive scaling to mutation factor\n            mutant = population[a] + dynamic_F * (population[b] - population[c])\n            mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n            return mutant\n\n        while num_evaluations < self.budget:\n            self.CR = 0.9 - (0.3 * (num_evaluations / self.budget)) + 0.1 * np.tanh(num_evaluations / self.budget)  # Modified line: Introduce learning rate update to crossover probability\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                mutant = mutate(i)\n                trial = np.where(np.random.rand(self.dim) < self.CR, mutant, 0.5 * (population[i] + best_solution))\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = trial\n\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                neighbor = population[i] + np.random.normal(0, 0.1 * self.temp / 1000, self.dim)\n                neighbor = np.clip(neighbor, self.lower_bound, self.upper_bound)\n                neighbor_fitness = func(neighbor)\n                num_evaluations += 1\n                delta_fit = neighbor_fitness - fitness[i]\n\n                if delta_fit < 0 or np.random.rand() < np.exp(-delta_fit / self.temp):\n                    population[i] = neighbor\n                    fitness[i] = neighbor_fitness\n                    if neighbor_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = neighbor\n\n            self.temp *= (self.cooling_rate - 0.01 * (num_evaluations / self.budget))\n            self.pop_size = max(4, int(8 * self.dim * (1 - num_evaluations / self.budget)))\n\n        return best_solution", "name": "HybridDESA", "description": "Introduce learning rate based update to the crossover probability to enhance exploration capabilities.", "configspace": "", "generation": 63, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "ba6bec03-35bc-47f6-92da-256729321735", "metadata": {"aucs": [0.5542661297327882, 0.5542661297327882, 0.5542661297327882, 0.5502131781010412, 0.5502131781010412, 0.5502131781010412, 0.5542576829571606, 0.5542576829571606, 0.5542576829571606, 0.3214986489793178, 0.3214986489793178, 0.3214986489793178, 0.3065416684084221, 0.3065416684084221, 0.3065416684084221, 0.3158065919396942, 0.3158065919396942, 0.3158065919396942, 0.13838181317138243, 0.13838181317138243, 0.13838181317138243, 0.12545327945625218, 0.12545327945625218, 0.12545327945625218, 0.12341091429570572, 0.12341091429570572, 0.12341091429570572, 0.10743269946665945, 0.10743269946665945, 0.10743269946665945, 0.1047695610119097, 0.1047695610119097, 0.1047695610119097, 0.11944414538050097, 0.11944414538050097, 0.11944414538050097, 0.8779320540888479, 0.8779320540888479, 0.8779320540888479, 0.877898188844995, 0.877898188844995, 0.877898188844995, 0.780456822675078, 0.780456822675078, 0.780456822675078, 0.3563309678652393, 0.3563309678652393, 0.3563309678652393, 0.3648861206411198, 0.3648861206411198, 0.3648861206411198, 0.3566431522159045, 0.3566431522159045, 0.3566431522159045, 0.6706862034126804, 0.6706862034126804, 0.6706862034126804, 0.3423519789978463, 0.3423519789978463, 0.3423519789978463, 0.72668247747501, 0.72668247747501, 0.72668247747501, 0.16919472358197873, 0.16919472358197873, 0.16919472358197873, 0.1651292859417527, 0.1651292859417527, 0.1651292859417527, 0.14989293282023441, 0.14989293282023441, 0.14989293282023441, 0.14250677278277657, 0.14250677278277657, 0.14250677278277657, 0.1589362233709246, 0.1589362233709246, 0.1589362233709246, 0.11857286877661266, 0.11857286877661266, 0.11857286877661266, 0.07046504581570368, 0.07046504581570368, 0.07046504581570368, 0.09291593469393078, 0.09291593469393078, 0.09291593469393078, 0.10976115636052908, 0.10976115636052908, 0.10976115636052908, 0.15193954057356018, 0.15193954057356018, 0.15193954057356018, 0.1246100015034457, 0.1246100015034457, 0.1246100015034457, 0.15848011734590806, 0.15848011734590806, 0.15848011734590806, 0.2108696082750796, 0.2108696082750796, 0.2108696082750796, 0.17218218373984961, 0.17218218373984961, 0.17218218373984961, 0.2329385394943635, 0.2329385394943635, 0.2329385394943635, 0.10591689517605296, 0.10591689517605296, 0.10591689517605296, 0.1552473120636909, 0.1552473120636909, 0.1552473120636909, 0.12199788623767405, 0.12199788623767405, 0.12199788623767405, 0.4593083307916008, 0.4593083307916008, 0.4593083307916008, 0.4628141347100536, 0.4628141347100536, 0.4628141347100536, 0.4549634008249209, 0.4549634008249209, 0.4549634008249209, 0.14035267481144242, 0.14035267481144242, 0.14035267481144242, 0.2647056139184356, 0.2647056139184356, 0.2647056139184356, 0.13663480456036958, 0.13663480456036958, 0.13663480456036958, 0.20334182760801733, 0.20334182760801733, 0.20334182760801733, 0.17681323754407496, 0.17681323754407496, 0.17681323754407496, 0.21871292997332525, 0.21871292997332525, 0.21871292997332525, 0.32825394007153186, 0.32825394007153186, 0.32825394007153186, 0.3326823931838664, 0.3326823931838664, 0.3326823931838664, 0.3421300043168062, 0.3421300043168062, 0.3421300043168062, 0.2724755833403435, 0.2724755833403435, 0.2724755833403435, 0.20678815163767184, 0.20678815163767184, 0.20678815163767184, 0.27220142835892625, 0.27220142835892625, 0.27220142835892625, 0.22900362693501475, 0.22900362693501475, 0.22900362693501475, 0.23990890494473383, 0.23990890494473383, 0.23990890494473383, 0.23540240144705793, 0.23540240144705793, 0.23540240144705793, 0.19425768105254093, 0.19425768105254093, 0.19425768105254093, 0.18912798567843125, 0.18912798567843125, 0.18912798567843125, 0.19106985300632762, 0.19106985300632762, 0.19106985300632762, 0.7025277924101051, 0.7025277924101051, 0.7025277924101051, 0.15133519477843094, 0.15133519477843094, 0.15133519477843094, 0.6240958961911556, 0.6240958961911556, 0.6240958961911556, 0.16573198475929618, 0.16573198475929618, 0.16573198475929618, 0.19732013670547166, 0.19732013670547166, 0.19732013670547166, 0.20412171108102173, 0.20412171108102173, 0.20412171108102173, 0.20894043819099617, 0.20894043819099617, 0.20894043819099617, 0.1933116987478113, 0.1933116987478113, 0.1933116987478113, 0.1933601729649982, 0.1933601729649982, 0.1933601729649982, 0.09564479257847158, 0.09564479257847158, 0.09564479257847158, 0.09537324264215674, 0.09537324264215674, 0.09537324264215674, 0.09878679773303067, 0.09878679773303067, 0.09878679773303067]}, "mutation_prompt": null}
{"id": "0dae23cc-9902-4a9a-8d16-5e6468396a0a", "solution": "import numpy as np\n\nclass HybridDESA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.pop_size = 10 * dim\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.temp = 1000.0  # Initial temperature for simulated annealing\n        self.cooling_rate = 0.99  # Cooling rate for simulated annealing\n\n    def __call__(self, func):\n        np.random.seed(42)\n        population = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        num_evaluations = self.pop_size\n\n        def mutate(target_idx):\n            indices = [idx for idx in range(self.pop_size) if idx != target_idx]\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            dynamic_F = self.F * (1.0 - (num_evaluations / self.budget)) * np.exp(-num_evaluations / self.budget)\n            dynamic_F *= (self.temp / 1000.0) * (1 + 0.1 * (num_evaluations / self.budget))  # Modified line: Adaptive scaling to mutation factor\n            mutant = population[a] + dynamic_F * (population[b] - population[c])\n            mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n            return mutant\n\n        while num_evaluations < self.budget:\n            self.CR = 0.9 - (0.3 * (num_evaluations / self.budget)) + 0.1 * np.tanh(num_evaluations / self.budget)  # Modified line: Introduce learning rate update to crossover probability\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                mutant = mutate(i)\n                trial = np.where(np.random.rand(self.dim) < self.CR, mutant, 0.5 * (population[i] + best_solution))\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = trial\n\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                neighbor = population[i] + np.random.normal(0, 0.1 * self.temp / 1000, self.dim)\n                neighbor = np.clip(neighbor, self.lower_bound, self.upper_bound)\n                neighbor_fitness = func(neighbor)\n                num_evaluations += 1\n                delta_fit = neighbor_fitness - fitness[i]\n\n                if delta_fit < 0 or np.random.rand() < np.exp(-delta_fit / self.temp):\n                    population[i] = neighbor\n                    fitness[i] = neighbor_fitness\n                    if neighbor_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = neighbor\n\n            self.temp *= (self.cooling_rate - 0.01 * (num_evaluations / self.budget))\n            self.pop_size = max(4, int(8 * self.dim * (1 - num_evaluations / self.budget)))\n\n        return best_solution", "name": "HybridDESA", "description": "Introduce learning rate based update to the crossover probability to enhance exploration capabilities.", "configspace": "", "generation": 63, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "ba6bec03-35bc-47f6-92da-256729321735", "metadata": {"aucs": [0.5542661297327882, 0.5542661297327882, 0.5542661297327882, 0.5502131781010412, 0.5502131781010412, 0.5502131781010412, 0.5542576829571606, 0.5542576829571606, 0.5542576829571606, 0.3214986489793178, 0.3214986489793178, 0.3214986489793178, 0.3065416684084221, 0.3065416684084221, 0.3065416684084221, 0.3158065919396942, 0.3158065919396942, 0.3158065919396942, 0.13838181317138243, 0.13838181317138243, 0.13838181317138243, 0.12545327945625218, 0.12545327945625218, 0.12545327945625218, 0.12341091429570572, 0.12341091429570572, 0.12341091429570572, 0.10743269946665945, 0.10743269946665945, 0.10743269946665945, 0.1047695610119097, 0.1047695610119097, 0.1047695610119097, 0.11944414538050097, 0.11944414538050097, 0.11944414538050097, 0.8779320540888479, 0.8779320540888479, 0.8779320540888479, 0.877898188844995, 0.877898188844995, 0.877898188844995, 0.780456822675078, 0.780456822675078, 0.780456822675078, 0.3563309678652393, 0.3563309678652393, 0.3563309678652393, 0.3648861206411198, 0.3648861206411198, 0.3648861206411198, 0.3566431522159045, 0.3566431522159045, 0.3566431522159045, 0.6706862034126804, 0.6706862034126804, 0.6706862034126804, 0.3423519789978463, 0.3423519789978463, 0.3423519789978463, 0.72668247747501, 0.72668247747501, 0.72668247747501, 0.16919472358197873, 0.16919472358197873, 0.16919472358197873, 0.1651292859417527, 0.1651292859417527, 0.1651292859417527, 0.14989293282023441, 0.14989293282023441, 0.14989293282023441, 0.14250677278277657, 0.14250677278277657, 0.14250677278277657, 0.1589362233709246, 0.1589362233709246, 0.1589362233709246, 0.11857286877661266, 0.11857286877661266, 0.11857286877661266, 0.07046504581570368, 0.07046504581570368, 0.07046504581570368, 0.09291593469393078, 0.09291593469393078, 0.09291593469393078, 0.10976115636052908, 0.10976115636052908, 0.10976115636052908, 0.15193954057356018, 0.15193954057356018, 0.15193954057356018, 0.1246100015034457, 0.1246100015034457, 0.1246100015034457, 0.15848011734590806, 0.15848011734590806, 0.15848011734590806, 0.2108696082750796, 0.2108696082750796, 0.2108696082750796, 0.17218218373984961, 0.17218218373984961, 0.17218218373984961, 0.2329385394943635, 0.2329385394943635, 0.2329385394943635, 0.10591689517605296, 0.10591689517605296, 0.10591689517605296, 0.1552473120636909, 0.1552473120636909, 0.1552473120636909, 0.12199788623767405, 0.12199788623767405, 0.12199788623767405, 0.4593083307916008, 0.4593083307916008, 0.4593083307916008, 0.4628141347100536, 0.4628141347100536, 0.4628141347100536, 0.4549634008249209, 0.4549634008249209, 0.4549634008249209, 0.14035267481144242, 0.14035267481144242, 0.14035267481144242, 0.2647056139184356, 0.2647056139184356, 0.2647056139184356, 0.13663480456036958, 0.13663480456036958, 0.13663480456036958, 0.20334182760801733, 0.20334182760801733, 0.20334182760801733, 0.17681323754407496, 0.17681323754407496, 0.17681323754407496, 0.21871292997332525, 0.21871292997332525, 0.21871292997332525, 0.32825394007153186, 0.32825394007153186, 0.32825394007153186, 0.3326823931838664, 0.3326823931838664, 0.3326823931838664, 0.3421300043168062, 0.3421300043168062, 0.3421300043168062, 0.2724755833403435, 0.2724755833403435, 0.2724755833403435, 0.20678815163767184, 0.20678815163767184, 0.20678815163767184, 0.27220142835892625, 0.27220142835892625, 0.27220142835892625, 0.22900362693501475, 0.22900362693501475, 0.22900362693501475, 0.23990890494473383, 0.23990890494473383, 0.23990890494473383, 0.23540240144705793, 0.23540240144705793, 0.23540240144705793, 0.19425768105254093, 0.19425768105254093, 0.19425768105254093, 0.18912798567843125, 0.18912798567843125, 0.18912798567843125, 0.19106985300632762, 0.19106985300632762, 0.19106985300632762, 0.7025277924101051, 0.7025277924101051, 0.7025277924101051, 0.15133519477843094, 0.15133519477843094, 0.15133519477843094, 0.6240958961911556, 0.6240958961911556, 0.6240958961911556, 0.16573198475929618, 0.16573198475929618, 0.16573198475929618, 0.19732013670547166, 0.19732013670547166, 0.19732013670547166, 0.20412171108102173, 0.20412171108102173, 0.20412171108102173, 0.20894043819099617, 0.20894043819099617, 0.20894043819099617, 0.1933116987478113, 0.1933116987478113, 0.1933116987478113, 0.1933601729649982, 0.1933601729649982, 0.1933601729649982, 0.09564479257847158, 0.09564479257847158, 0.09564479257847158, 0.09537324264215674, 0.09537324264215674, 0.09537324264215674, 0.09878679773303067, 0.09878679773303067, 0.09878679773303067]}, "mutation_prompt": null}
{"id": "16de1f49-5a54-456b-b4a3-6b74fcfe1d10", "solution": "import numpy as np\n\nclass HybridDESA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.pop_size = 10 * dim\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.temp = 1000.0  # Initial temperature for simulated annealing\n        self.cooling_rate = 0.99  # Cooling rate for simulated annealing\n\n    def __call__(self, func):\n        np.random.seed(42)\n        population = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        num_evaluations = self.pop_size\n\n        def mutate(target_idx):\n            indices = [idx for idx in range(self.pop_size) if idx != target_idx]\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            dynamic_F = self.F * (1.0 - (num_evaluations / self.budget)) * np.exp(-num_evaluations / self.budget)\n            dynamic_F *= (self.temp / 1000.0) * (1 + 0.15 * (num_evaluations / self.budget))  # Modified line: Slightly increased adaptive scaling to mutation factor\n            mutant = population[a] + dynamic_F * (population[b] - population[c])\n            mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n            return mutant\n\n        while num_evaluations < self.budget:\n            self.CR = 0.9 - (0.3 * (num_evaluations / self.budget)) + 0.1 * np.tanh(num_evaluations / self.budget)\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                mutant = mutate(i)\n                trial = np.where(np.random.rand(self.dim) < self.CR, mutant, 0.5 * (population[i] + best_solution))\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = trial\n\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                neighbor = population[i] + np.random.normal(0, 0.1 * self.temp / 1000, self.dim)\n                neighbor = np.clip(neighbor, self.lower_bound, self.upper_bound)\n                neighbor_fitness = func(neighbor)\n                num_evaluations += 1\n                delta_fit = neighbor_fitness - fitness[i]\n\n                if delta_fit < 0 or np.random.rand() < np.exp(-delta_fit / self.temp):\n                    population[i] = neighbor\n                    fitness[i] = neighbor_fitness\n                    if neighbor_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = neighbor\n\n            self.temp *= (self.cooling_rate - 0.01 * (num_evaluations / self.budget))\n            self.pop_size = max(4, int(8 * self.dim * (1 - num_evaluations / self.budget)))\n\n        return best_solution", "name": "HybridDESA", "description": "Adjust dynamic mutation factor scaling to enhance exploration-exploitation balance.", "configspace": "", "generation": 92, "fitness": 0.2720185316563026, "feedback": "The algorithm HybridDESA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.27 with standard deviation 0.19.", "error": "", "parent_id": "ba6bec03-35bc-47f6-92da-256729321735", "metadata": {"aucs": [0.5322257379576107, 0.5322257379576107, 0.5322257379576107, 0.5428085140550576, 0.5428085140550576, 0.5428085140550576, 0.547125730424805, 0.547125730424805, 0.547125730424805, 0.24756125555079067, 0.24756125555079067, 0.24756125555079067, 0.27747562251023694, 0.27747562251023694, 0.27747562251023694, 0.2684822292652913, 0.2684822292652913, 0.2684822292652913, 0.26516687483079204, 0.26516687483079204, 0.26516687483079204, 0.1042341623324885, 0.1042341623324885, 0.1042341623324885, 0.12647705561297784, 0.12647705561297784, 0.12647705561297784, 0.1031948638415624, 0.1031948638415624, 0.1031948638415624, 0.10055231700992373, 0.10055231700992373, 0.10055231700992373, 0.12004620691161838, 0.12004620691161838, 0.12004620691161838, 0.8383048902273056, 0.8383048902273056, 0.8383048902273056, 0.8448261078954227, 0.8448261078954227, 0.8448261078954227, 0.8376855234252261, 0.8376855234252261, 0.8376855234252261, 0.3535031059020105, 0.3535031059020105, 0.3535031059020105, 0.35505907103108847, 0.35505907103108847, 0.35505907103108847, 0.3656679158614681, 0.3656679158614681, 0.3656679158614681, 0.33865232788854227, 0.33865232788854227, 0.33865232788854227, 0.5782315143206114, 0.5782315143206114, 0.5782315143206114, 0.671346777824079, 0.671346777824079, 0.671346777824079, 0.17279226578761697, 0.17279226578761697, 0.17279226578761697, 0.17036103059556496, 0.17036103059556496, 0.17036103059556496, 0.14622758930687108, 0.14622758930687108, 0.14622758930687108, 0.15862473903848517, 0.15862473903848517, 0.15862473903848517, 0.15489472251793102, 0.15489472251793102, 0.15489472251793102, 0.1586389436059471, 0.1586389436059471, 0.1586389436059471, 0.11706679320103319, 0.11706679320103319, 0.11706679320103319, 0.06815969129609023, 0.06815969129609023, 0.06815969129609023, 0.09717318965690958, 0.09717318965690958, 0.09717318965690958, 0.14799895948971664, 0.14799895948971664, 0.14799895948971664, 0.10918809877830826, 0.10918809877830826, 0.10918809877830826, 0.13019733321546323, 0.13019733321546323, 0.13019733321546323, 0.1985633730943348, 0.1985633730943348, 0.1985633730943348, 0.13961729367390985, 0.13961729367390985, 0.13961729367390985, 0.20239416088342777, 0.20239416088342777, 0.20239416088342777, 0.14635262774336066, 0.14635262774336066, 0.14635262774336066, 0.16197669479668253, 0.16197669479668253, 0.16197669479668253, 0.15748660703044948, 0.15748660703044948, 0.15748660703044948, 0.45418212213872244, 0.45418212213872244, 0.45418212213872244, 0.4345096262529393, 0.4345096262529393, 0.4345096262529393, 0.43616917233487107, 0.43616917233487107, 0.43616917233487107, 0.15457589416266126, 0.15457589416266126, 0.15457589416266126, 0.1211292813127609, 0.1211292813127609, 0.1211292813127609, 0.15385927961002832, 0.15385927961002832, 0.15385927961002832, 0.21262351703000604, 0.21262351703000604, 0.21262351703000604, 0.2187752540296538, 0.2187752540296538, 0.2187752540296538, 0.2272599604065706, 0.2272599604065706, 0.2272599604065706, 0.3360728866046183, 0.3360728866046183, 0.3360728866046183, 0.3273277338579287, 0.3273277338579287, 0.3273277338579287, 0.33430193111743933, 0.33430193111743933, 0.33430193111743933, 0.2651165016549798, 0.2651165016549798, 0.2651165016549798, 0.1856803935703013, 0.1856803935703013, 0.1856803935703013, 0.29123477335401227, 0.29123477335401227, 0.29123477335401227, 0.20631145630896985, 0.20631145630896985, 0.20631145630896985, 0.24874231197017294, 0.24874231197017294, 0.24874231197017294, 0.21322027374149588, 0.21322027374149588, 0.21322027374149588, 0.17154729069467545, 0.17154729069467545, 0.17154729069467545, 0.2041713120181432, 0.2041713120181432, 0.2041713120181432, 0.3638298007688431, 0.3638298007688431, 0.3638298007688431, 0.717465533945614, 0.717465533945614, 0.717465533945614, 0.15444582045762878, 0.15444582045762878, 0.15444582045762878, 0.6273445783519054, 0.6273445783519054, 0.6273445783519054, 0.1655367397081664, 0.1655367397081664, 0.1655367397081664, 0.2011362606847752, 0.2011362606847752, 0.2011362606847752, 0.2048768611079026, 0.2048768611079026, 0.2048768611079026, 0.20128244881735735, 0.20128244881735735, 0.20128244881735735, 0.2078668029908587, 0.2078668029908587, 0.2078668029908587, 0.20886574398722957, 0.20886574398722957, 0.20886574398722957, 0.09714684590505607, 0.09714684590505607, 0.09714684590505607, 0.08867064692434012, 0.08867064692434012, 0.08867064692434012, 0.09371330504214381, 0.09371330504214381, 0.09371330504214381]}, "mutation_prompt": null}
{"id": "1ad1f117-1c38-460a-b0fc-e60d30430591", "solution": "import numpy as np\n\nclass HybridDESA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.pop_size = 10 * dim\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.temp = 1000.0  # Initial temperature for simulated annealing\n        self.cooling_rate = 0.99  # Cooling rate for simulated annealing\n\n    def __call__(self, func):\n        np.random.seed(42)\n        population = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        num_evaluations = self.pop_size\n\n        def mutate(target_idx):\n            indices = [idx for idx in range(self.pop_size) if idx != target_idx]\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            dynamic_F = self.F * (1.0 - (num_evaluations / self.budget)) * np.exp(-num_evaluations / self.budget)\n            dynamic_F *= (self.temp / 1000.0) * (1 + 0.1 * (num_evaluations / self.budget))  # Modified line: Adaptive scaling to mutation factor\n            mutant = population[a] + dynamic_F * (population[b] - population[c])\n            mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n            return mutant\n\n        while num_evaluations < self.budget:\n            self.CR = 0.9 - (0.3 * (num_evaluations / self.budget)) + 0.1 * np.tanh(num_evaluations / self.budget)  # Modified line: Introduce learning rate update to crossover probability\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                mutant = mutate(i)\n                trial = np.where(np.random.rand(self.dim) < self.CR, mutant, 0.5 * (population[i] + best_solution))\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = trial\n\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                neighbor = population[i] + np.random.normal(0, 0.1 * self.temp / 1000, self.dim)\n                neighbor = np.clip(neighbor, self.lower_bound, self.upper_bound)\n                neighbor_fitness = func(neighbor)\n                num_evaluations += 1\n                delta_fit = neighbor_fitness - fitness[i]\n\n                if delta_fit < 0 or np.random.rand() < np.exp(-delta_fit / self.temp):\n                    population[i] = neighbor\n                    fitness[i] = neighbor_fitness\n                    if neighbor_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = neighbor\n\n            self.temp *= (self.cooling_rate - 0.01 * (num_evaluations / self.budget))\n            self.pop_size = max(4, int(8 * self.dim * (1 - num_evaluations / self.budget)))\n\n        return best_solution", "name": "HybridDESA", "description": "Introduce learning rate based update to the crossover probability to enhance exploration capabilities.", "configspace": "", "generation": 63, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "ba6bec03-35bc-47f6-92da-256729321735", "metadata": {"aucs": [0.5542661297327882, 0.5542661297327882, 0.5542661297327882, 0.5502131781010412, 0.5502131781010412, 0.5502131781010412, 0.5542576829571606, 0.5542576829571606, 0.5542576829571606, 0.3214986489793178, 0.3214986489793178, 0.3214986489793178, 0.3065416684084221, 0.3065416684084221, 0.3065416684084221, 0.3158065919396942, 0.3158065919396942, 0.3158065919396942, 0.13838181317138243, 0.13838181317138243, 0.13838181317138243, 0.12545327945625218, 0.12545327945625218, 0.12545327945625218, 0.12341091429570572, 0.12341091429570572, 0.12341091429570572, 0.10743269946665945, 0.10743269946665945, 0.10743269946665945, 0.1047695610119097, 0.1047695610119097, 0.1047695610119097, 0.11944414538050097, 0.11944414538050097, 0.11944414538050097, 0.8779320540888479, 0.8779320540888479, 0.8779320540888479, 0.877898188844995, 0.877898188844995, 0.877898188844995, 0.780456822675078, 0.780456822675078, 0.780456822675078, 0.3563309678652393, 0.3563309678652393, 0.3563309678652393, 0.3648861206411198, 0.3648861206411198, 0.3648861206411198, 0.3566431522159045, 0.3566431522159045, 0.3566431522159045, 0.6706862034126804, 0.6706862034126804, 0.6706862034126804, 0.3423519789978463, 0.3423519789978463, 0.3423519789978463, 0.72668247747501, 0.72668247747501, 0.72668247747501, 0.16919472358197873, 0.16919472358197873, 0.16919472358197873, 0.1651292859417527, 0.1651292859417527, 0.1651292859417527, 0.14989293282023441, 0.14989293282023441, 0.14989293282023441, 0.14250677278277657, 0.14250677278277657, 0.14250677278277657, 0.1589362233709246, 0.1589362233709246, 0.1589362233709246, 0.11857286877661266, 0.11857286877661266, 0.11857286877661266, 0.07046504581570368, 0.07046504581570368, 0.07046504581570368, 0.09291593469393078, 0.09291593469393078, 0.09291593469393078, 0.10976115636052908, 0.10976115636052908, 0.10976115636052908, 0.15193954057356018, 0.15193954057356018, 0.15193954057356018, 0.1246100015034457, 0.1246100015034457, 0.1246100015034457, 0.15848011734590806, 0.15848011734590806, 0.15848011734590806, 0.2108696082750796, 0.2108696082750796, 0.2108696082750796, 0.17218218373984961, 0.17218218373984961, 0.17218218373984961, 0.2329385394943635, 0.2329385394943635, 0.2329385394943635, 0.10591689517605296, 0.10591689517605296, 0.10591689517605296, 0.1552473120636909, 0.1552473120636909, 0.1552473120636909, 0.12199788623767405, 0.12199788623767405, 0.12199788623767405, 0.4593083307916008, 0.4593083307916008, 0.4593083307916008, 0.4628141347100536, 0.4628141347100536, 0.4628141347100536, 0.4549634008249209, 0.4549634008249209, 0.4549634008249209, 0.14035267481144242, 0.14035267481144242, 0.14035267481144242, 0.2647056139184356, 0.2647056139184356, 0.2647056139184356, 0.13663480456036958, 0.13663480456036958, 0.13663480456036958, 0.20334182760801733, 0.20334182760801733, 0.20334182760801733, 0.17681323754407496, 0.17681323754407496, 0.17681323754407496, 0.21871292997332525, 0.21871292997332525, 0.21871292997332525, 0.32825394007153186, 0.32825394007153186, 0.32825394007153186, 0.3326823931838664, 0.3326823931838664, 0.3326823931838664, 0.3421300043168062, 0.3421300043168062, 0.3421300043168062, 0.2724755833403435, 0.2724755833403435, 0.2724755833403435, 0.20678815163767184, 0.20678815163767184, 0.20678815163767184, 0.27220142835892625, 0.27220142835892625, 0.27220142835892625, 0.22900362693501475, 0.22900362693501475, 0.22900362693501475, 0.23990890494473383, 0.23990890494473383, 0.23990890494473383, 0.23540240144705793, 0.23540240144705793, 0.23540240144705793, 0.19425768105254093, 0.19425768105254093, 0.19425768105254093, 0.18912798567843125, 0.18912798567843125, 0.18912798567843125, 0.19106985300632762, 0.19106985300632762, 0.19106985300632762, 0.7025277924101051, 0.7025277924101051, 0.7025277924101051, 0.15133519477843094, 0.15133519477843094, 0.15133519477843094, 0.6240958961911556, 0.6240958961911556, 0.6240958961911556, 0.16573198475929618, 0.16573198475929618, 0.16573198475929618, 0.19732013670547166, 0.19732013670547166, 0.19732013670547166, 0.20412171108102173, 0.20412171108102173, 0.20412171108102173, 0.20894043819099617, 0.20894043819099617, 0.20894043819099617, 0.1933116987478113, 0.1933116987478113, 0.1933116987478113, 0.1933601729649982, 0.1933601729649982, 0.1933601729649982, 0.09564479257847158, 0.09564479257847158, 0.09564479257847158, 0.09537324264215674, 0.09537324264215674, 0.09537324264215674, 0.09878679773303067, 0.09878679773303067, 0.09878679773303067]}, "mutation_prompt": null}
{"id": "2497dcab-aa06-4556-960b-95ae0c5afa7f", "solution": "import numpy as np\n\nclass HybridDESA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.pop_size = 10 * dim\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.temp = 1000.0  # Initial temperature for simulated annealing\n        self.cooling_rate = 0.99  # Cooling rate for simulated annealing\n\n    def __call__(self, func):\n        np.random.seed(42)\n        population = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        num_evaluations = self.pop_size\n\n        def mutate(target_idx):\n            indices = [idx for idx in range(self.pop_size) if idx != target_idx]\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            dynamic_F = self.F * (1.0 - (num_evaluations / self.budget)) * np.exp(-num_evaluations / self.budget)\n            dynamic_F *= (self.temp / 1000.0) * (1 + 0.1 * (num_evaluations / self.budget))  # Modified line: Adaptive scaling to mutation factor\n            mutant = population[a] + dynamic_F * (population[b] - population[c])\n            mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n            return mutant\n\n        while num_evaluations < self.budget:\n            self.CR = 0.9 - (0.3 * (num_evaluations / self.budget)) + 0.1 * np.tanh(num_evaluations / self.budget)  # Modified line: Introduce learning rate update to crossover probability\n            self.CR = 0.5 + 0.4 * (1 - 1 / (1 + np.exp(-10 * (num_evaluations / self.budget - 0.5))))  # Changed line: Use sigmoid function for crossover probability\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                mutant = mutate(i)\n                trial = np.where(np.random.rand(self.dim) < self.CR, mutant, 0.5 * (population[i] + best_solution))\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = trial\n\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                neighbor = population[i] + np.random.normal(0, 0.1 * self.temp / 1000, self.dim)\n                neighbor = np.clip(neighbor, self.lower_bound, self.upper_bound)\n                neighbor_fitness = func(neighbor)\n                num_evaluations += 1\n                delta_fit = neighbor_fitness - fitness[i]\n\n                if delta_fit < 0 or np.random.rand() < np.exp(-delta_fit / self.temp):\n                    population[i] = neighbor\n                    fitness[i] = neighbor_fitness\n                    if neighbor_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = neighbor\n\n            self.temp *= (self.cooling_rate - 0.01 * (num_evaluations / self.budget))\n            self.pop_size = max(4, int(8 * self.dim * (1 - num_evaluations / self.budget)))\n\n        return best_solution", "name": "HybridDESA", "description": "Adjust the crossover probability using a sigmoid function to improve the balance between exploration and exploitation.", "configspace": "", "generation": 94, "fitness": 0.2788570640920536, "feedback": "The algorithm HybridDESA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28 with standard deviation 0.19.", "error": "", "parent_id": "ba6bec03-35bc-47f6-92da-256729321735", "metadata": {"aucs": [0.5516461231386738, 0.5516461231386738, 0.5516461231386738, 0.5712851158603225, 0.5712851158603225, 0.5712851158603225, 0.5598342381646224, 0.5598342381646224, 0.5598342381646224, 0.3031264035508785, 0.3031264035508785, 0.3031264035508785, 0.2548268041511147, 0.2548268041511147, 0.2548268041511147, 0.29706132529527063, 0.29706132529527063, 0.29706132529527063, 0.12354463999008602, 0.12354463999008602, 0.12354463999008602, 0.15568658338627261, 0.15568658338627261, 0.15568658338627261, 0.1528540288995589, 0.1528540288995589, 0.1528540288995589, 0.09575359644884307, 0.09575359644884307, 0.09575359644884307, 0.08692418286380432, 0.08692418286380432, 0.08692418286380432, 0.11530078923902187, 0.11530078923902187, 0.11530078923902187, 0.8022630886589334, 0.8022630886589334, 0.8022630886589334, 0.8811363188008171, 0.8811363188008171, 0.8811363188008171, 0.8273191922525636, 0.8273191922525636, 0.8273191922525636, 0.3593262483615912, 0.3593262483615912, 0.3593262483615912, 0.3649047028381144, 0.3649047028381144, 0.3649047028381144, 0.36546267207638183, 0.36546267207638183, 0.36546267207638183, 0.7084362053186021, 0.7084362053186021, 0.7084362053186021, 0.7304204590936285, 0.7304204590936285, 0.7304204590936285, 0.28036499011471316, 0.28036499011471316, 0.28036499011471316, 0.15975135854870315, 0.15975135854870315, 0.15975135854870315, 0.15456222268164344, 0.15456222268164344, 0.15456222268164344, 0.15872853362900674, 0.15872853362900674, 0.15872853362900674, 0.17879489870799992, 0.17879489870799992, 0.17879489870799992, 0.1855315516942796, 0.1855315516942796, 0.1855315516942796, 0.18695640943697922, 0.18695640943697922, 0.18695640943697922, 0.07618665615034226, 0.07618665615034226, 0.07618665615034226, 0.12334802093657893, 0.12334802093657893, 0.12334802093657893, 0.11514406258605869, 0.11514406258605869, 0.11514406258605869, 0.15512063047980917, 0.15512063047980917, 0.15512063047980917, 0.16013286404402716, 0.16013286404402716, 0.16013286404402716, 0.1760909108810882, 0.1760909108810882, 0.1760909108810882, 0.16002766883464736, 0.16002766883464736, 0.16002766883464736, 0.07487974298653044, 0.07487974298653044, 0.07487974298653044, 0.1431017336741235, 0.1431017336741235, 0.1431017336741235, 0.15957486065121262, 0.15957486065121262, 0.15957486065121262, 0.17516284876553823, 0.17516284876553823, 0.17516284876553823, 0.13197102009488193, 0.13197102009488193, 0.13197102009488193, 0.42888360366529565, 0.42888360366529565, 0.42888360366529565, 0.4634963923826697, 0.4634963923826697, 0.4634963923826697, 0.44387690342905894, 0.44387690342905894, 0.44387690342905894, 0.11527106206213888, 0.11527106206213888, 0.11527106206213888, 0.15387651432527505, 0.15387651432527505, 0.15387651432527505, 0.26595216810902134, 0.26595216810902134, 0.26595216810902134, 0.195930126323411, 0.195930126323411, 0.195930126323411, 0.28330657896441747, 0.28330657896441747, 0.28330657896441747, 0.32413092742925986, 0.32413092742925986, 0.32413092742925986, 0.33116996082825056, 0.33116996082825056, 0.33116996082825056, 0.3301724571596204, 0.3301724571596204, 0.3301724571596204, 0.3445104815944636, 0.3445104815944636, 0.3445104815944636, 0.28208867357220835, 0.28208867357220835, 0.28208867357220835, 0.26003034453839824, 0.26003034453839824, 0.26003034453839824, 0.2833607058257881, 0.2833607058257881, 0.2833607058257881, 0.2316068917958216, 0.2316068917958216, 0.2316068917958216, 0.232285733485474, 0.232285733485474, 0.232285733485474, 0.23256467003010584, 0.23256467003010584, 0.23256467003010584, 0.22405421388832314, 0.22405421388832314, 0.22405421388832314, 0.17431301104828978, 0.17431301104828978, 0.17431301104828978, 0.22584976644035726, 0.22584976644035726, 0.22584976644035726, 0.7445290105706044, 0.7445290105706044, 0.7445290105706044, 0.6239130959856514, 0.6239130959856514, 0.6239130959856514, 0.15831868215054024, 0.15831868215054024, 0.15831868215054024, 0.16556867715249446, 0.16556867715249446, 0.16556867715249446, 0.20294553004295235, 0.20294553004295235, 0.20294553004295235, 0.20400827620232098, 0.20400827620232098, 0.20400827620232098, 0.20566723046848978, 0.20566723046848978, 0.20566723046848978, 0.20675736842872883, 0.20675736842872883, 0.20675736842872883, 0.19433376189521545, 0.19433376189521545, 0.19433376189521545, 0.09496267105103906, 0.09496267105103906, 0.09496267105103906, 0.09508326687554014, 0.09508326687554014, 0.09508326687554014, 0.09234615361936283, 0.09234615361936283, 0.09234615361936283]}, "mutation_prompt": null}
{"id": "69a503a1-2f26-47b7-a2c7-7bb08a4ea5e7", "solution": "import numpy as np\n\nclass HybridDESA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.pop_size = 10 * dim\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.temp = 1000.0  # Initial temperature for simulated annealing\n        self.cooling_rate = 0.99  # Cooling rate for simulated annealing\n\n    def __call__(self, func):\n        np.random.seed(42)\n        population = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        num_evaluations = self.pop_size\n\n        def mutate(target_idx):\n            indices = [idx for idx in range(self.pop_size) if idx != target_idx]\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            dynamic_F = self.F * (1.0 - (num_evaluations / self.budget)) * np.exp(-num_evaluations / self.budget)\n            dynamic_F *= (self.temp / 1000.0) * (1 + 0.1 * (num_evaluations / self.budget))  # Modified line: Adaptive scaling to mutation factor\n            mutant = population[a] + dynamic_F * (population[b] - population[c])\n            mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n            return mutant\n\n        while num_evaluations < self.budget:\n            self.CR = 0.9 - (0.3 * (num_evaluations / self.budget)) + 0.1 * np.tanh(num_evaluations / self.budget)  # Modified line: Introduce learning rate update to crossover probability\n            self.CR = 0.5 + 0.4 * (1 - 1 / (1 + np.exp(-10 * (num_evaluations / self.budget - 0.5))))  # Changed line: Use sigmoid function for crossover probability\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                mutant = mutate(i)\n                trial = np.where(np.random.rand(self.dim) < self.CR, mutant, 0.5 * (population[i] + best_solution))\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = trial\n\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                neighbor = population[i] + np.random.normal(0, 0.1 * self.temp / 1000, self.dim)\n                neighbor = np.clip(neighbor, self.lower_bound, self.upper_bound)\n                neighbor_fitness = func(neighbor)\n                num_evaluations += 1\n                delta_fit = neighbor_fitness - fitness[i]\n\n                if delta_fit < 0 or np.random.rand() < np.exp(-delta_fit / self.temp):\n                    population[i] = neighbor\n                    fitness[i] = neighbor_fitness\n                    if neighbor_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = neighbor\n\n            self.temp *= (self.cooling_rate - 0.01 * (num_evaluations / self.budget) * np.sin(num_evaluations / self.budget))  # Changed line: Adjust cooling rate dynamically\n            self.pop_size = max(4, int(8 * self.dim * (1 - num_evaluations / self.budget)))\n\n        return best_solution", "name": "HybridDESA", "description": "Employ a dynamic cooling rate adjustment based on evaluation progress to refine simulated annealing.", "configspace": "", "generation": 95, "fitness": 0.2666845455250865, "feedback": "The algorithm HybridDESA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.27 with standard deviation 0.20.", "error": "", "parent_id": "2497dcab-aa06-4556-960b-95ae0c5afa7f", "metadata": {"aucs": [0.5251644402444471, 0.5251644402444471, 0.5251644402444471, 0.5315072305371104, 0.5315072305371104, 0.5315072305371104, 0.542146243644352, 0.542146243644352, 0.542146243644352, 0.14845293996846887, 0.14845293996846887, 0.14845293996846887, 0.2593575861339895, 0.2593575861339895, 0.2593575861339895, 0.21879843000043153, 0.21879843000043153, 0.21879843000043153, 0.13791427627963027, 0.13791427627963027, 0.13791427627963027, 0.1240018920733732, 0.1240018920733732, 0.1240018920733732, 0.13975611583381853, 0.13975611583381853, 0.13975611583381853, 0.11260703506267056, 0.11260703506267056, 0.11260703506267056, 0.09837226318228531, 0.09837226318228531, 0.09837226318228531, 0.10598217345512095, 0.10598217345512095, 0.10598217345512095, 0.8299780789288992, 0.8299780789288992, 0.8299780789288992, 0.8786868724226333, 0.8786868724226333, 0.8786868724226333, 0.8124843193109185, 0.8124843193109185, 0.8124843193109185, 0.3381633263473044, 0.3381633263473044, 0.3381633263473044, 0.33991135274149065, 0.33991135274149065, 0.33991135274149065, 0.33293216071713383, 0.33293216071713383, 0.33293216071713383, 0.6687334578174742, 0.6687334578174742, 0.6687334578174742, 0.7715273908514374, 0.7715273908514374, 0.7715273908514374, 0.7137887296708751, 0.7137887296708751, 0.7137887296708751, 0.18638372357271082, 0.18638372357271082, 0.18638372357271082, 0.17137210971674288, 0.17137210971674288, 0.17137210971674288, 0.17514470680255434, 0.17514470680255434, 0.17514470680255434, 0.15896140236962075, 0.15896140236962075, 0.15896140236962075, 0.15054211035701703, 0.15054211035701703, 0.15054211035701703, 0.16347525195323287, 0.16347525195323287, 0.16347525195323287, 0.098148821044026, 0.098148821044026, 0.098148821044026, 0.0881636429335989, 0.0881636429335989, 0.0881636429335989, 0.10895475617106076, 0.10895475617106076, 0.10895475617106076, 0.1612343708886863, 0.1612343708886863, 0.1612343708886863, 0.16804616679557471, 0.16804616679557471, 0.16804616679557471, 0.2194626777826435, 0.2194626777826435, 0.2194626777826435, 0.1216052449105951, 0.1216052449105951, 0.1216052449105951, 0.07887314118946909, 0.07887314118946909, 0.07887314118946909, 0.1146261590361195, 0.1146261590361195, 0.1146261590361195, 0.1463900046125154, 0.1463900046125154, 0.1463900046125154, 0.1175667381677129, 0.1175667381677129, 0.1175667381677129, 0.10926177000211379, 0.10926177000211379, 0.10926177000211379, 0.43927068254529067, 0.43927068254529067, 0.43927068254529067, 0.43830501275875045, 0.43830501275875045, 0.43830501275875045, 0.44045745447271234, 0.44045745447271234, 0.44045745447271234, 0.13745244995419215, 0.13745244995419215, 0.13745244995419215, 0.11605262083539947, 0.11605262083539947, 0.11605262083539947, 0.15277768740934206, 0.15277768740934206, 0.15277768740934206, 0.28654001592652956, 0.28654001592652956, 0.28654001592652956, 0.2269898353938753, 0.2269898353938753, 0.2269898353938753, 0.22408395898171607, 0.22408395898171607, 0.22408395898171607, 0.328817709868666, 0.328817709868666, 0.328817709868666, 0.32288120626067973, 0.32288120626067973, 0.32288120626067973, 0.3278896295679091, 0.3278896295679091, 0.3278896295679091, 0.26585705983019414, 0.26585705983019414, 0.26585705983019414, 0.2645530749217402, 0.2645530749217402, 0.2645530749217402, 0.2750416609924674, 0.2750416609924674, 0.2750416609924674, 0.2519404670772526, 0.2519404670772526, 0.2519404670772526, 0.24223073663691863, 0.24223073663691863, 0.24223073663691863, 0.2018083338506682, 0.2018083338506682, 0.2018083338506682, 0.18204944414855395, 0.18204944414855395, 0.18204944414855395, 0.18614293746090949, 0.18614293746090949, 0.18614293746090949, 0.20060346375139093, 0.20060346375139093, 0.20060346375139093, 0.6834593921534927, 0.6834593921534927, 0.6834593921534927, 0.18147244336219803, 0.18147244336219803, 0.18147244336219803, 0.18082759272052473, 0.18082759272052473, 0.18082759272052473, 0.16531442504360216, 0.16531442504360216, 0.16531442504360216, 0.19843446229030448, 0.19843446229030448, 0.19843446229030448, 0.20460995776113788, 0.20460995776113788, 0.20460995776113788, 0.2194500206263439, 0.2194500206263439, 0.2194500206263439, 0.20988056240996844, 0.20988056240996844, 0.20988056240996844, 0.19047755415660028, 0.19047755415660028, 0.19047755415660028, 0.10172720788196088, 0.10172720788196088, 0.10172720788196088, 0.09686874968822667, 0.09686874968822667, 0.09686874968822667, 0.0885403535368503, 0.0885403535368503, 0.0885403535368503]}, "mutation_prompt": null}
{"id": "902b03d0-41fb-4fe0-869d-f35f413d79d7", "solution": "import numpy as np\n\nclass HybridDESA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.pop_size = 10 * dim\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.temp = 1000.0  # Initial temperature for simulated annealing\n        self.cooling_rate = 0.99  # Cooling rate for simulated annealing\n\n    def __call__(self, func):\n        np.random.seed(42)\n        population = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        num_evaluations = self.pop_size\n\n        def mutate(target_idx):\n            indices = [idx for idx in range(self.pop_size) if idx != target_idx]\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            dynamic_F = self.F * (1.0 - (num_evaluations / self.budget)) * np.exp(-num_evaluations / self.budget)\n            scaling_parameter = 1.2  # Added line: Introduce a scaling parameter for dynamic_F\n            dynamic_F *= (self.temp / 1000.0) * scaling_parameter * (1 + 0.1 * (num_evaluations / self.budget))\n            mutant = population[a] + dynamic_F * (population[b] - population[c])\n            mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n            return mutant\n\n        while num_evaluations < self.budget:\n            self.CR = 0.9 - (0.3 * (num_evaluations / self.budget)) + 0.1 * np.tanh(num_evaluations / self.budget)\n            self.CR = 0.5 + 0.4 * (1 - 1 / (1 + np.exp(-10 * (num_evaluations / self.budget - 0.5))))\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                mutant = mutate(i)\n                trial = np.where(np.random.rand(self.dim) < self.CR, mutant, 0.5 * (population[i] + best_solution))\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = trial\n\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                neighbor = population[i] + np.random.normal(0, 0.1 * self.temp / 1000, self.dim)\n                neighbor = np.clip(neighbor, self.lower_bound, self.upper_bound)\n                neighbor_fitness = func(neighbor)\n                num_evaluations += 1\n                delta_fit = neighbor_fitness - fitness[i]\n\n                if delta_fit < 0 or np.random.rand() < np.exp(-delta_fit / self.temp):\n                    population[i] = neighbor\n                    fitness[i] = neighbor_fitness\n                    if neighbor_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = neighbor\n\n            self.temp *= (self.cooling_rate - 0.01 * (num_evaluations / self.budget))\n            self.pop_size = max(4, int(8 * self.dim * (1 - num_evaluations / self.budget)))\n\n        return best_solution", "name": "HybridDESA", "description": "Fine-tune dynamic mutation factor by adding a scaling parameter to enhance the exploration-exploitation balance.", "configspace": "", "generation": 96, "fitness": 0.2649275138028814, "feedback": "The algorithm HybridDESA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.26 with standard deviation 0.19.", "error": "", "parent_id": "2497dcab-aa06-4556-960b-95ae0c5afa7f", "metadata": {"aucs": [0.5520151286328232, 0.5520151286328232, 0.5520151286328232, 0.5510047656016085, 0.5510047656016085, 0.5510047656016085, 0.5476583807229811, 0.5476583807229811, 0.5476583807229811, 0.268395478955201, 0.268395478955201, 0.268395478955201, 0.2634193901021942, 0.2634193901021942, 0.2634193901021942, 0.10346553076088016, 0.10346553076088016, 0.10346553076088016, 0.1515519550468989, 0.1515519550468989, 0.1515519550468989, 0.14678181250279976, 0.14678181250279976, 0.14678181250279976, 0.12373355016624776, 0.12373355016624776, 0.12373355016624776, 0.09674702556939996, 0.09674702556939996, 0.09674702556939996, 0.10519884593686801, 0.10519884593686801, 0.10519884593686801, 0.09311688477402225, 0.09311688477402225, 0.09311688477402225, 0.8666993448467974, 0.8666993448467974, 0.8666993448467974, 0.79672464332942, 0.79672464332942, 0.79672464332942, 0.8184483682897935, 0.8184483682897935, 0.8184483682897935, 0.363589338870568, 0.363589338870568, 0.363589338870568, 0.3474939492139223, 0.3474939492139223, 0.3474939492139223, 0.35737615392249755, 0.35737615392249755, 0.35737615392249755, 0.5496205848360931, 0.5496205848360931, 0.5496205848360931, 0.6884689254120929, 0.6884689254120929, 0.6884689254120929, 0.5970882059292344, 0.5970882059292344, 0.5970882059292344, 0.15531293228180076, 0.15531293228180076, 0.15531293228180076, 0.15328801707724438, 0.15328801707724438, 0.15328801707724438, 0.14400734392760584, 0.14400734392760584, 0.14400734392760584, 0.11167677783611074, 0.11167677783611074, 0.11167677783611074, 0.1520958110305548, 0.1520958110305548, 0.1520958110305548, 0.15259149302833508, 0.15259149302833508, 0.15259149302833508, 0.0992637447699275, 0.0992637447699275, 0.0992637447699275, 0.056532125211801953, 0.056532125211801953, 0.056532125211801953, 0.06899857837631518, 0.06899857837631518, 0.06899857837631518, 0.1705233779719254, 0.1705233779719254, 0.1705233779719254, 0.18122765057409373, 0.18122765057409373, 0.18122765057409373, 0.19742589207998074, 0.19742589207998074, 0.19742589207998074, 0.04614702212553634, 0.04614702212553634, 0.04614702212553634, 0.16474195820152293, 0.16474195820152293, 0.16474195820152293, 0.13850561930691785, 0.13850561930691785, 0.13850561930691785, 0.1488384292878614, 0.1488384292878614, 0.1488384292878614, 0.1515476692313159, 0.1515476692313159, 0.1515476692313159, 0.12527817952859954, 0.12527817952859954, 0.12527817952859954, 0.44838039443170163, 0.44838039443170163, 0.44838039443170163, 0.44562797279604205, 0.44562797279604205, 0.44562797279604205, 0.4421292631110517, 0.4421292631110517, 0.4421292631110517, 0.11271593283234849, 0.11271593283234849, 0.11271593283234849, 0.12740233035337512, 0.12740233035337512, 0.12740233035337512, 0.1294746036000608, 0.1294746036000608, 0.1294746036000608, 0.3008445031922834, 0.3008445031922834, 0.3008445031922834, 0.24210340295837807, 0.24210340295837807, 0.24210340295837807, 0.2004566692177394, 0.2004566692177394, 0.2004566692177394, 0.34419153599794006, 0.34419153599794006, 0.34419153599794006, 0.31333421156564023, 0.31333421156564023, 0.31333421156564023, 0.3443678818993039, 0.3443678818993039, 0.3443678818993039, 0.2791429589632154, 0.2791429589632154, 0.2791429589632154, 0.2680163619049436, 0.2680163619049436, 0.2680163619049436, 0.2750057700643813, 0.2750057700643813, 0.2750057700643813, 0.2273348686096358, 0.2273348686096358, 0.2273348686096358, 0.21348615072045352, 0.21348615072045352, 0.21348615072045352, 0.28460566580259705, 0.28460566580259705, 0.28460566580259705, 0.20342638365733878, 0.20342638365733878, 0.20342638365733878, 0.1869022362688748, 0.1869022362688748, 0.1869022362688748, 0.22453528582631643, 0.22453528582631643, 0.22453528582631643, 0.6795140233101997, 0.6795140233101997, 0.6795140233101997, 0.18351289684813732, 0.18351289684813732, 0.18351289684813732, 0.17666708926936447, 0.17666708926936447, 0.17666708926936447, 0.16087519240490833, 0.16087519240490833, 0.16087519240490833, 0.1628980410009232, 0.1628980410009232, 0.1628980410009232, 0.39697719992411995, 0.39697719992411995, 0.39697719992411995, 0.20386091317162003, 0.20386091317162003, 0.20386091317162003, 0.19878996513107305, 0.19878996513107305, 0.19878996513107305, 0.21259902049768975, 0.21259902049768975, 0.21259902049768975, 0.0970724970827479, 0.0970724970827479, 0.0970724970827479, 0.09179670938716333, 0.09179670938716333, 0.09179670938716333, 0.09013217673609775, 0.09013217673609775, 0.09013217673609775]}, "mutation_prompt": null}
{"id": "cf17ec36-d537-4562-a161-7c18cf3f67d0", "solution": "import numpy as np\n\nclass HybridDESA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.pop_size = 10 * dim\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.temp = 1000.0  # Initial temperature for simulated annealing\n        self.cooling_rate = 0.99  # Cooling rate for simulated annealing\n\n    def __call__(self, func):\n        np.random.seed(42)\n        population = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        num_evaluations = self.pop_size\n\n        def mutate(target_idx):\n            indices = [idx for idx in range(self.pop_size) if idx != target_idx]\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            dynamic_F = self.F * (1.0 - (num_evaluations / self.budget)) * np.exp(-num_evaluations / self.budget)\n            dynamic_F *= (self.temp / 1000.0) * (1 + 0.1 * (num_evaluations / self.budget))  # Modified line: Adaptive scaling to mutation factor\n            mutant = population[a] + dynamic_F * (population[b] - population[c])\n            mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n            return mutant\n\n        while num_evaluations < self.budget:\n            self.CR = 0.9 - (0.3 * (num_evaluations / self.budget)) + 0.1 * np.tanh(num_evaluations / self.budget)  # Modified line: Introduce learning rate update to crossover probability\n            self.CR = 0.5 + 0.4 * (1 - 1 / (1 + np.exp(-10 * (num_evaluations / self.budget - 0.5))))  # Changed line: Use sigmoid function for crossover probability\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                mutant = mutate(i)\n                trial = np.where(np.random.rand(self.dim) < self.CR, mutant, 0.5 * (population[i] + best_solution))\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = trial\n\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                neighbor = population[i] + np.random.normal(0, 0.1 * self.temp / 1000, self.dim)\n                neighbor = np.clip(neighbor, self.lower_bound, self.upper_bound)\n                neighbor_fitness = func(neighbor)\n                num_evaluations += 1\n                delta_fit = neighbor_fitness - fitness[i]\n\n                if delta_fit < 0 or np.random.rand() < np.exp(-delta_fit / self.temp):\n                    population[i] = neighbor\n                    fitness[i] = neighbor_fitness\n                    if neighbor_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = neighbor\n\n            self.temp *= (self.cooling_rate - 0.01 * (num_evaluations / self.budget))\n            self.pop_size = max(4, int(8 * self.dim * (1 - num_evaluations / self.budget)))\n\n        return best_solution", "name": "HybridDESA", "description": "Adjust the crossover probability using a sigmoid function to improve the balance between exploration and exploitation.", "configspace": "", "generation": 95, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "2497dcab-aa06-4556-960b-95ae0c5afa7f", "metadata": {"aucs": [0.5516461231386738, 0.5516461231386738, 0.5516461231386738, 0.5712851158603225, 0.5712851158603225, 0.5712851158603225, 0.5598342381646224, 0.5598342381646224, 0.5598342381646224, 0.3031264035508785, 0.3031264035508785, 0.3031264035508785, 0.2548268041511147, 0.2548268041511147, 0.2548268041511147, 0.29706132529527063, 0.29706132529527063, 0.29706132529527063, 0.12354463999008602, 0.12354463999008602, 0.12354463999008602, 0.15568658338627261, 0.15568658338627261, 0.15568658338627261, 0.1528540288995589, 0.1528540288995589, 0.1528540288995589, 0.09575359644884307, 0.09575359644884307, 0.09575359644884307, 0.08692418286380432, 0.08692418286380432, 0.08692418286380432, 0.11530078923902187, 0.11530078923902187, 0.11530078923902187, 0.8022630886589334, 0.8022630886589334, 0.8022630886589334, 0.8811363188008171, 0.8811363188008171, 0.8811363188008171, 0.8273191922525636, 0.8273191922525636, 0.8273191922525636, 0.3593262483615912, 0.3593262483615912, 0.3593262483615912, 0.3649047028381144, 0.3649047028381144, 0.3649047028381144, 0.36546267207638183, 0.36546267207638183, 0.36546267207638183, 0.7084362053186021, 0.7084362053186021, 0.7084362053186021, 0.7304204590936285, 0.7304204590936285, 0.7304204590936285, 0.28036499011471316, 0.28036499011471316, 0.28036499011471316, 0.15975135854870315, 0.15975135854870315, 0.15975135854870315, 0.15456222268164344, 0.15456222268164344, 0.15456222268164344, 0.15872853362900674, 0.15872853362900674, 0.15872853362900674, 0.17879489870799992, 0.17879489870799992, 0.17879489870799992, 0.1855315516942796, 0.1855315516942796, 0.1855315516942796, 0.18695640943697922, 0.18695640943697922, 0.18695640943697922, 0.07618665615034226, 0.07618665615034226, 0.07618665615034226, 0.12334802093657893, 0.12334802093657893, 0.12334802093657893, 0.11514406258605869, 0.11514406258605869, 0.11514406258605869, 0.15512063047980917, 0.15512063047980917, 0.15512063047980917, 0.16013286404402716, 0.16013286404402716, 0.16013286404402716, 0.1760909108810882, 0.1760909108810882, 0.1760909108810882, 0.16002766883464736, 0.16002766883464736, 0.16002766883464736, 0.07487974298653044, 0.07487974298653044, 0.07487974298653044, 0.1431017336741235, 0.1431017336741235, 0.1431017336741235, 0.15957486065121262, 0.15957486065121262, 0.15957486065121262, 0.17516284876553823, 0.17516284876553823, 0.17516284876553823, 0.13197102009488193, 0.13197102009488193, 0.13197102009488193, 0.42888360366529565, 0.42888360366529565, 0.42888360366529565, 0.4634963923826697, 0.4634963923826697, 0.4634963923826697, 0.44387690342905894, 0.44387690342905894, 0.44387690342905894, 0.11527106206213888, 0.11527106206213888, 0.11527106206213888, 0.15387651432527505, 0.15387651432527505, 0.15387651432527505, 0.26595216810902134, 0.26595216810902134, 0.26595216810902134, 0.195930126323411, 0.195930126323411, 0.195930126323411, 0.28330657896441747, 0.28330657896441747, 0.28330657896441747, 0.32413092742925986, 0.32413092742925986, 0.32413092742925986, 0.33116996082825056, 0.33116996082825056, 0.33116996082825056, 0.3301724571596204, 0.3301724571596204, 0.3301724571596204, 0.3445104815944636, 0.3445104815944636, 0.3445104815944636, 0.28208867357220835, 0.28208867357220835, 0.28208867357220835, 0.26003034453839824, 0.26003034453839824, 0.26003034453839824, 0.2833607058257881, 0.2833607058257881, 0.2833607058257881, 0.2316068917958216, 0.2316068917958216, 0.2316068917958216, 0.232285733485474, 0.232285733485474, 0.232285733485474, 0.23256467003010584, 0.23256467003010584, 0.23256467003010584, 0.22405421388832314, 0.22405421388832314, 0.22405421388832314, 0.17431301104828978, 0.17431301104828978, 0.17431301104828978, 0.22584976644035726, 0.22584976644035726, 0.22584976644035726, 0.7445290105706044, 0.7445290105706044, 0.7445290105706044, 0.6239130959856514, 0.6239130959856514, 0.6239130959856514, 0.15831868215054024, 0.15831868215054024, 0.15831868215054024, 0.16556867715249446, 0.16556867715249446, 0.16556867715249446, 0.20294553004295235, 0.20294553004295235, 0.20294553004295235, 0.20400827620232098, 0.20400827620232098, 0.20400827620232098, 0.20566723046848978, 0.20566723046848978, 0.20566723046848978, 0.20675736842872883, 0.20675736842872883, 0.20675736842872883, 0.19433376189521545, 0.19433376189521545, 0.19433376189521545, 0.09496267105103906, 0.09496267105103906, 0.09496267105103906, 0.09508326687554014, 0.09508326687554014, 0.09508326687554014, 0.09234615361936283, 0.09234615361936283, 0.09234615361936283]}, "mutation_prompt": null}
{"id": "8e6330c9-5651-46d4-b2c0-7015aa264c97", "solution": "import numpy as np\n\nclass HybridDESA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.pop_size = 10 * dim\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.temp = 1000.0  # Initial temperature for simulated annealing\n        self.cooling_rate = 0.99  # Cooling rate for simulated annealing\n\n    def __call__(self, func):\n        np.random.seed(42)\n        population = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        num_evaluations = self.pop_size\n\n        def mutate(target_idx):\n            indices = [idx for idx in range(self.pop_size) if idx != target_idx]\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            # Modified line: Tweak dynamic_F using exponential decay for improved convergence\n            dynamic_F = self.F * np.exp(-5 * (num_evaluations / self.budget)) * (1 + 0.1 * (num_evaluations / self.budget))\n            mutant = population[a] + dynamic_F * (population[b] - population[c])\n            mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n            return mutant\n\n        while num_evaluations < self.budget:\n            self.CR = 0.9 - (0.3 * (num_evaluations / self.budget)) + 0.1 * np.tanh(num_evaluations / self.budget)\n            self.CR = 0.5 + 0.4 * (1 - 1 / (1 + np.exp(-10 * (num_evaluations / self.budget - 0.5))))\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                mutant = mutate(i)\n                trial = np.where(np.random.rand(self.dim) < self.CR, mutant, 0.5 * (population[i] + best_solution))\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = trial\n\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                neighbor = population[i] + np.random.normal(0, 0.1 * self.temp / 1000, self.dim)\n                neighbor = np.clip(neighbor, self.lower_bound, self.upper_bound)\n                neighbor_fitness = func(neighbor)\n                num_evaluations += 1\n                delta_fit = neighbor_fitness - fitness[i]\n\n                if delta_fit < 0 or np.random.rand() < np.exp(-delta_fit / self.temp):\n                    population[i] = neighbor\n                    fitness[i] = neighbor_fitness\n                    if neighbor_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = neighbor\n\n            self.temp *= (self.cooling_rate - 0.01 * (num_evaluations / self.budget))\n            self.pop_size = max(4, int(8 * self.dim * (1 - num_evaluations / self.budget)))\n\n        return best_solution", "name": "HybridDESA", "description": "Fine-tune dynamic mutation factor scaling using exponential decay for better convergence.", "configspace": "", "generation": 98, "fitness": 0.27116400610587654, "feedback": "The algorithm HybridDESA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.27 with standard deviation 0.18.", "error": "", "parent_id": "2497dcab-aa06-4556-960b-95ae0c5afa7f", "metadata": {"aucs": [0.5561930895158871, 0.5561930895158871, 0.5561930895158871, 0.5510339448802528, 0.5510339448802528, 0.5510339448802528, 0.5666915095690426, 0.5666915095690426, 0.5666915095690426, 0.25158586518026416, 0.25158586518026416, 0.25158586518026416, 0.24850428229000077, 0.24850428229000077, 0.24850428229000077, 0.29420412692542186, 0.29420412692542186, 0.29420412692542186, 0.2695335452606943, 0.2695335452606943, 0.2695335452606943, 0.1329129764761161, 0.1329129764761161, 0.1329129764761161, 0.13892920319589086, 0.13892920319589086, 0.13892920319589086, 0.09491779361816932, 0.09491779361816932, 0.09491779361816932, 0.09752697059174809, 0.09752697059174809, 0.09752697059174809, 0.09904689749808226, 0.09904689749808226, 0.09904689749808226, 0.7728881234590513, 0.7728881234590513, 0.7728881234590513, 0.7028520441043693, 0.7028520441043693, 0.7028520441043693, 0.8167500362302358, 0.8167500362302358, 0.8167500362302358, 0.3374841781156058, 0.3374841781156058, 0.3374841781156058, 0.3547095392587354, 0.3547095392587354, 0.3547095392587354, 0.36325392026363856, 0.36325392026363856, 0.36325392026363856, 0.2550050609314528, 0.2550050609314528, 0.2550050609314528, 0.7194885586564801, 0.7194885586564801, 0.7194885586564801, 0.2091866592003535, 0.2091866592003535, 0.2091866592003535, 0.13120007484201834, 0.13120007484201834, 0.13120007484201834, 0.15723584109518562, 0.15723584109518562, 0.15723584109518562, 0.18063313448475016, 0.18063313448475016, 0.18063313448475016, 0.16023920995236351, 0.16023920995236351, 0.16023920995236351, 0.1742650772182387, 0.1742650772182387, 0.1742650772182387, 0.17834155244385452, 0.17834155244385452, 0.17834155244385452, 0.10114180878059797, 0.10114180878059797, 0.10114180878059797, 0.1466972741992848, 0.1466972741992848, 0.1466972741992848, 0.127691521140453, 0.127691521140453, 0.127691521140453, 0.23147879301560603, 0.23147879301560603, 0.23147879301560603, 0.1305738007522177, 0.1305738007522177, 0.1305738007522177, 0.13360472525424572, 0.13360472525424572, 0.13360472525424572, 0.15354630266944447, 0.15354630266944447, 0.15354630266944447, 0.13113192504437143, 0.13113192504437143, 0.13113192504437143, 0.12020540698698257, 0.12020540698698257, 0.12020540698698257, 0.15142952328643644, 0.15142952328643644, 0.15142952328643644, 0.1535987892022853, 0.1535987892022853, 0.1535987892022853, 0.1728818310977258, 0.1728818310977258, 0.1728818310977258, 0.4686141952702807, 0.4686141952702807, 0.4686141952702807, 0.45976978442330674, 0.45976978442330674, 0.45976978442330674, 0.44733459531293285, 0.44733459531293285, 0.44733459531293285, 0.1301965856164199, 0.1301965856164199, 0.1301965856164199, 0.10310177735823023, 0.10310177735823023, 0.10310177735823023, 0.15512563902872967, 0.15512563902872967, 0.15512563902872967, 0.19620499455835483, 0.19620499455835483, 0.19620499455835483, 0.27576877218806417, 0.27576877218806417, 0.27576877218806417, 0.21694912343578843, 0.21694912343578843, 0.21694912343578843, 0.3465259340229938, 0.3465259340229938, 0.3465259340229938, 0.34410458404213207, 0.34410458404213207, 0.34410458404213207, 0.3432373096058865, 0.3432373096058865, 0.3432373096058865, 0.28416147909253897, 0.28416147909253897, 0.28416147909253897, 0.2220688380520115, 0.2220688380520115, 0.2220688380520115, 0.27658235955812494, 0.27658235955812494, 0.27658235955812494, 0.2788935987164648, 0.2788935987164648, 0.2788935987164648, 0.23681845025383685, 0.23681845025383685, 0.23681845025383685, 0.21769020836663222, 0.21769020836663222, 0.21769020836663222, 0.23157702266827784, 0.23157702266827784, 0.23157702266827784, 0.19297077257579687, 0.19297077257579687, 0.19297077257579687, 0.17810811665484294, 0.17810811665484294, 0.17810811665484294, 0.18837325572988317, 0.18837325572988317, 0.18837325572988317, 0.6473973293731876, 0.6473973293731876, 0.6473973293731876, 0.7182865000761229, 0.7182865000761229, 0.7182865000761229, 0.16628796509174826, 0.16628796509174826, 0.16628796509174826, 0.20295979940530373, 0.20295979940530373, 0.20295979940530373, 0.5334983149594539, 0.5334983149594539, 0.5334983149594539, 0.19567701240606317, 0.19567701240606317, 0.19567701240606317, 0.20120313663254819, 0.20120313663254819, 0.20120313663254819, 0.19910473217812974, 0.19910473217812974, 0.19910473217812974, 0.09948726471097247, 0.09948726471097247, 0.09948726471097247, 0.10030485137914968, 0.10030485137914968, 0.10030485137914968, 0.0968292201913451, 0.0968292201913451, 0.0968292201913451]}, "mutation_prompt": null}
{"id": "ea2d19a3-d745-448e-a934-b9c4b3df0f0e", "solution": "import numpy as np\n\nclass HybridDESA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.pop_size = 10 * dim\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.temp = 1000.0  # Initial temperature for simulated annealing\n        self.cooling_rate = 0.99  # Cooling rate for simulated annealing\n\n    def __call__(self, func):\n        np.random.seed(42)\n        population = np.random.uniform(self.lower_bound, self.upper_bound, (self.pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        num_evaluations = self.pop_size\n\n        def mutate(target_idx):\n            indices = [idx for idx in range(self.pop_size) if idx != target_idx]\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            dynamic_F = self.F * (1.0 - (num_evaluations / self.budget)) * np.exp(-num_evaluations / self.budget)\n            dynamic_F *= (self.temp / 1000.0) * (1 + 0.1 * (num_evaluations / self.budget))  # Adaptive scaling to mutation factor\n            mutant = population[a] + dynamic_F * (population[b] - population[c])\n            mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n            return mutant\n\n        while num_evaluations < self.budget:\n            self.CR = 0.9 - (0.3 * (num_evaluations / self.budget)) + 0.1 * np.tanh(num_evaluations / self.budget)\n            self.CR = 0.5 + 0.4 * (1 - 1 / (1 + np.exp(-10 * (num_evaluations / self.budget - 0.5))))\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                mutant = mutate(i)\n                trial = np.where(np.random.rand(self.dim) < self.CR, mutant, 0.5 * (population[i] + best_solution))\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = trial\n\n            for i in range(self.pop_size):\n                if num_evaluations >= self.budget:\n                    break\n                neighbor = population[i] + np.random.normal(0, 0.1 * self.temp / 1000, self.dim)\n                neighbor = np.clip(neighbor, self.lower_bound, self.upper_bound)\n                neighbor_fitness = func(neighbor)\n                num_evaluations += 1\n                delta_fit = neighbor_fitness - fitness[i]\n\n                if delta_fit < 0 or np.random.rand() < np.exp(-delta_fit / self.temp):\n                    population[i] = neighbor\n                    fitness[i] = neighbor_fitness\n                    if neighbor_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = neighbor\n\n            self.temp *= (self.cooling_rate - 0.01 * (num_evaluations / self.budget))\n            self.pop_size = max(4, int(8 * self.dim * (1 - num_evaluations / self.budget)))\n\n        return best_solution", "name": "HybridDESA", "description": "Introduce a temperature-dependent exponential function to scale the mutation factor for better adaptation.", "configspace": "", "generation": 99, "fitness": 0.2788570640920536, "feedback": "The algorithm HybridDESA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28 with standard deviation 0.19.", "error": "", "parent_id": "2497dcab-aa06-4556-960b-95ae0c5afa7f", "metadata": {"aucs": [0.5516461231386738, 0.5516461231386738, 0.5516461231386738, 0.5712851158603225, 0.5712851158603225, 0.5712851158603225, 0.5598342381646224, 0.5598342381646224, 0.5598342381646224, 0.3031264035508785, 0.3031264035508785, 0.3031264035508785, 0.2548268041511147, 0.2548268041511147, 0.2548268041511147, 0.29706132529527063, 0.29706132529527063, 0.29706132529527063, 0.12354463999008602, 0.12354463999008602, 0.12354463999008602, 0.15568658338627261, 0.15568658338627261, 0.15568658338627261, 0.1528540288995589, 0.1528540288995589, 0.1528540288995589, 0.09575359644884307, 0.09575359644884307, 0.09575359644884307, 0.08692418286380432, 0.08692418286380432, 0.08692418286380432, 0.11530078923902187, 0.11530078923902187, 0.11530078923902187, 0.8022630886589334, 0.8022630886589334, 0.8022630886589334, 0.8811363188008171, 0.8811363188008171, 0.8811363188008171, 0.8273191922525636, 0.8273191922525636, 0.8273191922525636, 0.3593262483615912, 0.3593262483615912, 0.3593262483615912, 0.3649047028381144, 0.3649047028381144, 0.3649047028381144, 0.36546267207638183, 0.36546267207638183, 0.36546267207638183, 0.7084362053186021, 0.7084362053186021, 0.7084362053186021, 0.7304204590936285, 0.7304204590936285, 0.7304204590936285, 0.28036499011471316, 0.28036499011471316, 0.28036499011471316, 0.15975135854870315, 0.15975135854870315, 0.15975135854870315, 0.15456222268164344, 0.15456222268164344, 0.15456222268164344, 0.15872853362900674, 0.15872853362900674, 0.15872853362900674, 0.17879489870799992, 0.17879489870799992, 0.17879489870799992, 0.1855315516942796, 0.1855315516942796, 0.1855315516942796, 0.18695640943697922, 0.18695640943697922, 0.18695640943697922, 0.07618665615034226, 0.07618665615034226, 0.07618665615034226, 0.12334802093657893, 0.12334802093657893, 0.12334802093657893, 0.11514406258605869, 0.11514406258605869, 0.11514406258605869, 0.15512063047980917, 0.15512063047980917, 0.15512063047980917, 0.16013286404402716, 0.16013286404402716, 0.16013286404402716, 0.1760909108810882, 0.1760909108810882, 0.1760909108810882, 0.16002766883464736, 0.16002766883464736, 0.16002766883464736, 0.07487974298653044, 0.07487974298653044, 0.07487974298653044, 0.1431017336741235, 0.1431017336741235, 0.1431017336741235, 0.15957486065121262, 0.15957486065121262, 0.15957486065121262, 0.17516284876553823, 0.17516284876553823, 0.17516284876553823, 0.13197102009488193, 0.13197102009488193, 0.13197102009488193, 0.42888360366529565, 0.42888360366529565, 0.42888360366529565, 0.4634963923826697, 0.4634963923826697, 0.4634963923826697, 0.44387690342905894, 0.44387690342905894, 0.44387690342905894, 0.11527106206213888, 0.11527106206213888, 0.11527106206213888, 0.15387651432527505, 0.15387651432527505, 0.15387651432527505, 0.26595216810902134, 0.26595216810902134, 0.26595216810902134, 0.195930126323411, 0.195930126323411, 0.195930126323411, 0.28330657896441747, 0.28330657896441747, 0.28330657896441747, 0.32413092742925986, 0.32413092742925986, 0.32413092742925986, 0.33116996082825056, 0.33116996082825056, 0.33116996082825056, 0.3301724571596204, 0.3301724571596204, 0.3301724571596204, 0.3445104815944636, 0.3445104815944636, 0.3445104815944636, 0.28208867357220835, 0.28208867357220835, 0.28208867357220835, 0.26003034453839824, 0.26003034453839824, 0.26003034453839824, 0.2833607058257881, 0.2833607058257881, 0.2833607058257881, 0.2316068917958216, 0.2316068917958216, 0.2316068917958216, 0.232285733485474, 0.232285733485474, 0.232285733485474, 0.23256467003010584, 0.23256467003010584, 0.23256467003010584, 0.22405421388832314, 0.22405421388832314, 0.22405421388832314, 0.17431301104828978, 0.17431301104828978, 0.17431301104828978, 0.22584976644035726, 0.22584976644035726, 0.22584976644035726, 0.7445290105706044, 0.7445290105706044, 0.7445290105706044, 0.6239130959856514, 0.6239130959856514, 0.6239130959856514, 0.15831868215054024, 0.15831868215054024, 0.15831868215054024, 0.16556867715249446, 0.16556867715249446, 0.16556867715249446, 0.20294553004295235, 0.20294553004295235, 0.20294553004295235, 0.20400827620232098, 0.20400827620232098, 0.20400827620232098, 0.20566723046848978, 0.20566723046848978, 0.20566723046848978, 0.20675736842872883, 0.20675736842872883, 0.20675736842872883, 0.19433376189521545, 0.19433376189521545, 0.19433376189521545, 0.09496267105103906, 0.09496267105103906, 0.09496267105103906, 0.09508326687554014, 0.09508326687554014, 0.09508326687554014, 0.09234615361936283, 0.09234615361936283, 0.09234615361936283]}, "mutation_prompt": null}
