{"id": "d08d2b86-3573-449e-a4ff-a762f66abb09", "solution": "import numpy as np\n\nclass HybridOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = 20\n        self.mutation_factor = 0.8\n        self.crossover_probability = 0.9\n        self.population = None\n        self.fitness = None\n\n    def initialize_population(self):\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, \n                                            (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def differential_evolution(self, idx, func):\n        indices = [i for i in range(self.population_size) if i != idx]\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n        \n        cross_points = np.random.rand(self.dim) < self.crossover_probability\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        \n        trial = np.where(cross_points, mutant, self.population[idx])\n        trial_fitness = func(trial)\n        \n        if trial_fitness < self.fitness[idx]:\n            self.population[idx] = trial\n            self.fitness[idx] = trial_fitness\n\n    def adaptive_neighborhood_search(self, idx, func):\n        step_size = (self.upper_bound - self.lower_bound) * 0.1\n        for _ in range(5):\n            neighbor = self.population[idx] + np.random.uniform(-step_size, step_size, self.dim)\n            neighbor = np.clip(neighbor, self.lower_bound, self.upper_bound)\n            neighbor_fitness = func(neighbor)\n            if neighbor_fitness < self.fitness[idx]:\n                self.population[idx] = neighbor\n                self.fitness[idx] = neighbor_fitness\n\n    def __call__(self, func):\n        self.initialize_population()\n        evaluations = 0\n        self.evaluate_population(func)\n        evaluations += self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                self.differential_evolution(i, func)\n                evaluations += 1\n\n                if evaluations >= self.budget:\n                    break\n\n                self.adaptive_neighborhood_search(i, func)\n                evaluations += 1\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "HybridOptimization", "description": "A hybrid metaheuristic combining Differential Evolution and Adaptive Neighborhood Search to efficiently explore and exploit the search space.", "configspace": "", "generation": 0, "fitness": 0.2221378236455003, "feedback": "The algorithm HybridOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.22 with standard deviation 0.19.", "error": "", "parent_id": null, "metadata": {"aucs": [0.4674137508144287, 0.44856306054602857, 0.47064598152219417, 0.4746616542748644, 0.4647766507889749, 0.4972526399059394, 0.4667777846915562, 0.449047627578696, 0.4703649301070135, 0.16634869740759584, 0.1338448469221012, 0.11627203247295803, 0.12154203951780496, 0.1658240349695752, 0.1727071631096707, 0.15291991571959374, 0.1404256564879688, 0.1995110172120258, 0.08700434146898639, 0.09658319961792772, 0.09270846884307904, 0.09778008811312944, 0.08385067516471334, 0.09856660222666591, 0.11260171357612836, 0.11216948419756079, 0.0967224742509547, 0.07532679297238787, 0.07474398463241583, 0.08017699968116465, 0.07940231431015399, 0.08717006581377096, 0.07458132361273162, 0.10030456698073253, 0.07598925711037485, 0.08389475431773519, 0.9450774863306657, 0.9266674956057266, 0.9305092838711406, 0.9290459376213397, 0.9869289035965235, 0.957575424652132, 0.9148966893054411, 0.9556466940550921, 0.9830423903482322, 0.23438243477929677, 0.21159887450846038, 0.23450580091332573, 0.2262619004602553, 0.21231686572899222, 0.2456987687648613, 0.24601978955909654, 0.22097781129262573, 0.25583709849399217, 0.2686793918535537, 0.4280139980345248, 0.2733247628922678, 0.286842257234592, 0.26194212454056187, 0.283739551543004, 0.3099891346112761, 0.2836746720556256, 0.2780710629267422, 0.13867640629289657, 0.13253837104291777, 0.135398890979333, 0.13818580951894255, 0.1480918101525518, 0.13412856636265735, 0.15871011905480648, 0.1221000838738473, 0.14048854798747012, 0.13419508743284192, 0.1446209486441119, 0.14463425593305435, 0.1446470362011152, 0.13343438816438735, 0.1285332041874575, 0.12477620588683747, 0.14592332081470105, 0.14084202983840532, 0.06326228526031097, 0.028294956775866575, 0.045046647387326444, 0.0629577922152057, 0.05204374216179908, 0.050290792262433937, 0.0665868708759243, 0.020276422954836337, 0.0580761870418075, 0.19397627150594632, 0.16305383412003016, 0.16170299409905908, 0.14482943583238972, 0.11649889449088646, 0.20675417494147275, 0.2064513927503131, 0.18802140720898253, 0.20166874853287287, 9.999999999998899e-05, 0.008774625153221471, 0.005278076717364133, 0.0017689218041976318, 0.008921475707072024, 0.0020261312193975423, 9.999999999998899e-05, 9.999999999998899e-05, 0.007193018774835247, 0.1110316239298379, 0.09604647701933644, 0.10212850744811786, 0.08055035034915714, 0.11062418965369158, 0.1039455104719671, 0.12082827348874559, 0.10491145408581948, 0.10411364299636972, 0.41166133614418066, 0.4134234658432079, 0.4362452556316032, 0.44303141767103493, 0.3970482725667903, 0.3865620147515193, 0.396193727060302, 0.4214217276076897, 0.4045997372364062, 0.10143596295344381, 0.08692249235973726, 0.07700139663885841, 0.08882282930235874, 0.08061453628315796, 0.09858894369001703, 0.10426000502873412, 0.09778626884526054, 0.08724432900200607, 0.16944935725098043, 0.18396378572985117, 0.20557196330862126, 0.1775311958640967, 0.15258643998898191, 0.16851102245767047, 0.13966855662257427, 0.15266365848790797, 0.16751394707168255, 0.24607351285298806, 0.243295286497202, 0.2266538181112081, 0.2503427924643784, 0.2389658261332699, 0.22417112096043712, 0.25828840463927183, 0.2563137428621568, 0.24928826279762628, 0.18669871692334605, 0.1714042840541926, 0.18500202819598122, 0.19141517633128413, 0.17918188559211756, 0.1699824598841464, 0.18369311791601473, 0.19431342716398725, 0.1795559826540839, 0.2085382802299185, 0.18251775464623565, 0.2120733304978959, 0.19998615569171785, 0.22283037731167155, 0.20099633315347232, 0.21511278995250327, 0.19199221690464052, 0.180522120992065, 0.18159046490642916, 0.17966158014206213, 0.19781060145634932, 0.17588254196190956, 0.1909297493634743, 0.17619425984813575, 0.17786823009740693, 0.17583532356972942, 0.18054184085860148, 0.5187745724778281, 0.2946213686892851, 0.396784321152293, 0.5469744668940086, 0.18596713585091174, 0.33359079385870116, 0.16731390368300914, 0.3245791300659775, 0.40362366260094606, 0.374494356081682, 0.42777283537434374, 0.27470318208513245, 0.3931579392998268, 0.3229412328220088, 0.1983630339121072, 0.39325729586416136, 0.4983285156941907, 0.20425940746464144, 0.18547235703623022, 0.19150975868171305, 0.18300333109788536, 0.1721148432907177, 0.19053020134192733, 0.207975598518106, 0.18704422016125832, 0.20082534142020225, 0.19542347186781095, 0.07808237725283451, 0.07667161766331243, 0.07467644214354718, 0.08240799904396567, 0.08721165017292554, 0.0743760373550354, 0.072656143259613, 0.08353923175549116, 0.07499128035397118]}, "mutation_prompt": null}
{"id": "ce8a7d99-0615-4263-bdf5-d6bf2d71302e", "solution": "import numpy as np\n\nclass HybridOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = 20\n        self.mutation_factor = 0.8\n        self.crossover_probability = 0.9\n        self.population = None\n        self.fitness = None\n\n    def initialize_population(self):\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, \n                                            (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def differential_evolution(self, idx, func):\n        indices = [i for i in range(self.population_size) if i != idx]\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n        \n        cross_points = np.random.rand(self.dim) < self.crossover_probability\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        \n        trial = np.where(cross_points, mutant, self.population[idx])\n        trial_fitness = func(trial)\n        \n        if trial_fitness < self.fitness[idx]:\n            self.population[idx] = trial\n            self.fitness[idx] = trial_fitness\n\n    def adaptive_neighborhood_search(self, idx, func):\n        step_size = (self.upper_bound - self.lower_bound) * 0.1\n        for _ in range(5):\n            neighbor = self.population[idx] + np.random.uniform(-step_size, step_size, self.dim)\n            neighbor = np.clip(neighbor, self.lower_bound, self.upper_bound)\n            neighbor_fitness = func(neighbor)\n            if neighbor_fitness < self.fitness[idx]:\n                self.population[idx] = neighbor\n                self.fitness[idx] = neighbor_fitness\n\n    def __call__(self, func):\n        self.initialize_population()\n        evaluations = 0\n        self.evaluate_population(func)\n        evaluations += self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                self.differential_evolution(i, func)\n                evaluations += 1\n\n                if evaluations >= self.budget:\n                    break\n\n                self.adaptive_neighborhood_search(i, func)\n                evaluations += 1\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "HybridOptimization", "description": "A hybrid metaheuristic combining Differential Evolution and Adaptive Neighborhood Search to efficiently explore and exploit the search space.", "configspace": "", "generation": 1, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "d08d2b86-3573-449e-a4ff-a762f66abb09", "metadata": {"aucs": [0.4674137508144287, 0.44856306054602857, 0.47064598152219417, 0.4746616542748644, 0.4647766507889749, 0.4972526399059394, 0.4667777846915562, 0.449047627578696, 0.4703649301070135, 0.16634869740759584, 0.1338448469221012, 0.11627203247295803, 0.12154203951780496, 0.1658240349695752, 0.1727071631096707, 0.15291991571959374, 0.1404256564879688, 0.1995110172120258, 0.08700434146898639, 0.09658319961792772, 0.09270846884307904, 0.09778008811312944, 0.08385067516471334, 0.09856660222666591, 0.11260171357612836, 0.11216948419756079, 0.0967224742509547, 0.07532679297238787, 0.07474398463241583, 0.08017699968116465, 0.07940231431015399, 0.08717006581377096, 0.07458132361273162, 0.10030456698073253, 0.07598925711037485, 0.08389475431773519, 0.9450774863306657, 0.9266674956057266, 0.9305092838711406, 0.9290459376213397, 0.9869289035965235, 0.957575424652132, 0.9148966893054411, 0.9556466940550921, 0.9830423903482322, 0.23438243477929677, 0.21159887450846038, 0.23450580091332573, 0.2262619004602553, 0.21231686572899222, 0.2456987687648613, 0.24601978955909654, 0.22097781129262573, 0.25583709849399217, 0.2686793918535537, 0.4280139980345248, 0.2733247628922678, 0.286842257234592, 0.26194212454056187, 0.283739551543004, 0.3099891346112761, 0.2836746720556256, 0.2780710629267422, 0.13867640629289657, 0.13253837104291777, 0.135398890979333, 0.13818580951894255, 0.1480918101525518, 0.13412856636265735, 0.15871011905480648, 0.1221000838738473, 0.14048854798747012, 0.13419508743284192, 0.1446209486441119, 0.14463425593305435, 0.1446470362011152, 0.13343438816438735, 0.1285332041874575, 0.12477620588683747, 0.14592332081470105, 0.14084202983840532, 0.06326228526031097, 0.028294956775866575, 0.045046647387326444, 0.0629577922152057, 0.05204374216179908, 0.050290792262433937, 0.0665868708759243, 0.020276422954836337, 0.0580761870418075, 0.19397627150594632, 0.16305383412003016, 0.16170299409905908, 0.14482943583238972, 0.11649889449088646, 0.20675417494147275, 0.2064513927503131, 0.18802140720898253, 0.20166874853287287, 9.999999999998899e-05, 0.008774625153221471, 0.005278076717364133, 0.0017689218041976318, 0.008921475707072024, 0.0020261312193975423, 9.999999999998899e-05, 9.999999999998899e-05, 0.007193018774835247, 0.1110316239298379, 0.09604647701933644, 0.10212850744811786, 0.08055035034915714, 0.11062418965369158, 0.1039455104719671, 0.12082827348874559, 0.10491145408581948, 0.10411364299636972, 0.41166133614418066, 0.4134234658432079, 0.4362452556316032, 0.44303141767103493, 0.3970482725667903, 0.3865620147515193, 0.396193727060302, 0.4214217276076897, 0.4045997372364062, 0.10143596295344381, 0.08692249235973726, 0.07700139663885841, 0.08882282930235874, 0.08061453628315796, 0.09858894369001703, 0.10426000502873412, 0.09778626884526054, 0.08724432900200607, 0.16944935725098043, 0.18396378572985117, 0.20557196330862126, 0.1775311958640967, 0.15258643998898191, 0.16851102245767047, 0.13966855662257427, 0.15266365848790797, 0.16751394707168255, 0.24607351285298806, 0.243295286497202, 0.2266538181112081, 0.2503427924643784, 0.2389658261332699, 0.22417112096043712, 0.25828840463927183, 0.2563137428621568, 0.24928826279762628, 0.18669871692334605, 0.1714042840541926, 0.18500202819598122, 0.19141517633128413, 0.17918188559211756, 0.1699824598841464, 0.18369311791601473, 0.19431342716398725, 0.1795559826540839, 0.2085382802299185, 0.18251775464623565, 0.2120733304978959, 0.19998615569171785, 0.22283037731167155, 0.20099633315347232, 0.21511278995250327, 0.19199221690464052, 0.180522120992065, 0.18159046490642916, 0.17966158014206213, 0.19781060145634932, 0.17588254196190956, 0.1909297493634743, 0.17619425984813575, 0.17786823009740693, 0.17583532356972942, 0.18054184085860148, 0.5187745724778281, 0.2946213686892851, 0.396784321152293, 0.5469744668940086, 0.18596713585091174, 0.33359079385870116, 0.16731390368300914, 0.3245791300659775, 0.40362366260094606, 0.374494356081682, 0.42777283537434374, 0.27470318208513245, 0.3931579392998268, 0.3229412328220088, 0.1983630339121072, 0.39325729586416136, 0.4983285156941907, 0.20425940746464144, 0.18547235703623022, 0.19150975868171305, 0.18300333109788536, 0.1721148432907177, 0.19053020134192733, 0.207975598518106, 0.18704422016125832, 0.20082534142020225, 0.19542347186781095, 0.07808237725283451, 0.07667161766331243, 0.07467644214354718, 0.08240799904396567, 0.08721165017292554, 0.0743760373550354, 0.072656143259613, 0.08353923175549116, 0.07499128035397118]}, "mutation_prompt": null}
{"id": "f72c13fc-8994-42f9-922c-d29bffc4b40b", "solution": "import numpy as np\n\nclass HybridOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = 20\n        self.mutation_factor = 0.8\n        self.crossover_probability = 0.9\n        self.population = None\n        self.fitness = None\n\n    def initialize_population(self):\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, \n                                            (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def differential_evolution(self, idx, func):\n        indices = [i for i in range(self.population_size) if i != idx]\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n        \n        cross_points = np.random.rand(self.dim) < self.crossover_probability\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        \n        trial = np.where(cross_points, mutant, self.population[idx])\n        trial_fitness = func(trial)\n        \n        if trial_fitness < self.fitness[idx]:\n            self.population[idx] = trial\n            self.fitness[idx] = trial_fitness\n\n    def adaptive_neighborhood_search(self, idx, func):\n        step_size = (self.upper_bound - self.lower_bound) * 0.1\n        for _ in range(5):\n            neighbor = self.population[idx] + np.random.uniform(-step_size, step_size, self.dim)\n            neighbor = np.clip(neighbor, self.lower_bound, self.upper_bound)\n            neighbor_fitness = func(neighbor)\n            if neighbor_fitness < self.fitness[idx]:\n                self.population[idx] = neighbor\n                self.fitness[idx] = neighbor_fitness\n\n    def __call__(self, func):\n        self.initialize_population()\n        evaluations = 0\n        self.evaluate_population(func)\n        evaluations += self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                self.differential_evolution(i, func)\n                evaluations += 1\n\n                if evaluations >= self.budget:\n                    break\n\n                self.adaptive_neighborhood_search(i, func)\n                evaluations += 1\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "HybridOptimization", "description": "A hybrid metaheuristic combining Differential Evolution and Adaptive Neighborhood Search to efficiently explore and exploit the search space.", "configspace": "", "generation": 1, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "d08d2b86-3573-449e-a4ff-a762f66abb09", "metadata": {"aucs": [0.4674137508144287, 0.44856306054602857, 0.47064598152219417, 0.4746616542748644, 0.4647766507889749, 0.4972526399059394, 0.4667777846915562, 0.449047627578696, 0.4703649301070135, 0.16634869740759584, 0.1338448469221012, 0.11627203247295803, 0.12154203951780496, 0.1658240349695752, 0.1727071631096707, 0.15291991571959374, 0.1404256564879688, 0.1995110172120258, 0.08700434146898639, 0.09658319961792772, 0.09270846884307904, 0.09778008811312944, 0.08385067516471334, 0.09856660222666591, 0.11260171357612836, 0.11216948419756079, 0.0967224742509547, 0.07532679297238787, 0.07474398463241583, 0.08017699968116465, 0.07940231431015399, 0.08717006581377096, 0.07458132361273162, 0.10030456698073253, 0.07598925711037485, 0.08389475431773519, 0.9450774863306657, 0.9266674956057266, 0.9305092838711406, 0.9290459376213397, 0.9869289035965235, 0.957575424652132, 0.9148966893054411, 0.9556466940550921, 0.9830423903482322, 0.23438243477929677, 0.21159887450846038, 0.23450580091332573, 0.2262619004602553, 0.21231686572899222, 0.2456987687648613, 0.24601978955909654, 0.22097781129262573, 0.25583709849399217, 0.2686793918535537, 0.4280139980345248, 0.2733247628922678, 0.286842257234592, 0.26194212454056187, 0.283739551543004, 0.3099891346112761, 0.2836746720556256, 0.2780710629267422, 0.13867640629289657, 0.13253837104291777, 0.135398890979333, 0.13818580951894255, 0.1480918101525518, 0.13412856636265735, 0.15871011905480648, 0.1221000838738473, 0.14048854798747012, 0.13419508743284192, 0.1446209486441119, 0.14463425593305435, 0.1446470362011152, 0.13343438816438735, 0.1285332041874575, 0.12477620588683747, 0.14592332081470105, 0.14084202983840532, 0.06326228526031097, 0.028294956775866575, 0.045046647387326444, 0.0629577922152057, 0.05204374216179908, 0.050290792262433937, 0.0665868708759243, 0.020276422954836337, 0.0580761870418075, 0.19397627150594632, 0.16305383412003016, 0.16170299409905908, 0.14482943583238972, 0.11649889449088646, 0.20675417494147275, 0.2064513927503131, 0.18802140720898253, 0.20166874853287287, 9.999999999998899e-05, 0.008774625153221471, 0.005278076717364133, 0.0017689218041976318, 0.008921475707072024, 0.0020261312193975423, 9.999999999998899e-05, 9.999999999998899e-05, 0.007193018774835247, 0.1110316239298379, 0.09604647701933644, 0.10212850744811786, 0.08055035034915714, 0.11062418965369158, 0.1039455104719671, 0.12082827348874559, 0.10491145408581948, 0.10411364299636972, 0.41166133614418066, 0.4134234658432079, 0.4362452556316032, 0.44303141767103493, 0.3970482725667903, 0.3865620147515193, 0.396193727060302, 0.4214217276076897, 0.4045997372364062, 0.10143596295344381, 0.08692249235973726, 0.07700139663885841, 0.08882282930235874, 0.08061453628315796, 0.09858894369001703, 0.10426000502873412, 0.09778626884526054, 0.08724432900200607, 0.16944935725098043, 0.18396378572985117, 0.20557196330862126, 0.1775311958640967, 0.15258643998898191, 0.16851102245767047, 0.13966855662257427, 0.15266365848790797, 0.16751394707168255, 0.24607351285298806, 0.243295286497202, 0.2266538181112081, 0.2503427924643784, 0.2389658261332699, 0.22417112096043712, 0.25828840463927183, 0.2563137428621568, 0.24928826279762628, 0.18669871692334605, 0.1714042840541926, 0.18500202819598122, 0.19141517633128413, 0.17918188559211756, 0.1699824598841464, 0.18369311791601473, 0.19431342716398725, 0.1795559826540839, 0.2085382802299185, 0.18251775464623565, 0.2120733304978959, 0.19998615569171785, 0.22283037731167155, 0.20099633315347232, 0.21511278995250327, 0.19199221690464052, 0.180522120992065, 0.18159046490642916, 0.17966158014206213, 0.19781060145634932, 0.17588254196190956, 0.1909297493634743, 0.17619425984813575, 0.17786823009740693, 0.17583532356972942, 0.18054184085860148, 0.5187745724778281, 0.2946213686892851, 0.396784321152293, 0.5469744668940086, 0.18596713585091174, 0.33359079385870116, 0.16731390368300914, 0.3245791300659775, 0.40362366260094606, 0.374494356081682, 0.42777283537434374, 0.27470318208513245, 0.3931579392998268, 0.3229412328220088, 0.1983630339121072, 0.39325729586416136, 0.4983285156941907, 0.20425940746464144, 0.18547235703623022, 0.19150975868171305, 0.18300333109788536, 0.1721148432907177, 0.19053020134192733, 0.207975598518106, 0.18704422016125832, 0.20082534142020225, 0.19542347186781095, 0.07808237725283451, 0.07667161766331243, 0.07467644214354718, 0.08240799904396567, 0.08721165017292554, 0.0743760373550354, 0.072656143259613, 0.08353923175549116, 0.07499128035397118]}, "mutation_prompt": null}
{"id": "375a9d2b-3281-4c21-9ff5-adb488656748", "solution": "import numpy as np\n\nclass DynamicHybridOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = 20\n        self.mutation_factor = 0.8\n        self.crossover_probability = 0.9\n        self.dynamic_factor = 0.5\n        self.population = None\n        self.fitness = None\n\n    def initialize_population(self):\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, \n                                            (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def differential_evolution(self, idx, func):\n        indices = [i for i in range(self.population_size) if i != idx]\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n        \n        cross_points = np.random.rand(self.dim) < self.crossover_probability\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        \n        trial = np.where(cross_points, mutant, self.population[idx])\n        trial_fitness = func(trial)\n        \n        if trial_fitness < self.fitness[idx]:\n            self.population[idx] = trial\n            self.fitness[idx] = trial_fitness\n\n    def adaptive_neighborhood_search(self, idx, func):\n        step_size = (self.upper_bound - self.lower_bound) * self.dynamic_factor\n        for _ in range(5):\n            neighbor = self.population[idx] + np.random.uniform(-step_size, step_size, self.dim)\n            neighbor = np.clip(neighbor, self.lower_bound, self.upper_bound)\n            neighbor_fitness = func(neighbor)\n            if neighbor_fitness < self.fitness[idx]:\n                self.population[idx] = neighbor\n                self.fitness[idx] = neighbor_fitness\n\n    def update_dynamic_factor(self, evaluations):\n        self.dynamic_factor = 0.5 * (1 - evaluations / self.budget)\n\n    def __call__(self, func):\n        self.initialize_population()\n        evaluations = 0\n        self.evaluate_population(func)\n        evaluations += self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                self.differential_evolution(i, func)\n                evaluations += 1\n\n                if evaluations >= self.budget:\n                    break\n\n                self.adaptive_neighborhood_search(i, func)\n                evaluations += 1\n\n                self.update_dynamic_factor(evaluations)\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "DynamicHybridOptimization", "description": "A hybrid algorithm combining Differential Evolution and Adaptive Neighborhood Search with dynamic adaptation to balance exploration and exploitation.", "configspace": "", "generation": 3, "fitness": 0.18144899055275726, "feedback": "The algorithm DynamicHybridOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.18 with standard deviation 0.19.", "error": "", "parent_id": "d08d2b86-3573-449e-a4ff-a762f66abb09", "metadata": {"aucs": [0.3684035146740736, 0.3580961574956315, 0.4185374524096491, 0.40029918086810157, 0.4409269580597741, 0.4146752145442535, 0.43373161025905127, 0.4132859472885698, 0.39891285993142633, 0.1353740011843283, 0.10536190953401148, 0.14143180933837884, 0.09673769260594778, 0.15570257808702703, 0.13336788907686037, 0.13399207956578596, 0.1368847572805829, 0.1316069776019514, 0.07879866533599522, 0.07186142498392978, 0.07576177510246285, 0.06500794702743373, 0.0782284752289133, 0.08432220466617524, 0.06857567756652738, 0.07733422753356956, 0.08793723681553944, 0.06508069000200245, 0.07492591408553528, 0.0728861028577007, 0.06711574867830927, 0.0710894401031128, 0.05960202869673248, 0.05630131018120843, 0.06492749430022526, 0.06138231397554572, 0.9849713310034865, 0.9616386983556576, 0.9821689799415698, 0.975743528036085, 0.9754855381206797, 0.9767793227581896, 0.9729202021978066, 0.9577163126662303, 0.9956963986216911, 0.1500469011904153, 0.1653270688610009, 0.17265651399567872, 0.18588594847543605, 0.17889733458907808, 0.20384584898890745, 0.15480185501684007, 0.1602970796444304, 0.12686098293783366, 0.20799962888295243, 0.20579950591266194, 0.20626896747669599, 0.2002327308655436, 0.19811940359547997, 0.25513963589890076, 0.22929524630742248, 0.26400726558656196, 0.21974488068374842, 0.07067791398010759, 0.088448786997818, 0.12530389220183646, 0.09697865596007538, 0.11472204167176592, 0.08682837982627156, 0.1037700283608387, 0.10342166784569728, 0.10756438190393869, 0.01947819459971345, 0.04520194554435786, 0.01303061375455783, 0.11553766974115676, 0.09660955369156965, 0.07776582017371891, 0.05561429333689194, 0.10697650676429582, 0.09479320362448218, 0.012165701085332659, 0.016993472444727042, 0.004266518580249623, 0.012687023561946287, 0.024026794811108587, 0.007211494910537519, 0.06000524893141368, 0.012743376578981613, 0.014257779948145588, 0.16444704245353825, 0.1235882798561646, 0.12753512056677152, 0.16034514965173674, 0.09211976574570557, 0.09977642689089627, 0.1489798438762464, 0.15383616003737166, 0.13828646602795158, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.002519943438453298, 9.999999999998899e-05, 0.04760630572366531, 0.06625745995978727, 0.06294606951252024, 0.05746548171493726, 0.08502410286320339, 0.06846355927396874, 0.07713087317343637, 0.0849068982926805, 0.07407485820661552, 0.35131755848393975, 0.3512277502246395, 0.32588411612398993, 0.3426829787297061, 0.3632420654499836, 0.3109063828690126, 0.3063246262024669, 0.348756485982029, 0.33810634988329547, 0.064797466589253, 0.07136731401713314, 0.07409957536503864, 0.06781222269529608, 0.07546313139266059, 0.07586547146877942, 0.07063214614514202, 0.06796892285353073, 0.06278106678306117, 0.1323353991441638, 0.13013113550907252, 0.13406280501488854, 0.13181147857118758, 0.1391709152354733, 0.1380021432465819, 0.1520258111804067, 0.15199032672666912, 0.13949509321833498, 0.212938692979326, 0.1988165956781135, 0.18261070967560877, 0.24269951952882673, 0.2075911324799754, 0.20910356997062252, 0.22268870604981117, 0.2200671964780806, 0.20560022657058052, 0.16979951657817816, 0.14865418473892622, 0.15082398005054476, 0.15881881184677715, 0.13588950425165025, 0.14447133263548362, 0.17761891505845617, 0.17864484359320099, 0.15722178791728825, 0.1525227851104881, 0.17601691905678818, 0.1613296258120297, 0.1586779258439659, 0.17746440461557944, 0.16637650601625065, 0.16681190398420387, 0.16150074711189844, 0.18655191133619087, 0.1584583576170524, 0.17191601790381505, 0.16240516922508796, 0.1683926082619921, 0.1629112395840544, 0.16525640931623164, 0.156201699684413, 0.1699395609756128, 0.1555912652536121, 0.2754844172717975, 0.27332561798822186, 0.25752887716459816, 0.17665066469090196, 0.15162625098404803, 0.23738101558516111, 0.3099165099615987, 0.16288512241996866, 0.14653486454850007, 0.18837865681272292, 0.17780652567895283, 0.18239623533373306, 0.18738927793684246, 0.18812029512381667, 0.17057206123797697, 0.20614248613309305, 0.3273006712651111, 0.25231196665313216, 0.17651075876547007, 0.1940357640364605, 0.18778886971694353, 0.1891321496702486, 0.1886031669538505, 0.19006457425503076, 0.18026313590446386, 0.20089412434395804, 0.18918848141269495, 0.05354400608550991, 0.058822855372249405, 0.06316452799575722, 0.06109841966654772, 0.05833459070162095, 0.05213946035431505, 0.05427539528513481, 0.05843568556731005, 0.05428430896687553]}, "mutation_prompt": null}
{"id": "7703ffee-f13a-439f-ac29-e07a68b8c0ad", "solution": "import numpy as np\n\nclass EnhancedHybridOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = 30\n        self.mutation_factor = 0.7\n        self.crossover_probability = 0.85\n        self.learning_rate = 0.05\n        self.population = None\n        self.fitness = None\n\n    def initialize_population(self):\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, \n                                            (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def differential_evolution(self, idx, func):\n        indices = [i for i in range(self.population_size) if i != idx]\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n        \n        cross_points = np.random.rand(self.dim) < self.crossover_probability\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        \n        trial = np.where(cross_points, mutant, self.population[idx])\n        trial_fitness = func(trial)\n        \n        if trial_fitness < self.fitness[idx]:\n            self.population[idx] = trial\n            self.fitness[idx] = trial_fitness\n\n    def stochastic_gradient_descent(self, idx, func):\n        gradient = np.random.uniform(-1, 1, self.dim)\n        candidate = self.population[idx] - self.learning_rate * gradient\n        candidate = np.clip(candidate, self.lower_bound, self.upper_bound)\n        candidate_fitness = func(candidate)\n        if candidate_fitness < self.fitness[idx]:\n            self.population[idx] = candidate\n            self.fitness[idx] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population()\n        evaluations = 0\n        self.evaluate_population(func)\n        evaluations += self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                self.differential_evolution(i, func)\n                evaluations += 1\n\n                if evaluations >= self.budget:\n                    break\n\n                self.stochastic_gradient_descent(i, func)\n                evaluations += 1\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "EnhancedHybridOptimization", "description": "A hybrid metaheuristic combining Enhanced Differential Evolution with Stochastic Gradient Descent to efficiently navigate and exploit the search space.", "configspace": "", "generation": 4, "fitness": 0.31439345077402375, "feedback": "The algorithm EnhancedHybridOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.31 with standard deviation 0.22.", "error": "", "parent_id": "d08d2b86-3573-449e-a4ff-a762f66abb09", "metadata": {"aucs": [0.6931704255724962, 0.7398566314184243, 0.7363631708877643, 0.7013619291778275, 0.7573708011239838, 0.7450046322594615, 0.7480136279176042, 0.7240984800212156, 0.7508163152180503, 0.4593041609268519, 0.4318239996396773, 0.5046052709073789, 0.4894321432555032, 0.4557212535826579, 0.5109399003041877, 0.5068949800852436, 0.4724576980972849, 0.4819261059965778, 0.1472018684446017, 0.1430680314533609, 0.1633997467943682, 0.1402599982449385, 0.13807736818766148, 0.1312504586582498, 0.13644727554822134, 0.14021679734025927, 0.1387432350438026, 0.13024838013868067, 0.11347644285099101, 0.12295328165278296, 0.11360387870975752, 0.11282485819046562, 0.11583893211376217, 0.11869117448400757, 0.12862879822326834, 0.12362908797479666, 0.8898029386422015, 0.8970354985525315, 0.9312526100277113, 0.8845539903460342, 0.9382815684881814, 0.879335344321051, 0.8821865977660753, 0.8926960133904202, 0.9310566182060952, 0.3429061548217982, 0.325885766217167, 0.3342437714579124, 0.37210056136108405, 0.31996503718942504, 0.35238361197961354, 0.37545285566765196, 0.34489563108689647, 0.3452264813166208, 0.546862870329816, 0.5851349081163584, 0.5707142508178983, 0.6603756275240298, 0.6449425218254237, 0.7156868687518223, 0.5398368538790088, 0.6296487824730367, 0.6235568137284386, 0.18461889784571917, 0.2052445015500478, 0.35547321419530187, 0.24987283648091152, 0.20682237911689927, 0.19156364242479162, 0.22205300634812353, 0.13588091722944373, 0.24024359735035905, 0.20074007575110864, 0.2987547773290318, 0.20539544870880233, 0.22949723988123527, 0.23501865688774892, 0.28888833136166314, 0.24584893507300853, 0.2434732184402688, 0.22566625762284975, 0.20152866276482406, 0.1405922744828948, 0.1506271919083041, 0.1832472314831889, 0.17518230110234712, 0.18913049691936412, 0.1728723335276181, 0.17106770799304705, 0.19583121146148152, 0.2877205726891999, 0.32279605001573175, 0.286465167751503, 0.3505297784821718, 0.2627690098879092, 0.27495670386925086, 0.33000856901794573, 0.3152909474609441, 0.31736130984943023, 0.059064836318988845, 0.03590078243512085, 0.03252830158102693, 0.05040733864498437, 0.0831025212602875, 0.09060845418451713, 0.08722673328916608, 0.0705555615973088, 0.06839770405680079, 0.20012946169123125, 0.16796166821871739, 0.17355045156071158, 0.2003243173111945, 0.19127832310636184, 0.207805482289789, 0.18824367402707887, 0.20637505496158914, 0.19171682464378315, 0.533165015600636, 0.5470593570530393, 0.5035977646101049, 0.5380956916329129, 0.5446201036151472, 0.5248915345576753, 0.544877012301568, 0.506891811937749, 0.52349560945348, 0.11802367427264981, 0.13321722786188717, 0.1197628578582024, 0.1263108111385538, 0.1560368851685584, 0.1315710271845173, 0.14451371385310396, 0.1258796197041473, 0.11478106315546499, 0.17708154654921404, 0.16319703048416034, 0.18303515110987412, 0.21347899487049726, 0.17917703803687723, 0.17478285630506474, 0.2040009145921693, 0.17603979822678772, 0.24431101011168965, 0.27118587415424245, 0.29718783295501106, 0.2926072648554021, 0.27970734135629105, 0.2723073153007337, 0.2705705516306093, 0.3191426444446236, 0.34025643898264013, 0.29044262927678777, 0.19496860908313562, 0.225060695417428, 0.20113284317791857, 0.21581915895545567, 0.2178355236856726, 0.23789219155379193, 0.21944233421612025, 0.2075632685637152, 0.21759904005354636, 0.2159927735737145, 0.19939576696871852, 0.21223455107484956, 0.19944480903783357, 0.18088573048227186, 0.1969996698436165, 0.19027334408312513, 0.20358738583924396, 0.1979696719250713, 0.19091240166218337, 0.19357649559749956, 0.20776826721396435, 0.20793330494224527, 0.22083859459792088, 0.22756263671948662, 0.2302456408476664, 0.22903656234715153, 0.24480368958945653, 0.349722732685082, 0.5716595999517811, 0.16616495896007433, 0.7067696481919769, 0.5850270315558657, 0.5957836203850002, 0.15219634977442564, 0.5109192942295757, 0.4380936604662544, 0.5075885718270878, 0.31827612204371614, 0.3647973124407661, 0.6013636283701354, 0.1944937875040914, 0.6410292409864782, 0.5931368561490579, 0.5900244203697784, 0.7080507936118525, 0.19814729682394205, 0.18290467065448313, 0.21650156472901083, 0.1892427166135946, 0.1944891325075606, 0.2148404359807493, 0.2061098602159177, 0.1850431434460995, 0.19009920763320853, 0.09262341297226284, 0.08447777780323462, 0.08826989165039578, 0.08921347663378021, 0.07273955632388573, 0.07926942469608689, 0.08362425799940576, 0.07489777681438203, 0.07736352475964758]}, "mutation_prompt": null}
{"id": "7d0497b5-0da0-4b59-b6bf-e0ec8d2f34ae", "solution": "import numpy as np\n\nclass EnhancedHybridOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = 30\n        self.mutation_factor = 0.7\n        self.crossover_probability = 0.85\n        self.learning_rate = 0.05\n        self.population = None\n        self.fitness = None\n\n    def initialize_population(self):\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, \n                                            (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def differential_evolution(self, idx, func):\n        indices = [i for i in range(self.population_size) if i != idx]\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n        \n        cross_points = np.random.rand(self.dim) < self.crossover_probability\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        \n        trial = np.where(cross_points, mutant, self.population[idx])\n        trial_fitness = func(trial)\n        \n        if trial_fitness < self.fitness[idx]:\n            self.population[idx] = trial\n            self.fitness[idx] = trial_fitness\n\n    def stochastic_gradient_descent(self, idx, func):\n        gradient = np.random.uniform(-1, 1, self.dim)\n        candidate = self.population[idx] - self.learning_rate * gradient\n        candidate = np.clip(candidate, self.lower_bound, self.upper_bound)\n        candidate_fitness = func(candidate)\n        if candidate_fitness < self.fitness[idx]:\n            self.population[idx] = candidate\n            self.fitness[idx] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population()\n        evaluations = 0\n        self.evaluate_population(func)\n        evaluations += self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                self.differential_evolution(i, func)\n                evaluations += 1\n\n                if evaluations >= self.budget:\n                    break\n\n                self.stochastic_gradient_descent(i, func)\n                evaluations += 1\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "EnhancedHybridOptimization", "description": "A hybrid metaheuristic combining Enhanced Differential Evolution with Stochastic Gradient Descent to efficiently navigate and exploit the search space.", "configspace": "", "generation": 5, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "7703ffee-f13a-439f-ac29-e07a68b8c0ad", "metadata": {"aucs": [0.6931704255724962, 0.7398566314184243, 0.7363631708877643, 0.7013619291778275, 0.7573708011239838, 0.7450046322594615, 0.7480136279176042, 0.7240984800212156, 0.7508163152180503, 0.4593041609268519, 0.4318239996396773, 0.5046052709073789, 0.4894321432555032, 0.4557212535826579, 0.5109399003041877, 0.5068949800852436, 0.4724576980972849, 0.4819261059965778, 0.1472018684446017, 0.1430680314533609, 0.1633997467943682, 0.1402599982449385, 0.13807736818766148, 0.1312504586582498, 0.13644727554822134, 0.14021679734025927, 0.1387432350438026, 0.13024838013868067, 0.11347644285099101, 0.12295328165278296, 0.11360387870975752, 0.11282485819046562, 0.11583893211376217, 0.11869117448400757, 0.12862879822326834, 0.12362908797479666, 0.8898029386422015, 0.8970354985525315, 0.9312526100277113, 0.8845539903460342, 0.9382815684881814, 0.879335344321051, 0.8821865977660753, 0.8926960133904202, 0.9310566182060952, 0.3429061548217982, 0.325885766217167, 0.3342437714579124, 0.37210056136108405, 0.31996503718942504, 0.35238361197961354, 0.37545285566765196, 0.34489563108689647, 0.3452264813166208, 0.546862870329816, 0.5851349081163584, 0.5707142508178983, 0.6603756275240298, 0.6449425218254237, 0.7156868687518223, 0.5398368538790088, 0.6296487824730367, 0.6235568137284386, 0.18461889784571917, 0.2052445015500478, 0.35547321419530187, 0.24987283648091152, 0.20682237911689927, 0.19156364242479162, 0.22205300634812353, 0.13588091722944373, 0.24024359735035905, 0.20074007575110864, 0.2987547773290318, 0.20539544870880233, 0.22949723988123527, 0.23501865688774892, 0.28888833136166314, 0.24584893507300853, 0.2434732184402688, 0.22566625762284975, 0.20152866276482406, 0.1405922744828948, 0.1506271919083041, 0.1832472314831889, 0.17518230110234712, 0.18913049691936412, 0.1728723335276181, 0.17106770799304705, 0.19583121146148152, 0.2877205726891999, 0.32279605001573175, 0.286465167751503, 0.3505297784821718, 0.2627690098879092, 0.27495670386925086, 0.33000856901794573, 0.3152909474609441, 0.31736130984943023, 0.059064836318988845, 0.03590078243512085, 0.03252830158102693, 0.05040733864498437, 0.0831025212602875, 0.09060845418451713, 0.08722673328916608, 0.0705555615973088, 0.06839770405680079, 0.20012946169123125, 0.16796166821871739, 0.17355045156071158, 0.2003243173111945, 0.19127832310636184, 0.207805482289789, 0.18824367402707887, 0.20637505496158914, 0.19171682464378315, 0.533165015600636, 0.5470593570530393, 0.5035977646101049, 0.5380956916329129, 0.5446201036151472, 0.5248915345576753, 0.544877012301568, 0.506891811937749, 0.52349560945348, 0.11802367427264981, 0.13321722786188717, 0.1197628578582024, 0.1263108111385538, 0.1560368851685584, 0.1315710271845173, 0.14451371385310396, 0.1258796197041473, 0.11478106315546499, 0.17708154654921404, 0.16319703048416034, 0.18303515110987412, 0.21347899487049726, 0.17917703803687723, 0.17478285630506474, 0.2040009145921693, 0.17603979822678772, 0.24431101011168965, 0.27118587415424245, 0.29718783295501106, 0.2926072648554021, 0.27970734135629105, 0.2723073153007337, 0.2705705516306093, 0.3191426444446236, 0.34025643898264013, 0.29044262927678777, 0.19496860908313562, 0.225060695417428, 0.20113284317791857, 0.21581915895545567, 0.2178355236856726, 0.23789219155379193, 0.21944233421612025, 0.2075632685637152, 0.21759904005354636, 0.2159927735737145, 0.19939576696871852, 0.21223455107484956, 0.19944480903783357, 0.18088573048227186, 0.1969996698436165, 0.19027334408312513, 0.20358738583924396, 0.1979696719250713, 0.19091240166218337, 0.19357649559749956, 0.20776826721396435, 0.20793330494224527, 0.22083859459792088, 0.22756263671948662, 0.2302456408476664, 0.22903656234715153, 0.24480368958945653, 0.349722732685082, 0.5716595999517811, 0.16616495896007433, 0.7067696481919769, 0.5850270315558657, 0.5957836203850002, 0.15219634977442564, 0.5109192942295757, 0.4380936604662544, 0.5075885718270878, 0.31827612204371614, 0.3647973124407661, 0.6013636283701354, 0.1944937875040914, 0.6410292409864782, 0.5931368561490579, 0.5900244203697784, 0.7080507936118525, 0.19814729682394205, 0.18290467065448313, 0.21650156472901083, 0.1892427166135946, 0.1944891325075606, 0.2148404359807493, 0.2061098602159177, 0.1850431434460995, 0.19009920763320853, 0.09262341297226284, 0.08447777780323462, 0.08826989165039578, 0.08921347663378021, 0.07273955632388573, 0.07926942469608689, 0.08362425799940576, 0.07489777681438203, 0.07736352475964758]}, "mutation_prompt": null}
{"id": "6a904b7a-737d-497b-b2e1-0c5368eca0d0", "solution": "import numpy as np\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = 40\n        self.mutation_factor = 0.8\n        self.crossover_probability = 0.9\n        self.learning_rate = 0.1\n        self.population = None\n        self.fitness = None\n\n    def initialize_population(self):\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, \n                                            (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def adaptive_differential_evolution(self, idx, func):\n        indices = [i for i in range(self.population_size) if i != idx]\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        adaptive_factor = 0.5 + 0.5 * np.random.rand()\n        mutant = self.population[idx] + adaptive_factor * (mutant - self.population[idx])\n        mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n        \n        cross_points = np.random.rand(self.dim) < self.crossover_probability\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        \n        trial = np.where(cross_points, mutant, self.population[idx])\n        trial_fitness = func(trial)\n        \n        if trial_fitness < self.fitness[idx]:\n            self.population[idx] = trial\n            self.fitness[idx] = trial_fitness\n\n    def randomized_newton_update(self, idx, func):\n        gradient = np.random.uniform(-1, 1, self.dim)\n        hessian_approx = np.random.uniform(0.1, 0.5, self.dim)\n        candidate = self.population[idx] - self.learning_rate * gradient / (np.abs(hessian_approx) + 1e-9)\n        candidate = np.clip(candidate, self.lower_bound, self.upper_bound)\n        candidate_fitness = func(candidate)\n        if candidate_fitness < self.fitness[idx]:\n            self.population[idx] = candidate\n            self.fitness[idx] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population()\n        evaluations = 0\n        self.evaluate_population(func)\n        evaluations += self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                self.adaptive_differential_evolution(i, func)\n                evaluations += 1\n\n                if evaluations >= self.budget:\n                    break\n\n                self.randomized_newton_update(i, func)\n                evaluations += 1\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "AdaptiveHybridOptimizer", "description": "A novel hybrid optimization algorithm combining Adaptive Differential Evolution and Randomized Newton-like updates for efficient exploration and exploitation across diverse landscapes.", "configspace": "", "generation": 6, "fitness": 0.34111757565261597, "feedback": "The algorithm AdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.34 with standard deviation 0.23.", "error": "", "parent_id": "7703ffee-f13a-439f-ac29-e07a68b8c0ad", "metadata": {"aucs": [0.7810487762994811, 0.7737201709897309, 0.7984387123770895, 0.7474584068150549, 0.7893206249621149, 0.7810859249864255, 0.7651870047122677, 0.7637265331123596, 0.7544685530913278, 0.5745789060628225, 0.5568681199286232, 0.5923117409603715, 0.5867524787457852, 0.5588743250787239, 0.5727217353505709, 0.564497454703493, 0.5670209119249587, 0.5622127176723783, 0.10645007403955864, 0.10531591371851878, 0.10104588105069745, 0.12068071889648146, 0.104745313274058, 0.11475897315854577, 0.1036030526972892, 0.10418699385194452, 0.11564689102372794, 0.08915452135963042, 0.09036755158695209, 0.08605412146502023, 0.09209907981052001, 0.10215945806999516, 0.10112576268436513, 0.08113886941620818, 0.10085867441467866, 0.09448134176050316, 0.8854041122210992, 0.8450598443123811, 0.8045587166262524, 0.8475249501073782, 0.7906732554426446, 0.841626290225768, 0.8559539765904275, 0.8290424812601958, 0.8548272959571771, 0.3500139022371357, 0.3365419279069296, 0.3673658387342146, 0.38585778122735126, 0.3416543941999345, 0.3633605688963323, 0.31667506736193796, 0.3596771268188682, 0.37621268434668087, 0.7094126709237628, 0.6642717545892044, 0.7701150693052059, 0.7509132471346256, 0.7376084167694179, 0.7297780729549188, 0.7190871088720918, 0.687974617485412, 0.758968088817827, 0.1908801089794614, 0.1517127154599499, 0.1686397066901416, 0.17866888305762463, 0.242288603051677, 0.2638425564904663, 0.2229599553839462, 0.20076687819041095, 0.19593545634751364, 0.2343886605917752, 0.22531574876774763, 0.17625683222767297, 0.1955842840049694, 0.16973369324425436, 0.17910478209184455, 0.15326325686486075, 0.18099100509510058, 0.1509954393258326, 0.3615727583601135, 0.3235366556602163, 0.2975550934493706, 0.4052246037041095, 0.340788219764936, 0.31026755650887083, 0.3569595340176651, 0.36046757066857216, 0.35551196975396393, 0.4365316406164489, 0.5003887572451153, 0.48802112804753084, 0.454058276620417, 0.41123218678163553, 0.46426014387650116, 0.5316807812961412, 0.48454935010302613, 0.47725159673689754, 0.0996550641492755, 0.09744041393381808, 0.08758765713846406, 0.19648404038841327, 0.2028755357362937, 0.18506160268379912, 0.14796105930588654, 0.1327553911629471, 0.11862990580753685, 0.2651070379874545, 0.2860808352550124, 0.2861889412207068, 0.2943248931683756, 0.29881004133841915, 0.2956038879497268, 0.2820204209879317, 0.28218445436864736, 0.27131605941447756, 0.6604987905145381, 0.678955693812864, 0.6714016916242234, 0.6540933045348184, 0.644355241223366, 0.6604037364697297, 0.6461454929183823, 0.6777693333654089, 0.6919514842888317, 0.10895545731406653, 0.10366554215241375, 0.10414424980464088, 0.09905620517130498, 0.10524027368287792, 0.10015916590413365, 0.09124401115614422, 0.1107981254371232, 0.10179175808254892, 0.18467089112199042, 0.21259419915332634, 0.18770095922969743, 0.1658915836510021, 0.21721542075067468, 0.178781001606784, 0.1764057024621839, 0.19332285067087462, 0.19151095069324642, 0.35636426189943393, 0.3395450892395795, 0.3381264638524698, 0.3477240450449747, 0.3589652216995053, 0.3172612362278122, 0.3601306082743758, 0.36719521235558306, 0.37385246459663535, 0.29405963976866867, 0.2751206502786776, 0.27694231547097903, 0.28317780298785133, 0.2872164489468654, 0.2711396238397893, 0.263915956177461, 0.28760465968960414, 0.28056920710073674, 0.2112936102341484, 0.2064439874832209, 0.2063523841729491, 0.20734297985532146, 0.21629586977148674, 0.1980611890914219, 0.19161757525233147, 0.20912604076350916, 0.20683231848472283, 0.17654194682066793, 0.1909645057058449, 0.18361258283180082, 0.18372617960254733, 0.18866369160915086, 0.19187204579018624, 0.18150312469871188, 0.1865172298304969, 0.17545999362287312, 0.34419280001928676, 0.38719486935107417, 0.41695586794295525, 0.7264284984786947, 0.4113612324847006, 0.5721681590559813, 0.39979278519186323, 0.42121941708709265, 0.5838123745146581, 0.551214889780775, 0.43375498388803024, 0.25353520998947643, 0.5769025793102873, 0.29239937524698345, 0.3920150295345829, 0.40055467629442243, 0.2047291792771042, 0.2023666376633726, 0.20218569955934396, 0.19029231565765736, 0.1764943234515145, 0.19960538151715823, 0.1847146527197573, 0.1900736928732425, 0.18399209539069494, 0.1914989806452514, 0.17446921909487245, 0.09611047585459764, 0.09388395179953968, 0.07263957201672688, 0.08324402036538248, 0.09301557406941863, 0.0816197213135691, 0.0761938554643724, 0.07920831986013421, 0.07609546187946381]}, "mutation_prompt": null}
{"id": "56e215d7-27cd-4798-a176-5563396255c2", "solution": "import numpy as np\n\nclass EnhancedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = 40\n        self.mutation_factor = 0.8\n        self.crossover_probability = 0.9\n        self.learning_rate = 0.1\n        self.population = None\n        self.fitness = None\n        self.restart_threshold = 0.01  # New parameter for restarting stagnated individuals\n\n    def initialize_population(self):\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, \n                                            (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def adaptive_differential_evolution(self, idx, func):\n        indices = [i for i in range(self.population_size) if i != idx]\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        adaptive_factor = 0.5 + 0.5 * np.random.rand()\n        mutant = self.population[idx] + adaptive_factor * (mutant - self.population[idx])\n        mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n        \n        cross_points = np.random.rand(self.dim) < self.crossover_probability\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        \n        trial = np.where(cross_points, mutant, self.population[idx])\n        trial_fitness = func(trial)\n        \n        if trial_fitness < self.fitness[idx]:\n            self.population[idx] = trial\n            self.fitness[idx] = trial_fitness\n        elif np.random.rand() < 0.4:  # Restart mechanism for stagnated individuals\n            self.population[idx] = np.random.uniform(self.lower_bound, self.upper_bound, self.dim)\n            self.fitness[idx] = func(self.population[idx])\n\n    def local_search(self, idx, func):  # Renamed and refined local search function\n        gradient = np.random.uniform(-1, 1, self.dim)\n        candidate = self.population[idx] - self.learning_rate * gradient\n        candidate = np.clip(candidate, self.lower_bound, self.upper_bound)\n        candidate_fitness = func(candidate)\n        if candidate_fitness < self.fitness[idx]:\n            self.population[idx] = candidate\n            self.fitness[idx] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population()\n        evaluations = 0\n        self.evaluate_population(func)\n        evaluations += self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                self.adaptive_differential_evolution(i, func)\n                evaluations += 1\n\n                if evaluations >= self.budget:\n                    break\n\n                if self.fitness[i] > self.restart_threshold:  # Condition for local search\n                    self.local_search(i, func)\n                evaluations += 1\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "EnhancedHybridOptimizer", "description": "An enhanced hybrid optimization algorithm incorporating Adaptive Differential Evolution with a Restart Mechanism and Local Search for improved convergence and robustness on complex landscapes.", "configspace": "", "generation": 7, "fitness": 0.13084641463806676, "feedback": "The algorithm EnhancedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.13 with standard deviation 0.16.", "error": "", "parent_id": "6a904b7a-737d-497b-b2e1-0c5368eca0d0", "metadata": {"aucs": [0.19109670006581092, 0.20160726843078614, 0.234007872938636, 0.20256555992751424, 0.22439906817652888, 0.2096306690986638, 0.20122630790727192, 0.1936845004896428, 0.2327452582693289, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.06824080186419301, 0.0697951547539265, 0.0524118476767762, 0.06728201322469118, 0.059403191081271345, 0.05969417280796829, 0.06326346514975356, 0.062067698995606935, 0.08181522666023844, 0.04789795813888709, 0.034583077987187294, 0.07590288953769286, 0.044132441964333524, 0.03530257943384818, 0.053076650200159725, 0.05099171724546614, 0.05959557047203401, 0.06692929286737859, 0.5260482029507625, 0.7578470934653452, 0.8745381979047661, 0.8843429415419134, 0.7608062183397322, 0.9049198105681283, 0.9159416388596482, 0.6757492021000696, 0.7501710896393843, 0.08869773529562064, 0.09797891699574868, 0.07261168212878799, 0.0774571255565707, 0.09807404371635298, 0.0802711636264497, 0.10954362481668867, 0.09185849508824118, 0.09194938865432667, 0.13449671754986392, 0.1310626277560053, 0.12306346539951174, 0.14738854418198588, 0.1531080378035563, 0.13109248783537586, 0.14348455997187826, 0.17290069191460045, 0.14886180359599643, 0.019853330220969467, 0.043392492731907484, 0.03377028303149021, 0.03890047026925203, 9.999999999998899e-05, 0.01260360689494977, 0.008418436387737538, 0.01950735271477333, 0.007370603873474657, 0.026080169816272925, 0.055969551917051885, 0.015112299374873461, 0.05130576003774712, 0.04077271265261184, 0.014633619012700883, 0.026163999104288793, 0.010078050967106345, 0.03987749451064315, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.10386289008626426, 0.08754610817342401, 0.09387274952773694, 0.0758238576857031, 0.05986663360232525, 0.06378465954481527, 0.060353668872361266, 0.0832415485115875, 0.089735643272287, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.005574850085742988, 0.01203825570076511, 0.0032054999501509585, 0.00011400857051413471, 9.999999999998899e-05, 0.005000008040496273, 9.999999999998899e-05, 9.999999999998899e-05, 0.00012206534465808438, 0.21089782865618945, 0.21719143219889558, 0.21694012204111812, 0.1932717852622864, 0.19525082792448378, 0.195454720548063, 0.23288876890547772, 0.2159739348344255, 0.2226693270209532, 0.060501270365472704, 0.0703524496368263, 0.06125534341977967, 0.05180075952822849, 0.07228162629767743, 0.05769311470184124, 0.06814935466119643, 0.049012925729857826, 0.050488464205902694, 0.17213663349722153, 0.1621223956982869, 0.15852833036376723, 0.15095274796386615, 0.14465959333626144, 0.1311677006334303, 0.14395897288167347, 0.1511867117936847, 0.14716066434442876, 0.18346578138879555, 0.17161350285970745, 0.16926392696265335, 0.16672346964829232, 0.17054582140122743, 0.1801752235059888, 0.18108843798951035, 0.1844602087382582, 0.1906260625021653, 0.10645437854886419, 0.12810521229462613, 0.12728784835768459, 0.1217798341389904, 0.11453696932559543, 0.12088133673244938, 0.15055153510377983, 0.11708466284251628, 0.12177880736281244, 0.1859470673914393, 0.18501237970799245, 0.16898220682331522, 0.17819792728965278, 0.1679794264003951, 0.16986866706296, 0.18240311624587435, 0.19670212664634734, 0.17830375638446272, 0.15713442102942354, 0.1567832399639898, 0.14864437256132823, 0.16954825237498805, 0.15355892168763496, 0.16962070909315285, 0.15835779132058747, 0.16444317609437953, 0.1614515405500302, 0.1385616576509341, 0.1660045634821139, 0.15823355945236328, 0.16114229666720403, 0.16203090427770128, 0.18517061895515474, 0.15861852137744792, 0.32960347322152106, 0.15304153106869656, 0.1834215563481466, 0.21934672387366172, 0.1489428097986205, 0.15468058968280707, 0.1635714590509202, 0.18624085751072617, 0.15031894773820365, 0.12876960576511554, 0.1602703594359306, 0.18875167481461907, 0.17883876358513295, 0.1842039575716199, 0.18952596368261487, 0.22234941467690283, 0.19416920170141805, 0.1874943270387962, 0.17982434492316057, 0.18088805065532887, 0.06606266884884371, 0.07144675039199178, 0.06722393437694085, 0.059728775197986805, 0.06388474164524016, 0.06475385624000496, 0.060925558194869556, 0.06863503121524539, 0.0641714438116725]}, "mutation_prompt": null}
{"id": "58624245-4f6c-42a4-b7d6-7d1175725caf", "solution": "import numpy as np\n\nclass RefinedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = 40\n        self.mutation_factor = 0.8\n        self.crossover_probability = 0.9\n        self.learning_rate = 0.1\n        self.population = None\n        self.fitness = None\n        self.levy_exponent = 1.5  # Levy flight exponent\n\n    def initialize_population(self):\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound,\n                                            (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def adaptive_differential_evolution(self, idx, func):\n        indices = [i for i in range(self.population_size) if i != idx]\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        adaptive_factor = 0.5 + 0.5 * np.random.rand()\n        mutant = self.population[idx] + adaptive_factor * (mutant - self.population[idx])\n        mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n        \n        cross_points = np.random.rand(self.dim) < self.crossover_probability\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        \n        trial = np.where(cross_points, mutant, self.population[idx])\n        trial_fitness = func(trial)\n        \n        if trial_fitness < self.fitness[idx]:\n            self.population[idx] = trial\n            self.fitness[idx] = trial_fitness\n\n    def randomized_newton_update(self, idx, func):\n        gradient = np.random.uniform(-1, 1, self.dim)\n        hessian_approx = np.random.uniform(0.1, 0.5, self.dim)\n        candidate = self.population[idx] - self.learning_rate * gradient / (np.abs(hessian_approx) + 1e-9)\n        candidate = np.clip(candidate, self.lower_bound, self.upper_bound)\n        candidate_fitness = func(candidate)\n        if candidate_fitness < self.fitness[idx]:\n            self.population[idx] = candidate\n            self.fitness[idx] = candidate_fitness\n\n    def levy_flight(self, idx):\n        step = np.random.standard_cauchy(size=self.dim)\n        levy_step = step * np.power(np.abs(step), -1.0/self.levy_exponent)\n        candidate = self.population[idx] + levy_step\n        candidate = np.clip(candidate, self.lower_bound, self.upper_bound)\n        return candidate\n\n    def __call__(self, func):\n        self.initialize_population()\n        evaluations = 0\n        self.evaluate_population(func)\n        evaluations += self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                self.adaptive_differential_evolution(i, func)\n                evaluations += 1\n\n                if evaluations >= self.budget:\n                    break\n\n                self.randomized_newton_update(i, func)\n                evaluations += 1\n\n                if evaluations >= self.budget:\n                    break\n\n                candidate = self.levy_flight(i)\n                candidate_fitness = func(candidate)\n                if candidate_fitness < self.fitness[i]:\n                    self.population[i] = candidate\n                    self.fitness[i] = candidate_fitness\n                evaluations += 1\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "RefinedHybridOptimizer", "description": "A refined hybrid optimization algorithm incorporating Stochastic Levy Flights with Adaptive Differential Evolution and Randomized Newton-like updates for enhanced exploration and exploitation across complex landscapes.", "configspace": "", "generation": 8, "fitness": 0.2862903504563448, "feedback": "The algorithm RefinedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.29 with standard deviation 0.21.", "error": "", "parent_id": "6a904b7a-737d-497b-b2e1-0c5368eca0d0", "metadata": {"aucs": [0.6898693742757211, 0.7032083696618295, 0.6737599614540899, 0.6655239675666789, 0.6637549440812478, 0.6884498403098, 0.6998726137583025, 0.7112569866809555, 0.6915038925214201, 0.3692855604054808, 0.36625677981422733, 0.3967310482469327, 0.35975825556266516, 0.35794261180977605, 0.3598737458884991, 0.3536803576588028, 0.3216104102848202, 0.39280243581484886, 0.08601996408985901, 0.13248926888321244, 0.11955884673900985, 0.10683669537897056, 0.1134176085022951, 0.09698117638781112, 0.09409550406409783, 0.10457129665555254, 0.10289340038949035, 0.0780254529284724, 0.09028520229006076, 0.08496450922978616, 0.08391991825568257, 0.08864589682833734, 0.08857637998195678, 0.09530965428443983, 0.08337279355281813, 0.0801820249840195, 0.9392324910474867, 0.8959231948473585, 0.8731906709829603, 0.8811447054563868, 0.8795829216667244, 0.896705434884929, 0.9670649489507133, 0.9178379221695945, 0.9001729750611388, 0.2794050908889124, 0.2791125930752403, 0.26053891361636194, 0.28889576020322805, 0.25492160406767983, 0.30194808465968515, 0.28234243999515296, 0.27620142560434746, 0.2719350765887417, 0.6770944991245177, 0.6394062046882076, 0.5927082419040611, 0.6277516835716674, 0.6645444900878527, 0.6390951098043159, 0.6444306627981193, 0.6441878554513698, 0.6522285505530352, 0.17862025325440356, 0.15679077359742555, 0.1577207704427872, 0.1565122817977559, 0.1423548283943572, 0.14996606795164713, 0.1726181711147572, 0.1370297529348936, 0.1524419415544067, 0.18159039429571877, 0.16629265532832704, 0.12879255617961405, 0.16623530292150868, 0.14640998484846224, 0.21070257452118746, 0.18162638859523472, 0.15658181386337078, 0.16362874637804325, 0.17980666106779186, 0.13491339685527515, 0.22117108919071404, 0.14595692821434547, 0.18886854472972958, 0.19028836352058975, 0.11118475280205786, 0.16821721975755644, 0.1622649615457885, 0.3311208813467861, 0.29343833766276073, 0.3251838386412066, 0.27777666143261015, 0.3268144390911423, 0.28192480094595374, 0.3663867434433823, 0.34136928650615195, 0.36090182181385266, 0.05314641273485632, 0.0747246191661417, 0.07728210462301, 0.10570231934839702, 0.07861155955759003, 0.09002928417004208, 0.06179107207975354, 0.03605064731129137, 0.07785479083415925, 0.17790017902939692, 0.18963328429085868, 0.19631131802429558, 0.19544115535384754, 0.20862542978669807, 0.20792961002671573, 0.1813348355549662, 0.1998600852039697, 0.1867426701590309, 0.5447041280892087, 0.5405818842176191, 0.555447877513153, 0.5395543726001395, 0.5436275525025704, 0.5387353654661298, 0.5664291534950263, 0.559230206779151, 0.5635547215346666, 0.09193223683744423, 0.09644124510709884, 0.09219826011193111, 0.0957381976010836, 0.09584855565356032, 0.08949039124685243, 0.10796613348175865, 0.08658221741941619, 0.09077901670487609, 0.17758756191588532, 0.1995698430768823, 0.18113686787267103, 0.180212995191687, 0.1657748135768592, 0.18091684805261044, 0.18748608078267903, 0.1851463868446157, 0.16294016112602605, 0.2830548777298152, 0.293239977998074, 0.3032413463833724, 0.260435693062136, 0.30515525263218835, 0.2772161347106421, 0.2933951959913186, 0.3081446973315407, 0.2968097939631841, 0.2380113492141367, 0.21756875678691223, 0.2446030052285052, 0.19316625982763924, 0.20817793682739116, 0.23121844813020143, 0.23251460577916927, 0.241215963521901, 0.23596694937217488, 0.19861026614159605, 0.24138289190505247, 0.19734413493470382, 0.21281264633689057, 0.22293426767929903, 0.19492878488415044, 0.18990325826876642, 0.20065808639800276, 0.20036123052950994, 0.1904869815426532, 0.18204849966382253, 0.18180270729636228, 0.17969125891641724, 0.19126804313410428, 0.17719900387367182, 0.1927666775083844, 0.18732818951840513, 0.17774597394842295, 0.3563597114599485, 0.4150247901190912, 0.45429204949483926, 0.5027382080981151, 0.34916717891244275, 0.3829835609143196, 0.3752930128229899, 0.2857480532105998, 0.3753333681507337, 0.4140913002457093, 0.3760041303329137, 0.3324016108899346, 0.3278707938226495, 0.3370768724754716, 0.33416150161952596, 0.23827848582092392, 0.2756814454099322, 0.3286485186913658, 0.1831720052976854, 0.19073796855297698, 0.19314074441010853, 0.1848110009940378, 0.20525020900781965, 0.17468689972726625, 0.18699733855075773, 0.18580337596382668, 0.19971091163492927, 0.08071741305794566, 0.07395536861753405, 0.07867428451024694, 0.07647039147573798, 0.090213260799332, 0.0819683029794902, 0.08011208569482609, 0.08284617701338393, 0.09090548125057396]}, "mutation_prompt": null}
{"id": "75e4c814-d776-4401-825e-30dc1c6bbb68", "solution": "import numpy as np\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = 40\n        self.mutation_factor = 0.9  # Increased mutation factor for better exploration\n        self.crossover_probability = 0.85  # Slightly reduced crossover probability\n        self.learning_rate = 0.2  # Adjusted learning rate for quasi-Newton\n        self.population = None\n        self.fitness = None\n\n    def initialize_population(self):\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, \n                                            (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def adaptive_differential_evolution(self, idx, func):\n        indices = [i for i in range(self.population_size) if i != idx]\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        adaptive_factor = 0.6 + 0.4 * np.random.rand()  # Modified adaptive factor range\n        mutant = self.population[idx] + adaptive_factor * (mutant - self.population[idx])\n        mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n        \n        cross_points = np.random.rand(self.dim) < self.crossover_probability\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        \n        trial = np.where(cross_points, mutant, self.population[idx])\n        trial_fitness = func(trial)\n        \n        if trial_fitness < self.fitness[idx]:\n            self.population[idx] = trial\n            self.fitness[idx] = trial_fitness\n\n    def randomized_newton_update(self, idx, func):\n        gradient = np.random.uniform(-1, 1, self.dim)\n        hessian_approx = np.random.uniform(0.05, 0.4, self.dim)  # Adjusted hessian approximation range\n        candidate = self.population[idx] - self.learning_rate * gradient / (np.abs(hessian_approx) + 1e-9)\n        candidate = np.clip(candidate, self.lower_bound, self.upper_bound)\n        candidate_fitness = func(candidate)\n        if candidate_fitness < self.fitness[idx]:\n            self.population[idx] = candidate\n            self.fitness[idx] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population()\n        evaluations = 0\n        self.evaluate_population(func)\n        evaluations += self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                self.adaptive_differential_evolution(i, func)\n                evaluations += 1\n\n                if evaluations >= self.budget:\n                    break\n\n                self.randomized_newton_update(i, func)\n                evaluations += 1\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "AdaptiveHybridOptimizer", "description": "A hybrid optimizer integrating Adaptive Differential Evolution with Randomized Quasi-Newton updates, enhancing exploration with adaptive strategies and efficient curvature approximations for diverse problem landscapes.", "configspace": "", "generation": 9, "fitness": 0.2719085232029464, "feedback": "The algorithm AdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.27 with standard deviation 0.21.", "error": "", "parent_id": "6a904b7a-737d-497b-b2e1-0c5368eca0d0", "metadata": {"aucs": [0.7110795891227862, 0.6670983601461611, 0.6872690070834153, 0.6957804187434731, 0.7143057135215751, 0.6878972555682217, 0.6910711846175319, 0.6640681098999869, 0.690979492215706, 0.43910076796464925, 0.39904240657309176, 0.4370786789546772, 0.40631941971699903, 0.37869062497165373, 0.4129454535981786, 0.44933026839656676, 0.4513859830316337, 0.40681854441795906, 0.10330255974407121, 0.10676013207373669, 0.10782579657676272, 0.09682216049978065, 0.10103166229678195, 0.10515752017305491, 0.10887806727503535, 0.11217877234694718, 0.10379703889251835, 0.08346912741421364, 0.09294434938174723, 0.09097719377243774, 0.0876357467933403, 0.0868211995877407, 0.10127337462132668, 0.10349835579201394, 0.07992450876983004, 0.08882347051190631, 0.9452574576805222, 0.9079724115960667, 0.9044893547304625, 0.9164226879535465, 0.8969937366455893, 0.9447429761548468, 0.9018976789199753, 0.9144865515040445, 0.8968642065262957, 0.2916832935167054, 0.2675897648881099, 0.2825462442725103, 0.29564699558426133, 0.31546989559304617, 0.26665278702621054, 0.2863126465349526, 0.2961472554489102, 0.28175525180262495, 0.5594024836844046, 0.5106034160358738, 0.5687490475797241, 0.5796687956214316, 0.5967372750817904, 0.5774180835378535, 0.5667395175565426, 0.5858005640657025, 0.6408863829487557, 0.14389631354584287, 0.14391811676772592, 0.18563039350683186, 0.1458516342239098, 0.17775922467606764, 0.16820334333371123, 0.19974982452748258, 0.13915113021144387, 0.18517060048245615, 0.17356681304947175, 0.140453826082219, 0.16501409597005479, 0.18701088346471972, 0.16680571878350203, 0.1549476375651182, 0.21458228616039254, 0.15653278003002724, 0.1989651961452985, 0.13337941110891172, 0.10078049196472039, 0.11610873344035277, 0.13244071931051982, 0.0966365454410627, 0.09523793791366719, 0.12929413924841515, 0.12773904383764179, 0.11624818205319554, 0.24382210198939525, 0.24892363865541034, 0.24941047504561664, 0.23070254633392384, 0.23154703574320645, 0.21416155142067206, 0.3160668969883125, 0.30589565019744236, 0.2582270399408533, 0.018647445568280796, 0.03236162764298556, 0.021916382979198845, 0.03309214467655852, 0.05808071639712431, 0.027026983016745554, 0.04920076021957498, 0.04019368932546352, 0.028701497532778952, 0.1596373085081768, 0.15956563210192176, 0.15678160238760008, 0.15791190334735272, 0.1840642755780304, 0.17018268397897884, 0.1527062315968324, 0.16956942206862624, 0.1567551364010682, 0.49000121790453255, 0.5200317443427558, 0.5311186040448972, 0.5035179978608793, 0.5065706246468945, 0.5196420358217324, 0.5067322173471436, 0.5343138037606547, 0.5087032191989371, 0.09429462684469381, 0.08966834832025183, 0.08211585151129752, 0.09513416084035431, 0.08793073330486545, 0.09024179922846032, 0.08770733982650891, 0.0951235582533142, 0.10320700356422041, 0.15457369435479307, 0.14176355716323308, 0.16999706620925759, 0.1822929803300778, 0.14118207898016444, 0.14651469020922492, 0.1493518320228715, 0.1738106486099127, 0.1494119392481008, 0.28888568514758695, 0.30345396700956573, 0.296558852362909, 0.2949198440669173, 0.28344990242173773, 0.28294209727637276, 0.2964394593014035, 0.30633852335171985, 0.2756218135644216, 0.22900519094996874, 0.2184990075911073, 0.20438921975105695, 0.23006140581623036, 0.23321228874749167, 0.2108455028616809, 0.20889344966821832, 0.23559160890559339, 0.2352293928872108, 0.2091132509666439, 0.19186513828645257, 0.21305919677289198, 0.17672151743736708, 0.20349788710189143, 0.21331752453452268, 0.1943675292512247, 0.17952710122559035, 0.1689175230572615, 0.18495564169012269, 0.18292651266836746, 0.17199828301227138, 0.18310070587175864, 0.18195199941713613, 0.1777405801880716, 0.1735176728839699, 0.17523980504960723, 0.17988168540205374, 0.3262451760418269, 0.38773459236924057, 0.38830161003544184, 0.6052894619117826, 0.21811274488811927, 0.2941586936174727, 0.2351694757803613, 0.2865541519241791, 0.30996901139807775, 0.3837233255016326, 0.26415111677995684, 0.30035959297620074, 0.2954635984714541, 0.2832936763916464, 0.2805208198986562, 0.22805659808729617, 0.26243601587998233, 0.2061466613313332, 0.18497878717364102, 0.18257339671849748, 0.17181636734954897, 0.18733919618365713, 0.21260631341594927, 0.21108903151050773, 0.18852545207625082, 0.17952073981843808, 0.1809937905122887, 0.08812526422517986, 0.07658887872740028, 0.07918172432310566, 0.08088354443152967, 0.07987357733832157, 0.07393020713897969, 0.07311760497103792, 0.07408622006597143, 0.07978797227786671]}, "mutation_prompt": null}
{"id": "b0018397-2bb9-44dd-b1b1-e1a4b86f7a00", "solution": "import numpy as np\n\nclass EnhancedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = 40\n        self.mutation_factor_base = 0.5\n        self.crossover_probability_base = 0.7\n        self.learning_rate_base = 0.1\n        self.population = None\n        self.fitness = None\n\n    def initialize_population(self):\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, \n                                            (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def adaptive_differential_evolution(self, idx, func):\n        indices = [i for i in range(self.population_size) if i != idx]\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        adaptive_factor = self.mutation_factor_base + np.random.rand() * 0.5\n        mutant = self.population[a] + adaptive_factor * (self.population[b] - self.population[c])\n        mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n        \n        cross_points = np.random.rand(self.dim) < (self.crossover_probability_base + np.random.rand() * 0.3)\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        \n        trial = np.where(cross_points, mutant, self.population[idx])\n        trial_fitness = func(trial)\n        \n        if trial_fitness < self.fitness[idx]:\n            self.population[idx] = trial\n            self.fitness[idx] = trial_fitness\n\n    def reinforced_random_walk(self, idx, func):\n        perturbation = np.random.uniform(-0.5, 0.5, self.dim)\n        candidate = self.population[idx] + (self.learning_rate_base + 0.05 * np.random.rand()) * perturbation\n        candidate = np.clip(candidate, self.lower_bound, self.upper_bound)\n        candidate_fitness = func(candidate)\n        \n        if candidate_fitness < self.fitness[idx]:\n            self.population[idx] = candidate\n            self.fitness[idx] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population()\n        evaluations = 0\n        self.evaluate_population(func)\n        evaluations += self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                self.adaptive_differential_evolution(i, func)\n                evaluations += 1\n\n                if evaluations >= self.budget:\n                    break\n\n                self.reinforced_random_walk(i, func)\n                evaluations += 1\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "EnhancedHybridOptimizer", "description": "A hybrid optimization algorithm leveraging Adaptive Differential Evolution with stochastic control parameters and Reinforced Random Walk for enhanced convergence and diversity.", "configspace": "", "generation": 10, "fitness": 0.2594564825745884, "feedback": "The algorithm EnhancedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.26 with standard deviation 0.19.", "error": "", "parent_id": "6a904b7a-737d-497b-b2e1-0c5368eca0d0", "metadata": {"aucs": [0.5866835987191042, 0.5916523915965324, 0.5852542995444323, 0.6338070053206882, 0.5982845886445245, 0.6593213429022782, 0.5739085962347423, 0.6076773398723887, 0.6055668133380503, 0.28532764664651467, 0.29168903294406623, 0.31438390952767925, 0.3013640413668097, 0.2765045661143145, 0.25931622846175706, 0.31125918615410597, 0.28701462049063153, 0.290584977256202, 0.12946599314507434, 0.13458197039313602, 0.13428684445712114, 0.1269524389652943, 0.1317340751587146, 0.13561186471130515, 0.13085147243421946, 0.12232601589356196, 0.17829941771820845, 0.11165202649620676, 0.10195105606958854, 0.11890791805461542, 0.11625195591188453, 0.11210676637512862, 0.10412673108020809, 0.10992311295292934, 0.1162113008249791, 0.10384917813399985, 0.9613547375015398, 0.9356092786061538, 0.8980204588063042, 0.9079601496693456, 0.8538708977998083, 0.8713509407572435, 0.8899946593025919, 0.8774003438206288, 0.8868664705904278, 0.27212525930173115, 0.2893525778747248, 0.2505732894463275, 0.2539097062514446, 0.27000117956946446, 0.2731779656798765, 0.27637021299226217, 0.26343792068810246, 0.2682117108669265, 0.5078001132041763, 0.42840460323225216, 0.45528273614360537, 0.5891204470747159, 0.4908073519737044, 0.5284716953565761, 0.4157652994122063, 0.49997815851411087, 0.538387043679422, 0.25339638038434364, 0.171379534632074, 0.17335820424092796, 0.1119490626141143, 0.20370187114636995, 0.1716634979577789, 0.22330505579738857, 0.23313938424864955, 0.2014024867090367, 0.21354262810126579, 0.20409346109589344, 0.1488474684940727, 0.23307372973829765, 0.20207721488997188, 0.17145684086727908, 0.24826216868186413, 0.16854177348261368, 0.1777882916530662, 0.11724926628080035, 0.11785575379762758, 0.1130298531001821, 0.08076368022751834, 0.10873537574736236, 0.10510577956060096, 0.10134410875699196, 0.0977942165668666, 0.08250475961102821, 0.20623149721061662, 0.20917988684391697, 0.24509002679295222, 0.25100740668413624, 0.21303695302227166, 0.21069126789571846, 0.2543942532533221, 0.2665445897639469, 0.24423132742424036, 0.045888531393636356, 0.02633869363898722, 0.01906122305562563, 0.052812454117100005, 0.03326952103622771, 0.024180384201448413, 0.01958632404316618, 0.03329960959819911, 0.04330002822094925, 0.14288245006168232, 0.1493068863471546, 0.13927780893922936, 0.14408650129756784, 0.15024782352785837, 0.15391241686077595, 0.1418780642258285, 0.1482754459941914, 0.1542855974039381, 0.4405898720417305, 0.4539398615815342, 0.4906099520249102, 0.45781383382629326, 0.4582652870567787, 0.46238145259129126, 0.4424108671678515, 0.4607558676642153, 0.4663242389044828, 0.1044996651762018, 0.12050993353069384, 0.10543021899441374, 0.10534981080259276, 0.11888072808899708, 0.12958956029027446, 0.11807250624319987, 0.1145669303706287, 0.1040079924987507, 0.18260156669079897, 0.19397317178705686, 0.2336011466018596, 0.19531465242837454, 0.24353155147102756, 0.1836683310523738, 0.1699181474818875, 0.17250668544937509, 0.24260899202208053, 0.238089483492133, 0.2597726266990047, 0.2679335079578977, 0.2400030414146862, 0.25817907489138137, 0.24294550308977236, 0.2583564130673892, 0.27154442598047335, 0.2736522638006347, 0.19518819936896825, 0.20502425129329904, 0.1792250163134862, 0.18469320910789078, 0.2003625670847905, 0.17191558252761274, 0.20125166303897268, 0.21271762337682054, 0.17335862989158946, 0.2114230147854701, 0.19475977995322402, 0.19603797725986438, 0.19443708938501703, 0.1924839732376341, 0.18416592775585627, 0.1970514090023021, 0.18653502472070616, 0.19483084147458607, 0.19016464204654582, 0.2140566583535055, 0.2045024889892496, 0.20023941822912183, 0.18117353464321062, 0.22268551865103403, 0.20462636540707346, 0.19204960071614163, 0.22030220497434738, 0.19307152722161358, 0.596121864122152, 0.1788841924902308, 0.28014008010259905, 0.17724675653257982, 0.5469053928890087, 0.23058269040654822, 0.16722966182654764, 0.5861772968183523, 0.44212606987172953, 0.32649937771481896, 0.27233962249652455, 0.16986509004442718, 0.18318560339333212, 0.2844317364544675, 0.202230888706452, 0.5405227577479879, 0.20523597359284262, 0.18793475506480872, 0.1917675714550341, 0.1832897817176583, 0.18911811452546556, 0.19386968627358903, 0.20141222466998954, 0.20531607952338504, 0.201549551059218, 0.2026674068832386, 0.09012763796960188, 0.07876195579216072, 0.0816528754852972, 0.07031386006136053, 0.07962679769867187, 0.07859630154242658, 0.08539300246979586, 0.07800628841765211, 0.08298403398281984]}, "mutation_prompt": null}
{"id": "40be2100-6ec0-44ca-abed-ee2cbc7cd30e", "solution": "import numpy as np\n\nclass EnhancedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = 50\n        self.mutation_factor = 0.9\n        self.crossover_probability = 0.85\n        self.learning_rate = 0.05\n        self.population = None\n        self.fitness = None\n\n    def initialize_population(self):\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, \n                                            (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def adaptive_differential_evolution(self, idx, func):\n        indices = [i for i in range(self.population_size) if i != idx]\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        adaptive_factor = 0.6 + 0.4 * np.random.rand()\n        mutant = self.population[idx] + adaptive_factor * (mutant - self.population[idx])\n        mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n        \n        cross_points = np.random.rand(self.dim) < self.crossover_probability\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        \n        trial = np.where(cross_points, mutant, self.population[idx])\n        trial_fitness = func(trial)\n        \n        if trial_fitness < self.fitness[idx]:\n            self.population[idx] = trial\n            self.fitness[idx] = trial_fitness\n\n    def randomized_lbfgs_update(self, idx, func):\n        gradient_estimate = np.random.uniform(-1, 1, self.dim)\n        hessian_diag_approx = np.random.uniform(0.1, 0.3, self.dim)\n        candidate = self.population[idx] - self.learning_rate * gradient_estimate / (np.abs(hessian_diag_approx) + 1e-8)\n        candidate = np.clip(candidate, self.lower_bound, self.upper_bound)\n        candidate_fitness = func(candidate)\n        if candidate_fitness < self.fitness[idx]:\n            self.population[idx] = candidate\n            self.fitness[idx] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population()\n        evaluations = 0\n        self.evaluate_population(func)\n        evaluations += self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                self.adaptive_differential_evolution(i, func)\n                evaluations += 1\n\n                if evaluations >= self.budget:\n                    break\n\n                self.randomized_lbfgs_update(i, func)\n                evaluations += 1\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "EnhancedHybridOptimizer", "description": "A novel hybrid optimization algorithm incorporating Adaptive Differential Evolution with Randomized L-BFGS updates for enhanced convergence and robustness across a variety of optimization landscapes.", "configspace": "", "generation": 11, "fitness": 0.25627260728436707, "feedback": "The algorithm EnhancedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.26 with standard deviation 0.19.", "error": "", "parent_id": "6a904b7a-737d-497b-b2e1-0c5368eca0d0", "metadata": {"aucs": [0.6391995897852385, 0.6045631769569763, 0.6374106607321681, 0.639525606039737, 0.6302289244178507, 0.6324087932012732, 0.628005336222764, 0.6236848940895168, 0.6323005780813298, 0.31078942048713876, 0.30490306704064374, 0.32061742779095403, 0.3308275009362849, 0.3135584569380999, 0.3253090515614021, 0.30070841151308747, 0.32764049754552005, 0.3226707110146595, 0.1019138124791591, 0.11936554772670005, 0.10043455841497995, 0.10146082127889589, 0.11571968328950621, 0.09516554372062724, 0.09843616613442197, 0.10038515052174246, 0.10476188001652331, 0.08316433841690885, 0.0872512070594319, 0.08487703527131185, 0.09362025902080917, 0.08382079892929828, 0.08859386768587907, 0.09063928961139478, 0.08448433440425884, 0.08664231324946192, 0.8859789361537134, 0.9198648979280809, 0.8882617365598727, 0.8585397600758778, 0.8400546088681563, 0.8228110616338846, 0.8550232667410869, 0.8083232059324135, 0.9058021832673842, 0.2691831285746499, 0.24827557559500157, 0.24610937143983724, 0.2389785366476742, 0.24271528811644116, 0.23920972955080622, 0.27738152076234723, 0.2608298712753607, 0.25307406262829457, 0.49018348743132034, 0.5200205037868042, 0.4455208189388391, 0.6153717215660464, 0.5643441770875897, 0.5034283064612823, 0.4656210966983674, 0.5495527367687887, 0.568467302511614, 0.1807053539859027, 0.14774445400157366, 0.12773311473409832, 0.15417691295501967, 0.15875567714288374, 0.1610506659170564, 0.1602464112096268, 0.1486190164921034, 0.17527754482477753, 0.15489143343634004, 0.1344839745819003, 0.14218154667667693, 0.15925764614706572, 0.16622815695598858, 0.15799709691084485, 0.15922953743193669, 0.16358702977472117, 0.1515058154893093, 0.10705395156033082, 0.07213347979828721, 0.07000159691875996, 0.09259021131783651, 0.07793203354167111, 0.08286060505353121, 0.12380264850815714, 0.09059559812164553, 0.09141274602940175, 0.233885427851373, 0.2269808151499344, 0.23468127138910688, 0.20457076814797015, 0.18875177513608332, 0.2375110856486905, 0.2835700587470582, 0.22753222212108937, 0.2438048901610428, 0.031695982673383916, 0.023862866312848396, 0.016755115148474164, 0.023255755248620114, 0.011601868998068698, 0.02788718504518306, 0.034609782439699144, 0.030598338695696503, 0.04176768907615702, 0.14498347247107302, 0.15773430426402357, 0.1491466180636033, 0.15965168888075187, 0.1518054278255091, 0.15094931655134247, 0.1386772259442448, 0.12729053228626885, 0.14582742767187262, 0.4565109941541967, 0.46838013195716055, 0.4765830699018321, 0.4693954929248486, 0.4883618961823202, 0.4540571351757685, 0.48183194246820704, 0.46965612805785617, 0.4911425168710545, 0.09308049912610583, 0.09489666282596532, 0.09995169745719945, 0.10264251922787448, 0.08295483493362532, 0.08667952371929932, 0.09640628084262282, 0.0945069957707757, 0.10296001186901282, 0.2065141740416292, 0.20692836067043674, 0.16993645546027414, 0.1798536571738012, 0.17004378555148525, 0.17409578557478056, 0.16260884062707648, 0.18537049808943984, 0.1916071824811082, 0.25777235024637457, 0.2727463600594283, 0.2567840260400608, 0.2733758971037329, 0.24902893040553875, 0.2590616565325454, 0.25876631488039425, 0.2781927329877377, 0.2721177793089571, 0.2124826633289616, 0.20846902585094218, 0.18934771472999923, 0.1955658765818702, 0.213558116215113, 0.1917170123170615, 0.21800981047960455, 0.18905394861069447, 0.20033687742515172, 0.182578442568057, 0.18561779654921673, 0.17872182931753322, 0.1995394241967421, 0.20145731349885, 0.17094816907326404, 0.18854169133134635, 0.18607485703683957, 0.20919619953705826, 0.19279108077213003, 0.18551719221592844, 0.1926851842254449, 0.19773023763199193, 0.19603459150293312, 0.19072862669089952, 0.18393953695307952, 0.19223402617429786, 0.186211856835379, 0.2841156408403358, 0.35085714480420926, 0.25684148505609417, 0.4526208883895745, 0.42642547758133476, 0.3461502222500006, 0.3599450727973801, 0.267292065478823, 0.3190698332386559, 0.45488010154658254, 0.2904666443528058, 0.43477340257971486, 0.3263852295970757, 0.3179591555294564, 0.31725090617116425, 0.32450713250062047, 0.4196130051566964, 0.3876141462298448, 0.17858672428701317, 0.19808252453685105, 0.1847063367697993, 0.1838877738580772, 0.21240504097140378, 0.18283151319648805, 0.1797460574470039, 0.17876038205822564, 0.1702150966000372, 0.07543746272895224, 0.06670595192206197, 0.0675355174641954, 0.08602181426612332, 0.07484969630385307, 0.07724933247490096, 0.0842731196439721, 0.07612244548713265, 0.07734783090180808]}, "mutation_prompt": null}
{"id": "d8c697ae-d80e-4330-9423-90cc6ff47328", "solution": "import numpy as np\n\nclass EnhancedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = 50  # Increased population size for better exploration\n        self.mutation_factor = 0.85  # Slightly increased mutation factor\n        self.crossover_probability = 0.8  # Adjusted crossover for more diversity\n        self.learning_rate = 0.1\n        self.population = None\n        self.fitness = None\n\n    def initialize_population(self):\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, \n                                            (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def adaptive_differential_evolution(self, idx, func):\n        indices = [i for i in range(self.population_size) if i != idx]\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        adaptive_factor = 0.5 + 0.5 * np.random.rand() * np.random.rand()  # Added stochastic variability\n        mutant = self.population[idx] + adaptive_factor * (mutant - self.population[idx])\n        mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n        \n        cross_points = np.random.rand(self.dim) < self.crossover_probability\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        \n        trial = np.where(cross_points, mutant, self.population[idx])\n        trial_fitness = func(trial)\n        \n        if trial_fitness < self.fitness[idx]:\n            self.population[idx] = trial\n            self.fitness[idx] = trial_fitness\n\n    def randomized_newton_update(self, idx, func):\n        gradient = np.random.uniform(-1, 1, self.dim)\n        hessian_approx = np.random.uniform(0.1, 0.5, self.dim)\n        stochastic_lr = self.learning_rate * (0.9 + 0.2 * np.random.rand())  # Randomized learning rate\n        candidate = self.population[idx] - stochastic_lr * gradient / (np.abs(hessian_approx) + 1e-9)\n        candidate = np.clip(candidate, self.lower_bound, self.upper_bound)\n        candidate_fitness = func(candidate)\n        if candidate_fitness < self.fitness[idx]:\n            self.population[idx] = candidate\n            self.fitness[idx] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population()\n        evaluations = 0\n        self.evaluate_population(func)\n        evaluations += self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                self.adaptive_differential_evolution(i, func)\n                evaluations += 1\n\n                if evaluations >= self.budget:\n                    break\n\n                self.randomized_newton_update(i, func)\n                evaluations += 1\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "EnhancedHybridOptimizer", "description": "An enhanced hybrid optimizer using Adaptive Differential Evolution and Randomized Newton-like updates with stochastic learning rates for improved adaptability across complex landscapes.", "configspace": "", "generation": 12, "fitness": 0.28968514790452377, "feedback": "The algorithm EnhancedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.29 with standard deviation 0.21.", "error": "", "parent_id": "6a904b7a-737d-497b-b2e1-0c5368eca0d0", "metadata": {"aucs": [0.7694858616265279, 0.7523888894523086, 0.7575079234294777, 0.7357712930510398, 0.7306050014741416, 0.749722330433545, 0.7449878573508076, 0.737887835297415, 0.7472673206376266, 0.5086917677255645, 0.5083500132672969, 0.5013010570267746, 0.5214011041454625, 0.5026562725949795, 0.5206629326799552, 0.4520336839496767, 0.4959394134434255, 0.482942406028668, 0.11152397475190023, 0.11965787913615933, 0.11667528296246055, 0.11400350714464824, 0.11010307306931122, 0.12067141838297879, 0.12251076168026032, 0.1157801161766896, 0.12557325239011918, 0.11226671560034907, 0.09521234077574992, 0.08751922262350886, 0.09531274565442649, 0.09333050016696776, 0.0856068473680669, 0.09674542618319737, 0.09172183981664639, 0.09022424001938323, 0.8347557251719506, 0.8149153099903221, 0.843574720554591, 0.8441517207313736, 0.8250319710382322, 0.8475094052633544, 0.7841534355468956, 0.7880265007404708, 0.8037016860017925, 0.2574963959094755, 0.28722267273093016, 0.2840888085940203, 0.28853516478969254, 0.28267735285603435, 0.2932918605491457, 0.2900778580313652, 0.2609083875722722, 0.2950179375705747, 0.6067410486728406, 0.6239651302863121, 0.6315438235322244, 0.7228437341728083, 0.6382605760896357, 0.6843249635354506, 0.5800805891809873, 0.6310464896308899, 0.5985657925878223, 0.15700249147013512, 0.15475695709579362, 0.15115459718503532, 0.16117810792099385, 0.14072908015155206, 0.1572694521454332, 0.16470956653625912, 0.18159164000343175, 0.1950813658994648, 0.16515942080256152, 0.16656885681057798, 0.16023805461145824, 0.17981478264513773, 0.18722088687848182, 0.1524045538724802, 0.1565632838390718, 0.16523130754728355, 0.1670413991374764, 0.11858292794139946, 0.1471296922101355, 0.10966452119434489, 0.13449092719047184, 0.12335079441126529, 0.10800412206389787, 0.17901556903583826, 0.05805125470277217, 0.14608192076412396, 0.2685570508362761, 0.2881985048207786, 0.26560758318673383, 0.1765249570239974, 0.1726423441530084, 0.17799928550025834, 0.2906158734595703, 0.3457545164612761, 0.2465157902675117, 0.08083446805218486, 0.06431196883525714, 0.04925358275080749, 0.08782817558699585, 0.08877056574175168, 0.07023714263624425, 0.08959866902295943, 0.09029918046583407, 0.10556552302737132, 0.18757036053679732, 0.20551163043803422, 0.17301925470384028, 0.18307212847719123, 0.2144770865876129, 0.19034842144770237, 0.1861308889740536, 0.19557284390316088, 0.18083024987323837, 0.5635292902896585, 0.5537341615884839, 0.575294020700637, 0.5181904803988302, 0.5613390961684455, 0.5470247610657818, 0.5518380348809604, 0.5605959871952828, 0.5881271351863857, 0.10058919537802036, 0.10227188604803472, 0.09700428251496085, 0.1126914517483576, 0.10483996121745642, 0.11721039274694445, 0.10854764593686705, 0.09773608741859685, 0.09439358716126278, 0.19162069654108949, 0.19204184525045898, 0.18257985221802864, 0.20737992763814106, 0.18414391793971585, 0.1716411805852458, 0.20780552902959804, 0.1815613019127309, 0.18937805445651723, 0.3107895075204279, 0.31563438709535796, 0.30363917710519006, 0.33053188629889807, 0.345114520264064, 0.31376378303574914, 0.32896876609952064, 0.32609477576883505, 0.28844726208172144, 0.22497308480851919, 0.22845880330258894, 0.23342087750082907, 0.23651793468953752, 0.23539694803114475, 0.22171970939922914, 0.24584506209284596, 0.23715807899250618, 0.22458765394378144, 0.22552820436401622, 0.221700234363446, 0.21666719179712746, 0.217610020615573, 0.23952869973994007, 0.2175996018180496, 0.21063546682925727, 0.19294549826266683, 0.20811184248952563, 0.18697011395742646, 0.19268666699621806, 0.18350348371928615, 0.18334016483554505, 0.18739573777226892, 0.18188252259664728, 0.19047019575961466, 0.18134662809959423, 0.18428861752920522, 0.46585320433254873, 0.4229082915417681, 0.29431283119820495, 0.4040530141222397, 0.4230086113634318, 0.44570605347310854, 0.35243116379622885, 0.393283688297834, 0.3496085791540028, 0.33352623603366305, 0.2902223189132829, 0.5482860526589174, 0.3297570877829932, 0.2855058311747356, 0.2961741924375889, 0.21598208399766683, 0.2720804532158997, 0.2594642507316819, 0.17610808770189956, 0.1861045890624835, 0.18604970679992716, 0.18213788094348538, 0.17591070229450856, 0.2143772314970106, 0.1719197875485503, 0.1940237578472438, 0.1875105334900652, 0.08138880152267425, 0.08083375614810795, 0.07534959773352945, 0.09562999586129939, 0.08700132898672441, 0.07944547081539899, 0.08306687942300706, 0.0820807790568262, 0.08421949393963934]}, "mutation_prompt": null}
{"id": "b6e53236-b0f9-4afd-bccc-5b2fe212dfdb", "solution": "import numpy as np\n\nclass EnhancedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = 50  # Increased population size\n        self.mutation_factor = 0.7  # Adjusted mutation factor\n        self.crossover_probability = 0.8  # Adjusted crossover probability\n        self.learning_rate = 0.05  # Reduced learning rate\n        self.population = None\n        self.fitness = None\n        self.momentum = 0.9  # New momentum term for updates\n\n    def initialize_population(self):\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, \n                                            (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def adaptive_differential_evolution(self, idx, func):\n        indices = [i for i in range(self.population_size) if i != idx]\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        adaptive_factor = 0.4 + 0.6 * np.random.rand()  # Adjusted adaptive factor range\n        mutant = self.population[idx] + adaptive_factor * (mutant - self.population[idx])\n        mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n        \n        cross_points = np.random.rand(self.dim) < self.crossover_probability\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        \n        trial = np.where(cross_points, mutant, self.population[idx])\n        trial_fitness = func(trial)\n        \n        if trial_fitness < self.fitness[idx]:\n            self.population[idx] = trial\n            self.fitness[idx] = trial_fitness\n\n    def randomized_quasi_newton_update(self, idx, func):\n        gradient = np.random.uniform(-1, 1, self.dim)\n        hessian_approx = np.random.uniform(0.1, 0.4, self.dim)  # Slightly adjusted approximation\n        candidate = self.population[idx] - self.learning_rate * gradient / (np.abs(hessian_approx) + 1e-9)\n        candidate = np.clip(candidate, self.lower_bound, self.upper_bound)\n        candidate_fitness = func(candidate)\n        if candidate_fitness < self.fitness[idx]:\n            self.population[idx] = candidate\n            self.fitness[idx] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population()\n        evaluations = 0\n        self.evaluate_population(func)\n        evaluations += self.population_size\n\n        momentum_updates = np.zeros_like(self.population)\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                self.adaptive_differential_evolution(i, func)\n                momentum_updates[i] = self.momentum * momentum_updates[i] + (1 - self.momentum) * (self.population[i] - momentum_updates[i])\n                self.population[i] += momentum_updates[i]\n                evaluations += 1\n\n                if evaluations >= self.budget:\n                    break\n\n                self.randomized_quasi_newton_update(i, func)\n                evaluations += 1\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "EnhancedHybridOptimizer", "description": "An enhanced hybrid optimizer combining Adaptive Differential Evolution and Randomized Quasi-Newton updates, with refined exploration and exploitation for diverse optimization landscapes.", "configspace": "", "generation": 13, "fitness": 0.11084300080201596, "feedback": "The algorithm EnhancedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.11 with standard deviation 0.19.", "error": "", "parent_id": "6a904b7a-737d-497b-b2e1-0c5368eca0d0", "metadata": {"aucs": [0.1308339085162752, 0.13429821741447323, 0.1291953318567879, 0.16003947357945802, 0.17558215234450547, 0.15430775727558577, 0.1648956027219498, 0.1558410510550191, 0.13573347092233345, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.03226178334573593, 0.04530190347781826, 0.01784260054057918, 0.03220891634327894, 0.0380886918886858, 0.02854528953270763, 0.02807043842721213, 0.03240341944950664, 0.031402095125418916, 0.03353957687521503, 0.02467915773033824, 0.018361622341661166, 0.026206508922414273, 0.017808507226010795, 0.014186869627824117, 0.03191642958683283, 0.009023009534138149, 0.005000207557288583, 0.9569689431138636, 0.9598035819230445, 0.9422708993996027, 0.9620259894324794, 0.947316820365798, 0.9407240484050524, 0.9282939150481122, 0.9454032957694108, 0.9443677724149424, 0.06537478115085382, 0.03662218686472829, 0.034069883152796354, 0.06918511364461555, 0.02993586659524039, 0.032209277667557434, 0.038027187007191654, 0.05948472243913894, 0.06164649898588814, 0.10598695076352072, 0.11499382082419851, 0.07622419050821261, 0.1261766276285623, 0.10721685025945604, 0.11502957459788354, 0.11728698106897295, 0.18762966116159885, 0.1028657986025393, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.008425738515919656, 0.006951406087972289, 0.008493085291748526, 9.999999999998899e-05, 0.04471697778921169, 9.999999999998899e-05, 9.999999999998899e-05, 0.08072692972610362, 0.0026521578098228638, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.059800195648249455, 0.04707344663436597, 9.999999999998899e-05, 0.05881041634339523, 0.027175388974155834, 0.01477305474302848, 0.03426738767337101, 0.013135625078700808, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.004088898966951082, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.14241609732746563, 0.15863301050278955, 0.1504124097404972, 0.16356363383384287, 0.1671304507596062, 0.1528737284376126, 0.1666758253980588, 0.23078472012894213, 0.15179487851803075, 0.04000297711635703, 0.03515958809570674, 0.02964892851470735, 0.035497948062630846, 0.04843478751123709, 0.03681688614277323, 0.0457472927635767, 0.05277284729958576, 0.0267656688986897, 0.11649582212510157, 0.13349679816771498, 0.11842175776965747, 0.13430829256138654, 0.10852972522114013, 0.10798713552628325, 0.12365108606447262, 0.1278570600559289, 0.13455517479922452, 0.1855249332992127, 0.14690636667853563, 0.1613932299901244, 0.152420368499811, 0.16576170063670248, 0.13142059265937855, 0.1344732736721984, 0.17377774300845283, 0.16070053950877428, 0.1012687762218607, 0.09814578067667556, 0.08398582550377798, 0.07316035393543463, 0.08308398590503052, 0.09112693303539654, 0.09086337079322726, 0.10984067293824906, 0.0820619318020066, 0.2257909959799711, 0.12773292223682342, 0.16250452004539107, 0.16005352077438928, 0.12443183182304807, 0.12009110353738273, 0.13755030462960172, 0.14369610402082367, 0.13527558631997838, 0.13720949994555198, 0.1506381172006741, 9.999999999998899e-05, 0.13142890515432448, 0.15771310822590057, 0.006818991327182222, 0.14265215940410259, 0.14679823375873713, 9.999999999998899e-05, 0.13647181247464502, 0.1371000519599076, 0.11484470092324828, 0.1389647721451902, 0.1654198261209101, 0.11825402738972157, 0.11548828971512992, 0.14163151980537692, 0.11057282750009834, 0.11820109282593316, 0.11019888208018969, 0.16377844357655502, 0.07132454690163681, 0.07230878622151671, 0.16947047759324707, 0.09802551039611784, 0.18259529765329374, 0.15236633084841855, 0.1686811160416568, 0.17314271283946658, 0.17588153857043376, 0.1716329770480748, 0.17538896908827628, 0.16328192195609137, 0.19598568986210974, 0.20524051429308832, 0.1979739599947561, 0.05851080357956984, 0.04409998900592016, 0.04778743304479616, 0.04681102607767462, 0.045474093478907185, 0.045188506048502664, 0.03349346944879539, 0.0417442871268251, 0.027109213810302735]}, "mutation_prompt": null}
{"id": "7fd41505-3c71-4e06-a115-2d15f1bf9f35", "solution": "import numpy as np\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = 40\n        self.mutation_factor = 0.8\n        self.crossover_probability = 0.9\n        self.learning_rate = 0.1\n        self.population = None\n        self.fitness = None\n\n    def initialize_population(self):\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, \n                                            (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def adaptive_differential_evolution(self, idx, func):\n        indices = [i for i in range(self.population_size) if i != idx]\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        adaptive_factor = 0.5 + 0.5 * np.random.rand()\n        mutant = self.population[idx] + adaptive_factor * (mutant - self.population[idx])\n        mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n        \n        cross_points = np.random.rand(self.dim) < self.crossover_probability\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        \n        trial = np.where(cross_points, mutant, self.population[idx])\n        trial_fitness = func(trial)\n        \n        if trial_fitness < self.fitness[idx]:\n            self.population[idx] = trial\n            self.fitness[idx] = trial_fitness\n\n    def randomized_newton_update(self, idx, func):\n        gradient = np.random.uniform(-1, 1, self.dim)\n        hessian_approx = np.random.uniform(0.1, 0.5, self.dim)\n        candidate = self.population[idx] - self.learning_rate * gradient / (np.abs(hessian_approx) + 1e-9)\n        candidate = np.clip(candidate, self.lower_bound, self.upper_bound)\n        candidate_fitness = func(candidate)\n        if candidate_fitness < self.fitness[idx]:\n            self.population[idx] = candidate\n            self.fitness[idx] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population()\n        evaluations = 0\n        self.evaluate_population(func)\n        evaluations += self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                self.adaptive_differential_evolution(i, func)\n                evaluations += 1\n\n                if evaluations >= self.budget:\n                    break\n\n                self.randomized_newton_update(i, func)\n                evaluations += 1\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "AdaptiveHybridOptimizer", "description": "A novel hybrid optimization algorithm combining Adaptive Differential Evolution and Randomized Newton-like updates for efficient exploration and exploitation across diverse landscapes.", "configspace": "", "generation": 7, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "6a904b7a-737d-497b-b2e1-0c5368eca0d0", "metadata": {"aucs": [0.7810487762994811, 0.7737201709897309, 0.7984387123770895, 0.7474584068150549, 0.7893206249621149, 0.7810859249864255, 0.7651870047122677, 0.7637265331123596, 0.7544685530913278, 0.5745789060628225, 0.5568681199286232, 0.5923117409603715, 0.5867524787457852, 0.5588743250787239, 0.5727217353505709, 0.564497454703493, 0.5670209119249587, 0.5622127176723783, 0.10645007403955864, 0.10531591371851878, 0.10104588105069745, 0.12068071889648146, 0.104745313274058, 0.11475897315854577, 0.1036030526972892, 0.10418699385194452, 0.11564689102372794, 0.08915452135963042, 0.09036755158695209, 0.08605412146502023, 0.09209907981052001, 0.10215945806999516, 0.10112576268436513, 0.08113886941620818, 0.10085867441467866, 0.09448134176050316, 0.8854041122210992, 0.8450598443123811, 0.8045587166262524, 0.8475249501073782, 0.7906732554426446, 0.841626290225768, 0.8559539765904275, 0.8290424812601958, 0.8548272959571771, 0.3500139022371357, 0.3365419279069296, 0.3673658387342146, 0.38585778122735126, 0.3416543941999345, 0.3633605688963323, 0.31667506736193796, 0.3596771268188682, 0.37621268434668087, 0.7094126709237628, 0.6642717545892044, 0.7701150693052059, 0.7509132471346256, 0.7376084167694179, 0.7297780729549188, 0.7190871088720918, 0.687974617485412, 0.758968088817827, 0.1908801089794614, 0.1517127154599499, 0.1686397066901416, 0.17866888305762463, 0.242288603051677, 0.2638425564904663, 0.2229599553839462, 0.20076687819041095, 0.19593545634751364, 0.2343886605917752, 0.22531574876774763, 0.17625683222767297, 0.1955842840049694, 0.16973369324425436, 0.17910478209184455, 0.15326325686486075, 0.18099100509510058, 0.1509954393258326, 0.3615727583601135, 0.3235366556602163, 0.2975550934493706, 0.4052246037041095, 0.340788219764936, 0.31026755650887083, 0.3569595340176651, 0.36046757066857216, 0.35551196975396393, 0.4365316406164489, 0.5003887572451153, 0.48802112804753084, 0.454058276620417, 0.41123218678163553, 0.46426014387650116, 0.5316807812961412, 0.48454935010302613, 0.47725159673689754, 0.0996550641492755, 0.09744041393381808, 0.08758765713846406, 0.19648404038841327, 0.2028755357362937, 0.18506160268379912, 0.14796105930588654, 0.1327553911629471, 0.11862990580753685, 0.2651070379874545, 0.2860808352550124, 0.2861889412207068, 0.2943248931683756, 0.29881004133841915, 0.2956038879497268, 0.2820204209879317, 0.28218445436864736, 0.27131605941447756, 0.6604987905145381, 0.678955693812864, 0.6714016916242234, 0.6540933045348184, 0.644355241223366, 0.6604037364697297, 0.6461454929183823, 0.6777693333654089, 0.6919514842888317, 0.10895545731406653, 0.10366554215241375, 0.10414424980464088, 0.09905620517130498, 0.10524027368287792, 0.10015916590413365, 0.09124401115614422, 0.1107981254371232, 0.10179175808254892, 0.18467089112199042, 0.21259419915332634, 0.18770095922969743, 0.1658915836510021, 0.21721542075067468, 0.178781001606784, 0.1764057024621839, 0.19332285067087462, 0.19151095069324642, 0.35636426189943393, 0.3395450892395795, 0.3381264638524698, 0.3477240450449747, 0.3589652216995053, 0.3172612362278122, 0.3601306082743758, 0.36719521235558306, 0.37385246459663535, 0.29405963976866867, 0.2751206502786776, 0.27694231547097903, 0.28317780298785133, 0.2872164489468654, 0.2711396238397893, 0.263915956177461, 0.28760465968960414, 0.28056920710073674, 0.2112936102341484, 0.2064439874832209, 0.2063523841729491, 0.20734297985532146, 0.21629586977148674, 0.1980611890914219, 0.19161757525233147, 0.20912604076350916, 0.20683231848472283, 0.17654194682066793, 0.1909645057058449, 0.18361258283180082, 0.18372617960254733, 0.18866369160915086, 0.19187204579018624, 0.18150312469871188, 0.1865172298304969, 0.17545999362287312, 0.34419280001928676, 0.38719486935107417, 0.41695586794295525, 0.7264284984786947, 0.4113612324847006, 0.5721681590559813, 0.39979278519186323, 0.42121941708709265, 0.5838123745146581, 0.551214889780775, 0.43375498388803024, 0.25353520998947643, 0.5769025793102873, 0.29239937524698345, 0.3920150295345829, 0.40055467629442243, 0.2047291792771042, 0.2023666376633726, 0.20218569955934396, 0.19029231565765736, 0.1764943234515145, 0.19960538151715823, 0.1847146527197573, 0.1900736928732425, 0.18399209539069494, 0.1914989806452514, 0.17446921909487245, 0.09611047585459764, 0.09388395179953968, 0.07263957201672688, 0.08324402036538248, 0.09301557406941863, 0.0816197213135691, 0.0761938554643724, 0.07920831986013421, 0.07609546187946381]}, "mutation_prompt": null}
{"id": "023368ef-5d52-4414-868c-a5dc8216c134", "solution": "import numpy as np\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = 40\n        self.mutation_factor = 0.8\n        self.crossover_probability = 0.9\n        self.learning_rate = 0.1\n        self.population = None\n        self.fitness = None\n\n    def initialize_population(self):\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, \n                                            (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def adaptive_differential_evolution(self, idx, func):\n        indices = [i for i in range(self.population_size) if i != idx]\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        adaptive_factor = 0.5 + 0.5 * np.random.rand()\n        mutant = self.population[idx] + adaptive_factor * (mutant - self.population[idx])\n        mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n        \n        cross_points = np.random.rand(self.dim) < self.crossover_probability\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        \n        trial = np.where(cross_points, mutant, self.population[idx])\n        trial_fitness = func(trial)\n        \n        if trial_fitness < self.fitness[idx]:\n            self.population[idx] = trial\n            self.fitness[idx] = trial_fitness\n\n    def randomized_newton_update(self, idx, func):\n        gradient = np.random.uniform(-1, 1, self.dim)\n        hessian_approx = np.random.uniform(0.1, 0.5, self.dim)\n        candidate = self.population[idx] - self.learning_rate * gradient / (np.abs(hessian_approx) + 1e-9)\n        candidate = np.clip(candidate, self.lower_bound, self.upper_bound)\n        candidate_fitness = func(candidate)\n        if candidate_fitness < self.fitness[idx]:\n            self.population[idx] = candidate\n            self.fitness[idx] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population()\n        evaluations = 0\n        self.evaluate_population(func)\n        evaluations += self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                self.adaptive_differential_evolution(i, func)\n                evaluations += 1\n\n                if evaluations >= self.budget:\n                    break\n\n                self.randomized_newton_update(i, func)\n                evaluations += 1\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "AdaptiveHybridOptimizer", "description": "A novel hybrid optimization algorithm combining Adaptive Differential Evolution and Randomized Newton-like updates for efficient exploration and exploitation across diverse landscapes.", "configspace": "", "generation": 7, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "6a904b7a-737d-497b-b2e1-0c5368eca0d0", "metadata": {"aucs": [0.7810487762994811, 0.7737201709897309, 0.7984387123770895, 0.7474584068150549, 0.7893206249621149, 0.7810859249864255, 0.7651870047122677, 0.7637265331123596, 0.7544685530913278, 0.5745789060628225, 0.5568681199286232, 0.5923117409603715, 0.5867524787457852, 0.5588743250787239, 0.5727217353505709, 0.564497454703493, 0.5670209119249587, 0.5622127176723783, 0.10645007403955864, 0.10531591371851878, 0.10104588105069745, 0.12068071889648146, 0.104745313274058, 0.11475897315854577, 0.1036030526972892, 0.10418699385194452, 0.11564689102372794, 0.08915452135963042, 0.09036755158695209, 0.08605412146502023, 0.09209907981052001, 0.10215945806999516, 0.10112576268436513, 0.08113886941620818, 0.10085867441467866, 0.09448134176050316, 0.8854041122210992, 0.8450598443123811, 0.8045587166262524, 0.8475249501073782, 0.7906732554426446, 0.841626290225768, 0.8559539765904275, 0.8290424812601958, 0.8548272959571771, 0.3500139022371357, 0.3365419279069296, 0.3673658387342146, 0.38585778122735126, 0.3416543941999345, 0.3633605688963323, 0.31667506736193796, 0.3596771268188682, 0.37621268434668087, 0.7094126709237628, 0.6642717545892044, 0.7701150693052059, 0.7509132471346256, 0.7376084167694179, 0.7297780729549188, 0.7190871088720918, 0.687974617485412, 0.758968088817827, 0.1908801089794614, 0.1517127154599499, 0.1686397066901416, 0.17866888305762463, 0.242288603051677, 0.2638425564904663, 0.2229599553839462, 0.20076687819041095, 0.19593545634751364, 0.2343886605917752, 0.22531574876774763, 0.17625683222767297, 0.1955842840049694, 0.16973369324425436, 0.17910478209184455, 0.15326325686486075, 0.18099100509510058, 0.1509954393258326, 0.3615727583601135, 0.3235366556602163, 0.2975550934493706, 0.4052246037041095, 0.340788219764936, 0.31026755650887083, 0.3569595340176651, 0.36046757066857216, 0.35551196975396393, 0.4365316406164489, 0.5003887572451153, 0.48802112804753084, 0.454058276620417, 0.41123218678163553, 0.46426014387650116, 0.5316807812961412, 0.48454935010302613, 0.47725159673689754, 0.0996550641492755, 0.09744041393381808, 0.08758765713846406, 0.19648404038841327, 0.2028755357362937, 0.18506160268379912, 0.14796105930588654, 0.1327553911629471, 0.11862990580753685, 0.2651070379874545, 0.2860808352550124, 0.2861889412207068, 0.2943248931683756, 0.29881004133841915, 0.2956038879497268, 0.2820204209879317, 0.28218445436864736, 0.27131605941447756, 0.6604987905145381, 0.678955693812864, 0.6714016916242234, 0.6540933045348184, 0.644355241223366, 0.6604037364697297, 0.6461454929183823, 0.6777693333654089, 0.6919514842888317, 0.10895545731406653, 0.10366554215241375, 0.10414424980464088, 0.09905620517130498, 0.10524027368287792, 0.10015916590413365, 0.09124401115614422, 0.1107981254371232, 0.10179175808254892, 0.18467089112199042, 0.21259419915332634, 0.18770095922969743, 0.1658915836510021, 0.21721542075067468, 0.178781001606784, 0.1764057024621839, 0.19332285067087462, 0.19151095069324642, 0.35636426189943393, 0.3395450892395795, 0.3381264638524698, 0.3477240450449747, 0.3589652216995053, 0.3172612362278122, 0.3601306082743758, 0.36719521235558306, 0.37385246459663535, 0.29405963976866867, 0.2751206502786776, 0.27694231547097903, 0.28317780298785133, 0.2872164489468654, 0.2711396238397893, 0.263915956177461, 0.28760465968960414, 0.28056920710073674, 0.2112936102341484, 0.2064439874832209, 0.2063523841729491, 0.20734297985532146, 0.21629586977148674, 0.1980611890914219, 0.19161757525233147, 0.20912604076350916, 0.20683231848472283, 0.17654194682066793, 0.1909645057058449, 0.18361258283180082, 0.18372617960254733, 0.18866369160915086, 0.19187204579018624, 0.18150312469871188, 0.1865172298304969, 0.17545999362287312, 0.34419280001928676, 0.38719486935107417, 0.41695586794295525, 0.7264284984786947, 0.4113612324847006, 0.5721681590559813, 0.39979278519186323, 0.42121941708709265, 0.5838123745146581, 0.551214889780775, 0.43375498388803024, 0.25353520998947643, 0.5769025793102873, 0.29239937524698345, 0.3920150295345829, 0.40055467629442243, 0.2047291792771042, 0.2023666376633726, 0.20218569955934396, 0.19029231565765736, 0.1764943234515145, 0.19960538151715823, 0.1847146527197573, 0.1900736928732425, 0.18399209539069494, 0.1914989806452514, 0.17446921909487245, 0.09611047585459764, 0.09388395179953968, 0.07263957201672688, 0.08324402036538248, 0.09301557406941863, 0.0816197213135691, 0.0761938554643724, 0.07920831986013421, 0.07609546187946381]}, "mutation_prompt": null}
{"id": "53ea34aa-c756-4ae9-a23c-5e8e36df005d", "solution": "import numpy as np\n\nclass EnhancedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = 50\n        self.base_mutation_factor = 0.5\n        self.crossover_probability = 0.85\n        self.learning_rate = 0.05\n        self.population = None\n        self.fitness = None\n\n    def initialize_population(self):\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, \n                                            (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def adaptive_differential_evolution(self, idx, func):\n        indices = [i for i in range(self.population_size) if i != idx]\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        mutation_factor = self.base_mutation_factor + 0.4 * np.random.rand()\n        mutant = self.population[a] + mutation_factor * (self.population[b] - self.population[c])\n        mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n        \n        cross_points = np.random.rand(self.dim) < self.crossover_probability\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        \n        trial = np.where(cross_points, mutant, self.population[idx])\n        trial_fitness = func(trial)\n        \n        if trial_fitness < self.fitness[idx]:\n            self.population[idx] = trial\n            self.fitness[idx] = trial_fitness\n\n    def reinforced_newton_update(self, idx, func):\n        gradient = np.random.uniform(-1, 1, self.dim)\n        noise_term = np.random.normal(0, 0.1, self.dim)\n        candidate = self.population[idx] - self.learning_rate * gradient + noise_term\n        candidate = np.clip(candidate, self.lower_bound, self.upper_bound)\n        candidate_fitness = func(candidate)\n        if candidate_fitness < self.fitness[idx]:\n            self.population[idx] = candidate\n            self.fitness[idx] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population()\n        evaluations = 0\n        self.evaluate_population(func)\n        evaluations += self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                self.adaptive_differential_evolution(i, func)\n                evaluations += 1\n\n                if evaluations >= self.budget:\n                    break\n\n                self.reinforced_newton_update(i, func)\n                evaluations += 1\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "EnhancedHybridOptimizer", "description": "An enhanced hybrid optimizer integrating self-adaptive mechanisms in Differential Evolution and Reinforced Newton updates for balanced exploration-exploitation dynamics.", "configspace": "", "generation": 16, "fitness": 0.23671725955159428, "feedback": "The algorithm EnhancedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.24 with standard deviation 0.19.", "error": "", "parent_id": "6a904b7a-737d-497b-b2e1-0c5368eca0d0", "metadata": {"aucs": [0.5577006217956434, 0.5326534093607058, 0.5458037363556576, 0.5434342436870441, 0.5593558497876582, 0.5695215821404123, 0.5915015773642368, 0.572146378904106, 0.5733930488996444, 0.22463534140454744, 0.23098940904209397, 0.22915049955471456, 0.26726626598693837, 0.24017652779331533, 0.2509218551028495, 0.27154003753567546, 0.2687538614574245, 0.24082183670669588, 0.10536641470269881, 0.1127364799394509, 0.10210403392805933, 0.11351641432380677, 0.10932887304425565, 0.10760402603426933, 0.1004331811231658, 0.11622364988446021, 0.11437379463052477, 0.09015808056309127, 0.09350277541984797, 0.08967267506124466, 0.09125873235967485, 0.10005849778438092, 0.09004945362967409, 0.09183318144257246, 0.09758105256914662, 0.08816710476219702, 0.9299238202095432, 0.9148020193966069, 0.8806065712800796, 0.9110450897725247, 0.9189836677361435, 0.8782338725359855, 0.9169762521805414, 0.8586657598895086, 0.9252181246524004, 0.2405731163419611, 0.2205641377527633, 0.21461425705253134, 0.268227893136036, 0.21811537310770557, 0.24832633907371449, 0.252576933944218, 0.23113302461517882, 0.23985294038078897, 0.3655926935302829, 0.3703740289549714, 0.39243443479439255, 0.44307293518533264, 0.3693663536114675, 0.3572402701600056, 0.2911857958621875, 0.31917799121109014, 0.4474615439517615, 0.16410369956187565, 0.12282429152430374, 0.1490196806091122, 0.1769064047514165, 0.1478650906789709, 0.149255156233563, 0.1406945761460715, 0.16535316270131917, 0.14818408172560738, 0.17644893400598993, 0.1705828832987627, 0.13681186516665844, 0.15428481065759647, 0.16149325345247212, 0.18611096462023435, 0.15081451080385655, 0.12057336139541353, 0.14473743876097944, 0.07132280488798626, 0.07096296951620973, 0.06440651195901759, 0.07522972654837978, 0.06153021617411647, 0.04973341645475826, 0.07215259201175683, 0.08291683141056327, 0.04138584392793909, 0.17892565628841417, 0.17296144700089422, 0.19615125097682806, 0.18713291883682304, 0.18284284735023593, 0.15578429449499076, 0.23749681375122333, 0.2022025858061921, 0.20414833129003085, 0.017035284008916407, 0.009784786888517139, 0.01255683485166359, 0.024968719006292273, 0.0033847673790858845, 0.0051754294383421495, 0.026302725221621026, 0.01927738026967274, 0.005742148587168261, 0.14705574632443763, 0.12230902727328574, 0.12178057676487641, 0.12146842542141567, 0.132800722507176, 0.12848534860998895, 0.11011142715840538, 0.12577808112103883, 0.11531933499071734, 0.4266610301594076, 0.4247224098782256, 0.4242563217418236, 0.4326467007234325, 0.44057772693936226, 0.4512729555284688, 0.453152183912768, 0.4177689660049888, 0.4280483028776697, 0.10367561631539701, 0.09862655653570906, 0.1094270336705564, 0.10174435421325001, 0.09742399671273161, 0.09568204793030766, 0.11208139928437122, 0.093814494236801, 0.10417231809083494, 0.20113171913766137, 0.19162691563021172, 0.17585232914920357, 0.2178577814460958, 0.19123031139307267, 0.18469315821811583, 0.17568035624001077, 0.21093113576462375, 0.24000499732688263, 0.2463458549068127, 0.23084757150162427, 0.24640791964973918, 0.24764430492230882, 0.23516314173821085, 0.24257155829832755, 0.25666152482621707, 0.25388640828811126, 0.26140189227984667, 0.17445451625194697, 0.17923596309806944, 0.1647132887273257, 0.17582526710848834, 0.18694518712241004, 0.1750278808406982, 0.1821571410393823, 0.19302970235712869, 0.18714543035891584, 0.18458920697035086, 0.1908676344380058, 0.20348062695259028, 0.18695082350477288, 0.19048547681073746, 0.1916725557675908, 0.18180133882345262, 0.18031940482536268, 0.18777227632679794, 0.20574178751632877, 0.18032715366593444, 0.18594276886801775, 0.1915906192898622, 0.18185666933433953, 0.18915463169779434, 0.18685889480277362, 0.1857153144694147, 0.18374813451316774, 0.2118743947799936, 0.38698964040857076, 0.4149810238669408, 0.48252338744744927, 0.2501987883561142, 0.2780041470149046, 0.3098381122967141, 0.39189601343987657, 0.3630654572123613, 0.3558398405638804, 0.26217141024416024, 0.3700006287502168, 0.32628270860574626, 0.32953965915497374, 0.31094794049720864, 0.19890125649204182, 0.20340021223233284, 0.46586294245122695, 0.18734070364832078, 0.1837900593044185, 0.18729267207641676, 0.17409395025258323, 0.1942915048191891, 0.19125470179358783, 0.1792360952173061, 0.2021319244185632, 0.20799318712478243, 0.06691251630168149, 0.07292822323044879, 0.07041492323271636, 0.07087183380889406, 0.0686966753427003, 0.0790468349019533, 0.06655268852995799, 0.07062270054805453, 0.08126857881844296]}, "mutation_prompt": null}
{"id": "a218052b-9c76-4ef0-b0a9-f7a9e5b4b0b7", "solution": "import numpy as np\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = 40\n        self.mutation_factor = 0.8\n        self.crossover_probability = 0.9\n        self.learning_rate = 0.1\n        self.population = None\n        self.fitness = None\n\n    def initialize_population(self):\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, \n                                            (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def adaptive_differential_evolution(self, idx, func):\n        indices = [i for i in range(self.population_size) if i != idx]\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        adaptive_factor = 0.5 + 0.5 * np.random.rand()\n        mutant = self.population[idx] + adaptive_factor * (mutant - self.population[idx])\n        mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n        \n        cross_points = np.random.rand(self.dim) < self.crossover_probability\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        \n        trial = np.where(cross_points, mutant, self.population[idx])\n        trial_fitness = func(trial)\n        \n        if trial_fitness < self.fitness[idx]:\n            self.population[idx] = trial\n            self.fitness[idx] = trial_fitness\n\n    def randomized_newton_update(self, idx, func):\n        gradient = np.random.uniform(-1, 1, self.dim)\n        hessian_approx = np.random.uniform(0.1, 0.5, self.dim)\n        candidate = self.population[idx] - self.learning_rate * gradient / (np.abs(hessian_approx) + 1e-9)\n        candidate = np.clip(candidate, self.lower_bound, self.upper_bound)\n        candidate_fitness = func(candidate)\n        if candidate_fitness < self.fitness[idx]:\n            self.population[idx] = candidate\n            self.fitness[idx] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population()\n        evaluations = 0\n        self.evaluate_population(func)\n        evaluations += self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                self.adaptive_differential_evolution(i, func)\n                evaluations += 1\n\n                if evaluations >= self.budget:\n                    break\n\n                self.randomized_newton_update(i, func)\n                evaluations += 1\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "AdaptiveHybridOptimizer", "description": "A novel hybrid optimization algorithm combining Adaptive Differential Evolution and Randomized Newton-like updates for efficient exploration and exploitation across diverse landscapes.", "configspace": "", "generation": 7, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "6a904b7a-737d-497b-b2e1-0c5368eca0d0", "metadata": {"aucs": [0.7810487762994811, 0.7737201709897309, 0.7984387123770895, 0.7474584068150549, 0.7893206249621149, 0.7810859249864255, 0.7651870047122677, 0.7637265331123596, 0.7544685530913278, 0.5745789060628225, 0.5568681199286232, 0.5923117409603715, 0.5867524787457852, 0.5588743250787239, 0.5727217353505709, 0.564497454703493, 0.5670209119249587, 0.5622127176723783, 0.10645007403955864, 0.10531591371851878, 0.10104588105069745, 0.12068071889648146, 0.104745313274058, 0.11475897315854577, 0.1036030526972892, 0.10418699385194452, 0.11564689102372794, 0.08915452135963042, 0.09036755158695209, 0.08605412146502023, 0.09209907981052001, 0.10215945806999516, 0.10112576268436513, 0.08113886941620818, 0.10085867441467866, 0.09448134176050316, 0.8854041122210992, 0.8450598443123811, 0.8045587166262524, 0.8475249501073782, 0.7906732554426446, 0.841626290225768, 0.8559539765904275, 0.8290424812601958, 0.8548272959571771, 0.3500139022371357, 0.3365419279069296, 0.3673658387342146, 0.38585778122735126, 0.3416543941999345, 0.3633605688963323, 0.31667506736193796, 0.3596771268188682, 0.37621268434668087, 0.7094126709237628, 0.6642717545892044, 0.7701150693052059, 0.7509132471346256, 0.7376084167694179, 0.7297780729549188, 0.7190871088720918, 0.687974617485412, 0.758968088817827, 0.1908801089794614, 0.1517127154599499, 0.1686397066901416, 0.17866888305762463, 0.242288603051677, 0.2638425564904663, 0.2229599553839462, 0.20076687819041095, 0.19593545634751364, 0.2343886605917752, 0.22531574876774763, 0.17625683222767297, 0.1955842840049694, 0.16973369324425436, 0.17910478209184455, 0.15326325686486075, 0.18099100509510058, 0.1509954393258326, 0.3615727583601135, 0.3235366556602163, 0.2975550934493706, 0.4052246037041095, 0.340788219764936, 0.31026755650887083, 0.3569595340176651, 0.36046757066857216, 0.35551196975396393, 0.4365316406164489, 0.5003887572451153, 0.48802112804753084, 0.454058276620417, 0.41123218678163553, 0.46426014387650116, 0.5316807812961412, 0.48454935010302613, 0.47725159673689754, 0.0996550641492755, 0.09744041393381808, 0.08758765713846406, 0.19648404038841327, 0.2028755357362937, 0.18506160268379912, 0.14796105930588654, 0.1327553911629471, 0.11862990580753685, 0.2651070379874545, 0.2860808352550124, 0.2861889412207068, 0.2943248931683756, 0.29881004133841915, 0.2956038879497268, 0.2820204209879317, 0.28218445436864736, 0.27131605941447756, 0.6604987905145381, 0.678955693812864, 0.6714016916242234, 0.6540933045348184, 0.644355241223366, 0.6604037364697297, 0.6461454929183823, 0.6777693333654089, 0.6919514842888317, 0.10895545731406653, 0.10366554215241375, 0.10414424980464088, 0.09905620517130498, 0.10524027368287792, 0.10015916590413365, 0.09124401115614422, 0.1107981254371232, 0.10179175808254892, 0.18467089112199042, 0.21259419915332634, 0.18770095922969743, 0.1658915836510021, 0.21721542075067468, 0.178781001606784, 0.1764057024621839, 0.19332285067087462, 0.19151095069324642, 0.35636426189943393, 0.3395450892395795, 0.3381264638524698, 0.3477240450449747, 0.3589652216995053, 0.3172612362278122, 0.3601306082743758, 0.36719521235558306, 0.37385246459663535, 0.29405963976866867, 0.2751206502786776, 0.27694231547097903, 0.28317780298785133, 0.2872164489468654, 0.2711396238397893, 0.263915956177461, 0.28760465968960414, 0.28056920710073674, 0.2112936102341484, 0.2064439874832209, 0.2063523841729491, 0.20734297985532146, 0.21629586977148674, 0.1980611890914219, 0.19161757525233147, 0.20912604076350916, 0.20683231848472283, 0.17654194682066793, 0.1909645057058449, 0.18361258283180082, 0.18372617960254733, 0.18866369160915086, 0.19187204579018624, 0.18150312469871188, 0.1865172298304969, 0.17545999362287312, 0.34419280001928676, 0.38719486935107417, 0.41695586794295525, 0.7264284984786947, 0.4113612324847006, 0.5721681590559813, 0.39979278519186323, 0.42121941708709265, 0.5838123745146581, 0.551214889780775, 0.43375498388803024, 0.25353520998947643, 0.5769025793102873, 0.29239937524698345, 0.3920150295345829, 0.40055467629442243, 0.2047291792771042, 0.2023666376633726, 0.20218569955934396, 0.19029231565765736, 0.1764943234515145, 0.19960538151715823, 0.1847146527197573, 0.1900736928732425, 0.18399209539069494, 0.1914989806452514, 0.17446921909487245, 0.09611047585459764, 0.09388395179953968, 0.07263957201672688, 0.08324402036538248, 0.09301557406941863, 0.0816197213135691, 0.0761938554643724, 0.07920831986013421, 0.07609546187946381]}, "mutation_prompt": null}
{"id": "390fb10b-b1a7-4fda-bfce-d7b6d4431e0f", "solution": "import numpy as np\n\nclass RefinedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = 40\n        self.mutation_factor = 0.8\n        self.crossover_probability = 0.9\n        self.learning_rate = 0.1\n        self.population = None\n        self.fitness = None\n\n    def initialize_population(self):\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, \n                                            (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def adaptive_differential_evolution(self, idx, func):\n        indices = [i for i in range(self.population_size) if i != idx]\n        np.random.shuffle(indices)\n        a, b, c = indices[:3]\n        mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        adaptive_factor = 0.5 + 0.5 * np.random.rand()\n        mutant = self.population[idx] + adaptive_factor * (mutant - self.population[idx])\n        mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n        \n        cross_points = np.random.rand(self.dim) < self.crossover_probability\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        \n        trial = np.where(cross_points, mutant, self.population[idx])\n        trial_fitness = func(trial)\n        \n        if trial_fitness < self.fitness[idx]:\n            self.population[idx] = trial\n            self.fitness[idx] = trial_fitness\n\n    def randomized_newton_update(self, idx, func):\n        gradient = np.random.uniform(-1, 1, self.dim)\n        adaptive_lr = self.learning_rate / (1 + idx / self.population_size)\n        hessian_approx = np.random.uniform(0.1, 0.5, self.dim)\n        candidate = self.population[idx] - adaptive_lr * gradient / (np.abs(hessian_approx) + 1e-9)\n        candidate = np.clip(candidate, self.lower_bound, self.upper_bound)\n        candidate_fitness = func(candidate)\n        if candidate_fitness < self.fitness[idx]:\n            self.population[idx] = candidate\n            self.fitness[idx] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population()\n        evaluations = 0\n        self.evaluate_population(func)\n        evaluations += self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                self.adaptive_differential_evolution(i, func)\n                evaluations += 1\n\n                if evaluations >= self.budget:\n                    break\n\n                self.randomized_newton_update(i, func)\n                evaluations += 1\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "RefinedHybridOptimizer", "description": "A refined hybrid optimization algorithm that enhances exploration through stochastic rank-based selection and incorporates adaptive learning rate adjustments for improved convergence.", "configspace": "", "generation": 18, "fitness": 0.34159824593705745, "feedback": "The algorithm RefinedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.34 with standard deviation 0.23.", "error": "", "parent_id": "6a904b7a-737d-497b-b2e1-0c5368eca0d0", "metadata": {"aucs": [0.7998926377262636, 0.761429726165544, 0.7863085107779331, 0.7508095948300958, 0.7882340583889212, 0.7816710181219415, 0.7826294649647488, 0.7918404048180104, 0.7676018171732987, 0.5689355814544976, 0.5786091347521873, 0.6064432841944956, 0.4875777471533822, 0.5880862128826672, 0.5875553949838622, 0.5452458885894269, 0.549792395120432, 0.5742927984304333, 0.10548247785135645, 0.11285201831791869, 0.0925734721780318, 0.11436849439951724, 0.10592419427335498, 0.11336423572081245, 0.10345862089413227, 0.1269952222453321, 0.12152214562989194, 0.10204477557831337, 0.09901425922888485, 0.09577110764085917, 0.09397947463880463, 0.09193976260094838, 0.09380899156179712, 0.09210822057585066, 0.09527236403031891, 0.09495208182919956, 0.8203943085242258, 0.8371989577902406, 0.8144215895957443, 0.8440204151740751, 0.7981195572547755, 0.8598124593300991, 0.8324337981573954, 0.8546195368656158, 0.849854255620719, 0.3666013280562487, 0.3725233049177048, 0.3395845401363392, 0.39396992596302094, 0.37524282476543214, 0.35070565435275614, 0.33407804648082917, 0.38226003237655404, 0.349245265497106, 0.7018061463567615, 0.7233415701332743, 0.7088757356079152, 0.7982178557801977, 0.7714881430535534, 0.7264322401069573, 0.795367319615778, 0.7599066331977806, 0.7086678887052003, 0.201764961314991, 0.17282079779941106, 0.2263590169873433, 0.2721596471019315, 0.27597767251362215, 0.17937872167142266, 0.16761630117773363, 0.24905357666002514, 0.16922513461462385, 0.16335003951102212, 0.20535196113100707, 0.13361225495437334, 0.1579703207595461, 0.16150244697886662, 0.2167787719170582, 0.18782402154756306, 0.19564428973151649, 0.18163669853366704, 0.31867100857758623, 0.3425741905828079, 0.3304444894524823, 0.34738093096977485, 0.2877568817466949, 0.3234807865621129, 0.33894048928278153, 0.37013426300073216, 0.3588389116713273, 0.5180632809666004, 0.4637458915590531, 0.5043407285905045, 0.33215873995771494, 0.4542336970383146, 0.4548165201136448, 0.5156739638315224, 0.5239241206078717, 0.4755558899035023, 0.11585943854457781, 0.1638375146855462, 0.09872669469579576, 0.21730694094700664, 0.16634037144172908, 0.1654849161870532, 0.11664820618779581, 0.07087408397891926, 0.10845489196991798, 0.2843152982061281, 0.28613726624777835, 0.28229693759480057, 0.3072011097040027, 0.2853119052426477, 0.2916314440888641, 0.2999473256283228, 0.2859594986946987, 0.26193620963585085, 0.6913140588308797, 0.6786867206260565, 0.701588295399665, 0.6541883452840307, 0.6654199785954649, 0.6475450237226962, 0.6671814869069398, 0.7055073070884146, 0.6638784336673579, 0.11391843993435358, 0.10975448936033483, 0.10090549793679549, 0.10310324236480084, 0.10791804393203874, 0.10939104607676131, 0.09581307616921564, 0.09812125343721878, 0.09985837034065093, 0.18846301470631488, 0.22839262856185694, 0.22322725974210345, 0.19757465926045115, 0.1701987471183407, 0.18919254140941477, 0.20027451142437336, 0.20007046061634493, 0.20073311465283272, 0.3501650996213105, 0.35164157461146006, 0.34813603073325927, 0.34536700185434244, 0.3655415017581666, 0.3583096257802938, 0.35581920313139404, 0.40530574007584763, 0.36866959956081435, 0.26557307404745645, 0.2697168709798672, 0.24942936856729292, 0.259704041706049, 0.2626717448788001, 0.2794983152026371, 0.24933057131184155, 0.29414825313596626, 0.24778132121106566, 0.1964865341256079, 0.20130664628203954, 0.2003458670310626, 0.23375686724462996, 0.21330797625742404, 0.2022932961455597, 0.2034346087052653, 0.20158084356029105, 0.23035297301018787, 0.19254916882434714, 0.18814946260488974, 0.19842901699570903, 0.184403561312654, 0.186854624925067, 0.18528907418469243, 0.20214940877697307, 0.18935042119784207, 0.18539220063005257, 0.32818920794091344, 0.5959495761773042, 0.3200074905287692, 0.6508833616229394, 0.39195391293278525, 0.506566629143708, 0.4000391686588708, 0.3511867539905471, 0.39761398923874924, 0.45995799055147635, 0.4459678532604766, 0.33988555794646136, 0.5390455573695724, 0.23998167446344754, 0.31350500582590435, 0.523108579970601, 0.20391488842316685, 0.24657744080176514, 0.17212424028899986, 0.19143516576879105, 0.17627569278737343, 0.19918041607754244, 0.2059953338045003, 0.175787862497925, 0.19782638442055023, 0.2069171764155977, 0.20245082333553344, 0.08338336975119987, 0.08271298565658503, 0.08769925944284895, 0.09209940270208461, 0.08249412618243479, 0.09027444882925406, 0.07892761994153641, 0.08207905467688215, 0.08015318768749546]}, "mutation_prompt": null}
{"id": "d22f0677-8fdf-4558-b28b-8ca05d90f26b", "solution": "import numpy as np\n\nclass EnhancedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = 40\n        self.mutation_factor = 0.8\n        self.initial_crossover_probability = 0.9\n        self.learning_rate = 0.1\n        self.population = None\n        self.fitness = None\n\n    def initialize_population(self):\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, \n                                            (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def chaotic_mutation_dynamics(self, idx):\n        chaotic_seq = np.random.rand(self.dim)\n        return np.sin(2 * np.pi * chaotic_seq) * self.mutation_factor\n\n    def adaptive_differential_evolution(self, idx, func):\n        indices = [i for i in range(self.population_size) if i != idx]\n        np.random.shuffle(indices)\n        a, b, c = indices[:3]\n        chaotic_mutation = self.chaotic_mutation_dynamics(idx)\n        mutant = self.population[a] + chaotic_mutation * (self.population[b] - self.population[c])\n        adaptive_factor = 0.5 + 0.5 * np.random.rand()\n        mutant = self.population[idx] + adaptive_factor * (mutant - self.population[idx])\n        mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n        \n        dynamic_crossover_prob = self.initial_crossover_probability * (1 - idx / self.population_size)\n        cross_points = np.random.rand(self.dim) < dynamic_crossover_prob\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        \n        trial = np.where(cross_points, mutant, self.population[idx])\n        trial_fitness = func(trial)\n        \n        if trial_fitness < self.fitness[idx]:\n            self.population[idx] = trial\n            self.fitness[idx] = trial_fitness\n\n    def randomized_newton_update(self, idx, func):\n        gradient = np.random.uniform(-1, 1, self.dim)\n        adaptive_lr = self.learning_rate / (1 + idx / self.population_size)\n        hessian_approx = np.random.uniform(0.1, 0.5, self.dim)\n        candidate = self.population[idx] - adaptive_lr * gradient / (np.abs(hessian_approx) + 1e-9)\n        candidate = np.clip(candidate, self.lower_bound, self.upper_bound)\n        candidate_fitness = func(candidate)\n        if candidate_fitness < self.fitness[idx]:\n            self.population[idx] = candidate\n            self.fitness[idx] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population()\n        evaluations = 0\n        self.evaluate_population(func)\n        evaluations += self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                self.adaptive_differential_evolution(i, func)\n                evaluations += 1\n\n                if evaluations >= self.budget:\n                    break\n\n                self.randomized_newton_update(i, func)\n                evaluations += 1\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "EnhancedHybridOptimizer", "description": "An enhanced hybrid optimizer that utilizes chaotic mutation dynamics and dynamic crossover rates alongside adaptive learning for superior convergence.", "configspace": "", "generation": 19, "fitness": 0.25123648074695926, "feedback": "The algorithm EnhancedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.25 with standard deviation 0.21.", "error": "", "parent_id": "390fb10b-b1a7-4fda-bfce-d7b6d4431e0f", "metadata": {"aucs": [0.7666990256273033, 0.738518146762739, 0.740715544064076, 0.7474933338749852, 0.7231963096716265, 0.7399351432420187, 0.7213933626814301, 0.7342706056909194, 0.7193590719933325, 0.5195915716416325, 0.4825633604598323, 0.46450973702677156, 0.49029306457592503, 0.49763339951481544, 0.4971243594565029, 0.4894997321600355, 0.5106452580939915, 0.4761848162392819, 0.1856598509900056, 0.1573949014019861, 0.17426405474909934, 0.17573491470763947, 0.17078877083128285, 0.16372584172824156, 0.1534534289683681, 0.17931483866800568, 0.17639075640612778, 0.1418790802087213, 0.13146684800863928, 0.13481166219060814, 0.14157501807134176, 0.13002587696526102, 0.12480061176485402, 0.13299086039154573, 0.13556733875525384, 0.13857453622054894, 0.8231147207592524, 0.8359738788721112, 0.8359649997029737, 0.8804277760217012, 0.8045669114143401, 0.8833866322434324, 0.8769640715463399, 0.8167482882017993, 0.850365144735284, 0.23892563456680505, 0.2156873953720999, 0.255110592249032, 0.271757261536394, 0.23288224546032466, 0.2539967363482192, 0.24196047356595274, 0.22314891274629867, 0.23906708284608502, 0.28226046343516886, 0.27585780269234117, 0.24227194218981696, 0.3013543930596476, 0.3046255220132341, 0.4202855454562059, 0.31756536131457447, 0.22832799378045832, 0.26569144080548146, 0.16230592534045396, 0.16200734772644632, 0.16509237710124203, 0.16510768224396266, 0.20346294181666624, 0.19826503643503846, 0.1612583762605171, 0.15355375520444015, 0.18261063776584385, 0.1685435927881921, 0.13473559726765505, 0.15998052755896297, 0.14621870172819107, 0.1769153719869201, 0.16439447873155955, 0.13353779367479113, 0.1546403137929756, 0.15457729298284162, 0.009066940018699787, 9.999999999998899e-05, 0.020616011794301836, 9.999999999998899e-05, 9.999999999998899e-05, 0.01035912014384599, 0.01476458919376944, 0.009938015325907656, 0.004293212887643838, 0.08465217795112201, 0.08142556386446864, 0.10986792644531373, 0.07561842657005169, 0.12146207052860247, 0.07956557597136904, 0.10664616247601522, 0.13518534903839063, 0.08097415315121492, 0.0057240300459594495, 9.999999999998899e-05, 9.999999999998899e-05, 0.0015493139554184632, 0.002074167918833747, 0.0026863012886040316, 0.010519453503064158, 0.007831971379883673, 9.999999999998899e-05, 0.10876790206276032, 0.09192482609866404, 0.08606094393207553, 0.062183871518331935, 0.12679057147522477, 0.10452097643596336, 0.108766865654633, 0.11341659084398414, 0.09567828948555812, 0.44300627922764857, 0.47247836209622407, 0.4470222472185297, 0.4484140047162579, 0.42325105104051686, 0.4495410987627415, 0.4433588060509046, 0.45155096239671944, 0.45213381530586993, 0.09540674198673604, 0.11774119035357855, 0.09688727337453329, 0.09301974267341939, 0.08640811454007957, 0.09958800634149867, 0.08860389408096725, 0.11789186124314033, 0.10342591090536168, 0.1985055161496112, 0.16784117222675632, 0.20828321442806008, 0.17858917818468578, 0.17819068903735802, 0.18979978035686396, 0.19286865591224345, 0.19701889997946076, 0.18861715391256828, 0.24093667064610502, 0.24329720563765722, 0.2570172978700651, 0.25749985517151586, 0.243504895132299, 0.27839190401083846, 0.2640024639659181, 0.2736136375544834, 0.2343775710477336, 0.16250294612201377, 0.17852307181243154, 0.17811310040026673, 0.19919928326226433, 0.21937090942937543, 0.20996953966789456, 0.17987291419312113, 0.1919522327129244, 0.18401726312965183, 0.21160856315495125, 0.19695960967966253, 0.18948940010494408, 0.19741707911968986, 0.19694687655718746, 0.21224492043597265, 0.19054361671610887, 0.20066777065910124, 0.2008885022024386, 0.20639074252700096, 0.2186005912801623, 0.2186503469860246, 0.20220998991944739, 0.21432556393334168, 0.1953275005687879, 0.2042095011042645, 0.19953115202910077, 0.19110096679132926, 0.1777965038980618, 0.4562794332652439, 0.31500426231749357, 0.4834902499378966, 0.48038716268099924, 0.5272530202376675, 0.437959948036874, 0.4389944014749041, 0.5075254437927543, 0.38100653447265254, 0.2109912317669429, 0.2963367247972106, 0.3163686302171831, 0.23330949635911225, 0.29362332596162377, 0.31442668818765585, 0.2630467604884681, 0.42146822554525776, 0.20030119969249804, 0.19711036213749755, 0.17922685896802526, 0.18479270387878677, 0.19414085755820998, 0.18330023835432008, 0.17442968981717366, 0.1775076753176521, 0.18520155372995528, 0.07765683301579773, 0.07302445315772488, 0.09050562965057929, 0.07842942026681954, 0.08539255262879997, 0.06922327210287094, 0.08445099122233024, 0.07439348885695396, 0.08293156140429458]}, "mutation_prompt": null}
{"id": "fe6d3e22-3a5a-4618-b7ba-26e94487d2f3", "solution": "import numpy as np\n\nclass EnhancedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = 50  # Adjusted for diversity\n        self.mutation_factor = 0.9  # Slightly increased to boost exploration\n        self.crossover_probability = 0.85  # Slightly decreased to promote stability\n        self.learning_rate = 0.05  # Reduced for finer adjustments\n        self.population = None\n        self.fitness = None\n\n    def initialize_population(self):\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, \n                                            (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def adaptive_differential_evolution(self, idx, func):\n        indices = [i for i in range(self.population_size) if i != idx]\n        np.random.shuffle(indices)\n        a, b, c = indices[:3]\n        mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        adaptive_factor = 0.6 + 0.4 * np.random.rand()  # Modified adaptive scaling\n        mutant = self.population[idx] + adaptive_factor * (mutant - self.population[idx])\n        mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n        \n        cross_points = np.random.rand(self.dim) < self.crossover_probability\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        \n        trial = np.where(cross_points, mutant, self.population[idx])\n        trial_fitness = func(trial)\n        \n        if trial_fitness < self.fitness[idx]:\n            self.population[idx] = trial\n            self.fitness[idx] = trial_fitness\n\n    def diversity_entropy_mutation(self, idx, func):\n        entropy = -np.sum((self.population - self.population.mean(axis=0))**2, axis=1)\n        diversity_factor = np.exp(-entropy / (self.population_size * self.dim))\n        mutation_vector = np.random.normal(0, diversity_factor, self.dim)\n        candidate = self.population[idx] + self.learning_rate * mutation_vector\n        candidate = np.clip(candidate, self.lower_bound, self.upper_bound)\n        candidate_fitness = func(candidate)\n        if candidate_fitness < self.fitness[idx]:\n            self.population[idx] = candidate\n            self.fitness[idx] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population()\n        evaluations = 0\n        self.evaluate_population(func)\n        evaluations += self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                self.adaptive_differential_evolution(i, func)\n                evaluations += 1\n\n                if evaluations >= self.budget:\n                    break\n\n                self.diversity_entropy_mutation(i, func)\n                evaluations += 1\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "EnhancedHybridOptimizer", "description": "An enhanced hybrid optimization algorithm integrating selective stochastic rank-based exploration with adaptive learning and diversity-driven mutation strategies for robust convergence.", "configspace": "", "generation": 20, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('shape mismatch: objects cannot be broadcast to a single shape.  Mismatch is between arg 0 with shape (5,) and arg 2 with shape (50,).').", "error": "ValueError('shape mismatch: objects cannot be broadcast to a single shape.  Mismatch is between arg 0 with shape (5,) and arg 2 with shape (50,).')", "parent_id": "390fb10b-b1a7-4fda-bfce-d7b6d4431e0f", "metadata": {}, "mutation_prompt": null}
{"id": "245a6f34-013b-426a-a5de-21d24577ab2e", "solution": "import numpy as np\n\nclass EnhancedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = 40\n        self.mutation_factor = 0.7\n        self.crossover_probability = 0.85\n        self.learning_rate = 0.1\n        self.population = None\n        self.fitness = None\n\n    def initialize_population(self):\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, \n                                            (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def adaptive_differential_evolution(self, idx, func):\n        indices = [i for i in range(self.population_size) if i != idx]\n        np.random.shuffle(indices)\n        a, b, c = indices[:3]\n        mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        adaptive_factor = 0.6 + 0.4 * np.random.rand()\n        mutant = self.population[idx] + adaptive_factor * (mutant - self.population[idx])\n        mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n        \n        cross_points = np.random.rand(self.dim) < self.crossover_probability\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        \n        trial = np.where(cross_points, mutant, self.population[idx])\n        trial_fitness = func(trial)\n        \n        if trial_fitness < self.fitness[idx]:\n            self.population[idx] = trial\n            self.fitness[idx] = trial_fitness\n\n    def adaptive_gradient_update(self, idx, func):\n        gradient = np.random.uniform(-1, 1, self.dim)\n        adaptive_lr = self.learning_rate / (1 + np.sqrt(idx / self.population_size))\n        hessian_approx = np.random.uniform(0.05, 0.3, self.dim)\n        candidate = self.population[idx] - adaptive_lr * gradient / (np.abs(hessian_approx) + 1e-8)\n        candidate = np.clip(candidate, self.lower_bound, self.upper_bound)\n        candidate_fitness = func(candidate)\n        if candidate_fitness < self.fitness[idx]:\n            self.population[idx] = candidate\n            self.fitness[idx] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population()\n        evaluations = 0\n        self.evaluate_population(func)\n        evaluations += self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                self.adaptive_differential_evolution(i, func)\n                evaluations += 1\n\n                if evaluations >= self.budget:\n                    break\n\n                self.adaptive_gradient_update(i, func)\n                evaluations += 1\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "EnhancedHybridOptimizer", "description": "A novel hybrid optimization algorithm that leverages adaptive exploration-exploitation balance through improved differential evolution crossover and adaptive gradient perturbation.", "configspace": "", "generation": 21, "fitness": 0.3320680319293356, "feedback": "The algorithm EnhancedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.33 with standard deviation 0.23.", "error": "", "parent_id": "390fb10b-b1a7-4fda-bfce-d7b6d4431e0f", "metadata": {"aucs": [0.7935743055025727, 0.7779810566461001, 0.7821121839046337, 0.7803737703121003, 0.7852709959763209, 0.7629092804085243, 0.7938726214691836, 0.7604780498014974, 0.7829192460420789, 0.5651396273905906, 0.5974765425926134, 0.6020521122975566, 0.5937431479194764, 0.5820409803876081, 0.5725011729918901, 0.5623082494826561, 0.5859050691038525, 0.5808936411958932, 0.10800101182215771, 0.10848962458603606, 0.08966998662576098, 0.13619559807784665, 0.10568836504644352, 0.11180894076239023, 0.11843144141257744, 0.11665746745395245, 0.1362669070459699, 0.09683340339343682, 0.11080024025361312, 0.09952948256684135, 0.1029518822766744, 0.10041336138542833, 0.0982613648689099, 0.08788316716683553, 0.09489373608872842, 0.10106656546729886, 0.8539912875767308, 0.8818505363877903, 0.8895001536991186, 0.8450519522713609, 0.852384438740764, 0.8880837140762264, 0.8497236962178333, 0.8976078228795437, 0.8837155874996477, 0.36131172452598825, 0.3571055186692589, 0.3640867812811118, 0.38437718583702096, 0.3178096412830319, 0.36322534230714776, 0.34927293893185674, 0.3181939266252902, 0.3750574927653392, 0.7446730988137222, 0.7542568717500632, 0.7087665974247286, 0.7350563627601889, 0.7330805334140527, 0.6841479913395948, 0.7130344651984274, 0.731872817304274, 0.6943884039846342, 0.19766934114287193, 0.17724095754518143, 0.14658035092046462, 0.1665893301985476, 0.15571783186500376, 0.18854833977869434, 0.2111228894980548, 0.17925378095768008, 0.18589130933687836, 0.18470969902163348, 0.17923701635160127, 0.19556919546586782, 0.1901122469625327, 0.1808257033334375, 0.14927300951714917, 0.2434649019609375, 0.2068272326430428, 0.17654190747419163, 0.26890882354345225, 0.19384089429795182, 0.20769423397102427, 0.26788358572188076, 0.20907842736276083, 0.2438366335450498, 0.27052606933491563, 0.26217051059542495, 0.17296204315637098, 0.43309612715561707, 0.39678702807591526, 0.389139540514583, 0.13352076516034173, 0.3581738057347068, 0.3556576779156615, 0.43344407872508395, 0.4147875791238659, 0.3968250382849925, 0.1277798877842522, 0.07257066402270318, 0.109425272540418, 0.1316206103671702, 0.1113291766363993, 0.08153666203623311, 0.13472016437794387, 0.10076986624581075, 0.10753001449401922, 0.2486550331416162, 0.23760909719399814, 0.2564729433246915, 0.2525618294871441, 0.2768451947342334, 0.26720332069745834, 0.23981971244500033, 0.23246030743948431, 0.235088472408287, 0.6492009558902827, 0.6379720588233839, 0.6177223524247069, 0.6049477771104284, 0.6322632011167357, 0.6023699931896429, 0.6244067196188836, 0.6324639862194705, 0.6218341440526638, 0.08919510188681412, 0.10294467233163318, 0.08822106256284967, 0.10132558214741749, 0.09475601049400972, 0.09795030926740744, 0.10001146301799979, 0.09924423269824334, 0.09821095544252134, 0.20124986298548464, 0.18592923474617506, 0.17505246686836362, 0.19081824099966904, 0.17905424922971158, 0.1667992034670962, 0.19034491427048372, 0.17722207022654057, 0.17808506862887807, 0.32768226894750774, 0.3447035318383024, 0.3795040503573516, 0.3514792542838274, 0.3689344769088757, 0.38752732638132803, 0.34934540472464803, 0.39930852582436005, 0.38661588295635896, 0.25109409553628925, 0.2723400455028806, 0.24756922244210333, 0.26950161728841104, 0.30268844177154475, 0.25528648098392714, 0.2939403832712829, 0.3304761430308889, 0.26279953192521677, 0.2170628477804466, 0.21254459839085738, 0.1963813614240395, 0.19088919283111372, 0.2080621427755197, 0.19453908774601247, 0.21681477649646708, 0.19851011615877368, 0.21047610412248718, 0.18329642613786157, 0.2095817731558921, 0.1939281425424938, 0.19352940107699113, 0.18906503649662565, 0.18096328543674522, 0.19254129514971963, 0.1854936478000977, 0.1906174526157569, 0.5898698982417044, 0.394542656849002, 0.36724075021936287, 0.7685022435810688, 0.33455853847293593, 0.47001337613142635, 0.4585889072234951, 0.41713186000714353, 0.6093727128128301, 0.43508291709952995, 0.39290582043114686, 0.23289454100712315, 0.444335459723203, 0.34422501665512195, 0.3322297729227275, 0.44346298303382437, 0.36308120208320216, 0.7373096698639028, 0.19246637318751747, 0.18855746521211048, 0.19639029197498625, 0.18812527641305088, 0.1822539876023822, 0.1908568363891946, 0.20257328237389816, 0.1787138032427238, 0.18478849638412365, 0.08026776021517268, 0.09027340819086926, 0.07732303208128011, 0.09041541427117883, 0.08837977536563923, 0.08767554309926096, 0.07592136304103736, 0.07569466990267082, 0.0714529107391183]}, "mutation_prompt": null}
{"id": "7a0c6077-7b44-4108-b5dc-0f35af97a1c9", "solution": "import numpy as np\n\nclass EnhancedDualPhaseOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = 50\n        self.mutation_factor = 0.9\n        self.crossover_probability = 0.8\n        self.learning_rate = 0.05\n        self.population = None\n        self.fitness = None\n\n    def initialize_population(self):\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, \n                                            (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def adaptive_differential_evolution(self, idx, func):\n        indices = [i for i in range(self.population_size) if i != idx]\n        np.random.shuffle(indices)\n        a, b, c = indices[:3]\n        mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        adaptive_factor = 0.7 + 0.3 * np.random.rand()\n        mutant = self.population[idx] + adaptive_factor * (mutant - self.population[idx])\n        mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n        \n        cross_points = np.random.rand(self.dim) < self.crossover_probability\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        \n        trial = np.where(cross_points, mutant, self.population[idx])\n        trial_fitness = func(trial)\n        \n        if trial_fitness < self.fitness[idx]:\n            self.population[idx] = trial\n            self.fitness[idx] = trial_fitness\n\n    def stochastic_gradient_update(self, idx, func):\n        gradient = np.random.uniform(-0.5, 0.5, self.dim)\n        adaptive_lr = self.learning_rate * (1 - idx / self.population_size)\n        candidate = self.population[idx] - adaptive_lr * gradient\n        candidate = np.clip(candidate, self.lower_bound, self.upper_bound)\n        candidate_fitness = func(candidate)\n        if candidate_fitness < self.fitness[idx]:\n            self.population[idx] = candidate\n            self.fitness[idx] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population()\n        evaluations = 0\n        self.evaluate_population(func)\n        evaluations += self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                self.adaptive_differential_evolution(i, func)\n                evaluations += 1\n\n                if evaluations >= self.budget:\n                    break\n\n                self.stochastic_gradient_update(i, func)\n                evaluations += 1\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "EnhancedDualPhaseOptimizer", "description": "Enhanced Dual-Phase Optimizer combining adaptive differential evolution with stochastic gradient-based refinement for robust global convergence.", "configspace": "", "generation": 22, "fitness": 0.2125688645830374, "feedback": "The algorithm EnhancedDualPhaseOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.21 with standard deviation 0.17.", "error": "", "parent_id": "390fb10b-b1a7-4fda-bfce-d7b6d4431e0f", "metadata": {"aucs": [0.5510570100398846, 0.5499987878740404, 0.559101180514497, 0.5470620255025824, 0.5947751902569272, 0.5863173591569943, 0.5695565338749473, 0.5615666085702052, 0.5397694060815205, 0.25921267285357186, 0.2643240949863417, 0.2523909363819147, 0.2898513963744207, 0.27145857106410265, 0.2536902667055593, 0.2674032658523634, 0.2809951874211549, 0.2985820872373862, 0.11570378227995459, 0.1042840578029618, 0.11150115021607221, 0.10873519961532507, 0.12264384001889939, 0.11357617994506852, 0.1670184352613613, 0.12622491924307355, 0.12068512312027757, 0.0963344433971639, 0.08786478800746012, 0.10710626543033797, 0.10018288401442477, 0.0863260887183438, 0.09871863489175359, 0.09550031481973675, 0.10608306312035443, 0.104088932585365, 0.8756613360147787, 0.7469290909812291, 0.9233710380629301, 0.8037228782580097, 0.7997825238071203, 0.9004706282351175, 0.886209591881565, 0.7761011003186107, 0.8317096186214923, 0.19462357722328116, 0.18443177491459606, 0.19702165853903486, 0.2205612973669896, 0.18386216550695467, 0.191001286101679, 0.20635992663558722, 0.19355544020812554, 0.20373453790250018, 0.23454079175712217, 0.25103955553231594, 0.2367512409371081, 0.27040724374579805, 0.2857533960570965, 0.29187531143271617, 0.254154348016576, 0.2744685774874548, 0.282014880698198, 0.11985780670050261, 0.1251640785196787, 0.12915470407121843, 0.15816408590177033, 0.12285278681531075, 0.12370534901479224, 0.13908692734165562, 0.11810938330282039, 0.11772322021191539, 0.1463849069200589, 0.0776181016154156, 0.10135365054489631, 0.11349570916591678, 0.15100556618432748, 0.12661978730470203, 0.12797960788904927, 0.12471071349246499, 0.10865776535815708, 0.04244403899249061, 0.031298341155982734, 0.05412285283487184, 0.042073674149743834, 0.03335213553852978, 0.042453470692666584, 0.04528900120071688, 0.037241493624395106, 0.037727134492729464, 0.1489608710201108, 0.1690992854932869, 0.16018398262576827, 0.1674436174733439, 0.176454834898371, 0.15664223272423983, 0.18708803794099527, 0.20301426201596273, 0.19336823410237836, 0.016872962415933945, 0.031396263934977076, 0.02797994349659161, 0.02377093390034024, 0.020781145179909255, 0.03026025350469097, 0.0500026442166831, 0.027840898049766305, 0.02729729794434843, 0.10209795449600156, 0.10461447929093393, 0.11096028161381344, 0.10915033581047151, 0.09949132542605021, 0.11155617574521204, 0.10883793364607353, 0.10524754934460256, 0.09962739429132383, 0.42527362106867117, 0.4284438991867421, 0.4291448173168526, 0.43008321638460234, 0.41487893880966886, 0.44037550242066226, 0.4342364917070711, 0.42032205736980277, 0.4277325493238684, 0.10266109611769936, 0.08662637374058291, 0.10341369244535359, 0.1065528204816445, 0.10000405688014302, 0.09708337034271641, 0.12318154266105308, 0.10635691722652696, 0.11593053449945834, 0.17373804021807138, 0.1808133360013343, 0.16939792889978766, 0.13584223378277582, 0.1436712729721059, 0.1679014152219066, 0.2097426417924232, 0.1497134565513254, 0.1490255737848123, 0.24430983106746873, 0.24322634993917525, 0.2355251757821929, 0.2067553574987553, 0.22615725141882936, 0.2170767511633871, 0.22498501830050932, 0.26103513745368023, 0.23821961406947045, 0.16505979655351366, 0.16627600103228346, 0.17492469856964143, 0.16929119041655882, 0.15434060190808063, 0.14979911639401722, 0.17529214759116107, 0.1712861886779743, 0.1595425146955739, 0.21690185855981559, 0.2131708286497046, 0.210252083183468, 0.18549909585794278, 0.2427799407101615, 0.1941691241975132, 0.23162335366648823, 0.20213600744589266, 0.21882779744277536, 0.1841149188828748, 0.1715750022005741, 0.18112259388558594, 0.18005957063968836, 0.18520984434320098, 0.1758504670216493, 0.19265360781825513, 0.17870678176987387, 0.1917606904167406, 0.16624127184619042, 0.14758058186673173, 0.17828567560217712, 0.15515712508167834, 0.16160324433393725, 0.1835760600087336, 0.21332962844064207, 0.17342252952547643, 0.18905983772256152, 0.2204637747225695, 0.1810421739884891, 0.1515449716133126, 0.146458979671807, 0.25286225395039874, 0.17076067773314207, 0.254457099716079, 0.17906883692252917, 0.1509518773507299, 0.22805764445980037, 0.20716903097529016, 0.20063574787647132, 0.2105483203554781, 0.21859101969277128, 0.20714887700193974, 0.2221459361287682, 0.2060979216479969, 0.22620533059987846, 0.0834425140368068, 0.08064718886047106, 0.0715575672618568, 0.08780767110561938, 0.08709505755643676, 0.0880512988626524, 0.08585145969611618, 0.09605510044789667, 0.0827438337212032]}, "mutation_prompt": null}
{"id": "efbb7c19-f3df-4189-a739-7f522b7694f1", "solution": "import numpy as np\n\nclass DiversifiedAdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = 40\n        self.mutation_factor = 0.9  # Increased mutation factor for exploration\n        self.crossover_probability = 0.85  # Slightly reduced for better stability\n        self.learning_rate = 0.15  # Increased learning rate for faster adaptation\n        self.population = None\n        self.fitness = None\n\n    def initialize_population(self):\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, \n                                            (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def adaptive_differential_evolution(self, idx, func):\n        indices = [i for i in range(self.population_size) if i != idx]\n        np.random.shuffle(indices)\n        a, b, c = indices[:3]\n        mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        adaptive_factor = 0.6 + 0.4 * np.random.rand()  # Modified adaptive factor range\n        mutant = self.population[idx] + adaptive_factor * (mutant - self.population[idx])\n        mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n        \n        cross_points = np.random.rand(self.dim) < self.crossover_probability\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        \n        trial = np.where(cross_points, mutant, self.population[idx])\n        trial_fitness = func(trial)\n        \n        if trial_fitness < self.fitness[idx]:\n            self.population[idx] = trial\n            self.fitness[idx] = trial_fitness\n\n    def stochastic_gradient_update(self, idx, func):\n        gradient = np.random.uniform(-1, 1, self.dim)\n        adaptive_lr = self.learning_rate / (1 + idx / (self.population_size * 2))  # Adjusted learning rate adaptation\n        hessian_approx = np.random.uniform(0.2, 0.6, self.dim)  # Modified Hessian approximation range\n        candidate = self.population[idx] - adaptive_lr * gradient / (np.abs(hessian_approx) + 1e-9)\n        candidate = np.clip(candidate, self.lower_bound, self.upper_bound)\n        candidate_fitness = func(candidate)\n        if candidate_fitness < self.fitness[idx]:\n            self.population[idx] = candidate\n            self.fitness[idx] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population()\n        evaluations = 0\n        self.evaluate_population(func)\n        evaluations += self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                self.adaptive_differential_evolution(i, func)\n                evaluations += 1\n\n                if evaluations >= self.budget:\n                    break\n\n                self.stochastic_gradient_update(i, func)\n                evaluations += 1\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "DiversifiedAdaptiveHybridOptimizer", "description": "A diversified adaptive hybrid optimizer employing enhanced differential evolution and stochastic gradient strategies for robust convergence.", "configspace": "", "generation": 23, "fitness": 0.2830818677920324, "feedback": "The algorithm DiversifiedAdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28 with standard deviation 0.21.", "error": "", "parent_id": "390fb10b-b1a7-4fda-bfce-d7b6d4431e0f", "metadata": {"aucs": [0.6977855020920256, 0.6900539624227693, 0.7058057825643306, 0.6999271546978114, 0.7090223348033053, 0.6990829889775276, 0.6927626932789079, 0.7110605028355228, 0.6889489521212323, 0.44574768177802504, 0.4185734860960121, 0.4260942468279505, 0.4306279617853521, 0.45445853581559414, 0.41610393640936316, 0.380474661092263, 0.424918297271052, 0.4177484076932033, 0.12002257417193996, 0.09984872729907734, 0.0979226718216849, 0.10373074074573962, 0.10903105215306819, 0.10070720128328448, 0.10935380830611963, 0.11007296174679992, 0.10573499367811034, 0.10568859169620859, 0.0833900697900597, 0.09341141126687957, 0.09012480570509251, 0.11138921765870546, 0.10592863836774835, 0.09409138734802713, 0.09320691101682954, 0.08381705722964305, 0.8749743613535726, 0.8897069432069062, 0.8341274317985016, 0.8783020626438927, 0.8711061241647756, 0.883153043675118, 0.895739931018083, 0.844860649718752, 0.9349750722538126, 0.3263466833255023, 0.3118503389350413, 0.28698189293925247, 0.299564516507309, 0.30506404041626245, 0.30642501490828045, 0.29015769854256146, 0.3201720078215595, 0.29379912974841293, 0.5399972675307556, 0.638065092047079, 0.5630792466260819, 0.6290982320052017, 0.5831041135378854, 0.6087357443748269, 0.6044576038584082, 0.5819981035763622, 0.5996238663268962, 0.16995896340361227, 0.16044543909699116, 0.16798284611105607, 0.19701160016164554, 0.20204403745252097, 0.19934423316205652, 0.19778148353812341, 0.1824998077081359, 0.2215527187347357, 0.19961986448848257, 0.1683293245547156, 0.15988041580529722, 0.18645869750633481, 0.19123861028102285, 0.20486688123587127, 0.18003171054770606, 0.15826230677529474, 0.16679357113887472, 0.12744177203611273, 0.11111631044164494, 0.130777305096358, 0.13816969183858707, 0.15397684930237432, 0.12916051743127732, 0.13234336094727184, 0.12904649572110993, 0.17028561390436103, 0.3011948106536495, 0.243796244373797, 0.2763496334615143, 0.25888419362879933, 0.26272875043702626, 0.2296061173467583, 0.31617827387908193, 0.28890280510333466, 0.27942007954804093, 0.01747098704678207, 0.05087155291367684, 0.05955411824315959, 0.03243461967980932, 0.04591247759170258, 0.037834919557624525, 0.0666553153186904, 0.030387773297237075, 0.06122042588025056, 0.1655180755524558, 0.16004123680286775, 0.16879963070528725, 0.15990977899494263, 0.17309453836252564, 0.17260967095999435, 0.1644774962568768, 0.1839881563926249, 0.1781718740169932, 0.5372655452882978, 0.5260488667250312, 0.4965861314990311, 0.5137893451391953, 0.523065142881493, 0.503396469013643, 0.5223371483293227, 0.5371340368007747, 0.5437657102355782, 0.11916359677668975, 0.09913137395987792, 0.10296736983859223, 0.1111919409857004, 0.08665733338519677, 0.09665984231098879, 0.09191358238975122, 0.09135066632845279, 0.09317054049901818, 0.18268984854066816, 0.17788281689261565, 0.20840818351493262, 0.22363149439850372, 0.1809976874705741, 0.17959214675219648, 0.1859332417233498, 0.19354340696462113, 0.2153731238615314, 0.2667287803763757, 0.2971588313748109, 0.2777236540405703, 0.2746306589665891, 0.2762323327001832, 0.2608485633278459, 0.30112543358356725, 0.3180020756672177, 0.29293159978305305, 0.2195279125511339, 0.2280606605692258, 0.234967008458572, 0.19789895358597243, 0.20946543997254352, 0.20306929908400162, 0.22833240375051012, 0.24453136185669844, 0.19520197699665953, 0.23227986584565463, 0.19084705734966234, 0.1906358392869184, 0.22019163033709255, 0.19875762745913295, 0.20625084674984018, 0.19708118321231172, 0.19631653119077086, 0.20904521571289947, 0.18595709263936044, 0.18917227200039988, 0.18505715775251896, 0.18669932825448654, 0.20465389899950837, 0.1913223787181011, 0.1934113197223568, 0.18278120212970395, 0.1778384866826338, 0.31319028428237095, 0.3793016838483374, 0.48283369601900217, 0.5364641858635617, 0.3659961552087633, 0.4568571700325955, 0.41222177199017285, 0.3070857134478059, 0.3041774926007391, 0.41359337292135057, 0.3090906563836402, 0.3501687005199705, 0.3514655220043571, 0.20657894968712331, 0.2988473223466972, 0.39227666301938535, 0.39387923842649486, 0.20550590007034675, 0.17598279041384934, 0.21147097393649206, 0.19246396374974695, 0.18551774636015095, 0.18570524639865726, 0.19124034237712217, 0.19611790940971008, 0.18179142830238193, 0.1831408359740464, 0.07483731819869155, 0.08285983651037387, 0.0735613670339903, 0.07624507995590446, 0.08250586606255439, 0.09498697822231883, 0.07427555379768747, 0.07241038442327885, 0.07808276294939442]}, "mutation_prompt": null}
{"id": "6fc57d28-2055-47b5-a05a-6941bf9a89f1", "solution": "import numpy as np\n\nclass EnhancedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = 50  # slightly increased for diversity\n        self.mutation_factor = 0.85  # fine-tuned mutation factor\n        self.crossover_probability = 0.95  # increased for better recombination\n        self.learning_rate = 0.15  # adjusted learning rate\n        self.population = None\n        self.fitness = None\n\n    def initialize_population(self):\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, \n                                            (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def adaptive_differential_evolution(self, idx, func):\n        indices = [i for i in range(self.population_size) if i != idx]\n        np.random.shuffle(indices)\n        a, b, c = indices[:3]\n        mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        adaptive_factor = 0.4 + 0.6 * np.random.rand()  # adjusted range for adaptive factor\n        mutant = self.population[idx] + adaptive_factor * (mutant - self.population[idx])\n        mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n\n        cross_points = np.random.rand(self.dim) < self.crossover_probability\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n\n        trial = np.where(cross_points, mutant, self.population[idx])\n        trial_fitness = func(trial)\n\n        if trial_fitness < self.fitness[idx]:\n            self.population[idx] = trial\n            self.fitness[idx] = trial_fitness\n\n    def randomized_newton_update(self, idx, func):\n        gradient = np.random.uniform(-1, 1, self.dim)\n        adaptive_lr = self.learning_rate / (1 + 0.1 * idx / self.population_size)  # adjusted decay function\n        hessian_approx = np.random.uniform(0.2, 0.6, self.dim)  # adjusted range for better approximation\n        candidate = self.population[idx] - adaptive_lr * gradient / (np.abs(hessian_approx) + 1e-9)\n        candidate = np.clip(candidate, self.lower_bound, self.upper_bound)\n        candidate_fitness = func(candidate)\n        if candidate_fitness < self.fitness[idx]:\n            self.population[idx] = candidate\n            self.fitness[idx] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population()\n        evaluations = 0\n        self.evaluate_population(func)\n        evaluations += self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                self.adaptive_differential_evolution(i, func)\n                evaluations += 1\n\n                if evaluations >= self.budget:\n                    break\n\n                self.randomized_newton_update(i, func)\n                evaluations += 1\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "EnhancedHybridOptimizer", "description": "An enhanced hybrid optimizer utilizing a balance of exploration and exploitation through stochastic differential evolution and adaptive learning mechanisms for robust optimization.", "configspace": "", "generation": 24, "fitness": 0.3289801445795664, "feedback": "The algorithm EnhancedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.33 with standard deviation 0.22.", "error": "", "parent_id": "390fb10b-b1a7-4fda-bfce-d7b6d4431e0f", "metadata": {"aucs": [0.7525455155906587, 0.7234154674094335, 0.7308018575812814, 0.722517155693376, 0.7347636121914145, 0.734761378165831, 0.7417724735835407, 0.7220456472975648, 0.6927350984013476, 0.4856703091721545, 0.48954495185720803, 0.5254378922199308, 0.4845385513791013, 0.47444374169878645, 0.5348665402971609, 0.5079776956915028, 0.48933224613894344, 0.4516987549202115, 0.09809111730063214, 0.09999242414221243, 0.09474126514653669, 0.10070087621809565, 0.10375999421116233, 0.10673505748454026, 0.10460793933176626, 0.10569533909946438, 0.0960863654869718, 0.08360155604562136, 0.10147695344693364, 0.08123733070826678, 0.08519792264221793, 0.08639116910172018, 0.07741889648792111, 0.07872182232207192, 0.1145150531762914, 0.101338037149026, 0.819617961448628, 0.8015614346633347, 0.8600170565763792, 0.8589629279953337, 0.7791638711777167, 0.7684004216009221, 0.854614682690604, 0.8274503027134279, 0.7909861455533576, 0.2886388422062409, 0.29765607734808275, 0.29649711190834904, 0.28524427145484754, 0.3216081143957915, 0.29595209922858234, 0.3205022340037449, 0.31739517413170115, 0.30908673353348015, 0.7411697864253532, 0.7077650760322567, 0.7299097804126344, 0.7538841790872645, 0.7532344310458647, 0.7258396188988443, 0.7399503905801007, 0.7034388709942426, 0.722256054861608, 0.15554532071367522, 0.14876278492416495, 0.18190306531057654, 0.16527621047862417, 0.2134511473015892, 0.21144037687744244, 0.1884280109408124, 0.15680275905458496, 0.291507186788319, 0.14711236932528715, 0.17960258284965946, 0.25956032176017774, 0.1840399199696925, 0.1697041713993387, 0.1836423386718331, 0.19089156659506612, 0.17037261628572964, 0.16762836876696396, 0.26261695480720815, 0.32216846310980296, 0.37160720507664824, 0.37033771782659164, 0.37973469913148605, 0.3116726444510033, 0.4050182751508069, 0.3492327480541939, 0.3452708702554971, 0.5166154882850548, 0.5086645287404586, 0.5128193315263752, 0.4031913037306931, 0.5079502081082858, 0.45673017197368193, 0.5348363499230445, 0.5137893555065294, 0.4832952299460853, 0.19796775014886248, 0.15777675625686827, 0.17561981564869678, 0.211978817958285, 0.15352825101825107, 0.15127622444289424, 0.1305964637924324, 0.15235814243508117, 0.13261304019656528, 0.2985515312308826, 0.2767023371611861, 0.28587203307200193, 0.2772348561493202, 0.28700358177128205, 0.29035282915169325, 0.3004262190342898, 0.25835459631229063, 0.29898215639414005, 0.6523312850703242, 0.6413989833083871, 0.6351215360769598, 0.6295564589115206, 0.6335255405197158, 0.637600458307596, 0.6613600674434645, 0.6467470800064552, 0.6620426749085001, 0.10026338937737689, 0.0838047534624422, 0.10308283711440347, 0.12196366083466403, 0.0934677636922625, 0.09727616653889204, 0.09876131805557686, 0.10435408764312526, 0.09611640347102635, 0.1644548263087745, 0.16890739874175997, 0.19318257040587206, 0.17772917374802166, 0.1869904316610883, 0.177882828955502, 0.18357197239809087, 0.19968687788482897, 0.19632204859225877, 0.328817727129733, 0.29672648295473913, 0.3407854472469328, 0.309482825695119, 0.3230973277549348, 0.3184150820137954, 0.3309683764377078, 0.33721606253747405, 0.3305278686781191, 0.2586130519934935, 0.26163581548258696, 0.2639322999865771, 0.2469490771893923, 0.2626468390538369, 0.2628194350658368, 0.2639217456959265, 0.2657711684952424, 0.25160049385435246, 0.1931065135485175, 0.1994792827426476, 0.23125943698322438, 0.21986028173520522, 0.18734703303311706, 0.21822184518512555, 0.22802974695346356, 0.1980285869462457, 0.21689764859436378, 0.20258614078830806, 0.18215192718684525, 0.1885807587764785, 0.18639714026169463, 0.18551841308671868, 0.17241406925297564, 0.1749392657049197, 0.18423574569807066, 0.1793537488484208, 0.3392771949315463, 0.5124816904384016, 0.5335237796455989, 0.46372494829538424, 0.3145034118090916, 0.4798532272820747, 0.39419898458099123, 0.37576324351956547, 0.40358113424644637, 0.4298311200832605, 0.3471147498133409, 0.6793975808027042, 0.4822983442265769, 0.22063558008002837, 0.2459557875456274, 0.3273254356834939, 0.36283875282480493, 0.20356646249987875, 0.2019170933964397, 0.17364165414500554, 0.19271964346495396, 0.17933918961530382, 0.18739563355400501, 0.18711244237089253, 0.19988960013265822, 0.19780301279014434, 0.1937326761985868, 0.077285685301694, 0.08305952040172726, 0.07907407051947524, 0.07971084814845253, 0.07334136445166461, 0.08853950155265555, 0.07282181677482691, 0.08628282527050157, 0.09193619120591867]}, "mutation_prompt": null}
{"id": "5f8c62f0-28ee-4208-a765-89c1e597e2d7", "solution": "import numpy as np\n\nclass RefinedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = 40\n        self.mutation_factor = 0.8\n        self.crossover_probability = 0.9\n        self.learning_rate = 0.1\n        self.population = None\n        self.fitness = None\n\n    def initialize_population(self):\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, \n                                            (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def adaptive_differential_evolution(self, idx, func):\n        indices = [i for i in range(self.population_size) if i != idx]\n        np.random.shuffle(indices)\n        a, b, c = indices[:3]\n        mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        adaptive_factor = 0.5 + 0.5 * np.random.rand()\n        mutant = self.population[idx] + adaptive_factor * (mutant - self.population[idx])\n        mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n        \n        cross_points = np.random.rand(self.dim) < self.crossover_probability\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        \n        trial = np.where(cross_points, mutant, self.population[idx])\n        trial_fitness = func(trial)\n        \n        if trial_fitness < self.fitness[idx]:\n            self.population[idx] = trial\n            self.fitness[idx] = trial_fitness\n\n    def randomized_newton_update(self, idx, func):\n        gradient = np.random.uniform(-1, 1, self.dim)\n        adaptive_lr = self.learning_rate / (1 + idx / self.population_size)\n        hessian_approx = np.random.uniform(0.1, 0.5, self.dim)\n        candidate = self.population[idx] - adaptive_lr * gradient / (np.abs(hessian_approx) + 1e-9)\n        candidate = np.clip(candidate, self.lower_bound, self.upper_bound)\n        candidate_fitness = func(candidate)\n        if candidate_fitness < self.fitness[idx]:\n            self.population[idx] = candidate\n            self.fitness[idx] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population()\n        evaluations = 0\n        self.evaluate_population(func)\n        evaluations += self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                self.adaptive_differential_evolution(i, func)\n                evaluations += 1\n\n                if evaluations >= self.budget:\n                    break\n\n                self.randomized_newton_update(i, func)\n                evaluations += 1\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "RefinedHybridOptimizer", "description": "A refined hybrid optimization algorithm that enhances exploration through stochastic rank-based selection and incorporates adaptive learning rate adjustments for improved convergence.", "configspace": "", "generation": 19, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "390fb10b-b1a7-4fda-bfce-d7b6d4431e0f", "metadata": {"aucs": [0.7998926377262636, 0.761429726165544, 0.7863085107779331, 0.7508095948300958, 0.7882340583889212, 0.7816710181219415, 0.7826294649647488, 0.7918404048180104, 0.7676018171732987, 0.5689355814544976, 0.5786091347521873, 0.6064432841944956, 0.4875777471533822, 0.5880862128826672, 0.5875553949838622, 0.5452458885894269, 0.549792395120432, 0.5742927984304333, 0.10548247785135645, 0.11285201831791869, 0.0925734721780318, 0.11436849439951724, 0.10592419427335498, 0.11336423572081245, 0.10345862089413227, 0.1269952222453321, 0.12152214562989194, 0.10204477557831337, 0.09901425922888485, 0.09577110764085917, 0.09397947463880463, 0.09193976260094838, 0.09380899156179712, 0.09210822057585066, 0.09527236403031891, 0.09495208182919956, 0.8203943085242258, 0.8371989577902406, 0.8144215895957443, 0.8440204151740751, 0.7981195572547755, 0.8598124593300991, 0.8324337981573954, 0.8546195368656158, 0.849854255620719, 0.3666013280562487, 0.3725233049177048, 0.3395845401363392, 0.39396992596302094, 0.37524282476543214, 0.35070565435275614, 0.33407804648082917, 0.38226003237655404, 0.349245265497106, 0.7018061463567615, 0.7233415701332743, 0.7088757356079152, 0.7982178557801977, 0.7714881430535534, 0.7264322401069573, 0.795367319615778, 0.7599066331977806, 0.7086678887052003, 0.201764961314991, 0.17282079779941106, 0.2263590169873433, 0.2721596471019315, 0.27597767251362215, 0.17937872167142266, 0.16761630117773363, 0.24905357666002514, 0.16922513461462385, 0.16335003951102212, 0.20535196113100707, 0.13361225495437334, 0.1579703207595461, 0.16150244697886662, 0.2167787719170582, 0.18782402154756306, 0.19564428973151649, 0.18163669853366704, 0.31867100857758623, 0.3425741905828079, 0.3304444894524823, 0.34738093096977485, 0.2877568817466949, 0.3234807865621129, 0.33894048928278153, 0.37013426300073216, 0.3588389116713273, 0.5180632809666004, 0.4637458915590531, 0.5043407285905045, 0.33215873995771494, 0.4542336970383146, 0.4548165201136448, 0.5156739638315224, 0.5239241206078717, 0.4755558899035023, 0.11585943854457781, 0.1638375146855462, 0.09872669469579576, 0.21730694094700664, 0.16634037144172908, 0.1654849161870532, 0.11664820618779581, 0.07087408397891926, 0.10845489196991798, 0.2843152982061281, 0.28613726624777835, 0.28229693759480057, 0.3072011097040027, 0.2853119052426477, 0.2916314440888641, 0.2999473256283228, 0.2859594986946987, 0.26193620963585085, 0.6913140588308797, 0.6786867206260565, 0.701588295399665, 0.6541883452840307, 0.6654199785954649, 0.6475450237226962, 0.6671814869069398, 0.7055073070884146, 0.6638784336673579, 0.11391843993435358, 0.10975448936033483, 0.10090549793679549, 0.10310324236480084, 0.10791804393203874, 0.10939104607676131, 0.09581307616921564, 0.09812125343721878, 0.09985837034065093, 0.18846301470631488, 0.22839262856185694, 0.22322725974210345, 0.19757465926045115, 0.1701987471183407, 0.18919254140941477, 0.20027451142437336, 0.20007046061634493, 0.20073311465283272, 0.3501650996213105, 0.35164157461146006, 0.34813603073325927, 0.34536700185434244, 0.3655415017581666, 0.3583096257802938, 0.35581920313139404, 0.40530574007584763, 0.36866959956081435, 0.26557307404745645, 0.2697168709798672, 0.24942936856729292, 0.259704041706049, 0.2626717448788001, 0.2794983152026371, 0.24933057131184155, 0.29414825313596626, 0.24778132121106566, 0.1964865341256079, 0.20130664628203954, 0.2003458670310626, 0.23375686724462996, 0.21330797625742404, 0.2022932961455597, 0.2034346087052653, 0.20158084356029105, 0.23035297301018787, 0.19254916882434714, 0.18814946260488974, 0.19842901699570903, 0.184403561312654, 0.186854624925067, 0.18528907418469243, 0.20214940877697307, 0.18935042119784207, 0.18539220063005257, 0.32818920794091344, 0.5959495761773042, 0.3200074905287692, 0.6508833616229394, 0.39195391293278525, 0.506566629143708, 0.4000391686588708, 0.3511867539905471, 0.39761398923874924, 0.45995799055147635, 0.4459678532604766, 0.33988555794646136, 0.5390455573695724, 0.23998167446344754, 0.31350500582590435, 0.523108579970601, 0.20391488842316685, 0.24657744080176514, 0.17212424028899986, 0.19143516576879105, 0.17627569278737343, 0.19918041607754244, 0.2059953338045003, 0.175787862497925, 0.19782638442055023, 0.2069171764155977, 0.20245082333553344, 0.08338336975119987, 0.08271298565658503, 0.08769925944284895, 0.09209940270208461, 0.08249412618243479, 0.09027444882925406, 0.07892761994153641, 0.08207905467688215, 0.08015318768749546]}, "mutation_prompt": null}
{"id": "a65f2606-7958-461d-a7ae-d1e30d5c012c", "solution": "import numpy as np\n\nclass EnhancedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = 50\n        self.mutation_factor = 0.85\n        self.crossover_probability = 0.9\n        self.learning_rate = 0.05\n        self.elitism_ratio = 0.1\n        self.population = None\n        self.fitness = None\n\n    def initialize_population(self):\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, \n                                            (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def adaptive_differential_evolution(self, idx, func):\n        indices = [i for i in range(self.population_size) if i != idx]\n        np.random.shuffle(indices)\n        a, b, c = indices[:3]\n        mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        adaptive_factor = 0.6 + 0.4 * np.random.rand()\n        mutant = self.population[idx] + adaptive_factor * (mutant - self.population[idx])\n        mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n        \n        cross_points = np.random.rand(self.dim) < self.crossover_probability\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        \n        trial = np.where(cross_points, mutant, self.population[idx])\n        trial_fitness = func(trial)\n        \n        if trial_fitness < self.fitness[idx]:\n            self.population[idx] = trial\n            self.fitness[idx] = trial_fitness\n\n    def stochastic_gradient_update(self, idx, func):\n        gradient = np.random.uniform(-1, 1, self.dim)\n        adaptive_lr = self.learning_rate / (1 + idx / self.population_size)\n        candidate = self.population[idx] - adaptive_lr * gradient\n        candidate = np.clip(candidate, self.lower_bound, self.upper_bound)\n        candidate_fitness = func(candidate)\n        if candidate_fitness < self.fitness[idx]:\n            self.population[idx] = candidate\n            self.fitness[idx] = candidate_fitness\n\n    def elitist_selection(self):\n        elite_count = int(self.elitism_ratio * self.population_size)\n        elite_indices = np.argsort(self.fitness)[:elite_count]\n        return self.population[elite_indices], self.fitness[elite_indices]\n\n    def __call__(self, func):\n        self.initialize_population()\n        evaluations = 0\n        self.evaluate_population(func)\n        evaluations += self.population_size\n\n        while evaluations < self.budget:\n            elite_pop, elite_fit = self.elitist_selection()\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                self.adaptive_differential_evolution(i, func)\n                evaluations += 1\n\n                if evaluations >= self.budget:\n                    break\n\n                self.stochastic_gradient_update(i, func)\n                evaluations += 1\n\n            non_elite_indices = np.argsort(self.fitness)[len(elite_fit):]\n            self.population[non_elite_indices[:len(elite_pop)]] = elite_pop\n            self.fitness[non_elite_indices[:len(elite_fit)]] = elite_fit\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "EnhancedHybridOptimizer", "description": "An enhanced hybrid optimizer that combines adaptive differential evolution with stochastic gradient-like updates and incorporates elitist selection to improve convergence dynamics.", "configspace": "", "generation": 26, "fitness": 0.3309353466014882, "feedback": "The algorithm EnhancedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.33 with standard deviation 0.21.", "error": "", "parent_id": "390fb10b-b1a7-4fda-bfce-d7b6d4431e0f", "metadata": {"aucs": [0.7020858183940664, 0.7051699124164277, 0.7352989894774254, 0.7127413782369254, 0.7146059628254109, 0.7195966910031706, 0.7141856255061014, 0.6991269269868273, 0.7078952374496277, 0.47646341468337283, 0.47789503964134106, 0.41619840656345763, 0.4772499280819985, 0.49451502646861023, 0.4478073298127221, 0.454870861826407, 0.4769863388147688, 0.474879366248923, 0.14919407302991727, 0.13356311423093048, 0.14469671994418964, 0.15086119767532857, 0.13182232479307188, 0.1828276012751262, 0.12878474221910108, 0.16358694988295597, 0.13450253464913753, 0.1203187299349594, 0.11044602523145353, 0.12973002788465693, 0.12363254143531144, 0.11832505766844192, 0.12590974858362447, 0.11327266880843578, 0.11286079130919058, 0.11474866036064135, 0.7844580755819532, 0.7901200326106688, 0.9286021005627078, 0.7683525928540679, 0.9129996167102354, 0.9317416712511136, 0.8984536887131511, 0.8181036071400483, 0.9452571906629197, 0.35647024621208767, 0.31732849132608476, 0.31326764082787917, 0.33441999917157106, 0.31635501878898975, 0.32685458424010105, 0.3516072749759981, 0.35048872084612825, 0.3698131351147458, 0.6831013743039194, 0.7168554112198589, 0.636580810914782, 0.7170104110538427, 0.7575583727073654, 0.7348925194434113, 0.7350348770892339, 0.6830970852294787, 0.6634772919369738, 0.26160500929960506, 0.12737423638344336, 0.16692730254843013, 0.24175198418707744, 0.19622229452127793, 0.23222527376157143, 0.2085626396219289, 0.2624701202001908, 0.3696142293174688, 0.41482233493405263, 0.18939438956583388, 0.2748881279426839, 0.3412365574754074, 0.3683116449829815, 0.16759728477074243, 0.164837844214631, 0.1474199968176907, 0.2252840852210225, 0.2777167553456462, 0.26710750092313595, 0.25959650434379744, 0.29911128564797573, 0.2687911305153555, 0.2364440834050331, 0.2816209108978619, 0.3151029009878048, 0.28441788103905696, 0.4495086035057607, 0.42942942690188224, 0.40230481107938443, 0.428131892963289, 0.27346362709017136, 0.427875715187102, 0.46800139719170397, 0.45966707140003205, 0.4145429585295265, 0.11325200367774446, 0.08048552061353353, 0.12970754576344512, 0.15690891543109386, 0.13631504697806762, 0.11643295377031504, 0.11460309290486825, 0.10091125225755859, 0.07443579459089056, 0.23890569739678846, 0.2482005491045245, 0.22775003783298697, 0.2457993629176385, 0.23276799057939934, 0.27632228811105886, 0.24031908503824873, 0.22714893438971007, 0.2377983748170467, 0.57735873285857, 0.6114945036702669, 0.5615787893459839, 0.5883145663659465, 0.5908833310949391, 0.5759929891096173, 0.6039166973453738, 0.5778926251298697, 0.5815576972994686, 0.13776239214982322, 0.13382143874032104, 0.1418655071426076, 0.19322671062994545, 0.12569103373371748, 0.1482569459073615, 0.11979273168174565, 0.15025733735579327, 0.12809759683713962, 0.24132549001166959, 0.1978807578563223, 0.18501804164094204, 0.23504970025041538, 0.20500057363008106, 0.17199124774979846, 0.19063724885571942, 0.18531322503710068, 0.17843578335213028, 0.33139586416911837, 0.33546144675420486, 0.3404042770208957, 0.3360156220002747, 0.3077202177581103, 0.2916814522595619, 0.34703951614585204, 0.3866509649055674, 0.3214023948564574, 0.29221876226000554, 0.2278791231640268, 0.29501734357246057, 0.2916011677053312, 0.24421435056499474, 0.246186963732549, 0.2467306662403438, 0.29280363684207955, 0.2816868699998797, 0.2044218562609903, 0.20028669115132802, 0.2336570608823313, 0.19682035542199283, 0.18915071913412507, 0.20847847817706644, 0.20158478672893065, 0.20403450972225445, 0.2212642108784355, 0.19250745128593583, 0.23364442819379982, 0.19790438630606488, 0.2100133870810218, 0.2210021552582776, 0.24764347378488238, 0.19436091435453184, 0.17916446015400722, 0.20656115142439468, 0.29096424863684467, 0.17689620627439695, 0.1761935954305811, 0.6396432243791519, 0.26068349861858786, 0.5512378390146124, 0.5268927379314634, 0.1694090892523653, 0.6206832154794033, 0.7220035973300603, 0.20277378364421672, 0.25073477291132984, 0.1615529146334389, 0.5763093099227252, 0.7317824863777139, 0.19962814468703227, 0.468829804775323, 0.5930810215006361, 0.20052907505676498, 0.21685020611325911, 0.1932956734062552, 0.2261814943889714, 0.21926538561025, 0.2041385858170166, 0.22397632336812112, 0.20978752595329786, 0.21513699901349037, 0.09023781035175071, 0.08213255086734061, 0.08517607717098652, 0.09372730558148035, 0.1060758017641168, 0.08818228394792782, 0.08882706517320216, 0.09471631030494176, 0.087072226558782]}, "mutation_prompt": null}
{"id": "7b118954-f384-4d89-804f-27206357ce55", "solution": "import numpy as np\n\nclass EnhancedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = 50\n        self.mutation_factor = 0.9\n        self.crossover_probability = 0.85\n        self.learning_rate = 0.15\n        self.population = None\n        self.fitness = None\n\n    def initialize_population(self):\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, \n                                            (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def adaptive_differential_evolution(self, idx, func):\n        indices = [i for i in range(self.population_size) if i != idx]\n        np.random.shuffle(indices)\n        a, b, c = indices[:3]\n        mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        adaptive_factor = 0.6 + 0.4 * np.random.rand()\n        mutant = self.population[idx] + adaptive_factor * (mutant - self.population[idx])\n        mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n        \n        cross_points = np.random.rand(self.dim) < self.crossover_probability\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        \n        trial = np.where(cross_points, mutant, self.population[idx])\n        trial_fitness = func(trial)\n        \n        if trial_fitness < self.fitness[idx]:\n            self.population[idx] = trial\n            self.fitness[idx] = trial_fitness\n\n    def stochastic_gradient_perturbation(self, idx, func):\n        gradient = np.random.normal(0, 1, self.dim)\n        adaptive_lr = self.learning_rate / (1 + idx / (5.0 * self.population_size))\n        hessian_approx = np.random.uniform(0.1, 0.5, self.dim)\n        candidate = self.population[idx] - adaptive_lr * gradient / (np.abs(hessian_approx) + 1e-9)\n        candidate = np.clip(candidate, self.lower_bound, self.upper_bound)\n        candidate_fitness = func(candidate)\n        if candidate_fitness < self.fitness[idx]:\n            self.population[idx] = candidate\n            self.fitness[idx] = candidate_fitness\n\n    def preserve_diversity(self):\n        diversity_threshold = 1e-3\n        for i in range(self.population_size):\n            for j in range(i + 1, self.population_size):\n                if np.linalg.norm(self.population[i] - self.population[j]) < diversity_threshold:\n                    self.population[j] = np.random.uniform(self.lower_bound, self.upper_bound, self.dim)\n                    self.fitness[j] = np.inf\n\n    def __call__(self, func):\n        self.initialize_population()\n        evaluations = 0\n        self.evaluate_population(func)\n        evaluations += self.population_size\n\n        while evaluations < self.budget:\n            self.preserve_diversity()\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                self.adaptive_differential_evolution(i, func)\n                evaluations += 1\n\n                if evaluations >= self.budget:\n                    break\n\n                self.stochastic_gradient_perturbation(i, func)\n                evaluations += 1\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "EnhancedHybridOptimizer", "description": "An enhanced hybrid optimizer that combines adaptive differential evolution with stochastic gradient perturbations and diversity preservation for robust exploration and convergence.", "configspace": "", "generation": 27, "fitness": 0.2445252352795292, "feedback": "The algorithm EnhancedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.24 with standard deviation 0.19.", "error": "", "parent_id": "390fb10b-b1a7-4fda-bfce-d7b6d4431e0f", "metadata": {"aucs": [0.6050803143929181, 0.5618775226176758, 0.586386661930114, 0.6092129238983668, 0.5815787745053942, 0.5901302508005304, 0.6009970393775491, 0.5925857990763002, 0.5494481573061742, 0.29853348148368675, 0.3030529728051621, 0.2910394885728459, 0.273174318323378, 0.28261347111163615, 0.3143308519167607, 0.3228489915673274, 0.290671742098063, 0.2906164944542071, 0.10873938913910353, 0.09446029948375634, 0.09379669684686631, 0.10401799572122017, 0.10406449061529843, 0.09835045106810525, 0.0950393706043624, 0.09668287669843445, 0.10118657847092727, 0.08807502215526164, 0.09382725318808693, 0.09284987152498869, 0.08984370701848343, 0.09666081715230523, 0.09157295729068315, 0.09168370774945578, 0.08967860211217615, 0.08869045684432442, 0.8664393201673308, 0.8925108192887965, 0.8730696629170289, 0.9143888643177951, 0.86387030068931, 0.9350920427575954, 0.9057454211015423, 0.9489818804865864, 0.8591881956372192, 0.25855302451195716, 0.2615336164314692, 0.2628105792773374, 0.24230597168714563, 0.24304290703180353, 0.262457601508741, 0.2684004600799258, 0.24436150088242115, 0.2616622313795567, 0.4849615173371873, 0.6329385098510101, 0.29825233724111133, 0.4657026012190352, 0.47239552070666724, 0.5467958822171883, 0.47947275224512675, 0.47121708531251805, 0.5161389644868438, 0.1595317841995364, 0.1435264576568801, 0.15349780827975124, 0.1357813200560859, 0.14245143640613167, 0.14288724161621902, 0.17541726726713824, 0.14279916922230573, 0.14290234873374297, 0.1560639008377981, 0.14412568476774978, 0.14237665906214914, 0.14637442959022862, 0.14284980369880018, 0.16494114646757607, 0.155635134102311, 0.1396282654885732, 0.16281737657800877, 0.10231807247738889, 0.08890624070870456, 0.08512833003998599, 0.07263904352802708, 0.07661389826651044, 0.0992674811552311, 0.09846970453063109, 0.08943457577902936, 0.08407435993493384, 0.1960846335473102, 0.2060183639508456, 0.1886520633513975, 0.16414799744810793, 0.17658816890839768, 0.21994145638103613, 0.21545592721184814, 0.2381906406365144, 0.21514308877412192, 0.01627495484264474, 0.005649728492448602, 0.0019503426029057191, 0.0168260817909055, 0.012126997314631627, 0.020883309532885863, 0.012594178201342898, 0.01615273461679334, 0.007399911945191517, 0.1383377688163422, 0.1260856860937527, 0.1334496051063594, 0.1580868987622227, 0.1455244583765829, 0.13049778758942887, 0.1277318071240554, 0.1257665605802435, 0.13646422786039691, 0.46015610524153405, 0.4626628515250687, 0.4641480334728215, 0.4619194696389166, 0.4402553481737407, 0.47133491447555875, 0.47056181704970645, 0.46264590421789764, 0.4744517230975662, 0.0986752674656487, 0.09128522260597272, 0.08889960755915072, 0.09873952868988889, 0.08704271589996793, 0.07857534110629805, 0.07934272606712833, 0.10269708424764468, 0.10743881026443547, 0.16819029191647517, 0.15865567025107008, 0.16551751732487896, 0.19512123263727654, 0.1782047876195798, 0.1661111895827151, 0.15013754651157651, 0.17053375424752526, 0.15411437412489193, 0.26772275910483345, 0.2567639604891947, 0.2895943873255621, 0.25658554800761735, 0.26504132635579347, 0.2702529307592675, 0.2714510555334486, 0.2831363037620154, 0.2722384067986562, 0.20503129267621012, 0.19655725069313468, 0.1912367532597281, 0.20004749498203633, 0.1993881010116515, 0.19997798353873564, 0.19180940543048486, 0.21475431745095508, 0.2085655291879862, 0.22005306882793163, 0.21379406873135287, 0.18547649685117262, 0.20401394039436505, 0.18838065863635445, 0.19014235649217226, 0.19095440837061262, 0.18291770377087901, 0.1859734379114728, 0.17519092216107224, 0.18784414421998696, 0.1843313407245294, 0.1762084792282017, 0.17616586029287007, 0.17122370758059346, 0.17306684094050828, 0.1780759698950829, 0.17353447448258508, 0.281542879115102, 0.3267477113441639, 0.24560462177286158, 0.35319924117789014, 0.2922150837280676, 0.3061662279245475, 0.2860483232188389, 0.3347729089659165, 0.20503481089621123, 0.3135048670607997, 0.2809323592291708, 0.4165577629520577, 0.28073642962449996, 0.292803188542206, 0.29909744909239533, 0.2129946902521379, 0.31717511735659454, 0.24603188452761926, 0.18288509475249526, 0.19293544481730873, 0.18063038032157386, 0.18552952148279478, 0.18843248811954272, 0.21421963307270997, 0.1874066354665378, 0.19594547610088042, 0.17919377282061832, 0.0751443389377604, 0.07653454926434422, 0.07333443693106179, 0.07868723787491294, 0.07015069171465749, 0.07183733410107229, 0.07339113314506795, 0.07951653926844415, 0.07741534350913393]}, "mutation_prompt": null}
{"id": "208589dc-059b-4897-ac17-f74d5f9acda2", "solution": "import numpy as np\n\nclass SynergisticFusionOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = 50\n        self.mutation_factor = 0.6\n        self.crossover_probability = 0.85\n        self.learning_rate = 0.05\n        self.social_influence = 0.4\n        self.population = None\n        self.fitness = None\n\n    def initialize_population(self):\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, \n                                            (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def adaptive_differential_evolution(self, idx, func):\n        indices = [i for i in range(self.population_size) if i != idx]\n        np.random.shuffle(indices)\n        a, b, c = indices[:3]\n        mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        adaptive_factor = 0.3 + 0.7 * np.random.rand()\n        mutant = self.population[idx] + adaptive_factor * (mutant - self.population[idx])\n        mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n        \n        cross_points = np.random.rand(self.dim) < self.crossover_probability\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        \n        trial = np.where(cross_points, mutant, self.population[idx])\n        trial_fitness = func(trial)\n        \n        if trial_fitness < self.fitness[idx]:\n            self.population[idx] = trial\n            self.fitness[idx] = trial_fitness\n\n    def swarm_influence(self, idx, func):\n        best_idx = np.argmin(self.fitness)\n        g_best = self.population[best_idx]\n        direction_vector = self.social_influence * (g_best - self.population[idx])\n        candidate = self.population[idx] + direction_vector\n        candidate = np.clip(candidate, self.lower_bound, self.upper_bound)\n        candidate_fitness = func(candidate)\n        if candidate_fitness < self.fitness[idx]:\n            self.population[idx] = candidate\n            self.fitness[idx] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population()\n        evaluations = 0\n        self.evaluate_population(func)\n        evaluations += self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                if np.random.rand() < 0.5:\n                    self.adaptive_differential_evolution(i, func)\n                else:\n                    self.swarm_influence(i, func)\n                evaluations += 1\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "SynergisticFusionOptimizer", "description": "A synergistic fusion optimizer that marries swarm intelligence dynamics with adaptive differential evolution for robust convergence across diverse landscapes.", "configspace": "", "generation": 28, "fitness": 0.15089534973089347, "feedback": "The algorithm SynergisticFusionOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.15 with standard deviation 0.12.", "error": "", "parent_id": "390fb10b-b1a7-4fda-bfce-d7b6d4431e0f", "metadata": {"aucs": [0.37652032442759387, 0.5256930807354252, 0.28108237211377307, 0.12655027789001483, 0.20450033156954683, 0.1621672843262253, 0.2921443522615371, 0.40565790091067655, 0.17565094774594725, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.09791294934425843, 0.1147855357475297, 0.11730751700305231, 0.12162710776852925, 0.11917731940457565, 0.14341854587149194, 0.09337287269513672, 0.13184730665814748, 0.13617013994155192, 0.08623795747146978, 0.08872329183112793, 0.09047347766516889, 0.09975553211671262, 0.10799007767100666, 0.08117681580085245, 0.10583942142423497, 0.10934569119703919, 0.07808213696372401, 0.06592220521558301, 0.06715929331007342, 0.06664637141212648, 0.06710553729988389, 0.06494321325262842, 0.05872529638658941, 0.07433252725060391, 0.0640160146521892, 0.051126245450299534, 0.15231889427585743, 0.16405681995973043, 0.11148104428336958, 0.08883431941103404, 0.06088978050167515, 0.06924420936109776, 9.999999999998899e-05, 0.08775262982242948, 0.023214322213334304, 0.3974047362448969, 0.3912461996409793, 0.20901808149176904, 0.17913112756452543, 0.2746598143828465, 0.27869760884671946, 0.17879169512650406, 0.1493887902442299, 0.12221130298552818, 0.26455102562471455, 0.1505968618761423, 0.12021994537410485, 9.999999999998899e-05, 0.12752444835662824, 9.999999999998899e-05, 0.15429446607797603, 0.0855140662279158, 0.029891211577949828, 0.24875923833131497, 0.13153160304614575, 0.15430347955661416, 0.14222305070463104, 0.12916889462395176, 0.13804701655081952, 0.1403044136824254, 0.13023192601163647, 0.15270295249951538, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.021165404558681433, 9.999999999998899e-05, 9.999999999998899e-05, 0.14140352384535837, 0.04601761595726184, 0.09485266843924767, 0.06801740242203635, 0.04759489399719807, 0.05931692813497391, 0.09697425133081072, 0.06707918371519461, 0.09621267956843071, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.054504073981984424, 0.1307382245441795, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.0233395676561009, 9.999999999998899e-05, 9.999999999998899e-05, 9.999999999998899e-05, 0.026959474824024388, 0.02704680355995437, 0.09848822520917766, 0.020845253565905608, 0.3135827301444487, 0.3759647310348535, 0.2752453063270478, 0.2559704271505421, 0.2095183730744138, 0.16616891288109603, 0.48504804737273466, 0.37172507978428937, 0.3499162882748146, 0.08198242951347567, 0.0626462812662083, 0.0498908909359026, 0.12882428893046505, 0.11421734413844276, 0.09240898738156467, 0.11480239529834435, 0.13485796899348368, 0.1325886009554006, 0.13044088972161683, 0.14693275199419764, 0.11915087565192184, 0.1733561418651921, 0.12034793013016076, 0.13620295593323206, 0.13521823525510712, 0.14527770906319748, 0.19252147142671316, 0.33168678561426646, 0.3782282391997114, 0.3117787058367798, 0.2915240858548209, 0.2500352070077272, 0.2824102903767318, 0.20773142144629486, 0.2989257567921473, 0.27018800760043504, 0.15734001797988928, 0.23532120261613998, 0.3653650760272803, 0.24430744724481324, 0.18775237860571126, 0.22016036423153262, 0.1575380656204639, 0.2612720282248402, 0.20581208429385323, 0.20352125721195857, 0.21018144986634402, 0.18930349161788163, 0.22437264283484215, 0.23895431346598694, 0.2069981585443742, 0.23316500703799004, 0.22360135200475495, 0.19957522832054586, 0.18763370707673954, 0.1814508577996775, 0.19242381383713802, 0.19197966214667417, 0.20305352132026755, 0.18869091528686988, 0.18630612575973116, 0.18545013173244573, 0.19633203061578353, 0.41409154624202926, 0.18839963164540352, 0.18234298212424715, 0.5397205797886551, 0.20011485337622992, 0.5193152585796776, 0.7451002431263406, 0.16242007713469542, 0.4834858213419019, 0.4209551918373231, 0.20704868479669192, 0.2109487869646911, 0.16736149355525953, 0.18622869467154735, 0.1676517777712343, 0.27302177242652825, 0.16612195172003608, 0.22246742315036283, 0.1764165658588117, 0.1797982284954598, 0.18512042807363005, 0.1781007078213115, 0.17903510471917528, 0.1766977754081943, 0.18757450610799986, 0.176290742401929, 0.17461334938933792, 0.08151252133946196, 0.08157294266758985, 0.09302666699452256, 0.09100058459268168, 0.08838633472821733, 0.10641501777004359, 0.07138269179416534, 0.0917722966064215, 0.08765400672300316]}, "mutation_prompt": null}
{"id": "ab30eca6-cb36-4301-883c-b2e0ce352ed5", "solution": "import numpy as np\n\nclass DynamicStrategyOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = 50  # Increased slightly for better diversity\n        self.mutation_factor = 0.9  # Enhanced for stronger exploration\n        self.crossover_probability = 0.8  # Adjusted for balance in recombination\n        self.learning_rate = 0.05  # Reduced for fine-tuned local search\n        self.population = None\n        self.fitness = None\n\n    def initialize_population(self):\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, \n                                            (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def adaptive_differential_evolution(self, idx, func):\n        indices = [i for i in range(self.population_size) if i != idx]\n        np.random.shuffle(indices)\n        a, b, c = indices[:3]\n        mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        adaptive_factor = 0.6 + 0.4 * np.random.rand()  # Adjusted range for exploration\n        mutant = self.population[idx] + adaptive_factor * (mutant - self.population[idx])\n        cross_points = np.random.rand(self.dim) < self.crossover_probability\n        if not np.any(cross_points):\n            index = np.random.randint(0, self.dim)\n            cross_points[index] = True\n        \n        trial = np.where(cross_points, mutant, self.population[idx])\n        trial = np.clip(trial, self.lower_bound, self.upper_bound)\n        trial_fitness = func(trial)\n        \n        if trial_fitness < self.fitness[idx]:\n            self.population[idx] = trial\n            self.fitness[idx] = trial_fitness\n\n    def variational_stochastic_update(self, idx, func):\n        gradient = np.random.uniform(-2, 2, self.dim)  # Expanded to allow more diverse searches\n        adaptive_lr = self.learning_rate / (1 + idx / self.population_size)\n        hessian_approx = np.random.uniform(0.2, 0.6, self.dim)  # Broader range for more variability\n        candidate = self.population[idx] - adaptive_lr * gradient / (np.abs(hessian_approx) + 1e-9)\n        candidate = np.clip(candidate, self.lower_bound, self.upper_bound)\n        candidate_fitness = func(candidate)\n        if candidate_fitness < self.fitness[idx]:\n            self.population[idx] = candidate\n            self.fitness[idx] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population()\n        evaluations = 0\n        self.evaluate_population(func)\n        evaluations += self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                self.adaptive_differential_evolution(i, func)\n                evaluations += 1\n\n                if evaluations >= self.budget:\n                    break\n\n                self.variational_stochastic_update(i, func)\n                evaluations += 1\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "DynamicStrategyOptimizer", "description": "A dynamic strategy optimizer that blends adaptive differential evolution with variational stochastic updates for enhanced convergence in diverse problem landscapes.", "configspace": "", "generation": 29, "fitness": 0.2454934478349193, "feedback": "The algorithm DynamicStrategyOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.25 with standard deviation 0.19.", "error": "", "parent_id": "390fb10b-b1a7-4fda-bfce-d7b6d4431e0f", "metadata": {"aucs": [0.6131668928703129, 0.6530506654988273, 0.6249518621004191, 0.598758014543473, 0.6284404987151029, 0.6377798421544382, 0.6437657450756873, 0.6035723295619067, 0.6391566009727494, 0.3182415848245471, 0.30175767617186733, 0.30770768891405387, 0.2971912660489249, 0.28840682015376684, 0.30859690222687863, 0.31293117444498, 0.3068963748738134, 0.29729167367815645, 0.11757600663199341, 0.11954376250856569, 0.1188696922309237, 0.11458314646717227, 0.10492043638030335, 0.13263794226313674, 0.10698288686856106, 0.10790119140138121, 0.1107369526801587, 0.101317660474456, 0.09086493959916575, 0.09606855738013187, 0.09754623873660517, 0.08957236763421661, 0.09571764244280023, 0.11421057859236994, 0.09595375080511293, 0.0905006292017988, 0.8719768003812277, 0.8190532021487663, 0.8383186721566246, 0.8775573518850484, 0.8759554321021273, 0.8157273867767908, 0.802089048617291, 0.8805459139732975, 0.8443287847598677, 0.2621639933825126, 0.2468138168257642, 0.23427934264029737, 0.24394193683475274, 0.2502434822608586, 0.24922352586043117, 0.26614470424918113, 0.2587753199444768, 0.24965797369014342, 0.4269175406959014, 0.4041220075432119, 0.35052801710047277, 0.5231799154784806, 0.4323751399236385, 0.44426239221178265, 0.44565143320353906, 0.3670878747347339, 0.4400300073452226, 0.14860190969401688, 0.16259849980325725, 0.13400136172896615, 0.15921542412229106, 0.1643639276256712, 0.16016189261442415, 0.15158073376679515, 0.16510196984246284, 0.16948380531486196, 0.17988467980021783, 0.15633691553093554, 0.12230043907678156, 0.1462533553514741, 0.1654871651037878, 0.1462095752914222, 0.17239582990938795, 0.1296658175754708, 0.14560457103485203, 0.05066350075654269, 0.055663047385680176, 0.06832247156840854, 0.06780726753494659, 0.04068187089201858, 0.056964275412458076, 0.07464141970913463, 0.0668465570085851, 0.08654395702621243, 0.20246689901665604, 0.17277784760504078, 0.20108714760028135, 0.16874478980806107, 0.17675791592066437, 0.1519137452059156, 0.22419507930621618, 0.17711768361058655, 0.1841273442132927, 0.01679545619695677, 0.0024217595932496394, 0.008997775485774406, 0.008227733799478165, 0.012817767821867387, 0.009602277460966091, 0.031922747468105306, 0.01342305900427998, 0.047622376693679525, 0.11572815490042299, 0.13389930159004348, 0.12261713456439716, 0.13499871801687202, 0.13728868971783736, 0.138669052989997, 0.13736867997700042, 0.1296382872316102, 0.14069370166126283, 0.44873506887850545, 0.44991104430992157, 0.4650332347497522, 0.4507900439375506, 0.45028771698831316, 0.44720735438332193, 0.4801273712589583, 0.45694259621104283, 0.4962933435294019, 0.10895552947040998, 0.0906300331222345, 0.10357040177324883, 0.120826953146292, 0.09116669999463212, 0.10438857717604455, 0.13393157562658076, 0.09860625577643145, 0.09515846353219126, 0.1775662703526808, 0.212507346595459, 0.19258640696108953, 0.19292436986387862, 0.18130422080041686, 0.21202249897867453, 0.2035319563711876, 0.17502758840665755, 0.18917512784811052, 0.2372084274522097, 0.24576214300618426, 0.25487880629388115, 0.26031582449018575, 0.2367194680251099, 0.25035420819647924, 0.2666822558469274, 0.2503240065263953, 0.26920954734945535, 0.17524588261288687, 0.19995893886869198, 0.17949010501858864, 0.1916158948587796, 0.18688614660369574, 0.19339313583964246, 0.2096009385612616, 0.21399305042249406, 0.203009507829085, 0.2017174804050661, 0.2058088678778719, 0.2035356596025425, 0.18353815609667623, 0.2099630136675037, 0.22596395028348837, 0.20714027482329633, 0.1817477093454244, 0.18758477165490361, 0.1903356232736021, 0.18379430548406006, 0.1840308771300192, 0.19587955015237246, 0.17961459497508936, 0.19191988797371362, 0.18497376824152112, 0.19254658221040477, 0.18934260286399018, 0.38729586065753796, 0.33438419670662733, 0.3174218894033919, 0.3712609504190415, 0.38505327318292615, 0.3633869520337558, 0.2683012044768447, 0.322510142693718, 0.3650289577451056, 0.4254652234225085, 0.34612197898511243, 0.27751251040432556, 0.31151874606019114, 0.24666906736884142, 0.2928960509613767, 0.23338802208655784, 0.3740168629608497, 0.35587575359436197, 0.1881012399238896, 0.1797212096938885, 0.19833750785734672, 0.1853325661215124, 0.1938879745368065, 0.17135920389679582, 0.18223829515037182, 0.19125745056111887, 0.1762958377291336, 0.06737473523044646, 0.0697148925138843, 0.07939086269658457, 0.0724142534026756, 0.07363369558493482, 0.07906794427825592, 0.07796276734038643, 0.07771689811239324, 0.07395336184580414]}, "mutation_prompt": null}
{"id": "687bd633-2480-4a8d-874c-908d069edc63", "solution": "import numpy as np\n\nclass HybridSGDEOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = 50\n        self.mutation_factor = 0.9\n        self.crossover_probability = 0.8\n        self.learning_rate = 0.05\n        self.population = None\n        self.fitness = None\n\n    def initialize_population(self):\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, \n                                            (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def adaptive_differential_evolution(self, idx, func):\n        indices = [i for i in range(self.population_size) if i != idx]\n        np.random.shuffle(indices)\n        a, b, c = indices[:3]\n        mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        adaptive_factor = 0.4 + 0.6 * np.random.rand()\n        mutant = self.population[idx] + adaptive_factor * (mutant - self.population[idx])\n        mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n        \n        cross_points = np.random.rand(self.dim) < self.crossover_probability\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        \n        trial = np.where(cross_points, mutant, self.population[idx])\n        trial_fitness = func(trial)\n        \n        if trial_fitness < self.fitness[idx]:\n            self.population[idx] = trial\n            self.fitness[idx] = trial_fitness\n\n    def stochastic_gradient_descent(self, idx, func):\n        gradient = np.random.normal(0, 1, self.dim)\n        adaptive_lr = self.learning_rate / np.sqrt(1 + idx)\n        candidate = self.population[idx] - adaptive_lr * gradient\n        candidate = np.clip(candidate, self.lower_bound, self.upper_bound)\n        candidate_fitness = func(candidate)\n        if candidate_fitness < self.fitness[idx]:\n            self.population[idx] = candidate\n            self.fitness[idx] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population()\n        evaluations = 0\n        self.evaluate_population(func)\n        evaluations += self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                self.adaptive_differential_evolution(i, func)\n                evaluations += 1\n\n                if evaluations >= self.budget:\n                    break\n\n                self.stochastic_gradient_descent(i, func)\n                evaluations += 1\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "HybridSGDEOptimizer", "description": "A novel hybrid optimizer using differential evolution with stochastic gradient descent to balance exploration and exploitation effectively.", "configspace": "", "generation": 30, "fitness": 0.2613409073408053, "feedback": "The algorithm HybridSGDEOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.26 with standard deviation 0.18.", "error": "", "parent_id": "390fb10b-b1a7-4fda-bfce-d7b6d4431e0f", "metadata": {"aucs": [0.6841800217184981, 0.7045589450126943, 0.677843590705257, 0.6874272619005803, 0.6802617831240468, 0.6924032746171505, 0.6859122171216346, 0.6903071897420698, 0.650508015422532, 0.4138116163879918, 0.4267189904782356, 0.41890949214429085, 0.3973227623211948, 0.39069852573820474, 0.4359966240916391, 0.4194528618596741, 0.3977313873998236, 0.4034344168131515, 0.13292245219099608, 0.14153174634216759, 0.13276268746754305, 0.13141673908348972, 0.13197450963565638, 0.12875237526579186, 0.1571590417357327, 0.14047229730163213, 0.1386384544356457, 0.11658540285000851, 0.10061369933487407, 0.11923480810936782, 0.10523253261137466, 0.11797093770596878, 0.099155750771865, 0.10680588695869775, 0.12217726137658225, 0.10052125602134199, 0.7011870171138072, 0.6952421369368758, 0.6852016726875048, 0.7573151071202017, 0.9144039369409599, 0.7697236573123605, 0.7598520740070547, 0.7715393239038809, 0.7984037222504899, 0.27521287053718335, 0.2761446451615476, 0.23046435075784955, 0.2768810054554164, 0.23557992501851432, 0.28977461062022225, 0.2504443562779536, 0.26656214610306583, 0.26474323958330004, 0.5484405331661255, 0.47754609031467266, 0.6065176338614122, 0.5514395001915624, 0.6149058688571858, 0.5753222593368588, 0.5079198876116655, 0.5855936126964385, 0.48433727216544487, 0.18156357119384237, 0.135609352871929, 0.15521326746639574, 0.16537867357070624, 0.14692493661951067, 0.1324432391827013, 0.16086462021988723, 0.16911235947012115, 0.17263039159090876, 0.1416002692709164, 0.16399378285693822, 0.14640965187877586, 0.1621737193167524, 0.1520695192284074, 0.1553230238319353, 0.14343885149392221, 0.155877884718307, 0.14300274750241349, 0.12410028142021001, 0.12035062089758031, 0.09500927765658185, 0.10122715168874108, 0.09990107025311168, 0.11174253684921875, 0.12308615700294578, 0.12584966083078342, 0.12001363560075029, 0.2354265218167355, 0.23453777081932736, 0.26905606121101766, 0.228056157844845, 0.22089727018556504, 0.19081562137689256, 0.2588598943087759, 0.2589405544781538, 0.2735326602992917, 0.06497793715004441, 0.045803985335833186, 0.05176688504109339, 0.07368404048138932, 0.06364292798883542, 0.06493617672947516, 0.10343099518666388, 0.06937800140981398, 0.061038889413041364, 0.16517426886067388, 0.15770950832236708, 0.16129328417667188, 0.180096184320226, 0.18110927152794498, 0.1807807221239902, 0.14575601861270648, 0.1618595092337075, 0.16966652171776708, 0.48075243178270355, 0.5033128785083962, 0.5154676377373242, 0.5122592247689564, 0.5226551301161639, 0.5046217402958264, 0.5204201111391232, 0.5283030515071593, 0.5161310585081462, 0.11009132741911543, 0.12750606789803787, 0.10850541093038879, 0.12354740735624636, 0.11755317541931887, 0.13449226894214883, 0.1094965566014835, 0.13944943401079457, 0.12115185498150383, 0.1684914103352645, 0.150759000250423, 0.15451692558996533, 0.14499502728145164, 0.1469348566917581, 0.14660732018670608, 0.17291108596951832, 0.16861408354151008, 0.156363463419797, 0.2806114463809444, 0.25925319703837924, 0.26853175948289243, 0.2679535006292475, 0.28759624563080566, 0.26628060344601123, 0.2692355642642107, 0.29288271864096305, 0.2656269360489336, 0.2270590173389453, 0.20515833166611308, 0.21781483324588902, 0.1860059619826816, 0.1917806862362812, 0.20436866454325475, 0.2163309268502066, 0.22816431791672154, 0.21022376164970558, 0.24402986443064245, 0.21368982137214187, 0.21765055150259738, 0.2259601491765827, 0.21516907824162324, 0.22095676720734203, 0.20122307231682646, 0.2633420861881398, 0.2154788149938942, 0.19274184034912023, 0.18653208335804305, 0.18857332321699405, 0.19430778722972242, 0.18480611007715753, 0.1777423129033444, 0.20724700382330996, 0.19833428101479378, 0.18888583358483135, 0.1989263031269658, 0.24175713861313874, 0.1648225139128845, 0.32295505112520473, 0.2688264435132427, 0.27627431133499414, 0.24670605776103904, 0.1541637236631459, 0.25203175599137306, 0.2967626088135631, 0.2162820320616734, 0.22658945239685468, 0.3170993509838197, 0.17773081069358154, 0.23867030416959334, 0.20897341567650607, 0.19452478204613155, 0.19599296821218426, 0.21146026436177112, 0.20850438541659255, 0.2070686178375547, 0.20605617226066153, 0.21113003090852067, 0.2184650168658967, 0.20447686858800906, 0.21349542585705494, 0.21901348426860123, 0.09468201776888541, 0.09736256145254596, 0.09007344175038101, 0.09017154079584, 0.09297927495835845, 0.08592419631115011, 0.09458076298964768, 0.09470550480116402, 0.09369503291277548]}, "mutation_prompt": null}
{"id": "342f43a4-684e-48ef-97f2-273de36e6633", "solution": "import numpy as np\n\nclass StochasticEvolutionaryGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = 40\n        self.mutation_factor = 0.85\n        self.crossover_probability = 0.9\n        self.learning_rate = 0.15\n        self.population = None\n        self.fitness = None\n\n    def initialize_population(self):\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, \n                                            (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def adaptive_differential_evolution(self, idx, func):\n        indices = [i for i in range(self.population_size) if i != idx]\n        np.random.shuffle(indices)\n        a, b, c = indices[:3]\n        mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n        \n        cross_points = np.random.rand(self.dim) < self.crossover_probability\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        \n        trial = np.where(cross_points, mutant, self.population[idx])\n        trial_fitness = func(trial)\n        \n        if trial_fitness < self.fitness[idx]:\n            self.population[idx] = trial\n            self.fitness[idx] = trial_fitness\n\n    def stochastic_gradient_update(self, idx, func):\n        gradient = np.random.uniform(-1, 1, self.dim)\n        adaptive_lr = self.learning_rate / (1 + np.random.rand())\n        candidate = self.population[idx] - adaptive_lr * gradient\n        candidate = np.clip(candidate, self.lower_bound, self.upper_bound)\n        candidate_fitness = func(candidate)\n        if candidate_fitness < self.fitness[idx]:\n            self.population[idx] = candidate\n            self.fitness[idx] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population()\n        evaluations = 0\n        self.evaluate_population(func)\n        evaluations += self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                self.adaptive_differential_evolution(i, func)\n                evaluations += 1\n\n                if evaluations >= self.budget:\n                    break\n\n                self.stochastic_gradient_update(i, func)\n                evaluations += 1\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "StochasticEvolutionaryGradientDescent", "description": "Stochastic Evolutionary Gradient Descent (SEGD) combines differential evolution with stochastic gradient updates for robust exploration and efficient convergence.", "configspace": "", "generation": 31, "fitness": 0.23523274838920133, "feedback": "The algorithm StochasticEvolutionaryGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.24 with standard deviation 0.19.", "error": "", "parent_id": "390fb10b-b1a7-4fda-bfce-d7b6d4431e0f", "metadata": {"aucs": [0.491820066669744, 0.46974889635588246, 0.5011052036031943, 0.542568417168159, 0.5587395894772734, 0.5357452197416317, 0.5071465593476496, 0.5513574962551531, 0.5374116470514536, 0.21861623064740965, 0.21717776605772843, 0.20220184723404866, 0.15962602980350749, 0.20245821550995002, 0.22175410421473973, 0.21959246952579492, 0.2052344804325953, 0.21060153697688833, 0.11798659256019695, 0.10546543822534127, 0.11639356599430484, 0.11210055112235617, 0.1014941453252941, 0.0952914347198015, 0.11529096427214358, 0.10714514598691294, 0.11887430060374538, 0.08854821705772498, 0.09957901723675755, 0.08966548639747285, 0.11272990196631638, 0.09230647892114974, 0.10347180416112711, 0.09686666765375307, 0.09786048572997152, 0.09460610902881927, 0.8969038466377577, 0.8735162686236531, 0.9282760151810001, 0.9717206051176663, 0.9026125397299684, 0.9252763079661359, 0.8779055986025815, 0.9041455195191022, 0.9295728541872945, 0.2590495544266457, 0.2217281515816718, 0.24427383228290767, 0.22674947371842202, 0.2151636742544265, 0.22211620693475675, 0.24471830409451234, 0.24105176492034397, 0.24433276040156648, 0.43103257405758955, 0.36263359488194447, 0.3521728022920806, 0.46663048052206246, 0.43391680347011685, 0.42223158991647336, 0.4242380581612921, 0.4069637355187332, 0.49410349303174594, 0.1853501665160857, 0.17485969078246055, 0.12266554613165137, 0.1784230126202252, 0.1580774161281071, 0.20113476197514324, 0.13799995805409526, 0.17609486405554076, 0.17370475876814595, 0.13052099860171507, 0.11676721774884558, 0.12376491953824265, 0.1804055735511627, 0.17090917775908843, 0.15397154330105933, 0.14193216028206002, 0.13358436412209962, 0.1404591962202636, 0.074407427454242, 0.0833445722626337, 0.08103165582476524, 0.12230466734374712, 0.09213842191306554, 0.06363831967536426, 0.06991323820014828, 0.05938856745348764, 0.06604910467969949, 0.18485692220488315, 0.1900938870470138, 0.21535422934277526, 0.1986689580463592, 0.17779727912432974, 0.18696199193001617, 0.1929408478088629, 0.19183183092136247, 0.21003434047170155, 0.007595537657646534, 0.012910235219039001, 0.0232822413337046, 0.0035513010871854878, 0.012657766541285143, 0.022151730164764016, 0.022395735885218704, 0.006290278674063288, 0.01295435013284385, 0.1293777552556239, 0.1381549923768869, 0.1266613987698021, 0.14833872263524284, 0.13871349730843718, 0.12374347676247122, 0.1295925261805423, 0.12737589964116547, 0.12755358890648005, 0.4081085940937401, 0.4153160597108734, 0.4202754239870111, 0.4226108459193978, 0.42933977397532197, 0.40102165244887455, 0.41676769050385476, 0.4049336651830857, 0.43045960498465796, 0.0923219739991793, 0.09783846995952994, 0.10364496940087631, 0.09504695612224245, 0.11066045598087981, 0.10523480178835742, 0.1109534802404627, 0.09287616558877054, 0.0988888389386644, 0.16307357849227833, 0.16818579815818524, 0.16720542026230656, 0.19334018568234368, 0.17366670414321028, 0.18169641187933372, 0.24606968684551833, 0.2157498647557874, 0.17527695563530865, 0.23681441550639837, 0.24911661155879372, 0.25388901571858147, 0.22978033815984367, 0.2012629150282481, 0.20915006420530435, 0.26690466104104293, 0.23932404009911412, 0.2492496456319051, 0.19280811937670006, 0.16658324119067436, 0.18799897182427816, 0.14653754515022266, 0.15515888758080854, 0.15575983121661408, 0.18722188714368582, 0.19189448281199617, 0.1770101943785266, 0.21266924161368206, 0.1931194583381416, 0.17678472448645766, 0.193191873096084, 0.1795800151019704, 0.1716632663187374, 0.19515586757838943, 0.19286550517044454, 0.16739189223408535, 0.21001878631632187, 0.19600260687337845, 0.18954160951384857, 0.17663208162695487, 0.18059735824151246, 0.17935702211614257, 0.25036786305700265, 0.1709697782345655, 0.19084688734598, 0.1650603338000648, 0.49499858559965804, 0.4341996691719394, 0.48247928413280083, 0.18013896275730368, 0.2662510625103245, 0.3902055130465275, 0.3632764548368115, 0.47922836036744865, 0.39004879000338977, 0.19306597784358215, 0.333876871245972, 0.4038694907455418, 0.3419134638473402, 0.391515135386748, 0.2066823581290158, 0.19832053387281945, 0.2017012440683208, 0.1764232805286049, 0.1821676941290239, 0.19851129251514332, 0.17627010516080532, 0.18766557044733823, 0.18305375343622543, 0.18456386582321027, 0.18141957530203234, 0.19770046378268258, 0.07287890192703927, 0.0647878931094158, 0.07141185628631286, 0.07146132869806332, 0.08051574654164362, 0.08115404084410727, 0.061934334076387265, 0.06919141245626848, 0.07224459660978433]}, "mutation_prompt": null}
{"id": "c99fdca9-3efb-4c91-82f2-7a2ca8c86fcb", "solution": "import numpy as np\n\nclass ChaoticAdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = 40\n        self.mutation_factor = 0.9\n        self.crossover_probability = 0.8\n        self.learning_rate = 0.1\n        self.population = None\n        self.fitness = None\n\n    def initialize_population(self):\n        # Using a chaotic map for initialization\n        chaotic_map = np.random.rand(self.population_size, self.dim)\n        self.population = self.lower_bound + (self.upper_bound - self.lower_bound) * chaotic_map\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def adaptive_differential_evolution(self, idx, func):\n        indices = [i for i in range(self.population_size) if i != idx]\n        np.random.shuffle(indices)\n        a, b, c = indices[:3]\n        mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        adaptive_factor = 0.2 + 0.8 * np.random.rand()\n        mutant = self.population[idx] + adaptive_factor * (mutant - self.population[idx])\n        mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n        \n        cross_points = np.random.rand(self.dim) < self.crossover_probability\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        \n        trial = np.where(cross_points, mutant, self.population[idx])\n        trial_fitness = func(trial)\n        \n        if trial_fitness < self.fitness[idx]:\n            self.population[idx] = trial\n            self.fitness[idx] = trial_fitness\n\n    def enhanced_newton_update(self, idx, func):\n        gradient = np.random.uniform(-1, 1, self.dim)\n        adaptive_lr = self.learning_rate / (1 + (idx / self.population_size) ** 2)\n        hessian_approx = np.random.uniform(0.1, 0.4, self.dim)\n        candidate = self.population[idx] - adaptive_lr * gradient / (np.abs(hessian_approx) + 1e-9)\n        candidate = np.clip(candidate, self.lower_bound, self.upper_bound)\n        candidate_fitness = func(candidate)\n        if candidate_fitness < self.fitness[idx]:\n            self.population[idx] = candidate\n            self.fitness[idx] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population()\n        evaluations = 0\n        self.evaluate_population(func)\n        evaluations += self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                self.adaptive_differential_evolution(i, func)\n                evaluations += 1\n\n                if evaluations >= self.budget:\n                    break\n\n                self.enhanced_newton_update(i, func)\n                evaluations += 1\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "ChaoticAdaptiveHybridOptimizer", "description": "An adaptive hybrid optimization algorithm that improves exploration through chaotic mapping for initialization and refines convergence using dynamic crossover and mutation strategies.", "configspace": "", "generation": 32, "fitness": 0.3039660175498447, "feedback": "The algorithm ChaoticAdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.30 with standard deviation 0.21.", "error": "", "parent_id": "390fb10b-b1a7-4fda-bfce-d7b6d4431e0f", "metadata": {"aucs": [0.7680744514039699, 0.7728322987073389, 0.7688061901845977, 0.7430443164250204, 0.7564936112698245, 0.7480028746487839, 0.7659879820067503, 0.7710239553073336, 0.7685103514036694, 0.5426465520678376, 0.5320860585934901, 0.5563688574606999, 0.5506574831307736, 0.5480930669041211, 0.5192285081892768, 0.5523642230121595, 0.5284314219983605, 0.5166509118746174, 0.10810775527937455, 0.11101330056549086, 0.09763665570491209, 0.11423767404298801, 0.10951553659487911, 0.12078759030980546, 0.12408349726361889, 0.14372963817921758, 0.11249653387509984, 0.09820704537786351, 0.08746638204840018, 0.09315278479967581, 0.09621369889681664, 0.09398814187178506, 0.09869308595758663, 0.09312277697611793, 0.10083962601707863, 0.10275534927493224, 0.807549241643993, 0.8379701384669305, 0.8119512218635783, 0.8233758258021902, 0.8516487514766921, 0.8316750222888604, 0.846905748767627, 0.8570641136471109, 0.8639254576599155, 0.28568358786509096, 0.30525734497816415, 0.32539836410795, 0.32191735376921193, 0.3153989598049025, 0.3409371209599231, 0.33742443671725486, 0.32144524062888435, 0.28430960885413725, 0.6141027646458475, 0.5773911136607297, 0.6496452832819934, 0.6930094765215709, 0.6260185617719711, 0.6473525846194662, 0.6146105615135786, 0.6607251771325733, 0.6810212554779834, 0.16278793477688125, 0.13809666694221245, 0.2128980538253653, 0.1429037029967738, 0.16105724175956204, 0.19347260113815723, 0.15920486734843908, 0.21838993993195577, 0.22874087430737644, 0.15655439386713876, 0.17176406816298972, 0.1661921567654101, 0.16221597935011156, 0.16129217330604284, 0.21700216266593764, 0.19127347178935183, 0.21054484832709075, 0.15662179730122316, 0.138272946201489, 0.14138898262631427, 0.19504545482783875, 0.19053722864217493, 0.1868265768557733, 0.16617576119543886, 0.20406148332059904, 0.17728169557888973, 0.20044004618591993, 0.3527439604778839, 0.28779878753478705, 0.2878365695994267, 0.28268203654232227, 0.2669950446246049, 0.24245509759020578, 0.33821693537393904, 0.3233940700798985, 0.2903401310620978, 0.08459778336349633, 0.0978810323087117, 0.08931989778921723, 0.09964828978649853, 0.09192653298614462, 0.11052066728073873, 0.07219474688117045, 0.06480548224124671, 0.09535779672454103, 0.21511703470180243, 0.2046819582727527, 0.19752454753364324, 0.21800069121603927, 0.22422169686756632, 0.2295286538436464, 0.18890293839815875, 0.2054028220452031, 0.17263989113760492, 0.5977675039719477, 0.5975010639068343, 0.6047736084938756, 0.585254534992303, 0.5976146984608701, 0.5756764277120878, 0.5798097443755852, 0.6077320936889306, 0.6239613667648842, 0.09449314202517778, 0.10500933743998198, 0.10134964234569366, 0.10309519468575468, 0.09690161534062591, 0.08910782485339352, 0.10290583893222771, 0.09600139824759812, 0.0935985157769561, 0.18670812143435633, 0.19293969883960682, 0.19368281961554756, 0.18751893854402935, 0.22660759365305905, 0.20338232830968905, 0.18109032044033258, 0.16963956595976326, 0.17707173445240965, 0.29983201441120777, 0.30731628338478434, 0.29689201271357757, 0.32158185109322857, 0.31716442643041576, 0.3164395891216031, 0.3028914011097862, 0.34667984109313865, 0.31997183699317744, 0.24512255048943932, 0.23297803401329786, 0.23021475038378147, 0.22357044767024548, 0.23956468322229607, 0.22531146573293215, 0.22293456244353127, 0.23309730748995272, 0.23962346663447842, 0.1909665425481245, 0.24620429524622034, 0.1959962671162533, 0.21792302556027965, 0.20401522566957397, 0.20021323191535723, 0.22839575776203214, 0.22154146014633458, 0.211129019001836, 0.18199684227847024, 0.18023019309652077, 0.18351571321177917, 0.18276346637951735, 0.18484569298805875, 0.18858483802208803, 0.19576106185533548, 0.18273993716729764, 0.17973022557995677, 0.3842725575846345, 0.4854323003863579, 0.3155342019863713, 0.4590999455556617, 0.3176319006908198, 0.43020644461017965, 0.3560906235435679, 0.336393401441088, 0.34662772274848497, 0.5388028509479932, 0.4107305854036476, 0.5096783968118961, 0.32391303091800006, 0.31180911475669204, 0.32078233285030355, 0.3226525315099983, 0.28566802574984274, 0.4019933630591752, 0.18975730428918702, 0.1912588692228413, 0.1838725298167211, 0.18535969780661066, 0.18203274629916277, 0.1897975473720802, 0.1835689101992899, 0.18545366228430082, 0.18329480919971497, 0.09629838633933852, 0.07741091837143244, 0.08157169928120145, 0.0895013432400712, 0.08139088166455799, 0.09138225682260726, 0.0792661775724175, 0.07957518154601295, 0.08477083383254724]}, "mutation_prompt": null}
{"id": "98f70f61-4f86-435a-9eee-f7bb95a81b4d", "solution": "import numpy as np\n\nclass RefinedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = 40\n        self.mutation_factor = 0.8\n        self.crossover_probability = 0.9\n        self.learning_rate = 0.1\n        self.population = None\n        self.fitness = None\n\n    def initialize_population(self):\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, \n                                            (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def adaptive_differential_evolution(self, idx, func):\n        indices = [i for i in range(self.population_size) if i != idx]\n        np.random.shuffle(indices)\n        a, b, c = indices[:3]\n        mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        adaptive_factor = 0.5 + 0.5 * np.random.rand()\n        mutant = self.population[idx] + adaptive_factor * (mutant - self.population[idx])\n        mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n        \n        cross_points = np.random.rand(self.dim) < self.crossover_probability\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        \n        trial = np.where(cross_points, mutant, self.population[idx])\n        trial_fitness = func(trial)\n        \n        if trial_fitness < self.fitness[idx]:\n            self.population[idx] = trial\n            self.fitness[idx] = trial_fitness\n\n    def randomized_newton_update(self, idx, func):\n        gradient = np.random.uniform(-1, 1, self.dim)\n        adaptive_lr = self.learning_rate / (1 + idx / self.population_size)\n        hessian_approx = np.random.uniform(0.1, 0.5, self.dim)\n        candidate = self.population[idx] - adaptive_lr * gradient / (np.abs(hessian_approx) + 1e-9)\n        candidate = np.clip(candidate, self.lower_bound, self.upper_bound)\n        candidate_fitness = func(candidate)\n        if candidate_fitness < self.fitness[idx]:\n            self.population[idx] = candidate\n            self.fitness[idx] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population()\n        evaluations = 0\n        self.evaluate_population(func)\n        evaluations += self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                self.adaptive_differential_evolution(i, func)\n                evaluations += 1\n\n                if evaluations >= self.budget:\n                    break\n\n                self.randomized_newton_update(i, func)\n                evaluations += 1\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "RefinedHybridOptimizer", "description": "A refined hybrid optimization algorithm that enhances exploration through stochastic rank-based selection and incorporates adaptive learning rate adjustments for improved convergence.", "configspace": "", "generation": 19, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "390fb10b-b1a7-4fda-bfce-d7b6d4431e0f", "metadata": {"aucs": [0.7998926377262636, 0.761429726165544, 0.7863085107779331, 0.7508095948300958, 0.7882340583889212, 0.7816710181219415, 0.7826294649647488, 0.7918404048180104, 0.7676018171732987, 0.5689355814544976, 0.5786091347521873, 0.6064432841944956, 0.4875777471533822, 0.5880862128826672, 0.5875553949838622, 0.5452458885894269, 0.549792395120432, 0.5742927984304333, 0.10548247785135645, 0.11285201831791869, 0.0925734721780318, 0.11436849439951724, 0.10592419427335498, 0.11336423572081245, 0.10345862089413227, 0.1269952222453321, 0.12152214562989194, 0.10204477557831337, 0.09901425922888485, 0.09577110764085917, 0.09397947463880463, 0.09193976260094838, 0.09380899156179712, 0.09210822057585066, 0.09527236403031891, 0.09495208182919956, 0.8203943085242258, 0.8371989577902406, 0.8144215895957443, 0.8440204151740751, 0.7981195572547755, 0.8598124593300991, 0.8324337981573954, 0.8546195368656158, 0.849854255620719, 0.3666013280562487, 0.3725233049177048, 0.3395845401363392, 0.39396992596302094, 0.37524282476543214, 0.35070565435275614, 0.33407804648082917, 0.38226003237655404, 0.349245265497106, 0.7018061463567615, 0.7233415701332743, 0.7088757356079152, 0.7982178557801977, 0.7714881430535534, 0.7264322401069573, 0.795367319615778, 0.7599066331977806, 0.7086678887052003, 0.201764961314991, 0.17282079779941106, 0.2263590169873433, 0.2721596471019315, 0.27597767251362215, 0.17937872167142266, 0.16761630117773363, 0.24905357666002514, 0.16922513461462385, 0.16335003951102212, 0.20535196113100707, 0.13361225495437334, 0.1579703207595461, 0.16150244697886662, 0.2167787719170582, 0.18782402154756306, 0.19564428973151649, 0.18163669853366704, 0.31867100857758623, 0.3425741905828079, 0.3304444894524823, 0.34738093096977485, 0.2877568817466949, 0.3234807865621129, 0.33894048928278153, 0.37013426300073216, 0.3588389116713273, 0.5180632809666004, 0.4637458915590531, 0.5043407285905045, 0.33215873995771494, 0.4542336970383146, 0.4548165201136448, 0.5156739638315224, 0.5239241206078717, 0.4755558899035023, 0.11585943854457781, 0.1638375146855462, 0.09872669469579576, 0.21730694094700664, 0.16634037144172908, 0.1654849161870532, 0.11664820618779581, 0.07087408397891926, 0.10845489196991798, 0.2843152982061281, 0.28613726624777835, 0.28229693759480057, 0.3072011097040027, 0.2853119052426477, 0.2916314440888641, 0.2999473256283228, 0.2859594986946987, 0.26193620963585085, 0.6913140588308797, 0.6786867206260565, 0.701588295399665, 0.6541883452840307, 0.6654199785954649, 0.6475450237226962, 0.6671814869069398, 0.7055073070884146, 0.6638784336673579, 0.11391843993435358, 0.10975448936033483, 0.10090549793679549, 0.10310324236480084, 0.10791804393203874, 0.10939104607676131, 0.09581307616921564, 0.09812125343721878, 0.09985837034065093, 0.18846301470631488, 0.22839262856185694, 0.22322725974210345, 0.19757465926045115, 0.1701987471183407, 0.18919254140941477, 0.20027451142437336, 0.20007046061634493, 0.20073311465283272, 0.3501650996213105, 0.35164157461146006, 0.34813603073325927, 0.34536700185434244, 0.3655415017581666, 0.3583096257802938, 0.35581920313139404, 0.40530574007584763, 0.36866959956081435, 0.26557307404745645, 0.2697168709798672, 0.24942936856729292, 0.259704041706049, 0.2626717448788001, 0.2794983152026371, 0.24933057131184155, 0.29414825313596626, 0.24778132121106566, 0.1964865341256079, 0.20130664628203954, 0.2003458670310626, 0.23375686724462996, 0.21330797625742404, 0.2022932961455597, 0.2034346087052653, 0.20158084356029105, 0.23035297301018787, 0.19254916882434714, 0.18814946260488974, 0.19842901699570903, 0.184403561312654, 0.186854624925067, 0.18528907418469243, 0.20214940877697307, 0.18935042119784207, 0.18539220063005257, 0.32818920794091344, 0.5959495761773042, 0.3200074905287692, 0.6508833616229394, 0.39195391293278525, 0.506566629143708, 0.4000391686588708, 0.3511867539905471, 0.39761398923874924, 0.45995799055147635, 0.4459678532604766, 0.33988555794646136, 0.5390455573695724, 0.23998167446344754, 0.31350500582590435, 0.523108579970601, 0.20391488842316685, 0.24657744080176514, 0.17212424028899986, 0.19143516576879105, 0.17627569278737343, 0.19918041607754244, 0.2059953338045003, 0.175787862497925, 0.19782638442055023, 0.2069171764155977, 0.20245082333553344, 0.08338336975119987, 0.08271298565658503, 0.08769925944284895, 0.09209940270208461, 0.08249412618243479, 0.09027444882925406, 0.07892761994153641, 0.08207905467688215, 0.08015318768749546]}, "mutation_prompt": null}
{"id": "a0553c7b-2d1e-4faf-873e-b86a7933c1b3", "solution": "import numpy as np\n\nclass EnhancedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = 40\n        self.mutation_factor = 0.8\n        self.crossover_probability = 0.9\n        self.learning_rate = 0.1\n        self.population = None\n        self.fitness = None\n\n    def initialize_population(self):\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, \n                                            (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def adaptive_differential_evolution(self, idx, func):\n        if np.random.random() < 0.4:  # Probability to change the mutation strategy\n            indices = np.random.choice([i for i in range(self.population_size) if i != idx], 3, replace=False)\n        else:\n            indices = [i for i in range(self.population_size) if i != idx]\n            np.random.shuffle(indices)\n            a, b, c = indices[:3]\n\n        mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        adaptive_factor = 0.5 + 0.5 * np.random.rand()\n        mutant = self.population[idx] + adaptive_factor * (mutant - self.population[idx])\n        mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n        \n        cross_points = np.random.rand(self.dim) < self.crossover_probability\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        \n        trial = np.where(cross_points, mutant, self.population[idx])\n        trial_fitness = func(trial)\n        \n        if trial_fitness < self.fitness[idx]:\n            self.population[idx] = trial\n            self.fitness[idx] = trial_fitness\n\n    def randomized_newton_update(self, idx, func):\n        gradient = np.random.uniform(-1, 1, self.dim)\n        adaptive_lr = self.learning_rate / (1 + np.random.uniform(0, 1))  # Change learning rate adaptively\n        hessian_approx = np.random.uniform(0.1, 0.5, self.dim)\n        candidate = self.population[idx] - adaptive_lr * gradient / (np.abs(hessian_approx) + 1e-9)\n        candidate = np.clip(candidate, self.lower_bound, self.upper_bound)\n        candidate_fitness = func(candidate)\n        if candidate_fitness < self.fitness[idx]:\n            self.population[idx] = candidate\n            self.fitness[idx] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population()\n        evaluations = 0\n        self.evaluate_population(func)\n        evaluations += self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                self.adaptive_differential_evolution(i, func)\n                evaluations += 1\n\n                if evaluations >= self.budget:\n                    break\n\n                self.randomized_newton_update(i, func)\n                evaluations += 1\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "EnhancedHybridOptimizer", "description": "A refined hybrid optimization algorithm that integrates adaptive differential evolution with randomized Newton-like updates, enhanced by probabilistic selection of individuals for mutation and crossover.", "configspace": "", "generation": 34, "fitness": -Infinity, "feedback": "An exception occurred: UnboundLocalError(\"local variable 'a' referenced before assignment\").", "error": "UnboundLocalError(\"local variable 'a' referenced before assignment\")", "parent_id": "390fb10b-b1a7-4fda-bfce-d7b6d4431e0f", "metadata": {}, "mutation_prompt": null}
{"id": "49ba661b-18c9-4385-99f5-3ef39724dc69", "solution": "import numpy as np\n\nclass CooperativeHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = 40\n        self.mutation_factor = 0.85\n        self.crossover_probability = 0.8\n        self.learning_rate = 0.15\n        self.communication_factor = 0.5\n        self.population = None\n        self.fitness = None\n\n    def initialize_population(self):\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, \n                                            (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def adaptive_differential_evolution(self, idx, func):\n        indices = [i for i in range(self.population_size) if i != idx]\n        np.random.shuffle(indices)\n        a, b, c = indices[:3]\n        mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n        \n        cross_points = np.random.rand(self.dim) < self.crossover_probability\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        \n        trial = np.where(cross_points, mutant, self.population[idx])\n        trial_fitness = func(trial)\n        \n        if trial_fitness < self.fitness[idx]:\n            self.population[idx] = trial\n            self.fitness[idx] = trial_fitness\n\n    def cooperative_random_walk(self, idx, func):\n        neighbors = [i for i in range(self.population_size) if i != idx]\n        np.random.shuffle(neighbors)\n        \n        leader = self.population[np.argmin(self.fitness)]\n        influence = np.random.binomial(1, self.communication_factor, self.dim) * (leader - self.population[idx])\n        adaptive_lr = self.learning_rate / (1 + idx / self.population_size)\n        \n        candidate = self.population[idx] + adaptive_lr * influence\n        candidate = np.clip(candidate, self.lower_bound, self.upper_bound)\n        candidate_fitness = func(candidate)\n        \n        if candidate_fitness < self.fitness[idx]:\n            self.population[idx] = candidate\n            self.fitness[idx] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population()\n        evaluations = 0\n        self.evaluate_population(func)\n        evaluations += self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                self.adaptive_differential_evolution(i, func)\n                evaluations += 1\n\n                if evaluations >= self.budget:\n                    break\n\n                self.cooperative_random_walk(i, func)\n                evaluations += 1\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "CooperativeHybridOptimizer", "description": "A stochastic and cooperative hybrid optimizer that leverages adaptive differential evolution and adaptive random walks guided by a swarm-inspired communication strategy.", "configspace": "", "generation": 35, "fitness": 0.2717350090042816, "feedback": "The algorithm CooperativeHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.27 with standard deviation 0.21.", "error": "", "parent_id": "390fb10b-b1a7-4fda-bfce-d7b6d4431e0f", "metadata": {"aucs": [0.6619903277534964, 0.6730747429569093, 0.6681400268195159, 0.6680364750441349, 0.6844129509164624, 0.7050230599923399, 0.662958986672386, 0.6888921488249118, 0.7094375531881559, 0.40798263501786847, 0.39683706906921357, 0.4014086764501549, 0.42160578222615475, 0.44438748615285717, 0.43977976683997977, 0.40994242411419346, 0.40543255514358156, 0.4265471433005714, 0.11249016139379586, 0.10692734074161858, 0.10059879105095493, 0.11403737580688744, 0.11824252103243138, 0.10943684442939139, 0.13288015630632266, 0.11803569864348151, 0.10411362931766, 0.09985265203584182, 0.1100339928860693, 0.10175792124098293, 0.10940328180319359, 0.12400942708429952, 0.10652664164287473, 0.10450057106793498, 0.10014520129337545, 0.10034377948100659, 0.8582424209028736, 0.8512990605769253, 0.910103896905066, 0.9426195601765046, 0.900798804395701, 0.9696638792265475, 0.874393163586749, 0.8964942480576686, 0.9603722671274012, 0.33150826227757946, 0.3169977597793845, 0.30758447990328397, 0.35155438886578394, 0.3031845316945856, 0.336629767908431, 0.3215696720521517, 0.3249264971457315, 0.3482017203413996, 0.2742507537228134, 0.28019026146934334, 0.3475015640884438, 0.3210323840670196, 0.47954235445501403, 0.339964723164387, 0.486384444548693, 0.4687429144509849, 0.32947965120825207, 0.21341976680485153, 0.17978978282089886, 0.23305790221495526, 0.25607730085700176, 0.18307740530462147, 0.19232550189642938, 0.19569176223770546, 0.16843010013081472, 0.16106255301186434, 0.15672412577699013, 0.13583145350996984, 0.16192124372933336, 0.14952398335731265, 0.18059455681814307, 0.20798920354229955, 0.18105758504652614, 0.17458068617668554, 0.13195487225889668, 0.08838953354648804, 0.06628026250074859, 0.0792689895799189, 0.04210819438059221, 0.06519755100643487, 0.08667792110201988, 0.10803168635682925, 0.13929092342921212, 0.09394599827458583, 0.1856648725824731, 0.15933368529394776, 0.17984833684052115, 0.175747911353344, 0.17009359024951753, 0.17751170018256512, 0.2110585141377792, 0.21420220271262036, 0.2047310152199534, 0.08272069333938326, 0.06649622538776823, 0.06171255653472185, 0.09312699934528412, 0.08598111669858777, 0.06884619531463199, 0.11352871036714407, 0.054457581591918824, 0.11258279846994812, 0.17128988006617096, 0.18101993756807477, 0.19704552791031638, 0.1897072960323417, 0.17575718798504147, 0.19219495875815495, 0.18551816431955936, 0.18509163250498895, 0.20080652903106977, 0.5066660974118745, 0.49501743226511663, 0.5234216418600471, 0.5250704874569763, 0.5117538955266137, 0.5181962713641819, 0.5199374963895749, 0.5409243560741379, 0.5316593454762435, 0.08992639674138136, 0.0961722663229635, 0.0979798037787365, 0.0911945096597625, 0.09895136574005992, 0.09552536129870492, 0.09013400999250498, 0.10124836819447314, 0.09275159858281312, 0.14128964446243975, 0.15782282479549625, 0.16557480549626236, 0.1914496017450893, 0.16930303141313197, 0.14695370247362138, 0.14843139392418836, 0.15614328984820036, 0.1502627165319167, 0.23411660518919264, 0.2386744100956386, 0.23133989669423094, 0.22166182153298697, 0.2416936972956084, 0.22608364219146326, 0.253019555678769, 0.26737527489487867, 0.24732957188970683, 0.16331138541089962, 0.1750507397733514, 0.18874792657494743, 0.20491660809842693, 0.17430783486468915, 0.18318024602560412, 0.18679714762138544, 0.1875484132688391, 0.19194225267179177, 0.18314821142283289, 0.17703679104842995, 0.19089309954651745, 0.20034564780389175, 0.17895899098376977, 0.19276615948929232, 0.1989684028228237, 0.17388845537239506, 0.20840700277966817, 0.1768000490012388, 0.18343511407948365, 0.17936668631579833, 0.18183825553920596, 0.17223975888246545, 0.20075142302281535, 0.1806957843689434, 0.18586586894243096, 0.17557472584822098, 0.42148507523699574, 0.3584724747539162, 0.40245605826226105, 0.6200406771188947, 0.18059575115968296, 0.4934845237522911, 0.3731826112717812, 0.33181396060080737, 0.5139819376187645, 0.5804892045743453, 0.19808442068269183, 0.5846282399939835, 0.5219966581306995, 0.40654329125015454, 0.49233635116733754, 0.556409033395495, 0.6339315406029682, 0.19913793972045757, 0.18708236472761042, 0.19203369534282222, 0.17423174869354174, 0.18097118879701168, 0.1800773855868958, 0.17709572513576366, 0.1903867167287545, 0.168524796726627, 0.19612575658494558, 0.07074077550314994, 0.08090301533758848, 0.07240231706262334, 0.07473607071026167, 0.07378519250824389, 0.07270743795679069, 0.07352414316920619, 0.06894350795810333, 0.06874528992253559]}, "mutation_prompt": null}
{"id": "c341c616-5892-4bd3-baf4-ed5fb807efa3", "solution": "import numpy as np\n\nclass AdaptiveExplorationExploitationOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = 50\n        self.mutation_factor = 0.85\n        self.crossover_probability = 0.8\n        self.learning_rate = 0.05\n        self.population = None\n        self.fitness = None\n\n    def initialize_population(self):\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, \n                                            (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def noise_resilient_differential_evolution(self, idx, func):\n        indices = [i for i in range(self.population_size) if i != idx]\n        np.random.shuffle(indices)\n        a, b, c = indices[:3]\n        mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        adaptive_factor = 0.6 + 0.4 * np.random.rand()\n        mutant = self.population[idx] + adaptive_factor * (mutant - self.population[idx])\n        mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n        \n        cross_points = np.random.rand(self.dim) < self.crossover_probability\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        \n        trial = np.where(cross_points, mutant, self.population[idx])\n        trial_fitness = func(trial)\n        \n        if trial_fitness < self.fitness[idx]:\n            self.population[idx] = trial\n            self.fitness[idx] = trial_fitness\n\n    def self_adaptive_gradient_descent(self, idx, func):\n        gradient = np.random.uniform(-1, 1, self.dim)\n        adaptive_lr = self.learning_rate / (1 + idx / self.population_size)\n        candidate = self.population[idx] - adaptive_lr * gradient\n        candidate = np.clip(candidate, self.lower_bound, self.upper_bound)\n        candidate_fitness = func(candidate)\n        if candidate_fitness < self.fitness[idx]:\n            self.population[idx] = candidate\n            self.fitness[idx] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population()\n        evaluations = 0\n        self.evaluate_population(func)\n        evaluations += self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                self.noise_resilient_differential_evolution(i, func)\n                evaluations += 1\n\n                if evaluations >= self.budget:\n                    break\n\n                self.self_adaptive_gradient_descent(i, func)\n                evaluations += 1\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "AdaptiveExplorationExploitationOptimizer", "description": "A novel adaptive exploration-exploitation optimizer utilizing a dynamic balance between stochastic rank-based selection, noise-resilient differential evolution, and self-adaptive gradient descent.", "configspace": "", "generation": 36, "fitness": 0.24800092162644238, "feedback": "The algorithm AdaptiveExplorationExploitationOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.25 with standard deviation 0.18.", "error": "", "parent_id": "390fb10b-b1a7-4fda-bfce-d7b6d4431e0f", "metadata": {"aucs": [0.6572101095905842, 0.6225375022758197, 0.6601820679538941, 0.6678519794664288, 0.6532606964577438, 0.6256594358121917, 0.6448135917522295, 0.651248870877972, 0.6298090390582013, 0.35273456686503624, 0.35319526202672635, 0.3423824531278846, 0.3494454909571314, 0.35384776925416184, 0.36465762449076133, 0.3684942786516108, 0.300088499312107, 0.38310100250124113, 0.11488916638601077, 0.13344573739905918, 0.13634129033917342, 0.12883522611750653, 0.13087257121995732, 0.13371546259652756, 0.14488188116051914, 0.1313373570642309, 0.14702755285278524, 0.10543577639249979, 0.11168391692031232, 0.10082823009253972, 0.1309556192851814, 0.10680515409766544, 0.09777305877117959, 0.0923166104120895, 0.11112190222147567, 0.09864214632611323, 0.8478805108764147, 0.7378751432466877, 0.8023206770834661, 0.7882562233194499, 0.8159102074923849, 0.7671339750552038, 0.7214562906607602, 0.7422058684406097, 0.9250424584491856, 0.23778118689746153, 0.25514330124566276, 0.22318162794273755, 0.24979793014087504, 0.24373482613878839, 0.23984419870779283, 0.24502234763326913, 0.25779819752766475, 0.2612662577143118, 0.43538746099019476, 0.39513877889213234, 0.4397060096996668, 0.5181602320699283, 0.4988441066964364, 0.5749838339386119, 0.550413154081934, 0.48463376315668993, 0.5132101627358493, 0.18400335032372428, 0.18794805169861284, 0.15505933716690434, 0.17681350103125149, 0.177439516157318, 0.19152984834361064, 0.15774691893703785, 0.1457232208807442, 0.1801158625244168, 0.12540157554187914, 0.15726746059905905, 0.14871342836063794, 0.1790928567612351, 0.14862748955871952, 0.14898768557206277, 0.12957746217822896, 0.14324113312328846, 0.1380192434891382, 0.08079880476398027, 0.11005279610163565, 0.08774927973660962, 0.10047448496766442, 0.1107305319201658, 0.079275858400228, 0.07626052554975926, 0.07549740941415584, 0.10756423102174306, 0.21724847121133795, 0.25120986200485573, 0.21096476878234305, 0.21170580430779318, 0.19677389065663708, 0.21238830764746464, 0.24380928943881564, 0.2516709149578421, 0.2246559891977894, 0.03311151113841748, 0.02433093358735028, 0.023542157836717537, 0.028732536374604156, 0.021515321729791093, 0.040223564047862426, 0.026054207219316727, 0.01959920189654807, 0.03632047439777819, 0.12608097454108724, 0.15360715746123021, 0.12984921956931572, 0.14848624071738126, 0.14859787010514303, 0.15608795603730063, 0.1258004428304661, 0.1379286521515387, 0.1471668712518207, 0.4659758479135899, 0.46182764752975336, 0.4701370230576265, 0.4560954297573606, 0.4775710181572965, 0.463082781970071, 0.5005620083953982, 0.4625821008269424, 0.47282651145667043, 0.10827542334357987, 0.12430168381208062, 0.1212604643200973, 0.11495203709296908, 0.11973395649218743, 0.10649795878403823, 0.11279808900999999, 0.11675405012362716, 0.11609525128136688, 0.1673498468223008, 0.17102709950450212, 0.1764343283312002, 0.178410518022191, 0.2051512251407963, 0.17390938751711615, 0.18615944713743815, 0.18860262068409006, 0.1673542531675004, 0.2720390697917302, 0.25481020917106345, 0.24865049996592636, 0.2570665098832736, 0.23954551800847934, 0.24659215740048868, 0.26942393966889744, 0.2949688252703889, 0.252322399096674, 0.2004524989122759, 0.20878898208099528, 0.19054718780010182, 0.1913525404228028, 0.21164675728597393, 0.17091574230599804, 0.17822779849640047, 0.2148074578692012, 0.20681919911092284, 0.21969083589055016, 0.21151935044531922, 0.18589952744388494, 0.19758262286312234, 0.20719520000827052, 0.19599107367014235, 0.19498817695633652, 0.21179943190884298, 0.20832320243933533, 0.18456410371567622, 0.19318779413474174, 0.1907746587726249, 0.18973368518431433, 0.18988164722061196, 0.20441413488639149, 0.195551725758219, 0.22865520177068854, 0.18776131951178032, 0.16224212851637532, 0.15528304449103447, 0.18795774718921077, 0.34623263896315926, 0.2035505064175709, 0.2809882274586023, 0.19671511884239135, 0.28724383040291723, 0.1836297865676444, 0.32543896534076744, 0.24908436844178783, 0.2598890311680284, 0.18579769319919448, 0.17829822497847947, 0.2259159043368495, 0.2378344666268699, 0.35123562387699936, 0.24657364544283178, 0.199877081372841, 0.19459015811431413, 0.19315454540558674, 0.19838089688937277, 0.2037703219145902, 0.21160629281369359, 0.1968889528325748, 0.19954176266283863, 0.19818448270341715, 0.09349365814704891, 0.08600178559140914, 0.08262961984727313, 0.08245083552437504, 0.08695412187523122, 0.08097748133864513, 0.08035042563122519, 0.07420825837351053, 0.08827635709557557]}, "mutation_prompt": null}
{"id": "1459935d-0a5b-47de-8bfe-69f52cb3cb6c", "solution": "import numpy as np\n\nclass EnhancedProbabilisticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = 40\n        self.mutation_factor = 0.85\n        self.crossover_probability = 0.95\n        self.learning_rate = 0.2\n        self.diversity_factor = 0.1\n        self.population = None\n        self.fitness = None\n\n    def initialize_population(self):\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, \n                                            (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def adaptive_differential_evolution(self, idx, func):\n        indices = [i for i in range(self.population_size) if i != idx]\n        np.random.shuffle(indices)\n        a, b, c = indices[:3]\n        mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        adaptive_factor = 0.4 + 0.6 * np.random.rand()\n        mutant = self.population[idx] + adaptive_factor * (mutant - self.population[idx])\n        mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n        \n        cross_points = np.random.rand(self.dim) < self.crossover_probability\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        \n        trial = np.where(cross_points, mutant, self.population[idx])\n        trial_fitness = func(trial)\n        \n        if trial_fitness < self.fitness[idx]:\n            self.population[idx] = trial\n            self.fitness[idx] = trial_fitness\n\n    def stochastic_gradient_approx(self, idx, func):\n        gradient = np.random.normal(0, 1, self.dim)\n        adaptive_lr = self.learning_rate / (1 + idx / self.population_size)\n        candidate = self.population[idx] - adaptive_lr * gradient\n        candidate += self.diversity_factor * np.random.normal(0, 1, self.dim)\n        candidate = np.clip(candidate, self.lower_bound, self.upper_bound)\n        candidate_fitness = func(candidate)\n        if candidate_fitness < self.fitness[idx]:\n            self.population[idx] = candidate\n            self.fitness[idx] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population()\n        evaluations = 0\n        self.evaluate_population(func)\n        evaluations += self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                self.adaptive_differential_evolution(i, func)\n                evaluations += 1\n\n                if evaluations >= self.budget:\n                    break\n\n                self.stochastic_gradient_approx(i, func)\n                evaluations += 1\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "EnhancedProbabilisticOptimizer", "description": "Enhanced probabilistic hybrid optimizer balancing exploration and exploitation through diversity-oriented selection and adaptive parameter tuning for optimal convergence.", "configspace": "", "generation": 37, "fitness": 0.36516055413659027, "feedback": "The algorithm EnhancedProbabilisticOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.37 with standard deviation 0.24.", "error": "", "parent_id": "390fb10b-b1a7-4fda-bfce-d7b6d4431e0f", "metadata": {"aucs": [0.796727445346743, 0.7946092935536371, 0.8025157468331956, 0.7795890428061506, 0.7652866403837684, 0.7888980264133363, 0.7851539828124771, 0.7801166288092796, 0.778684214476141, 0.6066596860215113, 0.5509488918434079, 0.584709500115218, 0.6151880393982149, 0.6128285196462557, 0.5928554548480235, 0.18781852246903208, 0.6051102165828588, 0.6050716765741883, 0.12056849613284826, 0.10627786970516262, 0.11092740344636509, 0.12458142503345515, 0.10225070914012868, 0.10214739878052792, 0.11007052847089493, 0.11303443664409152, 0.11346829363668931, 0.09058514750802149, 0.09136224141572435, 0.09346304265481342, 0.08413688806219866, 0.10560994817344638, 0.09386944951658027, 0.1066829182603829, 0.09525027475772718, 0.09078080677184064, 0.8379701410373176, 0.8069154658397715, 0.8108437032002754, 0.8202640448333322, 0.7815128713795206, 0.8451084830274826, 0.836376388134602, 0.8441441978939892, 0.8509693104465095, 0.37150586919602946, 0.34273942956965175, 0.3313822553845982, 0.34968755932637086, 0.32960730205573996, 0.3754984208710611, 0.38284578850911233, 0.3840456442426208, 0.3280564413760412, 0.7576709220936438, 0.7335556841917643, 0.7658090217912067, 0.7934167558179147, 0.8025617081483369, 0.7999764269594076, 0.7299292035530152, 0.7697788108067551, 0.7651252822797626, 0.16167439560696928, 0.18696031846775318, 0.21158580997319565, 0.244349141547606, 0.2890559956537958, 0.16298570563996873, 0.21750653437837209, 0.21122767559906996, 0.22550289679700497, 0.15305768362463734, 0.14884944831507896, 0.18600965489753707, 0.1541357033150358, 0.19832744782165168, 0.18983314847500432, 0.1953210887165725, 0.20165646735698872, 0.13088573983753993, 0.489476775395826, 0.460206761694571, 0.280690722270316, 0.34064732845138956, 0.522953329166158, 0.46452239779932414, 0.4196789194960461, 0.4727221489210359, 0.44131563048738687, 0.6540713674624187, 0.6128130384068273, 0.5553131009305013, 0.5776200055109268, 0.4723536786910414, 0.6219680956281209, 0.6455804612075908, 0.6109257481083044, 0.5996992464126676, 0.15220127888973134, 0.11018658007737869, 0.2413715712685326, 0.2296484097510183, 0.2572737636354798, 0.10622686325653308, 0.14604068872930986, 0.14809138800719435, 0.12257392403735012, 0.36655256082417187, 0.34632185847096675, 0.3516395670957846, 0.33828319802853724, 0.3572041238804262, 0.35621925345698524, 0.34628041224843775, 0.36205674625584605, 0.3801826028652373, 0.7284525303733058, 0.7334889541026953, 0.7386557191632159, 0.7055754468414382, 0.7222856768071027, 0.7194774022217179, 0.518854950686749, 0.7321096452535195, 0.7397060954282213, 0.10374399773694243, 0.104282622260642, 0.1086381702881759, 0.11429613558107654, 0.11511930056195652, 0.10035386421161252, 0.09599830676572241, 0.11347281801819442, 0.11470225552281121, 0.14952999643648723, 0.16817216547016145, 0.17310734903563885, 0.1855986179207637, 0.18359480037739373, 0.19508916572226054, 0.20087729718879477, 0.17807185486405164, 0.20119801391244463, 0.3682121014468256, 0.35073539948175425, 0.3330439038672147, 0.4157077631363848, 0.37474021495439724, 0.36760475650845414, 0.38008256501576043, 0.39084781839078786, 0.3786757352451906, 0.30037946434366947, 0.2861866860206934, 0.2891587957817804, 0.2835627691001067, 0.2779602895144492, 0.28387893685888044, 0.29400381025955125, 0.3244615423116647, 0.30188351717337136, 0.20494558183242018, 0.19905376550038034, 0.20406514428030742, 0.2229753496453719, 0.1980660060283188, 0.21539123082194234, 0.21928087314384304, 0.20854328449838244, 0.2102975078755156, 0.18082128476991477, 0.1831737258940077, 0.18089956975930732, 0.18277323416104252, 0.1792695777259361, 0.17816092407226292, 0.19628636488975992, 0.19237612183491615, 0.1874718333986961, 0.6294666203187917, 0.602690555999032, 0.5454952462675013, 0.7727337342883757, 0.25677125313719296, 0.588947972014151, 0.5053651729784945, 0.42433758940449484, 0.610090527727432, 0.7148365734773846, 0.39316249107203016, 0.4609914040189693, 0.5414475146943718, 0.3738614312609654, 0.2972834077003401, 0.2719039283446757, 0.4464259994844204, 0.6865015215956998, 0.18541089636272012, 0.17521600614441235, 0.18498258988690697, 0.1923345960637889, 0.19442272937612437, 0.18424555156947642, 0.17449564646480575, 0.18372628686318526, 0.18823517758491137, 0.07953923390817585, 0.08429672806217459, 0.08351517483047155, 0.07872531348008505, 0.0885755282031725, 0.09071254814221597, 0.07022751225803625, 0.07907174920771143, 0.08022853716110878]}, "mutation_prompt": null}
{"id": "d5fdbe3e-8fe2-45b3-837d-3b54416f9573", "solution": "import numpy as np\n\nclass AugmentedProbabilisticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = 50  # Increased population size for better exploration\n        self.mutation_factor = 0.9  # Adjusted mutation factor for diversity\n        self.crossover_probability = 0.92  # Slightly reduced for robustness\n        self.learning_rate = 0.15  # Adjusted learning rate for fine-tuning\n        self.diversity_factor = 0.08  # Reduced to focus more on exploitation\n        self.population = None\n        self.fitness = None\n\n    def initialize_population(self):\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, \n                                            (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def adaptive_differential_evolution(self, idx, func):\n        indices = np.random.permutation([i for i in range(self.population_size) if i != idx])\n        a, b, c = indices[:3]\n        adaptive_factor = 0.5 + 0.5 * np.random.rand()  # Adjusted adaptive factor range\n        mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        mutant = self.population[idx] + adaptive_factor * (mutant - self.population[idx])\n        mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n        \n        cross_points = np.random.rand(self.dim) < self.crossover_probability\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        \n        trial = np.where(cross_points, mutant, self.population[idx])\n        trial_fitness = func(trial)\n        \n        if trial_fitness < self.fitness[idx]:\n            self.population[idx] = trial\n            self.fitness[idx] = trial_fitness\n\n    def stochastic_gradient_approx(self, idx, func):\n        gradient = np.random.normal(0, 1, self.dim)\n        adaptive_lr = self.learning_rate / (1 + 0.5 * idx / self.population_size)  # More aggressive adaption\n        candidate = self.population[idx] - adaptive_lr * gradient\n        candidate += self.diversity_factor * np.random.normal(0, 1, self.dim)\n        candidate = np.clip(candidate, self.lower_bound, self.upper_bound)\n        candidate_fitness = func(candidate)\n        if candidate_fitness < self.fitness[idx]:\n            self.population[idx] = candidate\n            self.fitness[idx] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population()\n        evaluations = 0\n        self.evaluate_population(func)\n        evaluations += self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                self.adaptive_differential_evolution(i, func)\n                evaluations += 1\n\n                if evaluations >= self.budget:\n                    break\n\n                self.stochastic_gradient_approx(i, func)\n                evaluations += 1\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "AugmentedProbabilisticOptimizer", "description": "Hybrid probabilistic optimizer augmented with self-adaptive mechanisms for enhanced global search and exploitation.", "configspace": "", "generation": 38, "fitness": 0.29548059688409845, "feedback": "The algorithm AugmentedProbabilisticOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.30 with standard deviation 0.21.", "error": "", "parent_id": "1459935d-0a5b-47de-8bfe-69f52cb3cb6c", "metadata": {"aucs": [0.6740820558308673, 0.695149701445037, 0.6940464282407145, 0.6831511468660809, 0.6991912029779264, 0.6703368402220908, 0.6866370458613814, 0.6557847865307675, 0.6792622864605471, 0.3699666814891561, 0.39260256120356296, 0.3879444426337848, 0.3969037048910711, 0.378708284555897, 0.4262209796461016, 0.40269597386090494, 0.404953620479067, 0.4100715851251786, 0.10999205573118964, 0.11347090473195642, 0.1004682025219017, 0.10812397557928999, 0.09855438401192473, 0.11089208093240432, 0.10727572558754928, 0.1101240002156827, 0.09428929562951194, 0.08345260501766738, 0.09104026267808751, 0.08861639311090208, 0.08691308488061544, 0.09529364379541272, 0.09011157875666209, 0.0874827090050827, 0.08770741224765755, 0.09267666032583532, 0.9368351853223624, 0.9269871204052962, 0.959556599561458, 0.895069238215505, 0.9112473777843993, 0.8208350555436719, 0.8113649982392694, 0.7881589314122174, 0.8087467159148531, 0.300674096030465, 0.2784966066210286, 0.29392804165164743, 0.30136834101292875, 0.2919438172326655, 0.2777913886814748, 0.317458262527925, 0.2854110523259509, 0.3051509435585723, 0.6267036058254467, 0.5852180450088131, 0.5864736120750489, 0.7023429129808831, 0.7566094147349318, 0.6123623718859557, 0.6528640930484562, 0.6155263712007034, 0.6756771105611294, 0.16177948755598937, 0.19306938329403722, 0.17286503252128038, 0.1679015993095443, 0.1468655694236468, 0.21246628096202413, 0.26190036346391643, 0.1900519300881972, 0.18671828762523812, 0.15268394588257306, 0.1753191280749754, 0.18477247186973544, 0.1863403433021158, 0.18504255425752258, 0.17898425847692745, 0.17070609419759508, 0.20593678232335066, 0.1959955016490662, 0.21303497082650225, 0.191084404309266, 0.24401572526498927, 0.23662899975182938, 0.21028446337421103, 0.19626518576952934, 0.21324551613010467, 0.2172113761835871, 0.2138720584292757, 0.3699513685921718, 0.3415645960815621, 0.37396956283010563, 0.35473229864495504, 0.312282436272897, 0.3820246182267186, 0.38446691614828776, 0.3705978880608434, 0.391045709520628, 0.049722794040643725, 0.06978362382600678, 0.07135234433047732, 0.0334108686274589, 0.09055505194405011, 0.1003462458047063, 0.08102453785840391, 0.0943804306775361, 0.09894428272884015, 0.19790667614001944, 0.20063362483844804, 0.22073881079516122, 0.22016299323771393, 0.20923904782714464, 0.22399059310695135, 0.200487390548844, 0.20689568190836627, 0.20926180667554206, 0.5628576817404363, 0.5427481935556037, 0.54790381728626, 0.5326226796029805, 0.5542718157577602, 0.5614904647047518, 0.5733459931928304, 0.5573108012528883, 0.5894063098394718, 0.09553356956565562, 0.09909151197270016, 0.10498699925123733, 0.09328693273835631, 0.12207057566619983, 0.10030441436272608, 0.09854092824179084, 0.10353295857797462, 0.10466663209866944, 0.2120009867229704, 0.18435936032189815, 0.18011315294650754, 0.26500434942155493, 0.20288052580109361, 0.20587192496889284, 0.23336745881721266, 0.16017993668938546, 0.16622004880109476, 0.31467585122574515, 0.28331981062682976, 0.29039842318202946, 0.26986589572282704, 0.3003188227824387, 0.2965351749538113, 0.30063988565689115, 0.3040844712827745, 0.2816557564106249, 0.2104086258642338, 0.22506313060219207, 0.23939600109138515, 0.22938728795228214, 0.2098078133147988, 0.2142056328313663, 0.23722660180493327, 0.21932570005475027, 0.2075978414573667, 0.21810604545919865, 0.20265730098874613, 0.21679893794462135, 0.2021256882093605, 0.21738493940929238, 0.19461559808330808, 0.20290489082907592, 0.20956526291860678, 0.1936861754416903, 0.19355593404781546, 0.18380247145707396, 0.19337840698800857, 0.17753805338263462, 0.18578346562434822, 0.2063445151548965, 0.18669567361501793, 0.1965608351670589, 0.17925982878198377, 0.31963241693475775, 0.449891718474167, 0.5815481209186646, 0.4306242807275388, 0.35653761523167304, 0.3464332112256798, 0.2397250460728213, 0.3668414658403081, 0.5038656375162365, 0.41792670947625776, 0.35792348330511237, 0.4461718290676645, 0.33560848080610606, 0.31497217474135764, 0.34997187730223744, 0.20326305685486934, 0.2782659996191291, 0.2458152190071462, 0.1843207225379475, 0.1883983192513735, 0.1826717062578378, 0.18714135276945743, 0.19638829685770265, 0.18274038061850872, 0.19808823607988857, 0.17961896310334147, 0.20710519024556884, 0.08022156121197244, 0.08181907608318051, 0.07658137629030537, 0.07462751063516448, 0.08498257959939015, 0.07365328990871711, 0.0804446054295308, 0.08712719259805146, 0.0707039240926306]}, "mutation_prompt": null}
{"id": "7b1c8f0a-ea41-47f7-ae8d-f435f6d17ed5", "solution": "import numpy as np\n\nclass EnhancedProbabilisticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = 40\n        self.mutation_factor = 0.85\n        self.crossover_probability = 0.95\n        self.learning_rate = 0.2\n        self.diversity_factor = 0.1\n        self.population = None\n        self.fitness = None\n\n    def initialize_population(self):\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, \n                                            (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def adaptive_differential_evolution(self, idx, func):\n        indices = [i for i in range(self.population_size) if i != idx]\n        np.random.shuffle(indices)\n        a, b, c = indices[:3]\n        mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        adaptive_factor = 0.4 + 0.6 * np.random.rand()\n        mutant = self.population[idx] + adaptive_factor * (mutant - self.population[idx])\n        mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n        \n        cross_points = np.random.rand(self.dim) < self.crossover_probability\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        \n        trial = np.where(cross_points, mutant, self.population[idx])\n        trial_fitness = func(trial)\n        \n        if trial_fitness < self.fitness[idx]:\n            self.population[idx] = trial\n            self.fitness[idx] = trial_fitness\n\n    def stochastic_gradient_approx(self, idx, func):\n        gradient = np.random.normal(0, 1, self.dim)\n        adaptive_lr = self.learning_rate / (1 + idx / self.population_size)\n        candidate = self.population[idx] - adaptive_lr * gradient\n        candidate += self.diversity_factor * np.random.normal(0, 1, self.dim)\n        candidate = np.clip(candidate, self.lower_bound, self.upper_bound)\n        candidate_fitness = func(candidate)\n        if candidate_fitness < self.fitness[idx]:\n            self.population[idx] = candidate\n            self.fitness[idx] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population()\n        evaluations = 0\n        self.evaluate_population(func)\n        evaluations += self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                self.adaptive_differential_evolution(i, func)\n                evaluations += 1\n\n                if evaluations >= self.budget:\n                    break\n\n                self.stochastic_gradient_approx(i, func)\n                evaluations += 1\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "EnhancedProbabilisticOptimizer", "description": "Enhanced probabilistic hybrid optimizer balancing exploration and exploitation through diversity-oriented selection and adaptive parameter tuning for optimal convergence.", "configspace": "", "generation": 38, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "1459935d-0a5b-47de-8bfe-69f52cb3cb6c", "metadata": {"aucs": [0.796727445346743, 0.7946092935536371, 0.8025157468331956, 0.7795890428061506, 0.7652866403837684, 0.7888980264133363, 0.7851539828124771, 0.7801166288092796, 0.778684214476141, 0.6066596860215113, 0.5509488918434079, 0.584709500115218, 0.6151880393982149, 0.6128285196462557, 0.5928554548480235, 0.18781852246903208, 0.6051102165828588, 0.6050716765741883, 0.12056849613284826, 0.10627786970516262, 0.11092740344636509, 0.12458142503345515, 0.10225070914012868, 0.10214739878052792, 0.11007052847089493, 0.11303443664409152, 0.11346829363668931, 0.09058514750802149, 0.09136224141572435, 0.09346304265481342, 0.08413688806219866, 0.10560994817344638, 0.09386944951658027, 0.1066829182603829, 0.09525027475772718, 0.09078080677184064, 0.8379701410373176, 0.8069154658397715, 0.8108437032002754, 0.8202640448333322, 0.7815128713795206, 0.8451084830274826, 0.836376388134602, 0.8441441978939892, 0.8509693104465095, 0.37150586919602946, 0.34273942956965175, 0.3313822553845982, 0.34968755932637086, 0.32960730205573996, 0.3754984208710611, 0.38284578850911233, 0.3840456442426208, 0.3280564413760412, 0.7576709220936438, 0.7335556841917643, 0.7658090217912067, 0.7934167558179147, 0.8025617081483369, 0.7999764269594076, 0.7299292035530152, 0.7697788108067551, 0.7651252822797626, 0.16167439560696928, 0.18696031846775318, 0.21158580997319565, 0.244349141547606, 0.2890559956537958, 0.16298570563996873, 0.21750653437837209, 0.21122767559906996, 0.22550289679700497, 0.15305768362463734, 0.14884944831507896, 0.18600965489753707, 0.1541357033150358, 0.19832744782165168, 0.18983314847500432, 0.1953210887165725, 0.20165646735698872, 0.13088573983753993, 0.489476775395826, 0.460206761694571, 0.280690722270316, 0.34064732845138956, 0.522953329166158, 0.46452239779932414, 0.4196789194960461, 0.4727221489210359, 0.44131563048738687, 0.6540713674624187, 0.6128130384068273, 0.5553131009305013, 0.5776200055109268, 0.4723536786910414, 0.6219680956281209, 0.6455804612075908, 0.6109257481083044, 0.5996992464126676, 0.15220127888973134, 0.11018658007737869, 0.2413715712685326, 0.2296484097510183, 0.2572737636354798, 0.10622686325653308, 0.14604068872930986, 0.14809138800719435, 0.12257392403735012, 0.36655256082417187, 0.34632185847096675, 0.3516395670957846, 0.33828319802853724, 0.3572041238804262, 0.35621925345698524, 0.34628041224843775, 0.36205674625584605, 0.3801826028652373, 0.7284525303733058, 0.7334889541026953, 0.7386557191632159, 0.7055754468414382, 0.7222856768071027, 0.7194774022217179, 0.518854950686749, 0.7321096452535195, 0.7397060954282213, 0.10374399773694243, 0.104282622260642, 0.1086381702881759, 0.11429613558107654, 0.11511930056195652, 0.10035386421161252, 0.09599830676572241, 0.11347281801819442, 0.11470225552281121, 0.14952999643648723, 0.16817216547016145, 0.17310734903563885, 0.1855986179207637, 0.18359480037739373, 0.19508916572226054, 0.20087729718879477, 0.17807185486405164, 0.20119801391244463, 0.3682121014468256, 0.35073539948175425, 0.3330439038672147, 0.4157077631363848, 0.37474021495439724, 0.36760475650845414, 0.38008256501576043, 0.39084781839078786, 0.3786757352451906, 0.30037946434366947, 0.2861866860206934, 0.2891587957817804, 0.2835627691001067, 0.2779602895144492, 0.28387893685888044, 0.29400381025955125, 0.3244615423116647, 0.30188351717337136, 0.20494558183242018, 0.19905376550038034, 0.20406514428030742, 0.2229753496453719, 0.1980660060283188, 0.21539123082194234, 0.21928087314384304, 0.20854328449838244, 0.2102975078755156, 0.18082128476991477, 0.1831737258940077, 0.18089956975930732, 0.18277323416104252, 0.1792695777259361, 0.17816092407226292, 0.19628636488975992, 0.19237612183491615, 0.1874718333986961, 0.6294666203187917, 0.602690555999032, 0.5454952462675013, 0.7727337342883757, 0.25677125313719296, 0.588947972014151, 0.5053651729784945, 0.42433758940449484, 0.610090527727432, 0.7148365734773846, 0.39316249107203016, 0.4609914040189693, 0.5414475146943718, 0.3738614312609654, 0.2972834077003401, 0.2719039283446757, 0.4464259994844204, 0.6865015215956998, 0.18541089636272012, 0.17521600614441235, 0.18498258988690697, 0.1923345960637889, 0.19442272937612437, 0.18424555156947642, 0.17449564646480575, 0.18372628686318526, 0.18823517758491137, 0.07953923390817585, 0.08429672806217459, 0.08351517483047155, 0.07872531348008505, 0.0885755282031725, 0.09071254814221597, 0.07022751225803625, 0.07907174920771143, 0.08022853716110878]}, "mutation_prompt": null}
{"id": "74445a0c-4d37-4dc8-9967-cc81a284b7f5", "solution": "import numpy as np\n\nclass EnhancedProbabilisticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = 40\n        self.mutation_factor = 0.85\n        self.crossover_probability = 0.95\n        self.learning_rate = 0.2\n        self.diversity_factor = 0.1\n        self.population = None\n        self.fitness = None\n\n    def initialize_population(self):\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, \n                                            (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def adaptive_differential_evolution(self, idx, func):\n        indices = [i for i in range(self.population_size) if i != idx]\n        np.random.shuffle(indices)\n        a, b, c = indices[:3]\n        mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        adaptive_factor = 0.4 + 0.6 * np.random.rand()\n        mutant = self.population[idx] + adaptive_factor * (mutant - self.population[idx])\n        mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n        \n        cross_points = np.random.rand(self.dim) < self.crossover_probability\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        \n        trial = np.where(cross_points, mutant, self.population[idx])\n        trial_fitness = func(trial)\n        \n        if trial_fitness < self.fitness[idx]:\n            self.population[idx] = trial\n            self.fitness[idx] = trial_fitness\n\n    def stochastic_gradient_approx(self, idx, func):\n        gradient = np.random.normal(0, 1, self.dim)\n        adaptive_lr = self.learning_rate / (1 + idx / self.population_size)\n        candidate = self.population[idx] - adaptive_lr * gradient\n        candidate += self.diversity_factor * np.random.normal(0, 1, self.dim)\n        candidate = np.clip(candidate, self.lower_bound, self.upper_bound)\n        candidate_fitness = func(candidate)\n        if candidate_fitness < self.fitness[idx]:\n            self.population[idx] = candidate\n            self.fitness[idx] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population()\n        evaluations = 0\n        self.evaluate_population(func)\n        evaluations += self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                self.adaptive_differential_evolution(i, func)\n                evaluations += 1\n\n                if evaluations >= self.budget:\n                    break\n\n                self.stochastic_gradient_approx(i, func)\n                evaluations += 1\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "EnhancedProbabilisticOptimizer", "description": "Enhanced probabilistic hybrid optimizer balancing exploration and exploitation through diversity-oriented selection and adaptive parameter tuning for optimal convergence.", "configspace": "", "generation": 38, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "1459935d-0a5b-47de-8bfe-69f52cb3cb6c", "metadata": {"aucs": [0.796727445346743, 0.7946092935536371, 0.8025157468331956, 0.7795890428061506, 0.7652866403837684, 0.7888980264133363, 0.7851539828124771, 0.7801166288092796, 0.778684214476141, 0.6066596860215113, 0.5509488918434079, 0.584709500115218, 0.6151880393982149, 0.6128285196462557, 0.5928554548480235, 0.18781852246903208, 0.6051102165828588, 0.6050716765741883, 0.12056849613284826, 0.10627786970516262, 0.11092740344636509, 0.12458142503345515, 0.10225070914012868, 0.10214739878052792, 0.11007052847089493, 0.11303443664409152, 0.11346829363668931, 0.09058514750802149, 0.09136224141572435, 0.09346304265481342, 0.08413688806219866, 0.10560994817344638, 0.09386944951658027, 0.1066829182603829, 0.09525027475772718, 0.09078080677184064, 0.8379701410373176, 0.8069154658397715, 0.8108437032002754, 0.8202640448333322, 0.7815128713795206, 0.8451084830274826, 0.836376388134602, 0.8441441978939892, 0.8509693104465095, 0.37150586919602946, 0.34273942956965175, 0.3313822553845982, 0.34968755932637086, 0.32960730205573996, 0.3754984208710611, 0.38284578850911233, 0.3840456442426208, 0.3280564413760412, 0.7576709220936438, 0.7335556841917643, 0.7658090217912067, 0.7934167558179147, 0.8025617081483369, 0.7999764269594076, 0.7299292035530152, 0.7697788108067551, 0.7651252822797626, 0.16167439560696928, 0.18696031846775318, 0.21158580997319565, 0.244349141547606, 0.2890559956537958, 0.16298570563996873, 0.21750653437837209, 0.21122767559906996, 0.22550289679700497, 0.15305768362463734, 0.14884944831507896, 0.18600965489753707, 0.1541357033150358, 0.19832744782165168, 0.18983314847500432, 0.1953210887165725, 0.20165646735698872, 0.13088573983753993, 0.489476775395826, 0.460206761694571, 0.280690722270316, 0.34064732845138956, 0.522953329166158, 0.46452239779932414, 0.4196789194960461, 0.4727221489210359, 0.44131563048738687, 0.6540713674624187, 0.6128130384068273, 0.5553131009305013, 0.5776200055109268, 0.4723536786910414, 0.6219680956281209, 0.6455804612075908, 0.6109257481083044, 0.5996992464126676, 0.15220127888973134, 0.11018658007737869, 0.2413715712685326, 0.2296484097510183, 0.2572737636354798, 0.10622686325653308, 0.14604068872930986, 0.14809138800719435, 0.12257392403735012, 0.36655256082417187, 0.34632185847096675, 0.3516395670957846, 0.33828319802853724, 0.3572041238804262, 0.35621925345698524, 0.34628041224843775, 0.36205674625584605, 0.3801826028652373, 0.7284525303733058, 0.7334889541026953, 0.7386557191632159, 0.7055754468414382, 0.7222856768071027, 0.7194774022217179, 0.518854950686749, 0.7321096452535195, 0.7397060954282213, 0.10374399773694243, 0.104282622260642, 0.1086381702881759, 0.11429613558107654, 0.11511930056195652, 0.10035386421161252, 0.09599830676572241, 0.11347281801819442, 0.11470225552281121, 0.14952999643648723, 0.16817216547016145, 0.17310734903563885, 0.1855986179207637, 0.18359480037739373, 0.19508916572226054, 0.20087729718879477, 0.17807185486405164, 0.20119801391244463, 0.3682121014468256, 0.35073539948175425, 0.3330439038672147, 0.4157077631363848, 0.37474021495439724, 0.36760475650845414, 0.38008256501576043, 0.39084781839078786, 0.3786757352451906, 0.30037946434366947, 0.2861866860206934, 0.2891587957817804, 0.2835627691001067, 0.2779602895144492, 0.28387893685888044, 0.29400381025955125, 0.3244615423116647, 0.30188351717337136, 0.20494558183242018, 0.19905376550038034, 0.20406514428030742, 0.2229753496453719, 0.1980660060283188, 0.21539123082194234, 0.21928087314384304, 0.20854328449838244, 0.2102975078755156, 0.18082128476991477, 0.1831737258940077, 0.18089956975930732, 0.18277323416104252, 0.1792695777259361, 0.17816092407226292, 0.19628636488975992, 0.19237612183491615, 0.1874718333986961, 0.6294666203187917, 0.602690555999032, 0.5454952462675013, 0.7727337342883757, 0.25677125313719296, 0.588947972014151, 0.5053651729784945, 0.42433758940449484, 0.610090527727432, 0.7148365734773846, 0.39316249107203016, 0.4609914040189693, 0.5414475146943718, 0.3738614312609654, 0.2972834077003401, 0.2719039283446757, 0.4464259994844204, 0.6865015215956998, 0.18541089636272012, 0.17521600614441235, 0.18498258988690697, 0.1923345960637889, 0.19442272937612437, 0.18424555156947642, 0.17449564646480575, 0.18372628686318526, 0.18823517758491137, 0.07953923390817585, 0.08429672806217459, 0.08351517483047155, 0.07872531348008505, 0.0885755282031725, 0.09071254814221597, 0.07022751225803625, 0.07907174920771143, 0.08022853716110878]}, "mutation_prompt": null}
{"id": "edd923fb-e26a-4362-be80-000d4db883d8", "solution": "import numpy as np\n\nclass EnhancedProbabilisticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = 40\n        self.mutation_factor = 0.85\n        self.crossover_probability = 0.95\n        self.learning_rate = 0.2\n        self.diversity_factor = 0.1\n        self.population = None\n        self.fitness = None\n\n    def initialize_population(self):\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, \n                                            (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def adaptive_differential_evolution(self, idx, func):\n        indices = [i for i in range(self.population_size) if i != idx]\n        np.random.shuffle(indices)\n        a, b, c = indices[:3]\n        mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        adaptive_factor = 0.4 + 0.6 * np.random.rand()\n        mutant = self.population[idx] + adaptive_factor * (mutant - self.population[idx])\n        mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n        \n        cross_points = np.random.rand(self.dim) < self.crossover_probability\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        \n        trial = np.where(cross_points, mutant, self.population[idx])\n        trial_fitness = func(trial)\n        \n        if trial_fitness < self.fitness[idx]:\n            self.population[idx] = trial\n            self.fitness[idx] = trial_fitness\n\n    def stochastic_gradient_approx(self, idx, func):\n        gradient = np.random.normal(0, 1, self.dim)\n        adaptive_lr = self.learning_rate / (1 + idx / self.population_size)\n        candidate = self.population[idx] - adaptive_lr * gradient\n        candidate += self.diversity_factor * np.random.normal(0, 1, self.dim)\n        candidate = np.clip(candidate, self.lower_bound, self.upper_bound)\n        candidate_fitness = func(candidate)\n        if candidate_fitness < self.fitness[idx]:\n            self.population[idx] = candidate\n            self.fitness[idx] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population()\n        evaluations = 0\n        self.evaluate_population(func)\n        evaluations += self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                self.adaptive_differential_evolution(i, func)\n                evaluations += 1\n\n                if evaluations >= self.budget:\n                    break\n\n                self.stochastic_gradient_approx(i, func)\n                evaluations += 1\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "EnhancedProbabilisticOptimizer", "description": "Enhanced probabilistic hybrid optimizer balancing exploration and exploitation through diversity-oriented selection and adaptive parameter tuning for optimal convergence.", "configspace": "", "generation": 38, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "1459935d-0a5b-47de-8bfe-69f52cb3cb6c", "metadata": {"aucs": [0.796727445346743, 0.7946092935536371, 0.8025157468331956, 0.7795890428061506, 0.7652866403837684, 0.7888980264133363, 0.7851539828124771, 0.7801166288092796, 0.778684214476141, 0.6066596860215113, 0.5509488918434079, 0.584709500115218, 0.6151880393982149, 0.6128285196462557, 0.5928554548480235, 0.18781852246903208, 0.6051102165828588, 0.6050716765741883, 0.12056849613284826, 0.10627786970516262, 0.11092740344636509, 0.12458142503345515, 0.10225070914012868, 0.10214739878052792, 0.11007052847089493, 0.11303443664409152, 0.11346829363668931, 0.09058514750802149, 0.09136224141572435, 0.09346304265481342, 0.08413688806219866, 0.10560994817344638, 0.09386944951658027, 0.1066829182603829, 0.09525027475772718, 0.09078080677184064, 0.8379701410373176, 0.8069154658397715, 0.8108437032002754, 0.8202640448333322, 0.7815128713795206, 0.8451084830274826, 0.836376388134602, 0.8441441978939892, 0.8509693104465095, 0.37150586919602946, 0.34273942956965175, 0.3313822553845982, 0.34968755932637086, 0.32960730205573996, 0.3754984208710611, 0.38284578850911233, 0.3840456442426208, 0.3280564413760412, 0.7576709220936438, 0.7335556841917643, 0.7658090217912067, 0.7934167558179147, 0.8025617081483369, 0.7999764269594076, 0.7299292035530152, 0.7697788108067551, 0.7651252822797626, 0.16167439560696928, 0.18696031846775318, 0.21158580997319565, 0.244349141547606, 0.2890559956537958, 0.16298570563996873, 0.21750653437837209, 0.21122767559906996, 0.22550289679700497, 0.15305768362463734, 0.14884944831507896, 0.18600965489753707, 0.1541357033150358, 0.19832744782165168, 0.18983314847500432, 0.1953210887165725, 0.20165646735698872, 0.13088573983753993, 0.489476775395826, 0.460206761694571, 0.280690722270316, 0.34064732845138956, 0.522953329166158, 0.46452239779932414, 0.4196789194960461, 0.4727221489210359, 0.44131563048738687, 0.6540713674624187, 0.6128130384068273, 0.5553131009305013, 0.5776200055109268, 0.4723536786910414, 0.6219680956281209, 0.6455804612075908, 0.6109257481083044, 0.5996992464126676, 0.15220127888973134, 0.11018658007737869, 0.2413715712685326, 0.2296484097510183, 0.2572737636354798, 0.10622686325653308, 0.14604068872930986, 0.14809138800719435, 0.12257392403735012, 0.36655256082417187, 0.34632185847096675, 0.3516395670957846, 0.33828319802853724, 0.3572041238804262, 0.35621925345698524, 0.34628041224843775, 0.36205674625584605, 0.3801826028652373, 0.7284525303733058, 0.7334889541026953, 0.7386557191632159, 0.7055754468414382, 0.7222856768071027, 0.7194774022217179, 0.518854950686749, 0.7321096452535195, 0.7397060954282213, 0.10374399773694243, 0.104282622260642, 0.1086381702881759, 0.11429613558107654, 0.11511930056195652, 0.10035386421161252, 0.09599830676572241, 0.11347281801819442, 0.11470225552281121, 0.14952999643648723, 0.16817216547016145, 0.17310734903563885, 0.1855986179207637, 0.18359480037739373, 0.19508916572226054, 0.20087729718879477, 0.17807185486405164, 0.20119801391244463, 0.3682121014468256, 0.35073539948175425, 0.3330439038672147, 0.4157077631363848, 0.37474021495439724, 0.36760475650845414, 0.38008256501576043, 0.39084781839078786, 0.3786757352451906, 0.30037946434366947, 0.2861866860206934, 0.2891587957817804, 0.2835627691001067, 0.2779602895144492, 0.28387893685888044, 0.29400381025955125, 0.3244615423116647, 0.30188351717337136, 0.20494558183242018, 0.19905376550038034, 0.20406514428030742, 0.2229753496453719, 0.1980660060283188, 0.21539123082194234, 0.21928087314384304, 0.20854328449838244, 0.2102975078755156, 0.18082128476991477, 0.1831737258940077, 0.18089956975930732, 0.18277323416104252, 0.1792695777259361, 0.17816092407226292, 0.19628636488975992, 0.19237612183491615, 0.1874718333986961, 0.6294666203187917, 0.602690555999032, 0.5454952462675013, 0.7727337342883757, 0.25677125313719296, 0.588947972014151, 0.5053651729784945, 0.42433758940449484, 0.610090527727432, 0.7148365734773846, 0.39316249107203016, 0.4609914040189693, 0.5414475146943718, 0.3738614312609654, 0.2972834077003401, 0.2719039283446757, 0.4464259994844204, 0.6865015215956998, 0.18541089636272012, 0.17521600614441235, 0.18498258988690697, 0.1923345960637889, 0.19442272937612437, 0.18424555156947642, 0.17449564646480575, 0.18372628686318526, 0.18823517758491137, 0.07953923390817585, 0.08429672806217459, 0.08351517483047155, 0.07872531348008505, 0.0885755282031725, 0.09071254814221597, 0.07022751225803625, 0.07907174920771143, 0.08022853716110878]}, "mutation_prompt": null}
{"id": "5a5d7d6d-2ec2-4f5c-8655-36dab5e240df", "solution": "import numpy as np\n\nclass EnhancedProbabilisticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = 40\n        self.mutation_factor = 0.85\n        self.crossover_probability = 0.95\n        self.learning_rate = 0.2\n        self.diversity_factor = 0.1\n        self.population = None\n        self.fitness = None\n\n    def initialize_population(self):\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, \n                                            (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def adaptive_differential_evolution(self, idx, func):\n        indices = [i for i in range(self.population_size) if i != idx]\n        np.random.shuffle(indices)\n        a, b, c = indices[:3]\n        mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        adaptive_factor = 0.4 + 0.6 * np.random.rand()\n        mutant = self.population[idx] + adaptive_factor * (mutant - self.population[idx])\n        mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n        \n        cross_points = np.random.rand(self.dim) < self.crossover_probability\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        \n        trial = np.where(cross_points, mutant, self.population[idx])\n        trial_fitness = func(trial)\n        \n        if trial_fitness < self.fitness[idx]:\n            self.population[idx] = trial\n            self.fitness[idx] = trial_fitness\n\n    def stochastic_gradient_approx(self, idx, func):\n        gradient = np.random.normal(0, 1, self.dim)\n        adaptive_lr = self.learning_rate / (1 + idx / self.population_size)\n        candidate = self.population[idx] - adaptive_lr * gradient\n        candidate += self.diversity_factor * np.random.normal(0, 1, self.dim)\n        candidate = np.clip(candidate, self.lower_bound, self.upper_bound)\n        candidate_fitness = func(candidate)\n        if candidate_fitness < self.fitness[idx]:\n            self.population[idx] = candidate\n            self.fitness[idx] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population()\n        evaluations = 0\n        self.evaluate_population(func)\n        evaluations += self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                self.adaptive_differential_evolution(i, func)\n                evaluations += 1\n\n                if evaluations >= self.budget:\n                    break\n\n                self.stochastic_gradient_approx(i, func)\n                evaluations += 1\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "EnhancedProbabilisticOptimizer", "description": "Enhanced probabilistic hybrid optimizer balancing exploration and exploitation through diversity-oriented selection and adaptive parameter tuning for optimal convergence.", "configspace": "", "generation": 38, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "1459935d-0a5b-47de-8bfe-69f52cb3cb6c", "metadata": {"aucs": [0.796727445346743, 0.7946092935536371, 0.8025157468331956, 0.7795890428061506, 0.7652866403837684, 0.7888980264133363, 0.7851539828124771, 0.7801166288092796, 0.778684214476141, 0.6066596860215113, 0.5509488918434079, 0.584709500115218, 0.6151880393982149, 0.6128285196462557, 0.5928554548480235, 0.18781852246903208, 0.6051102165828588, 0.6050716765741883, 0.12056849613284826, 0.10627786970516262, 0.11092740344636509, 0.12458142503345515, 0.10225070914012868, 0.10214739878052792, 0.11007052847089493, 0.11303443664409152, 0.11346829363668931, 0.09058514750802149, 0.09136224141572435, 0.09346304265481342, 0.08413688806219866, 0.10560994817344638, 0.09386944951658027, 0.1066829182603829, 0.09525027475772718, 0.09078080677184064, 0.8379701410373176, 0.8069154658397715, 0.8108437032002754, 0.8202640448333322, 0.7815128713795206, 0.8451084830274826, 0.836376388134602, 0.8441441978939892, 0.8509693104465095, 0.37150586919602946, 0.34273942956965175, 0.3313822553845982, 0.34968755932637086, 0.32960730205573996, 0.3754984208710611, 0.38284578850911233, 0.3840456442426208, 0.3280564413760412, 0.7576709220936438, 0.7335556841917643, 0.7658090217912067, 0.7934167558179147, 0.8025617081483369, 0.7999764269594076, 0.7299292035530152, 0.7697788108067551, 0.7651252822797626, 0.16167439560696928, 0.18696031846775318, 0.21158580997319565, 0.244349141547606, 0.2890559956537958, 0.16298570563996873, 0.21750653437837209, 0.21122767559906996, 0.22550289679700497, 0.15305768362463734, 0.14884944831507896, 0.18600965489753707, 0.1541357033150358, 0.19832744782165168, 0.18983314847500432, 0.1953210887165725, 0.20165646735698872, 0.13088573983753993, 0.489476775395826, 0.460206761694571, 0.280690722270316, 0.34064732845138956, 0.522953329166158, 0.46452239779932414, 0.4196789194960461, 0.4727221489210359, 0.44131563048738687, 0.6540713674624187, 0.6128130384068273, 0.5553131009305013, 0.5776200055109268, 0.4723536786910414, 0.6219680956281209, 0.6455804612075908, 0.6109257481083044, 0.5996992464126676, 0.15220127888973134, 0.11018658007737869, 0.2413715712685326, 0.2296484097510183, 0.2572737636354798, 0.10622686325653308, 0.14604068872930986, 0.14809138800719435, 0.12257392403735012, 0.36655256082417187, 0.34632185847096675, 0.3516395670957846, 0.33828319802853724, 0.3572041238804262, 0.35621925345698524, 0.34628041224843775, 0.36205674625584605, 0.3801826028652373, 0.7284525303733058, 0.7334889541026953, 0.7386557191632159, 0.7055754468414382, 0.7222856768071027, 0.7194774022217179, 0.518854950686749, 0.7321096452535195, 0.7397060954282213, 0.10374399773694243, 0.104282622260642, 0.1086381702881759, 0.11429613558107654, 0.11511930056195652, 0.10035386421161252, 0.09599830676572241, 0.11347281801819442, 0.11470225552281121, 0.14952999643648723, 0.16817216547016145, 0.17310734903563885, 0.1855986179207637, 0.18359480037739373, 0.19508916572226054, 0.20087729718879477, 0.17807185486405164, 0.20119801391244463, 0.3682121014468256, 0.35073539948175425, 0.3330439038672147, 0.4157077631363848, 0.37474021495439724, 0.36760475650845414, 0.38008256501576043, 0.39084781839078786, 0.3786757352451906, 0.30037946434366947, 0.2861866860206934, 0.2891587957817804, 0.2835627691001067, 0.2779602895144492, 0.28387893685888044, 0.29400381025955125, 0.3244615423116647, 0.30188351717337136, 0.20494558183242018, 0.19905376550038034, 0.20406514428030742, 0.2229753496453719, 0.1980660060283188, 0.21539123082194234, 0.21928087314384304, 0.20854328449838244, 0.2102975078755156, 0.18082128476991477, 0.1831737258940077, 0.18089956975930732, 0.18277323416104252, 0.1792695777259361, 0.17816092407226292, 0.19628636488975992, 0.19237612183491615, 0.1874718333986961, 0.6294666203187917, 0.602690555999032, 0.5454952462675013, 0.7727337342883757, 0.25677125313719296, 0.588947972014151, 0.5053651729784945, 0.42433758940449484, 0.610090527727432, 0.7148365734773846, 0.39316249107203016, 0.4609914040189693, 0.5414475146943718, 0.3738614312609654, 0.2972834077003401, 0.2719039283446757, 0.4464259994844204, 0.6865015215956998, 0.18541089636272012, 0.17521600614441235, 0.18498258988690697, 0.1923345960637889, 0.19442272937612437, 0.18424555156947642, 0.17449564646480575, 0.18372628686318526, 0.18823517758491137, 0.07953923390817585, 0.08429672806217459, 0.08351517483047155, 0.07872531348008505, 0.0885755282031725, 0.09071254814221597, 0.07022751225803625, 0.07907174920771143, 0.08022853716110878]}, "mutation_prompt": null}
{"id": "feed9b76-eeb7-4d5f-b850-f90120b39cf8", "solution": "import numpy as np\n\nclass EnhancedProbabilisticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = 40\n        self.mutation_factor = 0.85\n        self.crossover_probability = 0.95\n        self.learning_rate = 0.2\n        self.diversity_factor = 0.1\n        self.population = None\n        self.fitness = None\n\n    def initialize_population(self):\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, \n                                            (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def adaptive_differential_evolution(self, idx, func):\n        indices = [i for i in range(self.population_size) if i != idx]\n        np.random.shuffle(indices)\n        a, b, c = indices[:3]\n        mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        adaptive_factor = 0.4 + 0.6 * np.random.rand()\n        mutant = self.population[idx] + adaptive_factor * (mutant - self.population[idx])\n        mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n        \n        cross_points = np.random.rand(self.dim) < self.crossover_probability\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        \n        trial = np.where(cross_points, mutant, self.population[idx])\n        trial_fitness = func(trial)\n        \n        if trial_fitness < self.fitness[idx]:\n            self.population[idx] = trial\n            self.fitness[idx] = trial_fitness\n\n    def stochastic_gradient_approx(self, idx, func):\n        gradient = np.random.normal(0, 1, self.dim)\n        adaptive_lr = self.learning_rate / (1 + idx / self.population_size)\n        candidate = self.population[idx] - adaptive_lr * gradient\n        candidate += self.diversity_factor * np.random.normal(0, 1, self.dim)\n        candidate = np.clip(candidate, self.lower_bound, self.upper_bound)\n        candidate_fitness = func(candidate)\n        if candidate_fitness < self.fitness[idx]:\n            self.population[idx] = candidate\n            self.fitness[idx] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population()\n        evaluations = 0\n        self.evaluate_population(func)\n        evaluations += self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                self.adaptive_differential_evolution(i, func)\n                evaluations += 1\n\n                if evaluations >= self.budget:\n                    break\n\n                self.stochastic_gradient_approx(i, func)\n                evaluations += 1\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "EnhancedProbabilisticOptimizer", "description": "Enhanced probabilistic hybrid optimizer balancing exploration and exploitation through diversity-oriented selection and adaptive parameter tuning for optimal convergence.", "configspace": "", "generation": 38, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "1459935d-0a5b-47de-8bfe-69f52cb3cb6c", "metadata": {"aucs": [0.796727445346743, 0.7946092935536371, 0.8025157468331956, 0.7795890428061506, 0.7652866403837684, 0.7888980264133363, 0.7851539828124771, 0.7801166288092796, 0.778684214476141, 0.6066596860215113, 0.5509488918434079, 0.584709500115218, 0.6151880393982149, 0.6128285196462557, 0.5928554548480235, 0.18781852246903208, 0.6051102165828588, 0.6050716765741883, 0.12056849613284826, 0.10627786970516262, 0.11092740344636509, 0.12458142503345515, 0.10225070914012868, 0.10214739878052792, 0.11007052847089493, 0.11303443664409152, 0.11346829363668931, 0.09058514750802149, 0.09136224141572435, 0.09346304265481342, 0.08413688806219866, 0.10560994817344638, 0.09386944951658027, 0.1066829182603829, 0.09525027475772718, 0.09078080677184064, 0.8379701410373176, 0.8069154658397715, 0.8108437032002754, 0.8202640448333322, 0.7815128713795206, 0.8451084830274826, 0.836376388134602, 0.8441441978939892, 0.8509693104465095, 0.37150586919602946, 0.34273942956965175, 0.3313822553845982, 0.34968755932637086, 0.32960730205573996, 0.3754984208710611, 0.38284578850911233, 0.3840456442426208, 0.3280564413760412, 0.7576709220936438, 0.7335556841917643, 0.7658090217912067, 0.7934167558179147, 0.8025617081483369, 0.7999764269594076, 0.7299292035530152, 0.7697788108067551, 0.7651252822797626, 0.16167439560696928, 0.18696031846775318, 0.21158580997319565, 0.244349141547606, 0.2890559956537958, 0.16298570563996873, 0.21750653437837209, 0.21122767559906996, 0.22550289679700497, 0.15305768362463734, 0.14884944831507896, 0.18600965489753707, 0.1541357033150358, 0.19832744782165168, 0.18983314847500432, 0.1953210887165725, 0.20165646735698872, 0.13088573983753993, 0.489476775395826, 0.460206761694571, 0.280690722270316, 0.34064732845138956, 0.522953329166158, 0.46452239779932414, 0.4196789194960461, 0.4727221489210359, 0.44131563048738687, 0.6540713674624187, 0.6128130384068273, 0.5553131009305013, 0.5776200055109268, 0.4723536786910414, 0.6219680956281209, 0.6455804612075908, 0.6109257481083044, 0.5996992464126676, 0.15220127888973134, 0.11018658007737869, 0.2413715712685326, 0.2296484097510183, 0.2572737636354798, 0.10622686325653308, 0.14604068872930986, 0.14809138800719435, 0.12257392403735012, 0.36655256082417187, 0.34632185847096675, 0.3516395670957846, 0.33828319802853724, 0.3572041238804262, 0.35621925345698524, 0.34628041224843775, 0.36205674625584605, 0.3801826028652373, 0.7284525303733058, 0.7334889541026953, 0.7386557191632159, 0.7055754468414382, 0.7222856768071027, 0.7194774022217179, 0.518854950686749, 0.7321096452535195, 0.7397060954282213, 0.10374399773694243, 0.104282622260642, 0.1086381702881759, 0.11429613558107654, 0.11511930056195652, 0.10035386421161252, 0.09599830676572241, 0.11347281801819442, 0.11470225552281121, 0.14952999643648723, 0.16817216547016145, 0.17310734903563885, 0.1855986179207637, 0.18359480037739373, 0.19508916572226054, 0.20087729718879477, 0.17807185486405164, 0.20119801391244463, 0.3682121014468256, 0.35073539948175425, 0.3330439038672147, 0.4157077631363848, 0.37474021495439724, 0.36760475650845414, 0.38008256501576043, 0.39084781839078786, 0.3786757352451906, 0.30037946434366947, 0.2861866860206934, 0.2891587957817804, 0.2835627691001067, 0.2779602895144492, 0.28387893685888044, 0.29400381025955125, 0.3244615423116647, 0.30188351717337136, 0.20494558183242018, 0.19905376550038034, 0.20406514428030742, 0.2229753496453719, 0.1980660060283188, 0.21539123082194234, 0.21928087314384304, 0.20854328449838244, 0.2102975078755156, 0.18082128476991477, 0.1831737258940077, 0.18089956975930732, 0.18277323416104252, 0.1792695777259361, 0.17816092407226292, 0.19628636488975992, 0.19237612183491615, 0.1874718333986961, 0.6294666203187917, 0.602690555999032, 0.5454952462675013, 0.7727337342883757, 0.25677125313719296, 0.588947972014151, 0.5053651729784945, 0.42433758940449484, 0.610090527727432, 0.7148365734773846, 0.39316249107203016, 0.4609914040189693, 0.5414475146943718, 0.3738614312609654, 0.2972834077003401, 0.2719039283446757, 0.4464259994844204, 0.6865015215956998, 0.18541089636272012, 0.17521600614441235, 0.18498258988690697, 0.1923345960637889, 0.19442272937612437, 0.18424555156947642, 0.17449564646480575, 0.18372628686318526, 0.18823517758491137, 0.07953923390817585, 0.08429672806217459, 0.08351517483047155, 0.07872531348008505, 0.0885755282031725, 0.09071254814221597, 0.07022751225803625, 0.07907174920771143, 0.08022853716110878]}, "mutation_prompt": null}
{"id": "80f73113-b720-4044-8d7f-95543844befa", "solution": "import numpy as np\n\nclass EnhancedProbabilisticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = 40\n        self.mutation_factor = 0.85\n        self.crossover_probability = 0.95\n        self.learning_rate = 0.2\n        self.diversity_factor = 0.1\n        self.population = None\n        self.fitness = None\n\n    def initialize_population(self):\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, \n                                            (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def adaptive_differential_evolution(self, idx, func):\n        indices = [i for i in range(self.population_size) if i != idx]\n        np.random.shuffle(indices)\n        a, b, c = indices[:3]\n        mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        adaptive_factor = 0.4 + 0.6 * np.random.rand()\n        mutant = self.population[idx] + adaptive_factor * (mutant - self.population[idx])\n        mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n        \n        cross_points = np.random.rand(self.dim) < self.crossover_probability\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        \n        trial = np.where(cross_points, mutant, self.population[idx])\n        trial_fitness = func(trial)\n        \n        if trial_fitness < self.fitness[idx]:\n            self.population[idx] = trial\n            self.fitness[idx] = trial_fitness\n\n    def stochastic_gradient_approx(self, idx, func):\n        gradient = np.random.normal(0, 1, self.dim)\n        adaptive_lr = self.learning_rate / (1 + idx / self.population_size)\n        candidate = self.population[idx] - adaptive_lr * gradient\n        candidate += self.diversity_factor * np.random.normal(0, 1, self.dim)\n        candidate = np.clip(candidate, self.lower_bound, self.upper_bound)\n        candidate_fitness = func(candidate)\n        if candidate_fitness < self.fitness[idx]:\n            self.population[idx] = candidate\n            self.fitness[idx] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population()\n        evaluations = 0\n        self.evaluate_population(func)\n        evaluations += self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                self.adaptive_differential_evolution(i, func)\n                evaluations += 1\n\n                if evaluations >= self.budget:\n                    break\n\n                self.stochastic_gradient_approx(i, func)\n                evaluations += 1\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "EnhancedProbabilisticOptimizer", "description": "Enhanced probabilistic hybrid optimizer balancing exploration and exploitation through diversity-oriented selection and adaptive parameter tuning for optimal convergence.", "configspace": "", "generation": 38, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "1459935d-0a5b-47de-8bfe-69f52cb3cb6c", "metadata": {"aucs": [0.796727445346743, 0.7946092935536371, 0.8025157468331956, 0.7795890428061506, 0.7652866403837684, 0.7888980264133363, 0.7851539828124771, 0.7801166288092796, 0.778684214476141, 0.6066596860215113, 0.5509488918434079, 0.584709500115218, 0.6151880393982149, 0.6128285196462557, 0.5928554548480235, 0.18781852246903208, 0.6051102165828588, 0.6050716765741883, 0.12056849613284826, 0.10627786970516262, 0.11092740344636509, 0.12458142503345515, 0.10225070914012868, 0.10214739878052792, 0.11007052847089493, 0.11303443664409152, 0.11346829363668931, 0.09058514750802149, 0.09136224141572435, 0.09346304265481342, 0.08413688806219866, 0.10560994817344638, 0.09386944951658027, 0.1066829182603829, 0.09525027475772718, 0.09078080677184064, 0.8379701410373176, 0.8069154658397715, 0.8108437032002754, 0.8202640448333322, 0.7815128713795206, 0.8451084830274826, 0.836376388134602, 0.8441441978939892, 0.8509693104465095, 0.37150586919602946, 0.34273942956965175, 0.3313822553845982, 0.34968755932637086, 0.32960730205573996, 0.3754984208710611, 0.38284578850911233, 0.3840456442426208, 0.3280564413760412, 0.7576709220936438, 0.7335556841917643, 0.7658090217912067, 0.7934167558179147, 0.8025617081483369, 0.7999764269594076, 0.7299292035530152, 0.7697788108067551, 0.7651252822797626, 0.16167439560696928, 0.18696031846775318, 0.21158580997319565, 0.244349141547606, 0.2890559956537958, 0.16298570563996873, 0.21750653437837209, 0.21122767559906996, 0.22550289679700497, 0.15305768362463734, 0.14884944831507896, 0.18600965489753707, 0.1541357033150358, 0.19832744782165168, 0.18983314847500432, 0.1953210887165725, 0.20165646735698872, 0.13088573983753993, 0.489476775395826, 0.460206761694571, 0.280690722270316, 0.34064732845138956, 0.522953329166158, 0.46452239779932414, 0.4196789194960461, 0.4727221489210359, 0.44131563048738687, 0.6540713674624187, 0.6128130384068273, 0.5553131009305013, 0.5776200055109268, 0.4723536786910414, 0.6219680956281209, 0.6455804612075908, 0.6109257481083044, 0.5996992464126676, 0.15220127888973134, 0.11018658007737869, 0.2413715712685326, 0.2296484097510183, 0.2572737636354798, 0.10622686325653308, 0.14604068872930986, 0.14809138800719435, 0.12257392403735012, 0.36655256082417187, 0.34632185847096675, 0.3516395670957846, 0.33828319802853724, 0.3572041238804262, 0.35621925345698524, 0.34628041224843775, 0.36205674625584605, 0.3801826028652373, 0.7284525303733058, 0.7334889541026953, 0.7386557191632159, 0.7055754468414382, 0.7222856768071027, 0.7194774022217179, 0.518854950686749, 0.7321096452535195, 0.7397060954282213, 0.10374399773694243, 0.104282622260642, 0.1086381702881759, 0.11429613558107654, 0.11511930056195652, 0.10035386421161252, 0.09599830676572241, 0.11347281801819442, 0.11470225552281121, 0.14952999643648723, 0.16817216547016145, 0.17310734903563885, 0.1855986179207637, 0.18359480037739373, 0.19508916572226054, 0.20087729718879477, 0.17807185486405164, 0.20119801391244463, 0.3682121014468256, 0.35073539948175425, 0.3330439038672147, 0.4157077631363848, 0.37474021495439724, 0.36760475650845414, 0.38008256501576043, 0.39084781839078786, 0.3786757352451906, 0.30037946434366947, 0.2861866860206934, 0.2891587957817804, 0.2835627691001067, 0.2779602895144492, 0.28387893685888044, 0.29400381025955125, 0.3244615423116647, 0.30188351717337136, 0.20494558183242018, 0.19905376550038034, 0.20406514428030742, 0.2229753496453719, 0.1980660060283188, 0.21539123082194234, 0.21928087314384304, 0.20854328449838244, 0.2102975078755156, 0.18082128476991477, 0.1831737258940077, 0.18089956975930732, 0.18277323416104252, 0.1792695777259361, 0.17816092407226292, 0.19628636488975992, 0.19237612183491615, 0.1874718333986961, 0.6294666203187917, 0.602690555999032, 0.5454952462675013, 0.7727337342883757, 0.25677125313719296, 0.588947972014151, 0.5053651729784945, 0.42433758940449484, 0.610090527727432, 0.7148365734773846, 0.39316249107203016, 0.4609914040189693, 0.5414475146943718, 0.3738614312609654, 0.2972834077003401, 0.2719039283446757, 0.4464259994844204, 0.6865015215956998, 0.18541089636272012, 0.17521600614441235, 0.18498258988690697, 0.1923345960637889, 0.19442272937612437, 0.18424555156947642, 0.17449564646480575, 0.18372628686318526, 0.18823517758491137, 0.07953923390817585, 0.08429672806217459, 0.08351517483047155, 0.07872531348008505, 0.0885755282031725, 0.09071254814221597, 0.07022751225803625, 0.07907174920771143, 0.08022853716110878]}, "mutation_prompt": null}
{"id": "062189ca-e172-4d8d-a548-79ec4964ee0c", "solution": "import numpy as np\n\nclass EnhancedProbabilisticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = 40\n        self.mutation_factor = 0.85\n        self.crossover_probability = 0.95\n        self.learning_rate = 0.2\n        self.diversity_factor = 0.1\n        self.population = None\n        self.fitness = None\n\n    def initialize_population(self):\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, \n                                            (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def adaptive_differential_evolution(self, idx, func):\n        indices = [i for i in range(self.population_size) if i != idx]\n        np.random.shuffle(indices)\n        a, b, c = indices[:3]\n        mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        adaptive_factor = 0.4 + 0.6 * np.random.rand()\n        mutant = self.population[idx] + adaptive_factor * (mutant - self.population[idx])\n        mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n        \n        cross_points = np.random.rand(self.dim) < self.crossover_probability\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        \n        trial = np.where(cross_points, mutant, self.population[idx])\n        trial_fitness = func(trial)\n        \n        if trial_fitness < self.fitness[idx]:\n            self.population[idx] = trial\n            self.fitness[idx] = trial_fitness\n\n    def stochastic_gradient_approx(self, idx, func):\n        gradient = np.random.normal(0, 1, self.dim)\n        adaptive_lr = self.learning_rate / (1 + idx / self.population_size)\n        candidate = self.population[idx] - adaptive_lr * gradient\n        candidate += self.diversity_factor * np.random.normal(0, 1, self.dim)\n        candidate = np.clip(candidate, self.lower_bound, self.upper_bound)\n        candidate_fitness = func(candidate)\n        if candidate_fitness < self.fitness[idx]:\n            self.population[idx] = candidate\n            self.fitness[idx] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population()\n        evaluations = 0\n        self.evaluate_population(func)\n        evaluations += self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                self.adaptive_differential_evolution(i, func)\n                evaluations += 1\n\n                if evaluations >= self.budget:\n                    break\n\n                self.stochastic_gradient_approx(i, func)\n                evaluations += 1\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "EnhancedProbabilisticOptimizer", "description": "Enhanced probabilistic hybrid optimizer balancing exploration and exploitation through diversity-oriented selection and adaptive parameter tuning for optimal convergence.", "configspace": "", "generation": 38, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "1459935d-0a5b-47de-8bfe-69f52cb3cb6c", "metadata": {"aucs": [0.796727445346743, 0.7946092935536371, 0.8025157468331956, 0.7795890428061506, 0.7652866403837684, 0.7888980264133363, 0.7851539828124771, 0.7801166288092796, 0.778684214476141, 0.6066596860215113, 0.5509488918434079, 0.584709500115218, 0.6151880393982149, 0.6128285196462557, 0.5928554548480235, 0.18781852246903208, 0.6051102165828588, 0.6050716765741883, 0.12056849613284826, 0.10627786970516262, 0.11092740344636509, 0.12458142503345515, 0.10225070914012868, 0.10214739878052792, 0.11007052847089493, 0.11303443664409152, 0.11346829363668931, 0.09058514750802149, 0.09136224141572435, 0.09346304265481342, 0.08413688806219866, 0.10560994817344638, 0.09386944951658027, 0.1066829182603829, 0.09525027475772718, 0.09078080677184064, 0.8379701410373176, 0.8069154658397715, 0.8108437032002754, 0.8202640448333322, 0.7815128713795206, 0.8451084830274826, 0.836376388134602, 0.8441441978939892, 0.8509693104465095, 0.37150586919602946, 0.34273942956965175, 0.3313822553845982, 0.34968755932637086, 0.32960730205573996, 0.3754984208710611, 0.38284578850911233, 0.3840456442426208, 0.3280564413760412, 0.7576709220936438, 0.7335556841917643, 0.7658090217912067, 0.7934167558179147, 0.8025617081483369, 0.7999764269594076, 0.7299292035530152, 0.7697788108067551, 0.7651252822797626, 0.16167439560696928, 0.18696031846775318, 0.21158580997319565, 0.244349141547606, 0.2890559956537958, 0.16298570563996873, 0.21750653437837209, 0.21122767559906996, 0.22550289679700497, 0.15305768362463734, 0.14884944831507896, 0.18600965489753707, 0.1541357033150358, 0.19832744782165168, 0.18983314847500432, 0.1953210887165725, 0.20165646735698872, 0.13088573983753993, 0.489476775395826, 0.460206761694571, 0.280690722270316, 0.34064732845138956, 0.522953329166158, 0.46452239779932414, 0.4196789194960461, 0.4727221489210359, 0.44131563048738687, 0.6540713674624187, 0.6128130384068273, 0.5553131009305013, 0.5776200055109268, 0.4723536786910414, 0.6219680956281209, 0.6455804612075908, 0.6109257481083044, 0.5996992464126676, 0.15220127888973134, 0.11018658007737869, 0.2413715712685326, 0.2296484097510183, 0.2572737636354798, 0.10622686325653308, 0.14604068872930986, 0.14809138800719435, 0.12257392403735012, 0.36655256082417187, 0.34632185847096675, 0.3516395670957846, 0.33828319802853724, 0.3572041238804262, 0.35621925345698524, 0.34628041224843775, 0.36205674625584605, 0.3801826028652373, 0.7284525303733058, 0.7334889541026953, 0.7386557191632159, 0.7055754468414382, 0.7222856768071027, 0.7194774022217179, 0.518854950686749, 0.7321096452535195, 0.7397060954282213, 0.10374399773694243, 0.104282622260642, 0.1086381702881759, 0.11429613558107654, 0.11511930056195652, 0.10035386421161252, 0.09599830676572241, 0.11347281801819442, 0.11470225552281121, 0.14952999643648723, 0.16817216547016145, 0.17310734903563885, 0.1855986179207637, 0.18359480037739373, 0.19508916572226054, 0.20087729718879477, 0.17807185486405164, 0.20119801391244463, 0.3682121014468256, 0.35073539948175425, 0.3330439038672147, 0.4157077631363848, 0.37474021495439724, 0.36760475650845414, 0.38008256501576043, 0.39084781839078786, 0.3786757352451906, 0.30037946434366947, 0.2861866860206934, 0.2891587957817804, 0.2835627691001067, 0.2779602895144492, 0.28387893685888044, 0.29400381025955125, 0.3244615423116647, 0.30188351717337136, 0.20494558183242018, 0.19905376550038034, 0.20406514428030742, 0.2229753496453719, 0.1980660060283188, 0.21539123082194234, 0.21928087314384304, 0.20854328449838244, 0.2102975078755156, 0.18082128476991477, 0.1831737258940077, 0.18089956975930732, 0.18277323416104252, 0.1792695777259361, 0.17816092407226292, 0.19628636488975992, 0.19237612183491615, 0.1874718333986961, 0.6294666203187917, 0.602690555999032, 0.5454952462675013, 0.7727337342883757, 0.25677125313719296, 0.588947972014151, 0.5053651729784945, 0.42433758940449484, 0.610090527727432, 0.7148365734773846, 0.39316249107203016, 0.4609914040189693, 0.5414475146943718, 0.3738614312609654, 0.2972834077003401, 0.2719039283446757, 0.4464259994844204, 0.6865015215956998, 0.18541089636272012, 0.17521600614441235, 0.18498258988690697, 0.1923345960637889, 0.19442272937612437, 0.18424555156947642, 0.17449564646480575, 0.18372628686318526, 0.18823517758491137, 0.07953923390817585, 0.08429672806217459, 0.08351517483047155, 0.07872531348008505, 0.0885755282031725, 0.09071254814221597, 0.07022751225803625, 0.07907174920771143, 0.08022853716110878]}, "mutation_prompt": null}
{"id": "e068db85-5784-492c-b8b7-87daa574e01b", "solution": "import numpy as np\n\nclass DynamicMemoryHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = 50  # increased population size\n        self.mutation_factor = 0.9  # slightly increased mutation factor\n        self.crossover_probability = 0.9  # adjusted crossover probability\n        self.learning_rate = 0.15  # adjusted learning rate\n        self.diversity_factor = 0.2  # increased diversity factor\n        self.memory = []  # added memory for storing past evaluations\n        self.population = None\n        self.fitness = None\n\n    def initialize_population(self):\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, \n                                            (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.memory.append((self.population[i], self.fitness[i]))  # storing evaluations\n\n    def adaptive_differential_evolution(self, idx, func):\n        indices = [i for i in range(self.population_size) if i != idx]\n        np.random.shuffle(indices)\n        a, b, c = indices[:3]\n        mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        adaptive_factor = 0.5 + 0.5 * np.random.rand()  # refined adaptive factor range\n        mutant = self.population[idx] + adaptive_factor * (mutant - self.population[idx])\n        mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n        \n        cross_points = np.random.rand(self.dim) < self.crossover_probability\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        \n        trial = np.where(cross_points, mutant, self.population[idx])\n        trial_fitness = func(trial)\n        \n        if trial_fitness < self.fitness[idx]:\n            self.population[idx] = trial\n            self.fitness[idx] = trial_fitness\n            self.memory.append((trial, trial_fitness))  # update memory\n\n    def stochastic_gradient_approx(self, idx, func):\n        gradient = np.random.normal(0, 1, self.dim)\n        adaptive_lr = self.learning_rate / (1 + np.sqrt(idx / self.population_size))  # refined LR adaptation\n        candidate = self.population[idx] - adaptive_lr * gradient\n        candidate += self.diversity_factor * np.random.normal(0, 1, self.dim)\n        candidate = np.clip(candidate, self.lower_bound, self.upper_bound)\n        candidate_fitness = func(candidate)\n        \n        if candidate_fitness < self.fitness[idx]:\n            self.population[idx] = candidate\n            self.fitness[idx] = candidate_fitness\n            self.memory.append((candidate, candidate_fitness))  # update memory\n\n    def __call__(self, func):\n        self.initialize_population()\n        evaluations = 0\n        self.evaluate_population(func)\n        evaluations += self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                self.adaptive_differential_evolution(i, func)\n                evaluations += 1\n\n                if evaluations >= self.budget:\n                    break\n\n                self.stochastic_gradient_approx(i, func)\n                evaluations += 1\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "DynamicMemoryHybridOptimizer", "description": "Probabilistic dynamic hybrid optimizer that integrates adaptive parameter control and memory-based diversity to enhance convergence and robustness.", "configspace": "", "generation": 46, "fitness": 0.28388242846858813, "feedback": "The algorithm DynamicMemoryHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28 with standard deviation 0.20.", "error": "", "parent_id": "1459935d-0a5b-47de-8bfe-69f52cb3cb6c", "metadata": {"aucs": [0.6853420497856038, 0.6977019970511811, 0.6809847467379287, 0.6704221358483734, 0.667902742989429, 0.6688796806360173, 0.6762039369750517, 0.6896986715450311, 0.6770403892089208, 0.40741458651733264, 0.3658795310042259, 0.3791391092350208, 0.4055880954613187, 0.3871044965913295, 0.3793934186017587, 0.4062530831961597, 0.3924567310853788, 0.4113400899608214, 0.09578795176396415, 0.10111868981649208, 0.09667123422421664, 0.09255497794282541, 0.1117228812446378, 0.10017427793272149, 0.10946116830644681, 0.10841908981254433, 0.10310477270447338, 0.08664938507377085, 0.08296620019201328, 0.10710683656865005, 0.09355296451135742, 0.09997912530999298, 0.0891771456517525, 0.08644268558352675, 0.08419786777177451, 0.08408307698614903, 0.9329039773737094, 0.8954556631563615, 0.9595186787894764, 0.8520203376821333, 0.8319046289817198, 0.8524645143059877, 0.8382227777452824, 0.8423224581943594, 0.8129302554882275, 0.30183432518381126, 0.2738851473785777, 0.2936928668193405, 0.28581283739582664, 0.2899502696087928, 0.2866498494658076, 0.30228770876210276, 0.2874042130498077, 0.2960113262361589, 0.6513454312356584, 0.6198389754176212, 0.5945344429519274, 0.6548064088916687, 0.638507581163782, 0.6268807637967982, 0.6041772480668071, 0.5772511951037504, 0.5778159135188505, 0.15409992686710217, 0.1453360262070681, 0.19571663401998074, 0.17314892655334424, 0.17411482063336992, 0.214224064784169, 0.17972081880799484, 0.18241986495052775, 0.18480256105949688, 0.1707748744412222, 0.19527428078731957, 0.20865248356132426, 0.20664994468822695, 0.16058165052856022, 0.16738929443249195, 0.17285255041438286, 0.17049005603332423, 0.16365434899378706, 0.18149732771493932, 0.15707667651507728, 0.21832219992297663, 0.2003443714477371, 0.15710347560428162, 0.18171491273853257, 0.18717846462106158, 0.19973387538440635, 0.22749977699498836, 0.3728680817303843, 0.30310788557629376, 0.33824950153077793, 0.3140821667368995, 0.3133317901680839, 0.31008191591878786, 0.35349737454683483, 0.32849157270950424, 0.35186015073763666, 0.04433968076168293, 0.0655816657170828, 0.05838000153315803, 0.0483212007225442, 0.020290950608831237, 0.0587769252176642, 0.06472202379135783, 0.03656821908496044, 0.05374360553989299, 0.19687170628183792, 0.18408191834093035, 0.19443812858994647, 0.1857388721365264, 0.20258041243494818, 0.193989084420347, 0.17210539306092765, 0.18908917382660628, 0.18615767480634748, 0.5518177555083753, 0.5338198404012389, 0.5620867225595025, 0.5051250885224129, 0.5367909866731824, 0.5391598007613996, 0.5567466001021746, 0.5402394428120358, 0.5417906512839927, 0.1150407521736162, 0.11101562845165003, 0.10291416250374164, 0.0949367530286418, 0.09424891159321502, 0.10926847019744268, 0.08982083578909827, 0.10067312390474237, 0.09100292112064146, 0.20913314371488856, 0.18676452137849686, 0.1739693924084631, 0.21546135235052777, 0.17162450125113515, 0.1718974108438348, 0.1801164319838363, 0.19072865085936197, 0.18271769360904333, 0.25101874940907287, 0.29954910308230165, 0.2860247910549413, 0.28621074276190184, 0.2649925053010157, 0.27242903522042705, 0.29129487606103854, 0.30574778336603703, 0.2912682177616016, 0.21715424643790282, 0.20996439558903024, 0.2371014259404416, 0.21641676811583843, 0.21906700612720964, 0.23035584560218691, 0.2222012280325426, 0.23474549107196696, 0.22071873426886723, 0.20373616241157977, 0.18524593436075587, 0.19358388408078342, 0.22840437514275147, 0.19405325122403483, 0.20217977435906742, 0.20285752800694656, 0.18221777211188106, 0.21450852565380552, 0.18680030296784855, 0.19129255597730033, 0.1730536909984668, 0.17950179901581487, 0.17926125242581914, 0.19219375147962292, 0.18732560068718074, 0.18976885620799477, 0.1810651985968993, 0.37032989072782085, 0.35464346712545836, 0.30561348308872593, 0.3484563166733238, 0.37223078430457857, 0.4144883503723661, 0.3073473042423813, 0.39591083902273716, 0.36497545226385186, 0.3015324570040808, 0.38931676892512246, 0.3515913955983937, 0.31427187819445357, 0.2778678576918945, 0.3431697096722346, 0.20829766615174772, 0.3674269019920332, 0.22543159342975227, 0.1773855689269188, 0.17620005011325524, 0.1693902940080041, 0.2041712949190837, 0.18563981470785818, 0.20339535918123974, 0.17547689449634218, 0.1967085584837518, 0.1832612879829899, 0.08031465465580534, 0.07387033505044016, 0.07775949218343126, 0.0798429971802016, 0.09235129227413619, 0.0741175124179555, 0.07433298998388582, 0.09494959687007909, 0.07024482776810725]}, "mutation_prompt": null}
{"id": "c3cb1bbb-8b1a-48eb-8fdb-a29c22bd2f5b", "solution": "import numpy as np\n\nclass EnhancedProbabilisticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = 40\n        self.mutation_factor = 0.85\n        self.crossover_probability = 0.95\n        self.learning_rate = 0.2\n        self.diversity_factor = 0.1\n        self.population = None\n        self.fitness = None\n\n    def initialize_population(self):\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, \n                                            (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def adaptive_differential_evolution(self, idx, func):\n        indices = [i for i in range(self.population_size) if i != idx]\n        np.random.shuffle(indices)\n        a, b, c = indices[:3]\n        mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        adaptive_factor = 0.4 + 0.6 * np.random.rand()\n        mutant = self.population[idx] + adaptive_factor * (mutant - self.population[idx])\n        mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n        \n        cross_points = np.random.rand(self.dim) < self.crossover_probability\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        \n        trial = np.where(cross_points, mutant, self.population[idx])\n        trial_fitness = func(trial)\n        \n        if trial_fitness < self.fitness[idx]:\n            self.population[idx] = trial\n            self.fitness[idx] = trial_fitness\n\n    def stochastic_gradient_approx(self, idx, func):\n        gradient = np.random.normal(0, 1, self.dim)\n        adaptive_lr = self.learning_rate / (1 + idx / self.population_size)\n        candidate = self.population[idx] - adaptive_lr * gradient\n        candidate += self.diversity_factor * np.random.normal(0, 1, self.dim)\n        candidate = np.clip(candidate, self.lower_bound, self.upper_bound)\n        candidate_fitness = func(candidate)\n        if candidate_fitness < self.fitness[idx]:\n            self.population[idx] = candidate\n            self.fitness[idx] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population()\n        evaluations = 0\n        self.evaluate_population(func)\n        evaluations += self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                self.adaptive_differential_evolution(i, func)\n                evaluations += 1\n\n                if evaluations >= self.budget:\n                    break\n\n                self.stochastic_gradient_approx(i, func)\n                evaluations += 1\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "EnhancedProbabilisticOptimizer", "description": "Enhanced probabilistic hybrid optimizer balancing exploration and exploitation through diversity-oriented selection and adaptive parameter tuning for optimal convergence.", "configspace": "", "generation": 38, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "1459935d-0a5b-47de-8bfe-69f52cb3cb6c", "metadata": {"aucs": [0.796727445346743, 0.7946092935536371, 0.8025157468331956, 0.7795890428061506, 0.7652866403837684, 0.7888980264133363, 0.7851539828124771, 0.7801166288092796, 0.778684214476141, 0.6066596860215113, 0.5509488918434079, 0.584709500115218, 0.6151880393982149, 0.6128285196462557, 0.5928554548480235, 0.18781852246903208, 0.6051102165828588, 0.6050716765741883, 0.12056849613284826, 0.10627786970516262, 0.11092740344636509, 0.12458142503345515, 0.10225070914012868, 0.10214739878052792, 0.11007052847089493, 0.11303443664409152, 0.11346829363668931, 0.09058514750802149, 0.09136224141572435, 0.09346304265481342, 0.08413688806219866, 0.10560994817344638, 0.09386944951658027, 0.1066829182603829, 0.09525027475772718, 0.09078080677184064, 0.8379701410373176, 0.8069154658397715, 0.8108437032002754, 0.8202640448333322, 0.7815128713795206, 0.8451084830274826, 0.836376388134602, 0.8441441978939892, 0.8509693104465095, 0.37150586919602946, 0.34273942956965175, 0.3313822553845982, 0.34968755932637086, 0.32960730205573996, 0.3754984208710611, 0.38284578850911233, 0.3840456442426208, 0.3280564413760412, 0.7576709220936438, 0.7335556841917643, 0.7658090217912067, 0.7934167558179147, 0.8025617081483369, 0.7999764269594076, 0.7299292035530152, 0.7697788108067551, 0.7651252822797626, 0.16167439560696928, 0.18696031846775318, 0.21158580997319565, 0.244349141547606, 0.2890559956537958, 0.16298570563996873, 0.21750653437837209, 0.21122767559906996, 0.22550289679700497, 0.15305768362463734, 0.14884944831507896, 0.18600965489753707, 0.1541357033150358, 0.19832744782165168, 0.18983314847500432, 0.1953210887165725, 0.20165646735698872, 0.13088573983753993, 0.489476775395826, 0.460206761694571, 0.280690722270316, 0.34064732845138956, 0.522953329166158, 0.46452239779932414, 0.4196789194960461, 0.4727221489210359, 0.44131563048738687, 0.6540713674624187, 0.6128130384068273, 0.5553131009305013, 0.5776200055109268, 0.4723536786910414, 0.6219680956281209, 0.6455804612075908, 0.6109257481083044, 0.5996992464126676, 0.15220127888973134, 0.11018658007737869, 0.2413715712685326, 0.2296484097510183, 0.2572737636354798, 0.10622686325653308, 0.14604068872930986, 0.14809138800719435, 0.12257392403735012, 0.36655256082417187, 0.34632185847096675, 0.3516395670957846, 0.33828319802853724, 0.3572041238804262, 0.35621925345698524, 0.34628041224843775, 0.36205674625584605, 0.3801826028652373, 0.7284525303733058, 0.7334889541026953, 0.7386557191632159, 0.7055754468414382, 0.7222856768071027, 0.7194774022217179, 0.518854950686749, 0.7321096452535195, 0.7397060954282213, 0.10374399773694243, 0.104282622260642, 0.1086381702881759, 0.11429613558107654, 0.11511930056195652, 0.10035386421161252, 0.09599830676572241, 0.11347281801819442, 0.11470225552281121, 0.14952999643648723, 0.16817216547016145, 0.17310734903563885, 0.1855986179207637, 0.18359480037739373, 0.19508916572226054, 0.20087729718879477, 0.17807185486405164, 0.20119801391244463, 0.3682121014468256, 0.35073539948175425, 0.3330439038672147, 0.4157077631363848, 0.37474021495439724, 0.36760475650845414, 0.38008256501576043, 0.39084781839078786, 0.3786757352451906, 0.30037946434366947, 0.2861866860206934, 0.2891587957817804, 0.2835627691001067, 0.2779602895144492, 0.28387893685888044, 0.29400381025955125, 0.3244615423116647, 0.30188351717337136, 0.20494558183242018, 0.19905376550038034, 0.20406514428030742, 0.2229753496453719, 0.1980660060283188, 0.21539123082194234, 0.21928087314384304, 0.20854328449838244, 0.2102975078755156, 0.18082128476991477, 0.1831737258940077, 0.18089956975930732, 0.18277323416104252, 0.1792695777259361, 0.17816092407226292, 0.19628636488975992, 0.19237612183491615, 0.1874718333986961, 0.6294666203187917, 0.602690555999032, 0.5454952462675013, 0.7727337342883757, 0.25677125313719296, 0.588947972014151, 0.5053651729784945, 0.42433758940449484, 0.610090527727432, 0.7148365734773846, 0.39316249107203016, 0.4609914040189693, 0.5414475146943718, 0.3738614312609654, 0.2972834077003401, 0.2719039283446757, 0.4464259994844204, 0.6865015215956998, 0.18541089636272012, 0.17521600614441235, 0.18498258988690697, 0.1923345960637889, 0.19442272937612437, 0.18424555156947642, 0.17449564646480575, 0.18372628686318526, 0.18823517758491137, 0.07953923390817585, 0.08429672806217459, 0.08351517483047155, 0.07872531348008505, 0.0885755282031725, 0.09071254814221597, 0.07022751225803625, 0.07907174920771143, 0.08022853716110878]}, "mutation_prompt": null}
{"id": "f1eb269d-005d-4d83-a0fa-46a3ee07d881", "solution": "import numpy as np\n\nclass AdaptiveChaoticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = 40\n        self.mutation_factor = 0.90\n        self.crossover_probability = 0.90\n        self.learning_rate = 0.25\n        self.diversity_factor = 0.15\n        self.chaos_factor = 0.1\n        self.population = None\n        self.fitness = None\n\n    def initialize_population(self):\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, \n                                            (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def chaos_local_search(self, individual):\n        chaos_sequence = np.random.rand(self.dim)\n        return individual + self.chaos_factor * np.sin(2 * np.pi * chaos_sequence)\n\n    def adaptive_differential_evolution(self, idx, func):\n        indices = [i for i in range(self.population_size) if i != idx]\n        np.random.shuffle(indices)\n        a, b, c = indices[:3]\n        mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        adaptive_factor = 0.5 + 0.5 * np.random.rand()\n        mutant = self.population[idx] + adaptive_factor * (mutant - self.population[idx])\n        mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n        \n        cross_points = np.random.rand(self.dim) < self.crossover_probability\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        \n        trial = np.where(cross_points, mutant, self.population[idx])\n        trial_fitness = func(trial)\n        \n        if trial_fitness < self.fitness[idx]:\n            self.population[idx] = trial\n            self.fitness[idx] = trial_fitness\n\n    def stochastic_gradient_approx(self, idx, func):\n        gradient = np.random.normal(0, 1, self.dim)\n        adaptive_lr = self.learning_rate / (1 + idx / self.population_size)\n        candidate = self.population[idx] - adaptive_lr * gradient\n        candidate += self.diversity_factor * np.random.normal(0, 1, self.dim)\n        candidate = np.clip(candidate, self.lower_bound, self.upper_bound)\n        candidate += self.chaos_local_search(candidate)\n        candidate = np.clip(candidate, self.lower_bound, self.upper_bound)\n        candidate_fitness = func(candidate)\n        if candidate_fitness < self.fitness[idx]:\n            self.population[idx] = candidate\n            self.fitness[idx] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population()\n        evaluations = 0\n        self.evaluate_population(func)\n        evaluations += self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                self.adaptive_differential_evolution(i, func)\n                evaluations += 1\n\n                if evaluations >= self.budget:\n                    break\n\n                self.stochastic_gradient_approx(i, func)\n                evaluations += 1\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "AdaptiveChaoticOptimizer", "description": "Adaptive hybrid optimizer integrating chaotic local search and evolutionary strategies for enhanced convergence and diversity maintenance in complex landscapes.", "configspace": "", "generation": 48, "fitness": 0.2916293298961474, "feedback": "The algorithm AdaptiveChaoticOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.29 with standard deviation 0.23.", "error": "", "parent_id": "1459935d-0a5b-47de-8bfe-69f52cb3cb6c", "metadata": {"aucs": [0.722943938632467, 0.7153027788474051, 0.7053933538723007, 0.7184948754541021, 0.7302616847174435, 0.753831783394909, 0.7086845397294121, 0.7242870292279064, 0.7162535150208853, 0.505860801841178, 0.4778787430348346, 0.4900970510421625, 0.47735964551555865, 0.4655370198612089, 0.4463435283189787, 0.48386871574731616, 0.478799395414204, 0.43858403118905487, 0.0973587524878552, 0.10310397169265073, 0.10357239073343283, 0.10699407880471024, 0.10163197247652045, 0.09237754082034755, 0.09907198106392567, 0.08652856179826596, 0.09818830610765472, 0.08805162747611739, 0.08178122763562934, 0.08438471565358918, 0.08696421486592243, 0.08416141738854299, 0.0788170607014288, 0.08641226517830813, 0.08607402830811828, 0.09494063341636538, 0.9911665669546491, 0.9848316123654032, 0.9731756155996165, 0.9953340739766394, 0.9864864266666951, 0.9809963718239372, 0.9766973924610728, 0.9738060911815882, 0.983961331693747, 0.2844658379713686, 0.26180972173149175, 0.29718968747424135, 0.31437531425819687, 0.2830772744774601, 0.29070039109360424, 0.2441188168579257, 0.2849298715179408, 0.30667145316483957, 0.711614269001522, 0.6283825056849953, 0.6244407889104232, 0.6855185555723672, 0.6678573023104275, 0.666417757265561, 0.6898845060488394, 0.6938965072874099, 0.64305970364351, 0.16438343841810799, 0.18692187061264554, 0.16824598595106166, 0.13414679532511187, 0.1613609044937271, 0.136105337508157, 0.1576008967744036, 0.1997421741335561, 0.15142357339841073, 0.1580841324654222, 0.168677739004093, 0.13776251705520914, 0.1510310771783101, 0.2035085905116879, 0.19938805928689618, 0.14642433940711397, 0.1574720939346378, 0.17529289654427216, 0.21880114867757983, 0.23477988393579186, 0.2644674214793966, 0.2717772383843917, 0.2593314707634671, 0.2021053740997799, 0.26487477806953386, 0.2094281761128366, 0.2265704066839831, 0.3958006661313136, 0.31286070052282744, 0.3447158444490842, 0.33416292785770996, 0.32060758216746343, 0.32826140855637287, 0.39078264452064615, 0.3967791280742916, 0.38338019835889936, 0.07248897334245519, 0.07903850282260205, 0.07170679822827164, 0.060242680801294246, 0.1266160192044521, 0.13089552327903797, 0.07989425805395456, 0.051748341631477124, 0.0903155296338809, 0.19390601567674748, 0.2166306910927538, 0.20932911816996325, 0.24284132779218004, 0.2452036240169293, 0.2320616768802295, 0.21093528179936916, 0.21868403861106755, 0.23045322533156465, 0.57317624879931, 0.5896612285893879, 0.5585321193341214, 0.5595892276149019, 0.5790451853608827, 0.563077370993383, 0.5990470377086217, 0.5770940869160266, 0.560684172418872, 0.08014695471432409, 0.09777041675841669, 0.07787202412673089, 0.0927872250418168, 0.08561207092468526, 0.09556593220722587, 0.08623957531275706, 0.08817221719181634, 0.08877586025801543, 0.13291035170805243, 0.13764606839403315, 0.13933250276638032, 0.1379930069662494, 0.12101424975795616, 0.13726517916787961, 0.14096398275028, 0.13062776253540664, 0.132055660208382, 0.29044201766118505, 0.3095360727419133, 0.31053558485381805, 0.28880596392117264, 0.2872752464227042, 0.307984554234489, 0.3231097848412884, 0.3404369280117071, 0.32772898370587644, 0.23761809044801552, 0.25684248090076167, 0.23192355127913156, 0.23439864886161277, 0.23975626898366342, 0.22583444562072374, 0.20150959012547243, 0.2576971079831749, 0.2211211099871513, 0.18130043574781662, 0.19542841298563862, 0.17753386471151078, 0.1866847426312409, 0.17351036997190716, 0.20459667173615526, 0.18591887201282786, 0.20977775256563524, 0.18785886584828337, 0.17322943905874777, 0.16373571056474778, 0.17339295029944546, 0.16951578743690732, 0.17156164500248683, 0.17884606456994334, 0.16812461365881992, 0.1732292456200354, 0.17820933202959366, 0.16739740904577316, 0.5385206001042793, 0.21174873082682977, 0.5990221366733602, 0.14417525802692654, 0.23967380586595077, 0.15343652196681834, 0.20843452342683766, 0.15975977357164028, 0.2795297301813845, 0.2025466465654635, 0.31130973315880295, 0.17657219557163029, 0.15807652665676286, 0.16531872212872223, 0.20515889411333588, 0.2033544852637097, 0.2685289106914973, 0.19334294960294052, 0.18541185711183528, 0.18734738401937134, 0.20365333683126086, 0.16943060842244573, 0.18649414025815936, 0.18378393403263615, 0.1817305220702613, 0.1849328804038921, 0.07519980430949769, 0.07525455176819307, 0.07074218453876768, 0.09796805265569652, 0.07173552378576664, 0.06808106458129548, 0.07984350887525926, 0.08199411551086555, 0.06964133770585013]}, "mutation_prompt": null}
{"id": "2f07aa90-7a73-4cf8-8d90-e4343830cf3f", "solution": "import numpy as np\n\nclass EnhancedProbabilisticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = 40\n        self.mutation_factor = 0.85\n        self.crossover_probability = 0.95\n        self.learning_rate = 0.2\n        self.diversity_factor = 0.1\n        self.population = None\n        self.fitness = None\n\n    def initialize_population(self):\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, \n                                            (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def adaptive_differential_evolution(self, idx, func):\n        indices = [i for i in range(self.population_size) if i != idx]\n        np.random.shuffle(indices)\n        a, b, c = indices[:3]\n        mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        adaptive_factor = 0.4 + 0.6 * np.random.rand()\n        mutant = self.population[idx] + adaptive_factor * (mutant - self.population[idx])\n        mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n        \n        cross_points = np.random.rand(self.dim) < self.crossover_probability\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        \n        trial = np.where(cross_points, mutant, self.population[idx])\n        trial_fitness = func(trial)\n        \n        if trial_fitness < self.fitness[idx]:\n            self.population[idx] = trial\n            self.fitness[idx] = trial_fitness\n\n    def stochastic_gradient_approx(self, idx, func):\n        gradient = np.random.normal(0, 1, self.dim)\n        adaptive_lr = self.learning_rate / (1 + idx / self.population_size)\n        candidate = self.population[idx] - adaptive_lr * gradient\n        candidate += self.diversity_factor * np.random.normal(0, 1, self.dim)\n        candidate = np.clip(candidate, self.lower_bound, self.upper_bound)\n        candidate_fitness = func(candidate)\n        if candidate_fitness < self.fitness[idx]:\n            self.population[idx] = candidate\n            self.fitness[idx] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population()\n        evaluations = 0\n        self.evaluate_population(func)\n        evaluations += self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                self.adaptive_differential_evolution(i, func)\n                evaluations += 1\n\n                if evaluations >= self.budget:\n                    break\n\n                self.stochastic_gradient_approx(i, func)\n                evaluations += 1\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "EnhancedProbabilisticOptimizer", "description": "Enhanced probabilistic hybrid optimizer balancing exploration and exploitation through diversity-oriented selection and adaptive parameter tuning for optimal convergence.", "configspace": "", "generation": 38, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "1459935d-0a5b-47de-8bfe-69f52cb3cb6c", "metadata": {"aucs": [0.796727445346743, 0.7946092935536371, 0.8025157468331956, 0.7795890428061506, 0.7652866403837684, 0.7888980264133363, 0.7851539828124771, 0.7801166288092796, 0.778684214476141, 0.6066596860215113, 0.5509488918434079, 0.584709500115218, 0.6151880393982149, 0.6128285196462557, 0.5928554548480235, 0.18781852246903208, 0.6051102165828588, 0.6050716765741883, 0.12056849613284826, 0.10627786970516262, 0.11092740344636509, 0.12458142503345515, 0.10225070914012868, 0.10214739878052792, 0.11007052847089493, 0.11303443664409152, 0.11346829363668931, 0.09058514750802149, 0.09136224141572435, 0.09346304265481342, 0.08413688806219866, 0.10560994817344638, 0.09386944951658027, 0.1066829182603829, 0.09525027475772718, 0.09078080677184064, 0.8379701410373176, 0.8069154658397715, 0.8108437032002754, 0.8202640448333322, 0.7815128713795206, 0.8451084830274826, 0.836376388134602, 0.8441441978939892, 0.8509693104465095, 0.37150586919602946, 0.34273942956965175, 0.3313822553845982, 0.34968755932637086, 0.32960730205573996, 0.3754984208710611, 0.38284578850911233, 0.3840456442426208, 0.3280564413760412, 0.7576709220936438, 0.7335556841917643, 0.7658090217912067, 0.7934167558179147, 0.8025617081483369, 0.7999764269594076, 0.7299292035530152, 0.7697788108067551, 0.7651252822797626, 0.16167439560696928, 0.18696031846775318, 0.21158580997319565, 0.244349141547606, 0.2890559956537958, 0.16298570563996873, 0.21750653437837209, 0.21122767559906996, 0.22550289679700497, 0.15305768362463734, 0.14884944831507896, 0.18600965489753707, 0.1541357033150358, 0.19832744782165168, 0.18983314847500432, 0.1953210887165725, 0.20165646735698872, 0.13088573983753993, 0.489476775395826, 0.460206761694571, 0.280690722270316, 0.34064732845138956, 0.522953329166158, 0.46452239779932414, 0.4196789194960461, 0.4727221489210359, 0.44131563048738687, 0.6540713674624187, 0.6128130384068273, 0.5553131009305013, 0.5776200055109268, 0.4723536786910414, 0.6219680956281209, 0.6455804612075908, 0.6109257481083044, 0.5996992464126676, 0.15220127888973134, 0.11018658007737869, 0.2413715712685326, 0.2296484097510183, 0.2572737636354798, 0.10622686325653308, 0.14604068872930986, 0.14809138800719435, 0.12257392403735012, 0.36655256082417187, 0.34632185847096675, 0.3516395670957846, 0.33828319802853724, 0.3572041238804262, 0.35621925345698524, 0.34628041224843775, 0.36205674625584605, 0.3801826028652373, 0.7284525303733058, 0.7334889541026953, 0.7386557191632159, 0.7055754468414382, 0.7222856768071027, 0.7194774022217179, 0.518854950686749, 0.7321096452535195, 0.7397060954282213, 0.10374399773694243, 0.104282622260642, 0.1086381702881759, 0.11429613558107654, 0.11511930056195652, 0.10035386421161252, 0.09599830676572241, 0.11347281801819442, 0.11470225552281121, 0.14952999643648723, 0.16817216547016145, 0.17310734903563885, 0.1855986179207637, 0.18359480037739373, 0.19508916572226054, 0.20087729718879477, 0.17807185486405164, 0.20119801391244463, 0.3682121014468256, 0.35073539948175425, 0.3330439038672147, 0.4157077631363848, 0.37474021495439724, 0.36760475650845414, 0.38008256501576043, 0.39084781839078786, 0.3786757352451906, 0.30037946434366947, 0.2861866860206934, 0.2891587957817804, 0.2835627691001067, 0.2779602895144492, 0.28387893685888044, 0.29400381025955125, 0.3244615423116647, 0.30188351717337136, 0.20494558183242018, 0.19905376550038034, 0.20406514428030742, 0.2229753496453719, 0.1980660060283188, 0.21539123082194234, 0.21928087314384304, 0.20854328449838244, 0.2102975078755156, 0.18082128476991477, 0.1831737258940077, 0.18089956975930732, 0.18277323416104252, 0.1792695777259361, 0.17816092407226292, 0.19628636488975992, 0.19237612183491615, 0.1874718333986961, 0.6294666203187917, 0.602690555999032, 0.5454952462675013, 0.7727337342883757, 0.25677125313719296, 0.588947972014151, 0.5053651729784945, 0.42433758940449484, 0.610090527727432, 0.7148365734773846, 0.39316249107203016, 0.4609914040189693, 0.5414475146943718, 0.3738614312609654, 0.2972834077003401, 0.2719039283446757, 0.4464259994844204, 0.6865015215956998, 0.18541089636272012, 0.17521600614441235, 0.18498258988690697, 0.1923345960637889, 0.19442272937612437, 0.18424555156947642, 0.17449564646480575, 0.18372628686318526, 0.18823517758491137, 0.07953923390817585, 0.08429672806217459, 0.08351517483047155, 0.07872531348008505, 0.0885755282031725, 0.09071254814221597, 0.07022751225803625, 0.07907174920771143, 0.08022853716110878]}, "mutation_prompt": null}
{"id": "4a6df3c1-bb00-4695-a086-c9bdbee1fa3f", "solution": "import numpy as np\n\nclass SynergisticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = 50\n        self.mutation_factor = 0.9\n        self.crossover_probability = 0.9\n        self.learning_rate = 0.1\n        self.diversity_factor = 0.15\n        self.population = None\n        self.fitness = None\n\n    def initialize_population(self):\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, \n                                            (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def adaptive_mutation_crossover(self, idx, func):\n        indices = [i for i in range(self.population_size) if i != idx]\n        np.random.shuffle(indices)\n        a, b, c = indices[:3]\n        mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        adaptive_factor = 0.3 + 0.7 * np.random.rand()\n        mutant = self.population[idx] + adaptive_factor * (mutant - self.population[idx])\n        mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n        \n        cross_points = np.random.rand(self.dim) < self.crossover_probability\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        \n        trial = np.where(cross_points, mutant, self.population[idx])\n        trial_fitness = func(trial)\n        \n        if trial_fitness < self.fitness[idx]:\n            self.population[idx] = trial\n            self.fitness[idx] = trial_fitness\n\n    def diversity_adjustment(self, idx, func):\n        gradient = np.random.normal(0, 1, self.dim)\n        adaptive_lr = self.learning_rate / (1 + idx / self.population_size)\n        candidate = self.population[idx] - adaptive_lr * gradient\n        candidate += self.diversity_factor * np.random.normal(0, 1, self.dim)\n        candidate = np.clip(candidate, self.lower_bound, self.upper_bound)\n        candidate_fitness = func(candidate)\n        if candidate_fitness < self.fitness[idx]:\n            self.population[idx] = candidate\n            self.fitness[idx] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population()\n        evaluations = 0\n        self.evaluate_population(func)\n        evaluations += self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                self.adaptive_mutation_crossover(i, func)\n                evaluations += 1\n\n                if evaluations >= self.budget:\n                    break\n\n                self.diversity_adjustment(i, func)\n                evaluations += 1\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "SynergisticOptimizer", "description": "Synergistic optimizer integrating adaptive mutation and diversity-driven adjustment for balanced exploration and exploitation with dynamic convergence.", "configspace": "", "generation": 50, "fitness": 0.3044431859429007, "feedback": "The algorithm SynergisticOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.30 with standard deviation 0.21.", "error": "", "parent_id": "1459935d-0a5b-47de-8bfe-69f52cb3cb6c", "metadata": {"aucs": [0.7615222737302054, 0.748376991829246, 0.7531411178924469, 0.7089978424216876, 0.7157137149750163, 0.7306136572784434, 0.730676362554186, 0.7027661816935724, 0.7164311646305367, 0.45007426794938654, 0.4481346322605374, 0.4548941099231133, 0.4911076921063452, 0.4569010574866116, 0.4850527667309903, 0.4792600587909924, 0.4715878659630184, 0.4662074177471168, 0.10819515886425157, 0.10824047504907064, 0.11252468447256125, 0.13513092424876538, 0.123645394525843, 0.1077005599284534, 0.10426559934816326, 0.106934364988724, 0.10723971050588132, 0.0877570082781407, 0.08935310991966183, 0.09928245834840976, 0.10132508969322485, 0.09481408993905682, 0.09068601022829337, 0.09633604604245627, 0.0868704163674704, 0.1005199976178377, 0.7966453944475552, 0.808853128871433, 0.8421284921800515, 0.8443974658875262, 0.7481069485412724, 0.7854923981770172, 0.8128209132200624, 0.799951523530198, 0.8240263677935267, 0.326361056095425, 0.29173241383418314, 0.29368640899728227, 0.34116844469781504, 0.3029077202250312, 0.28525094333962164, 0.2799848899724773, 0.3088411940938849, 0.31503653600814385, 0.6098257128941401, 0.6774018898205698, 0.6140072179973048, 0.6792963893022617, 0.7448605301223155, 0.715728552768884, 0.5733279245937006, 0.6705352289464356, 0.621946916646996, 0.15324117827648953, 0.2137662864139429, 0.15928114286199613, 0.1761150727683587, 0.1351008218887011, 0.20413879039015348, 0.21016121531822396, 0.16534582124621522, 0.1539847595026349, 0.14293738320137694, 0.1899810301104795, 0.17824802631983605, 0.15715137272292434, 0.17590882858618784, 0.1698206252802844, 0.17432476168398936, 0.15632602376016946, 0.1813525575369367, 0.2045259267503885, 0.21515304915805933, 0.2602804426429822, 0.21607383645525247, 0.21036360313085756, 0.23082470717261938, 0.2910319628686846, 0.21957787286087838, 0.2975572211142514, 0.41180443338019646, 0.3406491770783626, 0.3918878797808726, 0.3947119087170817, 0.3642731343082488, 0.4456266341627416, 0.3881933242391977, 0.4189699813823784, 0.41633861262723637, 0.07523534830557488, 0.08123052375439932, 0.06142599296524898, 0.08045111739805999, 0.13245668143918465, 0.07100796414479904, 0.08575171006426241, 0.11780219276098414, 0.06422945057277862, 0.22448490133031584, 0.22489221985664165, 0.23514927132552577, 0.2320444921409468, 0.2449252679472267, 0.2352875938898722, 0.23554281176943437, 0.23308652341679215, 0.2136647802749454, 0.6210191874398505, 0.5969499414470489, 0.6223736508677511, 0.6017438298366123, 0.5888309159414942, 0.591780395811126, 0.6031911732392135, 0.6188095539126276, 0.6238385054238946, 0.09776670428413059, 0.09989784965631465, 0.1271549119777795, 0.09678021908771284, 0.09761044573316335, 0.09699491926630266, 0.10493778918744612, 0.10528691506604737, 0.09572578437590162, 0.18274511262733628, 0.22023227155152825, 0.2057803181475163, 0.19813982180533862, 0.18684734289916127, 0.17953373202642675, 0.20310988550870912, 0.1594821067898069, 0.1922759176519706, 0.3009336248493665, 0.30013045041272823, 0.31108256553726, 0.286922444680335, 0.30734140466953586, 0.31684348251901673, 0.3175204412944723, 0.30314484423632404, 0.28828791916869956, 0.24487122925275862, 0.2463457067258742, 0.2400618285736711, 0.23974758740721636, 0.2374651339376983, 0.24887465621252003, 0.22865530438993042, 0.244202673671022, 0.22997000679884516, 0.20436569033809215, 0.1973435377711117, 0.1985628271275396, 0.22042106886916557, 0.20438158212792323, 0.19715170554472605, 0.20223332290448515, 0.20739192325178213, 0.20976733135302106, 0.19238070575586352, 0.1972518378206195, 0.18801163611097071, 0.1839911150607335, 0.18450924835303617, 0.18264415769872933, 0.1851679463027015, 0.18761127012380197, 0.1833919096393991, 0.2078397156527606, 0.2736850397935524, 0.4360549813729898, 0.3895464826266254, 0.3864489939469832, 0.3785282214521749, 0.5177043071603898, 0.38020556167922437, 0.43100432721074844, 0.5516274365489209, 0.35358480396372394, 0.49196507523094346, 0.33301960064677083, 0.2410267288250536, 0.3035953236800234, 0.21013775529054102, 0.2945566398697296, 0.3694312863321654, 0.1963317028023397, 0.17580875513495775, 0.19835881416550782, 0.19418596570939028, 0.2044311636692242, 0.18362088016613454, 0.18253535041853985, 0.1927504701517022, 0.18214142892370921, 0.07716196028719968, 0.10572714945858519, 0.08834675509193601, 0.07792903191287537, 0.08062985871890582, 0.07567350676806073, 0.07608821864640647, 0.09076543803976167, 0.0791589838738006]}, "mutation_prompt": null}
{"id": "6c96789f-00db-4f42-aa82-e943dc7c9e58", "solution": "import numpy as np\n\nclass EnhancedProbabilisticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = 40\n        self.mutation_factor = 0.85\n        self.crossover_probability = 0.95\n        self.learning_rate = 0.2\n        self.diversity_factor = 0.1\n        self.population = None\n        self.fitness = None\n\n    def initialize_population(self):\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, \n                                            (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def adaptive_differential_evolution(self, idx, func):\n        indices = [i for i in range(self.population_size) if i != idx]\n        np.random.shuffle(indices)\n        a, b, c = indices[:3]\n        mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        adaptive_factor = 0.4 + 0.6 * np.random.rand()\n        mutant = self.population[idx] + adaptive_factor * (mutant - self.population[idx])\n        mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n        \n        cross_points = np.random.rand(self.dim) < self.crossover_probability\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        \n        trial = np.where(cross_points, mutant, self.population[idx])\n        trial_fitness = func(trial)\n        \n        if trial_fitness < self.fitness[idx]:\n            self.population[idx] = trial\n            self.fitness[idx] = trial_fitness\n\n    def stochastic_gradient_approx(self, idx, func):\n        gradient = np.random.normal(0, 1, self.dim)\n        adaptive_lr = self.learning_rate / (1 + idx / self.population_size)\n        candidate = self.population[idx] - adaptive_lr * gradient\n        candidate += self.diversity_factor * np.random.normal(0, 1, self.dim)\n        candidate = np.clip(candidate, self.lower_bound, self.upper_bound)\n        candidate_fitness = func(candidate)\n        if candidate_fitness < self.fitness[idx]:\n            self.population[idx] = candidate\n            self.fitness[idx] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population()\n        evaluations = 0\n        self.evaluate_population(func)\n        evaluations += self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                self.adaptive_differential_evolution(i, func)\n                evaluations += 1\n\n                if evaluations >= self.budget:\n                    break\n\n                self.stochastic_gradient_approx(i, func)\n                evaluations += 1\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "EnhancedProbabilisticOptimizer", "description": "Enhanced probabilistic hybrid optimizer balancing exploration and exploitation through diversity-oriented selection and adaptive parameter tuning for optimal convergence.", "configspace": "", "generation": 38, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "1459935d-0a5b-47de-8bfe-69f52cb3cb6c", "metadata": {"aucs": [0.796727445346743, 0.7946092935536371, 0.8025157468331956, 0.7795890428061506, 0.7652866403837684, 0.7888980264133363, 0.7851539828124771, 0.7801166288092796, 0.778684214476141, 0.6066596860215113, 0.5509488918434079, 0.584709500115218, 0.6151880393982149, 0.6128285196462557, 0.5928554548480235, 0.18781852246903208, 0.6051102165828588, 0.6050716765741883, 0.12056849613284826, 0.10627786970516262, 0.11092740344636509, 0.12458142503345515, 0.10225070914012868, 0.10214739878052792, 0.11007052847089493, 0.11303443664409152, 0.11346829363668931, 0.09058514750802149, 0.09136224141572435, 0.09346304265481342, 0.08413688806219866, 0.10560994817344638, 0.09386944951658027, 0.1066829182603829, 0.09525027475772718, 0.09078080677184064, 0.8379701410373176, 0.8069154658397715, 0.8108437032002754, 0.8202640448333322, 0.7815128713795206, 0.8451084830274826, 0.836376388134602, 0.8441441978939892, 0.8509693104465095, 0.37150586919602946, 0.34273942956965175, 0.3313822553845982, 0.34968755932637086, 0.32960730205573996, 0.3754984208710611, 0.38284578850911233, 0.3840456442426208, 0.3280564413760412, 0.7576709220936438, 0.7335556841917643, 0.7658090217912067, 0.7934167558179147, 0.8025617081483369, 0.7999764269594076, 0.7299292035530152, 0.7697788108067551, 0.7651252822797626, 0.16167439560696928, 0.18696031846775318, 0.21158580997319565, 0.244349141547606, 0.2890559956537958, 0.16298570563996873, 0.21750653437837209, 0.21122767559906996, 0.22550289679700497, 0.15305768362463734, 0.14884944831507896, 0.18600965489753707, 0.1541357033150358, 0.19832744782165168, 0.18983314847500432, 0.1953210887165725, 0.20165646735698872, 0.13088573983753993, 0.489476775395826, 0.460206761694571, 0.280690722270316, 0.34064732845138956, 0.522953329166158, 0.46452239779932414, 0.4196789194960461, 0.4727221489210359, 0.44131563048738687, 0.6540713674624187, 0.6128130384068273, 0.5553131009305013, 0.5776200055109268, 0.4723536786910414, 0.6219680956281209, 0.6455804612075908, 0.6109257481083044, 0.5996992464126676, 0.15220127888973134, 0.11018658007737869, 0.2413715712685326, 0.2296484097510183, 0.2572737636354798, 0.10622686325653308, 0.14604068872930986, 0.14809138800719435, 0.12257392403735012, 0.36655256082417187, 0.34632185847096675, 0.3516395670957846, 0.33828319802853724, 0.3572041238804262, 0.35621925345698524, 0.34628041224843775, 0.36205674625584605, 0.3801826028652373, 0.7284525303733058, 0.7334889541026953, 0.7386557191632159, 0.7055754468414382, 0.7222856768071027, 0.7194774022217179, 0.518854950686749, 0.7321096452535195, 0.7397060954282213, 0.10374399773694243, 0.104282622260642, 0.1086381702881759, 0.11429613558107654, 0.11511930056195652, 0.10035386421161252, 0.09599830676572241, 0.11347281801819442, 0.11470225552281121, 0.14952999643648723, 0.16817216547016145, 0.17310734903563885, 0.1855986179207637, 0.18359480037739373, 0.19508916572226054, 0.20087729718879477, 0.17807185486405164, 0.20119801391244463, 0.3682121014468256, 0.35073539948175425, 0.3330439038672147, 0.4157077631363848, 0.37474021495439724, 0.36760475650845414, 0.38008256501576043, 0.39084781839078786, 0.3786757352451906, 0.30037946434366947, 0.2861866860206934, 0.2891587957817804, 0.2835627691001067, 0.2779602895144492, 0.28387893685888044, 0.29400381025955125, 0.3244615423116647, 0.30188351717337136, 0.20494558183242018, 0.19905376550038034, 0.20406514428030742, 0.2229753496453719, 0.1980660060283188, 0.21539123082194234, 0.21928087314384304, 0.20854328449838244, 0.2102975078755156, 0.18082128476991477, 0.1831737258940077, 0.18089956975930732, 0.18277323416104252, 0.1792695777259361, 0.17816092407226292, 0.19628636488975992, 0.19237612183491615, 0.1874718333986961, 0.6294666203187917, 0.602690555999032, 0.5454952462675013, 0.7727337342883757, 0.25677125313719296, 0.588947972014151, 0.5053651729784945, 0.42433758940449484, 0.610090527727432, 0.7148365734773846, 0.39316249107203016, 0.4609914040189693, 0.5414475146943718, 0.3738614312609654, 0.2972834077003401, 0.2719039283446757, 0.4464259994844204, 0.6865015215956998, 0.18541089636272012, 0.17521600614441235, 0.18498258988690697, 0.1923345960637889, 0.19442272937612437, 0.18424555156947642, 0.17449564646480575, 0.18372628686318526, 0.18823517758491137, 0.07953923390817585, 0.08429672806217459, 0.08351517483047155, 0.07872531348008505, 0.0885755282031725, 0.09071254814221597, 0.07022751225803625, 0.07907174920771143, 0.08022853716110878]}, "mutation_prompt": null}
{"id": "737c32c5-4db5-4ce4-9c03-a40654d75c76", "solution": "import numpy as np\n\nclass EnhancedProbabilisticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = 40\n        self.mutation_factor = 0.85\n        self.crossover_probability = 0.95\n        self.learning_rate = 0.2\n        self.diversity_factor = 0.1\n        self.population = None\n        self.fitness = None\n\n    def initialize_population(self):\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, \n                                            (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def adaptive_differential_evolution(self, idx, func):\n        indices = [i for i in range(self.population_size) if i != idx]\n        np.random.shuffle(indices)\n        a, b, c = indices[:3]\n        mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        adaptive_factor = 0.4 + 0.6 * np.random.rand()\n        mutant = self.population[idx] + adaptive_factor * (mutant - self.population[idx])\n        mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n        \n        cross_points = np.random.rand(self.dim) < self.crossover_probability\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        \n        trial = np.where(cross_points, mutant, self.population[idx])\n        trial_fitness = func(trial)\n        \n        if trial_fitness < self.fitness[idx]:\n            self.population[idx] = trial\n            self.fitness[idx] = trial_fitness\n\n    def stochastic_gradient_approx(self, idx, func):\n        gradient = np.random.normal(0, 1, self.dim)\n        adaptive_lr = self.learning_rate / (1 + idx / self.population_size)\n        candidate = self.population[idx] - adaptive_lr * gradient\n        candidate += self.diversity_factor * np.random.normal(0, 1, self.dim)\n        candidate = np.clip(candidate, self.lower_bound, self.upper_bound)\n        candidate_fitness = func(candidate)\n        if candidate_fitness < self.fitness[idx]:\n            self.population[idx] = candidate\n            self.fitness[idx] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population()\n        evaluations = 0\n        self.evaluate_population(func)\n        evaluations += self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                self.adaptive_differential_evolution(i, func)\n                evaluations += 1\n\n                if evaluations >= self.budget:\n                    break\n\n                self.stochastic_gradient_approx(i, func)\n                evaluations += 1\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "EnhancedProbabilisticOptimizer", "description": "Enhanced probabilistic hybrid optimizer balancing exploration and exploitation through diversity-oriented selection and adaptive parameter tuning for optimal convergence.", "configspace": "", "generation": 38, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "1459935d-0a5b-47de-8bfe-69f52cb3cb6c", "metadata": {"aucs": [0.796727445346743, 0.7946092935536371, 0.8025157468331956, 0.7795890428061506, 0.7652866403837684, 0.7888980264133363, 0.7851539828124771, 0.7801166288092796, 0.778684214476141, 0.6066596860215113, 0.5509488918434079, 0.584709500115218, 0.6151880393982149, 0.6128285196462557, 0.5928554548480235, 0.18781852246903208, 0.6051102165828588, 0.6050716765741883, 0.12056849613284826, 0.10627786970516262, 0.11092740344636509, 0.12458142503345515, 0.10225070914012868, 0.10214739878052792, 0.11007052847089493, 0.11303443664409152, 0.11346829363668931, 0.09058514750802149, 0.09136224141572435, 0.09346304265481342, 0.08413688806219866, 0.10560994817344638, 0.09386944951658027, 0.1066829182603829, 0.09525027475772718, 0.09078080677184064, 0.8379701410373176, 0.8069154658397715, 0.8108437032002754, 0.8202640448333322, 0.7815128713795206, 0.8451084830274826, 0.836376388134602, 0.8441441978939892, 0.8509693104465095, 0.37150586919602946, 0.34273942956965175, 0.3313822553845982, 0.34968755932637086, 0.32960730205573996, 0.3754984208710611, 0.38284578850911233, 0.3840456442426208, 0.3280564413760412, 0.7576709220936438, 0.7335556841917643, 0.7658090217912067, 0.7934167558179147, 0.8025617081483369, 0.7999764269594076, 0.7299292035530152, 0.7697788108067551, 0.7651252822797626, 0.16167439560696928, 0.18696031846775318, 0.21158580997319565, 0.244349141547606, 0.2890559956537958, 0.16298570563996873, 0.21750653437837209, 0.21122767559906996, 0.22550289679700497, 0.15305768362463734, 0.14884944831507896, 0.18600965489753707, 0.1541357033150358, 0.19832744782165168, 0.18983314847500432, 0.1953210887165725, 0.20165646735698872, 0.13088573983753993, 0.489476775395826, 0.460206761694571, 0.280690722270316, 0.34064732845138956, 0.522953329166158, 0.46452239779932414, 0.4196789194960461, 0.4727221489210359, 0.44131563048738687, 0.6540713674624187, 0.6128130384068273, 0.5553131009305013, 0.5776200055109268, 0.4723536786910414, 0.6219680956281209, 0.6455804612075908, 0.6109257481083044, 0.5996992464126676, 0.15220127888973134, 0.11018658007737869, 0.2413715712685326, 0.2296484097510183, 0.2572737636354798, 0.10622686325653308, 0.14604068872930986, 0.14809138800719435, 0.12257392403735012, 0.36655256082417187, 0.34632185847096675, 0.3516395670957846, 0.33828319802853724, 0.3572041238804262, 0.35621925345698524, 0.34628041224843775, 0.36205674625584605, 0.3801826028652373, 0.7284525303733058, 0.7334889541026953, 0.7386557191632159, 0.7055754468414382, 0.7222856768071027, 0.7194774022217179, 0.518854950686749, 0.7321096452535195, 0.7397060954282213, 0.10374399773694243, 0.104282622260642, 0.1086381702881759, 0.11429613558107654, 0.11511930056195652, 0.10035386421161252, 0.09599830676572241, 0.11347281801819442, 0.11470225552281121, 0.14952999643648723, 0.16817216547016145, 0.17310734903563885, 0.1855986179207637, 0.18359480037739373, 0.19508916572226054, 0.20087729718879477, 0.17807185486405164, 0.20119801391244463, 0.3682121014468256, 0.35073539948175425, 0.3330439038672147, 0.4157077631363848, 0.37474021495439724, 0.36760475650845414, 0.38008256501576043, 0.39084781839078786, 0.3786757352451906, 0.30037946434366947, 0.2861866860206934, 0.2891587957817804, 0.2835627691001067, 0.2779602895144492, 0.28387893685888044, 0.29400381025955125, 0.3244615423116647, 0.30188351717337136, 0.20494558183242018, 0.19905376550038034, 0.20406514428030742, 0.2229753496453719, 0.1980660060283188, 0.21539123082194234, 0.21928087314384304, 0.20854328449838244, 0.2102975078755156, 0.18082128476991477, 0.1831737258940077, 0.18089956975930732, 0.18277323416104252, 0.1792695777259361, 0.17816092407226292, 0.19628636488975992, 0.19237612183491615, 0.1874718333986961, 0.6294666203187917, 0.602690555999032, 0.5454952462675013, 0.7727337342883757, 0.25677125313719296, 0.588947972014151, 0.5053651729784945, 0.42433758940449484, 0.610090527727432, 0.7148365734773846, 0.39316249107203016, 0.4609914040189693, 0.5414475146943718, 0.3738614312609654, 0.2972834077003401, 0.2719039283446757, 0.4464259994844204, 0.6865015215956998, 0.18541089636272012, 0.17521600614441235, 0.18498258988690697, 0.1923345960637889, 0.19442272937612437, 0.18424555156947642, 0.17449564646480575, 0.18372628686318526, 0.18823517758491137, 0.07953923390817585, 0.08429672806217459, 0.08351517483047155, 0.07872531348008505, 0.0885755282031725, 0.09071254814221597, 0.07022751225803625, 0.07907174920771143, 0.08022853716110878]}, "mutation_prompt": null}
{"id": "2858394b-b2b3-48b3-8335-b768eae2a640", "solution": "import numpy as np\n\nclass EnhancedProbabilisticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = 40\n        self.mutation_factor = 0.85\n        self.crossover_probability = 0.95\n        self.learning_rate = 0.2\n        self.diversity_factor = 0.1\n        self.population = None\n        self.fitness = None\n\n    def initialize_population(self):\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, \n                                            (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def adaptive_differential_evolution(self, idx, func):\n        indices = [i for i in range(self.population_size) if i != idx]\n        np.random.shuffle(indices)\n        a, b, c = indices[:3]\n        mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        adaptive_factor = 0.4 + 0.6 * np.random.rand()\n        mutant = self.population[idx] + adaptive_factor * (mutant - self.population[idx])\n        mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n        \n        cross_points = np.random.rand(self.dim) < self.crossover_probability\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        \n        trial = np.where(cross_points, mutant, self.population[idx])\n        trial_fitness = func(trial)\n        \n        if trial_fitness < self.fitness[idx]:\n            self.population[idx] = trial\n            self.fitness[idx] = trial_fitness\n\n    def stochastic_gradient_approx(self, idx, func):\n        gradient = np.random.normal(0, 1, self.dim)\n        adaptive_lr = self.learning_rate / (1 + idx / self.population_size)\n        candidate = self.population[idx] - adaptive_lr * gradient\n        candidate += self.diversity_factor * np.random.normal(0, 1, self.dim)\n        candidate = np.clip(candidate, self.lower_bound, self.upper_bound)\n        candidate_fitness = func(candidate)\n        if candidate_fitness < self.fitness[idx]:\n            self.population[idx] = candidate\n            self.fitness[idx] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population()\n        evaluations = 0\n        self.evaluate_population(func)\n        evaluations += self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                self.adaptive_differential_evolution(i, func)\n                evaluations += 1\n\n                if evaluations >= self.budget:\n                    break\n\n                self.stochastic_gradient_approx(i, func)\n                evaluations += 1\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "EnhancedProbabilisticOptimizer", "description": "Enhanced probabilistic hybrid optimizer balancing exploration and exploitation through diversity-oriented selection and adaptive parameter tuning for optimal convergence.", "configspace": "", "generation": 38, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "1459935d-0a5b-47de-8bfe-69f52cb3cb6c", "metadata": {"aucs": [0.796727445346743, 0.7946092935536371, 0.8025157468331956, 0.7795890428061506, 0.7652866403837684, 0.7888980264133363, 0.7851539828124771, 0.7801166288092796, 0.778684214476141, 0.6066596860215113, 0.5509488918434079, 0.584709500115218, 0.6151880393982149, 0.6128285196462557, 0.5928554548480235, 0.18781852246903208, 0.6051102165828588, 0.6050716765741883, 0.12056849613284826, 0.10627786970516262, 0.11092740344636509, 0.12458142503345515, 0.10225070914012868, 0.10214739878052792, 0.11007052847089493, 0.11303443664409152, 0.11346829363668931, 0.09058514750802149, 0.09136224141572435, 0.09346304265481342, 0.08413688806219866, 0.10560994817344638, 0.09386944951658027, 0.1066829182603829, 0.09525027475772718, 0.09078080677184064, 0.8379701410373176, 0.8069154658397715, 0.8108437032002754, 0.8202640448333322, 0.7815128713795206, 0.8451084830274826, 0.836376388134602, 0.8441441978939892, 0.8509693104465095, 0.37150586919602946, 0.34273942956965175, 0.3313822553845982, 0.34968755932637086, 0.32960730205573996, 0.3754984208710611, 0.38284578850911233, 0.3840456442426208, 0.3280564413760412, 0.7576709220936438, 0.7335556841917643, 0.7658090217912067, 0.7934167558179147, 0.8025617081483369, 0.7999764269594076, 0.7299292035530152, 0.7697788108067551, 0.7651252822797626, 0.16167439560696928, 0.18696031846775318, 0.21158580997319565, 0.244349141547606, 0.2890559956537958, 0.16298570563996873, 0.21750653437837209, 0.21122767559906996, 0.22550289679700497, 0.15305768362463734, 0.14884944831507896, 0.18600965489753707, 0.1541357033150358, 0.19832744782165168, 0.18983314847500432, 0.1953210887165725, 0.20165646735698872, 0.13088573983753993, 0.489476775395826, 0.460206761694571, 0.280690722270316, 0.34064732845138956, 0.522953329166158, 0.46452239779932414, 0.4196789194960461, 0.4727221489210359, 0.44131563048738687, 0.6540713674624187, 0.6128130384068273, 0.5553131009305013, 0.5776200055109268, 0.4723536786910414, 0.6219680956281209, 0.6455804612075908, 0.6109257481083044, 0.5996992464126676, 0.15220127888973134, 0.11018658007737869, 0.2413715712685326, 0.2296484097510183, 0.2572737636354798, 0.10622686325653308, 0.14604068872930986, 0.14809138800719435, 0.12257392403735012, 0.36655256082417187, 0.34632185847096675, 0.3516395670957846, 0.33828319802853724, 0.3572041238804262, 0.35621925345698524, 0.34628041224843775, 0.36205674625584605, 0.3801826028652373, 0.7284525303733058, 0.7334889541026953, 0.7386557191632159, 0.7055754468414382, 0.7222856768071027, 0.7194774022217179, 0.518854950686749, 0.7321096452535195, 0.7397060954282213, 0.10374399773694243, 0.104282622260642, 0.1086381702881759, 0.11429613558107654, 0.11511930056195652, 0.10035386421161252, 0.09599830676572241, 0.11347281801819442, 0.11470225552281121, 0.14952999643648723, 0.16817216547016145, 0.17310734903563885, 0.1855986179207637, 0.18359480037739373, 0.19508916572226054, 0.20087729718879477, 0.17807185486405164, 0.20119801391244463, 0.3682121014468256, 0.35073539948175425, 0.3330439038672147, 0.4157077631363848, 0.37474021495439724, 0.36760475650845414, 0.38008256501576043, 0.39084781839078786, 0.3786757352451906, 0.30037946434366947, 0.2861866860206934, 0.2891587957817804, 0.2835627691001067, 0.2779602895144492, 0.28387893685888044, 0.29400381025955125, 0.3244615423116647, 0.30188351717337136, 0.20494558183242018, 0.19905376550038034, 0.20406514428030742, 0.2229753496453719, 0.1980660060283188, 0.21539123082194234, 0.21928087314384304, 0.20854328449838244, 0.2102975078755156, 0.18082128476991477, 0.1831737258940077, 0.18089956975930732, 0.18277323416104252, 0.1792695777259361, 0.17816092407226292, 0.19628636488975992, 0.19237612183491615, 0.1874718333986961, 0.6294666203187917, 0.602690555999032, 0.5454952462675013, 0.7727337342883757, 0.25677125313719296, 0.588947972014151, 0.5053651729784945, 0.42433758940449484, 0.610090527727432, 0.7148365734773846, 0.39316249107203016, 0.4609914040189693, 0.5414475146943718, 0.3738614312609654, 0.2972834077003401, 0.2719039283446757, 0.4464259994844204, 0.6865015215956998, 0.18541089636272012, 0.17521600614441235, 0.18498258988690697, 0.1923345960637889, 0.19442272937612437, 0.18424555156947642, 0.17449564646480575, 0.18372628686318526, 0.18823517758491137, 0.07953923390817585, 0.08429672806217459, 0.08351517483047155, 0.07872531348008505, 0.0885755282031725, 0.09071254814221597, 0.07022751225803625, 0.07907174920771143, 0.08022853716110878]}, "mutation_prompt": null}
{"id": "d7843aa1-d2b2-4e0c-959d-0ac55c4ccc31", "solution": "import numpy as np\n\nclass EnhancedProbabilisticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = 40\n        self.mutation_factor = 0.85\n        self.crossover_probability = 0.95\n        self.learning_rate = 0.2\n        self.diversity_factor = 0.1\n        self.population = None\n        self.fitness = None\n\n    def initialize_population(self):\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, \n                                            (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def adaptive_differential_evolution(self, idx, func):\n        indices = [i for i in range(self.population_size) if i != idx]\n        np.random.shuffle(indices)\n        a, b, c = indices[:3]\n        mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        adaptive_factor = 0.4 + 0.6 * np.random.rand()\n        mutant = self.population[idx] + adaptive_factor * (mutant - self.population[idx])\n        mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n        \n        cross_points = np.random.rand(self.dim) < self.crossover_probability\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        \n        trial = np.where(cross_points, mutant, self.population[idx])\n        trial_fitness = func(trial)\n        \n        if trial_fitness < self.fitness[idx]:\n            self.population[idx] = trial\n            self.fitness[idx] = trial_fitness\n\n    def stochastic_gradient_approx(self, idx, func):\n        gradient = np.random.normal(0, 1, self.dim)\n        adaptive_lr = self.learning_rate / (1 + idx / self.population_size)\n        candidate = self.population[idx] - adaptive_lr * gradient\n        candidate += self.diversity_factor * np.random.normal(0, 1, self.dim)\n        candidate = np.clip(candidate, self.lower_bound, self.upper_bound)\n        candidate_fitness = func(candidate)\n        if candidate_fitness < self.fitness[idx]:\n            self.population[idx] = candidate\n            self.fitness[idx] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population()\n        evaluations = 0\n        self.evaluate_population(func)\n        evaluations += self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                self.adaptive_differential_evolution(i, func)\n                evaluations += 1\n\n                if evaluations >= self.budget:\n                    break\n\n                self.stochastic_gradient_approx(i, func)\n                evaluations += 1\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "EnhancedProbabilisticOptimizer", "description": "Enhanced probabilistic hybrid optimizer balancing exploration and exploitation through diversity-oriented selection and adaptive parameter tuning for optimal convergence.", "configspace": "", "generation": 38, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "1459935d-0a5b-47de-8bfe-69f52cb3cb6c", "metadata": {"aucs": [0.796727445346743, 0.7946092935536371, 0.8025157468331956, 0.7795890428061506, 0.7652866403837684, 0.7888980264133363, 0.7851539828124771, 0.7801166288092796, 0.778684214476141, 0.6066596860215113, 0.5509488918434079, 0.584709500115218, 0.6151880393982149, 0.6128285196462557, 0.5928554548480235, 0.18781852246903208, 0.6051102165828588, 0.6050716765741883, 0.12056849613284826, 0.10627786970516262, 0.11092740344636509, 0.12458142503345515, 0.10225070914012868, 0.10214739878052792, 0.11007052847089493, 0.11303443664409152, 0.11346829363668931, 0.09058514750802149, 0.09136224141572435, 0.09346304265481342, 0.08413688806219866, 0.10560994817344638, 0.09386944951658027, 0.1066829182603829, 0.09525027475772718, 0.09078080677184064, 0.8379701410373176, 0.8069154658397715, 0.8108437032002754, 0.8202640448333322, 0.7815128713795206, 0.8451084830274826, 0.836376388134602, 0.8441441978939892, 0.8509693104465095, 0.37150586919602946, 0.34273942956965175, 0.3313822553845982, 0.34968755932637086, 0.32960730205573996, 0.3754984208710611, 0.38284578850911233, 0.3840456442426208, 0.3280564413760412, 0.7576709220936438, 0.7335556841917643, 0.7658090217912067, 0.7934167558179147, 0.8025617081483369, 0.7999764269594076, 0.7299292035530152, 0.7697788108067551, 0.7651252822797626, 0.16167439560696928, 0.18696031846775318, 0.21158580997319565, 0.244349141547606, 0.2890559956537958, 0.16298570563996873, 0.21750653437837209, 0.21122767559906996, 0.22550289679700497, 0.15305768362463734, 0.14884944831507896, 0.18600965489753707, 0.1541357033150358, 0.19832744782165168, 0.18983314847500432, 0.1953210887165725, 0.20165646735698872, 0.13088573983753993, 0.489476775395826, 0.460206761694571, 0.280690722270316, 0.34064732845138956, 0.522953329166158, 0.46452239779932414, 0.4196789194960461, 0.4727221489210359, 0.44131563048738687, 0.6540713674624187, 0.6128130384068273, 0.5553131009305013, 0.5776200055109268, 0.4723536786910414, 0.6219680956281209, 0.6455804612075908, 0.6109257481083044, 0.5996992464126676, 0.15220127888973134, 0.11018658007737869, 0.2413715712685326, 0.2296484097510183, 0.2572737636354798, 0.10622686325653308, 0.14604068872930986, 0.14809138800719435, 0.12257392403735012, 0.36655256082417187, 0.34632185847096675, 0.3516395670957846, 0.33828319802853724, 0.3572041238804262, 0.35621925345698524, 0.34628041224843775, 0.36205674625584605, 0.3801826028652373, 0.7284525303733058, 0.7334889541026953, 0.7386557191632159, 0.7055754468414382, 0.7222856768071027, 0.7194774022217179, 0.518854950686749, 0.7321096452535195, 0.7397060954282213, 0.10374399773694243, 0.104282622260642, 0.1086381702881759, 0.11429613558107654, 0.11511930056195652, 0.10035386421161252, 0.09599830676572241, 0.11347281801819442, 0.11470225552281121, 0.14952999643648723, 0.16817216547016145, 0.17310734903563885, 0.1855986179207637, 0.18359480037739373, 0.19508916572226054, 0.20087729718879477, 0.17807185486405164, 0.20119801391244463, 0.3682121014468256, 0.35073539948175425, 0.3330439038672147, 0.4157077631363848, 0.37474021495439724, 0.36760475650845414, 0.38008256501576043, 0.39084781839078786, 0.3786757352451906, 0.30037946434366947, 0.2861866860206934, 0.2891587957817804, 0.2835627691001067, 0.2779602895144492, 0.28387893685888044, 0.29400381025955125, 0.3244615423116647, 0.30188351717337136, 0.20494558183242018, 0.19905376550038034, 0.20406514428030742, 0.2229753496453719, 0.1980660060283188, 0.21539123082194234, 0.21928087314384304, 0.20854328449838244, 0.2102975078755156, 0.18082128476991477, 0.1831737258940077, 0.18089956975930732, 0.18277323416104252, 0.1792695777259361, 0.17816092407226292, 0.19628636488975992, 0.19237612183491615, 0.1874718333986961, 0.6294666203187917, 0.602690555999032, 0.5454952462675013, 0.7727337342883757, 0.25677125313719296, 0.588947972014151, 0.5053651729784945, 0.42433758940449484, 0.610090527727432, 0.7148365734773846, 0.39316249107203016, 0.4609914040189693, 0.5414475146943718, 0.3738614312609654, 0.2972834077003401, 0.2719039283446757, 0.4464259994844204, 0.6865015215956998, 0.18541089636272012, 0.17521600614441235, 0.18498258988690697, 0.1923345960637889, 0.19442272937612437, 0.18424555156947642, 0.17449564646480575, 0.18372628686318526, 0.18823517758491137, 0.07953923390817585, 0.08429672806217459, 0.08351517483047155, 0.07872531348008505, 0.0885755282031725, 0.09071254814221597, 0.07022751225803625, 0.07907174920771143, 0.08022853716110878]}, "mutation_prompt": null}
{"id": "f5b9ddfb-7821-4f4f-9b15-92d5cd9fa8f7", "solution": "import numpy as np\n\nclass SynergisticCoevolutionaryLearner:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = 40\n        self.mutation_factor = 0.8\n        self.crossover_probability = 0.9\n        self.learning_rate = 0.25\n        self.diversity_factor = 0.15\n        self.population = None\n        self.fitness = None\n\n    def initialize_population(self):\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, \n                                            (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def coevolutionary_differential_evolution(self, idx, func):\n        indices = [i for i in range(self.population_size) if i != idx]\n        np.random.shuffle(indices)\n        a, b, c = indices[:3]\n        mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        adaptive_factor = 0.3 + 0.7 * np.random.rand()\n        mutant = self.population[idx] + adaptive_factor * (mutant - self.population[idx])\n        mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n        \n        cross_points = np.random.rand(self.dim) < self.crossover_probability\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        \n        trial = np.where(cross_points, mutant, self.population[idx])\n        trial_fitness = func(trial)\n        \n        if trial_fitness < self.fitness[idx]:\n            self.population[idx] = trial\n            self.fitness[idx] = trial_fitness\n\n    def adaptive_gradient_search(self, idx, func):\n        gradient = np.random.normal(0, 1, self.dim)\n        adaptive_lr = self.learning_rate / (1 + idx / (2 * self.population_size))\n        candidate = self.population[idx] - adaptive_lr * gradient\n        candidate += self.diversity_factor * np.random.normal(0, 1, self.dim)\n        candidate = np.clip(candidate, self.lower_bound, self.upper_bound)\n        candidate_fitness = func(candidate)\n        if candidate_fitness < self.fitness[idx]:\n            self.population[idx] = candidate\n            self.fitness[idx] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population()\n        evaluations = 0\n        self.evaluate_population(func)\n        evaluations += self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                self.coevolutionary_differential_evolution(i, func)\n                evaluations += 1\n\n                if evaluations >= self.budget:\n                    break\n\n                self.adaptive_gradient_search(i, func)\n                evaluations += 1\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "SynergisticCoevolutionaryLearner", "description": "Synergistic Coevolutionary Learner combines coevolution and adaptive learning to enhance exploration and convergence.", "configspace": "", "generation": 55, "fitness": 0.33982394871671356, "feedback": "The algorithm SynergisticCoevolutionaryLearner got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.34 with standard deviation 0.24.", "error": "", "parent_id": "1459935d-0a5b-47de-8bfe-69f52cb3cb6c", "metadata": {"aucs": [0.819844647346499, 0.8208889197755189, 0.8042380598891747, 0.7978242182733839, 0.7624195138877392, 0.7987354424532687, 0.7335229472778718, 0.804127104269911, 0.7930856788523938, 0.6368245915500229, 0.5104009398043584, 0.5856435017105621, 0.5887183247877619, 0.6394183219145106, 0.5985529814409165, 0.6004870936240208, 0.5737175980941738, 0.5341233143559936, 0.1116425231460938, 0.12293336894661588, 0.09855654516774759, 0.10960389878158094, 0.10756171111950386, 0.10663820681109204, 0.11057702864875008, 0.1379074272135984, 0.11302136989246558, 0.09519267597357406, 0.09949974970385689, 0.09462930410125647, 0.09238168214579967, 0.08467206605115463, 0.08761001786153133, 0.10117042777309215, 0.082125588292146, 0.10342710784790798, 0.863055528421748, 0.8571865822311194, 0.8492743791136794, 0.8545325628365839, 0.7708370824205457, 0.8410184705442277, 0.8468213821543811, 0.8162998282191791, 0.8581183890571572, 0.3321643563107113, 0.3054551607607696, 0.3411200414914173, 0.2958516251029396, 0.28256860951279283, 0.3053814365586811, 0.3247376090984925, 0.3143665192511669, 0.29308632148486524, 0.7318719203892912, 0.7499852536471325, 0.7446248194841492, 0.8213535067097874, 0.7745446567758214, 0.7526965493169753, 0.6854064691547941, 0.7346971919489582, 0.750568399634282, 0.17747125092507254, 0.13443386878852048, 0.15079058826222025, 0.23723868568487283, 0.16920139598802453, 0.15442092413923203, 0.1914533928207688, 0.37622979811226065, 0.22774759207529938, 0.16981556083109006, 0.15490963953568337, 0.19268738220782733, 0.17171248915259363, 0.1521597091064184, 0.1804797149209305, 0.18052377013125642, 0.2015565549134809, 0.18885323336855497, 0.2312717758519478, 0.20683203508798798, 0.23461749325678738, 0.3917400932603794, 0.41885860481629633, 0.0821830783606069, 0.16926224536768442, 0.30470879261497885, 0.26397735605179196, 0.570568144418406, 0.4997691433720306, 0.4886434894908611, 0.1606669870447236, 0.3166654187164276, 0.3207349926515076, 0.5931331824514592, 0.5237653563412936, 0.5173283518749912, 0.11394502787095662, 0.1744968282460433, 0.16529659522741602, 0.2441999541688984, 0.12447009919367125, 0.17333566803574418, 0.1376252306722605, 0.13848231591236904, 0.1592357607806052, 0.29899552174791155, 0.315857935593183, 0.33981904878600955, 0.3165749003450997, 0.28640570968766177, 0.207980522665563, 0.2800386923436434, 0.3280276471250628, 0.2347014812457906, 0.7210052585937972, 0.6400896722624301, 0.5850436981709088, 0.6740949670133528, 0.708955370938321, 0.6922464067534966, 0.727989968054831, 0.7180448730621392, 0.7131186085242476, 0.0891116334885722, 0.10341212860599003, 0.11429901367247386, 0.09801188611131528, 0.09540030784108589, 0.09111190439302574, 0.09992728194929457, 0.12057455961688357, 0.10210831003460297, 0.18137835526103385, 0.17027764386733868, 0.15436512479298836, 0.17644599978545594, 0.1815557355453764, 0.18481531945969742, 0.1730448221019083, 0.19574867603967094, 0.1844383215343739, 0.3582045244404092, 0.3650691068404107, 0.3789687719369118, 0.3629813574531421, 0.4038166462293903, 0.3709032455251995, 0.36710298232261374, 0.42353413052711, 0.3628831621270039, 0.21597262628686065, 0.2734334704248872, 0.2660130972335646, 0.3002417946804209, 0.2847658962968439, 0.28086880271686854, 0.279170038901819, 0.2988026033788076, 0.24378545949379737, 0.2059184589186046, 0.21122078658671162, 0.20112413080195624, 0.20852637227236626, 0.20050754827330786, 0.23118293950602642, 0.22000659968220326, 0.2148173323982806, 0.20875885654843374, 0.18368879482636824, 0.21030119870274466, 0.18100822628767987, 0.18606938237888393, 0.19438883741539958, 0.17664476975407617, 0.1846899834402843, 0.179490779209736, 0.1858161691976702, 0.6912082183153991, 0.7270160455462273, 0.27039040789475755, 0.810282874290447, 0.35283687728305546, 0.43223947412225006, 0.18099783152264137, 0.39241930911628065, 0.44940883768746065, 0.6897650405540658, 0.3286421336059563, 0.39077039263246227, 0.3954552173247845, 0.20159824987685038, 0.3120756489585531, 0.3199964981436112, 0.21759899989027975, 0.556002336604638, 0.18902485513624334, 0.1799864038624599, 0.17858363771544805, 0.1775205283816158, 0.19274193335645073, 0.19759160267733045, 0.19147396764557778, 0.188478240821787, 0.18612119203879363, 0.07969078940930574, 0.09386751586733466, 0.08625165041633431, 0.08754787221213955, 0.0948355467111629, 0.09812333485078528, 0.07532159429215057, 0.08235432649399155, 0.08120922500044236]}, "mutation_prompt": null}
{"id": "1a68f860-d69f-48fa-929b-6079c9a03563", "solution": "import numpy as np\n\nclass DifferentialGradientBasedProbabilisticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = 40\n        self.mutation_factor = 0.9  # Changed from 0.85 to improve diversity\n        self.crossover_probability = 0.9  # Slightly reduced to increase variety\n        self.learning_rate = 0.25  # Increased learning rate for more aggressive updates\n        self.diversity_factor = 0.15  # Increased diversity factor\n        self.convergence_factor = 0.5  # Added for gradual focusing\n        self.population = None\n        self.fitness = None\n\n    def initialize_population(self):\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, \n                                            (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def adaptive_differential_evolution(self, idx, func):\n        indices = [i for i in range(self.population_size) if i != idx]\n        np.random.shuffle(indices)\n        a, b, c = indices[:3]\n        mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        adaptive_factor = 0.5 + 0.5 * np.random.rand()  # Adjusted adaptive factor range\n        mutant = self.population[idx] + adaptive_factor * (mutant - self.population[idx])\n        mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n        \n        cross_points = np.random.rand(self.dim) < self.crossover_probability\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        \n        trial = np.where(cross_points, mutant, self.population[idx])\n        trial_fitness = func(trial)\n        \n        if trial_fitness < self.fitness[idx]:\n            self.population[idx] = trial\n            self.fitness[idx] = trial_fitness\n\n    def stochastic_gradient_approx(self, idx, func):\n        gradient = np.random.normal(0, 1, self.dim)\n        adaptive_lr = self.learning_rate / (1 + idx / self.population_size)\n        candidate = self.population[idx] - adaptive_lr * gradient\n        candidate += self.diversity_factor * np.random.normal(0, 1, self.dim)\n        candidate = np.clip(candidate, self.lower_bound, self.upper_bound)\n        candidate_fitness = func(candidate)\n        if candidate_fitness < self.fitness[idx]:\n            self.population[idx] = candidate\n            self.fitness[idx] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population()\n        evaluations = 0\n        self.evaluate_population(func)\n        evaluations += self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                self.adaptive_differential_evolution(i, func)\n                evaluations += 1\n\n                if evaluations >= self.budget:\n                    break\n\n                self.stochastic_gradient_approx(i, func)\n                evaluations += 1\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "DifferentialGradientBasedProbabilisticOptimizer", "description": "Differential Gradient-Based Probabilistic Optimizer combines adaptive differential evolution and gradient-driven perturbations for balanced exploration and exploitation.", "configspace": "", "generation": 56, "fitness": 0.3220706657984946, "feedback": "The algorithm DifferentialGradientBasedProbabilisticOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.32 with standard deviation 0.22.", "error": "", "parent_id": "1459935d-0a5b-47de-8bfe-69f52cb3cb6c", "metadata": {"aucs": [0.7514999064587259, 0.7520659639110268, 0.7543815687653668, 0.735930421154416, 0.7504424771536802, 0.7529200593440641, 0.7376652684962081, 0.7425526330911035, 0.7525836321218256, 0.5262310144534015, 0.5104868153514546, 0.4879679168992008, 0.5195132787622825, 0.47732811915602746, 0.5037086117838052, 0.47046387760149233, 0.5100278214105555, 0.4690732807108904, 0.1192433013966483, 0.13207387167587736, 0.09872978206575667, 0.1006668719672994, 0.10231143658516317, 0.10911718025782269, 0.09633325482805555, 0.11750219817811014, 0.1016368700931709, 0.08821807343384758, 0.1002842071268818, 0.0906563364646521, 0.09599458294125784, 0.09848642108545713, 0.09926470863349224, 0.09247488486549305, 0.08507385432331172, 0.08956344373710123, 0.9009098329899612, 0.9089451432837676, 0.8538117716013819, 0.8836526338458139, 0.8750920850504892, 0.8748719911688876, 0.8541502295076908, 0.8480076802757895, 0.9080841072179324, 0.3315718653136047, 0.32093647649604873, 0.3192741084292011, 0.37486613241048905, 0.3452254744621682, 0.3346319884759241, 0.3512301976370389, 0.3621616832062122, 0.35049242206960973, 0.6901857134745861, 0.6839760842732971, 0.5890590727387925, 0.740417394049371, 0.680373049709839, 0.7540303553864375, 0.697029271781356, 0.7326994461223575, 0.6901545318518361, 0.23162097033023765, 0.17479596817101795, 0.18990665925877903, 0.20575276625888794, 0.25123392046097015, 0.21950928468975772, 0.16587930779361781, 0.17646261209640435, 0.304507586070098, 0.17761047379466455, 0.20229681730636617, 0.15667352912337773, 0.19489679716382224, 0.2066793426537834, 0.16735160492524048, 0.1792956088122395, 0.18972376502270005, 0.14894521783295278, 0.2678182919305673, 0.2582834569337974, 0.25963354736067745, 0.2981777699573134, 0.24900263824903246, 0.23265823739303182, 0.2771514204510006, 0.2659076883779451, 0.28911399460933285, 0.46027608840967704, 0.4227453738617133, 0.42238627330828327, 0.39276627780945006, 0.3616228374470818, 0.4074344887582114, 0.4510847063487742, 0.4599284151221593, 0.4397376584710806, 0.11672528038781671, 0.09037267112075875, 0.07225423346532678, 0.16921713114820502, 0.13512995662462923, 0.09642273614186636, 0.10323869607782943, 0.1054452639358493, 0.0718812745591445, 0.2459292345958335, 0.25204626254959317, 0.2446302234483564, 0.24161182376150703, 0.24570579942206583, 0.22977366135883093, 0.24960900939126207, 0.23780729708992832, 0.26881525522338934, 0.6048165566321744, 0.6414604030775667, 0.6175637022732159, 0.5894014951876723, 0.5799751870583445, 0.5847273321537219, 0.6115561282354736, 0.6261660768111246, 0.6122236644439127, 0.10632898691856318, 0.10250436812402663, 0.10871763790527111, 0.09952820418651809, 0.09409946577252104, 0.10055047012751273, 0.10102101140933295, 0.10623878899510508, 0.10279446803361136, 0.1873660136243971, 0.19075387333672944, 0.19449888093870593, 0.19093770092268003, 0.20715844401056005, 0.17057832477611357, 0.17742759050747847, 0.1861860172804748, 0.17587870364193603, 0.3198976231702104, 0.31243755343350754, 0.31103467456936273, 0.3313534670423235, 0.33335385524822925, 0.3279322338802142, 0.3242870341670284, 0.3593292081625785, 0.3126965669327877, 0.23644513078071583, 0.24094782766539347, 0.2627268882598791, 0.2761645054585712, 0.2247314252148439, 0.22912727876436678, 0.2788758104210669, 0.2663518529748964, 0.23616131515387928, 0.21682255811372864, 0.20080635227630206, 0.19707080146241052, 0.23540821452194605, 0.1978395874899732, 0.1997623061999534, 0.20618802386371404, 0.21387006606476355, 0.21303237864343771, 0.19011537084506303, 0.1978703682412123, 0.19735525839409562, 0.19324233459863593, 0.19040057892239948, 0.19572521788466235, 0.19341548491033567, 0.19224199333020053, 0.1960004228721579, 0.6321010867310404, 0.28364490858106073, 0.5078832080176265, 0.6531371991994659, 0.42983681047501165, 0.42163312184955004, 0.39009904993076505, 0.17901510435931434, 0.2638580552248101, 0.6695962617060411, 0.387500928915957, 0.41583298192155027, 0.2537089933521277, 0.3046203402298008, 0.39105544587413554, 0.2900602630831123, 0.3839870634993181, 0.20113442939780501, 0.2009486362344295, 0.17688241476048694, 0.19823540423768338, 0.18733609166842458, 0.18881605978316962, 0.17334834551211753, 0.1817172797541977, 0.1931410521313629, 0.19440147817472153, 0.07572920329681687, 0.08290377392627635, 0.0779537725689966, 0.08406169604245717, 0.08724933225386922, 0.07385802980382361, 0.07569697245714047, 0.08193319299317503, 0.07906286222656034]}, "mutation_prompt": null}
{"id": "2a6113ae-27a4-4028-b998-80e97f6b39b7", "solution": "import numpy as np\n\nclass MemeticDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = 50  # Adjusted population size for increased diversity\n        self.mutation_factor = 0.9  # Adjusted mutation factor for better exploration\n        self.crossover_probability = 0.9  # Tweaked crossover probability for balance\n        self.learning_rate = 0.15  # Slightly reduced learning rate for finer local search\n        self.diversity_factor = 0.15  # Enhanced diversity factor for broader search space coverage\n        self.population = None\n        self.fitness = None\n\n    def initialize_population(self):\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, \n                                            (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def adaptive_differential_evolution(self, idx, func):\n        indices = [i for i in range(self.population_size) if i != idx]\n        np.random.shuffle(indices)\n        a, b, c = indices[:3]\n        mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        adaptive_factor = 0.4 + 0.5 * np.random.rand()  # Adjusted adaptive factor range\n        mutant = self.population[idx] + adaptive_factor * (mutant - self.population[idx])\n        mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n\n        cross_points = np.random.rand(self.dim) < self.crossover_probability\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n\n        trial = np.where(cross_points, mutant, self.population[idx])\n        trial_fitness = func(trial)\n\n        if trial_fitness < self.fitness[idx]:\n            self.population[idx] = trial\n            self.fitness[idx] = trial_fitness\n\n    def stochastic_gradient_approx(self, idx, func):\n        gradient = np.random.normal(0, 1, self.dim)\n        adaptive_lr = self.learning_rate / (1 + idx / self.population_size)\n        candidate = self.population[idx] - adaptive_lr * gradient\n        candidate += self.diversity_factor * np.random.normal(0, 1, self.dim)\n        candidate = np.clip(candidate, self.lower_bound, self.upper_bound)\n        candidate_fitness = func(candidate)\n        if candidate_fitness < self.fitness[idx]:\n            self.population[idx] = candidate\n            self.fitness[idx] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population()\n        evaluations = 0\n        self.evaluate_population(func)\n        evaluations += self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                self.adaptive_differential_evolution(i, func)\n                evaluations += 1\n\n                if evaluations >= self.budget:\n                    break\n\n                self.stochastic_gradient_approx(i, func)\n                evaluations += 1\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "MemeticDifferentialEvolution", "description": "Adaptive Memetic Differential Evolution with Stochastic Perturbations for Enhanced Convergence and Diversity", "configspace": "", "generation": 57, "fitness": 0.31042531603633006, "feedback": "The algorithm MemeticDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.31 with standard deviation 0.21.", "error": "", "parent_id": "1459935d-0a5b-47de-8bfe-69f52cb3cb6c", "metadata": {"aucs": [0.7619807445892499, 0.7267508716843445, 0.7588310315193959, 0.7204859152277875, 0.7126309883338293, 0.7319986198538541, 0.7275458323228909, 0.7256769873805569, 0.7355004742535863, 0.48116798984760856, 0.4967899445560662, 0.49848483846591973, 0.48249591075753084, 0.5167096617711903, 0.4817101461784671, 0.4764627221419573, 0.4930791193999007, 0.5180108837023512, 0.10613658744642929, 0.10698229046621488, 0.10450067450229494, 0.10850433547815752, 0.10111239173930209, 0.11159713408923255, 0.10402875594478334, 0.10537828227421908, 0.11692522725546128, 0.0880049795967991, 0.09038483094068883, 0.09568292309580628, 0.08946098866376395, 0.09222000676387421, 0.09454985290068652, 0.09875338944315881, 0.08734939576910017, 0.0932660453220382, 0.8154821624712568, 0.8090896957840017, 0.831476502651579, 0.8070207612761312, 0.7835024326838875, 0.7739723847832496, 0.7835909033640914, 0.7768648441200766, 0.7762370753156697, 0.310728614577088, 0.31541808001649896, 0.3387490865119144, 0.29391313200039626, 0.3268142295297841, 0.3283042349771106, 0.3106683192937949, 0.3118412427589765, 0.29438456045557637, 0.7117049022904613, 0.697956308160498, 0.6686575235028615, 0.6623838377724278, 0.7646222444693911, 0.7117689782124723, 0.6433290255829645, 0.6698262422318219, 0.7264362661203551, 0.2097451990896796, 0.1531840368992331, 0.2151014561493857, 0.2122249878076149, 0.18109291176992515, 0.15052625644449402, 0.20039324745893305, 0.17140351473283388, 0.18579691952151312, 0.16188663934126257, 0.16805917182745633, 0.1627949849775261, 0.1608844087513045, 0.15513276477067994, 0.15434391952890747, 0.16856234111710733, 0.1528440295281085, 0.1915447535629945, 0.2030798938111703, 0.23086722219165756, 0.27554909650294157, 0.23828529124352948, 0.234966957123389, 0.24818485692255832, 0.29740552025379074, 0.2381964819781609, 0.2915816606249588, 0.41681229898721395, 0.4044206349463292, 0.4296587319231283, 0.35049298050310906, 0.3836384961740059, 0.332043660751153, 0.4122366271297676, 0.394694583824887, 0.44344275061472693, 0.10407209829676645, 0.10632110011716955, 0.0806461560019025, 0.10505098224914478, 0.07956074998587881, 0.16381526375106747, 0.12869971299756866, 0.13114761771386818, 0.10676716670577313, 0.2242062957562111, 0.233144696359866, 0.23111870996229922, 0.2266011666227512, 0.24022229510848958, 0.2638745917699149, 0.2239700358727812, 0.2305604613443435, 0.24202761745576007, 0.631113661115622, 0.6094455529372631, 0.6503677855088944, 0.5957267523282478, 0.6063163261851279, 0.6002524122213544, 0.6068561964244406, 0.6335995400996703, 0.6246912184256566, 0.12134475681625567, 0.11915161156734633, 0.10915221564608435, 0.10891932941682847, 0.10003869524523157, 0.11392941165761183, 0.0957502522657081, 0.10697774217117662, 0.09957018286141905, 0.17043745156279588, 0.2015181385216821, 0.1877853152665483, 0.188133578451116, 0.18216602816859362, 0.1849775133028112, 0.1931627774404413, 0.1644544210329485, 0.18182308012466675, 0.28728861664168925, 0.30279964559585026, 0.3032560093151092, 0.29384544582905325, 0.315404599677616, 0.30140508521740017, 0.31640360989518146, 0.3193479261295167, 0.30341107987274785, 0.23337857843511411, 0.23682351276961477, 0.2531609224814174, 0.2358469732225632, 0.22680851225837084, 0.2439741841506642, 0.2645723188061496, 0.25929090142875366, 0.22262490964202175, 0.20184755128370435, 0.19065851008526813, 0.20463608307818149, 0.21275586971567717, 0.2042058620757944, 0.19682416642219092, 0.20633610460540353, 0.21947652438750276, 0.20591784099035493, 0.18747274882320064, 0.19579194386768017, 0.18773048413072968, 0.18359450253828336, 0.1893171302939468, 0.20982535929979274, 0.19829940849569316, 0.19016006497788407, 0.20047049993043464, 0.2613324005149841, 0.3307164827148078, 0.5397809464634118, 0.4523605828628683, 0.3081711949084235, 0.4250878526325237, 0.40234064759504196, 0.39565847157930856, 0.4236267128893787, 0.3275245622413572, 0.388528073786121, 0.41974211840668896, 0.3643024707990836, 0.30812015649485325, 0.43143788519506465, 0.23555050694132929, 0.3860173876067564, 0.3610275506253119, 0.1940683251455524, 0.17593852802827603, 0.18792858683154612, 0.18564194558251113, 0.18330079101219177, 0.18732965133591795, 0.1882765226452261, 0.17908542802656613, 0.17713298377311715, 0.08063703432269298, 0.07860321284392813, 0.0708507685504679, 0.08440727699335149, 0.10961812143538807, 0.07041794266430101, 0.08307521904552018, 0.07458069795229927, 0.0982434188152067]}, "mutation_prompt": null}
{"id": "a13ba1e6-8524-49df-939f-af27a0159dd8", "solution": "import numpy as np\n\nclass ProbabilisticAdaptiveStrategyOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = 40\n        self.mutation_factor = np.random.uniform(0.7, 0.9)\n        self.crossover_probability = np.random.uniform(0.9, 1.0)\n        self.learning_rate = 0.25\n        self.diversity_factor = 0.15\n        self.population = None\n        self.fitness = None\n\n    def initialize_population(self):\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, \n                                            (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def adaptive_differential_evolution(self, idx, func):\n        indices = [i for i in range(self.population_size) if i != idx]\n        np.random.shuffle(indices)\n        a, b, c = indices[:3]\n        mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        adaptive_factor = 0.5 + 0.5 * np.random.rand()\n        mutant = self.population[idx] + adaptive_factor * (mutant - self.population[idx])\n        mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n        \n        cross_points = np.random.rand(self.dim) < self.crossover_probability\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        \n        trial = np.where(cross_points, mutant, self.population[idx])\n        trial_fitness = func(trial)\n        \n        if trial_fitness < self.fitness[idx]:\n            self.population[idx] = trial\n            self.fitness[idx] = trial_fitness\n\n    def stochastic_gradient_approx(self, idx, func):\n        gradient = np.random.normal(0, 1, self.dim)\n        adaptive_lr = self.learning_rate / (1 + np.sqrt(idx / self.population_size))\n        candidate = self.population[idx] - adaptive_lr * gradient\n        candidate += self.diversity_factor * np.random.normal(0, 1, self.dim)\n        candidate = np.clip(candidate, self.lower_bound, self.upper_bound)\n        candidate_fitness = func(candidate)\n        if candidate_fitness < self.fitness[idx]:\n            self.population[idx] = candidate\n            self.fitness[idx] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population()\n        evaluations = 0\n        self.evaluate_population(func)\n        evaluations += self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                self.adaptive_differential_evolution(i, func)\n                evaluations += 1\n\n                if evaluations >= self.budget:\n                    break\n\n                self.stochastic_gradient_approx(i, func)\n                evaluations += 1\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "ProbabilisticAdaptiveStrategyOptimizer", "description": "Probabilistic Adaptive Strategy Optimizer using dynamic mutation strategies and real-time parameter adjustments for enhanced convergence.", "configspace": "", "generation": 58, "fitness": 0.36010103878657956, "feedback": "The algorithm ProbabilisticAdaptiveStrategyOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.36 with standard deviation 0.24.", "error": "", "parent_id": "1459935d-0a5b-47de-8bfe-69f52cb3cb6c", "metadata": {"aucs": [0.7884093023083348, 0.8131801236545085, 0.7896771613974398, 0.7733303745630149, 0.7792134277121319, 0.769728323365476, 0.781131314987004, 0.7901174032313416, 0.761980057223535, 0.5696103526959242, 0.589583733678229, 0.5799423526957548, 0.5668905525241261, 0.5813413177077204, 0.5787593616516511, 0.6032270701797475, 0.6023713197190679, 0.5675966184817498, 0.09754662314735418, 0.10207744605585944, 0.1238501347188421, 0.10192687079459173, 0.1305866826602018, 0.10993080648271714, 0.11114691447039127, 0.10782264377591533, 0.12460232240469105, 0.09163119118524243, 0.08924298214755344, 0.09337474543911084, 0.09645125668301846, 0.08830451821521779, 0.09464965752144017, 0.08764367026180153, 0.09796521736392949, 0.10696638867268538, 0.8819240448964696, 0.8398074187560888, 0.8519675182840353, 0.8378217547519461, 0.8332107092813149, 0.8843468552536324, 0.7814674497267756, 0.8099001844147051, 0.896225405514185, 0.3500184077899484, 0.3513480177438535, 0.3518253453904566, 0.37029505576071, 0.31024425334338135, 0.33509383846271534, 0.40305203390001676, 0.3206745906067512, 0.3439425486119262, 0.6595102807408482, 0.7772456929401199, 0.7436309382535238, 0.8168143127966396, 0.7574210658391249, 0.7906574503108967, 0.7896348482402921, 0.7664731731527223, 0.7634223782861225, 0.1470702164048513, 0.19105696551352425, 0.17060323490215978, 0.1955692452326906, 0.16228983343885917, 0.16361937870568255, 0.2418998031552011, 0.19475285860802638, 0.16918654831643687, 0.18305144714851318, 0.19051155211585213, 0.18060994781872397, 0.1684027529408615, 0.1984283142614074, 0.17771788959545898, 0.18934015644127045, 0.22113421610012451, 0.20226426122049446, 0.5251299818758408, 0.5409110688838987, 0.34379539279455484, 0.5360931012878655, 0.5149346540151871, 0.33550212353631437, 0.5378007711495607, 0.5150431362560776, 0.36163591747286705, 0.6189736709078295, 0.6207110513725975, 0.47161466740804425, 0.6332077793759286, 0.6346338950954573, 0.5062963393230699, 0.6419319126691014, 0.6489436008250276, 0.49504403732808, 0.13025056880741026, 0.19174237220455392, 0.060265945963704715, 0.14063018227404356, 0.12235195508656982, 0.15456923303854941, 0.1882607505771412, 0.24053517495585197, 0.09647543579901863, 0.37355781076437844, 0.4182513804537731, 0.28470217156083777, 0.41280058639714856, 0.419494307787574, 0.3067184449984024, 0.3461383736868988, 0.41297411647662907, 0.27710231235066873, 0.7313597200703823, 0.748198063596893, 0.6862605553117329, 0.7381619323472368, 0.7441200138408124, 0.6784866181579793, 0.7340824617892624, 0.7668629125018104, 0.6930223412091878, 0.10634539417736266, 0.10247853121690353, 0.1008846036328217, 0.11087656665458945, 0.09461434133120605, 0.10574073332012379, 0.10238134826477785, 0.12049121313449707, 0.10798887461493467, 0.1817548639240668, 0.18640916353618842, 0.19848854436875651, 0.18264769104033962, 0.2237350214210162, 0.1817543114393887, 0.19888815934677306, 0.22847970903453685, 0.18436971989899675, 0.38123463223789444, 0.4024991942288392, 0.36529120281467375, 0.38181012014228577, 0.40878701180696464, 0.356844685214037, 0.39143798570458543, 0.37294093438585374, 0.35230501901148537, 0.3268056008475072, 0.32235042786102874, 0.28483768995988923, 0.3050777790381669, 0.33960342760552, 0.26375609630844665, 0.3222048578356196, 0.35419830401620156, 0.29935954351650995, 0.25298254043230795, 0.1995267200082268, 0.20614760263286103, 0.21513129019830657, 0.18442168707123652, 0.20382134780883332, 0.21830286378429964, 0.22881140210141016, 0.20821693651578133, 0.18007094863968398, 0.1906352196824742, 0.1865496210912001, 0.18307719810175205, 0.1797276024735629, 0.19035432208571934, 0.18454551926952323, 0.18214412011255798, 0.18455964830472638, 0.32066665944472716, 0.17906791733527339, 0.18858324773767843, 0.387824145251747, 0.5669034166876695, 0.42046260723832396, 0.20664444734855592, 0.6676992991311796, 0.4339474678770938, 0.3783109462404317, 0.40917469886365165, 0.49421328376901885, 0.19992577215609886, 0.2534403130438212, 0.4689643762671165, 0.6248756855710685, 0.30806410234777193, 0.6228314079341521, 0.18854518906300888, 0.1887387540238582, 0.18838180602408394, 0.20657837642574772, 0.17427551928068863, 0.17885448968992845, 0.1870040344137609, 0.17914483103095624, 0.1991638342018759, 0.08346891168890891, 0.08484284180910096, 0.08351787278994471, 0.08069116690177724, 0.07701792019143394, 0.07670187306894605, 0.08914616590218938, 0.08798069317049939, 0.08059713243942512]}, "mutation_prompt": null}
{"id": "6f091c09-2590-461a-8fa5-1648b3588eff", "solution": "import numpy as np\n\nclass EnhancedProbabilisticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = 40\n        self.mutation_factor = 0.85\n        self.crossover_probability = 0.95\n        self.learning_rate = 0.2\n        self.diversity_factor = 0.1\n        self.population = None\n        self.fitness = None\n\n    def initialize_population(self):\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, \n                                            (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def adaptive_differential_evolution(self, idx, func):\n        indices = [i for i in range(self.population_size) if i != idx]\n        np.random.shuffle(indices)\n        a, b, c = indices[:3]\n        mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        adaptive_factor = 0.4 + 0.6 * np.random.rand()\n        mutant = self.population[idx] + adaptive_factor * (mutant - self.population[idx])\n        mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n        \n        cross_points = np.random.rand(self.dim) < self.crossover_probability\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        \n        trial = np.where(cross_points, mutant, self.population[idx])\n        trial_fitness = func(trial)\n        \n        if trial_fitness < self.fitness[idx]:\n            self.population[idx] = trial\n            self.fitness[idx] = trial_fitness\n\n    def stochastic_gradient_approx(self, idx, func):\n        gradient = np.random.normal(0, 1, self.dim)\n        adaptive_lr = self.learning_rate / (1 + idx / self.population_size)\n        candidate = self.population[idx] - adaptive_lr * gradient\n        candidate += self.diversity_factor * np.random.normal(0, 1, self.dim)\n        candidate = np.clip(candidate, self.lower_bound, self.upper_bound)\n        candidate_fitness = func(candidate)\n        if candidate_fitness < self.fitness[idx]:\n            self.population[idx] = candidate\n            self.fitness[idx] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population()\n        evaluations = 0\n        self.evaluate_population(func)\n        evaluations += self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                self.adaptive_differential_evolution(i, func)\n                evaluations += 1\n\n                if evaluations >= self.budget:\n                    break\n\n                self.stochastic_gradient_approx(i, func)\n                evaluations += 1\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "EnhancedProbabilisticOptimizer", "description": "Enhanced probabilistic hybrid optimizer balancing exploration and exploitation through diversity-oriented selection and adaptive parameter tuning for optimal convergence.", "configspace": "", "generation": 38, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "1459935d-0a5b-47de-8bfe-69f52cb3cb6c", "metadata": {"aucs": [0.796727445346743, 0.7946092935536371, 0.8025157468331956, 0.7795890428061506, 0.7652866403837684, 0.7888980264133363, 0.7851539828124771, 0.7801166288092796, 0.778684214476141, 0.6066596860215113, 0.5509488918434079, 0.584709500115218, 0.6151880393982149, 0.6128285196462557, 0.5928554548480235, 0.18781852246903208, 0.6051102165828588, 0.6050716765741883, 0.12056849613284826, 0.10627786970516262, 0.11092740344636509, 0.12458142503345515, 0.10225070914012868, 0.10214739878052792, 0.11007052847089493, 0.11303443664409152, 0.11346829363668931, 0.09058514750802149, 0.09136224141572435, 0.09346304265481342, 0.08413688806219866, 0.10560994817344638, 0.09386944951658027, 0.1066829182603829, 0.09525027475772718, 0.09078080677184064, 0.8379701410373176, 0.8069154658397715, 0.8108437032002754, 0.8202640448333322, 0.7815128713795206, 0.8451084830274826, 0.836376388134602, 0.8441441978939892, 0.8509693104465095, 0.37150586919602946, 0.34273942956965175, 0.3313822553845982, 0.34968755932637086, 0.32960730205573996, 0.3754984208710611, 0.38284578850911233, 0.3840456442426208, 0.3280564413760412, 0.7576709220936438, 0.7335556841917643, 0.7658090217912067, 0.7934167558179147, 0.8025617081483369, 0.7999764269594076, 0.7299292035530152, 0.7697788108067551, 0.7651252822797626, 0.16167439560696928, 0.18696031846775318, 0.21158580997319565, 0.244349141547606, 0.2890559956537958, 0.16298570563996873, 0.21750653437837209, 0.21122767559906996, 0.22550289679700497, 0.15305768362463734, 0.14884944831507896, 0.18600965489753707, 0.1541357033150358, 0.19832744782165168, 0.18983314847500432, 0.1953210887165725, 0.20165646735698872, 0.13088573983753993, 0.489476775395826, 0.460206761694571, 0.280690722270316, 0.34064732845138956, 0.522953329166158, 0.46452239779932414, 0.4196789194960461, 0.4727221489210359, 0.44131563048738687, 0.6540713674624187, 0.6128130384068273, 0.5553131009305013, 0.5776200055109268, 0.4723536786910414, 0.6219680956281209, 0.6455804612075908, 0.6109257481083044, 0.5996992464126676, 0.15220127888973134, 0.11018658007737869, 0.2413715712685326, 0.2296484097510183, 0.2572737636354798, 0.10622686325653308, 0.14604068872930986, 0.14809138800719435, 0.12257392403735012, 0.36655256082417187, 0.34632185847096675, 0.3516395670957846, 0.33828319802853724, 0.3572041238804262, 0.35621925345698524, 0.34628041224843775, 0.36205674625584605, 0.3801826028652373, 0.7284525303733058, 0.7334889541026953, 0.7386557191632159, 0.7055754468414382, 0.7222856768071027, 0.7194774022217179, 0.518854950686749, 0.7321096452535195, 0.7397060954282213, 0.10374399773694243, 0.104282622260642, 0.1086381702881759, 0.11429613558107654, 0.11511930056195652, 0.10035386421161252, 0.09599830676572241, 0.11347281801819442, 0.11470225552281121, 0.14952999643648723, 0.16817216547016145, 0.17310734903563885, 0.1855986179207637, 0.18359480037739373, 0.19508916572226054, 0.20087729718879477, 0.17807185486405164, 0.20119801391244463, 0.3682121014468256, 0.35073539948175425, 0.3330439038672147, 0.4157077631363848, 0.37474021495439724, 0.36760475650845414, 0.38008256501576043, 0.39084781839078786, 0.3786757352451906, 0.30037946434366947, 0.2861866860206934, 0.2891587957817804, 0.2835627691001067, 0.2779602895144492, 0.28387893685888044, 0.29400381025955125, 0.3244615423116647, 0.30188351717337136, 0.20494558183242018, 0.19905376550038034, 0.20406514428030742, 0.2229753496453719, 0.1980660060283188, 0.21539123082194234, 0.21928087314384304, 0.20854328449838244, 0.2102975078755156, 0.18082128476991477, 0.1831737258940077, 0.18089956975930732, 0.18277323416104252, 0.1792695777259361, 0.17816092407226292, 0.19628636488975992, 0.19237612183491615, 0.1874718333986961, 0.6294666203187917, 0.602690555999032, 0.5454952462675013, 0.7727337342883757, 0.25677125313719296, 0.588947972014151, 0.5053651729784945, 0.42433758940449484, 0.610090527727432, 0.7148365734773846, 0.39316249107203016, 0.4609914040189693, 0.5414475146943718, 0.3738614312609654, 0.2972834077003401, 0.2719039283446757, 0.4464259994844204, 0.6865015215956998, 0.18541089636272012, 0.17521600614441235, 0.18498258988690697, 0.1923345960637889, 0.19442272937612437, 0.18424555156947642, 0.17449564646480575, 0.18372628686318526, 0.18823517758491137, 0.07953923390817585, 0.08429672806217459, 0.08351517483047155, 0.07872531348008505, 0.0885755282031725, 0.09071254814221597, 0.07022751225803625, 0.07907174920771143, 0.08022853716110878]}, "mutation_prompt": null}
{"id": "33ef75be-2222-426f-8eac-c27516790405", "solution": "import numpy as np\n\nclass EnhancedProbabilisticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = 40\n        self.mutation_factor = 0.85\n        self.crossover_probability = 0.95\n        self.learning_rate = 0.2\n        self.diversity_factor = 0.1\n        self.population = None\n        self.fitness = None\n\n    def initialize_population(self):\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, \n                                            (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def adaptive_differential_evolution(self, idx, func):\n        indices = [i for i in range(self.population_size) if i != idx]\n        np.random.shuffle(indices)\n        a, b, c = indices[:3]\n        mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        adaptive_factor = 0.4 + 0.6 * np.random.rand()\n        mutant = self.population[idx] + adaptive_factor * (mutant - self.population[idx])\n        mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n        \n        cross_points = np.random.rand(self.dim) < self.crossover_probability\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        \n        trial = np.where(cross_points, mutant, self.population[idx])\n        trial_fitness = func(trial)\n        \n        if trial_fitness < self.fitness[idx]:\n            self.population[idx] = trial\n            self.fitness[idx] = trial_fitness\n\n    def stochastic_gradient_approx(self, idx, func):\n        gradient = np.random.normal(0, 1, self.dim)\n        adaptive_lr = self.learning_rate / (1 + idx / self.population_size)\n        candidate = self.population[idx] - adaptive_lr * gradient\n        candidate += self.diversity_factor * np.random.normal(0, 1, self.dim)\n        candidate = np.clip(candidate, self.lower_bound, self.upper_bound)\n        candidate_fitness = func(candidate)\n        if candidate_fitness < self.fitness[idx]:\n            self.population[idx] = candidate\n            self.fitness[idx] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population()\n        evaluations = 0\n        self.evaluate_population(func)\n        evaluations += self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                self.adaptive_differential_evolution(i, func)\n                evaluations += 1\n\n                if evaluations >= self.budget:\n                    break\n\n                self.stochastic_gradient_approx(i, func)\n                evaluations += 1\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "EnhancedProbabilisticOptimizer", "description": "Enhanced probabilistic hybrid optimizer balancing exploration and exploitation through diversity-oriented selection and adaptive parameter tuning for optimal convergence.", "configspace": "", "generation": 38, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "1459935d-0a5b-47de-8bfe-69f52cb3cb6c", "metadata": {"aucs": [0.796727445346743, 0.7946092935536371, 0.8025157468331956, 0.7795890428061506, 0.7652866403837684, 0.7888980264133363, 0.7851539828124771, 0.7801166288092796, 0.778684214476141, 0.6066596860215113, 0.5509488918434079, 0.584709500115218, 0.6151880393982149, 0.6128285196462557, 0.5928554548480235, 0.18781852246903208, 0.6051102165828588, 0.6050716765741883, 0.12056849613284826, 0.10627786970516262, 0.11092740344636509, 0.12458142503345515, 0.10225070914012868, 0.10214739878052792, 0.11007052847089493, 0.11303443664409152, 0.11346829363668931, 0.09058514750802149, 0.09136224141572435, 0.09346304265481342, 0.08413688806219866, 0.10560994817344638, 0.09386944951658027, 0.1066829182603829, 0.09525027475772718, 0.09078080677184064, 0.8379701410373176, 0.8069154658397715, 0.8108437032002754, 0.8202640448333322, 0.7815128713795206, 0.8451084830274826, 0.836376388134602, 0.8441441978939892, 0.8509693104465095, 0.37150586919602946, 0.34273942956965175, 0.3313822553845982, 0.34968755932637086, 0.32960730205573996, 0.3754984208710611, 0.38284578850911233, 0.3840456442426208, 0.3280564413760412, 0.7576709220936438, 0.7335556841917643, 0.7658090217912067, 0.7934167558179147, 0.8025617081483369, 0.7999764269594076, 0.7299292035530152, 0.7697788108067551, 0.7651252822797626, 0.16167439560696928, 0.18696031846775318, 0.21158580997319565, 0.244349141547606, 0.2890559956537958, 0.16298570563996873, 0.21750653437837209, 0.21122767559906996, 0.22550289679700497, 0.15305768362463734, 0.14884944831507896, 0.18600965489753707, 0.1541357033150358, 0.19832744782165168, 0.18983314847500432, 0.1953210887165725, 0.20165646735698872, 0.13088573983753993, 0.489476775395826, 0.460206761694571, 0.280690722270316, 0.34064732845138956, 0.522953329166158, 0.46452239779932414, 0.4196789194960461, 0.4727221489210359, 0.44131563048738687, 0.6540713674624187, 0.6128130384068273, 0.5553131009305013, 0.5776200055109268, 0.4723536786910414, 0.6219680956281209, 0.6455804612075908, 0.6109257481083044, 0.5996992464126676, 0.15220127888973134, 0.11018658007737869, 0.2413715712685326, 0.2296484097510183, 0.2572737636354798, 0.10622686325653308, 0.14604068872930986, 0.14809138800719435, 0.12257392403735012, 0.36655256082417187, 0.34632185847096675, 0.3516395670957846, 0.33828319802853724, 0.3572041238804262, 0.35621925345698524, 0.34628041224843775, 0.36205674625584605, 0.3801826028652373, 0.7284525303733058, 0.7334889541026953, 0.7386557191632159, 0.7055754468414382, 0.7222856768071027, 0.7194774022217179, 0.518854950686749, 0.7321096452535195, 0.7397060954282213, 0.10374399773694243, 0.104282622260642, 0.1086381702881759, 0.11429613558107654, 0.11511930056195652, 0.10035386421161252, 0.09599830676572241, 0.11347281801819442, 0.11470225552281121, 0.14952999643648723, 0.16817216547016145, 0.17310734903563885, 0.1855986179207637, 0.18359480037739373, 0.19508916572226054, 0.20087729718879477, 0.17807185486405164, 0.20119801391244463, 0.3682121014468256, 0.35073539948175425, 0.3330439038672147, 0.4157077631363848, 0.37474021495439724, 0.36760475650845414, 0.38008256501576043, 0.39084781839078786, 0.3786757352451906, 0.30037946434366947, 0.2861866860206934, 0.2891587957817804, 0.2835627691001067, 0.2779602895144492, 0.28387893685888044, 0.29400381025955125, 0.3244615423116647, 0.30188351717337136, 0.20494558183242018, 0.19905376550038034, 0.20406514428030742, 0.2229753496453719, 0.1980660060283188, 0.21539123082194234, 0.21928087314384304, 0.20854328449838244, 0.2102975078755156, 0.18082128476991477, 0.1831737258940077, 0.18089956975930732, 0.18277323416104252, 0.1792695777259361, 0.17816092407226292, 0.19628636488975992, 0.19237612183491615, 0.1874718333986961, 0.6294666203187917, 0.602690555999032, 0.5454952462675013, 0.7727337342883757, 0.25677125313719296, 0.588947972014151, 0.5053651729784945, 0.42433758940449484, 0.610090527727432, 0.7148365734773846, 0.39316249107203016, 0.4609914040189693, 0.5414475146943718, 0.3738614312609654, 0.2972834077003401, 0.2719039283446757, 0.4464259994844204, 0.6865015215956998, 0.18541089636272012, 0.17521600614441235, 0.18498258988690697, 0.1923345960637889, 0.19442272937612437, 0.18424555156947642, 0.17449564646480575, 0.18372628686318526, 0.18823517758491137, 0.07953923390817585, 0.08429672806217459, 0.08351517483047155, 0.07872531348008505, 0.0885755282031725, 0.09071254814221597, 0.07022751225803625, 0.07907174920771143, 0.08022853716110878]}, "mutation_prompt": null}
{"id": "ad338ade-147b-4184-bd16-6869c91ff3ab", "solution": "import numpy as np\n\nclass DynamicHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = 50  # Adjusted population size\n        self.mutation_factor = 0.9  # Adjusted mutation factor\n        self.crossover_probability = 0.9  # Adjusted crossover probability\n        self.learning_rate = 0.3  # Adjusted learning rate\n        self.diversity_factor = 0.15  # Adjusted diversity factor\n        self.population = None\n        self.fitness = None\n\n    def initialize_population(self):\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, \n                                            (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def adaptive_mutation(self, idx, func):\n        indices = [i for i in range(self.population_size) if i != idx]\n        np.random.shuffle(indices)\n        a, b, c = indices[:3]\n        mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        adaptive_factor = 0.5 + 0.5 * np.random.rand()  # Altered scaling range\n        mutant = self.population[idx] + adaptive_factor * (mutant - self.population[idx])\n        mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n        \n        cross_points = np.random.rand(self.dim) < self.crossover_probability\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        \n        trial = np.where(cross_points, mutant, self.population[idx])\n        trial_fitness = func(trial)\n        \n        if trial_fitness < self.fitness[idx]:\n            self.population[idx] = trial\n            self.fitness[idx] = trial_fitness\n\n    def stochastic_gradient_injection(self, idx, func):\n        gradient = np.random.normal(0, 1, self.dim)\n        adaptive_lr = self.learning_rate / (1 + np.sqrt(idx / self.population_size))  # Altered learning rate adaptation\n        candidate = self.population[idx] - adaptive_lr * gradient\n        candidate += self.diversity_factor * np.random.normal(0, 1, self.dim)\n        candidate = np.clip(candidate, self.lower_bound, self.upper_bound)\n        candidate_fitness = func(candidate)\n        if candidate_fitness < self.fitness[idx]:\n            self.population[idx] = candidate\n            self.fitness[idx] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population()\n        evaluations = 0\n        self.evaluate_population(func)\n        evaluations += self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                self.adaptive_mutation(i, func)\n                evaluations += 1\n\n                if evaluations >= self.budget:\n                    break\n\n                self.stochastic_gradient_injection(i, func)\n                evaluations += 1\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "DynamicHybridOptimizer", "description": "Dynamic hybrid optimizer combining adaptive differential evolution with mutation adaptation and stochastic gradient injection for enhanced convergence and diversity.", "configspace": "", "generation": 61, "fitness": 0.28462320268717395, "feedback": "The algorithm DynamicHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28 with standard deviation 0.20.", "error": "", "parent_id": "1459935d-0a5b-47de-8bfe-69f52cb3cb6c", "metadata": {"aucs": [0.6782672923940978, 0.6708463442837201, 0.6747603547907182, 0.6632116569221009, 0.6764807374716255, 0.6692541059115985, 0.7024972902007695, 0.6878898529308795, 0.6762139110848857, 0.4147082021431011, 0.3900665077443004, 0.39086064041211344, 0.3688020026905938, 0.3776386272327331, 0.4019056554284307, 0.3983609036604263, 0.38352999088518025, 0.40210670666838333, 0.10275782873294415, 0.09897066120571285, 0.11252763775531183, 0.11485285258168965, 0.10837805302556058, 0.10040528406740523, 0.10233913460431487, 0.10148852239143613, 0.10044667538116281, 0.08528278206101736, 0.08697665372656105, 0.08722831061327707, 0.08975851595503592, 0.10628699765631866, 0.08781773077593258, 0.08327387439775469, 0.08973251238592383, 0.09818734965881792, 0.9309115010325939, 0.8753949549618767, 0.9595785887646043, 0.8404259818736118, 0.8436516887233908, 0.8480943851033583, 0.7963646068192352, 0.8766697943664559, 0.8350984728383078, 0.2989924232812846, 0.28072726468742737, 0.29421977642900377, 0.28440626940266955, 0.28257029902545516, 0.2898633872988894, 0.2816070668920012, 0.2826296153988842, 0.30395278478640286, 0.5970013167976044, 0.6158947693039456, 0.6248797793022736, 0.621225292767147, 0.6398291197589527, 0.6533772134832566, 0.6300190835897599, 0.6390283408461597, 0.6604783888388359, 0.19344172331001452, 0.14096624002231106, 0.17122719704892864, 0.17924411129903872, 0.177005633847113, 0.17712680708179318, 0.17551890258309033, 0.17039402802066417, 0.18798107234337413, 0.18983678967320283, 0.15449530677978196, 0.1696026578964258, 0.15550227646952808, 0.18377198347795642, 0.1650889399337926, 0.19235597782221592, 0.16236100119530816, 0.19391842832562778, 0.16088020976934714, 0.17828733029715005, 0.17435748010626106, 0.18708601043695072, 0.1680105964130576, 0.17839189262417787, 0.16535015846759216, 0.17529183400224613, 0.20169625377229872, 0.31515535317453347, 0.3185452374479306, 0.30783274352805834, 0.3263457687586351, 0.30751145434125127, 0.3297488022569579, 0.3298194679906161, 0.3251874058349651, 0.360434709790032, 0.0461509930868198, 0.07846456167705784, 0.06152421063934799, 0.0757177726279733, 0.052483210225404586, 0.06781978810177547, 0.0729881834347611, 0.05630802144193281, 0.05359810492121597, 0.19939873782943496, 0.19176990890432433, 0.19351471270522336, 0.18904828181356392, 0.18530922765306956, 0.21641606477063713, 0.17921050538843553, 0.18022299820326915, 0.19589755231132144, 0.5590706486924759, 0.5286206976441603, 0.5415453768914604, 0.539656527679998, 0.5311312794186159, 0.5564735712514756, 0.544488187395587, 0.5304813160752122, 0.5450991565031675, 0.11598885274380533, 0.10061376734847494, 0.09751381970357287, 0.09299198076622361, 0.0963111837427636, 0.08896204626658455, 0.11450118705214518, 0.09917150116132345, 0.09794844558170557, 0.17724727132581486, 0.20841133041144266, 0.17671466659545698, 0.17252807475677, 0.22582835434803317, 0.16508728522713978, 0.1726653277091813, 0.16574954055328528, 0.15549143675940102, 0.27842278809786347, 0.2688656280614333, 0.29298549524269635, 0.27665465279417223, 0.28147672671939117, 0.28695264229714135, 0.2991719769960255, 0.30906106042153925, 0.27889966181675196, 0.20883259599040382, 0.22638456266375462, 0.2269784077546888, 0.21527399774502598, 0.2121101727244672, 0.22277646969337717, 0.2193130089509936, 0.2241950252711472, 0.22498467163588076, 0.19801515650335866, 0.19707308782914856, 0.20986857751111054, 0.18874482837073725, 0.20176588957197217, 0.19764526549557837, 0.20428017747355787, 0.20603477274269588, 0.19817464017319875, 0.1826497124647548, 0.187771113953204, 0.18467001657584037, 0.18363931430754332, 0.1822243138007874, 0.18358120902019293, 0.18439509675522991, 0.189169273667265, 0.17863406557238448, 0.36800856061197906, 0.25894888987966447, 0.392054852397154, 0.3376967030118764, 0.2888533139067767, 0.4158954560870317, 0.3798721135968346, 0.3003992201702804, 0.3721790229192736, 0.41044384191400984, 0.36141546594655205, 0.37860937326493327, 0.3458234595046993, 0.3398391434610344, 0.3143992563407403, 0.28135471897020115, 0.3665584865540008, 0.30548319425864534, 0.17593116894184402, 0.18136636173423382, 0.18568883847182316, 0.18181994081106967, 0.20093930076465705, 0.20248407154658143, 0.20461342225278623, 0.18140167738934831, 0.19843331463165903, 0.0737193860836789, 0.0874097005096619, 0.08543367027382298, 0.08385376359253627, 0.07624398619063522, 0.08061028698534067, 0.08020556060464135, 0.07164522524405259, 0.08038919325190108]}, "mutation_prompt": null}
{"id": "994fb30e-d692-47ad-bea0-64e165d4a915", "solution": "import numpy as np\n\nclass AdaptiveConvergenceOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = 50  # changed\n        self.mutation_factor = 0.9  # changed\n        self.crossover_probability = 0.9  # changed\n        self.learning_rate = 0.15  # changed\n        self.diversity_factor = 0.2  # changed\n        self.population = None\n        self.fitness = None\n\n    def initialize_population(self):\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, \n                                            (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def adaptive_differential_evolution(self, idx, func):\n        indices = [i for i in range(self.population_size) if i != idx]\n        np.random.shuffle(indices)\n        a, b, c = indices[:3]\n        mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        adaptive_factor = 0.4 + 0.6 * np.random.rand()\n        mutant = self.population[idx] + adaptive_factor * (mutant - self.population[idx])\n        mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n        \n        cross_points = np.random.rand(self.dim) < self.crossover_probability\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        \n        trial = np.where(cross_points, mutant, self.population[idx])\n        trial_fitness = func(trial)\n        \n        if trial_fitness < self.fitness[idx]:\n            self.population[idx] = trial\n            self.fitness[idx] = trial_fitness\n\n    def stochastic_gradient_approx(self, idx, func):\n        gradient = np.random.normal(0, 1, self.dim)\n        adaptive_lr = self.learning_rate / (1 + idx / self.population_size)\n        candidate = self.population[idx] - adaptive_lr * gradient\n        candidate += self.diversity_factor * np.random.normal(0, 1, self.dim)\n        candidate = np.clip(candidate, self.lower_bound, self.upper_bound)\n        candidate_fitness = func(candidate)\n        if candidate_fitness < self.fitness[idx]:\n            self.population[idx] = candidate\n            self.fitness[idx] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population()\n        evaluations = 0\n        self.evaluate_population(func)\n        evaluations += self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                self.adaptive_differential_evolution(i, func)\n                evaluations += 1\n\n                if evaluations >= self.budget:\n                    break\n\n                self.stochastic_gradient_approx(i, func)\n                evaluations += 1\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "AdaptiveConvergenceOptimizer", "description": "Adaptive convergence optimizer leveraging probabilistic mutation and crossover strategies with enhanced diversity control for efficient global exploration and local refinement.", "configspace": "", "generation": 62, "fitness": 0.29816717634459194, "feedback": "The algorithm AdaptiveConvergenceOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.30 with standard deviation 0.21.", "error": "", "parent_id": "1459935d-0a5b-47de-8bfe-69f52cb3cb6c", "metadata": {"aucs": [0.7403417136107944, 0.7168252362944828, 0.7100632720441934, 0.6892752931394144, 0.6971221459496273, 0.712735532188378, 0.6977549062499226, 0.6967342459200879, 0.7115174660017299, 0.4155112215944132, 0.461511209947169, 0.4396830768863904, 0.4493407228569739, 0.4446135874226561, 0.4639168631727004, 0.4529187822152284, 0.40077638885608535, 0.4564216339104079, 0.10809100469683586, 0.11097362716659187, 0.10022696364750794, 0.10049391130909557, 0.11327993225249977, 0.10948640985399416, 0.10732026338547918, 0.1056875133837466, 0.09655475863605456, 0.0957751881215636, 0.0814094660452509, 0.08561797200820587, 0.09364288592326286, 0.08848671895949389, 0.09468791118919984, 0.09612953721265638, 0.09213216341252661, 0.08671982985967897, 0.8341244148692505, 0.83571637542234, 0.8438080007546149, 0.8605600721482475, 0.8264111307530461, 0.8010821914916934, 0.8245167362324419, 0.8311086589980349, 0.7720403728487989, 0.2904599016488202, 0.3043807450079029, 0.28399693108396695, 0.3300660659111344, 0.3046765707760415, 0.3066017061728117, 0.30509082149381894, 0.29939308843072665, 0.2796416116957722, 0.6181785833565863, 0.6535661276099389, 0.6917176300895354, 0.6490237101921973, 0.678921343530777, 0.699544274602274, 0.6035161456785973, 0.6556719587952846, 0.6167370545641395, 0.1783459774907542, 0.16770067789935839, 0.171318851841339, 0.22428449983513665, 0.15646215109991057, 0.185724741373957, 0.18032148497581058, 0.17188874566720713, 0.1531626013063726, 0.1560080851694725, 0.1522179867938458, 0.17646720799526305, 0.16757861443733457, 0.18527393462212416, 0.22507023026446016, 0.17149625362219634, 0.16540931855539598, 0.1753350540687556, 0.17915907201001047, 0.2153175889234411, 0.23667580936313193, 0.2261387774075294, 0.22316911761028158, 0.21325692788674333, 0.256486976179365, 0.18086401568546306, 0.25826223100139645, 0.3696893709365907, 0.32802917806450027, 0.36141993381965276, 0.3568237559027544, 0.342925141223666, 0.3848863343347806, 0.4013501868215511, 0.3878335137412424, 0.4150376755037789, 0.0703350514737544, 0.11736153842894259, 0.06346473094899907, 0.11021680644642928, 0.0789317125750022, 0.0916874896791674, 0.04731997646387098, 0.0708216933002046, 0.09630700155318905, 0.2044077491113968, 0.22738654127392133, 0.21587811931137557, 0.21337015445525587, 0.22910669646402182, 0.2377628721119509, 0.21454416411761879, 0.2178308028457815, 0.22442872227627597, 0.5723155176803555, 0.5834394569542878, 0.5929012268548486, 0.5669252556478074, 0.5489474890587129, 0.5580680729319447, 0.5661071794865911, 0.609038023135748, 0.5921104328554434, 0.10653717447787292, 0.0995943792963242, 0.09802266275224014, 0.09771708183185479, 0.10912513721012185, 0.08972031572259276, 0.09501693345863549, 0.09901740286643468, 0.10614730009635465, 0.20483553473694338, 0.1785979440401072, 0.18499185862357204, 0.18998133061850553, 0.1662085014416318, 0.1853057220439044, 0.16990357286856705, 0.17385268523698194, 0.17044130361189347, 0.2901101734861298, 0.29594044165263245, 0.329328813490363, 0.28575845338894246, 0.2970648198509409, 0.3226605545676612, 0.2984905710361244, 0.29315591481690784, 0.32151419278107085, 0.22835128701989638, 0.24442365923409592, 0.22444783092910026, 0.21484616120426947, 0.22424240831647269, 0.2476157488383799, 0.2338912470251019, 0.25435425895801544, 0.2394381215440895, 0.2213443994195542, 0.18193068674867363, 0.21501421826102773, 0.23158177517322043, 0.2196635074947011, 0.22520742583149966, 0.20219520073652852, 0.19929549940354363, 0.20096162178363441, 0.182486703735392, 0.18020775328086636, 0.1780052915231789, 0.1863330814563975, 0.18077863788940052, 0.17690143516512746, 0.1826380535254747, 0.18674931557504026, 0.18221727711240454, 0.3174813180646151, 0.1768882995067088, 0.3976171244893194, 0.44063349854669254, 0.33606781433197863, 0.3969915241562155, 0.3888504749760303, 0.29450783669063274, 0.5074922658399244, 0.29540325901504216, 0.3797770031003189, 0.5707089727637411, 0.3105706349642243, 0.3199184853735949, 0.34688166767786366, 0.26474357782536984, 0.3410057696490597, 0.4112504345222424, 0.1941349081098045, 0.17381273444440393, 0.18517302801845337, 0.19863490678033402, 0.17750458658399415, 0.18947473986358065, 0.19972429161713878, 0.1842881426002061, 0.21289729030918436, 0.08522333709215213, 0.09323777434738678, 0.07795269107388048, 0.0713960457474132, 0.08661361936326306, 0.08458740186325231, 0.09121235567855801, 0.07763682729891208, 0.07291398459539145]}, "mutation_prompt": null}
{"id": "580da4ee-4c82-4a87-aca0-b1702f8dd714", "solution": "import numpy as np\n\nclass AdvancedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = 50  # Increased population size for better exploration\n        self.mutation_factor = 0.9  # Slightly higher mutation factor\n        self.crossover_probability = 0.9  # Reduced slightly for more robust convergence\n        self.learning_rate = 0.15  # Reduced learning rate for finer adjustments\n        self.diversity_factor = 0.1\n        self.population = None\n        self.fitness = None\n\n    def initialize_population(self):\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, \n                                            (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def adaptive_differential_evolution(self, idx, func):\n        indices = [i for i in range(self.population_size) if i != idx]\n        np.random.shuffle(indices)\n        a, b, c = indices[:3]\n        mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        adaptive_factor = 0.3 + 0.7 * np.random.rand()  # Adjusted range for adaptive factor\n        mutant = self.population[idx] + adaptive_factor * (mutant - self.population[idx])\n        mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n        \n        cross_points = np.random.rand(self.dim) < self.crossover_probability\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        \n        trial = np.where(cross_points, mutant, self.population[idx])\n        trial_fitness = func(trial)\n        \n        if trial_fitness < self.fitness[idx]:\n            self.population[idx] = trial\n            self.fitness[idx] = trial_fitness\n\n    def stochastic_gradient_approx(self, idx, func):\n        gradient = np.random.normal(0, 1, self.dim)\n        adaptive_lr = self.learning_rate / (1 + idx / self.population_size)\n        candidate = self.population[idx] - adaptive_lr * gradient\n        candidate += self.diversity_factor * np.random.normal(0, 1, self.dim)\n        candidate = np.clip(candidate, self.lower_bound, self.upper_bound)\n        candidate_fitness = func(candidate)\n        if candidate_fitness < self.fitness[idx]:\n            self.population[idx] = candidate\n            self.fitness[idx] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population()\n        evaluations = 0\n        self.evaluate_population(func)\n        evaluations += self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                self.adaptive_differential_evolution(i, func)\n                evaluations += 1\n\n                if evaluations >= self.budget:\n                    break\n\n                self.stochastic_gradient_approx(i, func)\n                evaluations += 1\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "AdvancedHybridOptimizer", "description": "Advanced hybrid optimizer leveraging adaptive differential evolution and stochastic gradient perturbations with enhanced diversity management for robust convergence.", "configspace": "", "generation": 63, "fitness": 0.3064902778986582, "feedback": "The algorithm AdvancedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.31 with standard deviation 0.21.", "error": "", "parent_id": "1459935d-0a5b-47de-8bfe-69f52cb3cb6c", "metadata": {"aucs": [0.7471810127376015, 0.7289433533434608, 0.7455550218589753, 0.6995657726019655, 0.7117721513446882, 0.7365569497318025, 0.7051645499236544, 0.7177994718377301, 0.72205557228386, 0.4683822559702625, 0.44880784061956047, 0.47479499448040374, 0.4619244961422916, 0.4345667356893316, 0.4804889498431524, 0.4807582411753285, 0.48700884221709995, 0.4650393995916786, 0.11859527540361559, 0.107439476361884, 0.11796629707977613, 0.11479935264187902, 0.11084443778660957, 0.11216224815152587, 0.11532042677017473, 0.11688985852820533, 0.11660049694156327, 0.0845218811577686, 0.0908923933088861, 0.0889753531268751, 0.09884292354207269, 0.09238228151798555, 0.10024176138196528, 0.09979583243113499, 0.10353882544798432, 0.09998505952006365, 0.8319521639983791, 0.793852751601241, 0.8612022609696687, 0.80806668143695, 0.8269497826963375, 0.7851180735183126, 0.7806327511152864, 0.8147858695981525, 0.7826607518474578, 0.3024410216676232, 0.3109519446950848, 0.2913113389287214, 0.3123663681108555, 0.30659954570875947, 0.28851887344176463, 0.31440362523348564, 0.3122250774099603, 0.29122533115576565, 0.6278825694738989, 0.6055981341882624, 0.6156657180714202, 0.7428055511488654, 0.7207737358205067, 0.6915702408731896, 0.6284883496433522, 0.6703024156344954, 0.6633306371503545, 0.15535705094726326, 0.20053561513011253, 0.15957474986294695, 0.21160540090973412, 0.1569561613482352, 0.16499223712402544, 0.2119595307116796, 0.1803559686947801, 0.14304052651870136, 0.19538248215612464, 0.19190890587831977, 0.1939130565323649, 0.1582115645228912, 0.1636511160900349, 0.24497261323985808, 0.184007216054631, 0.1876318192369164, 0.24317690072790776, 0.20624008451465425, 0.2588020661764818, 0.23788442363850604, 0.22457550243799684, 0.23847797498059586, 0.25274971086699694, 0.25768362994064087, 0.25656513912936596, 0.28927091813365047, 0.42208589471079694, 0.34068464867393655, 0.36363911337990806, 0.3696031022230263, 0.35721259846835174, 0.3834811498894396, 0.3794968958971272, 0.42498082154705163, 0.4609124958410643, 0.08383306357086118, 0.08592018613829022, 0.08350232597179763, 0.09759624579541282, 0.11614663930614755, 0.13976456500806977, 0.09998263137028884, 0.12529782509727316, 0.10232651111906421, 0.2358049377746393, 0.2326172772695856, 0.23959851600653748, 0.22230473960232944, 0.2565297129004317, 0.2566056897720368, 0.21276442206407387, 0.21447903601584184, 0.2404343502061118, 0.5945897119354275, 0.6164909555656806, 0.6341623502901239, 0.6154994637054805, 0.5870048558548511, 0.6092191143236418, 0.6300793674011996, 0.6191970169263157, 0.620761726992725, 0.10877680704948034, 0.10182526226741262, 0.10935526330228262, 0.11398182500783205, 0.11152115757542036, 0.09656798196637428, 0.10633929005856335, 0.10330880297352252, 0.09760504923097835, 0.18602032276670888, 0.19611010708469045, 0.18254360381517565, 0.18079785520966418, 0.20751078597964645, 0.18390707208486345, 0.18239900264619557, 0.15700294834262385, 0.1759950792055348, 0.3126957952559821, 0.28525218555217857, 0.2923990301269407, 0.3076538476883559, 0.29429770413115897, 0.29729241938537865, 0.3164848318409932, 0.31125914787703945, 0.30953969692221617, 0.2443670024376503, 0.23470729650282196, 0.26662686318681894, 0.2347993847929234, 0.252354008085371, 0.2547969573806683, 0.22807807061686125, 0.2569580381365294, 0.22429756390281375, 0.20928154769157992, 0.22050446872436036, 0.20993625899577173, 0.19410494898893038, 0.20636304183606047, 0.18967267751402095, 0.20134008183203234, 0.19671429654472572, 0.20911091367477175, 0.1844275204781144, 0.18344466838567242, 0.20136316134745125, 0.19252931612958468, 0.20188270609563408, 0.17621881496701064, 0.18880448621194468, 0.1913522495672917, 0.18544666919674468, 0.1849561584179471, 0.348352121238259, 0.31116038571009186, 0.42067828091246684, 0.34532901052231957, 0.3099837933460444, 0.4241513843874941, 0.3201643454970641, 0.42917865855029236, 0.4401622681004228, 0.42772117831442247, 0.6141445855177969, 0.3875798610439729, 0.2425576408621395, 0.3620814005123246, 0.21677792124058382, 0.3176309652324226, 0.37492516230834694, 0.19042626050380373, 0.18744407967553567, 0.1777904001859888, 0.18665570209528903, 0.18671431430981722, 0.19807216816285467, 0.18499642893680712, 0.18726287821007614, 0.19767577878983122, 0.07991233262394848, 0.0877593788269938, 0.07828310018408358, 0.08340666746452119, 0.07545490627200868, 0.08799238380550001, 0.07944150763689517, 0.07711499866639593, 0.0781020477227028]}, "mutation_prompt": null}
{"id": "2f51c667-ffd9-4e29-bddf-8f9049464957", "solution": "import numpy as np\n\nclass EnhancedProbabilisticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = 50\n        self.mutation_factor = 0.9\n        self.crossover_probability = 0.9\n        self.learning_rate = 0.15\n        self.diversity_factor = 0.15\n        self.population = None\n        self.fitness = None\n\n    def initialize_population(self):\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, \n                                            (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def adaptive_differential_evolution(self, idx, func):\n        indices = [i for i in range(self.population_size) if i != idx]\n        np.random.shuffle(indices)\n        a, b, c = indices[:3]\n        mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        adaptive_factor = 0.3 + 0.7 * np.random.rand()\n        mutant = self.population[idx] + adaptive_factor * (mutant - self.population[idx])\n        mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n        \n        cross_points = np.random.rand(self.dim) < self.crossover_probability\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        \n        trial = np.where(cross_points, mutant, self.population[idx])\n        trial_fitness = func(trial)\n        \n        if trial_fitness < self.fitness[idx]:\n            self.population[idx] = trial\n            self.fitness[idx] = trial_fitness\n\n    def stochastic_gradient_approx(self, idx, func):\n        gradient = np.random.normal(0, 1, self.dim)\n        adaptive_lr = self.learning_rate / (1 + np.sqrt(idx) / self.population_size)\n        candidate = self.population[idx] - adaptive_lr * gradient\n        candidate += self.diversity_factor * np.random.normal(0, 1, self.dim)\n        candidate = np.clip(candidate, self.lower_bound, self.upper_bound)\n        candidate_fitness = func(candidate)\n        if candidate_fitness < self.fitness[idx]:\n            self.population[idx] = candidate\n            self.fitness[idx] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population()\n        evaluations = 0\n        self.evaluate_population(func)\n        evaluations += self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                self.adaptive_differential_evolution(i, func)\n                evaluations += 1\n\n                if evaluations >= self.budget:\n                    break\n\n                self.stochastic_gradient_approx(i, func)\n                evaluations += 1\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "EnhancedProbabilisticOptimizer", "description": "Synergistic evolutionary optimizer leveraging adaptive differential evolution and multi-modal selection for robust convergence in diverse landscapes.", "configspace": "", "generation": 64, "fitness": 0.30694709491054056, "feedback": "The algorithm EnhancedProbabilisticOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.31 with standard deviation 0.21.", "error": "", "parent_id": "1459935d-0a5b-47de-8bfe-69f52cb3cb6c", "metadata": {"aucs": [0.7447147302074801, 0.7298533795106789, 0.7440335256580171, 0.7014206886147791, 0.7256670752214655, 0.7148196590323309, 0.722377313140747, 0.6998149384638314, 0.7307573285443931, 0.46607449364889386, 0.4773569154601325, 0.4620829539413457, 0.45576095109645665, 0.4678989330327662, 0.473958193668215, 0.4629030556311058, 0.4413143044795965, 0.4405112557849846, 0.09592390710445775, 0.1079656456026602, 0.11432949772987488, 0.1129426166816826, 0.1111305075449931, 0.11642300952841944, 0.09743868117213972, 0.13351033000849444, 0.11853079708588976, 0.09075093384483912, 0.07815060851962807, 0.09201083062681192, 0.09268421708768548, 0.09134066460350132, 0.08577885917848993, 0.09307574698609367, 0.08594311921638886, 0.08861930405131446, 0.8335062197492328, 0.8706077676161198, 0.8119627318169141, 0.7905358836730441, 0.7909084924329844, 0.7923588883076249, 0.8392444575420913, 0.8198833060875044, 0.8212126545708575, 0.29434139018538685, 0.3123626389396169, 0.30186686240248106, 0.2874930354284788, 0.3068629297251705, 0.31850479699740764, 0.3047360946092291, 0.29278049981301224, 0.2908456458736426, 0.6025718333617062, 0.6339135170273578, 0.6508398244138041, 0.6965446803417514, 0.6906740937785669, 0.7174408358109926, 0.5940852869887213, 0.6783506176873422, 0.6811456619791714, 0.15823263073951532, 0.17302018699109545, 0.15594428722420817, 0.17558451571386335, 0.17561536775723385, 0.15809179826570396, 0.21472998749184602, 0.17707332186825775, 0.19109551736542407, 0.16225281817947068, 0.20818052143918708, 0.21651353916761817, 0.14831457892810218, 0.14609567603485052, 0.1881281387078989, 0.1917962656731519, 0.18510741135935527, 0.19266414250142805, 0.22880448418478894, 0.25785518995641377, 0.2294908236045966, 0.25500400517482014, 0.22647144619803894, 0.23550231247388287, 0.2777125024771473, 0.25795467466036825, 0.27889788354193956, 0.4150073951068708, 0.38650237833667267, 0.41113601522028553, 0.37542306503731226, 0.3559745373718426, 0.3970341659964144, 0.4317183304247647, 0.3967894886804274, 0.4340769375037318, 0.06242014231938375, 0.09354315864617402, 0.07067387339921594, 0.08400294488770377, 0.11833950501690216, 0.10077402993501161, 0.09370803313872333, 0.12221732091880999, 0.08024160688436277, 0.22828075718146634, 0.24180292043562712, 0.2328541840585665, 0.23202483494383552, 0.251365028487453, 0.2505888487499658, 0.23054197007760147, 0.21701185373292664, 0.22609771422827485, 0.5924477920139367, 0.6210678332168432, 0.6104772360712973, 0.6052907910673004, 0.5997739653493664, 0.589640176437815, 0.5979099788813487, 0.5966640962152151, 0.6166782815940279, 0.09752546476732171, 0.09937685531281004, 0.10996074372893183, 0.10274024505074031, 0.09859495892451142, 0.10452346706467852, 0.10710691390287264, 0.09841908862127324, 0.11192875696784099, 0.23502191320190702, 0.19631944227857256, 0.22347727501773806, 0.18113819950780297, 0.19565278061161584, 0.1797992857986923, 0.17877396380788946, 0.18681065232903027, 0.191389141720263, 0.30232204017571807, 0.2959501605053766, 0.30152809566486727, 0.28130769650788723, 0.32919961587628854, 0.3305227784707946, 0.31275892568876695, 0.3165322686317974, 0.28979682685706953, 0.24827427704396665, 0.23093505158660432, 0.2594055974754842, 0.25210295943353644, 0.21923790493104356, 0.24393981938249887, 0.2300707792953396, 0.22601628380533567, 0.23239050073096268, 0.2594790623382641, 0.18616309244905138, 0.18915010613429173, 0.24487865852420032, 0.2230229290647533, 0.19037497390511882, 0.2253811738736221, 0.20861102416675636, 0.2233488851712998, 0.18900219295325926, 0.18813981232459376, 0.18934581328038047, 0.18654489100349447, 0.1818906111332349, 0.17687756888756978, 0.1783111424550976, 0.1851468180878154, 0.18065771304982714, 0.3718598576408896, 0.19053569930465486, 0.3862227069133459, 0.48156388362900493, 0.38444267349423256, 0.3858204004136184, 0.3585103296322937, 0.3991388602973287, 0.39193869023723515, 0.4803181578117822, 0.450340199123181, 0.5271602953962041, 0.41729732562008315, 0.2911276291248619, 0.296828550604182, 0.24139133961837156, 0.3048764954013541, 0.37844137315853765, 0.1867089180184116, 0.17858728756839282, 0.20526668097413636, 0.17831760065518032, 0.18803087931421614, 0.18642875727089547, 0.20050385717229147, 0.19402823806105585, 0.18344575801916863, 0.08131838290630022, 0.07740204516747451, 0.08691878709876888, 0.08466118225705233, 0.07959447547763376, 0.07562440723127106, 0.07590583131557227, 0.08107239065208072, 0.08208051649675163]}, "mutation_prompt": null}
{"id": "664fc480-8f1b-4736-9b82-f5e42eed5e86", "solution": "import numpy as np\n\nclass EnhancedProbabilisticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = 40\n        self.mutation_factor = 0.85\n        self.crossover_probability = 0.95\n        self.learning_rate = 0.2\n        self.diversity_factor = 0.1\n        self.population = None\n        self.fitness = None\n\n    def initialize_population(self):\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, \n                                            (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def adaptive_differential_evolution(self, idx, func):\n        indices = [i for i in range(self.population_size) if i != idx]\n        np.random.shuffle(indices)\n        a, b, c = indices[:3]\n        mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        adaptive_factor = 0.4 + 0.6 * np.random.rand()\n        mutant = self.population[idx] + adaptive_factor * (mutant - self.population[idx])\n        mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n        \n        cross_points = np.random.rand(self.dim) < self.crossover_probability\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        \n        trial = np.where(cross_points, mutant, self.population[idx])\n        trial_fitness = func(trial)\n        \n        if trial_fitness < self.fitness[idx]:\n            self.population[idx] = trial\n            self.fitness[idx] = trial_fitness\n\n    def stochastic_gradient_approx(self, idx, func):\n        gradient = np.random.normal(0, 1, self.dim)\n        adaptive_lr = self.learning_rate / (1 + idx / self.population_size)\n        candidate = self.population[idx] - adaptive_lr * gradient\n        candidate += self.diversity_factor * np.random.normal(0, 1, self.dim)\n        candidate = np.clip(candidate, self.lower_bound, self.upper_bound)\n        candidate_fitness = func(candidate)\n        if candidate_fitness < self.fitness[idx]:\n            self.population[idx] = candidate\n            self.fitness[idx] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population()\n        evaluations = 0\n        self.evaluate_population(func)\n        evaluations += self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                self.adaptive_differential_evolution(i, func)\n                evaluations += 1\n\n                if evaluations >= self.budget:\n                    break\n\n                self.stochastic_gradient_approx(i, func)\n                evaluations += 1\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "EnhancedProbabilisticOptimizer", "description": "Enhanced probabilistic hybrid optimizer balancing exploration and exploitation through diversity-oriented selection and adaptive parameter tuning for optimal convergence.", "configspace": "", "generation": 38, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "1459935d-0a5b-47de-8bfe-69f52cb3cb6c", "metadata": {"aucs": [0.796727445346743, 0.7946092935536371, 0.8025157468331956, 0.7795890428061506, 0.7652866403837684, 0.7888980264133363, 0.7851539828124771, 0.7801166288092796, 0.778684214476141, 0.6066596860215113, 0.5509488918434079, 0.584709500115218, 0.6151880393982149, 0.6128285196462557, 0.5928554548480235, 0.18781852246903208, 0.6051102165828588, 0.6050716765741883, 0.12056849613284826, 0.10627786970516262, 0.11092740344636509, 0.12458142503345515, 0.10225070914012868, 0.10214739878052792, 0.11007052847089493, 0.11303443664409152, 0.11346829363668931, 0.09058514750802149, 0.09136224141572435, 0.09346304265481342, 0.08413688806219866, 0.10560994817344638, 0.09386944951658027, 0.1066829182603829, 0.09525027475772718, 0.09078080677184064, 0.8379701410373176, 0.8069154658397715, 0.8108437032002754, 0.8202640448333322, 0.7815128713795206, 0.8451084830274826, 0.836376388134602, 0.8441441978939892, 0.8509693104465095, 0.37150586919602946, 0.34273942956965175, 0.3313822553845982, 0.34968755932637086, 0.32960730205573996, 0.3754984208710611, 0.38284578850911233, 0.3840456442426208, 0.3280564413760412, 0.7576709220936438, 0.7335556841917643, 0.7658090217912067, 0.7934167558179147, 0.8025617081483369, 0.7999764269594076, 0.7299292035530152, 0.7697788108067551, 0.7651252822797626, 0.16167439560696928, 0.18696031846775318, 0.21158580997319565, 0.244349141547606, 0.2890559956537958, 0.16298570563996873, 0.21750653437837209, 0.21122767559906996, 0.22550289679700497, 0.15305768362463734, 0.14884944831507896, 0.18600965489753707, 0.1541357033150358, 0.19832744782165168, 0.18983314847500432, 0.1953210887165725, 0.20165646735698872, 0.13088573983753993, 0.489476775395826, 0.460206761694571, 0.280690722270316, 0.34064732845138956, 0.522953329166158, 0.46452239779932414, 0.4196789194960461, 0.4727221489210359, 0.44131563048738687, 0.6540713674624187, 0.6128130384068273, 0.5553131009305013, 0.5776200055109268, 0.4723536786910414, 0.6219680956281209, 0.6455804612075908, 0.6109257481083044, 0.5996992464126676, 0.15220127888973134, 0.11018658007737869, 0.2413715712685326, 0.2296484097510183, 0.2572737636354798, 0.10622686325653308, 0.14604068872930986, 0.14809138800719435, 0.12257392403735012, 0.36655256082417187, 0.34632185847096675, 0.3516395670957846, 0.33828319802853724, 0.3572041238804262, 0.35621925345698524, 0.34628041224843775, 0.36205674625584605, 0.3801826028652373, 0.7284525303733058, 0.7334889541026953, 0.7386557191632159, 0.7055754468414382, 0.7222856768071027, 0.7194774022217179, 0.518854950686749, 0.7321096452535195, 0.7397060954282213, 0.10374399773694243, 0.104282622260642, 0.1086381702881759, 0.11429613558107654, 0.11511930056195652, 0.10035386421161252, 0.09599830676572241, 0.11347281801819442, 0.11470225552281121, 0.14952999643648723, 0.16817216547016145, 0.17310734903563885, 0.1855986179207637, 0.18359480037739373, 0.19508916572226054, 0.20087729718879477, 0.17807185486405164, 0.20119801391244463, 0.3682121014468256, 0.35073539948175425, 0.3330439038672147, 0.4157077631363848, 0.37474021495439724, 0.36760475650845414, 0.38008256501576043, 0.39084781839078786, 0.3786757352451906, 0.30037946434366947, 0.2861866860206934, 0.2891587957817804, 0.2835627691001067, 0.2779602895144492, 0.28387893685888044, 0.29400381025955125, 0.3244615423116647, 0.30188351717337136, 0.20494558183242018, 0.19905376550038034, 0.20406514428030742, 0.2229753496453719, 0.1980660060283188, 0.21539123082194234, 0.21928087314384304, 0.20854328449838244, 0.2102975078755156, 0.18082128476991477, 0.1831737258940077, 0.18089956975930732, 0.18277323416104252, 0.1792695777259361, 0.17816092407226292, 0.19628636488975992, 0.19237612183491615, 0.1874718333986961, 0.6294666203187917, 0.602690555999032, 0.5454952462675013, 0.7727337342883757, 0.25677125313719296, 0.588947972014151, 0.5053651729784945, 0.42433758940449484, 0.610090527727432, 0.7148365734773846, 0.39316249107203016, 0.4609914040189693, 0.5414475146943718, 0.3738614312609654, 0.2972834077003401, 0.2719039283446757, 0.4464259994844204, 0.6865015215956998, 0.18541089636272012, 0.17521600614441235, 0.18498258988690697, 0.1923345960637889, 0.19442272937612437, 0.18424555156947642, 0.17449564646480575, 0.18372628686318526, 0.18823517758491137, 0.07953923390817585, 0.08429672806217459, 0.08351517483047155, 0.07872531348008505, 0.0885755282031725, 0.09071254814221597, 0.07022751225803625, 0.07907174920771143, 0.08022853716110878]}, "mutation_prompt": null}
{"id": "7eec99d2-7a0a-4d6c-8b55-65be366d18a6", "solution": "import numpy as np\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = 50  # Changed from 40\n        self.mutation_factor = 0.9  # Changed from 0.85\n        self.crossover_probability = 0.9  # Changed from 0.95\n        self.learning_rate = 0.15  # Changed from 0.2\n        self.diversity_factor = 0.15  # Changed from 0.1\n        self.population = None\n        self.fitness = None\n\n    def initialize_population(self):\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, \n                                            (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def adaptive_differential_evolution(self, idx, func):\n        indices = [i for i in range(self.population_size) if i != idx]\n        np.random.shuffle(indices)\n        a, b, c = indices[:3]\n        mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        adaptive_factor = 0.5 + 0.5 * np.random.rand()  # Changed adaptive factor calculation\n        mutant = self.population[idx] + adaptive_factor * (mutant - self.population[idx])\n        mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n        \n        cross_points = np.random.rand(self.dim) < self.crossover_probability\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        \n        trial = np.where(cross_points, mutant, self.population[idx])\n        trial_fitness = func(trial)\n        \n        if trial_fitness < self.fitness[idx]:\n            self.population[idx] = trial\n            self.fitness[idx] = trial_fitness\n\n    def stochastic_gradient_approx(self, idx, func):\n        gradient = np.random.normal(0, 1, self.dim)\n        adaptive_lr = self.learning_rate / (1 + np.sqrt(idx + 1))  # Changed learning rate adaptation\n        candidate = self.population[idx] - adaptive_lr * gradient\n        candidate += self.diversity_factor * np.random.normal(0, 1, self.dim)\n        candidate = np.clip(candidate, self.lower_bound, self.upper_bound)\n        candidate_fitness = func(candidate)\n        if candidate_fitness < self.fitness[idx]:\n            self.population[idx] = candidate\n            self.fitness[idx] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population()\n        evaluations = 0\n        self.evaluate_population(func)\n        evaluations += self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                self.adaptive_differential_evolution(i, func)\n                evaluations += 1\n\n                if evaluations >= self.budget:\n                    break\n\n                self.stochastic_gradient_approx(i, func)\n                evaluations += 1\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "AdaptiveHybridOptimizer", "description": "Adaptive Hybrid Optimizer integrating differential evolution and stochastic updates with dynamic learning rates and diversity promotion.", "configspace": "", "generation": 66, "fitness": 0.2861952845509317, "feedback": "The algorithm AdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.29 with standard deviation 0.20.", "error": "", "parent_id": "1459935d-0a5b-47de-8bfe-69f52cb3cb6c", "metadata": {"aucs": [0.6861657419957004, 0.6795796747338481, 0.6650295269788529, 0.6746992210089068, 0.6692332771683727, 0.6590662386577553, 0.664158544763774, 0.6894659772374727, 0.6867034530735635, 0.4083829779460286, 0.40238860472751525, 0.3690431455356634, 0.3896104730512058, 0.3990263409319287, 0.36348908839851735, 0.3759040669570479, 0.37827692440581595, 0.3827505322390643, 0.1059892898391307, 0.09749083477001064, 0.12014504177085095, 0.10340547036090464, 0.09999307159243853, 0.11823720641488122, 0.1168319029796181, 0.1402985283417052, 0.1109744344971546, 0.09260151076703182, 0.09069173920308926, 0.08016223414801771, 0.08903493688497088, 0.10218777076505448, 0.08740868691999915, 0.09356238251054694, 0.09546699771491263, 0.07981214921582469, 0.9328210182229829, 0.8989410320120496, 0.95953818185629, 0.8447294928581109, 0.7813767952435993, 0.8780395788742583, 0.7897827081120072, 0.8019397490219655, 0.8226573922097236, 0.2884302762680463, 0.3013863155934777, 0.3024927151280835, 0.30689597168396, 0.2940960565924281, 0.2918678390592575, 0.27829347352055234, 0.27798732010503724, 0.30625420786160784, 0.585185450053128, 0.6390373510471129, 0.6300836272529674, 0.6625580126058125, 0.6316889504856902, 0.6271680906919361, 0.6379813648549497, 0.6234318579979782, 0.6500330597472592, 0.16539219544537243, 0.13824839889541474, 0.17692076417230762, 0.18348386761872482, 0.1641786789667925, 0.1916467010245828, 0.16931809071264325, 0.17293545818804046, 0.19213611643559447, 0.17356507440161073, 0.16466322925948862, 0.2008274830497132, 0.17618293838158006, 0.20661540918640453, 0.18084055831386114, 0.16806296138544907, 0.14678356212364885, 0.1855827626376485, 0.1698740880821702, 0.18075535211599725, 0.16329064560267104, 0.2214345135470892, 0.19700212212506274, 0.1705426957938322, 0.17510005340646873, 0.20083263289097342, 0.2081192828109233, 0.32265182115841573, 0.33969222897813356, 0.2844657639002912, 0.3337605713165639, 0.3091400155694849, 0.2887789475068384, 0.3365840059034475, 0.30681415826666303, 0.3511443591588872, 0.04346852499593168, 0.03840160881924548, 0.054579035070819515, 0.09074945230262932, 0.07062296919807942, 0.09487805097598012, 0.06661349656412108, 0.043778770986563686, 0.0612921004585838, 0.18209958087164302, 0.1827664957897639, 0.18688706272148037, 0.19209546516861342, 0.20511941510350606, 0.21261863649665125, 0.1856026383162298, 0.18262250523675005, 0.19648905747934298, 0.5394528695771881, 0.5376678678937813, 0.5744092871991118, 0.540142997923352, 0.5447854117086826, 0.5310409764001887, 0.5667682612325398, 0.5492764266472907, 0.5290374394363155, 0.10909320983136972, 0.09461126729449221, 0.11252774955457934, 0.1042497331839708, 0.11369028554884275, 0.09697338325723215, 0.10053616687386235, 0.10358141900839424, 0.08971781030997794, 0.17720163499434727, 0.19365827404095126, 0.1773154715983124, 0.19170399018800277, 0.19267339672328976, 0.21142243884031353, 0.17460698563019128, 0.19767948452319228, 0.1768466992758425, 0.28929554657203393, 0.27927407713215924, 0.2860909240056031, 0.2663726802117027, 0.2832590993994015, 0.2891297080445081, 0.29249563436679904, 0.28656122282810925, 0.2942463047667825, 0.21522145524410818, 0.22257918012909916, 0.2324601881811026, 0.20986808262709844, 0.20511708730840172, 0.21955281060241527, 0.21954876209049812, 0.24574028075887266, 0.21634882822387957, 0.21936759109149018, 0.1958939510062041, 0.18816473060406147, 0.20869476125137376, 0.18931203628737392, 0.1949228171523244, 0.2340847318274989, 0.1834737362568739, 0.2007571698220545, 0.18597452899614586, 0.17935416308511143, 0.19091555377946734, 0.1844766775141069, 0.18470366086821355, 0.18601860075080567, 0.17569929804592954, 0.19086218975256575, 0.17712664027433633, 0.30584726344649515, 0.37166308086847033, 0.5370900541343695, 0.41535099499404304, 0.30952240315566404, 0.389754590055409, 0.17755462512237918, 0.38113569231221667, 0.43934215096416274, 0.38469473870727955, 0.3684040875220458, 0.5923752648755912, 0.3079006787591212, 0.2654085356951802, 0.3253443517015586, 0.20435922982285715, 0.3087407109535585, 0.3490902717626799, 0.17940938415303154, 0.17843885794889192, 0.1908127464930559, 0.18066713081222752, 0.20204915328889383, 0.19275317362342748, 0.17507668399509912, 0.20424571165658467, 0.18926404436915778, 0.0870582287002244, 0.07378510191477439, 0.08164536740922035, 0.07383264436607329, 0.07068807378206043, 0.08110274287956787, 0.08121029965158122, 0.08014288373678902, 0.08876659745718574]}, "mutation_prompt": null}
{"id": "b96a2115-91bb-4254-9687-7e89bba795e8", "solution": "import numpy as np\n\nclass EnhancedProbabilisticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = 40\n        self.mutation_factor = 0.85\n        self.crossover_probability = 0.95\n        self.learning_rate = 0.2\n        self.diversity_factor = 0.1\n        self.population = None\n        self.fitness = None\n\n    def initialize_population(self):\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, \n                                            (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def adaptive_differential_evolution(self, idx, func):\n        indices = [i for i in range(self.population_size) if i != idx]\n        np.random.shuffle(indices)\n        a, b, c = indices[:3]\n        mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        adaptive_factor = 0.4 + 0.6 * np.random.rand()\n        mutant = self.population[idx] + adaptive_factor * (mutant - self.population[idx])\n        mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n        \n        cross_points = np.random.rand(self.dim) < self.crossover_probability\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        \n        trial = np.where(cross_points, mutant, self.population[idx])\n        trial_fitness = func(trial)\n        \n        if trial_fitness < self.fitness[idx]:\n            self.population[idx] = trial\n            self.fitness[idx] = trial_fitness\n\n    def stochastic_gradient_approx(self, idx, func):\n        gradient = np.random.normal(0, 1, self.dim)\n        adaptive_lr = self.learning_rate / (1 + idx / self.population_size)\n        candidate = self.population[idx] - adaptive_lr * gradient\n        candidate += self.diversity_factor * np.random.normal(0, 1, self.dim)\n        candidate = np.clip(candidate, self.lower_bound, self.upper_bound)\n        candidate_fitness = func(candidate)\n        if candidate_fitness < self.fitness[idx]:\n            self.population[idx] = candidate\n            self.fitness[idx] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population()\n        evaluations = 0\n        self.evaluate_population(func)\n        evaluations += self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                self.adaptive_differential_evolution(i, func)\n                evaluations += 1\n\n                if evaluations >= self.budget:\n                    break\n\n                self.stochastic_gradient_approx(i, func)\n                evaluations += 1\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "EnhancedProbabilisticOptimizer", "description": "Enhanced probabilistic hybrid optimizer balancing exploration and exploitation through diversity-oriented selection and adaptive parameter tuning for optimal convergence.", "configspace": "", "generation": 38, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "1459935d-0a5b-47de-8bfe-69f52cb3cb6c", "metadata": {"aucs": [0.796727445346743, 0.7946092935536371, 0.8025157468331956, 0.7795890428061506, 0.7652866403837684, 0.7888980264133363, 0.7851539828124771, 0.7801166288092796, 0.778684214476141, 0.6066596860215113, 0.5509488918434079, 0.584709500115218, 0.6151880393982149, 0.6128285196462557, 0.5928554548480235, 0.18781852246903208, 0.6051102165828588, 0.6050716765741883, 0.12056849613284826, 0.10627786970516262, 0.11092740344636509, 0.12458142503345515, 0.10225070914012868, 0.10214739878052792, 0.11007052847089493, 0.11303443664409152, 0.11346829363668931, 0.09058514750802149, 0.09136224141572435, 0.09346304265481342, 0.08413688806219866, 0.10560994817344638, 0.09386944951658027, 0.1066829182603829, 0.09525027475772718, 0.09078080677184064, 0.8379701410373176, 0.8069154658397715, 0.8108437032002754, 0.8202640448333322, 0.7815128713795206, 0.8451084830274826, 0.836376388134602, 0.8441441978939892, 0.8509693104465095, 0.37150586919602946, 0.34273942956965175, 0.3313822553845982, 0.34968755932637086, 0.32960730205573996, 0.3754984208710611, 0.38284578850911233, 0.3840456442426208, 0.3280564413760412, 0.7576709220936438, 0.7335556841917643, 0.7658090217912067, 0.7934167558179147, 0.8025617081483369, 0.7999764269594076, 0.7299292035530152, 0.7697788108067551, 0.7651252822797626, 0.16167439560696928, 0.18696031846775318, 0.21158580997319565, 0.244349141547606, 0.2890559956537958, 0.16298570563996873, 0.21750653437837209, 0.21122767559906996, 0.22550289679700497, 0.15305768362463734, 0.14884944831507896, 0.18600965489753707, 0.1541357033150358, 0.19832744782165168, 0.18983314847500432, 0.1953210887165725, 0.20165646735698872, 0.13088573983753993, 0.489476775395826, 0.460206761694571, 0.280690722270316, 0.34064732845138956, 0.522953329166158, 0.46452239779932414, 0.4196789194960461, 0.4727221489210359, 0.44131563048738687, 0.6540713674624187, 0.6128130384068273, 0.5553131009305013, 0.5776200055109268, 0.4723536786910414, 0.6219680956281209, 0.6455804612075908, 0.6109257481083044, 0.5996992464126676, 0.15220127888973134, 0.11018658007737869, 0.2413715712685326, 0.2296484097510183, 0.2572737636354798, 0.10622686325653308, 0.14604068872930986, 0.14809138800719435, 0.12257392403735012, 0.36655256082417187, 0.34632185847096675, 0.3516395670957846, 0.33828319802853724, 0.3572041238804262, 0.35621925345698524, 0.34628041224843775, 0.36205674625584605, 0.3801826028652373, 0.7284525303733058, 0.7334889541026953, 0.7386557191632159, 0.7055754468414382, 0.7222856768071027, 0.7194774022217179, 0.518854950686749, 0.7321096452535195, 0.7397060954282213, 0.10374399773694243, 0.104282622260642, 0.1086381702881759, 0.11429613558107654, 0.11511930056195652, 0.10035386421161252, 0.09599830676572241, 0.11347281801819442, 0.11470225552281121, 0.14952999643648723, 0.16817216547016145, 0.17310734903563885, 0.1855986179207637, 0.18359480037739373, 0.19508916572226054, 0.20087729718879477, 0.17807185486405164, 0.20119801391244463, 0.3682121014468256, 0.35073539948175425, 0.3330439038672147, 0.4157077631363848, 0.37474021495439724, 0.36760475650845414, 0.38008256501576043, 0.39084781839078786, 0.3786757352451906, 0.30037946434366947, 0.2861866860206934, 0.2891587957817804, 0.2835627691001067, 0.2779602895144492, 0.28387893685888044, 0.29400381025955125, 0.3244615423116647, 0.30188351717337136, 0.20494558183242018, 0.19905376550038034, 0.20406514428030742, 0.2229753496453719, 0.1980660060283188, 0.21539123082194234, 0.21928087314384304, 0.20854328449838244, 0.2102975078755156, 0.18082128476991477, 0.1831737258940077, 0.18089956975930732, 0.18277323416104252, 0.1792695777259361, 0.17816092407226292, 0.19628636488975992, 0.19237612183491615, 0.1874718333986961, 0.6294666203187917, 0.602690555999032, 0.5454952462675013, 0.7727337342883757, 0.25677125313719296, 0.588947972014151, 0.5053651729784945, 0.42433758940449484, 0.610090527727432, 0.7148365734773846, 0.39316249107203016, 0.4609914040189693, 0.5414475146943718, 0.3738614312609654, 0.2972834077003401, 0.2719039283446757, 0.4464259994844204, 0.6865015215956998, 0.18541089636272012, 0.17521600614441235, 0.18498258988690697, 0.1923345960637889, 0.19442272937612437, 0.18424555156947642, 0.17449564646480575, 0.18372628686318526, 0.18823517758491137, 0.07953923390817585, 0.08429672806217459, 0.08351517483047155, 0.07872531348008505, 0.0885755282031725, 0.09071254814221597, 0.07022751225803625, 0.07907174920771143, 0.08022853716110878]}, "mutation_prompt": null}
{"id": "ac8b1153-6a35-4265-a057-058e87dcb847", "solution": "import numpy as np\n\nclass EnhancedProbabilisticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = 40\n        self.mutation_factor = 0.85\n        self.crossover_probability = 0.95\n        self.learning_rate = 0.2\n        self.diversity_factor = 0.1\n        self.population = None\n        self.fitness = None\n\n    def initialize_population(self):\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, \n                                            (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def adaptive_differential_evolution(self, idx, func):\n        indices = [i for i in range(self.population_size) if i != idx]\n        np.random.shuffle(indices)\n        a, b, c = indices[:3]\n        mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        adaptive_factor = 0.4 + 0.6 * np.random.rand()\n        mutant = self.population[idx] + adaptive_factor * (mutant - self.population[idx])\n        mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n        \n        cross_points = np.random.rand(self.dim) < self.crossover_probability\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        \n        trial = np.where(cross_points, mutant, self.population[idx])\n        trial_fitness = func(trial)\n        \n        if trial_fitness < self.fitness[idx]:\n            self.population[idx] = trial\n            self.fitness[idx] = trial_fitness\n\n    def stochastic_gradient_approx(self, idx, func):\n        gradient = np.random.normal(0, 1, self.dim)\n        adaptive_lr = self.learning_rate / (1 + idx / self.population_size)\n        candidate = self.population[idx] - adaptive_lr * gradient\n        candidate += self.diversity_factor * np.random.normal(0, 1, self.dim)\n        candidate = np.clip(candidate, self.lower_bound, self.upper_bound)\n        candidate_fitness = func(candidate)\n        if candidate_fitness < self.fitness[idx]:\n            self.population[idx] = candidate\n            self.fitness[idx] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population()\n        evaluations = 0\n        self.evaluate_population(func)\n        evaluations += self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                self.adaptive_differential_evolution(i, func)\n                evaluations += 1\n\n                if evaluations >= self.budget:\n                    break\n\n                self.stochastic_gradient_approx(i, func)\n                evaluations += 1\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "EnhancedProbabilisticOptimizer", "description": "Enhanced probabilistic hybrid optimizer balancing exploration and exploitation through diversity-oriented selection and adaptive parameter tuning for optimal convergence.", "configspace": "", "generation": 38, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "1459935d-0a5b-47de-8bfe-69f52cb3cb6c", "metadata": {"aucs": [0.796727445346743, 0.7946092935536371, 0.8025157468331956, 0.7795890428061506, 0.7652866403837684, 0.7888980264133363, 0.7851539828124771, 0.7801166288092796, 0.778684214476141, 0.6066596860215113, 0.5509488918434079, 0.584709500115218, 0.6151880393982149, 0.6128285196462557, 0.5928554548480235, 0.18781852246903208, 0.6051102165828588, 0.6050716765741883, 0.12056849613284826, 0.10627786970516262, 0.11092740344636509, 0.12458142503345515, 0.10225070914012868, 0.10214739878052792, 0.11007052847089493, 0.11303443664409152, 0.11346829363668931, 0.09058514750802149, 0.09136224141572435, 0.09346304265481342, 0.08413688806219866, 0.10560994817344638, 0.09386944951658027, 0.1066829182603829, 0.09525027475772718, 0.09078080677184064, 0.8379701410373176, 0.8069154658397715, 0.8108437032002754, 0.8202640448333322, 0.7815128713795206, 0.8451084830274826, 0.836376388134602, 0.8441441978939892, 0.8509693104465095, 0.37150586919602946, 0.34273942956965175, 0.3313822553845982, 0.34968755932637086, 0.32960730205573996, 0.3754984208710611, 0.38284578850911233, 0.3840456442426208, 0.3280564413760412, 0.7576709220936438, 0.7335556841917643, 0.7658090217912067, 0.7934167558179147, 0.8025617081483369, 0.7999764269594076, 0.7299292035530152, 0.7697788108067551, 0.7651252822797626, 0.16167439560696928, 0.18696031846775318, 0.21158580997319565, 0.244349141547606, 0.2890559956537958, 0.16298570563996873, 0.21750653437837209, 0.21122767559906996, 0.22550289679700497, 0.15305768362463734, 0.14884944831507896, 0.18600965489753707, 0.1541357033150358, 0.19832744782165168, 0.18983314847500432, 0.1953210887165725, 0.20165646735698872, 0.13088573983753993, 0.489476775395826, 0.460206761694571, 0.280690722270316, 0.34064732845138956, 0.522953329166158, 0.46452239779932414, 0.4196789194960461, 0.4727221489210359, 0.44131563048738687, 0.6540713674624187, 0.6128130384068273, 0.5553131009305013, 0.5776200055109268, 0.4723536786910414, 0.6219680956281209, 0.6455804612075908, 0.6109257481083044, 0.5996992464126676, 0.15220127888973134, 0.11018658007737869, 0.2413715712685326, 0.2296484097510183, 0.2572737636354798, 0.10622686325653308, 0.14604068872930986, 0.14809138800719435, 0.12257392403735012, 0.36655256082417187, 0.34632185847096675, 0.3516395670957846, 0.33828319802853724, 0.3572041238804262, 0.35621925345698524, 0.34628041224843775, 0.36205674625584605, 0.3801826028652373, 0.7284525303733058, 0.7334889541026953, 0.7386557191632159, 0.7055754468414382, 0.7222856768071027, 0.7194774022217179, 0.518854950686749, 0.7321096452535195, 0.7397060954282213, 0.10374399773694243, 0.104282622260642, 0.1086381702881759, 0.11429613558107654, 0.11511930056195652, 0.10035386421161252, 0.09599830676572241, 0.11347281801819442, 0.11470225552281121, 0.14952999643648723, 0.16817216547016145, 0.17310734903563885, 0.1855986179207637, 0.18359480037739373, 0.19508916572226054, 0.20087729718879477, 0.17807185486405164, 0.20119801391244463, 0.3682121014468256, 0.35073539948175425, 0.3330439038672147, 0.4157077631363848, 0.37474021495439724, 0.36760475650845414, 0.38008256501576043, 0.39084781839078786, 0.3786757352451906, 0.30037946434366947, 0.2861866860206934, 0.2891587957817804, 0.2835627691001067, 0.2779602895144492, 0.28387893685888044, 0.29400381025955125, 0.3244615423116647, 0.30188351717337136, 0.20494558183242018, 0.19905376550038034, 0.20406514428030742, 0.2229753496453719, 0.1980660060283188, 0.21539123082194234, 0.21928087314384304, 0.20854328449838244, 0.2102975078755156, 0.18082128476991477, 0.1831737258940077, 0.18089956975930732, 0.18277323416104252, 0.1792695777259361, 0.17816092407226292, 0.19628636488975992, 0.19237612183491615, 0.1874718333986961, 0.6294666203187917, 0.602690555999032, 0.5454952462675013, 0.7727337342883757, 0.25677125313719296, 0.588947972014151, 0.5053651729784945, 0.42433758940449484, 0.610090527727432, 0.7148365734773846, 0.39316249107203016, 0.4609914040189693, 0.5414475146943718, 0.3738614312609654, 0.2972834077003401, 0.2719039283446757, 0.4464259994844204, 0.6865015215956998, 0.18541089636272012, 0.17521600614441235, 0.18498258988690697, 0.1923345960637889, 0.19442272937612437, 0.18424555156947642, 0.17449564646480575, 0.18372628686318526, 0.18823517758491137, 0.07953923390817585, 0.08429672806217459, 0.08351517483047155, 0.07872531348008505, 0.0885755282031725, 0.09071254814221597, 0.07022751225803625, 0.07907174920771143, 0.08022853716110878]}, "mutation_prompt": null}
{"id": "0eccc210-8cf9-4a6f-97d7-5471b9aa6a1d", "solution": "import numpy as np\n\nclass EnhancedProbabilisticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = 40\n        self.mutation_factor = 0.85\n        self.crossover_probability = 0.95\n        self.learning_rate = 0.2\n        self.diversity_factor = 0.1\n        self.population = None\n        self.fitness = None\n\n    def initialize_population(self):\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, \n                                            (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def adaptive_differential_evolution(self, idx, func):\n        indices = [i for i in range(self.population_size) if i != idx]\n        np.random.shuffle(indices)\n        a, b, c = indices[:3]\n        mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        adaptive_factor = 0.4 + 0.6 * np.random.rand()\n        mutant = self.population[idx] + adaptive_factor * (mutant - self.population[idx])\n        mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n        \n        cross_points = np.random.rand(self.dim) < self.crossover_probability\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        \n        trial = np.where(cross_points, mutant, self.population[idx])\n        trial_fitness = func(trial)\n        \n        if trial_fitness < self.fitness[idx]:\n            self.population[idx] = trial\n            self.fitness[idx] = trial_fitness\n\n    def stochastic_gradient_approx(self, idx, func):\n        gradient = np.random.normal(0, 1, self.dim)\n        adaptive_lr = self.learning_rate / (1 + idx / self.population_size)\n        candidate = self.population[idx] - adaptive_lr * gradient\n        candidate += self.diversity_factor * np.random.normal(0, 1, self.dim)\n        candidate = np.clip(candidate, self.lower_bound, self.upper_bound)\n        candidate_fitness = func(candidate)\n        if candidate_fitness < self.fitness[idx]:\n            self.population[idx] = candidate\n            self.fitness[idx] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population()\n        evaluations = 0\n        self.evaluate_population(func)\n        evaluations += self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                self.adaptive_differential_evolution(i, func)\n                evaluations += 1\n\n                if evaluations >= self.budget:\n                    break\n\n                self.stochastic_gradient_approx(i, func)\n                evaluations += 1\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "EnhancedProbabilisticOptimizer", "description": "Enhanced probabilistic hybrid optimizer balancing exploration and exploitation through diversity-oriented selection and adaptive parameter tuning for optimal convergence.", "configspace": "", "generation": 38, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "1459935d-0a5b-47de-8bfe-69f52cb3cb6c", "metadata": {"aucs": [0.796727445346743, 0.7946092935536371, 0.8025157468331956, 0.7795890428061506, 0.7652866403837684, 0.7888980264133363, 0.7851539828124771, 0.7801166288092796, 0.778684214476141, 0.6066596860215113, 0.5509488918434079, 0.584709500115218, 0.6151880393982149, 0.6128285196462557, 0.5928554548480235, 0.18781852246903208, 0.6051102165828588, 0.6050716765741883, 0.12056849613284826, 0.10627786970516262, 0.11092740344636509, 0.12458142503345515, 0.10225070914012868, 0.10214739878052792, 0.11007052847089493, 0.11303443664409152, 0.11346829363668931, 0.09058514750802149, 0.09136224141572435, 0.09346304265481342, 0.08413688806219866, 0.10560994817344638, 0.09386944951658027, 0.1066829182603829, 0.09525027475772718, 0.09078080677184064, 0.8379701410373176, 0.8069154658397715, 0.8108437032002754, 0.8202640448333322, 0.7815128713795206, 0.8451084830274826, 0.836376388134602, 0.8441441978939892, 0.8509693104465095, 0.37150586919602946, 0.34273942956965175, 0.3313822553845982, 0.34968755932637086, 0.32960730205573996, 0.3754984208710611, 0.38284578850911233, 0.3840456442426208, 0.3280564413760412, 0.7576709220936438, 0.7335556841917643, 0.7658090217912067, 0.7934167558179147, 0.8025617081483369, 0.7999764269594076, 0.7299292035530152, 0.7697788108067551, 0.7651252822797626, 0.16167439560696928, 0.18696031846775318, 0.21158580997319565, 0.244349141547606, 0.2890559956537958, 0.16298570563996873, 0.21750653437837209, 0.21122767559906996, 0.22550289679700497, 0.15305768362463734, 0.14884944831507896, 0.18600965489753707, 0.1541357033150358, 0.19832744782165168, 0.18983314847500432, 0.1953210887165725, 0.20165646735698872, 0.13088573983753993, 0.489476775395826, 0.460206761694571, 0.280690722270316, 0.34064732845138956, 0.522953329166158, 0.46452239779932414, 0.4196789194960461, 0.4727221489210359, 0.44131563048738687, 0.6540713674624187, 0.6128130384068273, 0.5553131009305013, 0.5776200055109268, 0.4723536786910414, 0.6219680956281209, 0.6455804612075908, 0.6109257481083044, 0.5996992464126676, 0.15220127888973134, 0.11018658007737869, 0.2413715712685326, 0.2296484097510183, 0.2572737636354798, 0.10622686325653308, 0.14604068872930986, 0.14809138800719435, 0.12257392403735012, 0.36655256082417187, 0.34632185847096675, 0.3516395670957846, 0.33828319802853724, 0.3572041238804262, 0.35621925345698524, 0.34628041224843775, 0.36205674625584605, 0.3801826028652373, 0.7284525303733058, 0.7334889541026953, 0.7386557191632159, 0.7055754468414382, 0.7222856768071027, 0.7194774022217179, 0.518854950686749, 0.7321096452535195, 0.7397060954282213, 0.10374399773694243, 0.104282622260642, 0.1086381702881759, 0.11429613558107654, 0.11511930056195652, 0.10035386421161252, 0.09599830676572241, 0.11347281801819442, 0.11470225552281121, 0.14952999643648723, 0.16817216547016145, 0.17310734903563885, 0.1855986179207637, 0.18359480037739373, 0.19508916572226054, 0.20087729718879477, 0.17807185486405164, 0.20119801391244463, 0.3682121014468256, 0.35073539948175425, 0.3330439038672147, 0.4157077631363848, 0.37474021495439724, 0.36760475650845414, 0.38008256501576043, 0.39084781839078786, 0.3786757352451906, 0.30037946434366947, 0.2861866860206934, 0.2891587957817804, 0.2835627691001067, 0.2779602895144492, 0.28387893685888044, 0.29400381025955125, 0.3244615423116647, 0.30188351717337136, 0.20494558183242018, 0.19905376550038034, 0.20406514428030742, 0.2229753496453719, 0.1980660060283188, 0.21539123082194234, 0.21928087314384304, 0.20854328449838244, 0.2102975078755156, 0.18082128476991477, 0.1831737258940077, 0.18089956975930732, 0.18277323416104252, 0.1792695777259361, 0.17816092407226292, 0.19628636488975992, 0.19237612183491615, 0.1874718333986961, 0.6294666203187917, 0.602690555999032, 0.5454952462675013, 0.7727337342883757, 0.25677125313719296, 0.588947972014151, 0.5053651729784945, 0.42433758940449484, 0.610090527727432, 0.7148365734773846, 0.39316249107203016, 0.4609914040189693, 0.5414475146943718, 0.3738614312609654, 0.2972834077003401, 0.2719039283446757, 0.4464259994844204, 0.6865015215956998, 0.18541089636272012, 0.17521600614441235, 0.18498258988690697, 0.1923345960637889, 0.19442272937612437, 0.18424555156947642, 0.17449564646480575, 0.18372628686318526, 0.18823517758491137, 0.07953923390817585, 0.08429672806217459, 0.08351517483047155, 0.07872531348008505, 0.0885755282031725, 0.09071254814221597, 0.07022751225803625, 0.07907174920771143, 0.08022853716110878]}, "mutation_prompt": null}
{"id": "890c65e6-8b33-42ff-bf9a-323587fef442", "solution": "import numpy as np\n\nclass EnhancedProbabilisticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = 40\n        self.mutation_factor = 0.85\n        self.crossover_probability = 0.95\n        self.learning_rate = 0.2\n        self.diversity_factor = 0.1\n        self.population = None\n        self.fitness = None\n\n    def initialize_population(self):\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, \n                                            (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def adaptive_differential_evolution(self, idx, func):\n        indices = [i for i in range(self.population_size) if i != idx]\n        np.random.shuffle(indices)\n        a, b, c = indices[:3]\n        mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        adaptive_factor = 0.4 + 0.6 * np.random.rand()\n        mutant = self.population[idx] + adaptive_factor * (mutant - self.population[idx])\n        mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n        \n        cross_points = np.random.rand(self.dim) < self.crossover_probability\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        \n        trial = np.where(cross_points, mutant, self.population[idx])\n        trial_fitness = func(trial)\n        \n        if trial_fitness < self.fitness[idx]:\n            self.population[idx] = trial\n            self.fitness[idx] = trial_fitness\n\n    def stochastic_gradient_approx(self, idx, func):\n        gradient = np.random.normal(0, 1, self.dim)\n        adaptive_lr = self.learning_rate / (1 + idx / self.population_size)\n        candidate = self.population[idx] - adaptive_lr * gradient\n        candidate += self.diversity_factor * np.random.normal(0, 1, self.dim)\n        candidate = np.clip(candidate, self.lower_bound, self.upper_bound)\n        candidate_fitness = func(candidate)\n        if candidate_fitness < self.fitness[idx]:\n            self.population[idx] = candidate\n            self.fitness[idx] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population()\n        evaluations = 0\n        self.evaluate_population(func)\n        evaluations += self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                self.adaptive_differential_evolution(i, func)\n                evaluations += 1\n\n                if evaluations >= self.budget:\n                    break\n\n                self.stochastic_gradient_approx(i, func)\n                evaluations += 1\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "EnhancedProbabilisticOptimizer", "description": "Enhanced probabilistic hybrid optimizer balancing exploration and exploitation through diversity-oriented selection and adaptive parameter tuning for optimal convergence.", "configspace": "", "generation": 38, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "1459935d-0a5b-47de-8bfe-69f52cb3cb6c", "metadata": {"aucs": [0.796727445346743, 0.7946092935536371, 0.8025157468331956, 0.7795890428061506, 0.7652866403837684, 0.7888980264133363, 0.7851539828124771, 0.7801166288092796, 0.778684214476141, 0.6066596860215113, 0.5509488918434079, 0.584709500115218, 0.6151880393982149, 0.6128285196462557, 0.5928554548480235, 0.18781852246903208, 0.6051102165828588, 0.6050716765741883, 0.12056849613284826, 0.10627786970516262, 0.11092740344636509, 0.12458142503345515, 0.10225070914012868, 0.10214739878052792, 0.11007052847089493, 0.11303443664409152, 0.11346829363668931, 0.09058514750802149, 0.09136224141572435, 0.09346304265481342, 0.08413688806219866, 0.10560994817344638, 0.09386944951658027, 0.1066829182603829, 0.09525027475772718, 0.09078080677184064, 0.8379701410373176, 0.8069154658397715, 0.8108437032002754, 0.8202640448333322, 0.7815128713795206, 0.8451084830274826, 0.836376388134602, 0.8441441978939892, 0.8509693104465095, 0.37150586919602946, 0.34273942956965175, 0.3313822553845982, 0.34968755932637086, 0.32960730205573996, 0.3754984208710611, 0.38284578850911233, 0.3840456442426208, 0.3280564413760412, 0.7576709220936438, 0.7335556841917643, 0.7658090217912067, 0.7934167558179147, 0.8025617081483369, 0.7999764269594076, 0.7299292035530152, 0.7697788108067551, 0.7651252822797626, 0.16167439560696928, 0.18696031846775318, 0.21158580997319565, 0.244349141547606, 0.2890559956537958, 0.16298570563996873, 0.21750653437837209, 0.21122767559906996, 0.22550289679700497, 0.15305768362463734, 0.14884944831507896, 0.18600965489753707, 0.1541357033150358, 0.19832744782165168, 0.18983314847500432, 0.1953210887165725, 0.20165646735698872, 0.13088573983753993, 0.489476775395826, 0.460206761694571, 0.280690722270316, 0.34064732845138956, 0.522953329166158, 0.46452239779932414, 0.4196789194960461, 0.4727221489210359, 0.44131563048738687, 0.6540713674624187, 0.6128130384068273, 0.5553131009305013, 0.5776200055109268, 0.4723536786910414, 0.6219680956281209, 0.6455804612075908, 0.6109257481083044, 0.5996992464126676, 0.15220127888973134, 0.11018658007737869, 0.2413715712685326, 0.2296484097510183, 0.2572737636354798, 0.10622686325653308, 0.14604068872930986, 0.14809138800719435, 0.12257392403735012, 0.36655256082417187, 0.34632185847096675, 0.3516395670957846, 0.33828319802853724, 0.3572041238804262, 0.35621925345698524, 0.34628041224843775, 0.36205674625584605, 0.3801826028652373, 0.7284525303733058, 0.7334889541026953, 0.7386557191632159, 0.7055754468414382, 0.7222856768071027, 0.7194774022217179, 0.518854950686749, 0.7321096452535195, 0.7397060954282213, 0.10374399773694243, 0.104282622260642, 0.1086381702881759, 0.11429613558107654, 0.11511930056195652, 0.10035386421161252, 0.09599830676572241, 0.11347281801819442, 0.11470225552281121, 0.14952999643648723, 0.16817216547016145, 0.17310734903563885, 0.1855986179207637, 0.18359480037739373, 0.19508916572226054, 0.20087729718879477, 0.17807185486405164, 0.20119801391244463, 0.3682121014468256, 0.35073539948175425, 0.3330439038672147, 0.4157077631363848, 0.37474021495439724, 0.36760475650845414, 0.38008256501576043, 0.39084781839078786, 0.3786757352451906, 0.30037946434366947, 0.2861866860206934, 0.2891587957817804, 0.2835627691001067, 0.2779602895144492, 0.28387893685888044, 0.29400381025955125, 0.3244615423116647, 0.30188351717337136, 0.20494558183242018, 0.19905376550038034, 0.20406514428030742, 0.2229753496453719, 0.1980660060283188, 0.21539123082194234, 0.21928087314384304, 0.20854328449838244, 0.2102975078755156, 0.18082128476991477, 0.1831737258940077, 0.18089956975930732, 0.18277323416104252, 0.1792695777259361, 0.17816092407226292, 0.19628636488975992, 0.19237612183491615, 0.1874718333986961, 0.6294666203187917, 0.602690555999032, 0.5454952462675013, 0.7727337342883757, 0.25677125313719296, 0.588947972014151, 0.5053651729784945, 0.42433758940449484, 0.610090527727432, 0.7148365734773846, 0.39316249107203016, 0.4609914040189693, 0.5414475146943718, 0.3738614312609654, 0.2972834077003401, 0.2719039283446757, 0.4464259994844204, 0.6865015215956998, 0.18541089636272012, 0.17521600614441235, 0.18498258988690697, 0.1923345960637889, 0.19442272937612437, 0.18424555156947642, 0.17449564646480575, 0.18372628686318526, 0.18823517758491137, 0.07953923390817585, 0.08429672806217459, 0.08351517483047155, 0.07872531348008505, 0.0885755282031725, 0.09071254814221597, 0.07022751225803625, 0.07907174920771143, 0.08022853716110878]}, "mutation_prompt": null}
{"id": "7a2f03aa-a4e4-4f85-afeb-89f459166e0d", "solution": "import numpy as np\n\nclass HybridAdaptiveOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = 50  # Changed from 40 to 50 for enhanced diversity\n        self.mutation_factor = 0.9  # Slightly increased for better exploration\n        self.crossover_probability = 0.9  # Reduced for better exploitation\n        self.learning_rate = 0.15  # Reduced for finer local search\n        self.diversity_factor = 0.15  # Increased to maintain diversity\n        self.population = None\n        self.fitness = None\n\n    def initialize_population(self):\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, \n                                            (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def adaptive_mutation(self, idx, func):\n        indices = [i for i in range(self.population_size) if i != idx]\n        np.random.shuffle(indices)\n        a, b, c = indices[:3]\n        mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        adaptive_factor = 0.5 + 0.5 * np.random.rand()  # Changed to favor exploration\n        mutant = self.population[idx] + adaptive_factor * (mutant - self.population[idx])\n        mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n        \n        cross_points = np.random.rand(self.dim) < self.crossover_probability\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        \n        trial = np.where(cross_points, mutant, self.population[idx])\n        trial_fitness = func(trial)\n        \n        if trial_fitness < self.fitness[idx]:\n            self.population[idx] = trial\n            self.fitness[idx] = trial_fitness\n\n    def dynamic_gradient_approx(self, idx, func):\n        gradient = np.random.normal(0, 1, self.dim)\n        adaptive_lr = self.learning_rate / (1 + idx / (0.5 * self.population_size))  # Tweaked learning rate decay\n        candidate = self.population[idx] - adaptive_lr * gradient\n        candidate += self.diversity_factor * np.random.normal(0, 1, self.dim)\n        candidate = np.clip(candidate, self.lower_bound, self.upper_bound)\n        candidate_fitness = func(candidate)\n        if candidate_fitness < self.fitness[idx]:\n            self.population[idx] = candidate\n            self.fitness[idx] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population()\n        evaluations = 0\n        self.evaluate_population(func)\n        evaluations += self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                self.adaptive_mutation(i, func)\n                evaluations += 1\n\n                if evaluations >= self.budget:\n                    break\n\n                self.dynamic_gradient_approx(i, func)\n                evaluations += 1\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "HybridAdaptiveOptimizer", "description": "Hybrid probabilistic optimizer leveraging adaptive mutation strategies and dynamic parameter tuning for enhanced exploration and robust convergence.", "configspace": "", "generation": 71, "fitness": 0.2865590547936053, "feedback": "The algorithm HybridAdaptiveOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.29 with standard deviation 0.20.", "error": "", "parent_id": "1459935d-0a5b-47de-8bfe-69f52cb3cb6c", "metadata": {"aucs": [0.6799988654186032, 0.6830386857410651, 0.7056154808587966, 0.6944125740048055, 0.6727450731563295, 0.6701055559994737, 0.6846029122073737, 0.68422194917945, 0.6944371665620332, 0.390206528547424, 0.3751450723727602, 0.4132192553814601, 0.391714626343015, 0.3838057436350132, 0.39536058803891816, 0.4012896516806759, 0.4236269849339449, 0.3756075255639475, 0.11149212714698242, 0.10161771826226906, 0.0942920577408598, 0.09002329435447687, 0.10148263386075762, 0.11977155370598247, 0.10529279394080915, 0.11468565984064316, 0.11389094233451724, 0.10452859763214095, 0.09833366580715741, 0.09350684539308873, 0.10080392750618417, 0.08560258985344227, 0.08534617056487026, 0.08644394024149682, 0.1038883368225002, 0.08667970098040734, 0.9324185871600228, 0.8941456663072005, 0.9595293423591048, 0.791342187095929, 0.8133075804864529, 0.8297680959992151, 0.8486311814278313, 0.8050345111583341, 0.8317866147768882, 0.2836060897668281, 0.3087976661072961, 0.2942084923333549, 0.3024549324725737, 0.29177850246115133, 0.2823816251209278, 0.27143658344308597, 0.2859398066193296, 0.3009111041604813, 0.6608418268913568, 0.6507294646148287, 0.6135599135420595, 0.6719121344122885, 0.6320809869229508, 0.5973753494517121, 0.6254124557337818, 0.637410549631618, 0.6645963350281106, 0.18767952354320738, 0.16350938117937164, 0.17335765939405956, 0.22299179174891293, 0.1829283115858068, 0.17333843808899219, 0.15621139891728197, 0.17869024992379323, 0.1974604478015589, 0.15988398843649998, 0.16024241972154063, 0.20875461636393222, 0.19726939125705956, 0.17975906207637726, 0.17363062906227555, 0.20565506149045443, 0.1932206688988375, 0.17416036939732804, 0.19310402717318165, 0.16510568609993592, 0.1716041128411966, 0.18729610179478395, 0.16923362945743958, 0.15773085255461916, 0.1888387684153362, 0.1805317953212292, 0.2121927895575777, 0.35345599775036907, 0.31365950383678576, 0.334231850086687, 0.3107679822353332, 0.29989052589456044, 0.3214656184900687, 0.35628505690344514, 0.3167515567072452, 0.35673338506900276, 0.0662641580823311, 0.07033941158585733, 0.0761902186453659, 0.07092674687104861, 0.048529855773282504, 0.03325371517193021, 0.06798170152562777, 0.02753396612102965, 0.05848457538470664, 0.20779856408377617, 0.1841206933802596, 0.19792905448286058, 0.18965819967103614, 0.2051893579023013, 0.21395563684620422, 0.18210373553427506, 0.18095234836806062, 0.2074470884303109, 0.5578408631355882, 0.5262010417288704, 0.5391694808481702, 0.5216200602463904, 0.5373726694601078, 0.5387897937922241, 0.5601714252647256, 0.5617421354101847, 0.5367614713845261, 0.09968576439405119, 0.09738035923133304, 0.09711699912020344, 0.08809380878379347, 0.09848890740324912, 0.11506152280638238, 0.1007530330797679, 0.09659004257569026, 0.1094923527986904, 0.16018471334668027, 0.1913784065342502, 0.18606362904193519, 0.16733076440838102, 0.18728082592414608, 0.1897002143072375, 0.19807881237394365, 0.1807160767310395, 0.17378698789899227, 0.2860102658400181, 0.28950546607535266, 0.2981178113044468, 0.28068286227341666, 0.28032050989455304, 0.28641708395245846, 0.30367564331180597, 0.28504545517209745, 0.3077503033997847, 0.2248895708391061, 0.2202738995524255, 0.24070355102408636, 0.2344191607592988, 0.21603361097767004, 0.218034713001663, 0.21915512641207835, 0.23625946084526472, 0.2270400789781526, 0.19228652605220176, 0.18845708876688128, 0.21727108210686297, 0.2090191901539571, 0.1908702798285592, 0.21231679961134797, 0.19446424668127327, 0.19731175702023562, 0.2206676051581855, 0.1938396862098375, 0.18202893420159383, 0.18724301707014246, 0.18922516309146264, 0.17974904351034893, 0.20135264093297545, 0.18512516499484, 0.19477417784083018, 0.17551439236344868, 0.30390146405363305, 0.21172037048362424, 0.4720646834871993, 0.3881159129971684, 0.3717582332739484, 0.36551918096301894, 0.27431857828271, 0.4980085302879891, 0.41537412539165597, 0.40809627149748884, 0.3005192033653946, 0.4718079095111106, 0.3798816180564124, 0.2552494254912385, 0.2825302197489302, 0.29792516883478626, 0.27059142058194907, 0.19898200862013005, 0.17766186801632744, 0.19351381293859693, 0.18379016983227858, 0.18195468001563497, 0.18575236992944633, 0.18279686240710724, 0.19212055790243798, 0.18913422730678875, 0.19034478577428637, 0.0780774453171742, 0.08365806348575677, 0.07977945054193225, 0.0754985011580549, 0.07645628556894868, 0.07771655531873478, 0.07364728585178659, 0.0820331349660619, 0.0792358648850372]}, "mutation_prompt": null}
{"id": "cf48acaf-133f-4d00-9a2e-3f8f5622bb37", "solution": "import numpy as np\n\nclass EnhancedAdaptiveMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = 50  # Increased population size for diversity\n        self.mutation_factor = 0.9  # Slight increase for stronger mutations\n        self.crossover_probability = 0.9  # Adjusted for better blending\n        self.learning_rate = 0.15  # Modified to control step size\n        self.diversity_factor = 0.15  # Enhanced for exploration\n        self.population = None\n        self.fitness = None\n\n    def initialize_population(self):\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, \n                                            (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def adaptive_differential_evolution(self, idx, func):\n        indices = [i for i in range(self.population_size) if i != idx]\n        np.random.shuffle(indices)\n        a, b, c = indices[:3]\n        mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        adaptive_factor = 0.5 + 0.5 * np.random.rand()  # Adjusted range for adaptive scaling\n        mutant = self.population[idx] + adaptive_factor * (mutant - self.population[idx])\n        mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n        \n        cross_points = np.random.rand(self.dim) < self.crossover_probability\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        \n        trial = np.where(cross_points, mutant, self.population[idx])\n        trial_fitness = func(trial)\n        \n        if trial_fitness < self.fitness[idx]:\n            self.population[idx] = trial\n            self.fitness[idx] = trial_fitness\n\n    def stochastic_gradient_approx(self, idx, func):\n        gradient = np.random.normal(0, 1, self.dim)\n        adaptive_lr = self.learning_rate / (1 + idx / self.population_size)\n        candidate = self.population[idx] - adaptive_lr * gradient\n        candidate += self.diversity_factor * np.random.normal(0, 1, self.dim)\n        candidate = np.clip(candidate, self.lower_bound, self.upper_bound)\n        candidate_fitness = func(candidate)\n        if candidate_fitness < self.fitness[idx]:\n            self.population[idx] = candidate\n            self.fitness[idx] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population()\n        evaluations = 0\n        self.evaluate_population(func)\n        evaluations += self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                if np.random.rand() < 0.5:  # Randomly choose between strategies\n                    self.adaptive_differential_evolution(i, func)\n                else:\n                    self.stochastic_gradient_approx(i, func)\n                \n                evaluations += 1\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "EnhancedAdaptiveMetaheuristicOptimizer", "description": "Enhanced Adaptive Metaheuristic Optimizer with diversity-driven exploration and adaptive strategy blending for efficient search and robust convergence.", "configspace": "", "generation": 72, "fitness": 0.277759209748253, "feedback": "The algorithm EnhancedAdaptiveMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28 with standard deviation 0.20.", "error": "", "parent_id": "1459935d-0a5b-47de-8bfe-69f52cb3cb6c", "metadata": {"aucs": [0.6913209428946847, 0.6669721259454688, 0.6432019989858384, 0.6739327382256868, 0.6362261626994531, 0.6453108803671953, 0.6807638581165838, 0.6822613819984582, 0.633763355518052, 0.3426454122139916, 0.3566279314648848, 0.3279421162345698, 0.3524241270203241, 0.3821665480891292, 0.35114973176969433, 0.31936417616976065, 0.3666385276594123, 0.34037525475345054, 0.11109780615539544, 0.09495065667227409, 0.10660737569330903, 0.097249811092742, 0.11143882055351828, 0.11762098404943278, 0.10658995507102986, 0.11604139073961273, 0.10410058741163009, 0.09188425813605172, 0.08044972231803982, 0.08517110194403887, 0.09648811324773654, 0.08244553889369888, 0.09449979430929978, 0.08250366049312485, 0.09031561707527269, 0.08672481378314312, 0.7674078381852995, 0.8754883053165503, 0.8522568534392545, 0.8961297771777431, 0.7995526002104776, 0.8224660875483851, 0.8399205144357246, 0.8284140838529845, 0.8958838975878576, 0.284525106670389, 0.2931378238414871, 0.2592253957474324, 0.2704547866138092, 0.25455209365016507, 0.2622421860247415, 0.28968614671379156, 0.2713507119851297, 0.2830585076097193, 0.6155864116031848, 0.6364931939245752, 0.6152877412017135, 0.6655345671967356, 0.7212130237946917, 0.6319582285065001, 0.6215007598939164, 0.6180284180453482, 0.5273720714453998, 0.16605354929931004, 0.12584208536620967, 0.15595834850965096, 0.15301405294716364, 0.1955587018420778, 0.2046768232558156, 0.18762510018821843, 0.1647128774301998, 0.21083193345833218, 0.1742403623519061, 0.19535927116006735, 0.15916399955933624, 0.1777144525960508, 0.16885742093588774, 0.16540579178807624, 0.18404339798173164, 0.14652650552801993, 0.15889692803829314, 0.18755600612398537, 0.21210435097965685, 0.17453302822530825, 0.20060773138843802, 0.18136091228791362, 0.16904557687698363, 0.1718676343258967, 0.1775333660913817, 0.1888002704882431, 0.349432757020428, 0.31634543304609253, 0.2841629704713702, 0.2761635140724994, 0.3107955707419866, 0.28923578985312937, 0.3444948481816128, 0.33455770737959467, 0.30462255163213514, 0.048088097728652635, 0.07659968017448282, 0.02363054056286784, 0.07034481021765349, 0.1095255395893624, 0.045505678788080095, 0.044064206462837374, 0.07744928842449894, 0.07388220599150863, 0.18012157748725144, 0.17819424370741288, 0.1822131214260737, 0.20225802461664766, 0.19119633793461677, 0.19377799642973526, 0.17741580719473127, 0.19222543331842346, 0.19039447574165957, 0.5310291037180989, 0.5242331571991232, 0.5382283569288135, 0.540113541953773, 0.4977003566105245, 0.5100903074291275, 0.5305599339226945, 0.5243206684575076, 0.5397520778285984, 0.10448413295375614, 0.09555230847332719, 0.1153291142364209, 0.10413238974340222, 0.09086877905937685, 0.09771710886831153, 0.09912459006166685, 0.09029535298669111, 0.09043177878975561, 0.1688783673662092, 0.16863719348010486, 0.22363086579786795, 0.16939775257099232, 0.21032943511747848, 0.16171472014747956, 0.18541445410595048, 0.20268790022394856, 0.18869581974813077, 0.2787310911526525, 0.2810747682656982, 0.283739848195109, 0.2828421370823274, 0.2794489121052809, 0.2695433529594795, 0.28583665199672037, 0.30306869743351184, 0.272896297355782, 0.22414760945304124, 0.2103040749122631, 0.22089931832848808, 0.2173002167678525, 0.21551220566040497, 0.19567779343874192, 0.2211468810101469, 0.2228017031511894, 0.2202413874548026, 0.19985518730093021, 0.19178208482084502, 0.19486798377105197, 0.2057226598545946, 0.25121183995142615, 0.19735965755008789, 0.23232111536548194, 0.18970933233995424, 0.19084001769345826, 0.21581371996859677, 0.18140390802260487, 0.18893329895267952, 0.18832623021455486, 0.19946526956896082, 0.17506343671385816, 0.1851697451214619, 0.1776478521319499, 0.1788957823036369, 0.5581482147396349, 0.2623441531739492, 0.335090785324585, 0.37959719608679987, 0.23129158488494972, 0.3769657012936516, 0.3429033342156331, 0.37159275540240544, 0.359255659087098, 0.32270573541193903, 0.2975582482061384, 0.409094179287105, 0.182957380995733, 0.2852597873479116, 0.2665700742571243, 0.33002086428503796, 0.2916512254840965, 0.2819972678265996, 0.18073505209788798, 0.1894193869462203, 0.18580419727121222, 0.18945450958082422, 0.18111411626225804, 0.19254680579571115, 0.1847839425552804, 0.20214282001817196, 0.18067302466399782, 0.0870624042346193, 0.07692053943612553, 0.0727418101423476, 0.08036733675923335, 0.07724508143965036, 0.08894680749189943, 0.0754051672194912, 0.08655006409738375, 0.07353735894269953]}, "mutation_prompt": null}
{"id": "6e892304-cb7a-4fa2-92bb-82f29874e4ad", "solution": "import numpy as np\n\nclass EnhancedProbabilisticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = 40\n        self.mutation_factor = 0.85\n        self.crossover_probability = 0.95\n        self.learning_rate = 0.2\n        self.diversity_factor = 0.1\n        self.population = None\n        self.fitness = None\n\n    def initialize_population(self):\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, \n                                            (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def adaptive_differential_evolution(self, idx, func):\n        indices = [i for i in range(self.population_size) if i != idx]\n        np.random.shuffle(indices)\n        a, b, c = indices[:3]\n        mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        adaptive_factor = 0.4 + 0.6 * np.random.rand()\n        mutant = self.population[idx] + adaptive_factor * (mutant - self.population[idx])\n        mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n        \n        cross_points = np.random.rand(self.dim) < self.crossover_probability\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        \n        trial = np.where(cross_points, mutant, self.population[idx])\n        trial_fitness = func(trial)\n        \n        if trial_fitness < self.fitness[idx]:\n            self.population[idx] = trial\n            self.fitness[idx] = trial_fitness\n\n    def stochastic_gradient_approx(self, idx, func):\n        gradient = np.random.normal(0, 1, self.dim)\n        adaptive_lr = self.learning_rate / (1 + idx / self.population_size)\n        candidate = self.population[idx] - adaptive_lr * gradient\n        candidate += self.diversity_factor * np.random.normal(0, 1, self.dim)\n        candidate = np.clip(candidate, self.lower_bound, self.upper_bound)\n        candidate_fitness = func(candidate)\n        if candidate_fitness < self.fitness[idx]:\n            self.population[idx] = candidate\n            self.fitness[idx] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population()\n        evaluations = 0\n        self.evaluate_population(func)\n        evaluations += self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                self.adaptive_differential_evolution(i, func)\n                evaluations += 1\n\n                if evaluations >= self.budget:\n                    break\n\n                self.stochastic_gradient_approx(i, func)\n                evaluations += 1\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "EnhancedProbabilisticOptimizer", "description": "Enhanced probabilistic hybrid optimizer balancing exploration and exploitation through diversity-oriented selection and adaptive parameter tuning for optimal convergence.", "configspace": "", "generation": 38, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "1459935d-0a5b-47de-8bfe-69f52cb3cb6c", "metadata": {"aucs": [0.796727445346743, 0.7946092935536371, 0.8025157468331956, 0.7795890428061506, 0.7652866403837684, 0.7888980264133363, 0.7851539828124771, 0.7801166288092796, 0.778684214476141, 0.6066596860215113, 0.5509488918434079, 0.584709500115218, 0.6151880393982149, 0.6128285196462557, 0.5928554548480235, 0.18781852246903208, 0.6051102165828588, 0.6050716765741883, 0.12056849613284826, 0.10627786970516262, 0.11092740344636509, 0.12458142503345515, 0.10225070914012868, 0.10214739878052792, 0.11007052847089493, 0.11303443664409152, 0.11346829363668931, 0.09058514750802149, 0.09136224141572435, 0.09346304265481342, 0.08413688806219866, 0.10560994817344638, 0.09386944951658027, 0.1066829182603829, 0.09525027475772718, 0.09078080677184064, 0.8379701410373176, 0.8069154658397715, 0.8108437032002754, 0.8202640448333322, 0.7815128713795206, 0.8451084830274826, 0.836376388134602, 0.8441441978939892, 0.8509693104465095, 0.37150586919602946, 0.34273942956965175, 0.3313822553845982, 0.34968755932637086, 0.32960730205573996, 0.3754984208710611, 0.38284578850911233, 0.3840456442426208, 0.3280564413760412, 0.7576709220936438, 0.7335556841917643, 0.7658090217912067, 0.7934167558179147, 0.8025617081483369, 0.7999764269594076, 0.7299292035530152, 0.7697788108067551, 0.7651252822797626, 0.16167439560696928, 0.18696031846775318, 0.21158580997319565, 0.244349141547606, 0.2890559956537958, 0.16298570563996873, 0.21750653437837209, 0.21122767559906996, 0.22550289679700497, 0.15305768362463734, 0.14884944831507896, 0.18600965489753707, 0.1541357033150358, 0.19832744782165168, 0.18983314847500432, 0.1953210887165725, 0.20165646735698872, 0.13088573983753993, 0.489476775395826, 0.460206761694571, 0.280690722270316, 0.34064732845138956, 0.522953329166158, 0.46452239779932414, 0.4196789194960461, 0.4727221489210359, 0.44131563048738687, 0.6540713674624187, 0.6128130384068273, 0.5553131009305013, 0.5776200055109268, 0.4723536786910414, 0.6219680956281209, 0.6455804612075908, 0.6109257481083044, 0.5996992464126676, 0.15220127888973134, 0.11018658007737869, 0.2413715712685326, 0.2296484097510183, 0.2572737636354798, 0.10622686325653308, 0.14604068872930986, 0.14809138800719435, 0.12257392403735012, 0.36655256082417187, 0.34632185847096675, 0.3516395670957846, 0.33828319802853724, 0.3572041238804262, 0.35621925345698524, 0.34628041224843775, 0.36205674625584605, 0.3801826028652373, 0.7284525303733058, 0.7334889541026953, 0.7386557191632159, 0.7055754468414382, 0.7222856768071027, 0.7194774022217179, 0.518854950686749, 0.7321096452535195, 0.7397060954282213, 0.10374399773694243, 0.104282622260642, 0.1086381702881759, 0.11429613558107654, 0.11511930056195652, 0.10035386421161252, 0.09599830676572241, 0.11347281801819442, 0.11470225552281121, 0.14952999643648723, 0.16817216547016145, 0.17310734903563885, 0.1855986179207637, 0.18359480037739373, 0.19508916572226054, 0.20087729718879477, 0.17807185486405164, 0.20119801391244463, 0.3682121014468256, 0.35073539948175425, 0.3330439038672147, 0.4157077631363848, 0.37474021495439724, 0.36760475650845414, 0.38008256501576043, 0.39084781839078786, 0.3786757352451906, 0.30037946434366947, 0.2861866860206934, 0.2891587957817804, 0.2835627691001067, 0.2779602895144492, 0.28387893685888044, 0.29400381025955125, 0.3244615423116647, 0.30188351717337136, 0.20494558183242018, 0.19905376550038034, 0.20406514428030742, 0.2229753496453719, 0.1980660060283188, 0.21539123082194234, 0.21928087314384304, 0.20854328449838244, 0.2102975078755156, 0.18082128476991477, 0.1831737258940077, 0.18089956975930732, 0.18277323416104252, 0.1792695777259361, 0.17816092407226292, 0.19628636488975992, 0.19237612183491615, 0.1874718333986961, 0.6294666203187917, 0.602690555999032, 0.5454952462675013, 0.7727337342883757, 0.25677125313719296, 0.588947972014151, 0.5053651729784945, 0.42433758940449484, 0.610090527727432, 0.7148365734773846, 0.39316249107203016, 0.4609914040189693, 0.5414475146943718, 0.3738614312609654, 0.2972834077003401, 0.2719039283446757, 0.4464259994844204, 0.6865015215956998, 0.18541089636272012, 0.17521600614441235, 0.18498258988690697, 0.1923345960637889, 0.19442272937612437, 0.18424555156947642, 0.17449564646480575, 0.18372628686318526, 0.18823517758491137, 0.07953923390817585, 0.08429672806217459, 0.08351517483047155, 0.07872531348008505, 0.0885755282031725, 0.09071254814221597, 0.07022751225803625, 0.07907174920771143, 0.08022853716110878]}, "mutation_prompt": null}
{"id": "da1d4d1d-96f2-4e70-b0dc-e4f6eba0d018", "solution": "import numpy as np\n\nclass ProbabilisticHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = 50  # Increased population size for greater diversity\n        self.mutation_factor = 0.9  # Slightly increased mutation factor for more exploration\n        self.crossover_probability = 0.9\n        self.learning_rate = 0.2\n        self.diversity_factor = 0.15  # Increased for more diversity\n        self.population = None\n        self.fitness = None\n\n    def initialize_population(self):\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, \n                                            (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def adaptive_differential_evolution(self, idx, func):\n        indices = [i for i in range(self.population_size) if i != idx]\n        np.random.shuffle(indices)\n        a, b, c = indices[:3]\n        mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        adaptive_factor = 0.3 + 0.7 * np.random.rand()  # Adjusted adaptive factor range\n        mutant = self.population[idx] + adaptive_factor * (mutant - self.population[idx])\n        mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n        \n        cross_points = np.random.rand(self.dim) < self.crossover_probability\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        \n        trial = np.where(cross_points, mutant, self.population[idx])\n        trial_fitness = func(trial)\n        \n        if trial_fitness < self.fitness[idx]:\n            self.population[idx] = trial\n            self.fitness[idx] = trial_fitness\n\n    def stochastic_gradient_approx(self, idx, func):\n        gradient = np.random.normal(0, 1, self.dim)\n        adaptive_lr = self.learning_rate / (1 + idx / self.population_size)\n        candidate = self.population[idx] - adaptive_lr * gradient\n        candidate += self.diversity_factor * np.random.normal(0, 1, self.dim)\n        candidate = np.clip(candidate, self.lower_bound, self.upper_bound)\n        candidate_fitness = func(candidate)\n        if candidate_fitness < self.fitness[idx]:\n            self.population[idx] = candidate\n            self.fitness[idx] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population()\n        evaluations = 0\n        self.evaluate_population(func)\n        evaluations += self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                # Adaptive strategy choice based on current fitness\n                if np.random.rand() < 0.5:\n                    self.adaptive_differential_evolution(i, func)\n                else:\n                    self.stochastic_gradient_approx(i, func)\n                evaluations += 1\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "ProbabilisticHybridOptimizer", "description": "Probabilistic Hybrid Optimizer with Adaptive Strategies for Enhanced Convergence and Diversity.", "configspace": "", "generation": 74, "fitness": 0.2984062468550271, "feedback": "The algorithm ProbabilisticHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.30 with standard deviation 0.20.", "error": "", "parent_id": "1459935d-0a5b-47de-8bfe-69f52cb3cb6c", "metadata": {"aucs": [0.7118940242658024, 0.7079528635413181, 0.7074786684019678, 0.6973820782646915, 0.700779035602858, 0.7049061055803725, 0.7147093435723811, 0.7204021438681298, 0.6994589790142871, 0.42200064500157664, 0.43404738293096157, 0.4129772003003478, 0.41020362194602744, 0.4559424697378661, 0.43827209923159005, 0.40681718816028456, 0.4292922105010254, 0.4097934736722503, 0.09865979834468819, 0.10472887115728102, 0.10132337967633664, 0.10628769992213094, 0.1063892239266141, 0.1048220051690627, 0.11857756979800393, 0.09616102296441809, 0.09788603815077479, 0.10125691777495216, 0.08499524292773353, 0.08482676540883372, 0.08210358055030642, 0.07967397780710161, 0.08084590605068276, 0.08793358366408943, 0.09357466817872939, 0.08491457459800278, 0.7552101309024912, 0.8228131562095902, 0.7594407964456287, 0.7956092744231239, 0.8110558158650855, 0.7673348453118056, 0.8059043255278433, 0.7923562596996402, 0.7795056273829354, 0.2961982748057217, 0.30172868586192336, 0.2761441119517234, 0.30188189635379736, 0.26077406674009307, 0.280770074957133, 0.31380054559272275, 0.31000747891788816, 0.28735944705080474, 0.6303958656260511, 0.5947443179813832, 0.644134219547019, 0.6946493261001463, 0.711698863107056, 0.6573102550187215, 0.6215884174257469, 0.7553891117079468, 0.6486957976226243, 0.15896663398427047, 0.20195933609455052, 0.1688255253743377, 0.15329393606802788, 0.15728137605587011, 0.1971235011477359, 0.1724663823517858, 0.18104991937116255, 0.17294649307266885, 0.2079828843095003, 0.15681074355878521, 0.16242203400821997, 0.18311300930750818, 0.15826756878117543, 0.1922675171328435, 0.18743010534678828, 0.1744957933243153, 0.1915243993148632, 0.22565906698000915, 0.2514657361596828, 0.23027079405932782, 0.21660316771525512, 0.23408814999912375, 0.20276731594944042, 0.2336995007555438, 0.2620460312252626, 0.2560039858217298, 0.39932178967591814, 0.3969276205486735, 0.334218220655325, 0.3373418315651706, 0.35281344253628677, 0.35667015520189693, 0.41018551274617676, 0.420329557435859, 0.3985610041827288, 0.09951647654610674, 0.06659860456497368, 0.08282449027926109, 0.06980255089281362, 0.14420162239510104, 0.09440285695850159, 0.06151480593142655, 0.10842470218677458, 0.0836244900016685, 0.22507832642271963, 0.22680649347814819, 0.212801788579834, 0.2195219274484974, 0.24984632665550588, 0.22744425233630294, 0.21335579917729552, 0.22007498210102994, 0.21812444844067191, 0.6002636945080648, 0.5938418146773882, 0.5941803380062145, 0.554871720010448, 0.5951301926291921, 0.5487019274153797, 0.5785145088687013, 0.605577738873727, 0.6008907731515879, 0.10422478847692951, 0.09843724324548364, 0.09676186916635143, 0.09929703318548055, 0.09772263601890951, 0.10226525748132753, 0.10300775808544516, 0.09252881173494931, 0.08993286320690752, 0.19581075871224451, 0.19038153937782398, 0.1933184009432124, 0.17583177628717217, 0.1626006349615553, 0.18655582160969464, 0.20053512366341653, 0.18765977565936187, 0.16738818314001236, 0.30825925107203755, 0.29850075876857685, 0.29765064875821723, 0.3028334454499836, 0.294113926415039, 0.3210897461376673, 0.3073771970099878, 0.3182493589198755, 0.30733635520419944, 0.2328605074783534, 0.24684177024475096, 0.2219207041224336, 0.24120836010257674, 0.23207758227513353, 0.2372399005326058, 0.2499345394222039, 0.27003887179888375, 0.22895410646857373, 0.19437136687318957, 0.24172051213755674, 0.2020863111151432, 0.22283195313856896, 0.18745515482261788, 0.22357721149350696, 0.2314207069706915, 0.20354404535580917, 0.21073392713661376, 0.18423178345606772, 0.18718603201641626, 0.19341787610250427, 0.18030924262710957, 0.18648736693607582, 0.18966792595101223, 0.17649817493678888, 0.17697056287509128, 0.1793572199529837, 0.3599508169994684, 0.23823613089471152, 0.48223739942656185, 0.5639552010366782, 0.4567066780580098, 0.5227824495736731, 0.4464367137483952, 0.3033364540597977, 0.3854888264212546, 0.5387996098699119, 0.3514204453435623, 0.33420712540640474, 0.20024076229411003, 0.4011709363271685, 0.28997448899911005, 0.2408762554243724, 0.2732156067847584, 0.2643731852537572, 0.17932309513410216, 0.18253454404022595, 0.17803747762222089, 0.17824816826333312, 0.18074901225093132, 0.1826656181933668, 0.18773045431162072, 0.21442112702255622, 0.1856810034911328, 0.09274422497305967, 0.0823370658161432, 0.0754340657957222, 0.0806992438041163, 0.08484637952717167, 0.07368351713209453, 0.07479557237434709, 0.0858258121453529, 0.0810601316762406]}, "mutation_prompt": null}
{"id": "7d3d0cc1-2bf6-4895-b10f-9bfabc5b997f", "solution": "import numpy as np\n\nclass EnhancedProbabilisticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = 40\n        self.mutation_factor = 0.85\n        self.crossover_probability = 0.95\n        self.learning_rate = 0.2\n        self.diversity_factor = 0.1\n        self.population = None\n        self.fitness = None\n\n    def initialize_population(self):\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, \n                                            (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def adaptive_differential_evolution(self, idx, func):\n        indices = [i for i in range(self.population_size) if i != idx]\n        np.random.shuffle(indices)\n        a, b, c = indices[:3]\n        mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        adaptive_factor = 0.4 + 0.6 * np.random.rand()\n        mutant = self.population[idx] + adaptive_factor * (mutant - self.population[idx])\n        mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n        \n        cross_points = np.random.rand(self.dim) < self.crossover_probability\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        \n        trial = np.where(cross_points, mutant, self.population[idx])\n        trial_fitness = func(trial)\n        \n        if trial_fitness < self.fitness[idx]:\n            self.population[idx] = trial\n            self.fitness[idx] = trial_fitness\n\n    def stochastic_gradient_approx(self, idx, func):\n        gradient = np.random.normal(0, 1, self.dim)\n        adaptive_lr = self.learning_rate / (1 + idx / self.population_size)\n        candidate = self.population[idx] - adaptive_lr * gradient\n        candidate += self.diversity_factor * np.random.normal(0, 1, self.dim)\n        candidate = np.clip(candidate, self.lower_bound, self.upper_bound)\n        candidate_fitness = func(candidate)\n        if candidate_fitness < self.fitness[idx]:\n            self.population[idx] = candidate\n            self.fitness[idx] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population()\n        evaluations = 0\n        self.evaluate_population(func)\n        evaluations += self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                self.adaptive_differential_evolution(i, func)\n                evaluations += 1\n\n                if evaluations >= self.budget:\n                    break\n\n                self.stochastic_gradient_approx(i, func)\n                evaluations += 1\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "EnhancedProbabilisticOptimizer", "description": "Enhanced probabilistic hybrid optimizer balancing exploration and exploitation through diversity-oriented selection and adaptive parameter tuning for optimal convergence.", "configspace": "", "generation": 38, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "1459935d-0a5b-47de-8bfe-69f52cb3cb6c", "metadata": {"aucs": [0.796727445346743, 0.7946092935536371, 0.8025157468331956, 0.7795890428061506, 0.7652866403837684, 0.7888980264133363, 0.7851539828124771, 0.7801166288092796, 0.778684214476141, 0.6066596860215113, 0.5509488918434079, 0.584709500115218, 0.6151880393982149, 0.6128285196462557, 0.5928554548480235, 0.18781852246903208, 0.6051102165828588, 0.6050716765741883, 0.12056849613284826, 0.10627786970516262, 0.11092740344636509, 0.12458142503345515, 0.10225070914012868, 0.10214739878052792, 0.11007052847089493, 0.11303443664409152, 0.11346829363668931, 0.09058514750802149, 0.09136224141572435, 0.09346304265481342, 0.08413688806219866, 0.10560994817344638, 0.09386944951658027, 0.1066829182603829, 0.09525027475772718, 0.09078080677184064, 0.8379701410373176, 0.8069154658397715, 0.8108437032002754, 0.8202640448333322, 0.7815128713795206, 0.8451084830274826, 0.836376388134602, 0.8441441978939892, 0.8509693104465095, 0.37150586919602946, 0.34273942956965175, 0.3313822553845982, 0.34968755932637086, 0.32960730205573996, 0.3754984208710611, 0.38284578850911233, 0.3840456442426208, 0.3280564413760412, 0.7576709220936438, 0.7335556841917643, 0.7658090217912067, 0.7934167558179147, 0.8025617081483369, 0.7999764269594076, 0.7299292035530152, 0.7697788108067551, 0.7651252822797626, 0.16167439560696928, 0.18696031846775318, 0.21158580997319565, 0.244349141547606, 0.2890559956537958, 0.16298570563996873, 0.21750653437837209, 0.21122767559906996, 0.22550289679700497, 0.15305768362463734, 0.14884944831507896, 0.18600965489753707, 0.1541357033150358, 0.19832744782165168, 0.18983314847500432, 0.1953210887165725, 0.20165646735698872, 0.13088573983753993, 0.489476775395826, 0.460206761694571, 0.280690722270316, 0.34064732845138956, 0.522953329166158, 0.46452239779932414, 0.4196789194960461, 0.4727221489210359, 0.44131563048738687, 0.6540713674624187, 0.6128130384068273, 0.5553131009305013, 0.5776200055109268, 0.4723536786910414, 0.6219680956281209, 0.6455804612075908, 0.6109257481083044, 0.5996992464126676, 0.15220127888973134, 0.11018658007737869, 0.2413715712685326, 0.2296484097510183, 0.2572737636354798, 0.10622686325653308, 0.14604068872930986, 0.14809138800719435, 0.12257392403735012, 0.36655256082417187, 0.34632185847096675, 0.3516395670957846, 0.33828319802853724, 0.3572041238804262, 0.35621925345698524, 0.34628041224843775, 0.36205674625584605, 0.3801826028652373, 0.7284525303733058, 0.7334889541026953, 0.7386557191632159, 0.7055754468414382, 0.7222856768071027, 0.7194774022217179, 0.518854950686749, 0.7321096452535195, 0.7397060954282213, 0.10374399773694243, 0.104282622260642, 0.1086381702881759, 0.11429613558107654, 0.11511930056195652, 0.10035386421161252, 0.09599830676572241, 0.11347281801819442, 0.11470225552281121, 0.14952999643648723, 0.16817216547016145, 0.17310734903563885, 0.1855986179207637, 0.18359480037739373, 0.19508916572226054, 0.20087729718879477, 0.17807185486405164, 0.20119801391244463, 0.3682121014468256, 0.35073539948175425, 0.3330439038672147, 0.4157077631363848, 0.37474021495439724, 0.36760475650845414, 0.38008256501576043, 0.39084781839078786, 0.3786757352451906, 0.30037946434366947, 0.2861866860206934, 0.2891587957817804, 0.2835627691001067, 0.2779602895144492, 0.28387893685888044, 0.29400381025955125, 0.3244615423116647, 0.30188351717337136, 0.20494558183242018, 0.19905376550038034, 0.20406514428030742, 0.2229753496453719, 0.1980660060283188, 0.21539123082194234, 0.21928087314384304, 0.20854328449838244, 0.2102975078755156, 0.18082128476991477, 0.1831737258940077, 0.18089956975930732, 0.18277323416104252, 0.1792695777259361, 0.17816092407226292, 0.19628636488975992, 0.19237612183491615, 0.1874718333986961, 0.6294666203187917, 0.602690555999032, 0.5454952462675013, 0.7727337342883757, 0.25677125313719296, 0.588947972014151, 0.5053651729784945, 0.42433758940449484, 0.610090527727432, 0.7148365734773846, 0.39316249107203016, 0.4609914040189693, 0.5414475146943718, 0.3738614312609654, 0.2972834077003401, 0.2719039283446757, 0.4464259994844204, 0.6865015215956998, 0.18541089636272012, 0.17521600614441235, 0.18498258988690697, 0.1923345960637889, 0.19442272937612437, 0.18424555156947642, 0.17449564646480575, 0.18372628686318526, 0.18823517758491137, 0.07953923390817585, 0.08429672806217459, 0.08351517483047155, 0.07872531348008505, 0.0885755282031725, 0.09071254814221597, 0.07022751225803625, 0.07907174920771143, 0.08022853716110878]}, "mutation_prompt": null}
{"id": "3a21b6d7-f881-4193-bf49-5198e5a8464b", "solution": "import numpy as np\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = 50  # Slightly increased population size\n        self.mutation_factor = 0.9  # Adjusted mutation factor\n        self.crossover_probability = 0.9  # Slightly adjusted crossover probability\n        self.learning_rate = 0.15  # Adjusted learning rate\n        self.diversity_factor = 0.15  # Adjusted diversity factor\n        self.population = None\n        self.fitness = None\n\n    def initialize_population(self):\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, \n                                            (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def adaptive_differential_evolution(self, idx, func):\n        indices = [i for i in range(self.population_size) if i != idx]\n        np.random.shuffle(indices)\n        a, b, c = indices[:3]\n        mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        adaptive_factor = 0.5 + 0.5 * np.random.rand()  # Adjusted adaptive factor range\n        mutant = self.population[idx] + adaptive_factor * (mutant - self.population[idx])\n        mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n        \n        cross_points = np.random.rand(self.dim) < self.crossover_probability\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        \n        trial = np.where(cross_points, mutant, self.population[idx])\n        trial_fitness = func(trial)\n        \n        if trial_fitness < self.fitness[idx]:\n            self.population[idx] = trial\n            self.fitness[idx] = trial_fitness\n\n    def stochastic_gradient_approx(self, idx, func):\n        gradient = np.random.normal(0, 1, self.dim)\n        adaptive_lr = self.learning_rate / (1 + idx / self.population_size)\n        candidate = self.population[idx] - adaptive_lr * gradient\n        candidate += self.diversity_factor * np.random.normal(0, 1, self.dim)\n        candidate = np.clip(candidate, self.lower_bound, self.upper_bound)\n        candidate_fitness = func(candidate)\n        if candidate_fitness < self.fitness[idx]:\n            self.population[idx] = candidate\n            self.fitness[idx] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population()\n        evaluations = 0\n        self.evaluate_population(func)\n        evaluations += self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                self.adaptive_differential_evolution(i, func)\n                evaluations += 1\n\n                if evaluations >= self.budget:\n                    break\n\n                self.stochastic_gradient_approx(i, func)\n                evaluations += 1\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "AdaptiveHybridOptimizer", "description": "An adaptive hybrid optimizer leveraging diversified evolutionary strategies and stochastic approximations for enhanced solution quality and convergence.", "configspace": "", "generation": 76, "fitness": 0.28464380004487566, "feedback": "The algorithm AdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28 with standard deviation 0.20.", "error": "", "parent_id": "1459935d-0a5b-47de-8bfe-69f52cb3cb6c", "metadata": {"aucs": [0.6605470981089374, 0.6705333162978047, 0.682656679345612, 0.6870462366398069, 0.6764314484115737, 0.6721790347437242, 0.6873868005389, 0.680474837501718, 0.6740844240018651, 0.39412523397984356, 0.4033880493938231, 0.37066156616365686, 0.3779976558053181, 0.37889566882987924, 0.4293950351474397, 0.33456375499286384, 0.3885975597540019, 0.39680154528103617, 0.0991100891829696, 0.09881105924463696, 0.10560200352786486, 0.11038283415025985, 0.10252540824031553, 0.10784084214169853, 0.12223141209512745, 0.0981509437868443, 0.10707142104976797, 0.09084689920795486, 0.08796292027796404, 0.0834827621977291, 0.08731877607607419, 0.09382163659722642, 0.08417798296687962, 0.08803116752676798, 0.08904276213179041, 0.09741516591511135, 0.9330340677339195, 0.8903544521339327, 0.9595482359922426, 0.8472812940810512, 0.831156342536172, 0.8868006653516718, 0.8336356791626951, 0.810465066732764, 0.8328332084726063, 0.29330640429065213, 0.2770840675480223, 0.29871527697086475, 0.29422678849611517, 0.30874950196305617, 0.30233181763475114, 0.3179037194906936, 0.2934780646421601, 0.29337107448635125, 0.5745709570749804, 0.6215608409047537, 0.59150819616658, 0.6640687460999966, 0.5936441979605623, 0.6357561346194105, 0.6037458291721465, 0.6172427988564269, 0.6901983890015502, 0.18026346894288892, 0.15195829610749134, 0.1675674627349315, 0.16083418361906965, 0.1645527970831152, 0.16634105988465364, 0.16837428144420175, 0.17460117704616773, 0.16290957281969476, 0.17284478349030064, 0.13118405808868283, 0.18343329546982734, 0.14196215231566567, 0.1806887077503605, 0.1743012728462503, 0.19725615059826396, 0.17080983828178542, 0.18750012872090305, 0.16830558062237555, 0.17865128610568082, 0.1626505351019183, 0.17756856095169715, 0.20370848355211235, 0.1692741707529969, 0.18152992263764856, 0.20746972547291265, 0.21477269435788682, 0.3468537685573496, 0.31905696761379954, 0.29185583574172425, 0.3457366880818512, 0.30170886947171915, 0.32365664121893445, 0.3164913339199248, 0.3229092164236631, 0.3499624884763325, 0.05139633726923898, 0.06215061673142264, 0.06708405436601861, 0.07182930584434888, 0.048941433815587354, 0.09187172738811622, 0.06161230381860183, 0.07616179832299552, 0.056215963915268174, 0.18903405034917864, 0.19781412815551946, 0.203726221921679, 0.20258483616185663, 0.19096012656940164, 0.21051534404463212, 0.1734292709956775, 0.19851487027189607, 0.1902279051326332, 0.5293213543865685, 0.5446427667045353, 0.5273268323875527, 0.5357154408077701, 0.5371640070472288, 0.5109768575860294, 0.5657553639764106, 0.5458906664900376, 0.5539427128758025, 0.12146128841983794, 0.09219426848017409, 0.0974922851901251, 0.09919425977266672, 0.09965138023027664, 0.08757479527620804, 0.10183746964300189, 0.10931141138344591, 0.0898269353790303, 0.18977193528222502, 0.19688289229124578, 0.1813414256158965, 0.20645954549796874, 0.18823268420294603, 0.17875755656418224, 0.2164362817371278, 0.19708864205139132, 0.1803887430433, 0.2634218541877267, 0.28398031838426807, 0.3092233056074375, 0.2612792643333183, 0.284652674000372, 0.28498940198305944, 0.2920656506648396, 0.30389358372407227, 0.2956416040204243, 0.21067846099870813, 0.22340856956989852, 0.232091862294602, 0.22162908185165509, 0.2225549711482976, 0.21163969456114773, 0.22499582363860438, 0.2336657962782921, 0.2256108377365006, 0.2195830985126841, 0.18847477546483005, 0.19129110014376471, 0.2320508483627849, 0.19344614185197084, 0.1914334795356296, 0.2064001036573624, 0.20641737556399242, 0.2148943878604389, 0.18661984670298193, 0.19699269348819115, 0.18531296748351456, 0.18535257744256173, 0.18097514632774314, 0.19102836700332682, 0.19511409975086136, 0.1850808825346124, 0.186144009091196, 0.18530644558023746, 0.2661401254716985, 0.3284256349077119, 0.4495965207678493, 0.3499391624856214, 0.3528440246863529, 0.34697025291435746, 0.3829250361828893, 0.3962659866983711, 0.3575730938131081, 0.2771940360331371, 0.3119683694436046, 0.3508141248297645, 0.3122889323528317, 0.3688535218574691, 0.30758967586330255, 0.375208569879371, 0.3788720246819124, 0.18605696952754747, 0.18495136831806447, 0.18521172376468553, 0.1836641186063428, 0.18850086881348294, 0.18106013814471034, 0.2018209999051609, 0.19503611855299374, 0.17892708160358273, 0.08287570506513398, 0.08497318008777688, 0.07559094268602606, 0.07716408225247617, 0.07393358482809231, 0.07645804158120606, 0.08211770520376083, 0.08296022640308598, 0.07652835760499455]}, "mutation_prompt": null}
{"id": "4e0af7a5-fece-40ce-a70c-46827a7fcb0b", "solution": "import numpy as np\n\nclass EnhancedProbabilisticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = 40\n        self.mutation_factor = 0.85\n        self.crossover_probability = 0.95\n        self.learning_rate = 0.2\n        self.diversity_factor = 0.1\n        self.population = None\n        self.fitness = None\n\n    def initialize_population(self):\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, \n                                            (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def adaptive_differential_evolution(self, idx, func):\n        indices = [i for i in range(self.population_size) if i != idx]\n        np.random.shuffle(indices)\n        a, b, c = indices[:3]\n        mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        adaptive_factor = 0.4 + 0.6 * np.random.rand()\n        mutant = self.population[idx] + adaptive_factor * (mutant - self.population[idx])\n        mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n        \n        cross_points = np.random.rand(self.dim) < self.crossover_probability\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        \n        trial = np.where(cross_points, mutant, self.population[idx])\n        trial_fitness = func(trial)\n        \n        if trial_fitness < self.fitness[idx]:\n            self.population[idx] = trial\n            self.fitness[idx] = trial_fitness\n\n    def stochastic_gradient_approx(self, idx, func):\n        gradient = np.random.normal(0, 1, self.dim)\n        adaptive_lr = self.learning_rate / (1 + idx / self.population_size)\n        candidate = self.population[idx] - adaptive_lr * gradient\n        candidate += self.diversity_factor * np.random.normal(0, 1, self.dim)\n        candidate = np.clip(candidate, self.lower_bound, self.upper_bound)\n        candidate_fitness = func(candidate)\n        if candidate_fitness < self.fitness[idx]:\n            self.population[idx] = candidate\n            self.fitness[idx] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population()\n        evaluations = 0\n        self.evaluate_population(func)\n        evaluations += self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                self.adaptive_differential_evolution(i, func)\n                evaluations += 1\n\n                if evaluations >= self.budget:\n                    break\n\n                self.stochastic_gradient_approx(i, func)\n                evaluations += 1\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "EnhancedProbabilisticOptimizer", "description": "Enhanced probabilistic hybrid optimizer balancing exploration and exploitation through diversity-oriented selection and adaptive parameter tuning for optimal convergence.", "configspace": "", "generation": 38, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "1459935d-0a5b-47de-8bfe-69f52cb3cb6c", "metadata": {"aucs": [0.796727445346743, 0.7946092935536371, 0.8025157468331956, 0.7795890428061506, 0.7652866403837684, 0.7888980264133363, 0.7851539828124771, 0.7801166288092796, 0.778684214476141, 0.6066596860215113, 0.5509488918434079, 0.584709500115218, 0.6151880393982149, 0.6128285196462557, 0.5928554548480235, 0.18781852246903208, 0.6051102165828588, 0.6050716765741883, 0.12056849613284826, 0.10627786970516262, 0.11092740344636509, 0.12458142503345515, 0.10225070914012868, 0.10214739878052792, 0.11007052847089493, 0.11303443664409152, 0.11346829363668931, 0.09058514750802149, 0.09136224141572435, 0.09346304265481342, 0.08413688806219866, 0.10560994817344638, 0.09386944951658027, 0.1066829182603829, 0.09525027475772718, 0.09078080677184064, 0.8379701410373176, 0.8069154658397715, 0.8108437032002754, 0.8202640448333322, 0.7815128713795206, 0.8451084830274826, 0.836376388134602, 0.8441441978939892, 0.8509693104465095, 0.37150586919602946, 0.34273942956965175, 0.3313822553845982, 0.34968755932637086, 0.32960730205573996, 0.3754984208710611, 0.38284578850911233, 0.3840456442426208, 0.3280564413760412, 0.7576709220936438, 0.7335556841917643, 0.7658090217912067, 0.7934167558179147, 0.8025617081483369, 0.7999764269594076, 0.7299292035530152, 0.7697788108067551, 0.7651252822797626, 0.16167439560696928, 0.18696031846775318, 0.21158580997319565, 0.244349141547606, 0.2890559956537958, 0.16298570563996873, 0.21750653437837209, 0.21122767559906996, 0.22550289679700497, 0.15305768362463734, 0.14884944831507896, 0.18600965489753707, 0.1541357033150358, 0.19832744782165168, 0.18983314847500432, 0.1953210887165725, 0.20165646735698872, 0.13088573983753993, 0.489476775395826, 0.460206761694571, 0.280690722270316, 0.34064732845138956, 0.522953329166158, 0.46452239779932414, 0.4196789194960461, 0.4727221489210359, 0.44131563048738687, 0.6540713674624187, 0.6128130384068273, 0.5553131009305013, 0.5776200055109268, 0.4723536786910414, 0.6219680956281209, 0.6455804612075908, 0.6109257481083044, 0.5996992464126676, 0.15220127888973134, 0.11018658007737869, 0.2413715712685326, 0.2296484097510183, 0.2572737636354798, 0.10622686325653308, 0.14604068872930986, 0.14809138800719435, 0.12257392403735012, 0.36655256082417187, 0.34632185847096675, 0.3516395670957846, 0.33828319802853724, 0.3572041238804262, 0.35621925345698524, 0.34628041224843775, 0.36205674625584605, 0.3801826028652373, 0.7284525303733058, 0.7334889541026953, 0.7386557191632159, 0.7055754468414382, 0.7222856768071027, 0.7194774022217179, 0.518854950686749, 0.7321096452535195, 0.7397060954282213, 0.10374399773694243, 0.104282622260642, 0.1086381702881759, 0.11429613558107654, 0.11511930056195652, 0.10035386421161252, 0.09599830676572241, 0.11347281801819442, 0.11470225552281121, 0.14952999643648723, 0.16817216547016145, 0.17310734903563885, 0.1855986179207637, 0.18359480037739373, 0.19508916572226054, 0.20087729718879477, 0.17807185486405164, 0.20119801391244463, 0.3682121014468256, 0.35073539948175425, 0.3330439038672147, 0.4157077631363848, 0.37474021495439724, 0.36760475650845414, 0.38008256501576043, 0.39084781839078786, 0.3786757352451906, 0.30037946434366947, 0.2861866860206934, 0.2891587957817804, 0.2835627691001067, 0.2779602895144492, 0.28387893685888044, 0.29400381025955125, 0.3244615423116647, 0.30188351717337136, 0.20494558183242018, 0.19905376550038034, 0.20406514428030742, 0.2229753496453719, 0.1980660060283188, 0.21539123082194234, 0.21928087314384304, 0.20854328449838244, 0.2102975078755156, 0.18082128476991477, 0.1831737258940077, 0.18089956975930732, 0.18277323416104252, 0.1792695777259361, 0.17816092407226292, 0.19628636488975992, 0.19237612183491615, 0.1874718333986961, 0.6294666203187917, 0.602690555999032, 0.5454952462675013, 0.7727337342883757, 0.25677125313719296, 0.588947972014151, 0.5053651729784945, 0.42433758940449484, 0.610090527727432, 0.7148365734773846, 0.39316249107203016, 0.4609914040189693, 0.5414475146943718, 0.3738614312609654, 0.2972834077003401, 0.2719039283446757, 0.4464259994844204, 0.6865015215956998, 0.18541089636272012, 0.17521600614441235, 0.18498258988690697, 0.1923345960637889, 0.19442272937612437, 0.18424555156947642, 0.17449564646480575, 0.18372628686318526, 0.18823517758491137, 0.07953923390817585, 0.08429672806217459, 0.08351517483047155, 0.07872531348008505, 0.0885755282031725, 0.09071254814221597, 0.07022751225803625, 0.07907174920771143, 0.08022853716110878]}, "mutation_prompt": null}
{"id": "13e1776d-a669-4772-8918-16da608a28c9", "solution": "import numpy as np\n\nclass EnhancedProbabilisticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = 40\n        self.mutation_factor = 0.85\n        self.crossover_probability = 0.95\n        self.learning_rate = 0.2\n        self.diversity_factor = 0.1\n        self.population = None\n        self.fitness = None\n\n    def initialize_population(self):\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, \n                                            (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def adaptive_differential_evolution(self, idx, func):\n        indices = [i for i in range(self.population_size) if i != idx]\n        np.random.shuffle(indices)\n        a, b, c = indices[:3]\n        mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        adaptive_factor = 0.4 + 0.6 * np.random.rand()\n        mutant = self.population[idx] + adaptive_factor * (mutant - self.population[idx])\n        mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n        \n        cross_points = np.random.rand(self.dim) < self.crossover_probability\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        \n        trial = np.where(cross_points, mutant, self.population[idx])\n        trial_fitness = func(trial)\n        \n        if trial_fitness < self.fitness[idx]:\n            self.population[idx] = trial\n            self.fitness[idx] = trial_fitness\n\n    def stochastic_gradient_approx(self, idx, func):\n        gradient = np.random.normal(0, 1, self.dim)\n        adaptive_lr = self.learning_rate / (1 + idx / self.population_size)\n        candidate = self.population[idx] - adaptive_lr * gradient\n        candidate += self.diversity_factor * np.random.normal(0, 1, self.dim)\n        candidate = np.clip(candidate, self.lower_bound, self.upper_bound)\n        candidate_fitness = func(candidate)\n        if candidate_fitness < self.fitness[idx]:\n            self.population[idx] = candidate\n            self.fitness[idx] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population()\n        evaluations = 0\n        self.evaluate_population(func)\n        evaluations += self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                self.adaptive_differential_evolution(i, func)\n                evaluations += 1\n\n                if evaluations >= self.budget:\n                    break\n\n                self.stochastic_gradient_approx(i, func)\n                evaluations += 1\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "EnhancedProbabilisticOptimizer", "description": "Enhanced probabilistic hybrid optimizer balancing exploration and exploitation through diversity-oriented selection and adaptive parameter tuning for optimal convergence.", "configspace": "", "generation": 38, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "1459935d-0a5b-47de-8bfe-69f52cb3cb6c", "metadata": {"aucs": [0.796727445346743, 0.7946092935536371, 0.8025157468331956, 0.7795890428061506, 0.7652866403837684, 0.7888980264133363, 0.7851539828124771, 0.7801166288092796, 0.778684214476141, 0.6066596860215113, 0.5509488918434079, 0.584709500115218, 0.6151880393982149, 0.6128285196462557, 0.5928554548480235, 0.18781852246903208, 0.6051102165828588, 0.6050716765741883, 0.12056849613284826, 0.10627786970516262, 0.11092740344636509, 0.12458142503345515, 0.10225070914012868, 0.10214739878052792, 0.11007052847089493, 0.11303443664409152, 0.11346829363668931, 0.09058514750802149, 0.09136224141572435, 0.09346304265481342, 0.08413688806219866, 0.10560994817344638, 0.09386944951658027, 0.1066829182603829, 0.09525027475772718, 0.09078080677184064, 0.8379701410373176, 0.8069154658397715, 0.8108437032002754, 0.8202640448333322, 0.7815128713795206, 0.8451084830274826, 0.836376388134602, 0.8441441978939892, 0.8509693104465095, 0.37150586919602946, 0.34273942956965175, 0.3313822553845982, 0.34968755932637086, 0.32960730205573996, 0.3754984208710611, 0.38284578850911233, 0.3840456442426208, 0.3280564413760412, 0.7576709220936438, 0.7335556841917643, 0.7658090217912067, 0.7934167558179147, 0.8025617081483369, 0.7999764269594076, 0.7299292035530152, 0.7697788108067551, 0.7651252822797626, 0.16167439560696928, 0.18696031846775318, 0.21158580997319565, 0.244349141547606, 0.2890559956537958, 0.16298570563996873, 0.21750653437837209, 0.21122767559906996, 0.22550289679700497, 0.15305768362463734, 0.14884944831507896, 0.18600965489753707, 0.1541357033150358, 0.19832744782165168, 0.18983314847500432, 0.1953210887165725, 0.20165646735698872, 0.13088573983753993, 0.489476775395826, 0.460206761694571, 0.280690722270316, 0.34064732845138956, 0.522953329166158, 0.46452239779932414, 0.4196789194960461, 0.4727221489210359, 0.44131563048738687, 0.6540713674624187, 0.6128130384068273, 0.5553131009305013, 0.5776200055109268, 0.4723536786910414, 0.6219680956281209, 0.6455804612075908, 0.6109257481083044, 0.5996992464126676, 0.15220127888973134, 0.11018658007737869, 0.2413715712685326, 0.2296484097510183, 0.2572737636354798, 0.10622686325653308, 0.14604068872930986, 0.14809138800719435, 0.12257392403735012, 0.36655256082417187, 0.34632185847096675, 0.3516395670957846, 0.33828319802853724, 0.3572041238804262, 0.35621925345698524, 0.34628041224843775, 0.36205674625584605, 0.3801826028652373, 0.7284525303733058, 0.7334889541026953, 0.7386557191632159, 0.7055754468414382, 0.7222856768071027, 0.7194774022217179, 0.518854950686749, 0.7321096452535195, 0.7397060954282213, 0.10374399773694243, 0.104282622260642, 0.1086381702881759, 0.11429613558107654, 0.11511930056195652, 0.10035386421161252, 0.09599830676572241, 0.11347281801819442, 0.11470225552281121, 0.14952999643648723, 0.16817216547016145, 0.17310734903563885, 0.1855986179207637, 0.18359480037739373, 0.19508916572226054, 0.20087729718879477, 0.17807185486405164, 0.20119801391244463, 0.3682121014468256, 0.35073539948175425, 0.3330439038672147, 0.4157077631363848, 0.37474021495439724, 0.36760475650845414, 0.38008256501576043, 0.39084781839078786, 0.3786757352451906, 0.30037946434366947, 0.2861866860206934, 0.2891587957817804, 0.2835627691001067, 0.2779602895144492, 0.28387893685888044, 0.29400381025955125, 0.3244615423116647, 0.30188351717337136, 0.20494558183242018, 0.19905376550038034, 0.20406514428030742, 0.2229753496453719, 0.1980660060283188, 0.21539123082194234, 0.21928087314384304, 0.20854328449838244, 0.2102975078755156, 0.18082128476991477, 0.1831737258940077, 0.18089956975930732, 0.18277323416104252, 0.1792695777259361, 0.17816092407226292, 0.19628636488975992, 0.19237612183491615, 0.1874718333986961, 0.6294666203187917, 0.602690555999032, 0.5454952462675013, 0.7727337342883757, 0.25677125313719296, 0.588947972014151, 0.5053651729784945, 0.42433758940449484, 0.610090527727432, 0.7148365734773846, 0.39316249107203016, 0.4609914040189693, 0.5414475146943718, 0.3738614312609654, 0.2972834077003401, 0.2719039283446757, 0.4464259994844204, 0.6865015215956998, 0.18541089636272012, 0.17521600614441235, 0.18498258988690697, 0.1923345960637889, 0.19442272937612437, 0.18424555156947642, 0.17449564646480575, 0.18372628686318526, 0.18823517758491137, 0.07953923390817585, 0.08429672806217459, 0.08351517483047155, 0.07872531348008505, 0.0885755282031725, 0.09071254814221597, 0.07022751225803625, 0.07907174920771143, 0.08022853716110878]}, "mutation_prompt": null}
{"id": "647f35a9-42c5-481d-aa24-833f85967fab", "solution": "import numpy as np\n\nclass SynergisticHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = 45\n        self.mutation_factor = 0.9  # Increased mutation factor for exploration\n        self.crossover_probability = 0.9  # Adjusted for better exploitation\n        self.learning_rate = 0.25  # Increased learning rate for faster adaptation\n        self.diversity_factor = 0.15  # Enhanced diversity to avoid local optima\n        self.population = None\n        self.fitness = None\n\n    def initialize_population(self):\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, \n                                            (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def adaptive_differential_evolution(self, idx, func):\n        indices = [i for i in range(self.population_size) if i != idx]\n        np.random.shuffle(indices)\n        a, b, c = indices[:3]\n        mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        adaptive_factor = 0.5 + 0.5 * np.random.rand()  # Adjusted adaptive factor range\n        mutant = self.population[idx] + adaptive_factor * (mutant - self.population[idx])\n        mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n        \n        cross_points = np.random.rand(self.dim) < self.crossover_probability\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        \n        trial = np.where(cross_points, mutant, self.population[idx])\n        trial_fitness = func(trial)\n        \n        if trial_fitness < self.fitness[idx]:\n            self.population[idx] = trial\n            self.fitness[idx] = trial_fitness\n\n    def stochastic_gradient_approx(self, idx, func):\n        gradient = np.random.normal(0, 1, self.dim)\n        adaptive_lr = self.learning_rate / (1 + idx / self.population_size)\n        candidate = self.population[idx] - adaptive_lr * gradient\n        candidate += self.diversity_factor * np.random.normal(0, 1, self.dim)\n        candidate = np.clip(candidate, self.lower_bound, self.upper_bound)\n        candidate_fitness = func(candidate)\n        if candidate_fitness < self.fitness[idx]:\n            self.population[idx] = candidate\n            self.fitness[idx] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population()\n        evaluations = 0\n        self.evaluate_population(func)\n        evaluations += self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                self.adaptive_differential_evolution(i, func)\n                evaluations += 1\n\n                if evaluations >= self.budget:\n                    break\n\n                self.stochastic_gradient_approx(i, func)\n                evaluations += 1\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "SynergisticHybridOptimizer", "description": "Synergistic hybrid optimizer using adaptive differential evolution and stochastic gradient adjustments with enhanced diversity for improved solution quality.", "configspace": "", "generation": 79, "fitness": 0.2984151650938042, "feedback": "The algorithm SynergisticHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.30 with standard deviation 0.21.", "error": "", "parent_id": "1459935d-0a5b-47de-8bfe-69f52cb3cb6c", "metadata": {"aucs": [0.708037467500757, 0.7156294000299348, 0.7356938277695491, 0.7074743915674169, 0.7173746483397898, 0.6976357529941377, 0.7049648078677296, 0.7095270831237401, 0.699407418478986, 0.45779038554808116, 0.4144292581605933, 0.4392613915883663, 0.4421115531080164, 0.43231559222427807, 0.43678565929918445, 0.44471294368123404, 0.4589391085976148, 0.46416636777400977, 0.1051288160977758, 0.11282024178603423, 0.09809134104534334, 0.10108935085698412, 0.1020655174119296, 0.11442562521749844, 0.10865720914249599, 0.10003583596663501, 0.10305154488828028, 0.1021759152805749, 0.08864323756673043, 0.1010929664087149, 0.08542854276571976, 0.08483749169899679, 0.087675316571308, 0.09633588148150463, 0.0957774461886991, 0.0908972231378431, 0.8289082166730375, 0.8857631875941296, 0.8270582936287552, 0.8858002158990157, 0.8723976904243478, 0.8708744816862798, 0.9021406375661778, 0.8202865357848383, 0.8880517786768838, 0.28838075923551865, 0.2892005097824544, 0.29581273678409203, 0.31140998559395394, 0.3286398520781677, 0.3227365420293423, 0.3171791026766808, 0.3196363573848744, 0.309978059553342, 0.6985442314847592, 0.7234380149770936, 0.6248634828196672, 0.7343691230029477, 0.6341826136942552, 0.7236667034264566, 0.6008091915991792, 0.6675871668251496, 0.621489101080594, 0.1796586438018225, 0.1764846551661885, 0.17690333615248732, 0.16754905733596137, 0.1749645079102129, 0.18169294316212548, 0.18231145462793164, 0.18429243019619557, 0.1820478305927795, 0.1816051511215394, 0.16179767649391374, 0.17301139365114426, 0.18269449978238717, 0.19481323439235632, 0.1705228247072027, 0.15632606469007382, 0.18558484973876033, 0.2286666811997784, 0.22703892050047525, 0.23328583411927584, 0.21149040813826503, 0.23396054148311085, 0.21654322281099336, 0.20207904904039198, 0.23481724790610026, 0.2260944993905596, 0.22280201094660212, 0.38429828172620273, 0.34099046102628383, 0.3655287604839157, 0.3290376943197083, 0.36030375569399153, 0.3665471558047616, 0.38769384107624194, 0.39076103292898035, 0.33937830614605213, 0.07511898098454473, 0.07095277352422324, 0.07770101948306363, 0.06627312282063325, 0.10855238681538226, 0.07137044525567415, 0.06699093535962242, 0.05281168256170654, 0.07041478196860151, 0.20742474645703357, 0.21714952544159316, 0.20861630008680954, 0.2375519281710956, 0.22100859882384416, 0.20784940848216538, 0.21345963994501094, 0.2138156473668652, 0.2229059914536603, 0.5360219497344539, 0.5984226381178706, 0.5647005804922722, 0.587528991749208, 0.568234847058523, 0.5594957324985936, 0.5628489343173272, 0.5901997228334013, 0.5997607725388163, 0.08999726151230547, 0.10341827344381294, 0.08091968213423661, 0.10640382162967976, 0.09569891285156451, 0.11067870519509271, 0.08959109860275216, 0.10525324047467621, 0.10946594534648502, 0.1672975041570195, 0.18192956328400212, 0.19134118088057617, 0.1932176436495966, 0.1833484994288198, 0.1765802292834605, 0.18560676214889194, 0.16472873814670952, 0.1707505665171457, 0.2856727554024716, 0.30163165189922847, 0.3033995638671203, 0.29746301479404247, 0.29733794283879733, 0.2787048183128884, 0.31055288464064557, 0.3379562702770069, 0.2922932147553087, 0.21100947557101513, 0.23090938586555454, 0.23571914824615514, 0.21012446421718867, 0.22970043283699504, 0.24264328980660566, 0.2711387299053617, 0.2195597203642351, 0.22502429759254639, 0.20266198214910314, 0.2074661549904636, 0.22004757735198077, 0.21281599807993223, 0.1940232971345336, 0.19795767275071385, 0.2124862536754767, 0.24408601664351548, 0.19884594967417857, 0.18028605432328515, 0.18856672821282483, 0.18102865965122206, 0.18757140550442386, 0.19448453655900344, 0.1920801184750648, 0.19177906841329118, 0.19047618747097284, 0.19076165644327825, 0.3094959749852807, 0.3958621133575143, 0.23918594154532735, 0.24228240782200905, 0.4137127474645801, 0.42983715831089, 0.34265316849047756, 0.3795819275227932, 0.40789277314760597, 0.6801196326512053, 0.30455490338516356, 0.35915442509807904, 0.4443682413486425, 0.2788186603881784, 0.32867180356060666, 0.2037563502543549, 0.19510268359898586, 0.2262785240514661, 0.18541224397556832, 0.18247433580658856, 0.17324848414485383, 0.18266174067791785, 0.18116428554833874, 0.1763646318868457, 0.18235280426865252, 0.18493085263511033, 0.17890711700989392, 0.08325132607515384, 0.07946188595405257, 0.08073078180043736, 0.08588415239558855, 0.08102160677537262, 0.0750955738180541, 0.0739301767853261, 0.0787725201178564, 0.08773622048443885]}, "mutation_prompt": null}
{"id": "e0cfdd9e-dc93-4129-b1dd-b7ba13125968", "solution": "import numpy as np\n\nclass EnhancedProbabilisticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = 40\n        self.mutation_factor = 0.85\n        self.crossover_probability = 0.95\n        self.learning_rate = 0.2\n        self.diversity_factor = 0.1\n        self.population = None\n        self.fitness = None\n\n    def initialize_population(self):\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, \n                                            (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def adaptive_differential_evolution(self, idx, func):\n        indices = [i for i in range(self.population_size) if i != idx]\n        np.random.shuffle(indices)\n        a, b, c = indices[:3]\n        mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        adaptive_factor = 0.4 + 0.6 * np.random.rand()\n        mutant = self.population[idx] + adaptive_factor * (mutant - self.population[idx])\n        mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n        \n        cross_points = np.random.rand(self.dim) < self.crossover_probability\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        \n        trial = np.where(cross_points, mutant, self.population[idx])\n        trial_fitness = func(trial)\n        \n        if trial_fitness < self.fitness[idx]:\n            self.population[idx] = trial\n            self.fitness[idx] = trial_fitness\n\n    def stochastic_gradient_approx(self, idx, func):\n        gradient = np.random.normal(0, 1, self.dim)\n        adaptive_lr = self.learning_rate / (1 + idx / self.population_size)\n        candidate = self.population[idx] - adaptive_lr * gradient\n        candidate += self.diversity_factor * np.random.normal(0, 1, self.dim)\n        candidate = np.clip(candidate, self.lower_bound, self.upper_bound)\n        candidate_fitness = func(candidate)\n        if candidate_fitness < self.fitness[idx]:\n            self.population[idx] = candidate\n            self.fitness[idx] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population()\n        evaluations = 0\n        self.evaluate_population(func)\n        evaluations += self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                self.adaptive_differential_evolution(i, func)\n                evaluations += 1\n\n                if evaluations >= self.budget:\n                    break\n\n                self.stochastic_gradient_approx(i, func)\n                evaluations += 1\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "EnhancedProbabilisticOptimizer", "description": "Enhanced probabilistic hybrid optimizer balancing exploration and exploitation through diversity-oriented selection and adaptive parameter tuning for optimal convergence.", "configspace": "", "generation": 38, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "1459935d-0a5b-47de-8bfe-69f52cb3cb6c", "metadata": {"aucs": [0.796727445346743, 0.7946092935536371, 0.8025157468331956, 0.7795890428061506, 0.7652866403837684, 0.7888980264133363, 0.7851539828124771, 0.7801166288092796, 0.778684214476141, 0.6066596860215113, 0.5509488918434079, 0.584709500115218, 0.6151880393982149, 0.6128285196462557, 0.5928554548480235, 0.18781852246903208, 0.6051102165828588, 0.6050716765741883, 0.12056849613284826, 0.10627786970516262, 0.11092740344636509, 0.12458142503345515, 0.10225070914012868, 0.10214739878052792, 0.11007052847089493, 0.11303443664409152, 0.11346829363668931, 0.09058514750802149, 0.09136224141572435, 0.09346304265481342, 0.08413688806219866, 0.10560994817344638, 0.09386944951658027, 0.1066829182603829, 0.09525027475772718, 0.09078080677184064, 0.8379701410373176, 0.8069154658397715, 0.8108437032002754, 0.8202640448333322, 0.7815128713795206, 0.8451084830274826, 0.836376388134602, 0.8441441978939892, 0.8509693104465095, 0.37150586919602946, 0.34273942956965175, 0.3313822553845982, 0.34968755932637086, 0.32960730205573996, 0.3754984208710611, 0.38284578850911233, 0.3840456442426208, 0.3280564413760412, 0.7576709220936438, 0.7335556841917643, 0.7658090217912067, 0.7934167558179147, 0.8025617081483369, 0.7999764269594076, 0.7299292035530152, 0.7697788108067551, 0.7651252822797626, 0.16167439560696928, 0.18696031846775318, 0.21158580997319565, 0.244349141547606, 0.2890559956537958, 0.16298570563996873, 0.21750653437837209, 0.21122767559906996, 0.22550289679700497, 0.15305768362463734, 0.14884944831507896, 0.18600965489753707, 0.1541357033150358, 0.19832744782165168, 0.18983314847500432, 0.1953210887165725, 0.20165646735698872, 0.13088573983753993, 0.489476775395826, 0.460206761694571, 0.280690722270316, 0.34064732845138956, 0.522953329166158, 0.46452239779932414, 0.4196789194960461, 0.4727221489210359, 0.44131563048738687, 0.6540713674624187, 0.6128130384068273, 0.5553131009305013, 0.5776200055109268, 0.4723536786910414, 0.6219680956281209, 0.6455804612075908, 0.6109257481083044, 0.5996992464126676, 0.15220127888973134, 0.11018658007737869, 0.2413715712685326, 0.2296484097510183, 0.2572737636354798, 0.10622686325653308, 0.14604068872930986, 0.14809138800719435, 0.12257392403735012, 0.36655256082417187, 0.34632185847096675, 0.3516395670957846, 0.33828319802853724, 0.3572041238804262, 0.35621925345698524, 0.34628041224843775, 0.36205674625584605, 0.3801826028652373, 0.7284525303733058, 0.7334889541026953, 0.7386557191632159, 0.7055754468414382, 0.7222856768071027, 0.7194774022217179, 0.518854950686749, 0.7321096452535195, 0.7397060954282213, 0.10374399773694243, 0.104282622260642, 0.1086381702881759, 0.11429613558107654, 0.11511930056195652, 0.10035386421161252, 0.09599830676572241, 0.11347281801819442, 0.11470225552281121, 0.14952999643648723, 0.16817216547016145, 0.17310734903563885, 0.1855986179207637, 0.18359480037739373, 0.19508916572226054, 0.20087729718879477, 0.17807185486405164, 0.20119801391244463, 0.3682121014468256, 0.35073539948175425, 0.3330439038672147, 0.4157077631363848, 0.37474021495439724, 0.36760475650845414, 0.38008256501576043, 0.39084781839078786, 0.3786757352451906, 0.30037946434366947, 0.2861866860206934, 0.2891587957817804, 0.2835627691001067, 0.2779602895144492, 0.28387893685888044, 0.29400381025955125, 0.3244615423116647, 0.30188351717337136, 0.20494558183242018, 0.19905376550038034, 0.20406514428030742, 0.2229753496453719, 0.1980660060283188, 0.21539123082194234, 0.21928087314384304, 0.20854328449838244, 0.2102975078755156, 0.18082128476991477, 0.1831737258940077, 0.18089956975930732, 0.18277323416104252, 0.1792695777259361, 0.17816092407226292, 0.19628636488975992, 0.19237612183491615, 0.1874718333986961, 0.6294666203187917, 0.602690555999032, 0.5454952462675013, 0.7727337342883757, 0.25677125313719296, 0.588947972014151, 0.5053651729784945, 0.42433758940449484, 0.610090527727432, 0.7148365734773846, 0.39316249107203016, 0.4609914040189693, 0.5414475146943718, 0.3738614312609654, 0.2972834077003401, 0.2719039283446757, 0.4464259994844204, 0.6865015215956998, 0.18541089636272012, 0.17521600614441235, 0.18498258988690697, 0.1923345960637889, 0.19442272937612437, 0.18424555156947642, 0.17449564646480575, 0.18372628686318526, 0.18823517758491137, 0.07953923390817585, 0.08429672806217459, 0.08351517483047155, 0.07872531348008505, 0.0885755282031725, 0.09071254814221597, 0.07022751225803625, 0.07907174920771143, 0.08022853716110878]}, "mutation_prompt": null}
{"id": "e5126e6d-8d2a-44a5-9f43-09b6e2e615c0", "solution": "import numpy as np\n\nclass ProbabilisticAdaptiveOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = 50  # Modified to potentially increase diversity\n        self.mutation_factor = 0.9  # Slightly increased for more exploration\n        self.crossover_probability = 0.9  # Lowered for increased trial variety\n        self.learning_rate = 0.25  # Increased for faster convergence\n        self.diversity_factor = 0.15  # Increased to diversify search\n        self.population = None\n        self.fitness = None\n\n    def initialize_population(self):\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, \n                                            (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def adaptive_differential_evolution(self, idx, func):\n        indices = [i for i in range(self.population_size) if i != idx]\n        np.random.shuffle(indices)\n        a, b, c = indices[:3]\n        mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        adaptive_factor = 0.5 + 0.5 * np.random.rand()  # Adaptiveness made more pronounced\n        mutant = self.population[idx] + adaptive_factor * (mutant - self.population[idx])\n        mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n        \n        cross_points = np.random.rand(self.dim) < self.crossover_probability\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        \n        trial = np.where(cross_points, mutant, self.population[idx])\n        trial_fitness = func(trial)\n        \n        if trial_fitness < self.fitness[idx]:\n            self.population[idx] = trial\n            self.fitness[idx] = trial_fitness\n\n    def stochastic_gradient_approx(self, idx, func):\n        gradient = np.random.normal(0, 1, self.dim)\n        adaptive_lr = self.learning_rate / (1 + idx / self.population_size)\n        candidate = self.population[idx] - adaptive_lr * gradient\n        candidate += self.diversity_factor * np.random.normal(0, 1, self.dim)\n        candidate = np.clip(candidate, self.lower_bound, self.upper_bound)\n        candidate_fitness = func(candidate)\n        if candidate_fitness < self.fitness[idx]:\n            self.population[idx] = candidate\n            self.fitness[idx] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population()\n        evaluations = 0\n        self.evaluate_population(func)\n        evaluations += self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                self.adaptive_differential_evolution(i, func)\n                evaluations += 1\n\n                if evaluations >= self.budget:\n                    break\n\n                self.stochastic_gradient_approx(i, func)\n                evaluations += 1\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "ProbabilisticAdaptiveOptimizer", "description": "Probabilistic Adaptive Evolutionary Optimizer enhancing global and local search using adaptive parameters and Gaussian perturbations.", "configspace": "", "generation": 81, "fitness": 0.2867892325103422, "feedback": "The algorithm ProbabilisticAdaptiveOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.29 with standard deviation 0.21.", "error": "", "parent_id": "1459935d-0a5b-47de-8bfe-69f52cb3cb6c", "metadata": {"aucs": [0.685996125823626, 0.6726892559032343, 0.6766863896319186, 0.6900952679215657, 0.6774749638542246, 0.6613003689390202, 0.6801080257844216, 0.6965612571853722, 0.7043901294259026, 0.3565872467395632, 0.3821492911772242, 0.3654771701327091, 0.39600847461543154, 0.3768865406771509, 0.3985105801175338, 0.37592008896046025, 0.36963483292670785, 0.403389848389779, 0.09050292072726007, 0.09529915279846812, 0.10241622210152834, 0.12179534190328833, 0.1034002598589403, 0.10334062346297412, 0.09702036901156696, 0.09711905028921397, 0.10244982073967912, 0.09514386600962821, 0.08504239481289022, 0.08928488295437087, 0.10003866216411295, 0.09130897319706166, 0.08867036915920234, 0.09130727307621, 0.08219868374556505, 0.10161009959200695, 0.9195024854240494, 0.8920774362778122, 0.9595505261913595, 0.8480792031134339, 0.8840701491577269, 0.8606574896536289, 0.8515900631267588, 0.9018063547664423, 0.8327530282438222, 0.28421152905083846, 0.2829619547453067, 0.289355534392855, 0.2860021745037007, 0.28203625868768645, 0.2880459503749583, 0.28588320442946213, 0.2642550180269342, 0.2952960523958589, 0.6087763127333304, 0.6306319766104242, 0.6480419992639233, 0.6554155495901273, 0.6509337084661213, 0.6688861131708337, 0.5546401176813094, 0.6641736485707055, 0.6154805268967648, 0.17152571275638728, 0.13890560139611174, 0.18229847003340616, 0.17561584315208079, 0.1638271210685469, 0.17072118012519655, 0.1740491625484356, 0.18675279859465488, 0.1917066221099687, 0.1945239164613466, 0.159891928177334, 0.1754634257312928, 0.15759334338380415, 0.18057483625469883, 0.17395291722287054, 0.21352879287018955, 0.16065268749594963, 0.17703709087242692, 0.17199142893325203, 0.18317279474104464, 0.20568814108628364, 0.18751791006298024, 0.18064141841967363, 0.19064388003728094, 0.2078381934835214, 0.18045479632043482, 0.23803357451015594, 0.34161465662829416, 0.34589459722289206, 0.35802767816351866, 0.3012456949703006, 0.3232730088676643, 0.34258971311510666, 0.3232327009151258, 0.3639532323969351, 0.39928848965314234, 0.0461774300815867, 0.077975687978725, 0.057129651094942946, 0.06604374984064942, 0.09013287591436403, 0.06389973399549764, 0.07191952932594636, 0.03296009099218278, 0.06324875509494032, 0.2014027167496506, 0.19816668226985446, 0.182253822982277, 0.20634110712625253, 0.19137242500957274, 0.20791604008528164, 0.17645369411287193, 0.18040151253400216, 0.2007670446142944, 0.5416524930427306, 0.5368015292660966, 0.5444628099606843, 0.5234747725756119, 0.5351097389066726, 0.5342906194121164, 0.5483628818629414, 0.5299041517307179, 0.5360716772214853, 0.09773717804495208, 0.09440043080194738, 0.09289183143407242, 0.09133349327599483, 0.09928581207841303, 0.11318735480233078, 0.09649741295606395, 0.09369332962414989, 0.0970560664361586, 0.17006676103773166, 0.19741875401049325, 0.16216012912989575, 0.1923065935967706, 0.1809141998297088, 0.1857275819668719, 0.17553087977028647, 0.2015468138003148, 0.20149436518262542, 0.28356074813129417, 0.28238439618825795, 0.2917550910994372, 0.27024772979665646, 0.2913052864086956, 0.3069234319019327, 0.2967259969116801, 0.29041295286952973, 0.317535017814604, 0.21830526514153858, 0.20715057447240948, 0.23440164674611574, 0.22109529962571683, 0.20748293846214627, 0.21624532397678986, 0.22202662960580188, 0.21500825807642499, 0.22056598048187193, 0.20145888515539878, 0.20324177279839373, 0.21280220409017025, 0.19094964860208985, 0.2174469532208112, 0.20249867336768868, 0.1803366144005435, 0.1998169401582346, 0.20347677121534957, 0.18291837643820152, 0.1770488900888969, 0.18943557122984545, 0.1858121541331087, 0.19135579648849343, 0.18548670620296293, 0.18246299456205917, 0.18919686870637098, 0.17795223691053175, 0.3597534147245832, 0.33711163025130186, 0.47839106111303353, 0.3646488720464306, 0.37809955734390865, 0.36996206955545996, 0.32864918012975564, 0.34568754676102054, 0.3701074006045595, 0.3528880239476001, 0.35552844655195726, 0.3457939190961039, 0.34767530224120125, 0.28100265461419893, 0.34698942693573476, 0.20315072493729136, 0.32587028991409484, 0.3518489849602272, 0.21368150746258663, 0.197160243335338, 0.17793572555829174, 0.2092278700592598, 0.1936428494479131, 0.1755801814029403, 0.1911802831667796, 0.1784140359550065, 0.20153743151400405, 0.08258576491808012, 0.07744576758221833, 0.07394143666740616, 0.07927934775598877, 0.07744222224148567, 0.07606767701041095, 0.07516240725263701, 0.08007142907662779, 0.08153414991939845]}, "mutation_prompt": null}
{"id": "a8c0f0c6-9ca7-45b9-b5fa-bf150ab47d62", "solution": "import numpy as np\n\nclass EnhancedProbabilisticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = 40\n        self.mutation_factor = 0.85\n        self.crossover_probability = 0.95\n        self.learning_rate = 0.2\n        self.diversity_factor = 0.1\n        self.population = None\n        self.fitness = None\n\n    def initialize_population(self):\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, \n                                            (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def adaptive_differential_evolution(self, idx, func):\n        indices = [i for i in range(self.population_size) if i != idx]\n        np.random.shuffle(indices)\n        a, b, c = indices[:3]\n        mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        adaptive_factor = 0.4 + 0.6 * np.random.rand()\n        mutant = self.population[idx] + adaptive_factor * (mutant - self.population[idx])\n        mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n        \n        cross_points = np.random.rand(self.dim) < self.crossover_probability\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        \n        trial = np.where(cross_points, mutant, self.population[idx])\n        trial_fitness = func(trial)\n        \n        if trial_fitness < self.fitness[idx]:\n            self.population[idx] = trial\n            self.fitness[idx] = trial_fitness\n\n    def stochastic_gradient_approx(self, idx, func):\n        gradient = np.random.normal(0, 1, self.dim)\n        adaptive_lr = self.learning_rate / (1 + idx / self.population_size)\n        candidate = self.population[idx] - adaptive_lr * gradient\n        candidate += self.diversity_factor * np.random.normal(0, 1, self.dim)\n        candidate = np.clip(candidate, self.lower_bound, self.upper_bound)\n        candidate_fitness = func(candidate)\n        if candidate_fitness < self.fitness[idx]:\n            self.population[idx] = candidate\n            self.fitness[idx] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population()\n        evaluations = 0\n        self.evaluate_population(func)\n        evaluations += self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                self.adaptive_differential_evolution(i, func)\n                evaluations += 1\n\n                if evaluations >= self.budget:\n                    break\n\n                self.stochastic_gradient_approx(i, func)\n                evaluations += 1\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "EnhancedProbabilisticOptimizer", "description": "Enhanced probabilistic hybrid optimizer balancing exploration and exploitation through diversity-oriented selection and adaptive parameter tuning for optimal convergence.", "configspace": "", "generation": 38, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "1459935d-0a5b-47de-8bfe-69f52cb3cb6c", "metadata": {"aucs": [0.796727445346743, 0.7946092935536371, 0.8025157468331956, 0.7795890428061506, 0.7652866403837684, 0.7888980264133363, 0.7851539828124771, 0.7801166288092796, 0.778684214476141, 0.6066596860215113, 0.5509488918434079, 0.584709500115218, 0.6151880393982149, 0.6128285196462557, 0.5928554548480235, 0.18781852246903208, 0.6051102165828588, 0.6050716765741883, 0.12056849613284826, 0.10627786970516262, 0.11092740344636509, 0.12458142503345515, 0.10225070914012868, 0.10214739878052792, 0.11007052847089493, 0.11303443664409152, 0.11346829363668931, 0.09058514750802149, 0.09136224141572435, 0.09346304265481342, 0.08413688806219866, 0.10560994817344638, 0.09386944951658027, 0.1066829182603829, 0.09525027475772718, 0.09078080677184064, 0.8379701410373176, 0.8069154658397715, 0.8108437032002754, 0.8202640448333322, 0.7815128713795206, 0.8451084830274826, 0.836376388134602, 0.8441441978939892, 0.8509693104465095, 0.37150586919602946, 0.34273942956965175, 0.3313822553845982, 0.34968755932637086, 0.32960730205573996, 0.3754984208710611, 0.38284578850911233, 0.3840456442426208, 0.3280564413760412, 0.7576709220936438, 0.7335556841917643, 0.7658090217912067, 0.7934167558179147, 0.8025617081483369, 0.7999764269594076, 0.7299292035530152, 0.7697788108067551, 0.7651252822797626, 0.16167439560696928, 0.18696031846775318, 0.21158580997319565, 0.244349141547606, 0.2890559956537958, 0.16298570563996873, 0.21750653437837209, 0.21122767559906996, 0.22550289679700497, 0.15305768362463734, 0.14884944831507896, 0.18600965489753707, 0.1541357033150358, 0.19832744782165168, 0.18983314847500432, 0.1953210887165725, 0.20165646735698872, 0.13088573983753993, 0.489476775395826, 0.460206761694571, 0.280690722270316, 0.34064732845138956, 0.522953329166158, 0.46452239779932414, 0.4196789194960461, 0.4727221489210359, 0.44131563048738687, 0.6540713674624187, 0.6128130384068273, 0.5553131009305013, 0.5776200055109268, 0.4723536786910414, 0.6219680956281209, 0.6455804612075908, 0.6109257481083044, 0.5996992464126676, 0.15220127888973134, 0.11018658007737869, 0.2413715712685326, 0.2296484097510183, 0.2572737636354798, 0.10622686325653308, 0.14604068872930986, 0.14809138800719435, 0.12257392403735012, 0.36655256082417187, 0.34632185847096675, 0.3516395670957846, 0.33828319802853724, 0.3572041238804262, 0.35621925345698524, 0.34628041224843775, 0.36205674625584605, 0.3801826028652373, 0.7284525303733058, 0.7334889541026953, 0.7386557191632159, 0.7055754468414382, 0.7222856768071027, 0.7194774022217179, 0.518854950686749, 0.7321096452535195, 0.7397060954282213, 0.10374399773694243, 0.104282622260642, 0.1086381702881759, 0.11429613558107654, 0.11511930056195652, 0.10035386421161252, 0.09599830676572241, 0.11347281801819442, 0.11470225552281121, 0.14952999643648723, 0.16817216547016145, 0.17310734903563885, 0.1855986179207637, 0.18359480037739373, 0.19508916572226054, 0.20087729718879477, 0.17807185486405164, 0.20119801391244463, 0.3682121014468256, 0.35073539948175425, 0.3330439038672147, 0.4157077631363848, 0.37474021495439724, 0.36760475650845414, 0.38008256501576043, 0.39084781839078786, 0.3786757352451906, 0.30037946434366947, 0.2861866860206934, 0.2891587957817804, 0.2835627691001067, 0.2779602895144492, 0.28387893685888044, 0.29400381025955125, 0.3244615423116647, 0.30188351717337136, 0.20494558183242018, 0.19905376550038034, 0.20406514428030742, 0.2229753496453719, 0.1980660060283188, 0.21539123082194234, 0.21928087314384304, 0.20854328449838244, 0.2102975078755156, 0.18082128476991477, 0.1831737258940077, 0.18089956975930732, 0.18277323416104252, 0.1792695777259361, 0.17816092407226292, 0.19628636488975992, 0.19237612183491615, 0.1874718333986961, 0.6294666203187917, 0.602690555999032, 0.5454952462675013, 0.7727337342883757, 0.25677125313719296, 0.588947972014151, 0.5053651729784945, 0.42433758940449484, 0.610090527727432, 0.7148365734773846, 0.39316249107203016, 0.4609914040189693, 0.5414475146943718, 0.3738614312609654, 0.2972834077003401, 0.2719039283446757, 0.4464259994844204, 0.6865015215956998, 0.18541089636272012, 0.17521600614441235, 0.18498258988690697, 0.1923345960637889, 0.19442272937612437, 0.18424555156947642, 0.17449564646480575, 0.18372628686318526, 0.18823517758491137, 0.07953923390817585, 0.08429672806217459, 0.08351517483047155, 0.07872531348008505, 0.0885755282031725, 0.09071254814221597, 0.07022751225803625, 0.07907174920771143, 0.08022853716110878]}, "mutation_prompt": null}
{"id": "d12d9358-7725-4fd9-be1d-9fcc1a4ff3b8", "solution": "import numpy as np\n\nclass ProbabilisticAdaptiveOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = 40\n        self.mutation_factor = 0.9\n        self.crossover_probability = 0.9\n        self.learning_rate = 0.25\n        self.diversity_factor = 0.15\n        self.population = None\n        self.fitness = None\n\n    def initialize_population(self):\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, \n                                            (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def adaptive_differential_evolution(self, idx, func):\n        indices = [i for i in range(self.population_size) if i != idx]\n        np.random.shuffle(indices)\n        a, b, c = indices[:3]\n        mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        adaptive_factor = 0.5 + 0.5 * np.random.rand()\n        mutant = self.population[idx] + adaptive_factor * (mutant - self.population[idx])\n        mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n        \n        cross_points = np.random.rand(self.dim) < self.crossover_probability\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        \n        trial = np.where(cross_points, mutant, self.population[idx])\n        trial_fitness = func(trial)\n        \n        if trial_fitness < self.fitness[idx]:\n            self.population[idx] = trial\n            self.fitness[idx] = trial_fitness\n\n    def stochastic_gradient_approx(self, idx, func):\n        gradient = np.random.normal(0, 1, self.dim)\n        adaptive_lr = self.learning_rate / (1 + 0.5 * idx / self.population_size)\n        candidate = self.population[idx] - adaptive_lr * gradient\n        candidate += self.diversity_factor * np.random.normal(0, 1, self.dim)\n        candidate = np.clip(candidate, self.lower_bound, self.upper_bound)\n        candidate_fitness = func(candidate)\n        if candidate_fitness < self.fitness[idx]:\n            self.population[idx] = candidate\n            self.fitness[idx] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population()\n        evaluations = 0\n        self.evaluate_population(func)\n        evaluations += self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                self.adaptive_differential_evolution(i, func)\n                evaluations += 1\n\n                if evaluations >= self.budget:\n                    break\n\n                self.stochastic_gradient_approx(i, func)\n                evaluations += 1\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "ProbabilisticAdaptiveOptimizer", "description": "Probabilistic adaptive hybrid optimizer leveraging enhanced mutation strategies and adaptive learning for improved optimization performance.", "configspace": "", "generation": 83, "fitness": 0.3186172730859664, "feedback": "The algorithm ProbabilisticAdaptiveOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.32 with standard deviation 0.22.", "error": "", "parent_id": "1459935d-0a5b-47de-8bfe-69f52cb3cb6c", "metadata": {"aucs": [0.7448193241418094, 0.7485054449609062, 0.7316933766342273, 0.7478696005122245, 0.7379999210573025, 0.7415807948427882, 0.7529189348020315, 0.7508675598483016, 0.7430088650397445, 0.5083170830384953, 0.5182690881071768, 0.5095277779184482, 0.4937053470843026, 0.5201037646663973, 0.4942442074260929, 0.48166062755320516, 0.5077856355716113, 0.5186417757934657, 0.11265801075597848, 0.10472931550106601, 0.11076982250851874, 0.11084219063734668, 0.0928053200853497, 0.10517658746757974, 0.1038343575834293, 0.10765128201751217, 0.10695956132347018, 0.10330879540552285, 0.09271563024283946, 0.08995443971392181, 0.08941044198617842, 0.09446767863026273, 0.09044084447974587, 0.0986939114574763, 0.0851232562962645, 0.09787792963665287, 0.8794225831780089, 0.8875438349874691, 0.852437737684449, 0.8785894174971971, 0.8746615086484284, 0.8570008544581441, 0.8346843270220169, 0.8600079435113326, 0.908318424985156, 0.337089194983903, 0.3359256946974414, 0.3422881803052875, 0.35397512366640527, 0.3445552095692498, 0.33615027786155083, 0.28468144624616254, 0.33831991521541116, 0.31974948095249967, 0.6852396877421179, 0.7034094692225299, 0.6973978789122826, 0.7047894693954981, 0.7333431318034231, 0.7112399121807291, 0.6602644230374237, 0.7063242855095795, 0.6681150080100052, 0.18235785535293514, 0.1826530692438092, 0.15306590712730916, 0.1480441297078754, 0.24774416863532212, 0.14918808486226265, 0.2132251858511398, 0.23156339521200686, 0.1966616872787369, 0.17307108510291214, 0.17408082621425425, 0.2061956778111832, 0.21742308810326239, 0.17036294723946188, 0.1639507814040173, 0.17608222969766119, 0.16461326176253588, 0.15046750449280322, 0.28431767460643775, 0.2705074124512101, 0.23029461420873154, 0.262893051369279, 0.261238616601571, 0.22917291162394726, 0.26832567615096903, 0.2466772170816245, 0.26214371607299247, 0.4020198322695505, 0.367293412363621, 0.41379994255717, 0.3894136364267424, 0.3810893447667434, 0.36674625557850005, 0.4267366940315158, 0.40833400265070197, 0.4259949949134554, 0.06350654143806445, 0.08116987625209948, 0.07011037387617358, 0.17573307687950657, 0.07552466699546456, 0.10493072642223522, 0.1165567478995303, 0.0632495223201216, 0.1009222315062791, 0.2428439611232931, 0.24240832369717924, 0.2503424850416681, 0.2595794051332979, 0.27712242889764493, 0.2514421171194057, 0.2664669998922906, 0.2299745008302253, 0.23605979080492823, 0.6079683655781406, 0.6390671567458612, 0.588867147031616, 0.5846781936317635, 0.5954228379389401, 0.6188467810441316, 0.6248429065878642, 0.6146194042070718, 0.6084399769331674, 0.11002663216354103, 0.103578329117753, 0.0987912038124592, 0.09618212424797801, 0.10526248373411218, 0.1054709988397402, 0.10373492861196099, 0.09479741009565823, 0.10479786017544412, 0.1680056956512359, 0.18410915572029785, 0.18100427286136356, 0.21737204427896561, 0.18796447248865356, 0.17700523724715478, 0.18950505796119665, 0.19394423995842092, 0.1707990598990078, 0.310626845296058, 0.3208880493353792, 0.32579539602954655, 0.33764140872361215, 0.2919037110655387, 0.29724372107700114, 0.3146973300666933, 0.33955589859864954, 0.30602187877404585, 0.23833340650731039, 0.2523407407017415, 0.24068795530081588, 0.2569187992109121, 0.2451249033417705, 0.23470701868070998, 0.24811619598437074, 0.27980258202660024, 0.24392204890339542, 0.21275886001568156, 0.22426453328244655, 0.2028561625534573, 0.2152411003114474, 0.1940861644725117, 0.18469205308612902, 0.1998335147070417, 0.20806814176030886, 0.22366352116403565, 0.19742317268594567, 0.18436667635120485, 0.1944538660130123, 0.17573464020981833, 0.18865940930384273, 0.17624880358027117, 0.1823151399695665, 0.19121531040782047, 0.18610264454588854, 0.6120043279995686, 0.18846025500012686, 0.39808004313855383, 0.6649856013340254, 0.49129749439740644, 0.5388814661363741, 0.2922746162238392, 0.19267256626665918, 0.39487775175794193, 0.6514584388069686, 0.3928929343310086, 0.3850100126615471, 0.35306001653662433, 0.31461681778332806, 0.3564466524930835, 0.30818859274678867, 0.382288705877723, 0.3208781378624357, 0.19900203162063013, 0.16998012032274035, 0.18984882568234684, 0.19215540626313743, 0.2115362804986517, 0.18522465215577555, 0.1842167612350013, 0.180674651014594, 0.1959119156570005, 0.08414126565349722, 0.0802900380170033, 0.09552416251771156, 0.08046487707247285, 0.08242112737778984, 0.08531333568208832, 0.07177615062754872, 0.07300939474841983, 0.06989338549523039]}, "mutation_prompt": null}
{"id": "2507cebd-60dd-4ae0-aa17-a32d53d163fb", "solution": "import numpy as np\n\nclass DualStrategyOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = 50  # increased population size\n        self.mutation_factor = 0.85\n        self.crossover_probability = 0.9  # slightly reduced crossover probability\n        self.learning_rate = 0.1  # reduced learning rate\n        self.diversity_factor = 0.15  # increased diversity factor\n        self.population = None\n        self.fitness = None\n\n    def initialize_population(self):\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, \n                                            (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def adaptive_differential_evolution(self, idx, func):\n        indices = [i for i in range(self.population_size) if i != idx]\n        np.random.shuffle(indices)\n        a, b, c = indices[:3]\n        mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        adaptive_factor = 0.5 + 0.5 * np.random.rand()  # slightly altered adaptive factor\n        mutant = self.population[idx] + adaptive_factor * (mutant - self.population[idx])\n        mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n\n        cross_points = np.random.rand(self.dim) < self.crossover_probability\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n\n        trial = np.where(cross_points, mutant, self.population[idx])\n        trial_fitness = func(trial)\n\n        if trial_fitness < self.fitness[idx]:\n            self.population[idx] = trial\n            self.fitness[idx] = trial_fitness\n\n    def stochastic_gradient_approx(self, idx, func):\n        gradient = np.random.normal(0, 1, self.dim)\n        adaptive_lr = self.learning_rate / (1 + idx / (2 * self.population_size))  # altered adaptive learning rate\n        candidate = self.population[idx] - adaptive_lr * gradient\n        candidate += self.diversity_factor * np.random.normal(0, 1, self.dim)\n        candidate = np.clip(candidate, self.lower_bound, self.upper_bound)\n        candidate_fitness = func(candidate)\n        if candidate_fitness < self.fitness[idx]:\n            self.population[idx] = candidate\n            self.fitness[idx] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population()\n        evaluations = 0\n        self.evaluate_population(func)\n        evaluations += self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                self.adaptive_differential_evolution(i, func)\n                evaluations += 1\n\n                if evaluations >= self.budget:\n                    break\n\n                self.stochastic_gradient_approx(i, func)\n                evaluations += 1\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "DualStrategyOptimizer", "description": "Dual-strategy optimizer blending adaptive differential evolution with stochastic gradient perturbation and dynamic population adjustments for enhanced convergence.", "configspace": "", "generation": 84, "fitness": 0.29806533350924885, "feedback": "The algorithm DualStrategyOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.30 with standard deviation 0.21.", "error": "", "parent_id": "1459935d-0a5b-47de-8bfe-69f52cb3cb6c", "metadata": {"aucs": [0.7147667340292495, 0.7128073396669508, 0.7036339112914356, 0.6993415760120307, 0.7068823630622052, 0.7138934862516686, 0.7182277944138333, 0.6968386569544687, 0.7046706411154069, 0.4128495778583172, 0.42360726498504464, 0.4399062690744817, 0.4156520355383245, 0.38771514217845393, 0.41407207844987426, 0.4357482745343465, 0.4174397538531529, 0.4106275732434669, 0.10991365025233357, 0.1032341240648953, 0.10295444074760018, 0.09180262817545248, 0.1098648433137509, 0.10517514953228979, 0.10924238378214446, 0.10438868803453916, 0.09714156682167341, 0.11012490586889401, 0.09061296382114903, 0.0883005069600975, 0.09150595835477848, 0.09122573675352963, 0.09706314668321736, 0.08470137538070777, 0.08857932096780985, 0.0913435547732282, 0.9329799949311501, 0.8872837987288742, 0.8522477480346032, 0.7985467779116604, 0.8217858259419025, 0.7832081251274741, 0.8561554636170818, 0.8423538386993645, 0.8217646229800215, 0.2857684692402258, 0.2968920700043317, 0.30017625144403337, 0.30835771616845187, 0.2973948014893365, 0.322510518227074, 0.2943403598645292, 0.3149247789959062, 0.27522433815588343, 0.6478222695815722, 0.6275116017922779, 0.6258372498361979, 0.7561537868919624, 0.7018332253593413, 0.693896393526292, 0.703941617428591, 0.637521819170018, 0.6177384772986272, 0.16740478673418369, 0.23313841615932918, 0.21947884403828644, 0.1784303364050588, 0.17964216315342985, 0.19552557080516464, 0.18602561981418442, 0.18155139038267032, 0.18280641984472257, 0.17905407031138443, 0.16880603271547112, 0.21732386929072023, 0.20177144401134772, 0.19256029850869572, 0.17146954212816834, 0.1852794699550504, 0.17789895244094478, 0.20027053999818278, 0.1957932133210365, 0.22240580567557267, 0.1930892912619674, 0.22179769209376687, 0.21292845612039535, 0.21401675983348634, 0.2189779190834148, 0.18760113421653823, 0.2314226924769437, 0.38011541641515145, 0.3401276246786007, 0.387878711253968, 0.3495369486462456, 0.2932775669264953, 0.3593857119047744, 0.3338688213965746, 0.3603024892124945, 0.36265225094355524, 0.08280400923885267, 0.0432219372450201, 0.07052488203438823, 0.10172442315236607, 0.09966445139438185, 0.12580200016252607, 0.05542344740860994, 0.035778734636242926, 0.09908706178496507, 0.20377955927007185, 0.21762569157562717, 0.20808501941168656, 0.22078101000331918, 0.2165717472977966, 0.2188047438624514, 0.20348627015730092, 0.20049490671556602, 0.21692366809470487, 0.5627676919409665, 0.5520303395999063, 0.5838378102976367, 0.5492190896834288, 0.5598732646169111, 0.554402384698502, 0.562550608158012, 0.5737440749946694, 0.5740754193224675, 0.11594514967867198, 0.1117740769556731, 0.10541934924313845, 0.1176491237441194, 0.09203624633021013, 0.1101285117598898, 0.10254612716040068, 0.1162527173802177, 0.09891648381610796, 0.20533229580753054, 0.20877432926181405, 0.17399459489742808, 0.17334351370427104, 0.18815972240336354, 0.17984334424852677, 0.19426170430287404, 0.17279820624783437, 0.19636554461476985, 0.28718842535711053, 0.28720859304697677, 0.303043783141274, 0.2972074180872861, 0.31375012383154566, 0.2997179635446837, 0.30911159987610815, 0.30170926588077907, 0.2853928496819794, 0.2309784300217932, 0.22785292885459474, 0.2440132291903746, 0.22356479489028902, 0.21611249824517376, 0.22062639377018645, 0.2320701016355885, 0.2555453044201479, 0.2611797515977826, 0.1975984270916088, 0.20377078199475607, 0.197327154084475, 0.2014682326901095, 0.2068739323954848, 0.2012936081253811, 0.20871575206589965, 0.20038870686462984, 0.2149337042838413, 0.18550679365593192, 0.18683555211126623, 0.18772529971107277, 0.18893881453746109, 0.19088514046856442, 0.18482131036808602, 0.20190073819662635, 0.1900641481657912, 0.1859069787340505, 0.1968630703415548, 0.1791698475578658, 0.4170325720644681, 0.45300655683772084, 0.3268601402599913, 0.4871017114977154, 0.5760525069690072, 0.4299472063263293, 0.40700570066816644, 0.407629640942316, 0.37968732233267133, 0.392531629997694, 0.31229047844793034, 0.27992170615767065, 0.407958070552941, 0.24449333097769133, 0.3369355399421451, 0.3106873735819853, 0.20106188725038343, 0.18769962693559483, 0.17797584103290032, 0.18721943071606684, 0.19730978952847955, 0.1938898535186374, 0.18744755975756144, 0.19482638949017506, 0.18439503887472397, 0.07916525431738952, 0.08599538587443534, 0.08411545494118056, 0.07861878014307844, 0.07840802521435031, 0.08615874937721346, 0.07988894188612827, 0.08179290677658768, 0.09947126856750876]}, "mutation_prompt": null}
{"id": "7ace8689-1b3a-47fc-af21-ad9d42f093bb", "solution": "import numpy as np\n\nclass AdaptiveEntropyOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = 40\n        self.mutation_factor = 0.9  # Adjusted mutation factor\n        self.crossover_probability = 0.9  # Adjusted crossover probability\n        self.learning_rate = 0.2\n        self.diversity_factor = 0.05  # Adjusted diversity factor\n        self.population = None\n        self.fitness = None\n\n    def initialize_population(self):\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, \n                                            (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def adaptive_mutation_crossover(self, idx, func):\n        indices = [i for i in range(self.population_size) if i != idx]\n        np.random.shuffle(indices)\n        a, b, c = indices[:3]\n        mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        entropy_factor = 0.5 + 0.5 * np.random.rand()\n        mutant = self.population[idx] + entropy_factor * (mutant - self.population[idx])\n        mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n        \n        cross_points = np.random.rand(self.dim) < self.crossover_probability\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        \n        trial = np.where(cross_points, mutant, self.population[idx])\n        trial_fitness = func(trial)\n        \n        if trial_fitness < self.fitness[idx]:\n            self.population[idx] = trial\n            self.fitness[idx] = trial_fitness\n\n    def stochastic_gradient_adjustment(self, idx, func):\n        gradient = np.random.normal(0, 1, self.dim)\n        adaptive_lr = self.learning_rate / (1 + idx / self.population_size)\n        candidate = self.population[idx] - adaptive_lr * gradient\n        candidate += self.diversity_factor * np.random.normal(0, 1, self.dim)\n        candidate = np.clip(candidate, self.lower_bound, self.upper_bound)\n        candidate_fitness = func(candidate)\n        if candidate_fitness < self.fitness[idx]:\n            self.population[idx] = candidate\n            self.fitness[idx] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population()\n        evaluations = 0\n        self.evaluate_population(func)\n        evaluations += self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                self.adaptive_mutation_crossover(i, func)\n                evaluations += 1\n\n                if evaluations >= self.budget:\n                    break\n\n                self.stochastic_gradient_adjustment(i, func)\n                evaluations += 1\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "AdaptiveEntropyOptimizer", "description": "Adaptive probabilistic optimizer utilizing dynamic mutation and crossover strategies with entropy-based selection to enhance convergence efficiency.", "configspace": "", "generation": 85, "fitness": 0.3245157842294618, "feedback": "The algorithm AdaptiveEntropyOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.32 with standard deviation 0.22.", "error": "", "parent_id": "1459935d-0a5b-47de-8bfe-69f52cb3cb6c", "metadata": {"aucs": [0.7472496837247439, 0.7511784732463006, 0.7326833033843249, 0.751865596409641, 0.7717974899035168, 0.7496290806749288, 0.7560550531916306, 0.7605657687328979, 0.7380507637013366, 0.509086576893925, 0.5008545323454696, 0.4502055337798758, 0.5358971613548087, 0.5332013451843954, 0.5496932128396942, 0.4964566559120792, 0.5096186979710398, 0.530922129025587, 0.11970316402994263, 0.12766737282784613, 0.1287359690118648, 0.13679088098893621, 0.11085933569805106, 0.11363225700066215, 0.11384144035518762, 0.10456645352886351, 0.11056072099737835, 0.09562151663835361, 0.1028867045191153, 0.08635920300482813, 0.09918810952608348, 0.09180320561884603, 0.08702770482651212, 0.0963678868806388, 0.09390239391296729, 0.1063040598568028, 0.8777402064277048, 0.8404981413697892, 0.9199407996051988, 0.8585016839233485, 0.8412540846564136, 0.8941908068135594, 0.8450035910255004, 0.8799847343775068, 0.8587122736921082, 0.3516297549932571, 0.328546296987412, 0.35302332102262846, 0.37496040525620467, 0.33539431167072087, 0.33041868550167586, 0.3443180187388646, 0.3680605915689651, 0.35131080018276883, 0.6793085180031773, 0.6760401759578701, 0.7033853608462999, 0.7377952265027717, 0.7106936504506793, 0.6925262099525412, 0.6406409802790733, 0.7229912278567311, 0.7206670797445278, 0.23151686040238484, 0.18973115435278265, 0.18982650660263234, 0.1572091564820528, 0.30718168321208517, 0.20064095952691163, 0.27923127358792743, 0.18885056958444713, 0.22574776165408383, 0.21308817874800712, 0.2713613269402477, 0.167590835663054, 0.1624032690104188, 0.19082974437963884, 0.19256359613750984, 0.2048692691275349, 0.22096220210579864, 0.15645535345595507, 0.25295527095896253, 0.26549911143865645, 0.2574223322174335, 0.26641768200742666, 0.2697394478529753, 0.20927830193674413, 0.2620577876263165, 0.2992959747224794, 0.2398107330895668, 0.3794066104013637, 0.4079185453348667, 0.4031488363748026, 0.3881542548582725, 0.31497874559174366, 0.38555916800343426, 0.47272570107496714, 0.3706066339238723, 0.39343177553533926, 0.06292201205148262, 0.0715591940852024, 0.11038623595653729, 0.16641245485123013, 0.16634689744093833, 0.13443012995738368, 0.12252212683452635, 0.10271654471422531, 0.08418904299163621, 0.241460345320628, 0.23651573720722607, 0.23758698169841863, 0.24853074488058524, 0.25259631200581745, 0.2270832802992342, 0.21557288512220563, 0.24051515273130164, 0.22048057244240382, 0.6165344626914079, 0.6098708247378808, 0.6255027462833272, 0.5797978766430361, 0.5838504510889593, 0.6021759656761385, 0.6117754764058513, 0.5936837875302002, 0.6322763038809304, 0.11443134590231452, 0.09175309218560346, 0.12145084931233596, 0.11673922122858238, 0.10977023760534188, 0.09584315809110011, 0.0964624904397855, 0.11170762555839042, 0.1286520706840747, 0.18163920758850904, 0.2013504081508105, 0.1951083064748128, 0.2152691163220425, 0.198098574839346, 0.1802856135380413, 0.21795656713979483, 0.18131719308975602, 0.23520217469072124, 0.32300729614802426, 0.3134838595039596, 0.3136644765412371, 0.3277758336061407, 0.31614533733886385, 0.3389672915943216, 0.3143308904885661, 0.3611925287728559, 0.31029135655810824, 0.21568679974667582, 0.23826182837367016, 0.2448652611754606, 0.24393585672097506, 0.2626203082479279, 0.2291456750074572, 0.2526360519040646, 0.26956545763942086, 0.25030193215513696, 0.20336694854449944, 0.19456685622770642, 0.18861330905337992, 0.21168867721380868, 0.19767868873684857, 0.20017533442840307, 0.20128268877462296, 0.19058451112345276, 0.2034139125473341, 0.20234449217122397, 0.19517843450340866, 0.18680750244932465, 0.20299935161346383, 0.20176969804222467, 0.19508111226870828, 0.1921756146564555, 0.20214429679185963, 0.1918777417863744, 0.6126062598515591, 0.4416520573631686, 0.49646518946424256, 0.5450319170258875, 0.39941957276671325, 0.5630178733146665, 0.5526758196305452, 0.4253339227165004, 0.41404142761485974, 0.5343099462003332, 0.3641226599801909, 0.30892370404502667, 0.33116967459384283, 0.3112165009964719, 0.402997220811127, 0.23932950762032856, 0.2627896277972299, 0.30884422090038277, 0.1867319784645476, 0.1784742622918175, 0.17891563009092948, 0.17893052827032063, 0.18097470121624792, 0.1836801974422969, 0.18476771441560258, 0.17992598164909102, 0.17657338479453133, 0.08249198648226908, 0.07321118182088304, 0.08994361679828633, 0.0791789200656513, 0.08621402251473465, 0.1019262005579491, 0.08286388248981458, 0.07295884162317667, 0.07890726022741201]}, "mutation_prompt": null}
{"id": "6291926d-0de9-4984-80e6-a1048e1e4be8", "solution": "import numpy as np\n\nclass ReinforcedProbabilisticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = 50\n        self.mutation_factor = 0.8\n        self.crossover_probability = 0.9\n        self.learning_rate = 0.15\n        self.diversity_factor = 0.15\n        self.population = None\n        self.fitness = None\n\n    def initialize_population(self):\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, \n                                            (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def adaptive_differential_evolution(self, idx, func):\n        indices = [i for i in range(self.population_size) if i != idx]\n        np.random.shuffle(indices)\n        a, b, c = indices[:3]\n        mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        adaptive_factor = 0.5 + 0.5 * np.random.rand()\n        mutant = self.population[idx] + adaptive_factor * (mutant - self.population[idx])\n        mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n        \n        cross_points = np.random.rand(self.dim) < self.crossover_probability\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        \n        trial = np.where(cross_points, mutant, self.population[idx])\n        trial_fitness = func(trial)\n        \n        if trial_fitness < self.fitness[idx]:\n            self.population[idx] = trial\n            self.fitness[idx] = trial_fitness\n\n    def stochastic_gradient_approx(self, idx, func):\n        gradient = np.random.normal(0, 1, self.dim)\n        adaptive_lr = self.learning_rate / (1 + idx / self.population_size)\n        candidate = self.population[idx] - adaptive_lr * gradient\n        candidate += self.diversity_factor * np.random.normal(0, 1, self.dim)\n        candidate = np.clip(candidate, self.lower_bound, self.upper_bound)\n        candidate_fitness = func(candidate)\n        if candidate_fitness < self.fitness[idx]:\n            self.population[idx] = candidate\n            self.fitness[idx] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population()\n        evaluations = 0\n        self.evaluate_population(func)\n        evaluations += self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                self.adaptive_differential_evolution(i, func)\n                evaluations += 1\n\n                if evaluations >= self.budget:\n                    break\n\n                self.stochastic_gradient_approx(i, func)\n                evaluations += 1\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "ReinforcedProbabilisticOptimizer", "description": "Reinforced probabilistic optimizer employing adaptive differential evolution and stochastic gradient approximation with enhanced exploration-exploitation dynamics.", "configspace": "", "generation": 86, "fitness": 0.30835294326347684, "feedback": "The algorithm ReinforcedProbabilisticOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.31 with standard deviation 0.21.", "error": "", "parent_id": "1459935d-0a5b-47de-8bfe-69f52cb3cb6c", "metadata": {"aucs": [0.7311180999329787, 0.7370413043184463, 0.7403708421007197, 0.7279103551745716, 0.7264678639134909, 0.6974178437804431, 0.7158517137954532, 0.7247905966458486, 0.7427136259575352, 0.46159574411247595, 0.449544033966242, 0.4499257524988568, 0.49802117867256623, 0.4679755011950205, 0.49474265582079957, 0.48586382129574524, 0.4741780670104435, 0.4665009793839169, 0.10934043159203644, 0.11419094054653545, 0.1119449960080443, 0.10701874450281046, 0.10247337106622723, 0.09707879685636622, 0.10500371497543293, 0.11884381093472252, 0.11450771398177617, 0.08782352532001969, 0.08825711511377343, 0.08291353032011373, 0.09416745835256646, 0.09884073747990185, 0.08328247349105344, 0.09354692195594705, 0.0862322018523668, 0.10301143434459115, 0.8406621234248298, 0.8640394360762227, 0.8392824295821351, 0.8353732654643713, 0.8113070664836948, 0.8165148202429175, 0.830980476923705, 0.8385992476384031, 0.8310522511101495, 0.3254432441266627, 0.30418407277738224, 0.326377649390564, 0.31954382966556427, 0.30875966075481365, 0.3048398889986411, 0.3157763696926238, 0.302916327400797, 0.3206305114658736, 0.7088123250254434, 0.6374409152709046, 0.7074319512700427, 0.6831297318311846, 0.6486697392898304, 0.6977268086095214, 0.6921955478761945, 0.7005097556899852, 0.6728438084260508, 0.19318202444292687, 0.21258606469116326, 0.1997367996982462, 0.13770657547477927, 0.18778993813239953, 0.16669242369778947, 0.16595983261393144, 0.21296209860733195, 0.17528980392624982, 0.1776081784177801, 0.12653893572689046, 0.18900933285842936, 0.1568055871159364, 0.15216308723188576, 0.1573272333103901, 0.20268942444268134, 0.1552536261500712, 0.17783392216116567, 0.21407347091353646, 0.20693022127186944, 0.22854761541026414, 0.217480128290378, 0.23837765810067024, 0.21297490513777062, 0.2571257628068985, 0.22196504435858777, 0.2667280367320194, 0.38620514044801824, 0.36729474385370564, 0.37992851003503936, 0.3203607735800341, 0.31671353774584354, 0.3536955169094945, 0.3774519778184774, 0.39281143783639205, 0.41714429553791177, 0.11781276729666657, 0.06677969411533957, 0.07229497558756115, 0.07771913153861443, 0.12862610491846904, 0.08168056926332179, 0.09534337147440541, 0.12028894209849117, 0.0934762358724216, 0.2380800664043934, 0.20502197859263926, 0.2251990349455597, 0.22792662759031657, 0.24172052434753188, 0.2459264917649202, 0.20840362619886055, 0.2066367184955914, 0.2258709519908182, 0.5832157068495261, 0.5913750181072992, 0.6117972358590165, 0.5961200913492496, 0.5836676351180776, 0.6080767022699076, 0.6046780728114043, 0.5991350155843582, 0.5974218614032716, 0.10346200693578489, 0.10468156013674501, 0.11888669385197403, 0.10520641812516873, 0.08701722988575089, 0.101408131442008, 0.09346036955913006, 0.11008427417582423, 0.1030569421626184, 0.20033773677314115, 0.19850849759788114, 0.1764134669903522, 0.1796534139488991, 0.16325373576772695, 0.19134324398467495, 0.21336609460279288, 0.18052346345784387, 0.18730431718922957, 0.2982210994781762, 0.3071639218805542, 0.2986960599931838, 0.3155039252979285, 0.3147817087323863, 0.3108862797600843, 0.3412031084582231, 0.3184894952258538, 0.311891481715998, 0.2394798549818984, 0.23151310700901462, 0.24940435919710813, 0.24277236783256417, 0.2314247997741986, 0.24914615322439448, 0.2409660966107292, 0.23866653890390166, 0.26316509309190306, 0.2070333635524595, 0.2223812184785906, 0.20445079619883044, 0.20860503612561165, 0.19465828400387242, 0.18896136166313426, 0.20354278269867287, 0.20926676130459676, 0.2116566725249187, 0.18764092762325257, 0.19453944476903784, 0.18694947263030137, 0.19245553760692535, 0.1862214573354125, 0.18476819448572968, 0.18464259663724214, 0.18721938588673315, 0.17949641222075952, 0.38882777538713453, 0.36782946420002305, 0.4753823936074578, 0.3509041243872971, 0.35873131897473154, 0.34462434060910596, 0.50468083095128, 0.3404910627298132, 0.39523448288406227, 0.4710588728309004, 0.3811606422429249, 0.6316516229748138, 0.38514792599077097, 0.32266795682890503, 0.323594531581085, 0.26743353107779877, 0.43537800994766007, 0.3935362158867626, 0.19254414597621217, 0.18126195510528642, 0.19396613303797128, 0.1831560188185708, 0.2236331920892668, 0.18180165378955948, 0.18843394105458455, 0.18058933388531673, 0.1987601353943711, 0.085864173468306, 0.0774336214409419, 0.0853492913198528, 0.07635716230581169, 0.07875958915834691, 0.07762960787465822, 0.08892670805227898, 0.08189561389778888, 0.08091564153426867]}, "mutation_prompt": null}
{"id": "3e6183be-9940-47e3-ac5c-4f80757462c6", "solution": "import numpy as np\n\nclass AdaptiveStochasticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = 50  # Changed from 40\n        self.mutation_factor = 0.9  # Changed from 0.85\n        self.crossover_probability = 0.9  # Changed from 0.95\n        self.learning_rate = 0.15  # Changed from 0.2\n        self.diversity_factor = 0.15  # Changed from 0.1\n        self.population = None\n        self.fitness = None\n\n    def initialize_population(self):\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, \n                                            (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def adaptive_differential_evolution(self, idx, func):\n        indices = [i for i in range(self.population_size) if i != idx]\n        np.random.shuffle(indices)\n        a, b, c = indices[:3]\n        mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        adaptive_factor = 0.5 + 0.5 * np.random.rand()  # Changed from 0.4 + 0.6\n        mutant = self.population[idx] + adaptive_factor * (mutant - self.population[idx])\n        mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n        \n        cross_points = np.random.rand(self.dim) < self.crossover_probability\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        \n        trial = np.where(cross_points, mutant, self.population[idx])\n        trial_fitness = func(trial)\n        \n        if trial_fitness < self.fitness[idx]:\n            self.population[idx] = trial\n            self.fitness[idx] = trial_fitness\n\n    def stochastic_gradient_approx(self, idx, func):\n        gradient = np.random.normal(0, 1, self.dim)\n        adaptive_lr = self.learning_rate / np.sqrt(1 + idx / self.population_size)  # Changed to sqrt decay\n        candidate = self.population[idx] - adaptive_lr * gradient\n        candidate += self.diversity_factor * np.random.normal(0, 1, self.dim)\n        candidate = np.clip(candidate, self.lower_bound, self.upper_bound)\n        candidate_fitness = func(candidate)\n        if candidate_fitness < self.fitness[idx]:\n            self.population[idx] = candidate\n            self.fitness[idx] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population()\n        evaluations = 0\n        self.evaluate_population(func)\n        evaluations += self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                self.adaptive_differential_evolution(i, func)\n                evaluations += 1\n\n                if evaluations >= self.budget:\n                    break\n\n                self.stochastic_gradient_approx(i, func)\n                evaluations += 1\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "AdaptiveStochasticOptimizer", "description": "Adaptive stochastic optimizer leveraging diversity and local learning for improved convergence in multidimensional spaces.", "configspace": "", "generation": 87, "fitness": 0.284130777396634, "feedback": "The algorithm AdaptiveStochasticOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28 with standard deviation 0.21.", "error": "", "parent_id": "1459935d-0a5b-47de-8bfe-69f52cb3cb6c", "metadata": {"aucs": [0.6889222971397222, 0.6820926710990286, 0.6909726415765711, 0.6871975437128073, 0.6475956155906829, 0.6834744839941286, 0.6774129678262799, 0.6752179680005856, 0.69282495724732, 0.3957218291766892, 0.3676735160725342, 0.4046114374459693, 0.4032299695366427, 0.3749694579353432, 0.36887071518160774, 0.37982700314553197, 0.40546835089480115, 0.3854990283138279, 0.09911304971161572, 0.1073690656625157, 0.12000228838188598, 0.10004575698689455, 0.10050196776723552, 0.08875235941317494, 0.10412595140596637, 0.11335008746753561, 0.11087241348712906, 0.08325720347740972, 0.08363235714155248, 0.089558266254816, 0.08767152696622127, 0.0915152715594737, 0.08089923670702626, 0.08833076192092615, 0.09096150148095261, 0.08660976050342195, 0.9225883018647171, 0.8913570561481473, 0.9595496448584547, 0.8025898953977992, 0.876360914438694, 0.8412938115270296, 0.8477790013430953, 0.8757876963329467, 0.8299350876332866, 0.29486620295972576, 0.2876095514588931, 0.2825602923727182, 0.2915765205712181, 0.28024739001959886, 0.32028883234904837, 0.28649284106500283, 0.29966113408439676, 0.27753050520739553, 0.5971491737354824, 0.6474766277944836, 0.6456361015473389, 0.6363450456894794, 0.6667957194830354, 0.6318143346886025, 0.6648844816867495, 0.6290176960210337, 0.5583003449113912, 0.16379687972119805, 0.14588506254261668, 0.16879621667760714, 0.1553265496448517, 0.156999277861544, 0.18656402160444185, 0.15197166012321173, 0.17617040986171417, 0.1533893492697911, 0.15439856908254224, 0.18267929303286146, 0.2164700357629803, 0.17556222694264467, 0.18705220371936115, 0.17311302585819122, 0.17729291490345223, 0.16071538060186363, 0.15799346388443902, 0.17304122974833813, 0.17854978028471546, 0.1833576060960539, 0.20148712348560616, 0.18071370047865587, 0.16539900298250032, 0.18872350037304786, 0.21530257518209872, 0.17448388585898778, 0.34295397408487804, 0.31976582572098544, 0.3266620045402223, 0.32851717094945443, 0.29824269495414624, 0.3203019756308567, 0.3448523793551971, 0.3409632640228578, 0.34074746289573643, 0.059681182607004835, 0.05634568699740705, 0.04392896317895101, 0.06498986089390424, 0.06150258402447217, 0.0870835211577431, 0.05090594927303116, 0.06086784370463849, 0.07637930414972138, 0.19972255973578068, 0.1850487868221029, 0.19121772883393173, 0.19448310293729332, 0.20165969211628276, 0.19401958200680325, 0.18839981025603914, 0.17941553193889248, 0.19735013978496607, 0.5504161902640285, 0.5201554324347234, 0.5251902005569751, 0.5332015128493692, 0.5436911985741038, 0.5493722579046865, 0.5379227408654098, 0.5542691848441399, 0.5450665394319628, 0.11115730103503452, 0.08975138550553785, 0.10129951017035133, 0.10235961036009589, 0.08672601229214782, 0.09601907556495703, 0.09668952485361515, 0.10006768276540645, 0.08947422718063003, 0.16399422986134105, 0.1812787652171466, 0.21393535926903484, 0.1511279482833312, 0.15526240865714735, 0.17319706016767122, 0.20456570396631502, 0.18788783944800302, 0.17300805983396528, 0.27893393205751904, 0.2763564807152047, 0.29334543406443636, 0.26793792593118837, 0.2662483134160374, 0.2854820454248249, 0.30291829896847444, 0.2898976538277327, 0.28787212926200867, 0.2206177952064775, 0.21290649643048398, 0.23989282597731443, 0.21130677837437883, 0.2146998184410961, 0.2359693043647011, 0.21893717616773878, 0.22204235416110585, 0.2199624521920458, 0.19693031799744287, 0.22944643333575632, 0.21389388781086804, 0.19045447757072176, 0.200173117712792, 0.1858705120174724, 0.19172524006830238, 0.19923742886191764, 0.21012314383114383, 0.19324320034912634, 0.1811595058811054, 0.18323238495088057, 0.17985035259685522, 0.1897635504863081, 0.18025014739017808, 0.18627521749808562, 0.19103381657207286, 0.18644486394848825, 0.3409677431903414, 0.2761893599745536, 0.3227282991024555, 0.42833114348500856, 0.29776838748862655, 0.3798456562534557, 0.3176188636493331, 0.349253421956251, 0.5637140848666614, 0.3994190585049937, 0.3413175038789811, 0.5301784346086273, 0.2554742485639804, 0.31983978348238307, 0.20397874480520573, 0.24103658652604332, 0.25316500725782676, 0.29880947219486653, 0.20553571573577534, 0.18497009584840285, 0.17593345201741417, 0.18962019136507569, 0.19102510889593383, 0.17802405256541443, 0.1910249816693993, 0.18281604761476877, 0.1904701060330003, 0.08830407293939024, 0.08237378517167249, 0.07522496558051883, 0.07744299720764758, 0.09938915615637256, 0.08648243726812344, 0.07510207102842759, 0.07926084127532762, 0.08320124995706812]}, "mutation_prompt": null}
{"id": "034d23a0-5da8-4724-bd91-a17720a478b0", "solution": "import numpy as np\n\nclass EnhancedProbabilisticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = 40\n        self.mutation_factor = 0.85\n        self.crossover_probability = 0.95\n        self.learning_rate = 0.2\n        self.diversity_factor = 0.1\n        self.population = None\n        self.fitness = None\n\n    def initialize_population(self):\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, \n                                            (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def adaptive_differential_evolution(self, idx, func):\n        indices = [i for i in range(self.population_size) if i != idx]\n        np.random.shuffle(indices)\n        a, b, c = indices[:3]\n        mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        adaptive_factor = 0.4 + 0.6 * np.random.rand()\n        mutant = self.population[idx] + adaptive_factor * (mutant - self.population[idx])\n        mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n        \n        cross_points = np.random.rand(self.dim) < self.crossover_probability\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        \n        trial = np.where(cross_points, mutant, self.population[idx])\n        trial_fitness = func(trial)\n        \n        if trial_fitness < self.fitness[idx]:\n            self.population[idx] = trial\n            self.fitness[idx] = trial_fitness\n\n    def stochastic_gradient_approx(self, idx, func):\n        gradient = np.random.normal(0, 1, self.dim)\n        adaptive_lr = self.learning_rate / (1 + idx / self.population_size)\n        candidate = self.population[idx] - adaptive_lr * gradient\n        candidate += self.diversity_factor * np.random.normal(0, 1, self.dim)\n        candidate = np.clip(candidate, self.lower_bound, self.upper_bound)\n        candidate_fitness = func(candidate)\n        if candidate_fitness < self.fitness[idx]:\n            self.population[idx] = candidate\n            self.fitness[idx] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population()\n        evaluations = 0\n        self.evaluate_population(func)\n        evaluations += self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                self.adaptive_differential_evolution(i, func)\n                evaluations += 1\n\n                if evaluations >= self.budget:\n                    break\n\n                self.stochastic_gradient_approx(i, func)\n                evaluations += 1\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "EnhancedProbabilisticOptimizer", "description": "Enhanced probabilistic hybrid optimizer balancing exploration and exploitation through diversity-oriented selection and adaptive parameter tuning for optimal convergence.", "configspace": "", "generation": 38, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "1459935d-0a5b-47de-8bfe-69f52cb3cb6c", "metadata": {"aucs": [0.796727445346743, 0.7946092935536371, 0.8025157468331956, 0.7795890428061506, 0.7652866403837684, 0.7888980264133363, 0.7851539828124771, 0.7801166288092796, 0.778684214476141, 0.6066596860215113, 0.5509488918434079, 0.584709500115218, 0.6151880393982149, 0.6128285196462557, 0.5928554548480235, 0.18781852246903208, 0.6051102165828588, 0.6050716765741883, 0.12056849613284826, 0.10627786970516262, 0.11092740344636509, 0.12458142503345515, 0.10225070914012868, 0.10214739878052792, 0.11007052847089493, 0.11303443664409152, 0.11346829363668931, 0.09058514750802149, 0.09136224141572435, 0.09346304265481342, 0.08413688806219866, 0.10560994817344638, 0.09386944951658027, 0.1066829182603829, 0.09525027475772718, 0.09078080677184064, 0.8379701410373176, 0.8069154658397715, 0.8108437032002754, 0.8202640448333322, 0.7815128713795206, 0.8451084830274826, 0.836376388134602, 0.8441441978939892, 0.8509693104465095, 0.37150586919602946, 0.34273942956965175, 0.3313822553845982, 0.34968755932637086, 0.32960730205573996, 0.3754984208710611, 0.38284578850911233, 0.3840456442426208, 0.3280564413760412, 0.7576709220936438, 0.7335556841917643, 0.7658090217912067, 0.7934167558179147, 0.8025617081483369, 0.7999764269594076, 0.7299292035530152, 0.7697788108067551, 0.7651252822797626, 0.16167439560696928, 0.18696031846775318, 0.21158580997319565, 0.244349141547606, 0.2890559956537958, 0.16298570563996873, 0.21750653437837209, 0.21122767559906996, 0.22550289679700497, 0.15305768362463734, 0.14884944831507896, 0.18600965489753707, 0.1541357033150358, 0.19832744782165168, 0.18983314847500432, 0.1953210887165725, 0.20165646735698872, 0.13088573983753993, 0.489476775395826, 0.460206761694571, 0.280690722270316, 0.34064732845138956, 0.522953329166158, 0.46452239779932414, 0.4196789194960461, 0.4727221489210359, 0.44131563048738687, 0.6540713674624187, 0.6128130384068273, 0.5553131009305013, 0.5776200055109268, 0.4723536786910414, 0.6219680956281209, 0.6455804612075908, 0.6109257481083044, 0.5996992464126676, 0.15220127888973134, 0.11018658007737869, 0.2413715712685326, 0.2296484097510183, 0.2572737636354798, 0.10622686325653308, 0.14604068872930986, 0.14809138800719435, 0.12257392403735012, 0.36655256082417187, 0.34632185847096675, 0.3516395670957846, 0.33828319802853724, 0.3572041238804262, 0.35621925345698524, 0.34628041224843775, 0.36205674625584605, 0.3801826028652373, 0.7284525303733058, 0.7334889541026953, 0.7386557191632159, 0.7055754468414382, 0.7222856768071027, 0.7194774022217179, 0.518854950686749, 0.7321096452535195, 0.7397060954282213, 0.10374399773694243, 0.104282622260642, 0.1086381702881759, 0.11429613558107654, 0.11511930056195652, 0.10035386421161252, 0.09599830676572241, 0.11347281801819442, 0.11470225552281121, 0.14952999643648723, 0.16817216547016145, 0.17310734903563885, 0.1855986179207637, 0.18359480037739373, 0.19508916572226054, 0.20087729718879477, 0.17807185486405164, 0.20119801391244463, 0.3682121014468256, 0.35073539948175425, 0.3330439038672147, 0.4157077631363848, 0.37474021495439724, 0.36760475650845414, 0.38008256501576043, 0.39084781839078786, 0.3786757352451906, 0.30037946434366947, 0.2861866860206934, 0.2891587957817804, 0.2835627691001067, 0.2779602895144492, 0.28387893685888044, 0.29400381025955125, 0.3244615423116647, 0.30188351717337136, 0.20494558183242018, 0.19905376550038034, 0.20406514428030742, 0.2229753496453719, 0.1980660060283188, 0.21539123082194234, 0.21928087314384304, 0.20854328449838244, 0.2102975078755156, 0.18082128476991477, 0.1831737258940077, 0.18089956975930732, 0.18277323416104252, 0.1792695777259361, 0.17816092407226292, 0.19628636488975992, 0.19237612183491615, 0.1874718333986961, 0.6294666203187917, 0.602690555999032, 0.5454952462675013, 0.7727337342883757, 0.25677125313719296, 0.588947972014151, 0.5053651729784945, 0.42433758940449484, 0.610090527727432, 0.7148365734773846, 0.39316249107203016, 0.4609914040189693, 0.5414475146943718, 0.3738614312609654, 0.2972834077003401, 0.2719039283446757, 0.4464259994844204, 0.6865015215956998, 0.18541089636272012, 0.17521600614441235, 0.18498258988690697, 0.1923345960637889, 0.19442272937612437, 0.18424555156947642, 0.17449564646480575, 0.18372628686318526, 0.18823517758491137, 0.07953923390817585, 0.08429672806217459, 0.08351517483047155, 0.07872531348008505, 0.0885755282031725, 0.09071254814221597, 0.07022751225803625, 0.07907174920771143, 0.08022853716110878]}, "mutation_prompt": null}
{"id": "1cb76df6-fef8-4855-84d9-23e954110856", "solution": "import numpy as np\n\nclass EnhancedProbabilisticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = 40\n        self.mutation_factor = 0.85\n        self.crossover_probability = 0.95\n        self.learning_rate = 0.2\n        self.diversity_factor = 0.1\n        self.population = None\n        self.fitness = None\n\n    def initialize_population(self):\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, \n                                            (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def adaptive_differential_evolution(self, idx, func):\n        indices = [i for i in range(self.population_size) if i != idx]\n        np.random.shuffle(indices)\n        a, b, c = indices[:3]\n        mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        adaptive_factor = 0.4 + 0.6 * np.random.rand()\n        mutant = self.population[idx] + adaptive_factor * (mutant - self.population[idx])\n        mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n        \n        cross_points = np.random.rand(self.dim) < self.crossover_probability\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        \n        trial = np.where(cross_points, mutant, self.population[idx])\n        trial_fitness = func(trial)\n        \n        if trial_fitness < self.fitness[idx]:\n            self.population[idx] = trial\n            self.fitness[idx] = trial_fitness\n\n    def stochastic_gradient_approx(self, idx, func):\n        gradient = np.random.normal(0, 1, self.dim)\n        adaptive_lr = self.learning_rate / (1 + idx / self.population_size)\n        candidate = self.population[idx] - adaptive_lr * gradient\n        candidate += self.diversity_factor * np.random.normal(0, 1, self.dim)\n        candidate = np.clip(candidate, self.lower_bound, self.upper_bound)\n        candidate_fitness = func(candidate)\n        if candidate_fitness < self.fitness[idx]:\n            self.population[idx] = candidate\n            self.fitness[idx] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population()\n        evaluations = 0\n        self.evaluate_population(func)\n        evaluations += self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                self.adaptive_differential_evolution(i, func)\n                evaluations += 1\n\n                if evaluations >= self.budget:\n                    break\n\n                self.stochastic_gradient_approx(i, func)\n                evaluations += 1\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "EnhancedProbabilisticOptimizer", "description": "Enhanced probabilistic hybrid optimizer balancing exploration and exploitation through diversity-oriented selection and adaptive parameter tuning for optimal convergence.", "configspace": "", "generation": 38, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "1459935d-0a5b-47de-8bfe-69f52cb3cb6c", "metadata": {"aucs": [0.796727445346743, 0.7946092935536371, 0.8025157468331956, 0.7795890428061506, 0.7652866403837684, 0.7888980264133363, 0.7851539828124771, 0.7801166288092796, 0.778684214476141, 0.6066596860215113, 0.5509488918434079, 0.584709500115218, 0.6151880393982149, 0.6128285196462557, 0.5928554548480235, 0.18781852246903208, 0.6051102165828588, 0.6050716765741883, 0.12056849613284826, 0.10627786970516262, 0.11092740344636509, 0.12458142503345515, 0.10225070914012868, 0.10214739878052792, 0.11007052847089493, 0.11303443664409152, 0.11346829363668931, 0.09058514750802149, 0.09136224141572435, 0.09346304265481342, 0.08413688806219866, 0.10560994817344638, 0.09386944951658027, 0.1066829182603829, 0.09525027475772718, 0.09078080677184064, 0.8379701410373176, 0.8069154658397715, 0.8108437032002754, 0.8202640448333322, 0.7815128713795206, 0.8451084830274826, 0.836376388134602, 0.8441441978939892, 0.8509693104465095, 0.37150586919602946, 0.34273942956965175, 0.3313822553845982, 0.34968755932637086, 0.32960730205573996, 0.3754984208710611, 0.38284578850911233, 0.3840456442426208, 0.3280564413760412, 0.7576709220936438, 0.7335556841917643, 0.7658090217912067, 0.7934167558179147, 0.8025617081483369, 0.7999764269594076, 0.7299292035530152, 0.7697788108067551, 0.7651252822797626, 0.16167439560696928, 0.18696031846775318, 0.21158580997319565, 0.244349141547606, 0.2890559956537958, 0.16298570563996873, 0.21750653437837209, 0.21122767559906996, 0.22550289679700497, 0.15305768362463734, 0.14884944831507896, 0.18600965489753707, 0.1541357033150358, 0.19832744782165168, 0.18983314847500432, 0.1953210887165725, 0.20165646735698872, 0.13088573983753993, 0.489476775395826, 0.460206761694571, 0.280690722270316, 0.34064732845138956, 0.522953329166158, 0.46452239779932414, 0.4196789194960461, 0.4727221489210359, 0.44131563048738687, 0.6540713674624187, 0.6128130384068273, 0.5553131009305013, 0.5776200055109268, 0.4723536786910414, 0.6219680956281209, 0.6455804612075908, 0.6109257481083044, 0.5996992464126676, 0.15220127888973134, 0.11018658007737869, 0.2413715712685326, 0.2296484097510183, 0.2572737636354798, 0.10622686325653308, 0.14604068872930986, 0.14809138800719435, 0.12257392403735012, 0.36655256082417187, 0.34632185847096675, 0.3516395670957846, 0.33828319802853724, 0.3572041238804262, 0.35621925345698524, 0.34628041224843775, 0.36205674625584605, 0.3801826028652373, 0.7284525303733058, 0.7334889541026953, 0.7386557191632159, 0.7055754468414382, 0.7222856768071027, 0.7194774022217179, 0.518854950686749, 0.7321096452535195, 0.7397060954282213, 0.10374399773694243, 0.104282622260642, 0.1086381702881759, 0.11429613558107654, 0.11511930056195652, 0.10035386421161252, 0.09599830676572241, 0.11347281801819442, 0.11470225552281121, 0.14952999643648723, 0.16817216547016145, 0.17310734903563885, 0.1855986179207637, 0.18359480037739373, 0.19508916572226054, 0.20087729718879477, 0.17807185486405164, 0.20119801391244463, 0.3682121014468256, 0.35073539948175425, 0.3330439038672147, 0.4157077631363848, 0.37474021495439724, 0.36760475650845414, 0.38008256501576043, 0.39084781839078786, 0.3786757352451906, 0.30037946434366947, 0.2861866860206934, 0.2891587957817804, 0.2835627691001067, 0.2779602895144492, 0.28387893685888044, 0.29400381025955125, 0.3244615423116647, 0.30188351717337136, 0.20494558183242018, 0.19905376550038034, 0.20406514428030742, 0.2229753496453719, 0.1980660060283188, 0.21539123082194234, 0.21928087314384304, 0.20854328449838244, 0.2102975078755156, 0.18082128476991477, 0.1831737258940077, 0.18089956975930732, 0.18277323416104252, 0.1792695777259361, 0.17816092407226292, 0.19628636488975992, 0.19237612183491615, 0.1874718333986961, 0.6294666203187917, 0.602690555999032, 0.5454952462675013, 0.7727337342883757, 0.25677125313719296, 0.588947972014151, 0.5053651729784945, 0.42433758940449484, 0.610090527727432, 0.7148365734773846, 0.39316249107203016, 0.4609914040189693, 0.5414475146943718, 0.3738614312609654, 0.2972834077003401, 0.2719039283446757, 0.4464259994844204, 0.6865015215956998, 0.18541089636272012, 0.17521600614441235, 0.18498258988690697, 0.1923345960637889, 0.19442272937612437, 0.18424555156947642, 0.17449564646480575, 0.18372628686318526, 0.18823517758491137, 0.07953923390817585, 0.08429672806217459, 0.08351517483047155, 0.07872531348008505, 0.0885755282031725, 0.09071254814221597, 0.07022751225803625, 0.07907174920771143, 0.08022853716110878]}, "mutation_prompt": null}
{"id": "3ca1f433-6e8d-4d95-b498-76b158f3215a", "solution": "import numpy as np\n\nclass EnhancedProbabilisticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = 40\n        self.mutation_factor = 0.85\n        self.crossover_probability = 0.95\n        self.learning_rate = 0.2\n        self.diversity_factor = 0.1\n        self.population = None\n        self.fitness = None\n\n    def initialize_population(self):\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, \n                                            (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def adaptive_differential_evolution(self, idx, func):\n        indices = [i for i in range(self.population_size) if i != idx]\n        np.random.shuffle(indices)\n        a, b, c = indices[:3]\n        mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        adaptive_factor = 0.4 + 0.6 * np.random.rand()\n        mutant = self.population[idx] + adaptive_factor * (mutant - self.population[idx])\n        mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n        \n        cross_points = np.random.rand(self.dim) < self.crossover_probability\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        \n        trial = np.where(cross_points, mutant, self.population[idx])\n        trial_fitness = func(trial)\n        \n        if trial_fitness < self.fitness[idx]:\n            self.population[idx] = trial\n            self.fitness[idx] = trial_fitness\n\n    def stochastic_gradient_approx(self, idx, func):\n        gradient = np.random.normal(0, 1, self.dim)\n        adaptive_lr = self.learning_rate / (1 + idx / self.population_size)\n        candidate = self.population[idx] - adaptive_lr * gradient\n        candidate += self.diversity_factor * np.random.normal(0, 1, self.dim)\n        candidate = np.clip(candidate, self.lower_bound, self.upper_bound)\n        candidate_fitness = func(candidate)\n        if candidate_fitness < self.fitness[idx]:\n            self.population[idx] = candidate\n            self.fitness[idx] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population()\n        evaluations = 0\n        self.evaluate_population(func)\n        evaluations += self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                self.adaptive_differential_evolution(i, func)\n                evaluations += 1\n\n                if evaluations >= self.budget:\n                    break\n\n                self.stochastic_gradient_approx(i, func)\n                evaluations += 1\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "EnhancedProbabilisticOptimizer", "description": "Enhanced probabilistic hybrid optimizer balancing exploration and exploitation through diversity-oriented selection and adaptive parameter tuning for optimal convergence.", "configspace": "", "generation": 38, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "1459935d-0a5b-47de-8bfe-69f52cb3cb6c", "metadata": {"aucs": [0.796727445346743, 0.7946092935536371, 0.8025157468331956, 0.7795890428061506, 0.7652866403837684, 0.7888980264133363, 0.7851539828124771, 0.7801166288092796, 0.778684214476141, 0.6066596860215113, 0.5509488918434079, 0.584709500115218, 0.6151880393982149, 0.6128285196462557, 0.5928554548480235, 0.18781852246903208, 0.6051102165828588, 0.6050716765741883, 0.12056849613284826, 0.10627786970516262, 0.11092740344636509, 0.12458142503345515, 0.10225070914012868, 0.10214739878052792, 0.11007052847089493, 0.11303443664409152, 0.11346829363668931, 0.09058514750802149, 0.09136224141572435, 0.09346304265481342, 0.08413688806219866, 0.10560994817344638, 0.09386944951658027, 0.1066829182603829, 0.09525027475772718, 0.09078080677184064, 0.8379701410373176, 0.8069154658397715, 0.8108437032002754, 0.8202640448333322, 0.7815128713795206, 0.8451084830274826, 0.836376388134602, 0.8441441978939892, 0.8509693104465095, 0.37150586919602946, 0.34273942956965175, 0.3313822553845982, 0.34968755932637086, 0.32960730205573996, 0.3754984208710611, 0.38284578850911233, 0.3840456442426208, 0.3280564413760412, 0.7576709220936438, 0.7335556841917643, 0.7658090217912067, 0.7934167558179147, 0.8025617081483369, 0.7999764269594076, 0.7299292035530152, 0.7697788108067551, 0.7651252822797626, 0.16167439560696928, 0.18696031846775318, 0.21158580997319565, 0.244349141547606, 0.2890559956537958, 0.16298570563996873, 0.21750653437837209, 0.21122767559906996, 0.22550289679700497, 0.15305768362463734, 0.14884944831507896, 0.18600965489753707, 0.1541357033150358, 0.19832744782165168, 0.18983314847500432, 0.1953210887165725, 0.20165646735698872, 0.13088573983753993, 0.489476775395826, 0.460206761694571, 0.280690722270316, 0.34064732845138956, 0.522953329166158, 0.46452239779932414, 0.4196789194960461, 0.4727221489210359, 0.44131563048738687, 0.6540713674624187, 0.6128130384068273, 0.5553131009305013, 0.5776200055109268, 0.4723536786910414, 0.6219680956281209, 0.6455804612075908, 0.6109257481083044, 0.5996992464126676, 0.15220127888973134, 0.11018658007737869, 0.2413715712685326, 0.2296484097510183, 0.2572737636354798, 0.10622686325653308, 0.14604068872930986, 0.14809138800719435, 0.12257392403735012, 0.36655256082417187, 0.34632185847096675, 0.3516395670957846, 0.33828319802853724, 0.3572041238804262, 0.35621925345698524, 0.34628041224843775, 0.36205674625584605, 0.3801826028652373, 0.7284525303733058, 0.7334889541026953, 0.7386557191632159, 0.7055754468414382, 0.7222856768071027, 0.7194774022217179, 0.518854950686749, 0.7321096452535195, 0.7397060954282213, 0.10374399773694243, 0.104282622260642, 0.1086381702881759, 0.11429613558107654, 0.11511930056195652, 0.10035386421161252, 0.09599830676572241, 0.11347281801819442, 0.11470225552281121, 0.14952999643648723, 0.16817216547016145, 0.17310734903563885, 0.1855986179207637, 0.18359480037739373, 0.19508916572226054, 0.20087729718879477, 0.17807185486405164, 0.20119801391244463, 0.3682121014468256, 0.35073539948175425, 0.3330439038672147, 0.4157077631363848, 0.37474021495439724, 0.36760475650845414, 0.38008256501576043, 0.39084781839078786, 0.3786757352451906, 0.30037946434366947, 0.2861866860206934, 0.2891587957817804, 0.2835627691001067, 0.2779602895144492, 0.28387893685888044, 0.29400381025955125, 0.3244615423116647, 0.30188351717337136, 0.20494558183242018, 0.19905376550038034, 0.20406514428030742, 0.2229753496453719, 0.1980660060283188, 0.21539123082194234, 0.21928087314384304, 0.20854328449838244, 0.2102975078755156, 0.18082128476991477, 0.1831737258940077, 0.18089956975930732, 0.18277323416104252, 0.1792695777259361, 0.17816092407226292, 0.19628636488975992, 0.19237612183491615, 0.1874718333986961, 0.6294666203187917, 0.602690555999032, 0.5454952462675013, 0.7727337342883757, 0.25677125313719296, 0.588947972014151, 0.5053651729784945, 0.42433758940449484, 0.610090527727432, 0.7148365734773846, 0.39316249107203016, 0.4609914040189693, 0.5414475146943718, 0.3738614312609654, 0.2972834077003401, 0.2719039283446757, 0.4464259994844204, 0.6865015215956998, 0.18541089636272012, 0.17521600614441235, 0.18498258988690697, 0.1923345960637889, 0.19442272937612437, 0.18424555156947642, 0.17449564646480575, 0.18372628686318526, 0.18823517758491137, 0.07953923390817585, 0.08429672806217459, 0.08351517483047155, 0.07872531348008505, 0.0885755282031725, 0.09071254814221597, 0.07022751225803625, 0.07907174920771143, 0.08022853716110878]}, "mutation_prompt": null}
{"id": "ff6988e7-1d76-48b6-941c-013d31028bd4", "solution": "import numpy as np\n\nclass AdaptiveDiversityDrivenOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = 40\n        self.mutation_factor = 0.85\n        self.crossover_probability = 0.9\n        self.learning_rate = 0.25\n        self.diversity_factor = 0.15\n        self.population = None\n        self.fitness = None\n\n    def initialize_population(self):\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, \n                                            (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def adaptive_differential_evolution(self, idx, func):\n        indices = [i for i in range(self.population_size) if i != idx]\n        np.random.shuffle(indices)\n        a, b, c = indices[:3]\n        mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        adaptive_factor = 0.3 + 0.7 * np.random.rand()\n        mutant = self.population[idx] + adaptive_factor * (mutant - self.population[idx])\n        mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n        \n        cross_points = np.random.rand(self.dim) < self.crossover_probability\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        \n        trial = np.where(cross_points, mutant, self.population[idx])\n        trial_fitness = func(trial)\n        \n        if trial_fitness < self.fitness[idx]:\n            self.population[idx] = trial\n            self.fitness[idx] = trial_fitness\n\n    def contextual_learning_mechanism(self, idx, func):\n        gradient = np.random.normal(0, 1, self.dim)\n        adaptive_lr = self.learning_rate / (1 + idx / self.population_size)\n        candidate = self.population[idx] - adaptive_lr * gradient\n        candidate += self.diversity_factor * np.random.normal(0, 1, self.dim)\n        candidate = np.clip(candidate, self.lower_bound, self.upper_bound)\n        candidate_fitness = func(candidate)\n        if candidate_fitness < self.fitness[idx]:\n            self.population[idx] = candidate\n            self.fitness[idx] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population()\n        evaluations = 0\n        self.evaluate_population(func)\n        evaluations += self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                self.adaptive_differential_evolution(i, func)\n                evaluations += 1\n\n                if evaluations >= self.budget:\n                    break\n\n                self.contextual_learning_mechanism(i, func)\n                evaluations += 1\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "AdaptiveDiversityDrivenOptimizer", "description": "Adaptive diversity-driven hybrid optimizer with a novel probabilistic neighborhood exploration and contextual learning mechanism.", "configspace": "", "generation": 91, "fitness": 0.344295167041837, "feedback": "The algorithm AdaptiveDiversityDrivenOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.34 with standard deviation 0.24.", "error": "", "parent_id": "1459935d-0a5b-47de-8bfe-69f52cb3cb6c", "metadata": {"aucs": [0.8019265857240244, 0.8068702625101143, 0.8038265674399211, 0.7738498507927153, 0.787136984508026, 0.7996326673906546, 0.7863941717154689, 0.798134580426159, 0.8051232013298641, 0.5900190873321698, 0.5554692580993706, 0.5645218872536222, 0.5920954861748753, 0.5547981979103636, 0.6044858296110648, 0.5774301825700561, 0.5968171211591641, 0.5976355007618175, 0.1162612844675518, 0.10155518566992738, 0.0970715628620088, 0.12624674790047397, 0.10600281937227973, 0.10822418423292368, 0.11390204530590087, 0.11462023928736342, 0.10733605622316422, 0.10364128395746097, 0.08707973402089064, 0.09019790125103411, 0.10430132399644176, 0.08856010112223034, 0.10528975209295699, 0.10259122444223923, 0.09346478198334518, 0.09333723814774408, 0.8826233570469896, 0.8938215094639764, 0.8567041853136861, 0.8528700686083772, 0.7926150815865891, 0.8158151601532754, 0.8557084721933664, 0.8442521374349153, 0.8786439190127354, 0.3033018970488339, 0.3531057381654985, 0.34479775560903403, 0.32767313998476344, 0.34855665957414184, 0.32722095364052184, 0.30088268329513024, 0.33801393924459955, 0.29280503152516135, 0.7269037100924626, 0.7381997565368487, 0.7455875788936535, 0.7834608980192435, 0.7576107630079284, 0.7528064177556066, 0.7458982166112695, 0.7547962943661797, 0.7688623510267928, 0.15487132843590767, 0.2045419924682088, 0.18734487119144982, 0.25987602235238516, 0.21035951997614522, 0.1516489197466513, 0.20845374093814728, 0.2905063440569705, 0.2061676276723715, 0.20845670333871524, 0.16007484490690205, 0.15268844628862788, 0.20449976278864246, 0.1608796994087568, 0.17893725806842775, 0.17422378582294273, 0.20447496412998112, 0.1517253215192268, 0.3663636434037547, 0.3037513236289442, 0.38133668702135237, 0.33627089850294856, 0.386902613425651, 0.3403554006589026, 0.39228710054252214, 0.40807777136763856, 0.4041953726727041, 0.48224263398522405, 0.45979436057659184, 0.4998822973915519, 0.12952892775457636, 0.37229416235452484, 0.43964375759541885, 0.36330607377756474, 0.48639892718887323, 0.5124273696470019, 0.11294802563273953, 0.10899423485901627, 0.099765611521489, 0.18137939529792912, 0.11666203081819326, 0.09446994698188849, 0.15890989416320644, 0.09989223720463236, 0.12240212744256063, 0.28879893600236917, 0.3086771426249012, 0.2688998714824423, 0.3200547931356277, 0.3136069137993407, 0.2717500259880281, 0.2981888051967805, 0.2364388894423597, 0.27851915088768087, 0.696078641096612, 0.7061970691273575, 0.7104264942050762, 0.6719557877966716, 0.6715509046398065, 0.7071773396877352, 0.6944824402383752, 0.6808169280708167, 0.6800420922669588, 0.09804736966525474, 0.10340935139525187, 0.11190438572729378, 0.1288172364545166, 0.10315455136757268, 0.09332403946416834, 0.10275742710160651, 0.10043413475398044, 0.0938568808491802, 0.1760934502356979, 0.2313957666787042, 0.18752317723582068, 0.188396094693991, 0.20108004321514972, 0.18868912693141515, 0.18635183234747021, 0.1565183464940475, 0.16891978425986365, 0.3223390404210066, 0.3294742029444073, 0.32018971677118324, 0.33787664662404926, 0.3659074499500774, 0.3376192319740643, 0.3576525463506809, 0.3694153274010591, 0.37069870876304245, 0.2816886720493297, 0.25243586438859744, 0.2730653687730019, 0.2762530854511449, 0.27512261121036563, 0.29494654767110784, 0.2609379827698788, 0.34465947551038956, 0.2556087983719061, 0.20192739829746165, 0.20747946450699628, 0.1990296720836574, 0.21274168990883546, 0.21716050049959645, 0.19079906879192465, 0.21847631419401836, 0.1982196042988167, 0.21312975507320564, 0.18676147065278592, 0.18584666253003734, 0.18406621319963434, 0.1985828734863342, 0.18062365444718576, 0.1865117965609775, 0.1878520070236751, 0.1933202950850471, 0.18943116335007326, 0.6612794405050126, 0.5151577596210886, 0.6130584346158341, 0.7336087872314108, 0.35080203818494415, 0.4731234342820678, 0.4645828606234982, 0.33597047852281836, 0.39006414832146374, 0.751600521685933, 0.36643502095833835, 0.2809974162047508, 0.47408347460362155, 0.25742865758159206, 0.3224396247454332, 0.3530636820639025, 0.21517443992955532, 0.4442593872987819, 0.1728636350116861, 0.21022064846441835, 0.20830444283174365, 0.1850842719450474, 0.18130712959085782, 0.19133143731281066, 0.1823968479155057, 0.19574066343755425, 0.18526937488231454, 0.08773853042447521, 0.08097107915224844, 0.08462235608209501, 0.08979517609908094, 0.08611278941934108, 0.08601853131317261, 0.08678207763242629, 0.09201215125706608, 0.08626668255457448]}, "mutation_prompt": null}
{"id": "f37ab268-a18f-410f-8181-8fba9b4261ba", "solution": "import numpy as np\n\nclass ProbabilisticAdaptiveOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = 50  # Adjusted for better diversity\n        self.mutation_factor = 0.9  # Slightly increased for enhanced exploration\n        self.crossover_probability = 0.9\n        self.learning_rate = 0.15  # Adjusted to balance exploration and exploitation\n        self.diversity_factor = 0.15\n        self.population = None\n        self.fitness = None\n\n    def initialize_population(self):\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, \n                                            (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def enhanced_differential_mutation(self, idx, func):\n        indices = [i for i in range(self.population_size) if i != idx]\n        np.random.shuffle(indices)\n        a, b, c = indices[:3]\n        mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        adaptive_factor = 0.5 + 0.5 * np.random.rand()  # Increased randomness for adaptation\n        mutant = self.population[idx] + adaptive_factor * (mutant - self.population[idx])\n        mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n        \n        cross_points = np.random.rand(self.dim) < self.crossover_probability\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        \n        trial = np.where(cross_points, mutant, self.population[idx])\n        trial_fitness = func(trial)\n        \n        if trial_fitness < self.fitness[idx]:\n            self.population[idx] = trial\n            self.fitness[idx] = trial_fitness\n\n    def stochastic_gradient_perturbation(self, idx, func):\n        gradient = np.random.normal(0, 1, self.dim)\n        adaptive_lr = self.learning_rate / (1 + idx / self.population_size)\n        candidate = self.population[idx] - adaptive_lr * gradient\n        candidate += self.diversity_factor * np.random.normal(0, 1, self.dim)\n        candidate = np.clip(candidate, self.lower_bound, self.upper_bound)\n        candidate_fitness = func(candidate)\n        if candidate_fitness < self.fitness[idx]:\n            self.population[idx] = candidate\n            self.fitness[idx] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population()\n        evaluations = 0\n        self.evaluate_population(func)\n        evaluations += self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                self.enhanced_differential_mutation(i, func)\n                evaluations += 1\n\n                if evaluations >= self.budget:\n                    break\n\n                self.stochastic_gradient_perturbation(i, func)\n                evaluations += 1\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "ProbabilisticAdaptiveOptimizer", "description": "Probabilistic Adaptive Optimizer leveraging enhanced mutation strategies and stochastic components for efficient convergence on diverse optimization landscapes.", "configspace": "", "generation": 92, "fitness": 0.28464380004487566, "feedback": "The algorithm ProbabilisticAdaptiveOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28 with standard deviation 0.20.", "error": "", "parent_id": "1459935d-0a5b-47de-8bfe-69f52cb3cb6c", "metadata": {"aucs": [0.6605470981089374, 0.6705333162978047, 0.682656679345612, 0.6870462366398069, 0.6764314484115737, 0.6721790347437242, 0.6873868005389, 0.680474837501718, 0.6740844240018651, 0.39412523397984356, 0.4033880493938231, 0.37066156616365686, 0.3779976558053181, 0.37889566882987924, 0.4293950351474397, 0.33456375499286384, 0.3885975597540019, 0.39680154528103617, 0.0991100891829696, 0.09881105924463696, 0.10560200352786486, 0.11038283415025985, 0.10252540824031553, 0.10784084214169853, 0.12223141209512745, 0.0981509437868443, 0.10707142104976797, 0.09084689920795486, 0.08796292027796404, 0.0834827621977291, 0.08731877607607419, 0.09382163659722642, 0.08417798296687962, 0.08803116752676798, 0.08904276213179041, 0.09741516591511135, 0.9330340677339195, 0.8903544521339327, 0.9595482359922426, 0.8472812940810512, 0.831156342536172, 0.8868006653516718, 0.8336356791626951, 0.810465066732764, 0.8328332084726063, 0.29330640429065213, 0.2770840675480223, 0.29871527697086475, 0.29422678849611517, 0.30874950196305617, 0.30233181763475114, 0.3179037194906936, 0.2934780646421601, 0.29337107448635125, 0.5745709570749804, 0.6215608409047537, 0.59150819616658, 0.6640687460999966, 0.5936441979605623, 0.6357561346194105, 0.6037458291721465, 0.6172427988564269, 0.6901983890015502, 0.18026346894288892, 0.15195829610749134, 0.1675674627349315, 0.16083418361906965, 0.1645527970831152, 0.16634105988465364, 0.16837428144420175, 0.17460117704616773, 0.16290957281969476, 0.17284478349030064, 0.13118405808868283, 0.18343329546982734, 0.14196215231566567, 0.1806887077503605, 0.1743012728462503, 0.19725615059826396, 0.17080983828178542, 0.18750012872090305, 0.16830558062237555, 0.17865128610568082, 0.1626505351019183, 0.17756856095169715, 0.20370848355211235, 0.1692741707529969, 0.18152992263764856, 0.20746972547291265, 0.21477269435788682, 0.3468537685573496, 0.31905696761379954, 0.29185583574172425, 0.3457366880818512, 0.30170886947171915, 0.32365664121893445, 0.3164913339199248, 0.3229092164236631, 0.3499624884763325, 0.05139633726923898, 0.06215061673142264, 0.06708405436601861, 0.07182930584434888, 0.048941433815587354, 0.09187172738811622, 0.06161230381860183, 0.07616179832299552, 0.056215963915268174, 0.18903405034917864, 0.19781412815551946, 0.203726221921679, 0.20258483616185663, 0.19096012656940164, 0.21051534404463212, 0.1734292709956775, 0.19851487027189607, 0.1902279051326332, 0.5293213543865685, 0.5446427667045353, 0.5273268323875527, 0.5357154408077701, 0.5371640070472288, 0.5109768575860294, 0.5657553639764106, 0.5458906664900376, 0.5539427128758025, 0.12146128841983794, 0.09219426848017409, 0.0974922851901251, 0.09919425977266672, 0.09965138023027664, 0.08757479527620804, 0.10183746964300189, 0.10931141138344591, 0.0898269353790303, 0.18977193528222502, 0.19688289229124578, 0.1813414256158965, 0.20645954549796874, 0.18823268420294603, 0.17875755656418224, 0.2164362817371278, 0.19708864205139132, 0.1803887430433, 0.2634218541877267, 0.28398031838426807, 0.3092233056074375, 0.2612792643333183, 0.284652674000372, 0.28498940198305944, 0.2920656506648396, 0.30389358372407227, 0.2956416040204243, 0.21067846099870813, 0.22340856956989852, 0.232091862294602, 0.22162908185165509, 0.2225549711482976, 0.21163969456114773, 0.22499582363860438, 0.2336657962782921, 0.2256108377365006, 0.2195830985126841, 0.18847477546483005, 0.19129110014376471, 0.2320508483627849, 0.19344614185197084, 0.1914334795356296, 0.2064001036573624, 0.20641737556399242, 0.2148943878604389, 0.18661984670298193, 0.19699269348819115, 0.18531296748351456, 0.18535257744256173, 0.18097514632774314, 0.19102836700332682, 0.19511409975086136, 0.1850808825346124, 0.186144009091196, 0.18530644558023746, 0.2661401254716985, 0.3284256349077119, 0.4495965207678493, 0.3499391624856214, 0.3528440246863529, 0.34697025291435746, 0.3829250361828893, 0.3962659866983711, 0.3575730938131081, 0.2771940360331371, 0.3119683694436046, 0.3508141248297645, 0.3122889323528317, 0.3688535218574691, 0.30758967586330255, 0.375208569879371, 0.3788720246819124, 0.18605696952754747, 0.18495136831806447, 0.18521172376468553, 0.1836641186063428, 0.18850086881348294, 0.18106013814471034, 0.2018209999051609, 0.19503611855299374, 0.17892708160358273, 0.08287570506513398, 0.08497318008777688, 0.07559094268602606, 0.07716408225247617, 0.07393358482809231, 0.07645804158120606, 0.08211770520376083, 0.08296022640308598, 0.07652835760499455]}, "mutation_prompt": null}
{"id": "21a50f06-9338-4207-9bc1-2ec7076de81a", "solution": "import numpy as np\n\nclass AdaptiveMultiStrategyOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = 50\n        self.mutation_factor = 0.9\n        self.crossover_probability = 0.9\n        self.learning_rate = 0.15\n        self.diversity_factor = 0.15\n        self.population = None\n        self.fitness = None\n\n    def initialize_population(self):\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, \n                                            (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def adaptive_differential_evolution(self, idx, func):\n        indices = [i for i in range(self.population_size) if i != idx]\n        np.random.shuffle(indices)\n        a, b, c = indices[:3]\n        mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        adaptive_factor = 0.35 + 0.65 * np.random.rand()\n        mutant = self.population[idx] + adaptive_factor * (mutant - self.population[idx])\n        mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n        \n        cross_points = np.random.rand(self.dim) < self.crossover_probability\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        \n        trial = np.where(cross_points, mutant, self.population[idx])\n        trial_fitness = func(trial)\n        \n        if trial_fitness < self.fitness[idx]:\n            self.population[idx] = trial\n            self.fitness[idx] = trial_fitness\n\n    def stochastic_gradient_approx(self, idx, func):\n        gradient = np.random.normal(0, 1, self.dim)\n        adaptive_lr = self.learning_rate / (1 + idx / self.population_size)\n        candidate = self.population[idx] - adaptive_lr * gradient\n        candidate += self.diversity_factor * np.random.normal(0, 1, self.dim)\n        candidate = np.clip(candidate, self.lower_bound, self.upper_bound)\n        candidate_fitness = func(candidate)\n        if candidate_fitness < self.fitness[idx]:\n            self.population[idx] = candidate\n            self.fitness[idx] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population()\n        evaluations = 0\n        self.evaluate_population(func)\n        evaluations += self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                self.adaptive_differential_evolution(i, func)\n                evaluations += 1\n\n                if evaluations >= self.budget:\n                    break\n\n                self.stochastic_gradient_approx(i, func)\n                evaluations += 1\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "AdaptiveMultiStrategyOptimizer", "description": "Adaptive multi-strategy optimizer combining diversified differential evolution with stochastic local search and dynamically adjusted parameters for robust performance.", "configspace": "", "generation": 93, "fitness": 0.3047321965542268, "feedback": "The algorithm AdaptiveMultiStrategyOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.30 with standard deviation 0.21.", "error": "", "parent_id": "1459935d-0a5b-47de-8bfe-69f52cb3cb6c", "metadata": {"aucs": [0.7395600711643502, 0.7239884599107721, 0.7246295191003792, 0.7012018301596415, 0.6989697689188381, 0.709857292588025, 0.7347229757745108, 0.7119827142633244, 0.7227193080508166, 0.44532086133324433, 0.46996777542297064, 0.4562913499597223, 0.4601192270501139, 0.42691749528279865, 0.4196605003632179, 0.43136646392480416, 0.4579872742404455, 0.47194928925272583, 0.10969743782010355, 0.10956114284727514, 0.11035464388669403, 0.09779595676743413, 0.11662801930000877, 0.10838203419329873, 0.11318218897020449, 0.12502279563278185, 0.11796609147333581, 0.09152648544483322, 0.08183947630272581, 0.08460904061468955, 0.09735760244028913, 0.09814526085573183, 0.09040800603466081, 0.083770166366123, 0.10635278526299119, 0.09080783109812474, 0.8026154981368951, 0.8094374344517573, 0.806207592630297, 0.7756354913862423, 0.8087784622157691, 0.7993597975436793, 0.781748638267829, 0.8208016888426437, 0.7931330726354426, 0.3015025008741893, 0.2999341945240771, 0.3167456657960984, 0.30173458348872095, 0.30368422759646496, 0.32100345564229293, 0.3045088299107288, 0.3166643766956285, 0.273501574077586, 0.6120827131102944, 0.6538878057428646, 0.6297770598666246, 0.7111112052744542, 0.6985879401588369, 0.6773055779436291, 0.7135715533849376, 0.7231679930804951, 0.6833007797595462, 0.15993613480118474, 0.13720230532491495, 0.15370986368913853, 0.17570126880343173, 0.1681779609978219, 0.19904651785388205, 0.1881488258680849, 0.16011023620984455, 0.1468779392663595, 0.15658353465186925, 0.18321666495519073, 0.18325471108622426, 0.1592457926137849, 0.19368489281232826, 0.19292325426435786, 0.17262557455685523, 0.1611105520813053, 0.22447733845835738, 0.24862143607231368, 0.23284750974469903, 0.25718886653022555, 0.2654691763474579, 0.22107751970174605, 0.22253079994042368, 0.2334938257431014, 0.2313616646029053, 0.2628025412414503, 0.38258767418511186, 0.3466729591582043, 0.4037364260619588, 0.38284071883841064, 0.3862791466938228, 0.384899513648723, 0.36437338623873616, 0.41723128913365326, 0.4285622661702928, 0.101444135393435, 0.11541021956160824, 0.06884038519157454, 0.0906730556322527, 0.10973827412896153, 0.07980662271612016, 0.08833306596835155, 0.10116563669165446, 0.06011047302684147, 0.22240058650007322, 0.232287644207389, 0.2243459552041338, 0.2469410485236163, 0.23088099187560662, 0.24756331569857304, 0.22210887880404262, 0.21949637091957186, 0.22461358661920738, 0.6031663262841749, 0.5728607771881584, 0.6140811548977154, 0.5803897418896797, 0.5798528331601487, 0.5774851786754895, 0.5884224471546976, 0.6070315968075717, 0.607168412803478, 0.10333014130117357, 0.10534091852723793, 0.10036570844927095, 0.10381995494730689, 0.11145960217404549, 0.12112172201175331, 0.11274562910570696, 0.09973556653226046, 0.1132050356266251, 0.19536723130941258, 0.19031342002752794, 0.18827378079118628, 0.18329047700368462, 0.2070903816865839, 0.28572980771804746, 0.21389519231935905, 0.16335149690054818, 0.1837888131642349, 0.28689476412763115, 0.2929940854316513, 0.3037899843154924, 0.3050769218379411, 0.31278066419228745, 0.32558279490907416, 0.3190490851186769, 0.3066696644390673, 0.309719000483729, 0.24045366438688953, 0.23322722949373942, 0.25686203690615805, 0.21694046545348, 0.23276734509953545, 0.23548940937184426, 0.21263961077041993, 0.2197305118229005, 0.2595834799046812, 0.2186087104328912, 0.21449827759293572, 0.20394321395492332, 0.20020949274667088, 0.19532674289600838, 0.1996904214258477, 0.21702348848850395, 0.18793763187982315, 0.20365074773433733, 0.18138056557691018, 0.19175222043878792, 0.174464414238805, 0.18592411312779866, 0.18444497228516143, 0.189843052896556, 0.19292601160437894, 0.1964338336375101, 0.19415545683210356, 0.326702421806516, 0.32579138170871336, 0.4251110399003576, 0.5405360318861555, 0.3788622157450936, 0.360102071080035, 0.2623686394358107, 0.30106097126756326, 0.4746121982853342, 0.3696038668608018, 0.346266892227885, 0.643524230074594, 0.44916215790140346, 0.34075610265870127, 0.34947373963766093, 0.4062904232368628, 0.2536573004393149, 0.347752611527512, 0.17941093513178763, 0.18369956975080337, 0.18169514328110692, 0.1791217889150838, 0.17664458177776543, 0.21602286645555013, 0.1750375286602599, 0.17350436153812865, 0.18548679009340152, 0.07425197703816344, 0.07737843797446442, 0.07378386448969398, 0.08992390209108547, 0.08132784964263706, 0.0734671296024424, 0.07721423962059526, 0.07728506608170105, 0.07493750345454353]}, "mutation_prompt": null}
{"id": "e3bbc142-370b-4827-b2c6-31c0671b6624", "solution": "import numpy as np\n\nclass EnhancedProbabilisticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = 40\n        self.mutation_factor = 0.85\n        self.crossover_probability = 0.95\n        self.learning_rate = 0.2\n        self.diversity_factor = 0.1\n        self.population = None\n        self.fitness = None\n\n    def initialize_population(self):\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, \n                                            (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def adaptive_differential_evolution(self, idx, func):\n        indices = [i for i in range(self.population_size) if i != idx]\n        np.random.shuffle(indices)\n        a, b, c = indices[:3]\n        mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        adaptive_factor = 0.4 + 0.6 * np.random.rand()\n        mutant = self.population[idx] + adaptive_factor * (mutant - self.population[idx])\n        mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n        \n        cross_points = np.random.rand(self.dim) < self.crossover_probability\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        \n        trial = np.where(cross_points, mutant, self.population[idx])\n        trial_fitness = func(trial)\n        \n        if trial_fitness < self.fitness[idx]:\n            self.population[idx] = trial\n            self.fitness[idx] = trial_fitness\n\n    def stochastic_gradient_approx(self, idx, func):\n        gradient = np.random.normal(0, 1, self.dim)\n        adaptive_lr = self.learning_rate / (1 + idx / self.population_size)\n        candidate = self.population[idx] - adaptive_lr * gradient\n        candidate += self.diversity_factor * np.random.normal(0, 1, self.dim)\n        candidate = np.clip(candidate, self.lower_bound, self.upper_bound)\n        candidate_fitness = func(candidate)\n        if candidate_fitness < self.fitness[idx]:\n            self.population[idx] = candidate\n            self.fitness[idx] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population()\n        evaluations = 0\n        self.evaluate_population(func)\n        evaluations += self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                self.adaptive_differential_evolution(i, func)\n                evaluations += 1\n\n                if evaluations >= self.budget:\n                    break\n\n                self.stochastic_gradient_approx(i, func)\n                evaluations += 1\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "EnhancedProbabilisticOptimizer", "description": "Enhanced probabilistic hybrid optimizer balancing exploration and exploitation through diversity-oriented selection and adaptive parameter tuning for optimal convergence.", "configspace": "", "generation": 38, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "1459935d-0a5b-47de-8bfe-69f52cb3cb6c", "metadata": {"aucs": [0.796727445346743, 0.7946092935536371, 0.8025157468331956, 0.7795890428061506, 0.7652866403837684, 0.7888980264133363, 0.7851539828124771, 0.7801166288092796, 0.778684214476141, 0.6066596860215113, 0.5509488918434079, 0.584709500115218, 0.6151880393982149, 0.6128285196462557, 0.5928554548480235, 0.18781852246903208, 0.6051102165828588, 0.6050716765741883, 0.12056849613284826, 0.10627786970516262, 0.11092740344636509, 0.12458142503345515, 0.10225070914012868, 0.10214739878052792, 0.11007052847089493, 0.11303443664409152, 0.11346829363668931, 0.09058514750802149, 0.09136224141572435, 0.09346304265481342, 0.08413688806219866, 0.10560994817344638, 0.09386944951658027, 0.1066829182603829, 0.09525027475772718, 0.09078080677184064, 0.8379701410373176, 0.8069154658397715, 0.8108437032002754, 0.8202640448333322, 0.7815128713795206, 0.8451084830274826, 0.836376388134602, 0.8441441978939892, 0.8509693104465095, 0.37150586919602946, 0.34273942956965175, 0.3313822553845982, 0.34968755932637086, 0.32960730205573996, 0.3754984208710611, 0.38284578850911233, 0.3840456442426208, 0.3280564413760412, 0.7576709220936438, 0.7335556841917643, 0.7658090217912067, 0.7934167558179147, 0.8025617081483369, 0.7999764269594076, 0.7299292035530152, 0.7697788108067551, 0.7651252822797626, 0.16167439560696928, 0.18696031846775318, 0.21158580997319565, 0.244349141547606, 0.2890559956537958, 0.16298570563996873, 0.21750653437837209, 0.21122767559906996, 0.22550289679700497, 0.15305768362463734, 0.14884944831507896, 0.18600965489753707, 0.1541357033150358, 0.19832744782165168, 0.18983314847500432, 0.1953210887165725, 0.20165646735698872, 0.13088573983753993, 0.489476775395826, 0.460206761694571, 0.280690722270316, 0.34064732845138956, 0.522953329166158, 0.46452239779932414, 0.4196789194960461, 0.4727221489210359, 0.44131563048738687, 0.6540713674624187, 0.6128130384068273, 0.5553131009305013, 0.5776200055109268, 0.4723536786910414, 0.6219680956281209, 0.6455804612075908, 0.6109257481083044, 0.5996992464126676, 0.15220127888973134, 0.11018658007737869, 0.2413715712685326, 0.2296484097510183, 0.2572737636354798, 0.10622686325653308, 0.14604068872930986, 0.14809138800719435, 0.12257392403735012, 0.36655256082417187, 0.34632185847096675, 0.3516395670957846, 0.33828319802853724, 0.3572041238804262, 0.35621925345698524, 0.34628041224843775, 0.36205674625584605, 0.3801826028652373, 0.7284525303733058, 0.7334889541026953, 0.7386557191632159, 0.7055754468414382, 0.7222856768071027, 0.7194774022217179, 0.518854950686749, 0.7321096452535195, 0.7397060954282213, 0.10374399773694243, 0.104282622260642, 0.1086381702881759, 0.11429613558107654, 0.11511930056195652, 0.10035386421161252, 0.09599830676572241, 0.11347281801819442, 0.11470225552281121, 0.14952999643648723, 0.16817216547016145, 0.17310734903563885, 0.1855986179207637, 0.18359480037739373, 0.19508916572226054, 0.20087729718879477, 0.17807185486405164, 0.20119801391244463, 0.3682121014468256, 0.35073539948175425, 0.3330439038672147, 0.4157077631363848, 0.37474021495439724, 0.36760475650845414, 0.38008256501576043, 0.39084781839078786, 0.3786757352451906, 0.30037946434366947, 0.2861866860206934, 0.2891587957817804, 0.2835627691001067, 0.2779602895144492, 0.28387893685888044, 0.29400381025955125, 0.3244615423116647, 0.30188351717337136, 0.20494558183242018, 0.19905376550038034, 0.20406514428030742, 0.2229753496453719, 0.1980660060283188, 0.21539123082194234, 0.21928087314384304, 0.20854328449838244, 0.2102975078755156, 0.18082128476991477, 0.1831737258940077, 0.18089956975930732, 0.18277323416104252, 0.1792695777259361, 0.17816092407226292, 0.19628636488975992, 0.19237612183491615, 0.1874718333986961, 0.6294666203187917, 0.602690555999032, 0.5454952462675013, 0.7727337342883757, 0.25677125313719296, 0.588947972014151, 0.5053651729784945, 0.42433758940449484, 0.610090527727432, 0.7148365734773846, 0.39316249107203016, 0.4609914040189693, 0.5414475146943718, 0.3738614312609654, 0.2972834077003401, 0.2719039283446757, 0.4464259994844204, 0.6865015215956998, 0.18541089636272012, 0.17521600614441235, 0.18498258988690697, 0.1923345960637889, 0.19442272937612437, 0.18424555156947642, 0.17449564646480575, 0.18372628686318526, 0.18823517758491137, 0.07953923390817585, 0.08429672806217459, 0.08351517483047155, 0.07872531348008505, 0.0885755282031725, 0.09071254814221597, 0.07022751225803625, 0.07907174920771143, 0.08022853716110878]}, "mutation_prompt": null}
{"id": "caef9fcd-dc97-41dd-8f95-55d76b769973", "solution": "import numpy as np\n\nclass EnhancedProbabilisticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = 40\n        self.mutation_factor = 0.85\n        self.crossover_probability = 0.95\n        self.learning_rate = 0.2\n        self.diversity_factor = 0.1\n        self.population = None\n        self.fitness = None\n\n    def initialize_population(self):\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, \n                                            (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def adaptive_differential_evolution(self, idx, func):\n        indices = [i for i in range(self.population_size) if i != idx]\n        np.random.shuffle(indices)\n        a, b, c = indices[:3]\n        mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        adaptive_factor = 0.4 + 0.6 * np.random.rand()\n        mutant = self.population[idx] + adaptive_factor * (mutant - self.population[idx])\n        mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n        \n        cross_points = np.random.rand(self.dim) < self.crossover_probability\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        \n        trial = np.where(cross_points, mutant, self.population[idx])\n        trial_fitness = func(trial)\n        \n        if trial_fitness < self.fitness[idx]:\n            self.population[idx] = trial\n            self.fitness[idx] = trial_fitness\n\n    def stochastic_gradient_approx(self, idx, func):\n        gradient = np.random.normal(0, 1, self.dim)\n        adaptive_lr = self.learning_rate / (1 + idx / self.population_size)\n        candidate = self.population[idx] - adaptive_lr * gradient\n        candidate += self.diversity_factor * np.random.normal(0, 1, self.dim)\n        candidate = np.clip(candidate, self.lower_bound, self.upper_bound)\n        candidate_fitness = func(candidate)\n        if candidate_fitness < self.fitness[idx]:\n            self.population[idx] = candidate\n            self.fitness[idx] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population()\n        evaluations = 0\n        self.evaluate_population(func)\n        evaluations += self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                self.adaptive_differential_evolution(i, func)\n                evaluations += 1\n\n                if evaluations >= self.budget:\n                    break\n\n                self.stochastic_gradient_approx(i, func)\n                evaluations += 1\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "EnhancedProbabilisticOptimizer", "description": "Enhanced probabilistic hybrid optimizer balancing exploration and exploitation through diversity-oriented selection and adaptive parameter tuning for optimal convergence.", "configspace": "", "generation": 38, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "1459935d-0a5b-47de-8bfe-69f52cb3cb6c", "metadata": {"aucs": [0.796727445346743, 0.7946092935536371, 0.8025157468331956, 0.7795890428061506, 0.7652866403837684, 0.7888980264133363, 0.7851539828124771, 0.7801166288092796, 0.778684214476141, 0.6066596860215113, 0.5509488918434079, 0.584709500115218, 0.6151880393982149, 0.6128285196462557, 0.5928554548480235, 0.18781852246903208, 0.6051102165828588, 0.6050716765741883, 0.12056849613284826, 0.10627786970516262, 0.11092740344636509, 0.12458142503345515, 0.10225070914012868, 0.10214739878052792, 0.11007052847089493, 0.11303443664409152, 0.11346829363668931, 0.09058514750802149, 0.09136224141572435, 0.09346304265481342, 0.08413688806219866, 0.10560994817344638, 0.09386944951658027, 0.1066829182603829, 0.09525027475772718, 0.09078080677184064, 0.8379701410373176, 0.8069154658397715, 0.8108437032002754, 0.8202640448333322, 0.7815128713795206, 0.8451084830274826, 0.836376388134602, 0.8441441978939892, 0.8509693104465095, 0.37150586919602946, 0.34273942956965175, 0.3313822553845982, 0.34968755932637086, 0.32960730205573996, 0.3754984208710611, 0.38284578850911233, 0.3840456442426208, 0.3280564413760412, 0.7576709220936438, 0.7335556841917643, 0.7658090217912067, 0.7934167558179147, 0.8025617081483369, 0.7999764269594076, 0.7299292035530152, 0.7697788108067551, 0.7651252822797626, 0.16167439560696928, 0.18696031846775318, 0.21158580997319565, 0.244349141547606, 0.2890559956537958, 0.16298570563996873, 0.21750653437837209, 0.21122767559906996, 0.22550289679700497, 0.15305768362463734, 0.14884944831507896, 0.18600965489753707, 0.1541357033150358, 0.19832744782165168, 0.18983314847500432, 0.1953210887165725, 0.20165646735698872, 0.13088573983753993, 0.489476775395826, 0.460206761694571, 0.280690722270316, 0.34064732845138956, 0.522953329166158, 0.46452239779932414, 0.4196789194960461, 0.4727221489210359, 0.44131563048738687, 0.6540713674624187, 0.6128130384068273, 0.5553131009305013, 0.5776200055109268, 0.4723536786910414, 0.6219680956281209, 0.6455804612075908, 0.6109257481083044, 0.5996992464126676, 0.15220127888973134, 0.11018658007737869, 0.2413715712685326, 0.2296484097510183, 0.2572737636354798, 0.10622686325653308, 0.14604068872930986, 0.14809138800719435, 0.12257392403735012, 0.36655256082417187, 0.34632185847096675, 0.3516395670957846, 0.33828319802853724, 0.3572041238804262, 0.35621925345698524, 0.34628041224843775, 0.36205674625584605, 0.3801826028652373, 0.7284525303733058, 0.7334889541026953, 0.7386557191632159, 0.7055754468414382, 0.7222856768071027, 0.7194774022217179, 0.518854950686749, 0.7321096452535195, 0.7397060954282213, 0.10374399773694243, 0.104282622260642, 0.1086381702881759, 0.11429613558107654, 0.11511930056195652, 0.10035386421161252, 0.09599830676572241, 0.11347281801819442, 0.11470225552281121, 0.14952999643648723, 0.16817216547016145, 0.17310734903563885, 0.1855986179207637, 0.18359480037739373, 0.19508916572226054, 0.20087729718879477, 0.17807185486405164, 0.20119801391244463, 0.3682121014468256, 0.35073539948175425, 0.3330439038672147, 0.4157077631363848, 0.37474021495439724, 0.36760475650845414, 0.38008256501576043, 0.39084781839078786, 0.3786757352451906, 0.30037946434366947, 0.2861866860206934, 0.2891587957817804, 0.2835627691001067, 0.2779602895144492, 0.28387893685888044, 0.29400381025955125, 0.3244615423116647, 0.30188351717337136, 0.20494558183242018, 0.19905376550038034, 0.20406514428030742, 0.2229753496453719, 0.1980660060283188, 0.21539123082194234, 0.21928087314384304, 0.20854328449838244, 0.2102975078755156, 0.18082128476991477, 0.1831737258940077, 0.18089956975930732, 0.18277323416104252, 0.1792695777259361, 0.17816092407226292, 0.19628636488975992, 0.19237612183491615, 0.1874718333986961, 0.6294666203187917, 0.602690555999032, 0.5454952462675013, 0.7727337342883757, 0.25677125313719296, 0.588947972014151, 0.5053651729784945, 0.42433758940449484, 0.610090527727432, 0.7148365734773846, 0.39316249107203016, 0.4609914040189693, 0.5414475146943718, 0.3738614312609654, 0.2972834077003401, 0.2719039283446757, 0.4464259994844204, 0.6865015215956998, 0.18541089636272012, 0.17521600614441235, 0.18498258988690697, 0.1923345960637889, 0.19442272937612437, 0.18424555156947642, 0.17449564646480575, 0.18372628686318526, 0.18823517758491137, 0.07953923390817585, 0.08429672806217459, 0.08351517483047155, 0.07872531348008505, 0.0885755282031725, 0.09071254814221597, 0.07022751225803625, 0.07907174920771143, 0.08022853716110878]}, "mutation_prompt": null}
{"id": "a36eaa25-1c68-459e-ab4b-b4300d15c51b", "solution": "import numpy as np\n\nclass EnhancedProbabilisticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = 40\n        self.mutation_factor = 0.85\n        self.crossover_probability = 0.95\n        self.learning_rate = 0.2\n        self.diversity_factor = 0.1\n        self.population = None\n        self.fitness = None\n\n    def initialize_population(self):\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, \n                                            (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def adaptive_differential_evolution(self, idx, func):\n        indices = [i for i in range(self.population_size) if i != idx]\n        np.random.shuffle(indices)\n        a, b, c = indices[:3]\n        mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        adaptive_factor = 0.4 + 0.6 * np.random.rand()\n        mutant = self.population[idx] + adaptive_factor * (mutant - self.population[idx])\n        mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n        \n        cross_points = np.random.rand(self.dim) < self.crossover_probability\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        \n        trial = np.where(cross_points, mutant, self.population[idx])\n        trial_fitness = func(trial)\n        \n        if trial_fitness < self.fitness[idx]:\n            self.population[idx] = trial\n            self.fitness[idx] = trial_fitness\n\n    def stochastic_gradient_approx(self, idx, func):\n        gradient = np.random.normal(0, 1, self.dim)\n        adaptive_lr = self.learning_rate / (1 + idx / self.population_size)\n        candidate = self.population[idx] - adaptive_lr * gradient\n        candidate += self.diversity_factor * np.random.normal(0, 1, self.dim)\n        candidate = np.clip(candidate, self.lower_bound, self.upper_bound)\n        candidate_fitness = func(candidate)\n        if candidate_fitness < self.fitness[idx]:\n            self.population[idx] = candidate\n            self.fitness[idx] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population()\n        evaluations = 0\n        self.evaluate_population(func)\n        evaluations += self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                self.adaptive_differential_evolution(i, func)\n                evaluations += 1\n\n                if evaluations >= self.budget:\n                    break\n\n                self.stochastic_gradient_approx(i, func)\n                evaluations += 1\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "EnhancedProbabilisticOptimizer", "description": "Enhanced probabilistic hybrid optimizer balancing exploration and exploitation through diversity-oriented selection and adaptive parameter tuning for optimal convergence.", "configspace": "", "generation": 38, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "1459935d-0a5b-47de-8bfe-69f52cb3cb6c", "metadata": {"aucs": [0.796727445346743, 0.7946092935536371, 0.8025157468331956, 0.7795890428061506, 0.7652866403837684, 0.7888980264133363, 0.7851539828124771, 0.7801166288092796, 0.778684214476141, 0.6066596860215113, 0.5509488918434079, 0.584709500115218, 0.6151880393982149, 0.6128285196462557, 0.5928554548480235, 0.18781852246903208, 0.6051102165828588, 0.6050716765741883, 0.12056849613284826, 0.10627786970516262, 0.11092740344636509, 0.12458142503345515, 0.10225070914012868, 0.10214739878052792, 0.11007052847089493, 0.11303443664409152, 0.11346829363668931, 0.09058514750802149, 0.09136224141572435, 0.09346304265481342, 0.08413688806219866, 0.10560994817344638, 0.09386944951658027, 0.1066829182603829, 0.09525027475772718, 0.09078080677184064, 0.8379701410373176, 0.8069154658397715, 0.8108437032002754, 0.8202640448333322, 0.7815128713795206, 0.8451084830274826, 0.836376388134602, 0.8441441978939892, 0.8509693104465095, 0.37150586919602946, 0.34273942956965175, 0.3313822553845982, 0.34968755932637086, 0.32960730205573996, 0.3754984208710611, 0.38284578850911233, 0.3840456442426208, 0.3280564413760412, 0.7576709220936438, 0.7335556841917643, 0.7658090217912067, 0.7934167558179147, 0.8025617081483369, 0.7999764269594076, 0.7299292035530152, 0.7697788108067551, 0.7651252822797626, 0.16167439560696928, 0.18696031846775318, 0.21158580997319565, 0.244349141547606, 0.2890559956537958, 0.16298570563996873, 0.21750653437837209, 0.21122767559906996, 0.22550289679700497, 0.15305768362463734, 0.14884944831507896, 0.18600965489753707, 0.1541357033150358, 0.19832744782165168, 0.18983314847500432, 0.1953210887165725, 0.20165646735698872, 0.13088573983753993, 0.489476775395826, 0.460206761694571, 0.280690722270316, 0.34064732845138956, 0.522953329166158, 0.46452239779932414, 0.4196789194960461, 0.4727221489210359, 0.44131563048738687, 0.6540713674624187, 0.6128130384068273, 0.5553131009305013, 0.5776200055109268, 0.4723536786910414, 0.6219680956281209, 0.6455804612075908, 0.6109257481083044, 0.5996992464126676, 0.15220127888973134, 0.11018658007737869, 0.2413715712685326, 0.2296484097510183, 0.2572737636354798, 0.10622686325653308, 0.14604068872930986, 0.14809138800719435, 0.12257392403735012, 0.36655256082417187, 0.34632185847096675, 0.3516395670957846, 0.33828319802853724, 0.3572041238804262, 0.35621925345698524, 0.34628041224843775, 0.36205674625584605, 0.3801826028652373, 0.7284525303733058, 0.7334889541026953, 0.7386557191632159, 0.7055754468414382, 0.7222856768071027, 0.7194774022217179, 0.518854950686749, 0.7321096452535195, 0.7397060954282213, 0.10374399773694243, 0.104282622260642, 0.1086381702881759, 0.11429613558107654, 0.11511930056195652, 0.10035386421161252, 0.09599830676572241, 0.11347281801819442, 0.11470225552281121, 0.14952999643648723, 0.16817216547016145, 0.17310734903563885, 0.1855986179207637, 0.18359480037739373, 0.19508916572226054, 0.20087729718879477, 0.17807185486405164, 0.20119801391244463, 0.3682121014468256, 0.35073539948175425, 0.3330439038672147, 0.4157077631363848, 0.37474021495439724, 0.36760475650845414, 0.38008256501576043, 0.39084781839078786, 0.3786757352451906, 0.30037946434366947, 0.2861866860206934, 0.2891587957817804, 0.2835627691001067, 0.2779602895144492, 0.28387893685888044, 0.29400381025955125, 0.3244615423116647, 0.30188351717337136, 0.20494558183242018, 0.19905376550038034, 0.20406514428030742, 0.2229753496453719, 0.1980660060283188, 0.21539123082194234, 0.21928087314384304, 0.20854328449838244, 0.2102975078755156, 0.18082128476991477, 0.1831737258940077, 0.18089956975930732, 0.18277323416104252, 0.1792695777259361, 0.17816092407226292, 0.19628636488975992, 0.19237612183491615, 0.1874718333986961, 0.6294666203187917, 0.602690555999032, 0.5454952462675013, 0.7727337342883757, 0.25677125313719296, 0.588947972014151, 0.5053651729784945, 0.42433758940449484, 0.610090527727432, 0.7148365734773846, 0.39316249107203016, 0.4609914040189693, 0.5414475146943718, 0.3738614312609654, 0.2972834077003401, 0.2719039283446757, 0.4464259994844204, 0.6865015215956998, 0.18541089636272012, 0.17521600614441235, 0.18498258988690697, 0.1923345960637889, 0.19442272937612437, 0.18424555156947642, 0.17449564646480575, 0.18372628686318526, 0.18823517758491137, 0.07953923390817585, 0.08429672806217459, 0.08351517483047155, 0.07872531348008505, 0.0885755282031725, 0.09071254814221597, 0.07022751225803625, 0.07907174920771143, 0.08022853716110878]}, "mutation_prompt": null}
{"id": "58472755-cf42-4861-9d9f-630e069d0901", "solution": "import numpy as np\n\nclass EnhancedAdaptiveOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = 50  # Changed population size to 50\n        self.mutation_factor = 0.9  # Adjusted mutation factor for more exploration\n        self.crossover_probability = 0.9  # Slightly reduced crossover probability\n        self.learning_rate = 0.1  # Lowered learning rate for finer updates\n        self.diversity_factor = 0.15  # Increased diversity factor for more exploration\n        self.population = None\n        self.fitness = None\n\n    def initialize_population(self):\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, \n                                            (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def adaptive_differential_evolution(self, idx, func):\n        indices = [i for i in range(self.population_size) if i != idx]\n        np.random.shuffle(indices)\n        a, b, c = indices[:3]\n        mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        adaptive_factor = 0.3 + 0.7 * np.random.rand()  # Broadened adaptive factor range\n        mutant = self.population[idx] + adaptive_factor * (mutant - self.population[idx])\n        mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n        \n        cross_points = np.random.rand(self.dim) < self.crossover_probability\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        \n        trial = np.where(cross_points, mutant, self.population[idx])\n        trial_fitness = func(trial)\n        \n        if trial_fitness < self.fitness[idx]:\n            self.population[idx] = trial\n            self.fitness[idx] = trial_fitness\n\n    def stochastic_gradient_approx(self, idx, func):\n        gradient = np.random.normal(0, 1, self.dim)\n        adaptive_lr = self.learning_rate / (1 + 0.5 * idx / self.population_size)  # Adjusted adaptive learning rate\n        candidate = self.population[idx] - adaptive_lr * gradient\n        candidate += self.diversity_factor * np.random.normal(0, 1, self.dim)\n        candidate = np.clip(candidate, self.lower_bound, self.upper_bound)\n        candidate_fitness = func(candidate)\n        if candidate_fitness < self.fitness[idx]:\n            self.population[idx] = candidate\n            self.fitness[idx] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population()\n        evaluations = 0\n        self.evaluate_population(func)\n        evaluations += self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                self.adaptive_differential_evolution(i, func)\n                evaluations += 1\n\n                if evaluations >= self.budget:\n                    break\n\n                self.stochastic_gradient_approx(i, func)\n                evaluations += 1\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "EnhancedAdaptiveOptimizer", "description": "Enhanced adaptive optimizer combining differential evolution and gradient approximation with dynamic mutation and learning rate.", "configspace": "", "generation": 97, "fitness": 0.30446380675313744, "feedback": "The algorithm EnhancedAdaptiveOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.30 with standard deviation 0.20.", "error": "", "parent_id": "1459935d-0a5b-47de-8bfe-69f52cb3cb6c", "metadata": {"aucs": [0.7512649824111229, 0.7344960182773626, 0.7393743970028968, 0.6944629604126296, 0.7120653228961304, 0.7231873235649164, 0.7182945388681846, 0.7123254399131013, 0.7271015527453265, 0.42763638303284734, 0.4600597178699205, 0.48341469734767306, 0.47306416210413904, 0.45759000184101717, 0.4439359205791118, 0.45046505279475824, 0.4209447156014343, 0.47775765563554773, 0.09702409826567615, 0.11778741483769029, 0.12127257790527268, 0.11631008048747593, 0.11556845306053787, 0.12370626105877203, 0.11360900894171833, 0.10936816943177985, 0.10873933540184078, 0.08490688601663698, 0.09166849808691391, 0.09400125322455244, 0.08564518138011079, 0.10074234442035901, 0.09302560305138685, 0.08590370552782645, 0.09997536985634137, 0.09884601772882229, 0.8082438486647613, 0.7929325246847259, 0.8216242201508219, 0.7896176088281257, 0.7481480855137106, 0.7705737284057944, 0.8169409903681347, 0.7782293334962123, 0.7635012446868433, 0.28968243812669914, 0.30225176430897716, 0.3092077843836212, 0.31347687578213834, 0.30386704613382176, 0.29328041852280373, 0.2982789503789016, 0.28917172641985833, 0.3116922862366145, 0.6692138785041551, 0.7030943828879115, 0.7377190006377173, 0.7657851795222177, 0.6799095840113675, 0.687301700166721, 0.5107116282618889, 0.6731911028480835, 0.6996398606157996, 0.17000666162047706, 0.21878620135194315, 0.16945753064352664, 0.19854633664167176, 0.1470346418644487, 0.21343166724314488, 0.18851920145497347, 0.164559133447716, 0.15709933081188265, 0.1652824826928021, 0.17276282807538168, 0.19793126128248273, 0.17655854259911596, 0.15986870385168073, 0.1503875061825578, 0.18519918295659832, 0.15334711614777186, 0.17571081680151746, 0.2364853606659434, 0.22194837267676426, 0.2436986067433604, 0.2295693829028469, 0.2807158058411162, 0.228477566016564, 0.25280828934939537, 0.26100520727499066, 0.29099397780580194, 0.385566062432131, 0.391879265438828, 0.42866215238501115, 0.35095089663081924, 0.36367661910979077, 0.34739588479330963, 0.4311994880950598, 0.3504487267154657, 0.4101311749214178, 0.06227871353725245, 0.09740931546404441, 0.058172173568500685, 0.12410510778635175, 0.13300682854978896, 0.10376357872450115, 0.1058555460834586, 0.10153069580629381, 0.11491394828145562, 0.22333665973600514, 0.2539389086069215, 0.21624881364003068, 0.22217975142898672, 0.2508082296688585, 0.2561410146082973, 0.21445309720889194, 0.22836180132291206, 0.22893042241518624, 0.621690618698522, 0.6076085689948116, 0.6312262334460701, 0.5944079345546185, 0.5913556448120031, 0.5603776442746136, 0.6022443701327173, 0.6000744147327781, 0.5677232930450056, 0.10329635896838152, 0.10900736415557499, 0.10154652323034585, 0.09476057610658317, 0.10021962159976006, 0.10681030740401021, 0.11234391322663828, 0.11288988157490498, 0.11417557535844736, 0.1917897941632275, 0.20173083478428877, 0.21228054626498527, 0.18659656252172874, 0.20651533422447688, 0.17270014696278324, 0.22342438633936734, 0.21104686457127975, 0.18929430265527425, 0.2918530273380835, 0.3122518378676995, 0.3155934670266163, 0.29666623548399185, 0.30339167313378557, 0.3095068604300283, 0.3044649500536495, 0.31168237355823836, 0.29174943064985237, 0.239425554789244, 0.21652300620165976, 0.22680421359136138, 0.2442932198400476, 0.23330320014727746, 0.2520271241217319, 0.24862308840202585, 0.2479834169948547, 0.24644369232233776, 0.22031805149987516, 0.20094126162385295, 0.20176681434165034, 0.22570626517727177, 0.21174905858572546, 0.19089787131295088, 0.20744399575098016, 0.19749592574445118, 0.19618670481119826, 0.18961612727740584, 0.18688794633824113, 0.18792821852703667, 0.18505482931848216, 0.17875855761039328, 0.19035064901390875, 0.17844309043176287, 0.20316381616330892, 0.18183340302987294, 0.22850548138101, 0.18620191577423684, 0.5268295901115836, 0.3675244133679556, 0.4034006865211264, 0.40093762922639287, 0.4396377498549374, 0.3125034873998176, 0.43554618952625523, 0.44592559076961, 0.35695443622229006, 0.5064669534273929, 0.2993126479411865, 0.23230315725319428, 0.41113812211329204, 0.2734864825635568, 0.32610974233297074, 0.37627670363427546, 0.18057826331356464, 0.18618510283762768, 0.1724853958161472, 0.18873979203565883, 0.19095155478076298, 0.18534818731226876, 0.19493152017484738, 0.18822155249265504, 0.19458610246077324, 0.08298253649524978, 0.08230437558504, 0.08724019791045834, 0.07986267336564579, 0.07930450132272249, 0.08411214107364062, 0.0789781977981211, 0.08348901570906186, 0.08218870742200701]}, "mutation_prompt": null}
{"id": "5e71b650-7925-4a91-a1df-a72c12a49678", "solution": "import numpy as np\n\nclass EnhancedProbabilisticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = 40\n        self.mutation_factor = 0.85\n        self.crossover_probability = 0.95\n        self.learning_rate = 0.2\n        self.diversity_factor = 0.1\n        self.population = None\n        self.fitness = None\n\n    def initialize_population(self):\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, \n                                            (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def adaptive_differential_evolution(self, idx, func):\n        indices = [i for i in range(self.population_size) if i != idx]\n        np.random.shuffle(indices)\n        a, b, c = indices[:3]\n        mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        adaptive_factor = 0.4 + 0.6 * np.random.rand()\n        mutant = self.population[idx] + adaptive_factor * (mutant - self.population[idx])\n        mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n        \n        cross_points = np.random.rand(self.dim) < self.crossover_probability\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        \n        trial = np.where(cross_points, mutant, self.population[idx])\n        trial_fitness = func(trial)\n        \n        if trial_fitness < self.fitness[idx]:\n            self.population[idx] = trial\n            self.fitness[idx] = trial_fitness\n\n    def stochastic_gradient_approx(self, idx, func):\n        gradient = np.random.normal(0, 1, self.dim)\n        adaptive_lr = self.learning_rate / (1 + idx / self.population_size)\n        candidate = self.population[idx] - adaptive_lr * gradient\n        candidate += self.diversity_factor * np.random.normal(0, 1, self.dim)\n        candidate = np.clip(candidate, self.lower_bound, self.upper_bound)\n        candidate_fitness = func(candidate)\n        if candidate_fitness < self.fitness[idx]:\n            self.population[idx] = candidate\n            self.fitness[idx] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population()\n        evaluations = 0\n        self.evaluate_population(func)\n        evaluations += self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                self.adaptive_differential_evolution(i, func)\n                evaluations += 1\n\n                if evaluations >= self.budget:\n                    break\n\n                self.stochastic_gradient_approx(i, func)\n                evaluations += 1\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "EnhancedProbabilisticOptimizer", "description": "Enhanced probabilistic hybrid optimizer balancing exploration and exploitation through diversity-oriented selection and adaptive parameter tuning for optimal convergence.", "configspace": "", "generation": 38, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "1459935d-0a5b-47de-8bfe-69f52cb3cb6c", "metadata": {"aucs": [0.796727445346743, 0.7946092935536371, 0.8025157468331956, 0.7795890428061506, 0.7652866403837684, 0.7888980264133363, 0.7851539828124771, 0.7801166288092796, 0.778684214476141, 0.6066596860215113, 0.5509488918434079, 0.584709500115218, 0.6151880393982149, 0.6128285196462557, 0.5928554548480235, 0.18781852246903208, 0.6051102165828588, 0.6050716765741883, 0.12056849613284826, 0.10627786970516262, 0.11092740344636509, 0.12458142503345515, 0.10225070914012868, 0.10214739878052792, 0.11007052847089493, 0.11303443664409152, 0.11346829363668931, 0.09058514750802149, 0.09136224141572435, 0.09346304265481342, 0.08413688806219866, 0.10560994817344638, 0.09386944951658027, 0.1066829182603829, 0.09525027475772718, 0.09078080677184064, 0.8379701410373176, 0.8069154658397715, 0.8108437032002754, 0.8202640448333322, 0.7815128713795206, 0.8451084830274826, 0.836376388134602, 0.8441441978939892, 0.8509693104465095, 0.37150586919602946, 0.34273942956965175, 0.3313822553845982, 0.34968755932637086, 0.32960730205573996, 0.3754984208710611, 0.38284578850911233, 0.3840456442426208, 0.3280564413760412, 0.7576709220936438, 0.7335556841917643, 0.7658090217912067, 0.7934167558179147, 0.8025617081483369, 0.7999764269594076, 0.7299292035530152, 0.7697788108067551, 0.7651252822797626, 0.16167439560696928, 0.18696031846775318, 0.21158580997319565, 0.244349141547606, 0.2890559956537958, 0.16298570563996873, 0.21750653437837209, 0.21122767559906996, 0.22550289679700497, 0.15305768362463734, 0.14884944831507896, 0.18600965489753707, 0.1541357033150358, 0.19832744782165168, 0.18983314847500432, 0.1953210887165725, 0.20165646735698872, 0.13088573983753993, 0.489476775395826, 0.460206761694571, 0.280690722270316, 0.34064732845138956, 0.522953329166158, 0.46452239779932414, 0.4196789194960461, 0.4727221489210359, 0.44131563048738687, 0.6540713674624187, 0.6128130384068273, 0.5553131009305013, 0.5776200055109268, 0.4723536786910414, 0.6219680956281209, 0.6455804612075908, 0.6109257481083044, 0.5996992464126676, 0.15220127888973134, 0.11018658007737869, 0.2413715712685326, 0.2296484097510183, 0.2572737636354798, 0.10622686325653308, 0.14604068872930986, 0.14809138800719435, 0.12257392403735012, 0.36655256082417187, 0.34632185847096675, 0.3516395670957846, 0.33828319802853724, 0.3572041238804262, 0.35621925345698524, 0.34628041224843775, 0.36205674625584605, 0.3801826028652373, 0.7284525303733058, 0.7334889541026953, 0.7386557191632159, 0.7055754468414382, 0.7222856768071027, 0.7194774022217179, 0.518854950686749, 0.7321096452535195, 0.7397060954282213, 0.10374399773694243, 0.104282622260642, 0.1086381702881759, 0.11429613558107654, 0.11511930056195652, 0.10035386421161252, 0.09599830676572241, 0.11347281801819442, 0.11470225552281121, 0.14952999643648723, 0.16817216547016145, 0.17310734903563885, 0.1855986179207637, 0.18359480037739373, 0.19508916572226054, 0.20087729718879477, 0.17807185486405164, 0.20119801391244463, 0.3682121014468256, 0.35073539948175425, 0.3330439038672147, 0.4157077631363848, 0.37474021495439724, 0.36760475650845414, 0.38008256501576043, 0.39084781839078786, 0.3786757352451906, 0.30037946434366947, 0.2861866860206934, 0.2891587957817804, 0.2835627691001067, 0.2779602895144492, 0.28387893685888044, 0.29400381025955125, 0.3244615423116647, 0.30188351717337136, 0.20494558183242018, 0.19905376550038034, 0.20406514428030742, 0.2229753496453719, 0.1980660060283188, 0.21539123082194234, 0.21928087314384304, 0.20854328449838244, 0.2102975078755156, 0.18082128476991477, 0.1831737258940077, 0.18089956975930732, 0.18277323416104252, 0.1792695777259361, 0.17816092407226292, 0.19628636488975992, 0.19237612183491615, 0.1874718333986961, 0.6294666203187917, 0.602690555999032, 0.5454952462675013, 0.7727337342883757, 0.25677125313719296, 0.588947972014151, 0.5053651729784945, 0.42433758940449484, 0.610090527727432, 0.7148365734773846, 0.39316249107203016, 0.4609914040189693, 0.5414475146943718, 0.3738614312609654, 0.2972834077003401, 0.2719039283446757, 0.4464259994844204, 0.6865015215956998, 0.18541089636272012, 0.17521600614441235, 0.18498258988690697, 0.1923345960637889, 0.19442272937612437, 0.18424555156947642, 0.17449564646480575, 0.18372628686318526, 0.18823517758491137, 0.07953923390817585, 0.08429672806217459, 0.08351517483047155, 0.07872531348008505, 0.0885755282031725, 0.09071254814221597, 0.07022751225803625, 0.07907174920771143, 0.08022853716110878]}, "mutation_prompt": null}
{"id": "b3f08453-7751-45a4-ba73-1e08988b9a40", "solution": "import numpy as np\n\nclass HybridAdaptiveDiversityOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lower_bound = -5.0\n        self.upper_bound = 5.0\n        self.population_size = 50\n        self.mutation_factor = 0.9\n        self.crossover_probability = 0.9\n        self.learning_rate = 0.15\n        self.diversity_factor = 0.15\n        self.population = None\n        self.fitness = None\n\n    def initialize_population(self):\n        self.population = np.random.uniform(self.lower_bound, self.upper_bound, \n                                            (self.population_size, self.dim))\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n\n    def adaptive_differential_evolution(self, idx, func):\n        indices = [i for i in range(self.population_size) if i != idx]\n        np.random.shuffle(indices)\n        a, b, c = indices[:3]\n        mutant = self.population[a] + self.mutation_factor * (self.population[b] - self.population[c])\n        adaptive_factor = 0.5 + 0.5 * np.random.rand()\n        mutant = self.population[idx] + adaptive_factor * (mutant - self.population[idx])\n        mutant = np.clip(mutant, self.lower_bound, self.upper_bound)\n        \n        cross_points = np.random.rand(self.dim) < self.crossover_probability\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        \n        trial = np.where(cross_points, mutant, self.population[idx])\n        trial_fitness = func(trial)\n        \n        if trial_fitness < self.fitness[idx]:\n            self.population[idx] = trial\n            self.fitness[idx] = trial_fitness\n\n    def stochastic_gradient_approx(self, idx, func):\n        gradient = np.random.normal(0, 1, self.dim)\n        adaptive_lr = self.learning_rate / (1 + idx / (2 * self.population_size))\n        candidate = self.population[idx] - adaptive_lr * gradient\n        candidate += self.diversity_factor * np.random.normal(0, 1, self.dim)\n        candidate = np.clip(candidate, self.lower_bound, self.upper_bound)\n        candidate_fitness = func(candidate)\n        if candidate_fitness < self.fitness[idx]:\n            self.population[idx] = candidate\n            self.fitness[idx] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population()\n        evaluations = 0\n        self.evaluate_population(func)\n        evaluations += self.population_size\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                self.adaptive_differential_evolution(i, func)\n                evaluations += 1\n\n                if evaluations >= self.budget:\n                    break\n\n                self.stochastic_gradient_approx(i, func)\n                evaluations += 1\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "HybridAdaptiveDiversityOptimizer", "description": "Hybrid Differential Evolution with Adaptive Diversity leveraging self-adaptive strategies and diversity-driven enhancements for robust optimization performance.", "configspace": "", "generation": 99, "fitness": 0.28637106575639126, "feedback": "The algorithm HybridAdaptiveDiversityOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.29 with standard deviation 0.21.", "error": "", "parent_id": "1459935d-0a5b-47de-8bfe-69f52cb3cb6c", "metadata": {"aucs": [0.6749982765983387, 0.6680985102179224, 0.6838033053202066, 0.6894691336366922, 0.6623035978592433, 0.6650051848398081, 0.6749033426259483, 0.6671751656425403, 0.6933897883082899, 0.3991789610788161, 0.3643541776333199, 0.3734473817159082, 0.3814581183529764, 0.4003058827433521, 0.39892018045109257, 0.4011867951215531, 0.39205241986537953, 0.39330683223028384, 0.09759416033370782, 0.10212633204917054, 0.10827572291929277, 0.11177729007983073, 0.10235621465404365, 0.10176527013173531, 0.10437964747922035, 0.11875736676310811, 0.13090478507071335, 0.08741169655225223, 0.08094376680797866, 0.08835071399443895, 0.08536643873004268, 0.09010534495793987, 0.08676076746403427, 0.08431053234923691, 0.08239942297493275, 0.09028813272376357, 0.9224757852621316, 0.8917039647666049, 0.9595490260117713, 0.849455054952259, 0.8534989063779939, 0.8409671460047666, 0.8456780383490028, 0.8758726359362297, 0.8382056701501527, 0.2973393762319685, 0.27342127710574315, 0.2990047937561904, 0.3047503670313991, 0.2760262274065993, 0.2931561278807274, 0.29000005772242876, 0.29144698412134384, 0.2794121311058747, 0.6594948557257373, 0.5614956325378277, 0.6193778105507193, 0.6432396913583673, 0.6780869658725743, 0.7034207106210251, 0.5824367901754515, 0.642300025003059, 0.7283662128472033, 0.17701477313910186, 0.1538642183893919, 0.15979574990757484, 0.17801741259913684, 0.15939459735510642, 0.18502442860003454, 0.1710237376383309, 0.17167041228727575, 0.18353495975409495, 0.1573278101679232, 0.15498184208867438, 0.2007668078771898, 0.14522478920205562, 0.1749867357975825, 0.20185168289679956, 0.18318474021867537, 0.17012355815953617, 0.17565994037910437, 0.16268601875433963, 0.1916309318391627, 0.18700063925403754, 0.18246677927223476, 0.1943535944559206, 0.1823233581434084, 0.21076686510123122, 0.1890720612252016, 0.1985463819946801, 0.3045289298123044, 0.32454225511043155, 0.3458796680319788, 0.31652701057079213, 0.3023577844816354, 0.2988271603781404, 0.353720023588298, 0.3298939341935314, 0.3526898141880176, 0.052062977683608236, 0.0606308160394905, 0.06987129047133456, 0.06849290949688924, 0.06328922173728546, 0.087138735135042, 0.0657060601042545, 0.08437549304045244, 0.07414773325361912, 0.22005610092963657, 0.18900947849277383, 0.18729436842794422, 0.20384257983568244, 0.20661328305406668, 0.2099714482951751, 0.1775768156323464, 0.18498265454112173, 0.19261935193970225, 0.548568084096747, 0.5192802292099477, 0.545299497158729, 0.5372214298193811, 0.5248576488599683, 0.5241646445372043, 0.5591292606515064, 0.5618059691948728, 0.5486370307472033, 0.10634761027235984, 0.08919140198460462, 0.10142955515431662, 0.08765777988916745, 0.0978522960674727, 0.09788532964307795, 0.09565779212776104, 0.08934713541846995, 0.10649207898727642, 0.18901701712869634, 0.21123268577211474, 0.19442233036321577, 0.20702425242628064, 0.17798139004197078, 0.18970616666322226, 0.20183906944332164, 0.18428917149010648, 0.16235404932866437, 0.2817070842996452, 0.28394669759744406, 0.3034578827077209, 0.2707972268923633, 0.2669789538734265, 0.30394721962480764, 0.3002550043955845, 0.2848840430147429, 0.31037056390627493, 0.22364228207342773, 0.24162167856345496, 0.24024099293180223, 0.20536908141567745, 0.2159370337142995, 0.20622096210513796, 0.20907410510192825, 0.21049688591119142, 0.2039260645174471, 0.20868126256076458, 0.2030077374186846, 0.22967176034540548, 0.19449093310730525, 0.19233615327448084, 0.19164126456312414, 0.20256700521175508, 0.21137656872113297, 0.18664804915816924, 0.184359969391091, 0.18384426696046907, 0.17922955360429838, 0.19371307652176517, 0.17936689902164693, 0.18523958291864306, 0.18224364923094172, 0.19684023848025245, 0.1967150171466283, 0.3579647405778965, 0.3144420170979637, 0.3530272821355208, 0.4282034084883637, 0.28343556091373, 0.39604022759992663, 0.3455825688839501, 0.408444032997178, 0.538574708985053, 0.4328210356828307, 0.3551093566014032, 0.4527313694075358, 0.21380169629475065, 0.3710466557977746, 0.19572364165352074, 0.20594890285909018, 0.22056107052599228, 0.32953105346013445, 0.18572653342727707, 0.17071832812409138, 0.1827944719756689, 0.1879960229684865, 0.19151456000979594, 0.1793446587521048, 0.18736063610710263, 0.17879445926555304, 0.19523280782629937, 0.07584616944313982, 0.07247521939591295, 0.07788472519485334, 0.07716935732283803, 0.07891954387182232, 0.0730960043252935, 0.08839575397972999, 0.08363921271741515, 0.07635910184428363]}, "mutation_prompt": null}
